2021-06-04 13:24:18 | [train_policy] Logging to ../models/adni_split0_TRPO_11_7.0_delta_fixed_1.0_1000_1000_2.0_fixed_full_1.0_1.0_11_old_MMSE_32
2021-06-04 13:24:18 | [train_policy] Setting seed to 1
2021-06-04 13:24:20 | [train_policy] Obtaining samples...
2021-06-04 13:24:20 | [train_policy] epoch #0 | Obtaining samples for iteration 0...
2021-06-04 13:24:21 | [train_policy] epoch #0 | Logging diagnostics...
2021-06-04 13:24:21 | [train_policy] epoch #0 | Optimizing policy...
2021-06-04 13:24:21 | [train_policy] epoch #0 | Computing loss before
2021-06-04 13:24:21 | [train_policy] epoch #0 | Computing KL before
2021-06-04 13:24:21 | [train_policy] epoch #0 | Optimizing
2021-06-04 13:24:21 | [train_policy] epoch #0 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:21 | [train_policy] epoch #0 | computing loss before
2021-06-04 13:24:21 | [train_policy] epoch #0 | computing gradient
2021-06-04 13:24:22 | [train_policy] epoch #0 | gradient computed
2021-06-04 13:24:22 | [train_policy] epoch #0 | computing descent direction
2021-06-04 13:24:23 | [train_policy] epoch #0 | descent direction computed
2021-06-04 13:24:23 | [train_policy] epoch #0 | backtrack iters: 0
2021-06-04 13:24:23 | [train_policy] epoch #0 | optimization finished
2021-06-04 13:24:23 | [train_policy] epoch #0 | Computing KL after
2021-06-04 13:24:23 | [train_policy] epoch #0 | Computing loss after
2021-06-04 13:24:23 | [train_policy] epoch #0 | Fitting baseline...
2021-06-04 13:24:23 | [train_policy] epoch #0 | Saving snapshot...
2021-06-04 13:24:23 | [train_policy] epoch #0 | Saved
2021-06-04 13:24:23 | [train_policy] epoch #0 | Time 3.83 s
2021-06-04 13:24:23 | [train_policy] epoch #0 | EpochTime 3.83 s
---------------------------------------  ----------------
EnvExecTime                                   0.695638
Evaluation/AverageDiscountedReturn        -5122.66
Evaluation/AverageReturn                  -5122.66
Evaluation/CompletionRate                     0
Evaluation/Iteration                          0
Evaluation/MaxReturn                        -46.1096
Evaluation/MinReturn                     -22000
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       6669.7
Extras/EpisodeRewardMean                  -5122.66
LinearFeatureBaseline/ExplainedVariance      -2.73945e-09
PolicyExecTime                                0.490795
ProcessExecTime                               0.0308578
TotalEnvSteps                              1012
policy/Entropy                                2.83575
policy/KL                                     0.00934829
policy/KLBefore                               0
policy/LossAfter                             -0.0567007
policy/LossBefore                            -7.89231e-09
policy/Perplexity                            17.0431
policy/dLoss                                  0.0567007
---------------------------------------  ----------------
2021-06-04 13:24:23 | [train_policy] epoch #1 | Obtaining samples for iteration 1...
2021-06-04 13:24:24 | [train_policy] epoch #1 | Logging diagnostics...
2021-06-04 13:24:24 | [train_policy] epoch #1 | Optimizing policy...
2021-06-04 13:24:24 | [train_policy] epoch #1 | Computing loss before
2021-06-04 13:24:24 | [train_policy] epoch #1 | Computing KL before
2021-06-04 13:24:24 | [train_policy] epoch #1 | Optimizing
2021-06-04 13:24:24 | [train_policy] epoch #1 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:24 | [train_policy] epoch #1 | computing loss before
2021-06-04 13:24:24 | [train_policy] epoch #1 | computing gradient
2021-06-04 13:24:24 | [train_policy] epoch #1 | gradient computed
2021-06-04 13:24:24 | [train_policy] epoch #1 | computing descent direction
2021-06-04 13:24:24 | [train_policy] epoch #1 | descent direction computed
2021-06-04 13:24:24 | [train_policy] epoch #1 | backtrack iters: 1
2021-06-04 13:24:24 | [train_policy] epoch #1 | optimization finished
2021-06-04 13:24:24 | [train_policy] epoch #1 | Computing KL after
2021-06-04 13:24:24 | [train_policy] epoch #1 | Computing loss after
2021-06-04 13:24:24 | [train_policy] epoch #1 | Fitting baseline...
2021-06-04 13:24:24 | [train_policy] epoch #1 | Saving snapshot...
2021-06-04 13:24:24 | [train_policy] epoch #1 | Saved
2021-06-04 13:24:24 | [train_policy] epoch #1 | Time 4.61 s
2021-06-04 13:24:24 | [train_policy] epoch #1 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28357
Evaluation/AverageDiscountedReturn        -3683.34
Evaluation/AverageReturn                  -3683.34
Evaluation/CompletionRate                     0
Evaluation/Iteration                          1
Evaluation/MaxReturn                        -46.6597
Evaluation/MinReturn                     -21882.7
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       5979.86
Extras/EpisodeRewardMean                  -3905.58
LinearFeatureBaseline/ExplainedVariance       0.207244
PolicyExecTime                                0.216426
ProcessExecTime                               0.0309243
TotalEnvSteps                              2024
policy/Entropy                                2.81225
policy/KL                                     0.0067576
policy/KLBefore                               0
policy/LossAfter                             -0.0501936
policy/LossBefore                            -7.06774e-09
policy/Perplexity                            16.6473
policy/dLoss                                  0.0501936
---------------------------------------  ----------------
2021-06-04 13:24:24 | [train_policy] epoch #2 | Obtaining samples for iteration 2...
2021-06-04 13:24:25 | [train_policy] epoch #2 | Logging diagnostics...
2021-06-04 13:24:25 | [train_policy] epoch #2 | Optimizing policy...
2021-06-04 13:24:25 | [train_policy] epoch #2 | Computing loss before
2021-06-04 13:24:25 | [train_policy] epoch #2 | Computing KL before
2021-06-04 13:24:25 | [train_policy] epoch #2 | Optimizing
2021-06-04 13:24:25 | [train_policy] epoch #2 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:25 | [train_policy] epoch #2 | computing loss before
2021-06-04 13:24:25 | [train_policy] epoch #2 | computing gradient
2021-06-04 13:24:25 | [train_policy] epoch #2 | gradient computed
2021-06-04 13:24:25 | [train_policy] epoch #2 | computing descent direction
2021-06-04 13:24:25 | [train_policy] epoch #2 | descent direction computed
2021-06-04 13:24:25 | [train_policy] epoch #2 | backtrack iters: 1
2021-06-04 13:24:25 | [train_policy] epoch #2 | optimization finished
2021-06-04 13:24:25 | [train_policy] epoch #2 | Computing KL after
2021-06-04 13:24:25 | [train_policy] epoch #2 | Computing loss after
2021-06-04 13:24:25 | [train_policy] epoch #2 | Fitting baseline...
2021-06-04 13:24:25 | [train_policy] epoch #2 | Saving snapshot...
2021-06-04 13:24:25 | [train_policy] epoch #2 | Saved
2021-06-04 13:24:25 | [train_policy] epoch #2 | Time 5.43 s
2021-06-04 13:24:25 | [train_policy] epoch #2 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.291049
Evaluation/AverageDiscountedReturn        -1294.8
Evaluation/AverageReturn                  -1294.8
Evaluation/CompletionRate                     0
Evaluation/Iteration                          2
Evaluation/MaxReturn                        -44.869
Evaluation/MinReturn                     -14289.2
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       2628.34
Extras/EpisodeRewardMean                  -1564.55
LinearFeatureBaseline/ExplainedVariance      -0.248466
PolicyExecTime                                0.250659
ProcessExecTime                               0.0318861
TotalEnvSteps                              3036
policy/Entropy                                2.81041
policy/KL                                     0.00681839
policy/KLBefore                               0
policy/LossAfter                             -0.0387484
policy/LossBefore                             2.23812e-08
policy/Perplexity                            16.6168
policy/dLoss                                  0.0387485
---------------------------------------  ----------------
2021-06-04 13:24:25 | [train_policy] epoch #3 | Obtaining samples for iteration 3...
2021-06-04 13:24:26 | [train_policy] epoch #3 | Logging diagnostics...
2021-06-04 13:24:26 | [train_policy] epoch #3 | Optimizing policy...
2021-06-04 13:24:26 | [train_policy] epoch #3 | Computing loss before
2021-06-04 13:24:26 | [train_policy] epoch #3 | Computing KL before
2021-06-04 13:24:26 | [train_policy] epoch #3 | Optimizing
2021-06-04 13:24:26 | [train_policy] epoch #3 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:26 | [train_policy] epoch #3 | computing loss before
2021-06-04 13:24:26 | [train_policy] epoch #3 | computing gradient
2021-06-04 13:24:26 | [train_policy] epoch #3 | gradient computed
2021-06-04 13:24:26 | [train_policy] epoch #3 | computing descent direction
2021-06-04 13:24:26 | [train_policy] epoch #3 | descent direction computed
2021-06-04 13:24:26 | [train_policy] epoch #3 | backtrack iters: 0
2021-06-04 13:24:26 | [train_policy] epoch #3 | optimization finished
2021-06-04 13:24:26 | [train_policy] epoch #3 | Computing KL after
2021-06-04 13:24:26 | [train_policy] epoch #3 | Computing loss after
2021-06-04 13:24:26 | [train_policy] epoch #3 | Fitting baseline...
2021-06-04 13:24:26 | [train_policy] epoch #3 | Saving snapshot...
2021-06-04 13:24:26 | [train_policy] epoch #3 | Saved
2021-06-04 13:24:26 | [train_policy] epoch #3 | Time 6.21 s
2021-06-04 13:24:26 | [train_policy] epoch #3 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.286147
Evaluation/AverageDiscountedReturn         -744.612
Evaluation/AverageReturn                   -744.612
Evaluation/CompletionRate                     0
Evaluation/Iteration                          3
Evaluation/MaxReturn                        -39.7719
Evaluation/MinReturn                     -12077
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       1983.87
Extras/EpisodeRewardMean                   -770.41
LinearFeatureBaseline/ExplainedVariance       0.148907
PolicyExecTime                                0.216938
ProcessExecTime                               0.0314178
TotalEnvSteps                              4048
policy/Entropy                                2.78259
policy/KL                                     0.00985825
policy/KLBefore                               0
policy/LossAfter                             -0.0420206
policy/LossBefore                            -5.18301e-09
policy/Perplexity                            16.1608
policy/dLoss                                  0.0420206
---------------------------------------  ----------------
2021-06-04 13:24:26 | [train_policy] epoch #4 | Obtaining samples for iteration 4...
2021-06-04 13:24:26 | [train_policy] epoch #4 | Logging diagnostics...
2021-06-04 13:24:26 | [train_policy] epoch #4 | Optimizing policy...
2021-06-04 13:24:26 | [train_policy] epoch #4 | Computing loss before
2021-06-04 13:24:26 | [train_policy] epoch #4 | Computing KL before
2021-06-04 13:24:26 | [train_policy] epoch #4 | Optimizing
2021-06-04 13:24:26 | [train_policy] epoch #4 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:26 | [train_policy] epoch #4 | computing loss before
2021-06-04 13:24:26 | [train_policy] epoch #4 | computing gradient
2021-06-04 13:24:26 | [train_policy] epoch #4 | gradient computed
2021-06-04 13:24:26 | [train_policy] epoch #4 | computing descent direction
2021-06-04 13:24:27 | [train_policy] epoch #4 | descent direction computed
2021-06-04 13:24:27 | [train_policy] epoch #4 | backtrack iters: 0
2021-06-04 13:24:27 | [train_policy] epoch #4 | optimization finished
2021-06-04 13:24:27 | [train_policy] epoch #4 | Computing KL after
2021-06-04 13:24:27 | [train_policy] epoch #4 | Computing loss after
2021-06-04 13:24:27 | [train_policy] epoch #4 | Fitting baseline...
2021-06-04 13:24:27 | [train_policy] epoch #4 | Saving snapshot...
2021-06-04 13:24:27 | [train_policy] epoch #4 | Saved
2021-06-04 13:24:27 | [train_policy] epoch #4 | Time 7.00 s
2021-06-04 13:24:27 | [train_policy] epoch #4 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286261
Evaluation/AverageDiscountedReturn        -288.814
Evaluation/AverageReturn                  -288.814
Evaluation/CompletionRate                    0
Evaluation/Iteration                         4
Evaluation/MaxReturn                       -38.0542
Evaluation/MinReturn                     -6051.79
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       876.987
Extras/EpisodeRewardMean                  -290.61
LinearFeatureBaseline/ExplainedVariance     -0.239738
PolicyExecTime                               0.220071
ProcessExecTime                              0.0314536
TotalEnvSteps                             5060
policy/Entropy                               2.74427
policy/KL                                    0.00955932
policy/KLBefore                              0
policy/LossAfter                            -0.0190431
policy/LossBefore                            1.81405e-08
policy/Perplexity                           15.5533
policy/dLoss                                 0.0190431
---------------------------------------  ---------------
2021-06-04 13:24:27 | [train_policy] epoch #5 | Obtaining samples for iteration 5...
2021-06-04 13:24:27 | [train_policy] epoch #5 | Logging diagnostics...
2021-06-04 13:24:27 | [train_policy] epoch #5 | Optimizing policy...
2021-06-04 13:24:27 | [train_policy] epoch #5 | Computing loss before
2021-06-04 13:24:27 | [train_policy] epoch #5 | Computing KL before
2021-06-04 13:24:27 | [train_policy] epoch #5 | Optimizing
2021-06-04 13:24:27 | [train_policy] epoch #5 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:27 | [train_policy] epoch #5 | computing loss before
2021-06-04 13:24:27 | [train_policy] epoch #5 | computing gradient
2021-06-04 13:24:27 | [train_policy] epoch #5 | gradient computed
2021-06-04 13:24:27 | [train_policy] epoch #5 | computing descent direction
2021-06-04 13:24:27 | [train_policy] epoch #5 | descent direction computed
2021-06-04 13:24:27 | [train_policy] epoch #5 | backtrack iters: 1
2021-06-04 13:24:27 | [train_policy] epoch #5 | optimization finished
2021-06-04 13:24:27 | [train_policy] epoch #5 | Computing KL after
2021-06-04 13:24:27 | [train_policy] epoch #5 | Computing loss after
2021-06-04 13:24:27 | [train_policy] epoch #5 | Fitting baseline...
2021-06-04 13:24:27 | [train_policy] epoch #5 | Saving snapshot...
2021-06-04 13:24:27 | [train_policy] epoch #5 | Saved
2021-06-04 13:24:27 | [train_policy] epoch #5 | Time 7.80 s
2021-06-04 13:24:27 | [train_policy] epoch #5 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287271
Evaluation/AverageDiscountedReturn         -506.516
Evaluation/AverageReturn                   -506.516
Evaluation/CompletionRate                     0
Evaluation/Iteration                          5
Evaluation/MaxReturn                        -44.9822
Evaluation/MinReturn                     -12431.9
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       1745.84
Extras/EpisodeRewardMean                   -471.339
LinearFeatureBaseline/ExplainedVariance       0.0344033
PolicyExecTime                                0.228582
ProcessExecTime                               0.0315831
TotalEnvSteps                              6072
policy/Entropy                                2.69098
policy/KL                                     0.00695014
policy/KLBefore                               0
policy/LossAfter                             -0.0366384
policy/LossBefore                            -1.06016e-08
policy/Perplexity                            14.746
policy/dLoss                                  0.0366384
---------------------------------------  ----------------
2021-06-04 13:24:27 | [train_policy] epoch #6 | Obtaining samples for iteration 6...
2021-06-04 13:24:28 | [train_policy] epoch #6 | Logging diagnostics...
2021-06-04 13:24:28 | [train_policy] epoch #6 | Optimizing policy...
2021-06-04 13:24:28 | [train_policy] epoch #6 | Computing loss before
2021-06-04 13:24:28 | [train_policy] epoch #6 | Computing KL before
2021-06-04 13:24:28 | [train_policy] epoch #6 | Optimizing
2021-06-04 13:24:28 | [train_policy] epoch #6 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:28 | [train_policy] epoch #6 | computing loss before
2021-06-04 13:24:28 | [train_policy] epoch #6 | computing gradient
2021-06-04 13:24:28 | [train_policy] epoch #6 | gradient computed
2021-06-04 13:24:28 | [train_policy] epoch #6 | computing descent direction
2021-06-04 13:24:28 | [train_policy] epoch #6 | descent direction computed
2021-06-04 13:24:28 | [train_policy] epoch #6 | backtrack iters: 0
2021-06-04 13:24:28 | [train_policy] epoch #6 | optimization finished
2021-06-04 13:24:28 | [train_policy] epoch #6 | Computing KL after
2021-06-04 13:24:28 | [train_policy] epoch #6 | Computing loss after
2021-06-04 13:24:28 | [train_policy] epoch #6 | Fitting baseline...
2021-06-04 13:24:28 | [train_policy] epoch #6 | Saving snapshot...
2021-06-04 13:24:28 | [train_policy] epoch #6 | Saved
2021-06-04 13:24:28 | [train_policy] epoch #6 | Time 8.57 s
2021-06-04 13:24:28 | [train_policy] epoch #6 | EpochTime 0.75 s
---------------------------------------  --------------
EnvExecTime                                  0.287556
Evaluation/AverageDiscountedReturn        -105.083
Evaluation/AverageReturn                  -105.083
Evaluation/CompletionRate                    0
Evaluation/Iteration                         6
Evaluation/MaxReturn                       -45.1769
Evaluation/MinReturn                     -2550.05
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       269.606
Extras/EpisodeRewardMean                  -112.693
LinearFeatureBaseline/ExplainedVariance     -7.88974
PolicyExecTime                               0.208782
ProcessExecTime                              0.031651
TotalEnvSteps                             7084
policy/Entropy                               2.68803
policy/KL                                    0.00845201
policy/KLBefore                              0
policy/LossAfter                            -0.0229809
policy/LossBefore                            3.8637e-08
policy/Perplexity                           14.7027
policy/dLoss                                 0.022981
---------------------------------------  --------------
2021-06-04 13:24:28 | [train_policy] epoch #7 | Obtaining samples for iteration 7...
2021-06-04 13:24:29 | [train_policy] epoch #7 | Logging diagnostics...
2021-06-04 13:24:29 | [train_policy] epoch #7 | Optimizing policy...
2021-06-04 13:24:29 | [train_policy] epoch #7 | Computing loss before
2021-06-04 13:24:29 | [train_policy] epoch #7 | Computing KL before
2021-06-04 13:24:29 | [train_policy] epoch #7 | Optimizing
2021-06-04 13:24:29 | [train_policy] epoch #7 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:29 | [train_policy] epoch #7 | computing loss before
2021-06-04 13:24:29 | [train_policy] epoch #7 | computing gradient
2021-06-04 13:24:29 | [train_policy] epoch #7 | gradient computed
2021-06-04 13:24:29 | [train_policy] epoch #7 | computing descent direction
2021-06-04 13:24:29 | [train_policy] epoch #7 | descent direction computed
2021-06-04 13:24:29 | [train_policy] epoch #7 | backtrack iters: 1
2021-06-04 13:24:29 | [train_policy] epoch #7 | optimization finished
2021-06-04 13:24:29 | [train_policy] epoch #7 | Computing KL after
2021-06-04 13:24:29 | [train_policy] epoch #7 | Computing loss after
2021-06-04 13:24:29 | [train_policy] epoch #7 | Fitting baseline...
2021-06-04 13:24:29 | [train_policy] epoch #7 | Saving snapshot...
2021-06-04 13:24:29 | [train_policy] epoch #7 | Saved
2021-06-04 13:24:29 | [train_policy] epoch #7 | Time 9.38 s
2021-06-04 13:24:29 | [train_policy] epoch #7 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.288023
Evaluation/AverageDiscountedReturn        -173.397
Evaluation/AverageReturn                  -173.397
Evaluation/CompletionRate                    0
Evaluation/Iteration                         7
Evaluation/MaxReturn                       -48.3632
Evaluation/MinReturn                     -2932.42
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       430.828
Extras/EpisodeRewardMean                  -164.712
LinearFeatureBaseline/ExplainedVariance      0.0131095
PolicyExecTime                               0.22699
ProcessExecTime                              0.0317173
TotalEnvSteps                             8096
policy/Entropy                               2.661
policy/KL                                    0.00663461
policy/KLBefore                              0
policy/LossAfter                            -0.0328874
policy/LossBefore                           -5.6542e-09
policy/Perplexity                           14.3105
policy/dLoss                                 0.0328874
---------------------------------------  --------------
2021-06-04 13:24:29 | [train_policy] epoch #8 | Obtaining samples for iteration 8...
2021-06-04 13:24:30 | [train_policy] epoch #8 | Logging diagnostics...
2021-06-04 13:24:30 | [train_policy] epoch #8 | Optimizing policy...
2021-06-04 13:24:30 | [train_policy] epoch #8 | Computing loss before
2021-06-04 13:24:30 | [train_policy] epoch #8 | Computing KL before
2021-06-04 13:24:30 | [train_policy] epoch #8 | Optimizing
2021-06-04 13:24:30 | [train_policy] epoch #8 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:30 | [train_policy] epoch #8 | computing loss before
2021-06-04 13:24:30 | [train_policy] epoch #8 | computing gradient
2021-06-04 13:24:30 | [train_policy] epoch #8 | gradient computed
2021-06-04 13:24:30 | [train_policy] epoch #8 | computing descent direction
2021-06-04 13:24:30 | [train_policy] epoch #8 | descent direction computed
2021-06-04 13:24:30 | [train_policy] epoch #8 | backtrack iters: 1
2021-06-04 13:24:30 | [train_policy] epoch #8 | optimization finished
2021-06-04 13:24:30 | [train_policy] epoch #8 | Computing KL after
2021-06-04 13:24:30 | [train_policy] epoch #8 | Computing loss after
2021-06-04 13:24:30 | [train_policy] epoch #8 | Fitting baseline...
2021-06-04 13:24:30 | [train_policy] epoch #8 | Saving snapshot...
2021-06-04 13:24:30 | [train_policy] epoch #8 | Saved
2021-06-04 13:24:30 | [train_policy] epoch #8 | Time 10.19 s
2021-06-04 13:24:30 | [train_policy] epoch #8 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                  0.293169
Evaluation/AverageDiscountedReturn        -113.322
Evaluation/AverageReturn                  -113.322
Evaluation/CompletionRate                    0
Evaluation/Iteration                         8
Evaluation/MaxReturn                       -50.6686
Evaluation/MinReturn                     -2135.54
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       295.714
Extras/EpisodeRewardMean                  -110.03
LinearFeatureBaseline/ExplainedVariance     -0.0616328
PolicyExecTime                               0.232147
ProcessExecTime                              0.0323112
TotalEnvSteps                             9108
policy/Entropy                               2.62293
policy/KL                                    0.00730207
policy/KLBefore                              0
policy/LossAfter                            -0.0301082
policy/LossBefore                           -6.59656e-09
policy/Perplexity                           13.776
policy/dLoss                                 0.0301082
---------------------------------------  ---------------
2021-06-04 13:24:30 | [train_policy] epoch #9 | Obtaining samples for iteration 9...
2021-06-04 13:24:30 | [train_policy] epoch #9 | Logging diagnostics...
2021-06-04 13:24:30 | [train_policy] epoch #9 | Optimizing policy...
2021-06-04 13:24:30 | [train_policy] epoch #9 | Computing loss before
2021-06-04 13:24:30 | [train_policy] epoch #9 | Computing KL before
2021-06-04 13:24:30 | [train_policy] epoch #9 | Optimizing
2021-06-04 13:24:30 | [train_policy] epoch #9 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:30 | [train_policy] epoch #9 | computing loss before
2021-06-04 13:24:30 | [train_policy] epoch #9 | computing gradient
2021-06-04 13:24:30 | [train_policy] epoch #9 | gradient computed
2021-06-04 13:24:30 | [train_policy] epoch #9 | computing descent direction
2021-06-04 13:24:31 | [train_policy] epoch #9 | descent direction computed
2021-06-04 13:24:31 | [train_policy] epoch #9 | backtrack iters: 1
2021-06-04 13:24:31 | [train_policy] epoch #9 | optimization finished
2021-06-04 13:24:31 | [train_policy] epoch #9 | Computing KL after
2021-06-04 13:24:31 | [train_policy] epoch #9 | Computing loss after
2021-06-04 13:24:31 | [train_policy] epoch #9 | Fitting baseline...
2021-06-04 13:24:31 | [train_policy] epoch #9 | Saving snapshot...
2021-06-04 13:24:31 | [train_policy] epoch #9 | Saved
2021-06-04 13:24:31 | [train_policy] epoch #9 | Time 11.00 s
2021-06-04 13:24:31 | [train_policy] epoch #9 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.284138
Evaluation/AverageDiscountedReturn         -66.6334
Evaluation/AverageReturn                   -66.6334
Evaluation/CompletionRate                    0
Evaluation/Iteration                         9
Evaluation/MaxReturn                       -42.8101
Evaluation/MinReturn                       -81.8643
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.00272
Extras/EpisodeRewardMean                   -66.6743
LinearFeatureBaseline/ExplainedVariance     -0.979874
PolicyExecTime                               0.228524
ProcessExecTime                              0.0313451
TotalEnvSteps                            10120
policy/Entropy                               2.62769
policy/KL                                    0.00949231
policy/KLBefore                              0
policy/LossAfter                            -0.0296194
policy/LossBefore                            1.22508e-08
policy/Perplexity                           13.8417
policy/dLoss                                 0.0296194
---------------------------------------  ---------------
2021-06-04 13:24:31 | [train_policy] epoch #10 | Obtaining samples for iteration 10...
2021-06-04 13:24:31 | [train_policy] epoch #10 | Logging diagnostics...
2021-06-04 13:24:31 | [train_policy] epoch #10 | Optimizing policy...
2021-06-04 13:24:31 | [train_policy] epoch #10 | Computing loss before
2021-06-04 13:24:31 | [train_policy] epoch #10 | Computing KL before
2021-06-04 13:24:31 | [train_policy] epoch #10 | Optimizing
2021-06-04 13:24:31 | [train_policy] epoch #10 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:31 | [train_policy] epoch #10 | computing loss before
2021-06-04 13:24:31 | [train_policy] epoch #10 | computing gradient
2021-06-04 13:24:31 | [train_policy] epoch #10 | gradient computed
2021-06-04 13:24:31 | [train_policy] epoch #10 | computing descent direction
2021-06-04 13:24:31 | [train_policy] epoch #10 | descent direction computed
2021-06-04 13:24:31 | [train_policy] epoch #10 | backtrack iters: 0
2021-06-04 13:24:31 | [train_policy] epoch #10 | optimization finished
2021-06-04 13:24:31 | [train_policy] epoch #10 | Computing KL after
2021-06-04 13:24:31 | [train_policy] epoch #10 | Computing loss after
2021-06-04 13:24:31 | [train_policy] epoch #10 | Fitting baseline...
2021-06-04 13:24:31 | [train_policy] epoch #10 | Saving snapshot...
2021-06-04 13:24:31 | [train_policy] epoch #10 | Saved
2021-06-04 13:24:31 | [train_policy] epoch #10 | Time 11.79 s
2021-06-04 13:24:31 | [train_policy] epoch #10 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286486
Evaluation/AverageDiscountedReturn         -94.7807
Evaluation/AverageReturn                   -94.7807
Evaluation/CompletionRate                    0
Evaluation/Iteration                        10
Evaluation/MaxReturn                       -50.3663
Evaluation/MinReturn                     -2060.71
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       212.895
Extras/EpisodeRewardMean                   -92.9078
LinearFeatureBaseline/ExplainedVariance      0.0697027
PolicyExecTime                               0.231472
ProcessExecTime                              0.0314908
TotalEnvSteps                            11132
policy/Entropy                               2.60775
policy/KL                                    0.00888305
policy/KLBefore                              0
policy/LossAfter                            -0.056827
policy/LossBefore                            8.48129e-09
policy/Perplexity                           13.5685
policy/dLoss                                 0.056827
---------------------------------------  ---------------
2021-06-04 13:24:31 | [train_policy] epoch #11 | Obtaining samples for iteration 11...
2021-06-04 13:24:32 | [train_policy] epoch #11 | Logging diagnostics...
2021-06-04 13:24:32 | [train_policy] epoch #11 | Optimizing policy...
2021-06-04 13:24:32 | [train_policy] epoch #11 | Computing loss before
2021-06-04 13:24:32 | [train_policy] epoch #11 | Computing KL before
2021-06-04 13:24:32 | [train_policy] epoch #11 | Optimizing
2021-06-04 13:24:32 | [train_policy] epoch #11 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:32 | [train_policy] epoch #11 | computing loss before
2021-06-04 13:24:32 | [train_policy] epoch #11 | computing gradient
2021-06-04 13:24:32 | [train_policy] epoch #11 | gradient computed
2021-06-04 13:24:32 | [train_policy] epoch #11 | computing descent direction
2021-06-04 13:24:32 | [train_policy] epoch #11 | descent direction computed
2021-06-04 13:24:32 | [train_policy] epoch #11 | backtrack iters: 1
2021-06-04 13:24:32 | [train_policy] epoch #11 | optimization finished
2021-06-04 13:24:32 | [train_policy] epoch #11 | Computing KL after
2021-06-04 13:24:32 | [train_policy] epoch #11 | Computing loss after
2021-06-04 13:24:32 | [train_policy] epoch #11 | Fitting baseline...
2021-06-04 13:24:32 | [train_policy] epoch #11 | Saving snapshot...
2021-06-04 13:24:32 | [train_policy] epoch #11 | Saved
2021-06-04 13:24:32 | [train_policy] epoch #11 | Time 12.58 s
2021-06-04 13:24:32 | [train_policy] epoch #11 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285071
Evaluation/AverageDiscountedReturn         -71.5651
Evaluation/AverageReturn                   -71.5651
Evaluation/CompletionRate                    0
Evaluation/Iteration                        11
Evaluation/MaxReturn                       -46.4773
Evaluation/MinReturn                      -644.529
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        60.485
Extras/EpisodeRewardMean                   -70.9443
LinearFeatureBaseline/ExplainedVariance      0.107507
PolicyExecTime                               0.224946
ProcessExecTime                              0.0314033
TotalEnvSteps                            12144
policy/Entropy                               2.59793
policy/KL                                    0.00786002
policy/KLBefore                              0
policy/LossAfter                            -0.0261813
policy/LossBefore                           -2.40303e-08
policy/Perplexity                           13.436
policy/dLoss                                 0.0261813
---------------------------------------  ---------------
2021-06-04 13:24:32 | [train_policy] epoch #12 | Obtaining samples for iteration 12...
2021-06-04 13:24:33 | [train_policy] epoch #12 | Logging diagnostics...
2021-06-04 13:24:33 | [train_policy] epoch #12 | Optimizing policy...
2021-06-04 13:24:33 | [train_policy] epoch #12 | Computing loss before
2021-06-04 13:24:33 | [train_policy] epoch #12 | Computing KL before
2021-06-04 13:24:33 | [train_policy] epoch #12 | Optimizing
2021-06-04 13:24:33 | [train_policy] epoch #12 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:33 | [train_policy] epoch #12 | computing loss before
2021-06-04 13:24:33 | [train_policy] epoch #12 | computing gradient
2021-06-04 13:24:33 | [train_policy] epoch #12 | gradient computed
2021-06-04 13:24:33 | [train_policy] epoch #12 | computing descent direction
2021-06-04 13:24:33 | [train_policy] epoch #12 | descent direction computed
2021-06-04 13:24:33 | [train_policy] epoch #12 | backtrack iters: 1
2021-06-04 13:24:33 | [train_policy] epoch #12 | optimization finished
2021-06-04 13:24:33 | [train_policy] epoch #12 | Computing KL after
2021-06-04 13:24:33 | [train_policy] epoch #12 | Computing loss after
2021-06-04 13:24:33 | [train_policy] epoch #12 | Fitting baseline...
2021-06-04 13:24:33 | [train_policy] epoch #12 | Saving snapshot...
2021-06-04 13:24:33 | [train_policy] epoch #12 | Saved
2021-06-04 13:24:33 | [train_policy] epoch #12 | Time 13.41 s
2021-06-04 13:24:33 | [train_policy] epoch #12 | EpochTime 0.81 s
---------------------------------------  --------------
EnvExecTime                                  0.296095
Evaluation/AverageDiscountedReturn         -77.9967
Evaluation/AverageReturn                   -77.9967
Evaluation/CompletionRate                    0
Evaluation/Iteration                        12
Evaluation/MaxReturn                       -50.2932
Evaluation/MinReturn                     -1151.39
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       113.429
Extras/EpisodeRewardMean                   -76.861
LinearFeatureBaseline/ExplainedVariance      0.220383
PolicyExecTime                               0.238749
ProcessExecTime                              0.0319257
TotalEnvSteps                            13156
policy/Entropy                               2.56392
policy/KL                                    0.00857441
policy/KLBefore                              0
policy/LossAfter                            -0.024485
policy/LossBefore                           -5.6542e-09
policy/Perplexity                           12.9866
policy/dLoss                                 0.024485
---------------------------------------  --------------
2021-06-04 13:24:33 | [train_policy] epoch #13 | Obtaining samples for iteration 13...
2021-06-04 13:24:34 | [train_policy] epoch #13 | Logging diagnostics...
2021-06-04 13:24:34 | [train_policy] epoch #13 | Optimizing policy...
2021-06-04 13:24:34 | [train_policy] epoch #13 | Computing loss before
2021-06-04 13:24:34 | [train_policy] epoch #13 | Computing KL before
2021-06-04 13:24:34 | [train_policy] epoch #13 | Optimizing
2021-06-04 13:24:34 | [train_policy] epoch #13 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:34 | [train_policy] epoch #13 | computing loss before
2021-06-04 13:24:34 | [train_policy] epoch #13 | computing gradient
2021-06-04 13:24:34 | [train_policy] epoch #13 | gradient computed
2021-06-04 13:24:34 | [train_policy] epoch #13 | computing descent direction
2021-06-04 13:24:34 | [train_policy] epoch #13 | descent direction computed
2021-06-04 13:24:34 | [train_policy] epoch #13 | backtrack iters: 0
2021-06-04 13:24:34 | [train_policy] epoch #13 | optimization finished
2021-06-04 13:24:34 | [train_policy] epoch #13 | Computing KL after
2021-06-04 13:24:34 | [train_policy] epoch #13 | Computing loss after
2021-06-04 13:24:34 | [train_policy] epoch #13 | Fitting baseline...
2021-06-04 13:24:34 | [train_policy] epoch #13 | Saving snapshot...
2021-06-04 13:24:34 | [train_policy] epoch #13 | Saved
2021-06-04 13:24:34 | [train_policy] epoch #13 | Time 14.20 s
2021-06-04 13:24:34 | [train_policy] epoch #13 | EpochTime 0.76 s
---------------------------------------  --------------
EnvExecTime                                  0.288296
Evaluation/AverageDiscountedReturn         -71.4876
Evaluation/AverageReturn                   -71.4876
Evaluation/CompletionRate                    0
Evaluation/Iteration                        13
Evaluation/MaxReturn                       -49.6466
Evaluation/MinReturn                      -850.647
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        81.9406
Extras/EpisodeRewardMean                   -70.9407
LinearFeatureBaseline/ExplainedVariance      0.2435
PolicyExecTime                               0.219945
ProcessExecTime                              0.0318086
TotalEnvSteps                            14168
policy/Entropy                               2.55372
policy/KL                                    0.00688058
policy/KLBefore                              0
policy/LossAfter                            -0.0444095
policy/LossBefore                            3.4043e-08
policy/Perplexity                           12.8548
policy/dLoss                                 0.0444096
---------------------------------------  --------------
2021-06-04 13:24:34 | [train_policy] epoch #14 | Obtaining samples for iteration 14...
2021-06-04 13:24:34 | [train_policy] epoch #14 | Logging diagnostics...
2021-06-04 13:24:34 | [train_policy] epoch #14 | Optimizing policy...
2021-06-04 13:24:34 | [train_policy] epoch #14 | Computing loss before
2021-06-04 13:24:34 | [train_policy] epoch #14 | Computing KL before
2021-06-04 13:24:34 | [train_policy] epoch #14 | Optimizing
2021-06-04 13:24:34 | [train_policy] epoch #14 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:34 | [train_policy] epoch #14 | computing loss before
2021-06-04 13:24:34 | [train_policy] epoch #14 | computing gradient
2021-06-04 13:24:34 | [train_policy] epoch #14 | gradient computed
2021-06-04 13:24:34 | [train_policy] epoch #14 | computing descent direction
2021-06-04 13:24:35 | [train_policy] epoch #14 | descent direction computed
2021-06-04 13:24:35 | [train_policy] epoch #14 | backtrack iters: 1
2021-06-04 13:24:35 | [train_policy] epoch #14 | optimization finished
2021-06-04 13:24:35 | [train_policy] epoch #14 | Computing KL after
2021-06-04 13:24:35 | [train_policy] epoch #14 | Computing loss after
2021-06-04 13:24:35 | [train_policy] epoch #14 | Fitting baseline...
2021-06-04 13:24:35 | [train_policy] epoch #14 | Saving snapshot...
2021-06-04 13:24:35 | [train_policy] epoch #14 | Saved
2021-06-04 13:24:35 | [train_policy] epoch #14 | Time 15.00 s
2021-06-04 13:24:35 | [train_policy] epoch #14 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.2852
Evaluation/AverageDiscountedReturn         -66.5627
Evaluation/AverageReturn                   -66.5627
Evaluation/CompletionRate                    0
Evaluation/Iteration                        14
Evaluation/MaxReturn                       -48.0745
Evaluation/MinReturn                      -186.857
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        18.8422
Extras/EpisodeRewardMean                   -66.4039
LinearFeatureBaseline/ExplainedVariance      0.44959
PolicyExecTime                               0.228463
ProcessExecTime                              0.0313313
TotalEnvSteps                            15180
policy/Entropy                               2.48757
policy/KL                                    0.00710883
policy/KLBefore                              0
policy/LossAfter                            -0.0180786
policy/LossBefore                           -0
policy/Perplexity                           12.032
policy/dLoss                                 0.0180786
---------------------------------------  --------------
2021-06-04 13:24:35 | [train_policy] epoch #15 | Obtaining samples for iteration 15...
2021-06-04 13:24:35 | [train_policy] epoch #15 | Logging diagnostics...
2021-06-04 13:24:35 | [train_policy] epoch #15 | Optimizing policy...
2021-06-04 13:24:35 | [train_policy] epoch #15 | Computing loss before
2021-06-04 13:24:35 | [train_policy] epoch #15 | Computing KL before
2021-06-04 13:24:35 | [train_policy] epoch #15 | Optimizing
2021-06-04 13:24:35 | [train_policy] epoch #15 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:35 | [train_policy] epoch #15 | computing loss before
2021-06-04 13:24:35 | [train_policy] epoch #15 | computing gradient
2021-06-04 13:24:35 | [train_policy] epoch #15 | gradient computed
2021-06-04 13:24:35 | [train_policy] epoch #15 | computing descent direction
2021-06-04 13:24:35 | [train_policy] epoch #15 | descent direction computed
2021-06-04 13:24:35 | [train_policy] epoch #15 | backtrack iters: 0
2021-06-04 13:24:35 | [train_policy] epoch #15 | optimization finished
2021-06-04 13:24:35 | [train_policy] epoch #15 | Computing KL after
2021-06-04 13:24:35 | [train_policy] epoch #15 | Computing loss after
2021-06-04 13:24:35 | [train_policy] epoch #15 | Fitting baseline...
2021-06-04 13:24:35 | [train_policy] epoch #15 | Saving snapshot...
2021-06-04 13:24:35 | [train_policy] epoch #15 | Saved
2021-06-04 13:24:35 | [train_policy] epoch #15 | Time 15.78 s
2021-06-04 13:24:35 | [train_policy] epoch #15 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.285193
Evaluation/AverageDiscountedReturn         -61.7612
Evaluation/AverageReturn                   -61.7612
Evaluation/CompletionRate                    0
Evaluation/Iteration                        15
Evaluation/MaxReturn                       -47.4019
Evaluation/MinReturn                       -79.3068
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.07376
Extras/EpisodeRewardMean                   -62.9212
LinearFeatureBaseline/ExplainedVariance      0.938586
PolicyExecTime                               0.225605
ProcessExecTime                              0.0313554
TotalEnvSteps                            16192
policy/Entropy                               2.47716
policy/KL                                    0.00972453
policy/KLBefore                              0
policy/LossAfter                            -0.0443576
policy/LossBefore                            5.27725e-08
policy/Perplexity                           11.9074
policy/dLoss                                 0.0443577
---------------------------------------  ---------------
2021-06-04 13:24:35 | [train_policy] epoch #16 | Obtaining samples for iteration 16...
2021-06-04 13:24:36 | [train_policy] epoch #16 | Logging diagnostics...
2021-06-04 13:24:36 | [train_policy] epoch #16 | Optimizing policy...
2021-06-04 13:24:36 | [train_policy] epoch #16 | Computing loss before
2021-06-04 13:24:36 | [train_policy] epoch #16 | Computing KL before
2021-06-04 13:24:36 | [train_policy] epoch #16 | Optimizing
2021-06-04 13:24:36 | [train_policy] epoch #16 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:36 | [train_policy] epoch #16 | computing loss before
2021-06-04 13:24:36 | [train_policy] epoch #16 | computing gradient
2021-06-04 13:24:36 | [train_policy] epoch #16 | gradient computed
2021-06-04 13:24:36 | [train_policy] epoch #16 | computing descent direction
2021-06-04 13:24:36 | [train_policy] epoch #16 | descent direction computed
2021-06-04 13:24:36 | [train_policy] epoch #16 | backtrack iters: 1
2021-06-04 13:24:36 | [train_policy] epoch #16 | optimization finished
2021-06-04 13:24:36 | [train_policy] epoch #16 | Computing KL after
2021-06-04 13:24:36 | [train_policy] epoch #16 | Computing loss after
2021-06-04 13:24:36 | [train_policy] epoch #16 | Fitting baseline...
2021-06-04 13:24:36 | [train_policy] epoch #16 | Saving snapshot...
2021-06-04 13:24:36 | [train_policy] epoch #16 | Saved
2021-06-04 13:24:36 | [train_policy] epoch #16 | Time 16.58 s
2021-06-04 13:24:36 | [train_policy] epoch #16 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.287666
Evaluation/AverageDiscountedReturn         -60.6705
Evaluation/AverageReturn                   -60.6705
Evaluation/CompletionRate                    0
Evaluation/Iteration                        16
Evaluation/MaxReturn                       -44.7885
Evaluation/MinReturn                       -75.6125
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.56826
Extras/EpisodeRewardMean                   -60.6918
LinearFeatureBaseline/ExplainedVariance      0.96074
PolicyExecTime                               0.23239
ProcessExecTime                              0.0316005
TotalEnvSteps                            17204
policy/Entropy                               2.48309
policy/KL                                    0.00654399
policy/KLBefore                              0
policy/LossAfter                            -0.0432797
policy/LossBefore                           -1.69626e-08
policy/Perplexity                           11.9782
policy/dLoss                                 0.0432797
---------------------------------------  ---------------
2021-06-04 13:24:36 | [train_policy] epoch #17 | Obtaining samples for iteration 17...
2021-06-04 13:24:37 | [train_policy] epoch #17 | Logging diagnostics...
2021-06-04 13:24:37 | [train_policy] epoch #17 | Optimizing policy...
2021-06-04 13:24:37 | [train_policy] epoch #17 | Computing loss before
2021-06-04 13:24:37 | [train_policy] epoch #17 | Computing KL before
2021-06-04 13:24:37 | [train_policy] epoch #17 | Optimizing
2021-06-04 13:24:37 | [train_policy] epoch #17 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:37 | [train_policy] epoch #17 | computing loss before
2021-06-04 13:24:37 | [train_policy] epoch #17 | computing gradient
2021-06-04 13:24:37 | [train_policy] epoch #17 | gradient computed
2021-06-04 13:24:37 | [train_policy] epoch #17 | computing descent direction
2021-06-04 13:24:37 | [train_policy] epoch #17 | descent direction computed
2021-06-04 13:24:37 | [train_policy] epoch #17 | backtrack iters: 0
2021-06-04 13:24:37 | [train_policy] epoch #17 | optimization finished
2021-06-04 13:24:37 | [train_policy] epoch #17 | Computing KL after
2021-06-04 13:24:37 | [train_policy] epoch #17 | Computing loss after
2021-06-04 13:24:37 | [train_policy] epoch #17 | Fitting baseline...
2021-06-04 13:24:37 | [train_policy] epoch #17 | Saving snapshot...
2021-06-04 13:24:37 | [train_policy] epoch #17 | Saved
2021-06-04 13:24:37 | [train_policy] epoch #17 | Time 17.37 s
2021-06-04 13:24:37 | [train_policy] epoch #17 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.288682
Evaluation/AverageDiscountedReturn         -81.7775
Evaluation/AverageReturn                   -81.7775
Evaluation/CompletionRate                    0
Evaluation/Iteration                        17
Evaluation/MaxReturn                       -46.6136
Evaluation/MinReturn                     -2056.15
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       207.095
Extras/EpisodeRewardMean                   -79.9119
LinearFeatureBaseline/ExplainedVariance      0.0962828
PolicyExecTime                               0.219349
ProcessExecTime                              0.0316169
TotalEnvSteps                            18216
policy/Entropy                               2.47591
policy/KL                                    0.0099051
policy/KLBefore                              0
policy/LossAfter                            -0.0440909
policy/LossBefore                            1.31931e-08
policy/Perplexity                           11.8925
policy/dLoss                                 0.0440909
---------------------------------------  ---------------
2021-06-04 13:24:37 | [train_policy] epoch #18 | Obtaining samples for iteration 18...
2021-06-04 13:24:38 | [train_policy] epoch #18 | Logging diagnostics...
2021-06-04 13:24:38 | [train_policy] epoch #18 | Optimizing policy...
2021-06-04 13:24:38 | [train_policy] epoch #18 | Computing loss before
2021-06-04 13:24:38 | [train_policy] epoch #18 | Computing KL before
2021-06-04 13:24:38 | [train_policy] epoch #18 | Optimizing
2021-06-04 13:24:38 | [train_policy] epoch #18 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:38 | [train_policy] epoch #18 | computing loss before
2021-06-04 13:24:38 | [train_policy] epoch #18 | computing gradient
2021-06-04 13:24:38 | [train_policy] epoch #18 | gradient computed
2021-06-04 13:24:38 | [train_policy] epoch #18 | computing descent direction
2021-06-04 13:24:38 | [train_policy] epoch #18 | descent direction computed
2021-06-04 13:24:38 | [train_policy] epoch #18 | backtrack iters: 0
2021-06-04 13:24:38 | [train_policy] epoch #18 | optimization finished
2021-06-04 13:24:38 | [train_policy] epoch #18 | Computing KL after
2021-06-04 13:24:38 | [train_policy] epoch #18 | Computing loss after
2021-06-04 13:24:38 | [train_policy] epoch #18 | Fitting baseline...
2021-06-04 13:24:38 | [train_policy] epoch #18 | Saving snapshot...
2021-06-04 13:24:38 | [train_policy] epoch #18 | Saved
2021-06-04 13:24:38 | [train_policy] epoch #18 | Time 18.16 s
2021-06-04 13:24:38 | [train_policy] epoch #18 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.28418
Evaluation/AverageDiscountedReturn         -82.9594
Evaluation/AverageReturn                   -82.9594
Evaluation/CompletionRate                    0
Evaluation/Iteration                        18
Evaluation/MaxReturn                       -42.9473
Evaluation/MinReturn                     -2047.9
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       206.711
Extras/EpisodeRewardMean                   -80.8634
LinearFeatureBaseline/ExplainedVariance      0.0538834
PolicyExecTime                               0.222768
ProcessExecTime                              0.0312424
TotalEnvSteps                            19228
policy/Entropy                               2.41438
policy/KL                                    0.00942015
policy/KLBefore                              0
policy/LossAfter                            -0.0258373
policy/LossBefore                            7.06774e-09
policy/Perplexity                           11.1828
policy/dLoss                                 0.0258373
---------------------------------------  ---------------
2021-06-04 13:24:38 | [train_policy] epoch #19 | Obtaining samples for iteration 19...
2021-06-04 13:24:38 | [train_policy] epoch #19 | Logging diagnostics...
2021-06-04 13:24:38 | [train_policy] epoch #19 | Optimizing policy...
2021-06-04 13:24:38 | [train_policy] epoch #19 | Computing loss before
2021-06-04 13:24:38 | [train_policy] epoch #19 | Computing KL before
2021-06-04 13:24:38 | [train_policy] epoch #19 | Optimizing
2021-06-04 13:24:38 | [train_policy] epoch #19 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:38 | [train_policy] epoch #19 | computing loss before
2021-06-04 13:24:38 | [train_policy] epoch #19 | computing gradient
2021-06-04 13:24:38 | [train_policy] epoch #19 | gradient computed
2021-06-04 13:24:38 | [train_policy] epoch #19 | computing descent direction
2021-06-04 13:24:38 | [train_policy] epoch #19 | descent direction computed
2021-06-04 13:24:38 | [train_policy] epoch #19 | backtrack iters: 1
2021-06-04 13:24:38 | [train_policy] epoch #19 | optimization finished
2021-06-04 13:24:38 | [train_policy] epoch #19 | Computing KL after
2021-06-04 13:24:38 | [train_policy] epoch #19 | Computing loss after
2021-06-04 13:24:39 | [train_policy] epoch #19 | Fitting baseline...
2021-06-04 13:24:39 | [train_policy] epoch #19 | Saving snapshot...
2021-06-04 13:24:39 | [train_policy] epoch #19 | Saved
2021-06-04 13:24:39 | [train_policy] epoch #19 | Time 18.94 s
2021-06-04 13:24:39 | [train_policy] epoch #19 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                  0.284961
Evaluation/AverageDiscountedReturn         -58.0678
Evaluation/AverageReturn                   -58.0678
Evaluation/CompletionRate                    0
Evaluation/Iteration                        19
Evaluation/MaxReturn                       -39.4737
Evaluation/MinReturn                       -94.5962
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.14259
Extras/EpisodeRewardMean                   -57.9234
LinearFeatureBaseline/ExplainedVariance      0.558672
PolicyExecTime                               0.210275
ProcessExecTime                              0.031337
TotalEnvSteps                            20240
policy/Entropy                               2.40713
policy/KL                                    0.00715282
policy/KLBefore                              0
policy/LossAfter                            -0.0262864
policy/LossBefore                           -1.64914e-08
policy/Perplexity                           11.1021
policy/dLoss                                 0.0262864
---------------------------------------  ---------------
2021-06-04 13:24:39 | [train_policy] epoch #20 | Obtaining samples for iteration 20...
2021-06-04 13:24:39 | [train_policy] epoch #20 | Logging diagnostics...
2021-06-04 13:24:39 | [train_policy] epoch #20 | Optimizing policy...
2021-06-04 13:24:39 | [train_policy] epoch #20 | Computing loss before
2021-06-04 13:24:39 | [train_policy] epoch #20 | Computing KL before
2021-06-04 13:24:39 | [train_policy] epoch #20 | Optimizing
2021-06-04 13:24:39 | [train_policy] epoch #20 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:39 | [train_policy] epoch #20 | computing loss before
2021-06-04 13:24:39 | [train_policy] epoch #20 | computing gradient
2021-06-04 13:24:39 | [train_policy] epoch #20 | gradient computed
2021-06-04 13:24:39 | [train_policy] epoch #20 | computing descent direction
2021-06-04 13:24:39 | [train_policy] epoch #20 | descent direction computed
2021-06-04 13:24:39 | [train_policy] epoch #20 | backtrack iters: 1
2021-06-04 13:24:39 | [train_policy] epoch #20 | optimization finished
2021-06-04 13:24:39 | [train_policy] epoch #20 | Computing KL after
2021-06-04 13:24:39 | [train_policy] epoch #20 | Computing loss after
2021-06-04 13:24:39 | [train_policy] epoch #20 | Fitting baseline...
2021-06-04 13:24:39 | [train_policy] epoch #20 | Saving snapshot...
2021-06-04 13:24:39 | [train_policy] epoch #20 | Saved
2021-06-04 13:24:39 | [train_policy] epoch #20 | Time 19.73 s
2021-06-04 13:24:39 | [train_policy] epoch #20 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.287648
Evaluation/AverageDiscountedReturn         -58.8253
Evaluation/AverageReturn                   -58.8253
Evaluation/CompletionRate                    0
Evaluation/Iteration                        20
Evaluation/MaxReturn                       -42.0908
Evaluation/MinReturn                      -107.256
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.92338
Extras/EpisodeRewardMean                   -59.2438
LinearFeatureBaseline/ExplainedVariance      0.928403
PolicyExecTime                               0.230084
ProcessExecTime                              0.0316422
TotalEnvSteps                            21252
policy/Entropy                               2.39113
policy/KL                                    0.00668978
policy/KLBefore                              0
policy/LossAfter                            -0.0306618
policy/LossBefore                           -5.18301e-09
policy/Perplexity                           10.9259
policy/dLoss                                 0.0306618
---------------------------------------  ---------------
2021-06-04 13:24:39 | [train_policy] epoch #21 | Obtaining samples for iteration 21...
2021-06-04 13:24:40 | [train_policy] epoch #21 | Logging diagnostics...
2021-06-04 13:24:40 | [train_policy] epoch #21 | Optimizing policy...
2021-06-04 13:24:40 | [train_policy] epoch #21 | Computing loss before
2021-06-04 13:24:40 | [train_policy] epoch #21 | Computing KL before
2021-06-04 13:24:40 | [train_policy] epoch #21 | Optimizing
2021-06-04 13:24:40 | [train_policy] epoch #21 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:40 | [train_policy] epoch #21 | computing loss before
2021-06-04 13:24:40 | [train_policy] epoch #21 | computing gradient
2021-06-04 13:24:40 | [train_policy] epoch #21 | gradient computed
2021-06-04 13:24:40 | [train_policy] epoch #21 | computing descent direction
2021-06-04 13:24:40 | [train_policy] epoch #21 | descent direction computed
2021-06-04 13:24:40 | [train_policy] epoch #21 | backtrack iters: 1
2021-06-04 13:24:40 | [train_policy] epoch #21 | optimization finished
2021-06-04 13:24:40 | [train_policy] epoch #21 | Computing KL after
2021-06-04 13:24:40 | [train_policy] epoch #21 | Computing loss after
2021-06-04 13:24:40 | [train_policy] epoch #21 | Fitting baseline...
2021-06-04 13:24:40 | [train_policy] epoch #21 | Saving snapshot...
2021-06-04 13:24:40 | [train_policy] epoch #21 | Saved
2021-06-04 13:24:40 | [train_policy] epoch #21 | Time 20.54 s
2021-06-04 13:24:40 | [train_policy] epoch #21 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.301006
Evaluation/AverageDiscountedReturn         -77.715
Evaluation/AverageReturn                   -77.715
Evaluation/CompletionRate                    0
Evaluation/Iteration                        21
Evaluation/MaxReturn                       -45.6608
Evaluation/MinReturn                     -2043.32
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       206.215
Extras/EpisodeRewardMean                   -75.9258
LinearFeatureBaseline/ExplainedVariance      0.0193748
PolicyExecTime                               0.227328
ProcessExecTime                              0.0329976
TotalEnvSteps                            22264
policy/Entropy                               2.35727
policy/KL                                    0.00648399
policy/KLBefore                              0
policy/LossAfter                            -0.0292512
policy/LossBefore                           -1.60202e-08
policy/Perplexity                           10.5621
policy/dLoss                                 0.0292512
---------------------------------------  ---------------
2021-06-04 13:24:40 | [train_policy] epoch #22 | Obtaining samples for iteration 22...
2021-06-04 13:24:41 | [train_policy] epoch #22 | Logging diagnostics...
2021-06-04 13:24:41 | [train_policy] epoch #22 | Optimizing policy...
2021-06-04 13:24:41 | [train_policy] epoch #22 | Computing loss before
2021-06-04 13:24:41 | [train_policy] epoch #22 | Computing KL before
2021-06-04 13:24:41 | [train_policy] epoch #22 | Optimizing
2021-06-04 13:24:41 | [train_policy] epoch #22 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:41 | [train_policy] epoch #22 | computing loss before
2021-06-04 13:24:41 | [train_policy] epoch #22 | computing gradient
2021-06-04 13:24:41 | [train_policy] epoch #22 | gradient computed
2021-06-04 13:24:41 | [train_policy] epoch #22 | computing descent direction
2021-06-04 13:24:41 | [train_policy] epoch #22 | descent direction computed
2021-06-04 13:24:41 | [train_policy] epoch #22 | backtrack iters: 1
2021-06-04 13:24:41 | [train_policy] epoch #22 | optimization finished
2021-06-04 13:24:41 | [train_policy] epoch #22 | Computing KL after
2021-06-04 13:24:41 | [train_policy] epoch #22 | Computing loss after
2021-06-04 13:24:41 | [train_policy] epoch #22 | Fitting baseline...
2021-06-04 13:24:41 | [train_policy] epoch #22 | Saving snapshot...
2021-06-04 13:24:41 | [train_policy] epoch #22 | Saved
2021-06-04 13:24:41 | [train_policy] epoch #22 | Time 21.32 s
2021-06-04 13:24:41 | [train_policy] epoch #22 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.289244
Evaluation/AverageDiscountedReturn        -102.335
Evaluation/AverageReturn                  -102.335
Evaluation/CompletionRate                    0
Evaluation/Iteration                        22
Evaluation/MaxReturn                       -42.6
Evaluation/MinReturn                     -4040.84
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       413.457
Extras/EpisodeRewardMean                   -99.2739
LinearFeatureBaseline/ExplainedVariance      0.0120122
PolicyExecTime                               0.212874
ProcessExecTime                              0.0317671
TotalEnvSteps                            23276
policy/Entropy                               2.30829
policy/KL                                    0.00706849
policy/KLBefore                              0
policy/LossAfter                            -0.022293
policy/LossBefore                           -3.76946e-09
policy/Perplexity                           10.0572
policy/dLoss                                 0.022293
---------------------------------------  ---------------
2021-06-04 13:24:41 | [train_policy] epoch #23 | Obtaining samples for iteration 23...
2021-06-04 13:24:42 | [train_policy] epoch #23 | Logging diagnostics...
2021-06-04 13:24:42 | [train_policy] epoch #23 | Optimizing policy...
2021-06-04 13:24:42 | [train_policy] epoch #23 | Computing loss before
2021-06-04 13:24:42 | [train_policy] epoch #23 | Computing KL before
2021-06-04 13:24:42 | [train_policy] epoch #23 | Optimizing
2021-06-04 13:24:42 | [train_policy] epoch #23 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:42 | [train_policy] epoch #23 | computing loss before
2021-06-04 13:24:42 | [train_policy] epoch #23 | computing gradient
2021-06-04 13:24:42 | [train_policy] epoch #23 | gradient computed
2021-06-04 13:24:42 | [train_policy] epoch #23 | computing descent direction
2021-06-04 13:24:42 | [train_policy] epoch #23 | descent direction computed
2021-06-04 13:24:42 | [train_policy] epoch #23 | backtrack iters: 0
2021-06-04 13:24:42 | [train_policy] epoch #23 | optimization finished
2021-06-04 13:24:42 | [train_policy] epoch #23 | Computing KL after
2021-06-04 13:24:42 | [train_policy] epoch #23 | Computing loss after
2021-06-04 13:24:42 | [train_policy] epoch #23 | Fitting baseline...
2021-06-04 13:24:42 | [train_policy] epoch #23 | Saving snapshot...
2021-06-04 13:24:42 | [train_policy] epoch #23 | Saved
2021-06-04 13:24:42 | [train_policy] epoch #23 | Time 22.12 s
2021-06-04 13:24:42 | [train_policy] epoch #23 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.288513
Evaluation/AverageDiscountedReturn         -79.0956
Evaluation/AverageReturn                   -79.0956
Evaluation/CompletionRate                    0
Evaluation/Iteration                        23
Evaluation/MaxReturn                       -45.4471
Evaluation/MinReturn                     -2066.87
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.464
Extras/EpisodeRewardMean                   -77.2436
LinearFeatureBaseline/ExplainedVariance     -0.0386644
PolicyExecTime                               0.229798
ProcessExecTime                              0.0316138
TotalEnvSteps                            24288
policy/Entropy                               2.30969
policy/KL                                    0.00912523
policy/KLBefore                              0
policy/LossAfter                            -0.0199611
policy/LossBefore                           -7.06774e-09
policy/Perplexity                           10.0713
policy/dLoss                                 0.0199611
---------------------------------------  ---------------
2021-06-04 13:24:42 | [train_policy] epoch #24 | Obtaining samples for iteration 24...
2021-06-04 13:24:42 | [train_policy] epoch #24 | Logging diagnostics...
2021-06-04 13:24:42 | [train_policy] epoch #24 | Optimizing policy...
2021-06-04 13:24:42 | [train_policy] epoch #24 | Computing loss before
2021-06-04 13:24:42 | [train_policy] epoch #24 | Computing KL before
2021-06-04 13:24:42 | [train_policy] epoch #24 | Optimizing
2021-06-04 13:24:42 | [train_policy] epoch #24 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:42 | [train_policy] epoch #24 | computing loss before
2021-06-04 13:24:42 | [train_policy] epoch #24 | computing gradient
2021-06-04 13:24:42 | [train_policy] epoch #24 | gradient computed
2021-06-04 13:24:42 | [train_policy] epoch #24 | computing descent direction
2021-06-04 13:24:42 | [train_policy] epoch #24 | descent direction computed
2021-06-04 13:24:42 | [train_policy] epoch #24 | backtrack iters: 1
2021-06-04 13:24:42 | [train_policy] epoch #24 | optimization finished
2021-06-04 13:24:42 | [train_policy] epoch #24 | Computing KL after
2021-06-04 13:24:42 | [train_policy] epoch #24 | Computing loss after
2021-06-04 13:24:42 | [train_policy] epoch #24 | Fitting baseline...
2021-06-04 13:24:42 | [train_policy] epoch #24 | Saving snapshot...
2021-06-04 13:24:43 | [train_policy] epoch #24 | Saved
2021-06-04 13:24:43 | [train_policy] epoch #24 | Time 22.90 s
2021-06-04 13:24:43 | [train_policy] epoch #24 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285361
Evaluation/AverageDiscountedReturn         -64.6677
Evaluation/AverageReturn                   -64.6677
Evaluation/CompletionRate                    0
Evaluation/Iteration                        24
Evaluation/MaxReturn                       -39.4503
Evaluation/MinReturn                      -737.726
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        70.8538
Extras/EpisodeRewardMean                   -64.0908
LinearFeatureBaseline/ExplainedVariance     -1.78657
PolicyExecTime                               0.226455
ProcessExecTime                              0.0312481
TotalEnvSteps                            25300
policy/Entropy                               2.28613
policy/KL                                    0.00652985
policy/KLBefore                              0
policy/LossAfter                            -0.0271886
policy/LossBefore                           -1.27219e-08
policy/Perplexity                            9.83675
policy/dLoss                                 0.0271886
---------------------------------------  ---------------
2021-06-04 13:24:43 | [train_policy] epoch #25 | Obtaining samples for iteration 25...
2021-06-04 13:24:43 | [train_policy] epoch #25 | Logging diagnostics...
2021-06-04 13:24:43 | [train_policy] epoch #25 | Optimizing policy...
2021-06-04 13:24:43 | [train_policy] epoch #25 | Computing loss before
2021-06-04 13:24:43 | [train_policy] epoch #25 | Computing KL before
2021-06-04 13:24:43 | [train_policy] epoch #25 | Optimizing
2021-06-04 13:24:43 | [train_policy] epoch #25 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:43 | [train_policy] epoch #25 | computing loss before
2021-06-04 13:24:43 | [train_policy] epoch #25 | computing gradient
2021-06-04 13:24:43 | [train_policy] epoch #25 | gradient computed
2021-06-04 13:24:43 | [train_policy] epoch #25 | computing descent direction
2021-06-04 13:24:43 | [train_policy] epoch #25 | descent direction computed
2021-06-04 13:24:43 | [train_policy] epoch #25 | backtrack iters: 0
2021-06-04 13:24:43 | [train_policy] epoch #25 | optimization finished
2021-06-04 13:24:43 | [train_policy] epoch #25 | Computing KL after
2021-06-04 13:24:43 | [train_policy] epoch #25 | Computing loss after
2021-06-04 13:24:43 | [train_policy] epoch #25 | Fitting baseline...
2021-06-04 13:24:43 | [train_policy] epoch #25 | Saving snapshot...
2021-06-04 13:24:43 | [train_policy] epoch #25 | Saved
2021-06-04 13:24:43 | [train_policy] epoch #25 | Time 23.70 s
2021-06-04 13:24:43 | [train_policy] epoch #25 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.291612
Evaluation/AverageDiscountedReturn         -57.1635
Evaluation/AverageReturn                   -57.1635
Evaluation/CompletionRate                    0
Evaluation/Iteration                        25
Evaluation/MaxReturn                       -45.3084
Evaluation/MinReturn                       -71.5883
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.566
Extras/EpisodeRewardMean                   -56.843
LinearFeatureBaseline/ExplainedVariance      0.18263
PolicyExecTime                               0.231936
ProcessExecTime                              0.0320666
TotalEnvSteps                            26312
policy/Entropy                               2.26039
policy/KL                                    0.00971134
policy/KLBefore                              0
policy/LossAfter                            -0.0259345
policy/LossBefore                           -2.92133e-08
policy/Perplexity                            9.58685
policy/dLoss                                 0.0259345
---------------------------------------  ---------------
2021-06-04 13:24:43 | [train_policy] epoch #26 | Obtaining samples for iteration 26...
2021-06-04 13:24:44 | [train_policy] epoch #26 | Logging diagnostics...
2021-06-04 13:24:44 | [train_policy] epoch #26 | Optimizing policy...
2021-06-04 13:24:44 | [train_policy] epoch #26 | Computing loss before
2021-06-04 13:24:44 | [train_policy] epoch #26 | Computing KL before
2021-06-04 13:24:44 | [train_policy] epoch #26 | Optimizing
2021-06-04 13:24:44 | [train_policy] epoch #26 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:44 | [train_policy] epoch #26 | computing loss before
2021-06-04 13:24:44 | [train_policy] epoch #26 | computing gradient
2021-06-04 13:24:44 | [train_policy] epoch #26 | gradient computed
2021-06-04 13:24:44 | [train_policy] epoch #26 | computing descent direction
2021-06-04 13:24:44 | [train_policy] epoch #26 | descent direction computed
2021-06-04 13:24:44 | [train_policy] epoch #26 | backtrack iters: 1
2021-06-04 13:24:44 | [train_policy] epoch #26 | optimization finished
2021-06-04 13:24:44 | [train_policy] epoch #26 | Computing KL after
2021-06-04 13:24:44 | [train_policy] epoch #26 | Computing loss after
2021-06-04 13:24:44 | [train_policy] epoch #26 | Fitting baseline...
2021-06-04 13:24:44 | [train_policy] epoch #26 | Saving snapshot...
2021-06-04 13:24:44 | [train_policy] epoch #26 | Saved
2021-06-04 13:24:44 | [train_policy] epoch #26 | Time 24.51 s
2021-06-04 13:24:44 | [train_policy] epoch #26 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.286772
Evaluation/AverageDiscountedReturn         -56.9706
Evaluation/AverageReturn                   -56.9706
Evaluation/CompletionRate                    0
Evaluation/Iteration                        26
Evaluation/MaxReturn                       -44.9717
Evaluation/MinReturn                       -71.8147
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.54105
Extras/EpisodeRewardMean                   -57.0968
LinearFeatureBaseline/ExplainedVariance      0.970825
PolicyExecTime                               0.231871
ProcessExecTime                              0.0314443
TotalEnvSteps                            27324
policy/Entropy                               2.26352
policy/KL                                    0.00646922
policy/KLBefore                              0
policy/LossAfter                            -0.0408292
policy/LossBefore                            1.64914e-08
policy/Perplexity                            9.61689
policy/dLoss                                 0.0408293
---------------------------------------  ---------------
2021-06-04 13:24:44 | [train_policy] epoch #27 | Obtaining samples for iteration 27...
2021-06-04 13:24:45 | [train_policy] epoch #27 | Logging diagnostics...
2021-06-04 13:24:45 | [train_policy] epoch #27 | Optimizing policy...
2021-06-04 13:24:45 | [train_policy] epoch #27 | Computing loss before
2021-06-04 13:24:45 | [train_policy] epoch #27 | Computing KL before
2021-06-04 13:24:45 | [train_policy] epoch #27 | Optimizing
2021-06-04 13:24:45 | [train_policy] epoch #27 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:45 | [train_policy] epoch #27 | computing loss before
2021-06-04 13:24:45 | [train_policy] epoch #27 | computing gradient
2021-06-04 13:24:45 | [train_policy] epoch #27 | gradient computed
2021-06-04 13:24:45 | [train_policy] epoch #27 | computing descent direction
2021-06-04 13:24:45 | [train_policy] epoch #27 | descent direction computed
2021-06-04 13:24:45 | [train_policy] epoch #27 | backtrack iters: 1
2021-06-04 13:24:45 | [train_policy] epoch #27 | optimization finished
2021-06-04 13:24:45 | [train_policy] epoch #27 | Computing KL after
2021-06-04 13:24:45 | [train_policy] epoch #27 | Computing loss after
2021-06-04 13:24:45 | [train_policy] epoch #27 | Fitting baseline...
2021-06-04 13:24:45 | [train_policy] epoch #27 | Saving snapshot...
2021-06-04 13:24:45 | [train_policy] epoch #27 | Saved
2021-06-04 13:24:45 | [train_policy] epoch #27 | Time 25.29 s
2021-06-04 13:24:45 | [train_policy] epoch #27 | EpochTime 0.76 s
---------------------------------------  --------------
EnvExecTime                                  0.285851
Evaluation/AverageDiscountedReturn         -58.5456
Evaluation/AverageReturn                   -58.5456
Evaluation/CompletionRate                    0
Evaluation/Iteration                        27
Evaluation/MaxReturn                       -42.8457
Evaluation/MinReturn                      -354.704
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        31.5914
Extras/EpisodeRewardMean                   -58.493
LinearFeatureBaseline/ExplainedVariance      0.582467
PolicyExecTime                               0.212945
ProcessExecTime                              0.0315092
TotalEnvSteps                            28336
policy/Entropy                               2.22289
policy/KL                                    0.00657903
policy/KLBefore                              0
policy/LossAfter                            -0.0313658
policy/LossBefore                            2.8271e-09
policy/Perplexity                            9.234
policy/dLoss                                 0.0313658
---------------------------------------  --------------
2021-06-04 13:24:45 | [train_policy] epoch #28 | Obtaining samples for iteration 28...
2021-06-04 13:24:46 | [train_policy] epoch #28 | Logging diagnostics...
2021-06-04 13:24:46 | [train_policy] epoch #28 | Optimizing policy...
2021-06-04 13:24:46 | [train_policy] epoch #28 | Computing loss before
2021-06-04 13:24:46 | [train_policy] epoch #28 | Computing KL before
2021-06-04 13:24:46 | [train_policy] epoch #28 | Optimizing
2021-06-04 13:24:46 | [train_policy] epoch #28 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:46 | [train_policy] epoch #28 | computing loss before
2021-06-04 13:24:46 | [train_policy] epoch #28 | computing gradient
2021-06-04 13:24:46 | [train_policy] epoch #28 | gradient computed
2021-06-04 13:24:46 | [train_policy] epoch #28 | computing descent direction
2021-06-04 13:24:46 | [train_policy] epoch #28 | descent direction computed
2021-06-04 13:24:46 | [train_policy] epoch #28 | backtrack iters: 3
2021-06-04 13:24:46 | [train_policy] epoch #28 | optimization finished
2021-06-04 13:24:46 | [train_policy] epoch #28 | Computing KL after
2021-06-04 13:24:46 | [train_policy] epoch #28 | Computing loss after
2021-06-04 13:24:46 | [train_policy] epoch #28 | Fitting baseline...
2021-06-04 13:24:46 | [train_policy] epoch #28 | Saving snapshot...
2021-06-04 13:24:46 | [train_policy] epoch #28 | Saved
2021-06-04 13:24:46 | [train_policy] epoch #28 | Time 26.09 s
2021-06-04 13:24:46 | [train_policy] epoch #28 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.28478
Evaluation/AverageDiscountedReturn         -78.6511
Evaluation/AverageReturn                   -78.6511
Evaluation/CompletionRate                    0
Evaluation/Iteration                        28
Evaluation/MaxReturn                       -41.8843
Evaluation/MinReturn                     -2051.89
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       207.306
Extras/EpisodeRewardMean                   -76.798
LinearFeatureBaseline/ExplainedVariance      0.00755768
PolicyExecTime                               0.229121
ProcessExecTime                              0.0314209
TotalEnvSteps                            29348
policy/Entropy                               2.20139
policy/KL                                    0.00516466
policy/KLBefore                              0
policy/LossAfter                            -0.0147895
policy/LossBefore                            3.76946e-09
policy/Perplexity                            9.03761
policy/dLoss                                 0.0147895
---------------------------------------  ---------------
2021-06-04 13:24:46 | [train_policy] epoch #29 | Obtaining samples for iteration 29...
2021-06-04 13:24:46 | [train_policy] epoch #29 | Logging diagnostics...
2021-06-04 13:24:46 | [train_policy] epoch #29 | Optimizing policy...
2021-06-04 13:24:46 | [train_policy] epoch #29 | Computing loss before
2021-06-04 13:24:46 | [train_policy] epoch #29 | Computing KL before
2021-06-04 13:24:46 | [train_policy] epoch #29 | Optimizing
2021-06-04 13:24:46 | [train_policy] epoch #29 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:46 | [train_policy] epoch #29 | computing loss before
2021-06-04 13:24:46 | [train_policy] epoch #29 | computing gradient
2021-06-04 13:24:46 | [train_policy] epoch #29 | gradient computed
2021-06-04 13:24:46 | [train_policy] epoch #29 | computing descent direction
2021-06-04 13:24:46 | [train_policy] epoch #29 | descent direction computed
2021-06-04 13:24:46 | [train_policy] epoch #29 | backtrack iters: 1
2021-06-04 13:24:46 | [train_policy] epoch #29 | optimization finished
2021-06-04 13:24:46 | [train_policy] epoch #29 | Computing KL after
2021-06-04 13:24:46 | [train_policy] epoch #29 | Computing loss after
2021-06-04 13:24:46 | [train_policy] epoch #29 | Fitting baseline...
2021-06-04 13:24:46 | [train_policy] epoch #29 | Saving snapshot...
2021-06-04 13:24:46 | [train_policy] epoch #29 | Saved
2021-06-04 13:24:46 | [train_policy] epoch #29 | Time 26.87 s
2021-06-04 13:24:46 | [train_policy] epoch #29 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.289844
Evaluation/AverageDiscountedReturn         -57.7593
Evaluation/AverageReturn                   -57.7593
Evaluation/CompletionRate                    0
Evaluation/Iteration                        29
Evaluation/MaxReturn                       -45.0881
Evaluation/MinReturn                      -162.181
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        12.769
Extras/EpisodeRewardMean                   -58.853
LinearFeatureBaseline/ExplainedVariance     -3.20481
PolicyExecTime                               0.209217
ProcessExecTime                              0.0318301
TotalEnvSteps                            30360
policy/Entropy                               2.19116
policy/KL                                    0.00653703
policy/KLBefore                              0
policy/LossAfter                            -0.0215886
policy/LossBefore                           -1.41355e-09
policy/Perplexity                            8.9456
policy/dLoss                                 0.0215886
---------------------------------------  ---------------
2021-06-04 13:24:47 | [train_policy] epoch #30 | Obtaining samples for iteration 30...
2021-06-04 13:24:47 | [train_policy] epoch #30 | Logging diagnostics...
2021-06-04 13:24:47 | [train_policy] epoch #30 | Optimizing policy...
2021-06-04 13:24:47 | [train_policy] epoch #30 | Computing loss before
2021-06-04 13:24:47 | [train_policy] epoch #30 | Computing KL before
2021-06-04 13:24:47 | [train_policy] epoch #30 | Optimizing
2021-06-04 13:24:47 | [train_policy] epoch #30 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:47 | [train_policy] epoch #30 | computing loss before
2021-06-04 13:24:47 | [train_policy] epoch #30 | computing gradient
2021-06-04 13:24:47 | [train_policy] epoch #30 | gradient computed
2021-06-04 13:24:47 | [train_policy] epoch #30 | computing descent direction
2021-06-04 13:24:47 | [train_policy] epoch #30 | descent direction computed
2021-06-04 13:24:47 | [train_policy] epoch #30 | backtrack iters: 1
2021-06-04 13:24:47 | [train_policy] epoch #30 | optimization finished
2021-06-04 13:24:47 | [train_policy] epoch #30 | Computing KL after
2021-06-04 13:24:47 | [train_policy] epoch #30 | Computing loss after
2021-06-04 13:24:47 | [train_policy] epoch #30 | Fitting baseline...
2021-06-04 13:24:47 | [train_policy] epoch #30 | Saving snapshot...
2021-06-04 13:24:47 | [train_policy] epoch #30 | Saved
2021-06-04 13:24:47 | [train_policy] epoch #30 | Time 27.66 s
2021-06-04 13:24:47 | [train_policy] epoch #30 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286482
Evaluation/AverageDiscountedReturn         -55.7071
Evaluation/AverageReturn                   -55.7071
Evaluation/CompletionRate                    0
Evaluation/Iteration                        30
Evaluation/MaxReturn                       -38.6816
Evaluation/MinReturn                       -84.408
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.23965
Extras/EpisodeRewardMean                   -56.0373
LinearFeatureBaseline/ExplainedVariance      0.874826
PolicyExecTime                               0.214937
ProcessExecTime                              0.0314372
TotalEnvSteps                            31372
policy/Entropy                               2.17471
policy/KL                                    0.00657753
policy/KLBefore                              0
policy/LossAfter                            -0.0366919
policy/LossBefore                           -1.41355e-09
policy/Perplexity                            8.79963
policy/dLoss                                 0.0366919
---------------------------------------  ---------------
2021-06-04 13:24:47 | [train_policy] epoch #31 | Obtaining samples for iteration 31...
2021-06-04 13:24:48 | [train_policy] epoch #31 | Logging diagnostics...
2021-06-04 13:24:48 | [train_policy] epoch #31 | Optimizing policy...
2021-06-04 13:24:48 | [train_policy] epoch #31 | Computing loss before
2021-06-04 13:24:48 | [train_policy] epoch #31 | Computing KL before
2021-06-04 13:24:48 | [train_policy] epoch #31 | Optimizing
2021-06-04 13:24:48 | [train_policy] epoch #31 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:48 | [train_policy] epoch #31 | computing loss before
2021-06-04 13:24:48 | [train_policy] epoch #31 | computing gradient
2021-06-04 13:24:48 | [train_policy] epoch #31 | gradient computed
2021-06-04 13:24:48 | [train_policy] epoch #31 | computing descent direction
2021-06-04 13:24:48 | [train_policy] epoch #31 | descent direction computed
2021-06-04 13:24:48 | [train_policy] epoch #31 | backtrack iters: 0
2021-06-04 13:24:48 | [train_policy] epoch #31 | optimization finished
2021-06-04 13:24:48 | [train_policy] epoch #31 | Computing KL after
2021-06-04 13:24:48 | [train_policy] epoch #31 | Computing loss after
2021-06-04 13:24:48 | [train_policy] epoch #31 | Fitting baseline...
2021-06-04 13:24:48 | [train_policy] epoch #31 | Saving snapshot...
2021-06-04 13:24:48 | [train_policy] epoch #31 | Saved
2021-06-04 13:24:48 | [train_policy] epoch #31 | Time 28.45 s
2021-06-04 13:24:48 | [train_policy] epoch #31 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.284703
Evaluation/AverageDiscountedReturn         -54.5575
Evaluation/AverageReturn                   -54.5575
Evaluation/CompletionRate                    0
Evaluation/Iteration                        31
Evaluation/MaxReturn                       -39.6922
Evaluation/MinReturn                       -71.4153
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.13817
Extras/EpisodeRewardMean                   -54.4558
LinearFeatureBaseline/ExplainedVariance      0.952196
PolicyExecTime                               0.231835
ProcessExecTime                              0.0312791
TotalEnvSteps                            32384
policy/Entropy                               2.16167
policy/KL                                    0.00978353
policy/KLBefore                              0
policy/LossAfter                            -0.0342906
policy/LossBefore                            1.5549e-08
policy/Perplexity                            8.68565
policy/dLoss                                 0.0342907
---------------------------------------  --------------
2021-06-04 13:24:48 | [train_policy] epoch #32 | Obtaining samples for iteration 32...
2021-06-04 13:24:49 | [train_policy] epoch #32 | Logging diagnostics...
2021-06-04 13:24:49 | [train_policy] epoch #32 | Optimizing policy...
2021-06-04 13:24:49 | [train_policy] epoch #32 | Computing loss before
2021-06-04 13:24:49 | [train_policy] epoch #32 | Computing KL before
2021-06-04 13:24:49 | [train_policy] epoch #32 | Optimizing
2021-06-04 13:24:49 | [train_policy] epoch #32 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:49 | [train_policy] epoch #32 | computing loss before
2021-06-04 13:24:49 | [train_policy] epoch #32 | computing gradient
2021-06-04 13:24:49 | [train_policy] epoch #32 | gradient computed
2021-06-04 13:24:49 | [train_policy] epoch #32 | computing descent direction
2021-06-04 13:24:49 | [train_policy] epoch #32 | descent direction computed
2021-06-04 13:24:49 | [train_policy] epoch #32 | backtrack iters: 1
2021-06-04 13:24:49 | [train_policy] epoch #32 | optimization finished
2021-06-04 13:24:49 | [train_policy] epoch #32 | Computing KL after
2021-06-04 13:24:49 | [train_policy] epoch #32 | Computing loss after
2021-06-04 13:24:49 | [train_policy] epoch #32 | Fitting baseline...
2021-06-04 13:24:49 | [train_policy] epoch #32 | Saving snapshot...
2021-06-04 13:24:49 | [train_policy] epoch #32 | Saved
2021-06-04 13:24:49 | [train_policy] epoch #32 | Time 29.25 s
2021-06-04 13:24:49 | [train_policy] epoch #32 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.285763
Evaluation/AverageDiscountedReturn         -63.2787
Evaluation/AverageReturn                   -63.2787
Evaluation/CompletionRate                    0
Evaluation/Iteration                        32
Evaluation/MaxReturn                       -38.8925
Evaluation/MinReturn                      -781.36
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        76.7263
Extras/EpisodeRewardMean                   -62.5008
LinearFeatureBaseline/ExplainedVariance      0.0617628
PolicyExecTime                               0.227787
ProcessExecTime                              0.0313554
TotalEnvSteps                            33396
policy/Entropy                               2.12613
policy/KL                                    0.00680928
policy/KLBefore                              0
policy/LossAfter                            -0.014721
policy/LossBefore                            1.5549e-08
policy/Perplexity                            8.38236
policy/dLoss                                 0.014721
---------------------------------------  --------------
2021-06-04 13:24:49 | [train_policy] epoch #33 | Obtaining samples for iteration 33...
2021-06-04 13:24:50 | [train_policy] epoch #33 | Logging diagnostics...
2021-06-04 13:24:50 | [train_policy] epoch #33 | Optimizing policy...
2021-06-04 13:24:50 | [train_policy] epoch #33 | Computing loss before
2021-06-04 13:24:50 | [train_policy] epoch #33 | Computing KL before
2021-06-04 13:24:50 | [train_policy] epoch #33 | Optimizing
2021-06-04 13:24:50 | [train_policy] epoch #33 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:50 | [train_policy] epoch #33 | computing loss before
2021-06-04 13:24:50 | [train_policy] epoch #33 | computing gradient
2021-06-04 13:24:50 | [train_policy] epoch #33 | gradient computed
2021-06-04 13:24:50 | [train_policy] epoch #33 | computing descent direction
2021-06-04 13:24:50 | [train_policy] epoch #33 | descent direction computed
2021-06-04 13:24:50 | [train_policy] epoch #33 | backtrack iters: 1
2021-06-04 13:24:50 | [train_policy] epoch #33 | optimization finished
2021-06-04 13:24:50 | [train_policy] epoch #33 | Computing KL after
2021-06-04 13:24:50 | [train_policy] epoch #33 | Computing loss after
2021-06-04 13:24:50 | [train_policy] epoch #33 | Fitting baseline...
2021-06-04 13:24:50 | [train_policy] epoch #33 | Saving snapshot...
2021-06-04 13:24:50 | [train_policy] epoch #33 | Saved
2021-06-04 13:24:50 | [train_policy] epoch #33 | Time 30.05 s
2021-06-04 13:24:50 | [train_policy] epoch #33 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.286357
Evaluation/AverageDiscountedReturn         -54.7629
Evaluation/AverageReturn                   -54.7629
Evaluation/CompletionRate                    0
Evaluation/Iteration                        33
Evaluation/MaxReturn                       -41.7541
Evaluation/MinReturn                       -67.6026
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.28883
Extras/EpisodeRewardMean                   -54.3307
LinearFeatureBaseline/ExplainedVariance      0.168956
PolicyExecTime                               0.237742
ProcessExecTime                              0.0313954
TotalEnvSteps                            34408
policy/Entropy                               2.117
policy/KL                                    0.0064306
policy/KLBefore                              0
policy/LossAfter                            -0.0157609
policy/LossBefore                           -9.18807e-09
policy/Perplexity                            8.30619
policy/dLoss                                 0.0157609
---------------------------------------  ---------------
2021-06-04 13:24:50 | [train_policy] epoch #34 | Obtaining samples for iteration 34...
2021-06-04 13:24:50 | [train_policy] epoch #34 | Logging diagnostics...
2021-06-04 13:24:50 | [train_policy] epoch #34 | Optimizing policy...
2021-06-04 13:24:50 | [train_policy] epoch #34 | Computing loss before
2021-06-04 13:24:50 | [train_policy] epoch #34 | Computing KL before
2021-06-04 13:24:50 | [train_policy] epoch #34 | Optimizing
2021-06-04 13:24:50 | [train_policy] epoch #34 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:50 | [train_policy] epoch #34 | computing loss before
2021-06-04 13:24:50 | [train_policy] epoch #34 | computing gradient
2021-06-04 13:24:50 | [train_policy] epoch #34 | gradient computed
2021-06-04 13:24:50 | [train_policy] epoch #34 | computing descent direction
2021-06-04 13:24:50 | [train_policy] epoch #34 | descent direction computed
2021-06-04 13:24:50 | [train_policy] epoch #34 | backtrack iters: 1
2021-06-04 13:24:50 | [train_policy] epoch #34 | optimization finished
2021-06-04 13:24:50 | [train_policy] epoch #34 | Computing KL after
2021-06-04 13:24:50 | [train_policy] epoch #34 | Computing loss after
2021-06-04 13:24:50 | [train_policy] epoch #34 | Fitting baseline...
2021-06-04 13:24:50 | [train_policy] epoch #34 | Saving snapshot...
2021-06-04 13:24:50 | [train_policy] epoch #34 | Saved
2021-06-04 13:24:50 | [train_policy] epoch #34 | Time 30.86 s
2021-06-04 13:24:50 | [train_policy] epoch #34 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.294285
Evaluation/AverageDiscountedReturn         -54.469
Evaluation/AverageReturn                   -54.469
Evaluation/CompletionRate                    0
Evaluation/Iteration                        34
Evaluation/MaxReturn                       -37.9706
Evaluation/MinReturn                       -72.5525
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.65753
Extras/EpisodeRewardMean                   -54.5735
LinearFeatureBaseline/ExplainedVariance      0.960442
PolicyExecTime                               0.225921
ProcessExecTime                              0.0322757
TotalEnvSteps                            35420
policy/Entropy                               2.13787
policy/KL                                    0.0066692
policy/KLBefore                              0
policy/LossAfter                            -0.033877
policy/LossBefore                            3.29828e-09
policy/Perplexity                            8.48133
policy/dLoss                                 0.033877
---------------------------------------  ---------------
2021-06-04 13:24:50 | [train_policy] epoch #35 | Obtaining samples for iteration 35...
2021-06-04 13:24:51 | [train_policy] epoch #35 | Logging diagnostics...
2021-06-04 13:24:51 | [train_policy] epoch #35 | Optimizing policy...
2021-06-04 13:24:51 | [train_policy] epoch #35 | Computing loss before
2021-06-04 13:24:51 | [train_policy] epoch #35 | Computing KL before
2021-06-04 13:24:51 | [train_policy] epoch #35 | Optimizing
2021-06-04 13:24:51 | [train_policy] epoch #35 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:51 | [train_policy] epoch #35 | computing loss before
2021-06-04 13:24:51 | [train_policy] epoch #35 | computing gradient
2021-06-04 13:24:51 | [train_policy] epoch #35 | gradient computed
2021-06-04 13:24:51 | [train_policy] epoch #35 | computing descent direction
2021-06-04 13:24:51 | [train_policy] epoch #35 | descent direction computed
2021-06-04 13:24:51 | [train_policy] epoch #35 | backtrack iters: 1
2021-06-04 13:24:51 | [train_policy] epoch #35 | optimization finished
2021-06-04 13:24:51 | [train_policy] epoch #35 | Computing KL after
2021-06-04 13:24:51 | [train_policy] epoch #35 | Computing loss after
2021-06-04 13:24:51 | [train_policy] epoch #35 | Fitting baseline...
2021-06-04 13:24:51 | [train_policy] epoch #35 | Saving snapshot...
2021-06-04 13:24:51 | [train_policy] epoch #35 | Saved
2021-06-04 13:24:51 | [train_policy] epoch #35 | Time 31.69 s
2021-06-04 13:24:51 | [train_policy] epoch #35 | EpochTime 0.81 s
---------------------------------------  ---------------
EnvExecTime                                  0.301463
Evaluation/AverageDiscountedReturn         -53.1066
Evaluation/AverageReturn                   -53.1066
Evaluation/CompletionRate                    0
Evaluation/Iteration                        35
Evaluation/MaxReturn                       -45.3346
Evaluation/MinReturn                       -74.9752
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.71357
Extras/EpisodeRewardMean                   -53.3004
LinearFeatureBaseline/ExplainedVariance      0.955272
PolicyExecTime                               0.247057
ProcessExecTime                              0.0330567
TotalEnvSteps                            36432
policy/Entropy                               2.13948
policy/KL                                    0.00646827
policy/KLBefore                              0
policy/LossAfter                            -0.034601
policy/LossBefore                           -1.36643e-08
policy/Perplexity                            8.49504
policy/dLoss                                 0.034601
---------------------------------------  ---------------
2021-06-04 13:24:51 | [train_policy] epoch #36 | Obtaining samples for iteration 36...
2021-06-04 13:24:52 | [train_policy] epoch #36 | Logging diagnostics...
2021-06-04 13:24:52 | [train_policy] epoch #36 | Optimizing policy...
2021-06-04 13:24:52 | [train_policy] epoch #36 | Computing loss before
2021-06-04 13:24:52 | [train_policy] epoch #36 | Computing KL before
2021-06-04 13:24:52 | [train_policy] epoch #36 | Optimizing
2021-06-04 13:24:52 | [train_policy] epoch #36 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:52 | [train_policy] epoch #36 | computing loss before
2021-06-04 13:24:52 | [train_policy] epoch #36 | computing gradient
2021-06-04 13:24:52 | [train_policy] epoch #36 | gradient computed
2021-06-04 13:24:52 | [train_policy] epoch #36 | computing descent direction
2021-06-04 13:24:52 | [train_policy] epoch #36 | descent direction computed
2021-06-04 13:24:52 | [train_policy] epoch #36 | backtrack iters: 1
2021-06-04 13:24:52 | [train_policy] epoch #36 | optimization finished
2021-06-04 13:24:52 | [train_policy] epoch #36 | Computing KL after
2021-06-04 13:24:52 | [train_policy] epoch #36 | Computing loss after
2021-06-04 13:24:52 | [train_policy] epoch #36 | Fitting baseline...
2021-06-04 13:24:52 | [train_policy] epoch #36 | Saving snapshot...
2021-06-04 13:24:52 | [train_policy] epoch #36 | Saved
2021-06-04 13:24:52 | [train_policy] epoch #36 | Time 32.50 s
2021-06-04 13:24:52 | [train_policy] epoch #36 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.303071
Evaluation/AverageDiscountedReturn         -52.9547
Evaluation/AverageReturn                   -52.9547
Evaluation/CompletionRate                    0
Evaluation/Iteration                        36
Evaluation/MaxReturn                       -37.2652
Evaluation/MinReturn                       -73.1587
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.8029
Extras/EpisodeRewardMean                   -52.7592
LinearFeatureBaseline/ExplainedVariance      0.961864
PolicyExecTime                               0.217401
ProcessExecTime                              0.0323424
TotalEnvSteps                            37444
policy/Entropy                               2.13207
policy/KL                                    0.00666044
policy/KLBefore                              0
policy/LossAfter                            -0.0280581
policy/LossBefore                           -1.41355e-08
policy/Perplexity                            8.43232
policy/dLoss                                 0.0280581
---------------------------------------  ---------------
2021-06-04 13:24:52 | [train_policy] epoch #37 | Obtaining samples for iteration 37...
2021-06-04 13:24:53 | [train_policy] epoch #37 | Logging diagnostics...
2021-06-04 13:24:53 | [train_policy] epoch #37 | Optimizing policy...
2021-06-04 13:24:53 | [train_policy] epoch #37 | Computing loss before
2021-06-04 13:24:53 | [train_policy] epoch #37 | Computing KL before
2021-06-04 13:24:53 | [train_policy] epoch #37 | Optimizing
2021-06-04 13:24:53 | [train_policy] epoch #37 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:53 | [train_policy] epoch #37 | computing loss before
2021-06-04 13:24:53 | [train_policy] epoch #37 | computing gradient
2021-06-04 13:24:53 | [train_policy] epoch #37 | gradient computed
2021-06-04 13:24:53 | [train_policy] epoch #37 | computing descent direction
2021-06-04 13:24:53 | [train_policy] epoch #37 | descent direction computed
2021-06-04 13:24:53 | [train_policy] epoch #37 | backtrack iters: 1
2021-06-04 13:24:53 | [train_policy] epoch #37 | optimization finished
2021-06-04 13:24:53 | [train_policy] epoch #37 | Computing KL after
2021-06-04 13:24:53 | [train_policy] epoch #37 | Computing loss after
2021-06-04 13:24:53 | [train_policy] epoch #37 | Fitting baseline...
2021-06-04 13:24:53 | [train_policy] epoch #37 | Saving snapshot...
2021-06-04 13:24:53 | [train_policy] epoch #37 | Saved
2021-06-04 13:24:53 | [train_policy] epoch #37 | Time 33.29 s
2021-06-04 13:24:53 | [train_policy] epoch #37 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.287313
Evaluation/AverageDiscountedReturn         -52.8721
Evaluation/AverageReturn                   -52.8721
Evaluation/CompletionRate                    0
Evaluation/Iteration                        37
Evaluation/MaxReturn                       -39.8667
Evaluation/MinReturn                       -97.0365
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.63511
Extras/EpisodeRewardMean                   -52.8055
LinearFeatureBaseline/ExplainedVariance      0.889803
PolicyExecTime                               0.234381
ProcessExecTime                              0.0316145
TotalEnvSteps                            38456
policy/Entropy                               2.10399
policy/KL                                    0.00723354
policy/KLBefore                              0
policy/LossAfter                            -0.0252242
policy/LossBefore                           -6.59656e-09
policy/Perplexity                            8.19886
policy/dLoss                                 0.0252242
---------------------------------------  ---------------
2021-06-04 13:24:53 | [train_policy] epoch #38 | Obtaining samples for iteration 38...
2021-06-04 13:24:54 | [train_policy] epoch #38 | Logging diagnostics...
2021-06-04 13:24:54 | [train_policy] epoch #38 | Optimizing policy...
2021-06-04 13:24:54 | [train_policy] epoch #38 | Computing loss before
2021-06-04 13:24:54 | [train_policy] epoch #38 | Computing KL before
2021-06-04 13:24:54 | [train_policy] epoch #38 | Optimizing
2021-06-04 13:24:54 | [train_policy] epoch #38 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:54 | [train_policy] epoch #38 | computing loss before
2021-06-04 13:24:54 | [train_policy] epoch #38 | computing gradient
2021-06-04 13:24:54 | [train_policy] epoch #38 | gradient computed
2021-06-04 13:24:54 | [train_policy] epoch #38 | computing descent direction
2021-06-04 13:24:54 | [train_policy] epoch #38 | descent direction computed
2021-06-04 13:24:54 | [train_policy] epoch #38 | backtrack iters: 1
2021-06-04 13:24:54 | [train_policy] epoch #38 | optimization finished
2021-06-04 13:24:54 | [train_policy] epoch #38 | Computing KL after
2021-06-04 13:24:54 | [train_policy] epoch #38 | Computing loss after
2021-06-04 13:24:54 | [train_policy] epoch #38 | Fitting baseline...
2021-06-04 13:24:54 | [train_policy] epoch #38 | Saving snapshot...
2021-06-04 13:24:54 | [train_policy] epoch #38 | Saved
2021-06-04 13:24:54 | [train_policy] epoch #38 | Time 34.09 s
2021-06-04 13:24:54 | [train_policy] epoch #38 | EpochTime 0.77 s
---------------------------------------  --------------
EnvExecTime                                  0.285367
Evaluation/AverageDiscountedReturn         -94.8927
Evaluation/AverageReturn                   -94.8927
Evaluation/CompletionRate                    0
Evaluation/Iteration                        38
Evaluation/MaxReturn                       -39.3914
Evaluation/MinReturn                     -4058.57
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       415.537
Extras/EpisodeRewardMean                   -91.1222
LinearFeatureBaseline/ExplainedVariance      0.00521326
PolicyExecTime                               0.227771
ProcessExecTime                              0.0312576
TotalEnvSteps                            39468
policy/Entropy                               2.12027
policy/KL                                    0.00689534
policy/KLBefore                              0
policy/LossAfter                            -0.0262496
policy/LossBefore                           -1.0366e-08
policy/Perplexity                            8.33338
policy/dLoss                                 0.0262496
---------------------------------------  --------------
2021-06-04 13:24:54 | [train_policy] epoch #39 | Obtaining samples for iteration 39...
2021-06-04 13:24:54 | [train_policy] epoch #39 | Logging diagnostics...
2021-06-04 13:24:54 | [train_policy] epoch #39 | Optimizing policy...
2021-06-04 13:24:54 | [train_policy] epoch #39 | Computing loss before
2021-06-04 13:24:54 | [train_policy] epoch #39 | Computing KL before
2021-06-04 13:24:54 | [train_policy] epoch #39 | Optimizing
2021-06-04 13:24:54 | [train_policy] epoch #39 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:54 | [train_policy] epoch #39 | computing loss before
2021-06-04 13:24:54 | [train_policy] epoch #39 | computing gradient
2021-06-04 13:24:54 | [train_policy] epoch #39 | gradient computed
2021-06-04 13:24:54 | [train_policy] epoch #39 | computing descent direction
2021-06-04 13:24:54 | [train_policy] epoch #39 | descent direction computed
2021-06-04 13:24:54 | [train_policy] epoch #39 | backtrack iters: 0
2021-06-04 13:24:54 | [train_policy] epoch #39 | optimization finished
2021-06-04 13:24:54 | [train_policy] epoch #39 | Computing KL after
2021-06-04 13:24:54 | [train_policy] epoch #39 | Computing loss after
2021-06-04 13:24:54 | [train_policy] epoch #39 | Fitting baseline...
2021-06-04 13:24:54 | [train_policy] epoch #39 | Saving snapshot...
2021-06-04 13:24:54 | [train_policy] epoch #39 | Saved
2021-06-04 13:24:54 | [train_policy] epoch #39 | Time 34.88 s
2021-06-04 13:24:54 | [train_policy] epoch #39 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.289816
Evaluation/AverageDiscountedReturn         -70.0389
Evaluation/AverageReturn                   -70.0389
Evaluation/CompletionRate                    0
Evaluation/Iteration                        39
Evaluation/MaxReturn                       -39.5686
Evaluation/MinReturn                     -1478.97
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       148.425
Extras/EpisodeRewardMean                  -108.502
LinearFeatureBaseline/ExplainedVariance     -2.38259
PolicyExecTime                               0.230631
ProcessExecTime                              0.0317175
TotalEnvSteps                            40480
policy/Entropy                               2.10124
policy/KL                                    0.00985164
policy/KLBefore                              0
policy/LossAfter                            -0.0348701
policy/LossBefore                            2.35591e-10
policy/Perplexity                            8.17627
policy/dLoss                                 0.0348701
---------------------------------------  ---------------
2021-06-04 13:24:55 | [train_policy] epoch #40 | Obtaining samples for iteration 40...
2021-06-04 13:24:55 | [train_policy] epoch #40 | Logging diagnostics...
2021-06-04 13:24:55 | [train_policy] epoch #40 | Optimizing policy...
2021-06-04 13:24:55 | [train_policy] epoch #40 | Computing loss before
2021-06-04 13:24:55 | [train_policy] epoch #40 | Computing KL before
2021-06-04 13:24:55 | [train_policy] epoch #40 | Optimizing
2021-06-04 13:24:55 | [train_policy] epoch #40 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:55 | [train_policy] epoch #40 | computing loss before
2021-06-04 13:24:55 | [train_policy] epoch #40 | computing gradient
2021-06-04 13:24:55 | [train_policy] epoch #40 | gradient computed
2021-06-04 13:24:55 | [train_policy] epoch #40 | computing descent direction
2021-06-04 13:24:55 | [train_policy] epoch #40 | descent direction computed
2021-06-04 13:24:55 | [train_policy] epoch #40 | backtrack iters: 1
2021-06-04 13:24:55 | [train_policy] epoch #40 | optimization finished
2021-06-04 13:24:55 | [train_policy] epoch #40 | Computing KL after
2021-06-04 13:24:55 | [train_policy] epoch #40 | Computing loss after
2021-06-04 13:24:55 | [train_policy] epoch #40 | Fitting baseline...
2021-06-04 13:24:55 | [train_policy] epoch #40 | Saving snapshot...
2021-06-04 13:24:55 | [train_policy] epoch #40 | Saved
2021-06-04 13:24:55 | [train_policy] epoch #40 | Time 35.68 s
2021-06-04 13:24:55 | [train_policy] epoch #40 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.289222
Evaluation/AverageDiscountedReturn        -101.002
Evaluation/AverageReturn                  -101.002
Evaluation/CompletionRate                    0
Evaluation/Iteration                        40
Evaluation/MaxReturn                       -39.4545
Evaluation/MinReturn                     -2066.59
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       296.432
Extras/EpisodeRewardMean                   -97.0306
LinearFeatureBaseline/ExplainedVariance     -0.154581
PolicyExecTime                               0.233651
ProcessExecTime                              0.0317729
TotalEnvSteps                            41492
policy/Entropy                               2.09268
policy/KL                                    0.00643462
policy/KLBefore                              0
policy/LossAfter                            -0.0352083
policy/LossBefore                           -5.4186e-09
policy/Perplexity                            8.10659
policy/dLoss                                 0.0352083
---------------------------------------  --------------
2021-06-04 13:24:55 | [train_policy] epoch #41 | Obtaining samples for iteration 41...
2021-06-04 13:24:56 | [train_policy] epoch #41 | Logging diagnostics...
2021-06-04 13:24:56 | [train_policy] epoch #41 | Optimizing policy...
2021-06-04 13:24:56 | [train_policy] epoch #41 | Computing loss before
2021-06-04 13:24:56 | [train_policy] epoch #41 | Computing KL before
2021-06-04 13:24:56 | [train_policy] epoch #41 | Optimizing
2021-06-04 13:24:56 | [train_policy] epoch #41 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:56 | [train_policy] epoch #41 | computing loss before
2021-06-04 13:24:56 | [train_policy] epoch #41 | computing gradient
2021-06-04 13:24:56 | [train_policy] epoch #41 | gradient computed
2021-06-04 13:24:56 | [train_policy] epoch #41 | computing descent direction
2021-06-04 13:24:56 | [train_policy] epoch #41 | descent direction computed
2021-06-04 13:24:56 | [train_policy] epoch #41 | backtrack iters: 0
2021-06-04 13:24:56 | [train_policy] epoch #41 | optimization finished
2021-06-04 13:24:56 | [train_policy] epoch #41 | Computing KL after
2021-06-04 13:24:56 | [train_policy] epoch #41 | Computing loss after
2021-06-04 13:24:56 | [train_policy] epoch #41 | Fitting baseline...
2021-06-04 13:24:56 | [train_policy] epoch #41 | Saving snapshot...
2021-06-04 13:24:56 | [train_policy] epoch #41 | Saved
2021-06-04 13:24:56 | [train_policy] epoch #41 | Time 36.48 s
2021-06-04 13:24:56 | [train_policy] epoch #41 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.288264
Evaluation/AverageDiscountedReturn         -53.5639
Evaluation/AverageReturn                   -53.5639
Evaluation/CompletionRate                    0
Evaluation/Iteration                        41
Evaluation/MaxReturn                       -41.673
Evaluation/MinReturn                      -130.141
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        10.0365
Extras/EpisodeRewardMean                   -53.2404
LinearFeatureBaseline/ExplainedVariance    -42.8555
PolicyExecTime                               0.233432
ProcessExecTime                              0.0315211
TotalEnvSteps                            42504
policy/Entropy                               2.11726
policy/KL                                    0.00969652
policy/KLBefore                              0
policy/LossAfter                            -0.024753
policy/LossBefore                            3.06269e-08
policy/Perplexity                            8.30831
policy/dLoss                                 0.024753
---------------------------------------  ---------------
2021-06-04 13:24:56 | [train_policy] epoch #42 | Obtaining samples for iteration 42...
2021-06-04 13:24:57 | [train_policy] epoch #42 | Logging diagnostics...
2021-06-04 13:24:57 | [train_policy] epoch #42 | Optimizing policy...
2021-06-04 13:24:57 | [train_policy] epoch #42 | Computing loss before
2021-06-04 13:24:57 | [train_policy] epoch #42 | Computing KL before
2021-06-04 13:24:57 | [train_policy] epoch #42 | Optimizing
2021-06-04 13:24:57 | [train_policy] epoch #42 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:57 | [train_policy] epoch #42 | computing loss before
2021-06-04 13:24:57 | [train_policy] epoch #42 | computing gradient
2021-06-04 13:24:57 | [train_policy] epoch #42 | gradient computed
2021-06-04 13:24:57 | [train_policy] epoch #42 | computing descent direction
2021-06-04 13:24:57 | [train_policy] epoch #42 | descent direction computed
2021-06-04 13:24:57 | [train_policy] epoch #42 | backtrack iters: 1
2021-06-04 13:24:57 | [train_policy] epoch #42 | optimization finished
2021-06-04 13:24:57 | [train_policy] epoch #42 | Computing KL after
2021-06-04 13:24:57 | [train_policy] epoch #42 | Computing loss after
2021-06-04 13:24:57 | [train_policy] epoch #42 | Fitting baseline...
2021-06-04 13:24:57 | [train_policy] epoch #42 | Saving snapshot...
2021-06-04 13:24:57 | [train_policy] epoch #42 | Saved
2021-06-04 13:24:57 | [train_policy] epoch #42 | Time 37.28 s
2021-06-04 13:24:57 | [train_policy] epoch #42 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.28951
Evaluation/AverageDiscountedReturn         -52.7179
Evaluation/AverageReturn                   -52.7179
Evaluation/CompletionRate                    0
Evaluation/Iteration                        42
Evaluation/MaxReturn                       -37.5331
Evaluation/MinReturn                       -80.5897
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.50429
Extras/EpisodeRewardMean                   -53.4802
LinearFeatureBaseline/ExplainedVariance      0.937468
PolicyExecTime                               0.236146
ProcessExecTime                              0.0317292
TotalEnvSteps                            43516
policy/Entropy                               2.064
policy/KL                                    0.00710048
policy/KLBefore                              0
policy/LossAfter                            -0.0219265
policy/LossBefore                           -7.30334e-09
policy/Perplexity                            7.87741
policy/dLoss                                 0.0219265
---------------------------------------  ---------------
2021-06-04 13:24:57 | [train_policy] epoch #43 | Obtaining samples for iteration 43...
2021-06-04 13:24:58 | [train_policy] epoch #43 | Logging diagnostics...
2021-06-04 13:24:58 | [train_policy] epoch #43 | Optimizing policy...
2021-06-04 13:24:58 | [train_policy] epoch #43 | Computing loss before
2021-06-04 13:24:58 | [train_policy] epoch #43 | Computing KL before
2021-06-04 13:24:58 | [train_policy] epoch #43 | Optimizing
2021-06-04 13:24:58 | [train_policy] epoch #43 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:58 | [train_policy] epoch #43 | computing loss before
2021-06-04 13:24:58 | [train_policy] epoch #43 | computing gradient
2021-06-04 13:24:58 | [train_policy] epoch #43 | gradient computed
2021-06-04 13:24:58 | [train_policy] epoch #43 | computing descent direction
2021-06-04 13:24:58 | [train_policy] epoch #43 | descent direction computed
2021-06-04 13:24:58 | [train_policy] epoch #43 | backtrack iters: 1
2021-06-04 13:24:58 | [train_policy] epoch #43 | optimization finished
2021-06-04 13:24:58 | [train_policy] epoch #43 | Computing KL after
2021-06-04 13:24:58 | [train_policy] epoch #43 | Computing loss after
2021-06-04 13:24:58 | [train_policy] epoch #43 | Fitting baseline...
2021-06-04 13:24:58 | [train_policy] epoch #43 | Saving snapshot...
2021-06-04 13:24:58 | [train_policy] epoch #43 | Saved
2021-06-04 13:24:58 | [train_policy] epoch #43 | Time 38.09 s
2021-06-04 13:24:58 | [train_policy] epoch #43 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.286131
Evaluation/AverageDiscountedReturn         -52.3317
Evaluation/AverageReturn                   -52.3317
Evaluation/CompletionRate                    0
Evaluation/Iteration                        43
Evaluation/MaxReturn                       -38.6294
Evaluation/MinReturn                      -113.864
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        10.3463
Extras/EpisodeRewardMean                   -52.301
LinearFeatureBaseline/ExplainedVariance      0.735105
PolicyExecTime                               0.233329
ProcessExecTime                              0.0313735
TotalEnvSteps                            44528
policy/Entropy                               1.99854
policy/KL                                    0.00685616
policy/KLBefore                              0
policy/LossAfter                            -0.0226473
policy/LossBefore                           -1.0366e-08
policy/Perplexity                            7.37827
policy/dLoss                                 0.0226473
---------------------------------------  --------------
2021-06-04 13:24:58 | [train_policy] epoch #44 | Obtaining samples for iteration 44...
2021-06-04 13:24:58 | [train_policy] epoch #44 | Logging diagnostics...
2021-06-04 13:24:58 | [train_policy] epoch #44 | Optimizing policy...
2021-06-04 13:24:58 | [train_policy] epoch #44 | Computing loss before
2021-06-04 13:24:58 | [train_policy] epoch #44 | Computing KL before
2021-06-04 13:24:58 | [train_policy] epoch #44 | Optimizing
2021-06-04 13:24:58 | [train_policy] epoch #44 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:58 | [train_policy] epoch #44 | computing loss before
2021-06-04 13:24:58 | [train_policy] epoch #44 | computing gradient
2021-06-04 13:24:58 | [train_policy] epoch #44 | gradient computed
2021-06-04 13:24:58 | [train_policy] epoch #44 | computing descent direction
2021-06-04 13:24:58 | [train_policy] epoch #44 | descent direction computed
2021-06-04 13:24:58 | [train_policy] epoch #44 | backtrack iters: 0
2021-06-04 13:24:58 | [train_policy] epoch #44 | optimization finished
2021-06-04 13:24:58 | [train_policy] epoch #44 | Computing KL after
2021-06-04 13:24:58 | [train_policy] epoch #44 | Computing loss after
2021-06-04 13:24:58 | [train_policy] epoch #44 | Fitting baseline...
2021-06-04 13:24:58 | [train_policy] epoch #44 | Saving snapshot...
2021-06-04 13:24:58 | [train_policy] epoch #44 | Saved
2021-06-04 13:24:58 | [train_policy] epoch #44 | Time 38.86 s
2021-06-04 13:24:58 | [train_policy] epoch #44 | EpochTime 0.75 s
---------------------------------------  --------------
EnvExecTime                                  0.290677
Evaluation/AverageDiscountedReturn         -52.4745
Evaluation/AverageReturn                   -52.4745
Evaluation/CompletionRate                    0
Evaluation/Iteration                        44
Evaluation/MaxReturn                       -36.411
Evaluation/MinReturn                       -88.9906
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.74252
Extras/EpisodeRewardMean                   -52.4573
LinearFeatureBaseline/ExplainedVariance      0.873182
PolicyExecTime                               0.210998
ProcessExecTime                              0.0318148
TotalEnvSteps                            45540
policy/Entropy                               1.96288
policy/KL                                    0.00891012
policy/KLBefore                              0
policy/LossAfter                            -0.0198739
policy/LossBefore                           -1.5549e-08
policy/Perplexity                            7.11979
policy/dLoss                                 0.0198738
---------------------------------------  --------------
2021-06-04 13:24:58 | [train_policy] epoch #45 | Obtaining samples for iteration 45...
2021-06-04 13:24:59 | [train_policy] epoch #45 | Logging diagnostics...
2021-06-04 13:24:59 | [train_policy] epoch #45 | Optimizing policy...
2021-06-04 13:24:59 | [train_policy] epoch #45 | Computing loss before
2021-06-04 13:24:59 | [train_policy] epoch #45 | Computing KL before
2021-06-04 13:24:59 | [train_policy] epoch #45 | Optimizing
2021-06-04 13:24:59 | [train_policy] epoch #45 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:24:59 | [train_policy] epoch #45 | computing loss before
2021-06-04 13:24:59 | [train_policy] epoch #45 | computing gradient
2021-06-04 13:24:59 | [train_policy] epoch #45 | gradient computed
2021-06-04 13:24:59 | [train_policy] epoch #45 | computing descent direction
2021-06-04 13:24:59 | [train_policy] epoch #45 | descent direction computed
2021-06-04 13:24:59 | [train_policy] epoch #45 | backtrack iters: 0
2021-06-04 13:24:59 | [train_policy] epoch #45 | optimization finished
2021-06-04 13:24:59 | [train_policy] epoch #45 | Computing KL after
2021-06-04 13:24:59 | [train_policy] epoch #45 | Computing loss after
2021-06-04 13:24:59 | [train_policy] epoch #45 | Fitting baseline...
2021-06-04 13:24:59 | [train_policy] epoch #45 | Saving snapshot...
2021-06-04 13:24:59 | [train_policy] epoch #45 | Saved
2021-06-04 13:24:59 | [train_policy] epoch #45 | Time 39.65 s
2021-06-04 13:24:59 | [train_policy] epoch #45 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.287279
Evaluation/AverageDiscountedReturn         -51.4886
Evaluation/AverageReturn                   -51.4886
Evaluation/CompletionRate                    0
Evaluation/Iteration                        45
Evaluation/MaxReturn                       -39.0268
Evaluation/MinReturn                       -70.0793
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.02459
Extras/EpisodeRewardMean                   -51.7147
LinearFeatureBaseline/ExplainedVariance      0.951928
PolicyExecTime                               0.221466
ProcessExecTime                              0.0314922
TotalEnvSteps                            46552
policy/Entropy                               1.9487
policy/KL                                    0.00861981
policy/KLBefore                              0
policy/LossAfter                            -0.0270694
policy/LossBefore                            7.53893e-09
policy/Perplexity                            7.01954
policy/dLoss                                 0.0270694
---------------------------------------  ---------------
2021-06-04 13:24:59 | [train_policy] epoch #46 | Obtaining samples for iteration 46...
2021-06-04 13:25:00 | [train_policy] epoch #46 | Logging diagnostics...
2021-06-04 13:25:00 | [train_policy] epoch #46 | Optimizing policy...
2021-06-04 13:25:00 | [train_policy] epoch #46 | Computing loss before
2021-06-04 13:25:00 | [train_policy] epoch #46 | Computing KL before
2021-06-04 13:25:00 | [train_policy] epoch #46 | Optimizing
2021-06-04 13:25:00 | [train_policy] epoch #46 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:00 | [train_policy] epoch #46 | computing loss before
2021-06-04 13:25:00 | [train_policy] epoch #46 | computing gradient
2021-06-04 13:25:00 | [train_policy] epoch #46 | gradient computed
2021-06-04 13:25:00 | [train_policy] epoch #46 | computing descent direction
2021-06-04 13:25:00 | [train_policy] epoch #46 | descent direction computed
2021-06-04 13:25:00 | [train_policy] epoch #46 | backtrack iters: 1
2021-06-04 13:25:00 | [train_policy] epoch #46 | optimization finished
2021-06-04 13:25:00 | [train_policy] epoch #46 | Computing KL after
2021-06-04 13:25:00 | [train_policy] epoch #46 | Computing loss after
2021-06-04 13:25:00 | [train_policy] epoch #46 | Fitting baseline...
2021-06-04 13:25:00 | [train_policy] epoch #46 | Saving snapshot...
2021-06-04 13:25:00 | [train_policy] epoch #46 | Saved
2021-06-04 13:25:00 | [train_policy] epoch #46 | Time 40.50 s
2021-06-04 13:25:00 | [train_policy] epoch #46 | EpochTime 0.84 s
---------------------------------------  ---------------
EnvExecTime                                  0.321656
Evaluation/AverageDiscountedReturn         -82.3815
Evaluation/AverageReturn                   -82.3815
Evaluation/CompletionRate                    0
Evaluation/Iteration                        46
Evaluation/MaxReturn                       -41.4237
Evaluation/MinReturn                     -2060.29
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       217.015
Extras/EpisodeRewardMean                   -80.1439
LinearFeatureBaseline/ExplainedVariance      0.0102246
PolicyExecTime                               0.245828
ProcessExecTime                              0.0354905
TotalEnvSteps                            47564
policy/Entropy                               1.91133
policy/KL                                    0.0064497
policy/KLBefore                              0
policy/LossAfter                            -0.0119162
policy/LossBefore                            1.31931e-08
policy/Perplexity                            6.76206
policy/dLoss                                 0.0119162
---------------------------------------  ---------------
2021-06-04 13:25:00 | [train_policy] epoch #47 | Obtaining samples for iteration 47...
2021-06-04 13:25:01 | [train_policy] epoch #47 | Logging diagnostics...
2021-06-04 13:25:01 | [train_policy] epoch #47 | Optimizing policy...
2021-06-04 13:25:01 | [train_policy] epoch #47 | Computing loss before
2021-06-04 13:25:01 | [train_policy] epoch #47 | Computing KL before
2021-06-04 13:25:01 | [train_policy] epoch #47 | Optimizing
2021-06-04 13:25:01 | [train_policy] epoch #47 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:01 | [train_policy] epoch #47 | computing loss before
2021-06-04 13:25:01 | [train_policy] epoch #47 | computing gradient
2021-06-04 13:25:01 | [train_policy] epoch #47 | gradient computed
2021-06-04 13:25:01 | [train_policy] epoch #47 | computing descent direction
2021-06-04 13:25:01 | [train_policy] epoch #47 | descent direction computed
2021-06-04 13:25:01 | [train_policy] epoch #47 | backtrack iters: 1
2021-06-04 13:25:01 | [train_policy] epoch #47 | optimization finished
2021-06-04 13:25:01 | [train_policy] epoch #47 | Computing KL after
2021-06-04 13:25:01 | [train_policy] epoch #47 | Computing loss after
2021-06-04 13:25:01 | [train_policy] epoch #47 | Fitting baseline...
2021-06-04 13:25:01 | [train_policy] epoch #47 | Saving snapshot...
2021-06-04 13:25:01 | [train_policy] epoch #47 | Saved
2021-06-04 13:25:01 | [train_policy] epoch #47 | Time 41.36 s
2021-06-04 13:25:01 | [train_policy] epoch #47 | EpochTime 0.83 s
---------------------------------------  ---------------
EnvExecTime                                  0.325615
Evaluation/AverageDiscountedReturn         -51.9274
Evaluation/AverageReturn                   -51.9274
Evaluation/CompletionRate                    0
Evaluation/Iteration                        47
Evaluation/MaxReturn                       -40.9871
Evaluation/MinReturn                       -70.1573
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.1439
Extras/EpisodeRewardMean                   -51.7983
LinearFeatureBaseline/ExplainedVariance    -20.5446
PolicyExecTime                               0.235477
ProcessExecTime                              0.0357988
TotalEnvSteps                            48576
policy/Entropy                               1.90882
policy/KL                                    0.00662061
policy/KLBefore                              0
policy/LossAfter                            -0.0201158
policy/LossBefore                           -5.88979e-09
policy/Perplexity                            6.74511
policy/dLoss                                 0.0201158
---------------------------------------  ---------------
2021-06-04 13:25:01 | [train_policy] epoch #48 | Obtaining samples for iteration 48...
2021-06-04 13:25:02 | [train_policy] epoch #48 | Logging diagnostics...
2021-06-04 13:25:02 | [train_policy] epoch #48 | Optimizing policy...
2021-06-04 13:25:02 | [train_policy] epoch #48 | Computing loss before
2021-06-04 13:25:02 | [train_policy] epoch #48 | Computing KL before
2021-06-04 13:25:02 | [train_policy] epoch #48 | Optimizing
2021-06-04 13:25:02 | [train_policy] epoch #48 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:02 | [train_policy] epoch #48 | computing loss before
2021-06-04 13:25:02 | [train_policy] epoch #48 | computing gradient
2021-06-04 13:25:02 | [train_policy] epoch #48 | gradient computed
2021-06-04 13:25:02 | [train_policy] epoch #48 | computing descent direction
2021-06-04 13:25:02 | [train_policy] epoch #48 | descent direction computed
2021-06-04 13:25:02 | [train_policy] epoch #48 | backtrack iters: 1
2021-06-04 13:25:02 | [train_policy] epoch #48 | optimization finished
2021-06-04 13:25:02 | [train_policy] epoch #48 | Computing KL after
2021-06-04 13:25:02 | [train_policy] epoch #48 | Computing loss after
2021-06-04 13:25:02 | [train_policy] epoch #48 | Fitting baseline...
2021-06-04 13:25:02 | [train_policy] epoch #48 | Saving snapshot...
2021-06-04 13:25:02 | [train_policy] epoch #48 | Saved
2021-06-04 13:25:02 | [train_policy] epoch #48 | Time 42.16 s
2021-06-04 13:25:02 | [train_policy] epoch #48 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.289127
Evaluation/AverageDiscountedReturn         -73.6736
Evaluation/AverageReturn                   -73.6736
Evaluation/CompletionRate                    0
Evaluation/Iteration                        48
Evaluation/MaxReturn                       -38.5195
Evaluation/MinReturn                     -2067.87
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.145
Extras/EpisodeRewardMean                   -71.4964
LinearFeatureBaseline/ExplainedVariance      0.0101164
PolicyExecTime                               0.227217
ProcessExecTime                              0.0316837
TotalEnvSteps                            49588
policy/Entropy                               1.89845
policy/KL                                    0.00685991
policy/KLBefore                              0
policy/LossAfter                            -0.0181343
policy/LossBefore                           -8.01011e-09
policy/Perplexity                            6.67554
policy/dLoss                                 0.0181343
---------------------------------------  ---------------
2021-06-04 13:25:02 | [train_policy] epoch #49 | Obtaining samples for iteration 49...
2021-06-04 13:25:02 | [train_policy] epoch #49 | Logging diagnostics...
2021-06-04 13:25:02 | [train_policy] epoch #49 | Optimizing policy...
2021-06-04 13:25:02 | [train_policy] epoch #49 | Computing loss before
2021-06-04 13:25:02 | [train_policy] epoch #49 | Computing KL before
2021-06-04 13:25:02 | [train_policy] epoch #49 | Optimizing
2021-06-04 13:25:02 | [train_policy] epoch #49 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:02 | [train_policy] epoch #49 | computing loss before
2021-06-04 13:25:02 | [train_policy] epoch #49 | computing gradient
2021-06-04 13:25:02 | [train_policy] epoch #49 | gradient computed
2021-06-04 13:25:02 | [train_policy] epoch #49 | computing descent direction
2021-06-04 13:25:02 | [train_policy] epoch #49 | descent direction computed
2021-06-04 13:25:03 | [train_policy] epoch #49 | backtrack iters: 1
2021-06-04 13:25:03 | [train_policy] epoch #49 | optimization finished
2021-06-04 13:25:03 | [train_policy] epoch #49 | Computing KL after
2021-06-04 13:25:03 | [train_policy] epoch #49 | Computing loss after
2021-06-04 13:25:03 | [train_policy] epoch #49 | Fitting baseline...
2021-06-04 13:25:03 | [train_policy] epoch #49 | Saving snapshot...
2021-06-04 13:25:03 | [train_policy] epoch #49 | Saved
2021-06-04 13:25:03 | [train_policy] epoch #49 | Time 42.96 s
2021-06-04 13:25:03 | [train_policy] epoch #49 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.288748
Evaluation/AverageDiscountedReturn         -50.6799
Evaluation/AverageReturn                   -50.6799
Evaluation/CompletionRate                    0
Evaluation/Iteration                        49
Evaluation/MaxReturn                       -39.8902
Evaluation/MinReturn                       -70.6952
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.51689
Extras/EpisodeRewardMean                   -50.9517
LinearFeatureBaseline/ExplainedVariance    -47.3925
PolicyExecTime                               0.227865
ProcessExecTime                              0.03158
TotalEnvSteps                            50600
policy/Entropy                               1.89061
policy/KL                                    0.00652779
policy/KLBefore                              0
policy/LossAfter                            -0.0193667
policy/LossBefore                           -6.59656e-09
policy/Perplexity                            6.62338
policy/dLoss                                 0.0193666
---------------------------------------  ---------------
2021-06-04 13:25:03 | [train_policy] epoch #50 | Obtaining samples for iteration 50...
2021-06-04 13:25:03 | [train_policy] epoch #50 | Logging diagnostics...
2021-06-04 13:25:03 | [train_policy] epoch #50 | Optimizing policy...
2021-06-04 13:25:03 | [train_policy] epoch #50 | Computing loss before
2021-06-04 13:25:03 | [train_policy] epoch #50 | Computing KL before
2021-06-04 13:25:03 | [train_policy] epoch #50 | Optimizing
2021-06-04 13:25:03 | [train_policy] epoch #50 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:03 | [train_policy] epoch #50 | computing loss before
2021-06-04 13:25:03 | [train_policy] epoch #50 | computing gradient
2021-06-04 13:25:03 | [train_policy] epoch #50 | gradient computed
2021-06-04 13:25:03 | [train_policy] epoch #50 | computing descent direction
2021-06-04 13:25:03 | [train_policy] epoch #50 | descent direction computed
2021-06-04 13:25:03 | [train_policy] epoch #50 | backtrack iters: 0
2021-06-04 13:25:03 | [train_policy] epoch #50 | optimization finished
2021-06-04 13:25:03 | [train_policy] epoch #50 | Computing KL after
2021-06-04 13:25:03 | [train_policy] epoch #50 | Computing loss after
2021-06-04 13:25:03 | [train_policy] epoch #50 | Fitting baseline...
2021-06-04 13:25:03 | [train_policy] epoch #50 | Saving snapshot...
2021-06-04 13:25:03 | [train_policy] epoch #50 | Saved
2021-06-04 13:25:03 | [train_policy] epoch #50 | Time 43.75 s
2021-06-04 13:25:03 | [train_policy] epoch #50 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.28952
Evaluation/AverageDiscountedReturn         -50.7621
Evaluation/AverageReturn                   -50.7621
Evaluation/CompletionRate                    0
Evaluation/Iteration                        50
Evaluation/MaxReturn                       -36.6961
Evaluation/MinReturn                       -74.2224
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.56702
Extras/EpisodeRewardMean                   -50.822
LinearFeatureBaseline/ExplainedVariance      0.952985
PolicyExecTime                               0.221933
ProcessExecTime                              0.0317411
TotalEnvSteps                            51612
policy/Entropy                               1.90656
policy/KL                                    0.00971858
policy/KLBefore                              0
policy/LossAfter                            -0.0234037
policy/LossBefore                           -1.41355e-08
policy/Perplexity                            6.72989
policy/dLoss                                 0.0234037
---------------------------------------  ---------------
2021-06-04 13:25:03 | [train_policy] epoch #51 | Obtaining samples for iteration 51...
2021-06-04 13:25:04 | [train_policy] epoch #51 | Logging diagnostics...
2021-06-04 13:25:04 | [train_policy] epoch #51 | Optimizing policy...
2021-06-04 13:25:04 | [train_policy] epoch #51 | Computing loss before
2021-06-04 13:25:04 | [train_policy] epoch #51 | Computing KL before
2021-06-04 13:25:04 | [train_policy] epoch #51 | Optimizing
2021-06-04 13:25:04 | [train_policy] epoch #51 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:04 | [train_policy] epoch #51 | computing loss before
2021-06-04 13:25:04 | [train_policy] epoch #51 | computing gradient
2021-06-04 13:25:04 | [train_policy] epoch #51 | gradient computed
2021-06-04 13:25:04 | [train_policy] epoch #51 | computing descent direction
2021-06-04 13:25:04 | [train_policy] epoch #51 | descent direction computed
2021-06-04 13:25:04 | [train_policy] epoch #51 | backtrack iters: 1
2021-06-04 13:25:04 | [train_policy] epoch #51 | optimization finished
2021-06-04 13:25:04 | [train_policy] epoch #51 | Computing KL after
2021-06-04 13:25:04 | [train_policy] epoch #51 | Computing loss after
2021-06-04 13:25:04 | [train_policy] epoch #51 | Fitting baseline...
2021-06-04 13:25:04 | [train_policy] epoch #51 | Saving snapshot...
2021-06-04 13:25:04 | [train_policy] epoch #51 | Saved
2021-06-04 13:25:04 | [train_policy] epoch #51 | Time 44.55 s
2021-06-04 13:25:04 | [train_policy] epoch #51 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.287779
Evaluation/AverageDiscountedReturn         -52.9843
Evaluation/AverageReturn                   -52.9843
Evaluation/CompletionRate                    0
Evaluation/Iteration                        51
Evaluation/MaxReturn                       -35.8583
Evaluation/MinReturn                      -258.056
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        22.6214
Extras/EpisodeRewardMean                   -53.2739
LinearFeatureBaseline/ExplainedVariance      0.477141
PolicyExecTime                               0.233612
ProcessExecTime                              0.031631
TotalEnvSteps                            52624
policy/Entropy                               1.88388
policy/KL                                    0.00647304
policy/KLBefore                              0
policy/LossAfter                            -0.0179144
policy/LossBefore                           -1.27219e-08
policy/Perplexity                            6.57901
policy/dLoss                                 0.0179144
---------------------------------------  ---------------
2021-06-04 13:25:04 | [train_policy] epoch #52 | Obtaining samples for iteration 52...
2021-06-04 13:25:05 | [train_policy] epoch #52 | Logging diagnostics...
2021-06-04 13:25:05 | [train_policy] epoch #52 | Optimizing policy...
2021-06-04 13:25:05 | [train_policy] epoch #52 | Computing loss before
2021-06-04 13:25:05 | [train_policy] epoch #52 | Computing KL before
2021-06-04 13:25:05 | [train_policy] epoch #52 | Optimizing
2021-06-04 13:25:05 | [train_policy] epoch #52 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:05 | [train_policy] epoch #52 | computing loss before
2021-06-04 13:25:05 | [train_policy] epoch #52 | computing gradient
2021-06-04 13:25:05 | [train_policy] epoch #52 | gradient computed
2021-06-04 13:25:05 | [train_policy] epoch #52 | computing descent direction
2021-06-04 13:25:05 | [train_policy] epoch #52 | descent direction computed
2021-06-04 13:25:05 | [train_policy] epoch #52 | backtrack iters: 0
2021-06-04 13:25:05 | [train_policy] epoch #52 | optimization finished
2021-06-04 13:25:05 | [train_policy] epoch #52 | Computing KL after
2021-06-04 13:25:05 | [train_policy] epoch #52 | Computing loss after
2021-06-04 13:25:05 | [train_policy] epoch #52 | Fitting baseline...
2021-06-04 13:25:05 | [train_policy] epoch #52 | Saving snapshot...
2021-06-04 13:25:05 | [train_policy] epoch #52 | Saved
2021-06-04 13:25:05 | [train_policy] epoch #52 | Time 45.35 s
2021-06-04 13:25:05 | [train_policy] epoch #52 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.288329
Evaluation/AverageDiscountedReturn         -52.9896
Evaluation/AverageReturn                   -52.9896
Evaluation/CompletionRate                    0
Evaluation/Iteration                        52
Evaluation/MaxReturn                       -36.2974
Evaluation/MinReturn                      -288.859
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        25.6211
Extras/EpisodeRewardMean                   -52.6893
LinearFeatureBaseline/ExplainedVariance      0.564965
PolicyExecTime                               0.234661
ProcessExecTime                              0.0313644
TotalEnvSteps                            53636
policy/Entropy                               1.89225
policy/KL                                    0.00959447
policy/KLBefore                              0
policy/LossAfter                            -0.0162147
policy/LossBefore                            1.5549e-08
policy/Perplexity                            6.63426
policy/dLoss                                 0.0162147
---------------------------------------  --------------
2021-06-04 13:25:05 | [train_policy] epoch #53 | Obtaining samples for iteration 53...
2021-06-04 13:25:06 | [train_policy] epoch #53 | Logging diagnostics...
2021-06-04 13:25:06 | [train_policy] epoch #53 | Optimizing policy...
2021-06-04 13:25:06 | [train_policy] epoch #53 | Computing loss before
2021-06-04 13:25:06 | [train_policy] epoch #53 | Computing KL before
2021-06-04 13:25:06 | [train_policy] epoch #53 | Optimizing
2021-06-04 13:25:06 | [train_policy] epoch #53 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:06 | [train_policy] epoch #53 | computing loss before
2021-06-04 13:25:06 | [train_policy] epoch #53 | computing gradient
2021-06-04 13:25:06 | [train_policy] epoch #53 | gradient computed
2021-06-04 13:25:06 | [train_policy] epoch #53 | computing descent direction
2021-06-04 13:25:06 | [train_policy] epoch #53 | descent direction computed
2021-06-04 13:25:06 | [train_policy] epoch #53 | backtrack iters: 0
2021-06-04 13:25:06 | [train_policy] epoch #53 | optimization finished
2021-06-04 13:25:06 | [train_policy] epoch #53 | Computing KL after
2021-06-04 13:25:06 | [train_policy] epoch #53 | Computing loss after
2021-06-04 13:25:06 | [train_policy] epoch #53 | Fitting baseline...
2021-06-04 13:25:06 | [train_policy] epoch #53 | Saving snapshot...
2021-06-04 13:25:06 | [train_policy] epoch #53 | Saved
2021-06-04 13:25:06 | [train_policy] epoch #53 | Time 46.14 s
2021-06-04 13:25:06 | [train_policy] epoch #53 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.288419
Evaluation/AverageDiscountedReturn         -95.4055
Evaluation/AverageReturn                   -95.4055
Evaluation/CompletionRate                    0
Evaluation/Iteration                        53
Evaluation/MaxReturn                       -39.6341
Evaluation/MinReturn                     -2062.91
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       293.228
Extras/EpisodeRewardMean                   -92.0995
LinearFeatureBaseline/ExplainedVariance      0.0108166
PolicyExecTime                               0.225734
ProcessExecTime                              0.0316818
TotalEnvSteps                            54648
policy/Entropy                               1.91805
policy/KL                                    0.00985996
policy/KLBefore                              0
policy/LossAfter                            -0.0170196
policy/LossBefore                            4.71183e-10
policy/Perplexity                            6.80766
policy/dLoss                                 0.0170196
---------------------------------------  ---------------
2021-06-04 13:25:06 | [train_policy] epoch #54 | Obtaining samples for iteration 54...
2021-06-04 13:25:06 | [train_policy] epoch #54 | Logging diagnostics...
2021-06-04 13:25:06 | [train_policy] epoch #54 | Optimizing policy...
2021-06-04 13:25:06 | [train_policy] epoch #54 | Computing loss before
2021-06-04 13:25:06 | [train_policy] epoch #54 | Computing KL before
2021-06-04 13:25:06 | [train_policy] epoch #54 | Optimizing
2021-06-04 13:25:06 | [train_policy] epoch #54 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:06 | [train_policy] epoch #54 | computing loss before
2021-06-04 13:25:06 | [train_policy] epoch #54 | computing gradient
2021-06-04 13:25:06 | [train_policy] epoch #54 | gradient computed
2021-06-04 13:25:06 | [train_policy] epoch #54 | computing descent direction
2021-06-04 13:25:07 | [train_policy] epoch #54 | descent direction computed
2021-06-04 13:25:07 | [train_policy] epoch #54 | backtrack iters: 1
2021-06-04 13:25:07 | [train_policy] epoch #54 | optimization finished
2021-06-04 13:25:07 | [train_policy] epoch #54 | Computing KL after
2021-06-04 13:25:07 | [train_policy] epoch #54 | Computing loss after
2021-06-04 13:25:07 | [train_policy] epoch #54 | Fitting baseline...
2021-06-04 13:25:07 | [train_policy] epoch #54 | Saving snapshot...
2021-06-04 13:25:07 | [train_policy] epoch #54 | Saved
2021-06-04 13:25:07 | [train_policy] epoch #54 | Time 46.96 s
2021-06-04 13:25:07 | [train_policy] epoch #54 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                  0.292983
Evaluation/AverageDiscountedReturn         -52.7548
Evaluation/AverageReturn                   -52.7548
Evaluation/CompletionRate                    0
Evaluation/Iteration                        54
Evaluation/MaxReturn                       -39.6813
Evaluation/MinReturn                       -91.1762
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.80247
Extras/EpisodeRewardMean                   -72.5848
LinearFeatureBaseline/ExplainedVariance    -34.8335
PolicyExecTime                               0.23849
ProcessExecTime                              0.0322838
TotalEnvSteps                            55660
policy/Entropy                               1.9233
policy/KL                                    0.00683224
policy/KLBefore                              0
policy/LossAfter                            -0.0134258
policy/LossBefore                            2.63862e-08
policy/Perplexity                            6.84349
policy/dLoss                                 0.0134258
---------------------------------------  ---------------
2021-06-04 13:25:07 | [train_policy] epoch #55 | Obtaining samples for iteration 55...
2021-06-04 13:25:07 | [train_policy] epoch #55 | Logging diagnostics...
2021-06-04 13:25:07 | [train_policy] epoch #55 | Optimizing policy...
2021-06-04 13:25:07 | [train_policy] epoch #55 | Computing loss before
2021-06-04 13:25:07 | [train_policy] epoch #55 | Computing KL before
2021-06-04 13:25:07 | [train_policy] epoch #55 | Optimizing
2021-06-04 13:25:07 | [train_policy] epoch #55 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:07 | [train_policy] epoch #55 | computing loss before
2021-06-04 13:25:07 | [train_policy] epoch #55 | computing gradient
2021-06-04 13:25:07 | [train_policy] epoch #55 | gradient computed
2021-06-04 13:25:07 | [train_policy] epoch #55 | computing descent direction
2021-06-04 13:25:07 | [train_policy] epoch #55 | descent direction computed
2021-06-04 13:25:07 | [train_policy] epoch #55 | backtrack iters: 1
2021-06-04 13:25:07 | [train_policy] epoch #55 | optimization finished
2021-06-04 13:25:07 | [train_policy] epoch #55 | Computing KL after
2021-06-04 13:25:07 | [train_policy] epoch #55 | Computing loss after
2021-06-04 13:25:07 | [train_policy] epoch #55 | Fitting baseline...
2021-06-04 13:25:07 | [train_policy] epoch #55 | Saving snapshot...
2021-06-04 13:25:07 | [train_policy] epoch #55 | Saved
2021-06-04 13:25:07 | [train_policy] epoch #55 | Time 47.75 s
2021-06-04 13:25:07 | [train_policy] epoch #55 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.290088
Evaluation/AverageDiscountedReturn         -50.1717
Evaluation/AverageReturn                   -50.1717
Evaluation/CompletionRate                    0
Evaluation/Iteration                        55
Evaluation/MaxReturn                       -40.6154
Evaluation/MinReturn                       -80.6735
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.96913
Extras/EpisodeRewardMean                   -50.4984
LinearFeatureBaseline/ExplainedVariance      0.935854
PolicyExecTime                               0.224701
ProcessExecTime                              0.0319734
TotalEnvSteps                            56672
policy/Entropy                               1.9016
policy/KL                                    0.00663156
policy/KLBefore                              0
policy/LossAfter                            -0.0231512
policy/LossBefore                           -1.69626e-08
policy/Perplexity                            6.69663
policy/dLoss                                 0.0231511
---------------------------------------  ---------------
2021-06-04 13:25:07 | [train_policy] epoch #56 | Obtaining samples for iteration 56...
2021-06-04 13:25:08 | [train_policy] epoch #56 | Logging diagnostics...
2021-06-04 13:25:08 | [train_policy] epoch #56 | Optimizing policy...
2021-06-04 13:25:08 | [train_policy] epoch #56 | Computing loss before
2021-06-04 13:25:08 | [train_policy] epoch #56 | Computing KL before
2021-06-04 13:25:08 | [train_policy] epoch #56 | Optimizing
2021-06-04 13:25:08 | [train_policy] epoch #56 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:08 | [train_policy] epoch #56 | computing loss before
2021-06-04 13:25:08 | [train_policy] epoch #56 | computing gradient
2021-06-04 13:25:08 | [train_policy] epoch #56 | gradient computed
2021-06-04 13:25:08 | [train_policy] epoch #56 | computing descent direction
2021-06-04 13:25:08 | [train_policy] epoch #56 | descent direction computed
2021-06-04 13:25:08 | [train_policy] epoch #56 | backtrack iters: 1
2021-06-04 13:25:08 | [train_policy] epoch #56 | optimization finished
2021-06-04 13:25:08 | [train_policy] epoch #56 | Computing KL after
2021-06-04 13:25:08 | [train_policy] epoch #56 | Computing loss after
2021-06-04 13:25:08 | [train_policy] epoch #56 | Fitting baseline...
2021-06-04 13:25:08 | [train_policy] epoch #56 | Saving snapshot...
2021-06-04 13:25:08 | [train_policy] epoch #56 | Saved
2021-06-04 13:25:08 | [train_policy] epoch #56 | Time 48.54 s
2021-06-04 13:25:08 | [train_policy] epoch #56 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286455
Evaluation/AverageDiscountedReturn         -50.2214
Evaluation/AverageReturn                   -50.2214
Evaluation/CompletionRate                    0
Evaluation/Iteration                        56
Evaluation/MaxReturn                       -35.8467
Evaluation/MinReturn                       -70.2005
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.62718
Extras/EpisodeRewardMean                   -50.368
LinearFeatureBaseline/ExplainedVariance      0.953184
PolicyExecTime                               0.218904
ProcessExecTime                              0.0314484
TotalEnvSteps                            57684
policy/Entropy                               1.87152
policy/KL                                    0.0065363
policy/KLBefore                              0
policy/LossAfter                            -0.0210513
policy/LossBefore                            4.71183e-09
policy/Perplexity                            6.49819
policy/dLoss                                 0.0210514
---------------------------------------  ---------------
2021-06-04 13:25:08 | [train_policy] epoch #57 | Obtaining samples for iteration 57...
2021-06-04 13:25:09 | [train_policy] epoch #57 | Logging diagnostics...
2021-06-04 13:25:09 | [train_policy] epoch #57 | Optimizing policy...
2021-06-04 13:25:09 | [train_policy] epoch #57 | Computing loss before
2021-06-04 13:25:09 | [train_policy] epoch #57 | Computing KL before
2021-06-04 13:25:09 | [train_policy] epoch #57 | Optimizing
2021-06-04 13:25:09 | [train_policy] epoch #57 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:09 | [train_policy] epoch #57 | computing loss before
2021-06-04 13:25:09 | [train_policy] epoch #57 | computing gradient
2021-06-04 13:25:09 | [train_policy] epoch #57 | gradient computed
2021-06-04 13:25:09 | [train_policy] epoch #57 | computing descent direction
2021-06-04 13:25:09 | [train_policy] epoch #57 | descent direction computed
2021-06-04 13:25:09 | [train_policy] epoch #57 | backtrack iters: 1
2021-06-04 13:25:09 | [train_policy] epoch #57 | optimization finished
2021-06-04 13:25:09 | [train_policy] epoch #57 | Computing KL after
2021-06-04 13:25:09 | [train_policy] epoch #57 | Computing loss after
2021-06-04 13:25:09 | [train_policy] epoch #57 | Fitting baseline...
2021-06-04 13:25:09 | [train_policy] epoch #57 | Saving snapshot...
2021-06-04 13:25:09 | [train_policy] epoch #57 | Saved
2021-06-04 13:25:09 | [train_policy] epoch #57 | Time 49.33 s
2021-06-04 13:25:09 | [train_policy] epoch #57 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.287634
Evaluation/AverageDiscountedReturn         -50.9204
Evaluation/AverageReturn                   -50.9204
Evaluation/CompletionRate                    0
Evaluation/Iteration                        57
Evaluation/MaxReturn                       -38.5103
Evaluation/MinReturn                      -126.441
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        10.1668
Extras/EpisodeRewardMean                   -50.7554
LinearFeatureBaseline/ExplainedVariance      0.854982
PolicyExecTime                               0.221035
ProcessExecTime                              0.0316584
TotalEnvSteps                            58696
policy/Entropy                               1.78371
policy/KL                                    0.0065686
policy/KLBefore                              0
policy/LossAfter                            -0.0231907
policy/LossBefore                           -2.26168e-08
policy/Perplexity                            5.95191
policy/dLoss                                 0.0231907
---------------------------------------  ---------------
2021-06-04 13:25:09 | [train_policy] epoch #58 | Obtaining samples for iteration 58...
2021-06-04 13:25:10 | [train_policy] epoch #58 | Logging diagnostics...
2021-06-04 13:25:10 | [train_policy] epoch #58 | Optimizing policy...
2021-06-04 13:25:10 | [train_policy] epoch #58 | Computing loss before
2021-06-04 13:25:10 | [train_policy] epoch #58 | Computing KL before
2021-06-04 13:25:10 | [train_policy] epoch #58 | Optimizing
2021-06-04 13:25:10 | [train_policy] epoch #58 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:10 | [train_policy] epoch #58 | computing loss before
2021-06-04 13:25:10 | [train_policy] epoch #58 | computing gradient
2021-06-04 13:25:10 | [train_policy] epoch #58 | gradient computed
2021-06-04 13:25:10 | [train_policy] epoch #58 | computing descent direction
2021-06-04 13:25:10 | [train_policy] epoch #58 | descent direction computed
2021-06-04 13:25:10 | [train_policy] epoch #58 | backtrack iters: 1
2021-06-04 13:25:10 | [train_policy] epoch #58 | optimization finished
2021-06-04 13:25:10 | [train_policy] epoch #58 | Computing KL after
2021-06-04 13:25:10 | [train_policy] epoch #58 | Computing loss after
2021-06-04 13:25:10 | [train_policy] epoch #58 | Fitting baseline...
2021-06-04 13:25:10 | [train_policy] epoch #58 | Saving snapshot...
2021-06-04 13:25:10 | [train_policy] epoch #58 | Saved
2021-06-04 13:25:10 | [train_policy] epoch #58 | Time 50.14 s
2021-06-04 13:25:10 | [train_policy] epoch #58 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.292353
Evaluation/AverageDiscountedReturn         -50.7583
Evaluation/AverageReturn                   -50.7583
Evaluation/CompletionRate                    0
Evaluation/Iteration                        58
Evaluation/MaxReturn                       -39.8438
Evaluation/MinReturn                       -85.4648
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.85593
Extras/EpisodeRewardMean                   -50.8841
LinearFeatureBaseline/ExplainedVariance      0.915412
PolicyExecTime                               0.237165
ProcessExecTime                              0.0319567
TotalEnvSteps                            59708
policy/Entropy                               1.73918
policy/KL                                    0.00681671
policy/KLBefore                              0
policy/LossAfter                            -0.0182698
policy/LossBefore                            2.59151e-08
policy/Perplexity                            5.69269
policy/dLoss                                 0.0182699
---------------------------------------  ---------------
2021-06-04 13:25:10 | [train_policy] epoch #59 | Obtaining samples for iteration 59...
2021-06-04 13:25:10 | [train_policy] epoch #59 | Logging diagnostics...
2021-06-04 13:25:10 | [train_policy] epoch #59 | Optimizing policy...
2021-06-04 13:25:10 | [train_policy] epoch #59 | Computing loss before
2021-06-04 13:25:10 | [train_policy] epoch #59 | Computing KL before
2021-06-04 13:25:10 | [train_policy] epoch #59 | Optimizing
2021-06-04 13:25:10 | [train_policy] epoch #59 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:10 | [train_policy] epoch #59 | computing loss before
2021-06-04 13:25:10 | [train_policy] epoch #59 | computing gradient
2021-06-04 13:25:10 | [train_policy] epoch #59 | gradient computed
2021-06-04 13:25:10 | [train_policy] epoch #59 | computing descent direction
2021-06-04 13:25:10 | [train_policy] epoch #59 | descent direction computed
2021-06-04 13:25:10 | [train_policy] epoch #59 | backtrack iters: 1
2021-06-04 13:25:10 | [train_policy] epoch #59 | optimization finished
2021-06-04 13:25:10 | [train_policy] epoch #59 | Computing KL after
2021-06-04 13:25:10 | [train_policy] epoch #59 | Computing loss after
2021-06-04 13:25:10 | [train_policy] epoch #59 | Fitting baseline...
2021-06-04 13:25:11 | [train_policy] epoch #59 | Saving snapshot...
2021-06-04 13:25:11 | [train_policy] epoch #59 | Saved
2021-06-04 13:25:11 | [train_policy] epoch #59 | Time 50.93 s
2021-06-04 13:25:11 | [train_policy] epoch #59 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286267
Evaluation/AverageDiscountedReturn         -70.9614
Evaluation/AverageReturn                   -70.9614
Evaluation/CompletionRate                    0
Evaluation/Iteration                        59
Evaluation/MaxReturn                       -36.2658
Evaluation/MinReturn                     -2055.54
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.118
Extras/EpisodeRewardMean                   -69.2593
LinearFeatureBaseline/ExplainedVariance      0.0110335
PolicyExecTime                               0.218503
ProcessExecTime                              0.0314326
TotalEnvSteps                            60720
policy/Entropy                               1.74197
policy/KL                                    0.00731781
policy/KLBefore                              0
policy/LossAfter                            -0.022642
policy/LossBefore                            7.53893e-09
policy/Perplexity                            5.7086
policy/dLoss                                 0.022642
---------------------------------------  ---------------
2021-06-04 13:25:11 | [train_policy] epoch #60 | Obtaining samples for iteration 60...
2021-06-04 13:25:11 | [train_policy] epoch #60 | Logging diagnostics...
2021-06-04 13:25:11 | [train_policy] epoch #60 | Optimizing policy...
2021-06-04 13:25:11 | [train_policy] epoch #60 | Computing loss before
2021-06-04 13:25:11 | [train_policy] epoch #60 | Computing KL before
2021-06-04 13:25:11 | [train_policy] epoch #60 | Optimizing
2021-06-04 13:25:11 | [train_policy] epoch #60 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:11 | [train_policy] epoch #60 | computing loss before
2021-06-04 13:25:11 | [train_policy] epoch #60 | computing gradient
2021-06-04 13:25:11 | [train_policy] epoch #60 | gradient computed
2021-06-04 13:25:11 | [train_policy] epoch #60 | computing descent direction
2021-06-04 13:25:11 | [train_policy] epoch #60 | descent direction computed
2021-06-04 13:25:11 | [train_policy] epoch #60 | backtrack iters: 1
2021-06-04 13:25:11 | [train_policy] epoch #60 | optimization finished
2021-06-04 13:25:11 | [train_policy] epoch #60 | Computing KL after
2021-06-04 13:25:11 | [train_policy] epoch #60 | Computing loss after
2021-06-04 13:25:11 | [train_policy] epoch #60 | Fitting baseline...
2021-06-04 13:25:11 | [train_policy] epoch #60 | Saving snapshot...
2021-06-04 13:25:11 | [train_policy] epoch #60 | Saved
2021-06-04 13:25:11 | [train_policy] epoch #60 | Time 51.71 s
2021-06-04 13:25:11 | [train_policy] epoch #60 | EpochTime 0.77 s
---------------------------------------  --------------
EnvExecTime                                  0.284578
Evaluation/AverageDiscountedReturn         -56.3315
Evaluation/AverageReturn                   -56.3315
Evaluation/CompletionRate                    0
Evaluation/Iteration                        60
Evaluation/MaxReturn                       -36.5408
Evaluation/MinReturn                      -387.171
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        37.0888
Extras/EpisodeRewardMean                   -55.7276
LinearFeatureBaseline/ExplainedVariance    -12.5335
PolicyExecTime                               0.220032
ProcessExecTime                              0.0311861
TotalEnvSteps                            61732
policy/Entropy                               1.7227
policy/KL                                    0.00673758
policy/KLBefore                              0
policy/LossAfter                            -0.0282832
policy/LossBefore                           -2.8271e-09
policy/Perplexity                            5.59963
policy/dLoss                                 0.0282832
---------------------------------------  --------------
2021-06-04 13:25:11 | [train_policy] epoch #61 | Obtaining samples for iteration 61...
2021-06-04 13:25:12 | [train_policy] epoch #61 | Logging diagnostics...
2021-06-04 13:25:12 | [train_policy] epoch #61 | Optimizing policy...
2021-06-04 13:25:12 | [train_policy] epoch #61 | Computing loss before
2021-06-04 13:25:12 | [train_policy] epoch #61 | Computing KL before
2021-06-04 13:25:12 | [train_policy] epoch #61 | Optimizing
2021-06-04 13:25:12 | [train_policy] epoch #61 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:12 | [train_policy] epoch #61 | computing loss before
2021-06-04 13:25:12 | [train_policy] epoch #61 | computing gradient
2021-06-04 13:25:12 | [train_policy] epoch #61 | gradient computed
2021-06-04 13:25:12 | [train_policy] epoch #61 | computing descent direction
2021-06-04 13:25:12 | [train_policy] epoch #61 | descent direction computed
2021-06-04 13:25:12 | [train_policy] epoch #61 | backtrack iters: 1
2021-06-04 13:25:12 | [train_policy] epoch #61 | optimization finished
2021-06-04 13:25:12 | [train_policy] epoch #61 | Computing KL after
2021-06-04 13:25:12 | [train_policy] epoch #61 | Computing loss after
2021-06-04 13:25:12 | [train_policy] epoch #61 | Fitting baseline...
2021-06-04 13:25:12 | [train_policy] epoch #61 | Saving snapshot...
2021-06-04 13:25:12 | [train_policy] epoch #61 | Saved
2021-06-04 13:25:12 | [train_policy] epoch #61 | Time 52.50 s
2021-06-04 13:25:12 | [train_policy] epoch #61 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.287006
Evaluation/AverageDiscountedReturn         -71.1811
Evaluation/AverageReturn                   -71.1811
Evaluation/CompletionRate                    0
Evaluation/Iteration                        61
Evaluation/MaxReturn                       -38.3289
Evaluation/MinReturn                     -2065.68
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.161
Extras/EpisodeRewardMean                   -69.4647
LinearFeatureBaseline/ExplainedVariance      0.00948439
PolicyExecTime                               0.226258
ProcessExecTime                              0.0315619
TotalEnvSteps                            62744
policy/Entropy                               1.71743
policy/KL                                    0.00634915
policy/KLBefore                              0
policy/LossAfter                            -0.021809
policy/LossBefore                           -1.41355e-09
policy/Perplexity                            5.57022
policy/dLoss                                 0.021809
---------------------------------------  ---------------
2021-06-04 13:25:12 | [train_policy] epoch #62 | Obtaining samples for iteration 62...
2021-06-04 13:25:13 | [train_policy] epoch #62 | Logging diagnostics...
2021-06-04 13:25:13 | [train_policy] epoch #62 | Optimizing policy...
2021-06-04 13:25:13 | [train_policy] epoch #62 | Computing loss before
2021-06-04 13:25:13 | [train_policy] epoch #62 | Computing KL before
2021-06-04 13:25:13 | [train_policy] epoch #62 | Optimizing
2021-06-04 13:25:13 | [train_policy] epoch #62 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:13 | [train_policy] epoch #62 | computing loss before
2021-06-04 13:25:13 | [train_policy] epoch #62 | computing gradient
2021-06-04 13:25:13 | [train_policy] epoch #62 | gradient computed
2021-06-04 13:25:13 | [train_policy] epoch #62 | computing descent direction
2021-06-04 13:25:13 | [train_policy] epoch #62 | descent direction computed
2021-06-04 13:25:13 | [train_policy] epoch #62 | backtrack iters: 0
2021-06-04 13:25:13 | [train_policy] epoch #62 | optimization finished
2021-06-04 13:25:13 | [train_policy] epoch #62 | Computing KL after
2021-06-04 13:25:13 | [train_policy] epoch #62 | Computing loss after
2021-06-04 13:25:13 | [train_policy] epoch #62 | Fitting baseline...
2021-06-04 13:25:13 | [train_policy] epoch #62 | Saving snapshot...
2021-06-04 13:25:13 | [train_policy] epoch #62 | Saved
2021-06-04 13:25:13 | [train_policy] epoch #62 | Time 53.30 s
2021-06-04 13:25:13 | [train_policy] epoch #62 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.287447
Evaluation/AverageDiscountedReturn         -50.3821
Evaluation/AverageReturn                   -50.3821
Evaluation/CompletionRate                    0
Evaluation/Iteration                        62
Evaluation/MaxReturn                       -36.9847
Evaluation/MinReturn                      -155.581
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        12.5129
Extras/EpisodeRewardMean                   -49.7807
LinearFeatureBaseline/ExplainedVariance    -19.4057
PolicyExecTime                               0.229299
ProcessExecTime                              0.0315871
TotalEnvSteps                            63756
policy/Entropy                               1.67388
policy/KL                                    0.00993922
policy/KLBefore                              0
policy/LossAfter                            -0.022234
policy/LossBefore                           -9.42366e-10
policy/Perplexity                            5.33282
policy/dLoss                                 0.022234
---------------------------------------  ---------------
2021-06-04 13:25:13 | [train_policy] epoch #63 | Obtaining samples for iteration 63...
2021-06-04 13:25:14 | [train_policy] epoch #63 | Logging diagnostics...
2021-06-04 13:25:14 | [train_policy] epoch #63 | Optimizing policy...
2021-06-04 13:25:14 | [train_policy] epoch #63 | Computing loss before
2021-06-04 13:25:14 | [train_policy] epoch #63 | Computing KL before
2021-06-04 13:25:14 | [train_policy] epoch #63 | Optimizing
2021-06-04 13:25:14 | [train_policy] epoch #63 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:14 | [train_policy] epoch #63 | computing loss before
2021-06-04 13:25:14 | [train_policy] epoch #63 | computing gradient
2021-06-04 13:25:14 | [train_policy] epoch #63 | gradient computed
2021-06-04 13:25:14 | [train_policy] epoch #63 | computing descent direction
2021-06-04 13:25:14 | [train_policy] epoch #63 | descent direction computed
2021-06-04 13:25:14 | [train_policy] epoch #63 | backtrack iters: 0
2021-06-04 13:25:14 | [train_policy] epoch #63 | optimization finished
2021-06-04 13:25:14 | [train_policy] epoch #63 | Computing KL after
2021-06-04 13:25:14 | [train_policy] epoch #63 | Computing loss after
2021-06-04 13:25:14 | [train_policy] epoch #63 | Fitting baseline...
2021-06-04 13:25:14 | [train_policy] epoch #63 | Saving snapshot...
2021-06-04 13:25:14 | [train_policy] epoch #63 | Saved
2021-06-04 13:25:14 | [train_policy] epoch #63 | Time 54.09 s
2021-06-04 13:25:14 | [train_policy] epoch #63 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.289362
Evaluation/AverageDiscountedReturn         -50.3206
Evaluation/AverageReturn                   -50.3206
Evaluation/CompletionRate                    0
Evaluation/Iteration                        63
Evaluation/MaxReturn                       -39.5049
Evaluation/MinReturn                       -67.8315
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.81338
Extras/EpisodeRewardMean                   -50.146
LinearFeatureBaseline/ExplainedVariance      0.94887
PolicyExecTime                               0.222915
ProcessExecTime                              0.0318632
TotalEnvSteps                            64768
policy/Entropy                               1.68559
policy/KL                                    0.00992086
policy/KLBefore                              0
policy/LossAfter                            -0.024427
policy/LossBefore                            9.89484e-09
policy/Perplexity                            5.39565
policy/dLoss                                 0.024427
---------------------------------------  ---------------
2021-06-04 13:25:14 | [train_policy] epoch #64 | Obtaining samples for iteration 64...
2021-06-04 13:25:14 | [train_policy] epoch #64 | Logging diagnostics...
2021-06-04 13:25:14 | [train_policy] epoch #64 | Optimizing policy...
2021-06-04 13:25:14 | [train_policy] epoch #64 | Computing loss before
2021-06-04 13:25:14 | [train_policy] epoch #64 | Computing KL before
2021-06-04 13:25:14 | [train_policy] epoch #64 | Optimizing
2021-06-04 13:25:14 | [train_policy] epoch #64 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:14 | [train_policy] epoch #64 | computing loss before
2021-06-04 13:25:14 | [train_policy] epoch #64 | computing gradient
2021-06-04 13:25:14 | [train_policy] epoch #64 | gradient computed
2021-06-04 13:25:14 | [train_policy] epoch #64 | computing descent direction
2021-06-04 13:25:14 | [train_policy] epoch #64 | descent direction computed
2021-06-04 13:25:14 | [train_policy] epoch #64 | backtrack iters: 1
2021-06-04 13:25:14 | [train_policy] epoch #64 | optimization finished
2021-06-04 13:25:14 | [train_policy] epoch #64 | Computing KL after
2021-06-04 13:25:14 | [train_policy] epoch #64 | Computing loss after
2021-06-04 13:25:14 | [train_policy] epoch #64 | Fitting baseline...
2021-06-04 13:25:14 | [train_policy] epoch #64 | Saving snapshot...
2021-06-04 13:25:14 | [train_policy] epoch #64 | Saved
2021-06-04 13:25:14 | [train_policy] epoch #64 | Time 54.87 s
2021-06-04 13:25:14 | [train_policy] epoch #64 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.285214
Evaluation/AverageDiscountedReturn         -72.7114
Evaluation/AverageReturn                   -72.7114
Evaluation/CompletionRate                    0
Evaluation/Iteration                        64
Evaluation/MaxReturn                       -37.5104
Evaluation/MinReturn                     -2040.31
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       206.582
Extras/EpisodeRewardMean                   -71.1629
LinearFeatureBaseline/ExplainedVariance      0.0131533
PolicyExecTime                               0.2199
ProcessExecTime                              0.0313671
TotalEnvSteps                            65780
policy/Entropy                               1.6123
policy/KL                                    0.00665039
policy/KLBefore                              0
policy/LossAfter                            -0.0130114
policy/LossBefore                            2.92133e-08
policy/Perplexity                            5.01431
policy/dLoss                                 0.0130115
---------------------------------------  ---------------
2021-06-04 13:25:14 | [train_policy] epoch #65 | Obtaining samples for iteration 65...
2021-06-04 13:25:15 | [train_policy] epoch #65 | Logging diagnostics...
2021-06-04 13:25:15 | [train_policy] epoch #65 | Optimizing policy...
2021-06-04 13:25:15 | [train_policy] epoch #65 | Computing loss before
2021-06-04 13:25:15 | [train_policy] epoch #65 | Computing KL before
2021-06-04 13:25:15 | [train_policy] epoch #65 | Optimizing
2021-06-04 13:25:15 | [train_policy] epoch #65 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:15 | [train_policy] epoch #65 | computing loss before
2021-06-04 13:25:15 | [train_policy] epoch #65 | computing gradient
2021-06-04 13:25:15 | [train_policy] epoch #65 | gradient computed
2021-06-04 13:25:15 | [train_policy] epoch #65 | computing descent direction
2021-06-04 13:25:15 | [train_policy] epoch #65 | descent direction computed
2021-06-04 13:25:15 | [train_policy] epoch #65 | backtrack iters: 0
2021-06-04 13:25:15 | [train_policy] epoch #65 | optimization finished
2021-06-04 13:25:15 | [train_policy] epoch #65 | Computing KL after
2021-06-04 13:25:15 | [train_policy] epoch #65 | Computing loss after
2021-06-04 13:25:15 | [train_policy] epoch #65 | Fitting baseline...
2021-06-04 13:25:15 | [train_policy] epoch #65 | Saving snapshot...
2021-06-04 13:25:15 | [train_policy] epoch #65 | Saved
2021-06-04 13:25:15 | [train_policy] epoch #65 | Time 55.65 s
2021-06-04 13:25:15 | [train_policy] epoch #65 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286135
Evaluation/AverageDiscountedReturn         -50.2803
Evaluation/AverageReturn                   -50.2803
Evaluation/CompletionRate                    0
Evaluation/Iteration                        65
Evaluation/MaxReturn                       -35.2763
Evaluation/MinReturn                       -88.9744
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.22922
Extras/EpisodeRewardMean                   -70.3427
LinearFeatureBaseline/ExplainedVariance     -5.31385
PolicyExecTime                               0.222628
ProcessExecTime                              0.0313916
TotalEnvSteps                            66792
policy/Entropy                               1.61155
policy/KL                                    0.00962137
policy/KLBefore                              0
policy/LossAfter                            -0.0289403
policy/LossBefore                            2.92133e-08
policy/Perplexity                            5.01058
policy/dLoss                                 0.0289404
---------------------------------------  ---------------
2021-06-04 13:25:15 | [train_policy] epoch #66 | Obtaining samples for iteration 66...
2021-06-04 13:25:16 | [train_policy] epoch #66 | Logging diagnostics...
2021-06-04 13:25:16 | [train_policy] epoch #66 | Optimizing policy...
2021-06-04 13:25:16 | [train_policy] epoch #66 | Computing loss before
2021-06-04 13:25:16 | [train_policy] epoch #66 | Computing KL before
2021-06-04 13:25:16 | [train_policy] epoch #66 | Optimizing
2021-06-04 13:25:16 | [train_policy] epoch #66 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:16 | [train_policy] epoch #66 | computing loss before
2021-06-04 13:25:16 | [train_policy] epoch #66 | computing gradient
2021-06-04 13:25:16 | [train_policy] epoch #66 | gradient computed
2021-06-04 13:25:16 | [train_policy] epoch #66 | computing descent direction
2021-06-04 13:25:16 | [train_policy] epoch #66 | descent direction computed
2021-06-04 13:25:16 | [train_policy] epoch #66 | backtrack iters: 1
2021-06-04 13:25:16 | [train_policy] epoch #66 | optimization finished
2021-06-04 13:25:16 | [train_policy] epoch #66 | Computing KL after
2021-06-04 13:25:16 | [train_policy] epoch #66 | Computing loss after
2021-06-04 13:25:16 | [train_policy] epoch #66 | Fitting baseline...
2021-06-04 13:25:16 | [train_policy] epoch #66 | Saving snapshot...
2021-06-04 13:25:16 | [train_policy] epoch #66 | Saved
2021-06-04 13:25:16 | [train_policy] epoch #66 | Time 56.45 s
2021-06-04 13:25:16 | [train_policy] epoch #66 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.287261
Evaluation/AverageDiscountedReturn         -50.7362
Evaluation/AverageReturn                   -50.7362
Evaluation/CompletionRate                    0
Evaluation/Iteration                        66
Evaluation/MaxReturn                       -39.0056
Evaluation/MinReturn                       -83.5375
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.0498
Extras/EpisodeRewardMean                   -50.6601
LinearFeatureBaseline/ExplainedVariance      0.900971
PolicyExecTime                               0.228887
ProcessExecTime                              0.032057
TotalEnvSteps                            67804
policy/Entropy                               1.53856
policy/KL                                    0.00674286
policy/KLBefore                              0
policy/LossAfter                            -0.0173445
policy/LossBefore                           -1.41355e-08
policy/Perplexity                            4.65786
policy/dLoss                                 0.0173445
---------------------------------------  ---------------
2021-06-04 13:25:16 | [train_policy] epoch #67 | Obtaining samples for iteration 67...
2021-06-04 13:25:17 | [train_policy] epoch #67 | Logging diagnostics...
2021-06-04 13:25:17 | [train_policy] epoch #67 | Optimizing policy...
2021-06-04 13:25:17 | [train_policy] epoch #67 | Computing loss before
2021-06-04 13:25:17 | [train_policy] epoch #67 | Computing KL before
2021-06-04 13:25:17 | [train_policy] epoch #67 | Optimizing
2021-06-04 13:25:17 | [train_policy] epoch #67 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:17 | [train_policy] epoch #67 | computing loss before
2021-06-04 13:25:17 | [train_policy] epoch #67 | computing gradient
2021-06-04 13:25:17 | [train_policy] epoch #67 | gradient computed
2021-06-04 13:25:17 | [train_policy] epoch #67 | computing descent direction
2021-06-04 13:25:17 | [train_policy] epoch #67 | descent direction computed
2021-06-04 13:25:17 | [train_policy] epoch #67 | backtrack iters: 0
2021-06-04 13:25:17 | [train_policy] epoch #67 | optimization finished
2021-06-04 13:25:17 | [train_policy] epoch #67 | Computing KL after
2021-06-04 13:25:17 | [train_policy] epoch #67 | Computing loss after
2021-06-04 13:25:17 | [train_policy] epoch #67 | Fitting baseline...
2021-06-04 13:25:17 | [train_policy] epoch #67 | Saving snapshot...
2021-06-04 13:25:17 | [train_policy] epoch #67 | Saved
2021-06-04 13:25:17 | [train_policy] epoch #67 | Time 57.23 s
2021-06-04 13:25:17 | [train_policy] epoch #67 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.290569
Evaluation/AverageDiscountedReturn         -49.5454
Evaluation/AverageReturn                   -49.5454
Evaluation/CompletionRate                    0
Evaluation/Iteration                        67
Evaluation/MaxReturn                       -35.7078
Evaluation/MinReturn                       -73.0099
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.23708
Extras/EpisodeRewardMean                   -49.6554
LinearFeatureBaseline/ExplainedVariance      0.929609
PolicyExecTime                               0.217881
ProcessExecTime                              0.0320792
TotalEnvSteps                            68816
policy/Entropy                               1.5042
policy/KL                                    0.00995473
policy/KLBefore                              0
policy/LossAfter                            -0.0216148
policy/LossBefore                           -4.71183e-09
policy/Perplexity                            4.50055
policy/dLoss                                 0.0216148
---------------------------------------  ---------------
2021-06-04 13:25:17 | [train_policy] epoch #68 | Obtaining samples for iteration 68...
2021-06-04 13:25:17 | [train_policy] epoch #68 | Logging diagnostics...
2021-06-04 13:25:17 | [train_policy] epoch #68 | Optimizing policy...
2021-06-04 13:25:17 | [train_policy] epoch #68 | Computing loss before
2021-06-04 13:25:17 | [train_policy] epoch #68 | Computing KL before
2021-06-04 13:25:18 | [train_policy] epoch #68 | Optimizing
2021-06-04 13:25:18 | [train_policy] epoch #68 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:18 | [train_policy] epoch #68 | computing loss before
2021-06-04 13:25:18 | [train_policy] epoch #68 | computing gradient
2021-06-04 13:25:18 | [train_policy] epoch #68 | gradient computed
2021-06-04 13:25:18 | [train_policy] epoch #68 | computing descent direction
2021-06-04 13:25:18 | [train_policy] epoch #68 | descent direction computed
2021-06-04 13:25:18 | [train_policy] epoch #68 | backtrack iters: 1
2021-06-04 13:25:18 | [train_policy] epoch #68 | optimization finished
2021-06-04 13:25:18 | [train_policy] epoch #68 | Computing KL after
2021-06-04 13:25:18 | [train_policy] epoch #68 | Computing loss after
2021-06-04 13:25:18 | [train_policy] epoch #68 | Fitting baseline...
2021-06-04 13:25:18 | [train_policy] epoch #68 | Saving snapshot...
2021-06-04 13:25:18 | [train_policy] epoch #68 | Saved
2021-06-04 13:25:18 | [train_policy] epoch #68 | Time 58.04 s
2021-06-04 13:25:18 | [train_policy] epoch #68 | EpochTime 0.79 s
---------------------------------------  --------------
EnvExecTime                                  0.288894
Evaluation/AverageDiscountedReturn         -51.0315
Evaluation/AverageReturn                   -51.0315
Evaluation/CompletionRate                    0
Evaluation/Iteration                        68
Evaluation/MaxReturn                       -35.45
Evaluation/MinReturn                      -156.538
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        12.9336
Extras/EpisodeRewardMean                   -50.867
LinearFeatureBaseline/ExplainedVariance      0.836207
PolicyExecTime                               0.239797
ProcessExecTime                              0.0317166
TotalEnvSteps                            69828
policy/Entropy                               1.44599
policy/KL                                    0.00688838
policy/KLBefore                              0
policy/LossAfter                            -0.0223138
policy/LossBefore                           -0
policy/Perplexity                            4.24605
policy/dLoss                                 0.0223138
---------------------------------------  --------------
2021-06-04 13:25:18 | [train_policy] epoch #69 | Obtaining samples for iteration 69...
2021-06-04 13:25:18 | [train_policy] epoch #69 | Logging diagnostics...
2021-06-04 13:25:18 | [train_policy] epoch #69 | Optimizing policy...
2021-06-04 13:25:18 | [train_policy] epoch #69 | Computing loss before
2021-06-04 13:25:18 | [train_policy] epoch #69 | Computing KL before
2021-06-04 13:25:18 | [train_policy] epoch #69 | Optimizing
2021-06-04 13:25:18 | [train_policy] epoch #69 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:18 | [train_policy] epoch #69 | computing loss before
2021-06-04 13:25:18 | [train_policy] epoch #69 | computing gradient
2021-06-04 13:25:18 | [train_policy] epoch #69 | gradient computed
2021-06-04 13:25:18 | [train_policy] epoch #69 | computing descent direction
2021-06-04 13:25:18 | [train_policy] epoch #69 | descent direction computed
2021-06-04 13:25:18 | [train_policy] epoch #69 | backtrack iters: 1
2021-06-04 13:25:18 | [train_policy] epoch #69 | optimization finished
2021-06-04 13:25:18 | [train_policy] epoch #69 | Computing KL after
2021-06-04 13:25:18 | [train_policy] epoch #69 | Computing loss after
2021-06-04 13:25:18 | [train_policy] epoch #69 | Fitting baseline...
2021-06-04 13:25:18 | [train_policy] epoch #69 | Saving snapshot...
2021-06-04 13:25:18 | [train_policy] epoch #69 | Saved
2021-06-04 13:25:18 | [train_policy] epoch #69 | Time 58.84 s
2021-06-04 13:25:18 | [train_policy] epoch #69 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.287178
Evaluation/AverageDiscountedReturn         -48.4566
Evaluation/AverageReturn                   -48.4566
Evaluation/CompletionRate                    0
Evaluation/Iteration                        69
Evaluation/MaxReturn                       -38.9495
Evaluation/MinReturn                       -75.1747
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.43801
Extras/EpisodeRewardMean                   -48.6328
LinearFeatureBaseline/ExplainedVariance      0.934278
PolicyExecTime                               0.227335
ProcessExecTime                              0.0315452
TotalEnvSteps                            70840
policy/Entropy                               1.40714
policy/KL                                    0.00652661
policy/KLBefore                              0
policy/LossAfter                            -0.0178227
policy/LossBefore                            7.06774e-10
policy/Perplexity                            4.08427
policy/dLoss                                 0.0178227
---------------------------------------  ---------------
2021-06-04 13:25:18 | [train_policy] epoch #70 | Obtaining samples for iteration 70...
2021-06-04 13:25:19 | [train_policy] epoch #70 | Logging diagnostics...
2021-06-04 13:25:19 | [train_policy] epoch #70 | Optimizing policy...
2021-06-04 13:25:19 | [train_policy] epoch #70 | Computing loss before
2021-06-04 13:25:19 | [train_policy] epoch #70 | Computing KL before
2021-06-04 13:25:19 | [train_policy] epoch #70 | Optimizing
2021-06-04 13:25:19 | [train_policy] epoch #70 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:19 | [train_policy] epoch #70 | computing loss before
2021-06-04 13:25:19 | [train_policy] epoch #70 | computing gradient
2021-06-04 13:25:19 | [train_policy] epoch #70 | gradient computed
2021-06-04 13:25:19 | [train_policy] epoch #70 | computing descent direction
2021-06-04 13:25:19 | [train_policy] epoch #70 | descent direction computed
2021-06-04 13:25:19 | [train_policy] epoch #70 | backtrack iters: 1
2021-06-04 13:25:19 | [train_policy] epoch #70 | optimization finished
2021-06-04 13:25:19 | [train_policy] epoch #70 | Computing KL after
2021-06-04 13:25:19 | [train_policy] epoch #70 | Computing loss after
2021-06-04 13:25:19 | [train_policy] epoch #70 | Fitting baseline...
2021-06-04 13:25:19 | [train_policy] epoch #70 | Saving snapshot...
2021-06-04 13:25:19 | [train_policy] epoch #70 | Saved
2021-06-04 13:25:19 | [train_policy] epoch #70 | Time 59.63 s
2021-06-04 13:25:19 | [train_policy] epoch #70 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285313
Evaluation/AverageDiscountedReturn         -49.0475
Evaluation/AverageReturn                   -49.0475
Evaluation/CompletionRate                    0
Evaluation/Iteration                        70
Evaluation/MaxReturn                       -37.5414
Evaluation/MinReturn                       -78.1167
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.55488
Extras/EpisodeRewardMean                   -49.0658
LinearFeatureBaseline/ExplainedVariance      0.938823
PolicyExecTime                               0.225379
ProcessExecTime                              0.031703
TotalEnvSteps                            71852
policy/Entropy                               1.37821
policy/KL                                    0.00657617
policy/KLBefore                              0
policy/LossAfter                            -0.0202055
policy/LossBefore                            4.47624e-09
policy/Perplexity                            3.9678
policy/dLoss                                 0.0202055
---------------------------------------  ---------------
2021-06-04 13:25:19 | [train_policy] epoch #71 | Obtaining samples for iteration 71...
2021-06-04 13:25:20 | [train_policy] epoch #71 | Logging diagnostics...
2021-06-04 13:25:20 | [train_policy] epoch #71 | Optimizing policy...
2021-06-04 13:25:20 | [train_policy] epoch #71 | Computing loss before
2021-06-04 13:25:20 | [train_policy] epoch #71 | Computing KL before
2021-06-04 13:25:20 | [train_policy] epoch #71 | Optimizing
2021-06-04 13:25:20 | [train_policy] epoch #71 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:20 | [train_policy] epoch #71 | computing loss before
2021-06-04 13:25:20 | [train_policy] epoch #71 | computing gradient
2021-06-04 13:25:20 | [train_policy] epoch #71 | gradient computed
2021-06-04 13:25:20 | [train_policy] epoch #71 | computing descent direction
2021-06-04 13:25:20 | [train_policy] epoch #71 | descent direction computed
2021-06-04 13:25:20 | [train_policy] epoch #71 | backtrack iters: 1
2021-06-04 13:25:20 | [train_policy] epoch #71 | optimization finished
2021-06-04 13:25:20 | [train_policy] epoch #71 | Computing KL after
2021-06-04 13:25:20 | [train_policy] epoch #71 | Computing loss after
2021-06-04 13:25:20 | [train_policy] epoch #71 | Fitting baseline...
2021-06-04 13:25:20 | [train_policy] epoch #71 | Saving snapshot...
2021-06-04 13:25:20 | [train_policy] epoch #71 | Saved
2021-06-04 13:25:20 | [train_policy] epoch #71 | Time 60.46 s
2021-06-04 13:25:20 | [train_policy] epoch #71 | EpochTime 0.81 s
---------------------------------------  ---------------
EnvExecTime                                  0.300559
Evaluation/AverageDiscountedReturn         -49.7286
Evaluation/AverageReturn                   -49.7286
Evaluation/CompletionRate                    0
Evaluation/Iteration                        71
Evaluation/MaxReturn                       -38.1792
Evaluation/MinReturn                       -68.6388
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.93432
Extras/EpisodeRewardMean                   -49.2601
LinearFeatureBaseline/ExplainedVariance      0.940198
PolicyExecTime                               0.248723
ProcessExecTime                              0.0332077
TotalEnvSteps                            72864
policy/Entropy                               1.35795
policy/KL                                    0.00661616
policy/KLBefore                              0
policy/LossAfter                            -0.0211843
policy/LossBefore                           -5.18301e-09
policy/Perplexity                            3.88821
policy/dLoss                                 0.0211843
---------------------------------------  ---------------
2021-06-04 13:25:20 | [train_policy] epoch #72 | Obtaining samples for iteration 72...
2021-06-04 13:25:21 | [train_policy] epoch #72 | Logging diagnostics...
2021-06-04 13:25:21 | [train_policy] epoch #72 | Optimizing policy...
2021-06-04 13:25:21 | [train_policy] epoch #72 | Computing loss before
2021-06-04 13:25:21 | [train_policy] epoch #72 | Computing KL before
2021-06-04 13:25:21 | [train_policy] epoch #72 | Optimizing
2021-06-04 13:25:21 | [train_policy] epoch #72 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:21 | [train_policy] epoch #72 | computing loss before
2021-06-04 13:25:21 | [train_policy] epoch #72 | computing gradient
2021-06-04 13:25:21 | [train_policy] epoch #72 | gradient computed
2021-06-04 13:25:21 | [train_policy] epoch #72 | computing descent direction
2021-06-04 13:25:21 | [train_policy] epoch #72 | descent direction computed
2021-06-04 13:25:21 | [train_policy] epoch #72 | backtrack iters: 1
2021-06-04 13:25:21 | [train_policy] epoch #72 | optimization finished
2021-06-04 13:25:21 | [train_policy] epoch #72 | Computing KL after
2021-06-04 13:25:21 | [train_policy] epoch #72 | Computing loss after
2021-06-04 13:25:21 | [train_policy] epoch #72 | Fitting baseline...
2021-06-04 13:25:21 | [train_policy] epoch #72 | Saving snapshot...
2021-06-04 13:25:21 | [train_policy] epoch #72 | Saved
2021-06-04 13:25:21 | [train_policy] epoch #72 | Time 61.28 s
2021-06-04 13:25:21 | [train_policy] epoch #72 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.291821
Evaluation/AverageDiscountedReturn         -71.6838
Evaluation/AverageReturn                   -71.6838
Evaluation/CompletionRate                    0
Evaluation/Iteration                        72
Evaluation/MaxReturn                       -37.8826
Evaluation/MinReturn                     -2062.81
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.843
Extras/EpisodeRewardMean                   -69.7126
LinearFeatureBaseline/ExplainedVariance      0.0113047
PolicyExecTime                               0.238169
ProcessExecTime                              0.0319383
TotalEnvSteps                            73876
policy/Entropy                               1.34605
policy/KL                                    0.00633825
policy/KLBefore                              0
policy/LossAfter                            -0.0178267
policy/LossBefore                            2.35591e-09
policy/Perplexity                            3.84221
policy/dLoss                                 0.0178267
---------------------------------------  ---------------
2021-06-04 13:25:21 | [train_policy] epoch #73 | Obtaining samples for iteration 73...
2021-06-04 13:25:22 | [train_policy] epoch #73 | Logging diagnostics...
2021-06-04 13:25:22 | [train_policy] epoch #73 | Optimizing policy...
2021-06-04 13:25:22 | [train_policy] epoch #73 | Computing loss before
2021-06-04 13:25:22 | [train_policy] epoch #73 | Computing KL before
2021-06-04 13:25:22 | [train_policy] epoch #73 | Optimizing
2021-06-04 13:25:22 | [train_policy] epoch #73 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:22 | [train_policy] epoch #73 | computing loss before
2021-06-04 13:25:22 | [train_policy] epoch #73 | computing gradient
2021-06-04 13:25:22 | [train_policy] epoch #73 | gradient computed
2021-06-04 13:25:22 | [train_policy] epoch #73 | computing descent direction
2021-06-04 13:25:22 | [train_policy] epoch #73 | descent direction computed
2021-06-04 13:25:22 | [train_policy] epoch #73 | backtrack iters: 1
2021-06-04 13:25:22 | [train_policy] epoch #73 | optimization finished
2021-06-04 13:25:22 | [train_policy] epoch #73 | Computing KL after
2021-06-04 13:25:22 | [train_policy] epoch #73 | Computing loss after
2021-06-04 13:25:22 | [train_policy] epoch #73 | Fitting baseline...
2021-06-04 13:25:22 | [train_policy] epoch #73 | Saving snapshot...
2021-06-04 13:25:22 | [train_policy] epoch #73 | Saved
2021-06-04 13:25:22 | [train_policy] epoch #73 | Time 62.06 s
2021-06-04 13:25:22 | [train_policy] epoch #73 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.284755
Evaluation/AverageDiscountedReturn         -49.3651
Evaluation/AverageReturn                   -49.3651
Evaluation/CompletionRate                    0
Evaluation/Iteration                        73
Evaluation/MaxReturn                       -39.0232
Evaluation/MinReturn                       -83.4032
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.89602
Extras/EpisodeRewardMean                   -49.3703
LinearFeatureBaseline/ExplainedVariance    -13.9946
PolicyExecTime                               0.2161
ProcessExecTime                              0.0312719
TotalEnvSteps                            74888
policy/Entropy                               1.35504
policy/KL                                    0.00732592
policy/KLBefore                              0
policy/LossAfter                            -0.0203218
policy/LossBefore                            3.62811e-08
policy/Perplexity                            3.87691
policy/dLoss                                 0.0203219
---------------------------------------  ---------------
2021-06-04 13:25:22 | [train_policy] epoch #74 | Obtaining samples for iteration 74...
2021-06-04 13:25:22 | [train_policy] epoch #74 | Logging diagnostics...
2021-06-04 13:25:22 | [train_policy] epoch #74 | Optimizing policy...
2021-06-04 13:25:22 | [train_policy] epoch #74 | Computing loss before
2021-06-04 13:25:22 | [train_policy] epoch #74 | Computing KL before
2021-06-04 13:25:22 | [train_policy] epoch #74 | Optimizing
2021-06-04 13:25:22 | [train_policy] epoch #74 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:22 | [train_policy] epoch #74 | computing loss before
2021-06-04 13:25:22 | [train_policy] epoch #74 | computing gradient
2021-06-04 13:25:22 | [train_policy] epoch #74 | gradient computed
2021-06-04 13:25:22 | [train_policy] epoch #74 | computing descent direction
2021-06-04 13:25:22 | [train_policy] epoch #74 | descent direction computed
2021-06-04 13:25:22 | [train_policy] epoch #74 | backtrack iters: 1
2021-06-04 13:25:22 | [train_policy] epoch #74 | optimization finished
2021-06-04 13:25:22 | [train_policy] epoch #74 | Computing KL after
2021-06-04 13:25:22 | [train_policy] epoch #74 | Computing loss after
2021-06-04 13:25:22 | [train_policy] epoch #74 | Fitting baseline...
2021-06-04 13:25:22 | [train_policy] epoch #74 | Saving snapshot...
2021-06-04 13:25:22 | [train_policy] epoch #74 | Saved
2021-06-04 13:25:22 | [train_policy] epoch #74 | Time 62.87 s
2021-06-04 13:25:22 | [train_policy] epoch #74 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.289695
Evaluation/AverageDiscountedReturn         -48.071
Evaluation/AverageReturn                   -48.071
Evaluation/CompletionRate                    0
Evaluation/Iteration                        74
Evaluation/MaxReturn                       -36.3175
Evaluation/MinReturn                       -66.7551
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.81514
Extras/EpisodeRewardMean                   -47.925
LinearFeatureBaseline/ExplainedVariance      0.959824
PolicyExecTime                               0.227889
ProcessExecTime                              0.0317924
TotalEnvSteps                            75900
policy/Entropy                               1.3193
policy/KL                                    0.00688277
policy/KLBefore                              0
policy/LossAfter                            -0.0187299
policy/LossBefore                            8.95248e-09
policy/Perplexity                            3.74081
policy/dLoss                                 0.0187299
---------------------------------------  ---------------
2021-06-04 13:25:22 | [train_policy] epoch #75 | Obtaining samples for iteration 75...
2021-06-04 13:25:23 | [train_policy] epoch #75 | Logging diagnostics...
2021-06-04 13:25:23 | [train_policy] epoch #75 | Optimizing policy...
2021-06-04 13:25:23 | [train_policy] epoch #75 | Computing loss before
2021-06-04 13:25:23 | [train_policy] epoch #75 | Computing KL before
2021-06-04 13:25:23 | [train_policy] epoch #75 | Optimizing
2021-06-04 13:25:23 | [train_policy] epoch #75 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:23 | [train_policy] epoch #75 | computing loss before
2021-06-04 13:25:23 | [train_policy] epoch #75 | computing gradient
2021-06-04 13:25:23 | [train_policy] epoch #75 | gradient computed
2021-06-04 13:25:23 | [train_policy] epoch #75 | computing descent direction
2021-06-04 13:25:23 | [train_policy] epoch #75 | descent direction computed
2021-06-04 13:25:23 | [train_policy] epoch #75 | backtrack iters: 0
2021-06-04 13:25:23 | [train_policy] epoch #75 | optimization finished
2021-06-04 13:25:23 | [train_policy] epoch #75 | Computing KL after
2021-06-04 13:25:23 | [train_policy] epoch #75 | Computing loss after
2021-06-04 13:25:23 | [train_policy] epoch #75 | Fitting baseline...
2021-06-04 13:25:23 | [train_policy] epoch #75 | Saving snapshot...
2021-06-04 13:25:23 | [train_policy] epoch #75 | Saved
2021-06-04 13:25:23 | [train_policy] epoch #75 | Time 63.67 s
2021-06-04 13:25:23 | [train_policy] epoch #75 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.285348
Evaluation/AverageDiscountedReturn         -50.647
Evaluation/AverageReturn                   -50.647
Evaluation/CompletionRate                    0
Evaluation/Iteration                        75
Evaluation/MaxReturn                       -39.1092
Evaluation/MinReturn                       -76.1205
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.05378
Extras/EpisodeRewardMean                   -50.6196
LinearFeatureBaseline/ExplainedVariance      0.921799
PolicyExecTime                               0.239614
ProcessExecTime                              0.031327
TotalEnvSteps                            76912
policy/Entropy                               1.30494
policy/KL                                    0.00945602
policy/KLBefore                              0
policy/LossAfter                            -0.024185
policy/LossBefore                           -7.06774e-09
policy/Perplexity                            3.68748
policy/dLoss                                 0.0241849
---------------------------------------  ---------------
2021-06-04 13:25:23 | [train_policy] epoch #76 | Obtaining samples for iteration 76...
2021-06-04 13:25:24 | [train_policy] epoch #76 | Logging diagnostics...
2021-06-04 13:25:24 | [train_policy] epoch #76 | Optimizing policy...
2021-06-04 13:25:24 | [train_policy] epoch #76 | Computing loss before
2021-06-04 13:25:24 | [train_policy] epoch #76 | Computing KL before
2021-06-04 13:25:24 | [train_policy] epoch #76 | Optimizing
2021-06-04 13:25:24 | [train_policy] epoch #76 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:24 | [train_policy] epoch #76 | computing loss before
2021-06-04 13:25:24 | [train_policy] epoch #76 | computing gradient
2021-06-04 13:25:24 | [train_policy] epoch #76 | gradient computed
2021-06-04 13:25:24 | [train_policy] epoch #76 | computing descent direction
2021-06-04 13:25:24 | [train_policy] epoch #76 | descent direction computed
2021-06-04 13:25:24 | [train_policy] epoch #76 | backtrack iters: 1
2021-06-04 13:25:24 | [train_policy] epoch #76 | optimization finished
2021-06-04 13:25:24 | [train_policy] epoch #76 | Computing KL after
2021-06-04 13:25:24 | [train_policy] epoch #76 | Computing loss after
2021-06-04 13:25:24 | [train_policy] epoch #76 | Fitting baseline...
2021-06-04 13:25:24 | [train_policy] epoch #76 | Saving snapshot...
2021-06-04 13:25:24 | [train_policy] epoch #76 | Saved
2021-06-04 13:25:24 | [train_policy] epoch #76 | Time 64.49 s
2021-06-04 13:25:24 | [train_policy] epoch #76 | EpochTime 0.79 s
---------------------------------------  --------------
EnvExecTime                                  0.286815
Evaluation/AverageDiscountedReturn         -48.2329
Evaluation/AverageReturn                   -48.2329
Evaluation/CompletionRate                    0
Evaluation/Iteration                        76
Evaluation/MaxReturn                       -37.6933
Evaluation/MinReturn                       -63.7557
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.12086
Extras/EpisodeRewardMean                   -48.2831
LinearFeatureBaseline/ExplainedVariance      0.938041
PolicyExecTime                               0.241505
ProcessExecTime                              0.031491
TotalEnvSteps                            77924
policy/Entropy                               1.2567
policy/KL                                    0.0065971
policy/KLBefore                              0
policy/LossAfter                            -0.0190231
policy/LossBefore                           -5.6542e-09
policy/Perplexity                            3.51381
policy/dLoss                                 0.0190231
---------------------------------------  --------------
2021-06-04 13:25:24 | [train_policy] epoch #77 | Obtaining samples for iteration 77...
2021-06-04 13:25:25 | [train_policy] epoch #77 | Logging diagnostics...
2021-06-04 13:25:25 | [train_policy] epoch #77 | Optimizing policy...
2021-06-04 13:25:25 | [train_policy] epoch #77 | Computing loss before
2021-06-04 13:25:25 | [train_policy] epoch #77 | Computing KL before
2021-06-04 13:25:25 | [train_policy] epoch #77 | Optimizing
2021-06-04 13:25:25 | [train_policy] epoch #77 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:25 | [train_policy] epoch #77 | computing loss before
2021-06-04 13:25:25 | [train_policy] epoch #77 | computing gradient
2021-06-04 13:25:25 | [train_policy] epoch #77 | gradient computed
2021-06-04 13:25:25 | [train_policy] epoch #77 | computing descent direction
2021-06-04 13:25:25 | [train_policy] epoch #77 | descent direction computed
2021-06-04 13:25:25 | [train_policy] epoch #77 | backtrack iters: 0
2021-06-04 13:25:25 | [train_policy] epoch #77 | optimization finished
2021-06-04 13:25:25 | [train_policy] epoch #77 | Computing KL after
2021-06-04 13:25:25 | [train_policy] epoch #77 | Computing loss after
2021-06-04 13:25:25 | [train_policy] epoch #77 | Fitting baseline...
2021-06-04 13:25:25 | [train_policy] epoch #77 | Saving snapshot...
2021-06-04 13:25:25 | [train_policy] epoch #77 | Saved
2021-06-04 13:25:25 | [train_policy] epoch #77 | Time 65.29 s
2021-06-04 13:25:25 | [train_policy] epoch #77 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.290644
Evaluation/AverageDiscountedReturn         -48.9388
Evaluation/AverageReturn                   -48.9388
Evaluation/CompletionRate                    0
Evaluation/Iteration                        77
Evaluation/MaxReturn                       -37.4582
Evaluation/MinReturn                       -80.5037
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.90484
Extras/EpisodeRewardMean                   -48.8692
LinearFeatureBaseline/ExplainedVariance      0.935479
PolicyExecTime                               0.238664
ProcessExecTime                              0.0319152
TotalEnvSteps                            78936
policy/Entropy                               1.25279
policy/KL                                    0.00900088
policy/KLBefore                              0
policy/LossAfter                            -0.0253507
policy/LossBefore                            9.89484e-09
policy/Perplexity                            3.50009
policy/dLoss                                 0.0253507
---------------------------------------  ---------------
2021-06-04 13:25:25 | [train_policy] epoch #78 | Obtaining samples for iteration 78...
2021-06-04 13:25:26 | [train_policy] epoch #78 | Logging diagnostics...
2021-06-04 13:25:26 | [train_policy] epoch #78 | Optimizing policy...
2021-06-04 13:25:26 | [train_policy] epoch #78 | Computing loss before
2021-06-04 13:25:26 | [train_policy] epoch #78 | Computing KL before
2021-06-04 13:25:26 | [train_policy] epoch #78 | Optimizing
2021-06-04 13:25:26 | [train_policy] epoch #78 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:26 | [train_policy] epoch #78 | computing loss before
2021-06-04 13:25:26 | [train_policy] epoch #78 | computing gradient
2021-06-04 13:25:26 | [train_policy] epoch #78 | gradient computed
2021-06-04 13:25:26 | [train_policy] epoch #78 | computing descent direction
2021-06-04 13:25:26 | [train_policy] epoch #78 | descent direction computed
2021-06-04 13:25:26 | [train_policy] epoch #78 | backtrack iters: 0
2021-06-04 13:25:26 | [train_policy] epoch #78 | optimization finished
2021-06-04 13:25:26 | [train_policy] epoch #78 | Computing KL after
2021-06-04 13:25:26 | [train_policy] epoch #78 | Computing loss after
2021-06-04 13:25:26 | [train_policy] epoch #78 | Fitting baseline...
2021-06-04 13:25:26 | [train_policy] epoch #78 | Saving snapshot...
2021-06-04 13:25:26 | [train_policy] epoch #78 | Saved
2021-06-04 13:25:26 | [train_policy] epoch #78 | Time 66.08 s
2021-06-04 13:25:26 | [train_policy] epoch #78 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286147
Evaluation/AverageDiscountedReturn         -48.8277
Evaluation/AverageReturn                   -48.8277
Evaluation/CompletionRate                    0
Evaluation/Iteration                        78
Evaluation/MaxReturn                       -37.4141
Evaluation/MinReturn                       -90.9943
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.17807
Extras/EpisodeRewardMean                   -48.798
LinearFeatureBaseline/ExplainedVariance      0.872543
PolicyExecTime                               0.225747
ProcessExecTime                              0.0313778
TotalEnvSteps                            79948
policy/Entropy                               1.2618
policy/KL                                    0.00865924
policy/KLBefore                              0
policy/LossAfter                            -0.0188579
policy/LossBefore                            1.88473e-09
policy/Perplexity                            3.53176
policy/dLoss                                 0.0188579
---------------------------------------  ---------------
2021-06-04 13:25:26 | [train_policy] epoch #79 | Obtaining samples for iteration 79...
2021-06-04 13:25:26 | [train_policy] epoch #79 | Logging diagnostics...
2021-06-04 13:25:26 | [train_policy] epoch #79 | Optimizing policy...
2021-06-04 13:25:26 | [train_policy] epoch #79 | Computing loss before
2021-06-04 13:25:26 | [train_policy] epoch #79 | Computing KL before
2021-06-04 13:25:26 | [train_policy] epoch #79 | Optimizing
2021-06-04 13:25:26 | [train_policy] epoch #79 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:26 | [train_policy] epoch #79 | computing loss before
2021-06-04 13:25:26 | [train_policy] epoch #79 | computing gradient
2021-06-04 13:25:26 | [train_policy] epoch #79 | gradient computed
2021-06-04 13:25:26 | [train_policy] epoch #79 | computing descent direction
2021-06-04 13:25:26 | [train_policy] epoch #79 | descent direction computed
2021-06-04 13:25:26 | [train_policy] epoch #79 | backtrack iters: 1
2021-06-04 13:25:26 | [train_policy] epoch #79 | optimization finished
2021-06-04 13:25:26 | [train_policy] epoch #79 | Computing KL after
2021-06-04 13:25:26 | [train_policy] epoch #79 | Computing loss after
2021-06-04 13:25:26 | [train_policy] epoch #79 | Fitting baseline...
2021-06-04 13:25:26 | [train_policy] epoch #79 | Saving snapshot...
2021-06-04 13:25:26 | [train_policy] epoch #79 | Saved
2021-06-04 13:25:26 | [train_policy] epoch #79 | Time 66.89 s
2021-06-04 13:25:26 | [train_policy] epoch #79 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.289234
Evaluation/AverageDiscountedReturn         -69.1847
Evaluation/AverageReturn                   -69.1847
Evaluation/CompletionRate                    0
Evaluation/Iteration                        79
Evaluation/MaxReturn                       -37.7455
Evaluation/MinReturn                     -2063.21
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.121
Extras/EpisodeRewardMean                   -67.7377
LinearFeatureBaseline/ExplainedVariance      0.0173458
PolicyExecTime                               0.233091
ProcessExecTime                              0.0315094
TotalEnvSteps                            80960
policy/Entropy                               1.27762
policy/KL                                    0.00695495
policy/KLBefore                              0
policy/LossAfter                            -0.0217299
policy/LossBefore                           -1.08372e-08
policy/Perplexity                            3.5881
policy/dLoss                                 0.0217298
---------------------------------------  ---------------
2021-06-04 13:25:27 | [train_policy] epoch #80 | Obtaining samples for iteration 80...
2021-06-04 13:25:27 | [train_policy] epoch #80 | Logging diagnostics...
2021-06-04 13:25:27 | [train_policy] epoch #80 | Optimizing policy...
2021-06-04 13:25:27 | [train_policy] epoch #80 | Computing loss before
2021-06-04 13:25:27 | [train_policy] epoch #80 | Computing KL before
2021-06-04 13:25:27 | [train_policy] epoch #80 | Optimizing
2021-06-04 13:25:27 | [train_policy] epoch #80 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:27 | [train_policy] epoch #80 | computing loss before
2021-06-04 13:25:27 | [train_policy] epoch #80 | computing gradient
2021-06-04 13:25:27 | [train_policy] epoch #80 | gradient computed
2021-06-04 13:25:27 | [train_policy] epoch #80 | computing descent direction
2021-06-04 13:25:27 | [train_policy] epoch #80 | descent direction computed
2021-06-04 13:25:27 | [train_policy] epoch #80 | backtrack iters: 1
2021-06-04 13:25:27 | [train_policy] epoch #80 | optimization finished
2021-06-04 13:25:27 | [train_policy] epoch #80 | Computing KL after
2021-06-04 13:25:27 | [train_policy] epoch #80 | Computing loss after
2021-06-04 13:25:27 | [train_policy] epoch #80 | Fitting baseline...
2021-06-04 13:25:27 | [train_policy] epoch #80 | Saving snapshot...
2021-06-04 13:25:27 | [train_policy] epoch #80 | Saved
2021-06-04 13:25:27 | [train_policy] epoch #80 | Time 67.69 s
2021-06-04 13:25:27 | [train_policy] epoch #80 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.286131
Evaluation/AverageDiscountedReturn         -49.132
Evaluation/AverageReturn                   -49.132
Evaluation/CompletionRate                    0
Evaluation/Iteration                        80
Evaluation/MaxReturn                       -39.8411
Evaluation/MinReturn                       -70.161
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.81621
Extras/EpisodeRewardMean                   -49.0309
LinearFeatureBaseline/ExplainedVariance    -36.0624
PolicyExecTime                               0.22947
ProcessExecTime                              0.0314181
TotalEnvSteps                            81972
policy/Entropy                               1.27263
policy/KL                                    0.00681887
policy/KLBefore                              0
policy/LossAfter                            -0.0248747
policy/LossBefore                            1.18974e-08
policy/Perplexity                            3.57023
policy/dLoss                                 0.0248747
---------------------------------------  ---------------
2021-06-04 13:25:27 | [train_policy] epoch #81 | Obtaining samples for iteration 81...
2021-06-04 13:25:28 | [train_policy] epoch #81 | Logging diagnostics...
2021-06-04 13:25:28 | [train_policy] epoch #81 | Optimizing policy...
2021-06-04 13:25:28 | [train_policy] epoch #81 | Computing loss before
2021-06-04 13:25:28 | [train_policy] epoch #81 | Computing KL before
2021-06-04 13:25:28 | [train_policy] epoch #81 | Optimizing
2021-06-04 13:25:28 | [train_policy] epoch #81 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:28 | [train_policy] epoch #81 | computing loss before
2021-06-04 13:25:28 | [train_policy] epoch #81 | computing gradient
2021-06-04 13:25:28 | [train_policy] epoch #81 | gradient computed
2021-06-04 13:25:28 | [train_policy] epoch #81 | computing descent direction
2021-06-04 13:25:28 | [train_policy] epoch #81 | descent direction computed
2021-06-04 13:25:28 | [train_policy] epoch #81 | backtrack iters: 1
2021-06-04 13:25:28 | [train_policy] epoch #81 | optimization finished
2021-06-04 13:25:28 | [train_policy] epoch #81 | Computing KL after
2021-06-04 13:25:28 | [train_policy] epoch #81 | Computing loss after
2021-06-04 13:25:28 | [train_policy] epoch #81 | Fitting baseline...
2021-06-04 13:25:28 | [train_policy] epoch #81 | Saving snapshot...
2021-06-04 13:25:28 | [train_policy] epoch #81 | Saved
2021-06-04 13:25:28 | [train_policy] epoch #81 | Time 68.49 s
2021-06-04 13:25:28 | [train_policy] epoch #81 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285814
Evaluation/AverageDiscountedReturn         -47.6815
Evaluation/AverageReturn                   -47.6815
Evaluation/CompletionRate                    0
Evaluation/Iteration                        81
Evaluation/MaxReturn                       -34.4884
Evaluation/MinReturn                       -83.3448
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.55275
Extras/EpisodeRewardMean                   -47.7708
LinearFeatureBaseline/ExplainedVariance      0.919692
PolicyExecTime                               0.231551
ProcessExecTime                              0.0312722
TotalEnvSteps                            82984
policy/Entropy                               1.23086
policy/KL                                    0.00745686
policy/KLBefore                              0
policy/LossAfter                            -0.022247
policy/LossBefore                           -2.02609e-08
policy/Perplexity                            3.42419
policy/dLoss                                 0.022247
---------------------------------------  ---------------
2021-06-04 13:25:28 | [train_policy] epoch #82 | Obtaining samples for iteration 82...
2021-06-04 13:25:29 | [train_policy] epoch #82 | Logging diagnostics...
2021-06-04 13:25:29 | [train_policy] epoch #82 | Optimizing policy...
2021-06-04 13:25:29 | [train_policy] epoch #82 | Computing loss before
2021-06-04 13:25:29 | [train_policy] epoch #82 | Computing KL before
2021-06-04 13:25:29 | [train_policy] epoch #82 | Optimizing
2021-06-04 13:25:29 | [train_policy] epoch #82 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:29 | [train_policy] epoch #82 | computing loss before
2021-06-04 13:25:29 | [train_policy] epoch #82 | computing gradient
2021-06-04 13:25:29 | [train_policy] epoch #82 | gradient computed
2021-06-04 13:25:29 | [train_policy] epoch #82 | computing descent direction
2021-06-04 13:25:29 | [train_policy] epoch #82 | descent direction computed
2021-06-04 13:25:29 | [train_policy] epoch #82 | backtrack iters: 1
2021-06-04 13:25:29 | [train_policy] epoch #82 | optimization finished
2021-06-04 13:25:29 | [train_policy] epoch #82 | Computing KL after
2021-06-04 13:25:29 | [train_policy] epoch #82 | Computing loss after
2021-06-04 13:25:29 | [train_policy] epoch #82 | Fitting baseline...
2021-06-04 13:25:29 | [train_policy] epoch #82 | Saving snapshot...
2021-06-04 13:25:29 | [train_policy] epoch #82 | Saved
2021-06-04 13:25:29 | [train_policy] epoch #82 | Time 69.30 s
2021-06-04 13:25:29 | [train_policy] epoch #82 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.290808
Evaluation/AverageDiscountedReturn         -69.9151
Evaluation/AverageReturn                   -69.9151
Evaluation/CompletionRate                    0
Evaluation/Iteration                        82
Evaluation/MaxReturn                       -37.9531
Evaluation/MinReturn                     -2061.88
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.915
Extras/EpisodeRewardMean                   -68.0115
LinearFeatureBaseline/ExplainedVariance      0.0119242
PolicyExecTime                               0.234011
ProcessExecTime                              0.0320108
TotalEnvSteps                            83996
policy/Entropy                               1.24129
policy/KL                                    0.0067582
policy/KLBefore                              0
policy/LossAfter                            -0.0178228
policy/LossBefore                           -1.22508e-08
policy/Perplexity                            3.46009
policy/dLoss                                 0.0178228
---------------------------------------  ---------------
2021-06-04 13:25:29 | [train_policy] epoch #83 | Obtaining samples for iteration 83...
2021-06-04 13:25:30 | [train_policy] epoch #83 | Logging diagnostics...
2021-06-04 13:25:30 | [train_policy] epoch #83 | Optimizing policy...
2021-06-04 13:25:30 | [train_policy] epoch #83 | Computing loss before
2021-06-04 13:25:30 | [train_policy] epoch #83 | Computing KL before
2021-06-04 13:25:30 | [train_policy] epoch #83 | Optimizing
2021-06-04 13:25:30 | [train_policy] epoch #83 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:30 | [train_policy] epoch #83 | computing loss before
2021-06-04 13:25:30 | [train_policy] epoch #83 | computing gradient
2021-06-04 13:25:30 | [train_policy] epoch #83 | gradient computed
2021-06-04 13:25:30 | [train_policy] epoch #83 | computing descent direction
2021-06-04 13:25:30 | [train_policy] epoch #83 | descent direction computed
2021-06-04 13:25:30 | [train_policy] epoch #83 | backtrack iters: 0
2021-06-04 13:25:30 | [train_policy] epoch #83 | optimization finished
2021-06-04 13:25:30 | [train_policy] epoch #83 | Computing KL after
2021-06-04 13:25:30 | [train_policy] epoch #83 | Computing loss after
2021-06-04 13:25:30 | [train_policy] epoch #83 | Fitting baseline...
2021-06-04 13:25:30 | [train_policy] epoch #83 | Saving snapshot...
2021-06-04 13:25:30 | [train_policy] epoch #83 | Saved
2021-06-04 13:25:30 | [train_policy] epoch #83 | Time 70.08 s
2021-06-04 13:25:30 | [train_policy] epoch #83 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.28486
Evaluation/AverageDiscountedReturn         -47.3695
Evaluation/AverageReturn                   -47.3695
Evaluation/CompletionRate                    0
Evaluation/Iteration                        83
Evaluation/MaxReturn                       -35.0826
Evaluation/MinReturn                       -66.1355
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.24952
Extras/EpisodeRewardMean                   -47.7836
LinearFeatureBaseline/ExplainedVariance    -23.4146
PolicyExecTime                               0.223529
ProcessExecTime                              0.0313308
TotalEnvSteps                            85008
policy/Entropy                               1.20062
policy/KL                                    0.00925045
policy/KLBefore                              0
policy/LossAfter                            -0.0158442
policy/LossBefore                           -2.80354e-08
policy/Perplexity                            3.32218
policy/dLoss                                 0.0158441
---------------------------------------  ---------------
2021-06-04 13:25:30 | [train_policy] epoch #84 | Obtaining samples for iteration 84...
2021-06-04 13:25:30 | [train_policy] epoch #84 | Logging diagnostics...
2021-06-04 13:25:30 | [train_policy] epoch #84 | Optimizing policy...
2021-06-04 13:25:30 | [train_policy] epoch #84 | Computing loss before
2021-06-04 13:25:30 | [train_policy] epoch #84 | Computing KL before
2021-06-04 13:25:30 | [train_policy] epoch #84 | Optimizing
2021-06-04 13:25:30 | [train_policy] epoch #84 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:30 | [train_policy] epoch #84 | computing loss before
2021-06-04 13:25:30 | [train_policy] epoch #84 | computing gradient
2021-06-04 13:25:30 | [train_policy] epoch #84 | gradient computed
2021-06-04 13:25:30 | [train_policy] epoch #84 | computing descent direction
2021-06-04 13:25:30 | [train_policy] epoch #84 | descent direction computed
2021-06-04 13:25:30 | [train_policy] epoch #84 | backtrack iters: 1
2021-06-04 13:25:30 | [train_policy] epoch #84 | optimization finished
2021-06-04 13:25:30 | [train_policy] epoch #84 | Computing KL after
2021-06-04 13:25:30 | [train_policy] epoch #84 | Computing loss after
2021-06-04 13:25:30 | [train_policy] epoch #84 | Fitting baseline...
2021-06-04 13:25:30 | [train_policy] epoch #84 | Saving snapshot...
2021-06-04 13:25:30 | [train_policy] epoch #84 | Saved
2021-06-04 13:25:30 | [train_policy] epoch #84 | Time 70.89 s
2021-06-04 13:25:30 | [train_policy] epoch #84 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.290584
Evaluation/AverageDiscountedReturn         -69.33
Evaluation/AverageReturn                   -69.33
Evaluation/CompletionRate                    0
Evaluation/Iteration                        84
Evaluation/MaxReturn                       -37.0301
Evaluation/MinReturn                     -2060.64
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.937
Extras/EpisodeRewardMean                   -67.7944
LinearFeatureBaseline/ExplainedVariance      0.0106149
PolicyExecTime                               0.235048
ProcessExecTime                              0.0318887
TotalEnvSteps                            86020
policy/Entropy                               1.17408
policy/KL                                    0.00636246
policy/KLBefore                              0
policy/LossAfter                            -0.0125822
policy/LossBefore                            1.69626e-08
policy/Perplexity                            3.23517
policy/dLoss                                 0.0125822
---------------------------------------  ---------------
2021-06-04 13:25:31 | [train_policy] epoch #85 | Obtaining samples for iteration 85...
2021-06-04 13:25:31 | [train_policy] epoch #85 | Logging diagnostics...
2021-06-04 13:25:31 | [train_policy] epoch #85 | Optimizing policy...
2021-06-04 13:25:31 | [train_policy] epoch #85 | Computing loss before
2021-06-04 13:25:31 | [train_policy] epoch #85 | Computing KL before
2021-06-04 13:25:31 | [train_policy] epoch #85 | Optimizing
2021-06-04 13:25:31 | [train_policy] epoch #85 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:31 | [train_policy] epoch #85 | computing loss before
2021-06-04 13:25:31 | [train_policy] epoch #85 | computing gradient
2021-06-04 13:25:31 | [train_policy] epoch #85 | gradient computed
2021-06-04 13:25:31 | [train_policy] epoch #85 | computing descent direction
2021-06-04 13:25:31 | [train_policy] epoch #85 | descent direction computed
2021-06-04 13:25:31 | [train_policy] epoch #85 | backtrack iters: 1
2021-06-04 13:25:31 | [train_policy] epoch #85 | optimization finished
2021-06-04 13:25:31 | [train_policy] epoch #85 | Computing KL after
2021-06-04 13:25:31 | [train_policy] epoch #85 | Computing loss after
2021-06-04 13:25:31 | [train_policy] epoch #85 | Fitting baseline...
2021-06-04 13:25:31 | [train_policy] epoch #85 | Saving snapshot...
2021-06-04 13:25:31 | [train_policy] epoch #85 | Saved
2021-06-04 13:25:31 | [train_policy] epoch #85 | Time 71.68 s
2021-06-04 13:25:31 | [train_policy] epoch #85 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285428
Evaluation/AverageDiscountedReturn         -69.2627
Evaluation/AverageReturn                   -69.2627
Evaluation/CompletionRate                    0
Evaluation/Iteration                        85
Evaluation/MaxReturn                       -36.6009
Evaluation/MinReturn                     -2053.79
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.112
Extras/EpisodeRewardMean                   -67.4421
LinearFeatureBaseline/ExplainedVariance      0.0178969
PolicyExecTime                               0.230795
ProcessExecTime                              0.0313735
TotalEnvSteps                            87032
policy/Entropy                               1.15966
policy/KL                                    0.00809843
policy/KLBefore                              0
policy/LossAfter                            -0.0254754
policy/LossBefore                            1.83761e-08
policy/Perplexity                            3.18885
policy/dLoss                                 0.0254754
---------------------------------------  ---------------
2021-06-04 13:25:31 | [train_policy] epoch #86 | Obtaining samples for iteration 86...
2021-06-04 13:25:32 | [train_policy] epoch #86 | Logging diagnostics...
2021-06-04 13:25:32 | [train_policy] epoch #86 | Optimizing policy...
2021-06-04 13:25:32 | [train_policy] epoch #86 | Computing loss before
2021-06-04 13:25:32 | [train_policy] epoch #86 | Computing KL before
2021-06-04 13:25:32 | [train_policy] epoch #86 | Optimizing
2021-06-04 13:25:32 | [train_policy] epoch #86 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:32 | [train_policy] epoch #86 | computing loss before
2021-06-04 13:25:32 | [train_policy] epoch #86 | computing gradient
2021-06-04 13:25:32 | [train_policy] epoch #86 | gradient computed
2021-06-04 13:25:32 | [train_policy] epoch #86 | computing descent direction
2021-06-04 13:25:32 | [train_policy] epoch #86 | descent direction computed
2021-06-04 13:25:32 | [train_policy] epoch #86 | backtrack iters: 0
2021-06-04 13:25:32 | [train_policy] epoch #86 | optimization finished
2021-06-04 13:25:32 | [train_policy] epoch #86 | Computing KL after
2021-06-04 13:25:32 | [train_policy] epoch #86 | Computing loss after
2021-06-04 13:25:32 | [train_policy] epoch #86 | Fitting baseline...
2021-06-04 13:25:32 | [train_policy] epoch #86 | Saving snapshot...
2021-06-04 13:25:32 | [train_policy] epoch #86 | Saved
2021-06-04 13:25:32 | [train_policy] epoch #86 | Time 72.48 s
2021-06-04 13:25:32 | [train_policy] epoch #86 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285744
Evaluation/AverageDiscountedReturn         -48.126
Evaluation/AverageReturn                   -48.126
Evaluation/CompletionRate                    0
Evaluation/Iteration                        86
Evaluation/MaxReturn                       -36.4155
Evaluation/MinReturn                       -86.3143
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.86098
Extras/EpisodeRewardMean                   -48.2809
LinearFeatureBaseline/ExplainedVariance    -27.5661
PolicyExecTime                               0.232143
ProcessExecTime                              0.0313966
TotalEnvSteps                            88044
policy/Entropy                               1.18183
policy/KL                                    0.00973035
policy/KLBefore                              0
policy/LossAfter                            -0.0329929
policy/LossBefore                            7.30334e-09
policy/Perplexity                            3.26033
policy/dLoss                                 0.0329929
---------------------------------------  ---------------
2021-06-04 13:25:32 | [train_policy] epoch #87 | Obtaining samples for iteration 87...
2021-06-04 13:25:33 | [train_policy] epoch #87 | Logging diagnostics...
2021-06-04 13:25:33 | [train_policy] epoch #87 | Optimizing policy...
2021-06-04 13:25:33 | [train_policy] epoch #87 | Computing loss before
2021-06-04 13:25:33 | [train_policy] epoch #87 | Computing KL before
2021-06-04 13:25:33 | [train_policy] epoch #87 | Optimizing
2021-06-04 13:25:33 | [train_policy] epoch #87 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:33 | [train_policy] epoch #87 | computing loss before
2021-06-04 13:25:33 | [train_policy] epoch #87 | computing gradient
2021-06-04 13:25:33 | [train_policy] epoch #87 | gradient computed
2021-06-04 13:25:33 | [train_policy] epoch #87 | computing descent direction
2021-06-04 13:25:33 | [train_policy] epoch #87 | descent direction computed
2021-06-04 13:25:33 | [train_policy] epoch #87 | backtrack iters: 0
2021-06-04 13:25:33 | [train_policy] epoch #87 | optimization finished
2021-06-04 13:25:33 | [train_policy] epoch #87 | Computing KL after
2021-06-04 13:25:33 | [train_policy] epoch #87 | Computing loss after
2021-06-04 13:25:33 | [train_policy] epoch #87 | Fitting baseline...
2021-06-04 13:25:33 | [train_policy] epoch #87 | Saving snapshot...
2021-06-04 13:25:33 | [train_policy] epoch #87 | Saved
2021-06-04 13:25:33 | [train_policy] epoch #87 | Time 73.28 s
2021-06-04 13:25:33 | [train_policy] epoch #87 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.289729
Evaluation/AverageDiscountedReturn         -47.7416
Evaluation/AverageReturn                   -47.7416
Evaluation/CompletionRate                    0
Evaluation/Iteration                        87
Evaluation/MaxReturn                       -37.0833
Evaluation/MinReturn                       -61.4966
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.33082
Extras/EpisodeRewardMean                   -47.7334
LinearFeatureBaseline/ExplainedVariance      0.947509
PolicyExecTime                               0.23269
ProcessExecTime                              0.0318
TotalEnvSteps                            89056
policy/Entropy                               1.12273
policy/KL                                    0.00992271
policy/KLBefore                              0
policy/LossAfter                            -0.0205207
policy/LossBefore                           -2.35591e-10
policy/Perplexity                            3.07325
policy/dLoss                                 0.0205207
---------------------------------------  ---------------
2021-06-04 13:25:33 | [train_policy] epoch #88 | Obtaining samples for iteration 88...
2021-06-04 13:25:34 | [train_policy] epoch #88 | Logging diagnostics...
2021-06-04 13:25:34 | [train_policy] epoch #88 | Optimizing policy...
2021-06-04 13:25:34 | [train_policy] epoch #88 | Computing loss before
2021-06-04 13:25:34 | [train_policy] epoch #88 | Computing KL before
2021-06-04 13:25:34 | [train_policy] epoch #88 | Optimizing
2021-06-04 13:25:34 | [train_policy] epoch #88 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:34 | [train_policy] epoch #88 | computing loss before
2021-06-04 13:25:34 | [train_policy] epoch #88 | computing gradient
2021-06-04 13:25:34 | [train_policy] epoch #88 | gradient computed
2021-06-04 13:25:34 | [train_policy] epoch #88 | computing descent direction
2021-06-04 13:25:34 | [train_policy] epoch #88 | descent direction computed
2021-06-04 13:25:34 | [train_policy] epoch #88 | backtrack iters: 1
2021-06-04 13:25:34 | [train_policy] epoch #88 | optimization finished
2021-06-04 13:25:34 | [train_policy] epoch #88 | Computing KL after
2021-06-04 13:25:34 | [train_policy] epoch #88 | Computing loss after
2021-06-04 13:25:34 | [train_policy] epoch #88 | Fitting baseline...
2021-06-04 13:25:34 | [train_policy] epoch #88 | Saving snapshot...
2021-06-04 13:25:34 | [train_policy] epoch #88 | Saved
2021-06-04 13:25:34 | [train_policy] epoch #88 | Time 74.06 s
2021-06-04 13:25:34 | [train_policy] epoch #88 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285783
Evaluation/AverageDiscountedReturn         -50.2919
Evaluation/AverageReturn                   -50.2919
Evaluation/CompletionRate                    0
Evaluation/Iteration                        88
Evaluation/MaxReturn                       -38.6094
Evaluation/MinReturn                      -218.789
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        19.9227
Extras/EpisodeRewardMean                   -50.4252
LinearFeatureBaseline/ExplainedVariance      0.367753
PolicyExecTime                               0.220338
ProcessExecTime                              0.0314553
TotalEnvSteps                            90068
policy/Entropy                               1.06094
policy/KL                                    0.00671496
policy/KLBefore                              0
policy/LossAfter                            -0.0147474
policy/LossBefore                           -4.82963e-09
policy/Perplexity                            2.88908
policy/dLoss                                 0.0147474
---------------------------------------  ---------------
2021-06-04 13:25:34 | [train_policy] epoch #89 | Obtaining samples for iteration 89...
2021-06-04 13:25:34 | [train_policy] epoch #89 | Logging diagnostics...
2021-06-04 13:25:34 | [train_policy] epoch #89 | Optimizing policy...
2021-06-04 13:25:34 | [train_policy] epoch #89 | Computing loss before
2021-06-04 13:25:34 | [train_policy] epoch #89 | Computing KL before
2021-06-04 13:25:34 | [train_policy] epoch #89 | Optimizing
2021-06-04 13:25:34 | [train_policy] epoch #89 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:34 | [train_policy] epoch #89 | computing loss before
2021-06-04 13:25:34 | [train_policy] epoch #89 | computing gradient
2021-06-04 13:25:34 | [train_policy] epoch #89 | gradient computed
2021-06-04 13:25:34 | [train_policy] epoch #89 | computing descent direction
2021-06-04 13:25:34 | [train_policy] epoch #89 | descent direction computed
2021-06-04 13:25:34 | [train_policy] epoch #89 | backtrack iters: 0
2021-06-04 13:25:34 | [train_policy] epoch #89 | optimization finished
2021-06-04 13:25:34 | [train_policy] epoch #89 | Computing KL after
2021-06-04 13:25:34 | [train_policy] epoch #89 | Computing loss after
2021-06-04 13:25:34 | [train_policy] epoch #89 | Fitting baseline...
2021-06-04 13:25:34 | [train_policy] epoch #89 | Saving snapshot...
2021-06-04 13:25:34 | [train_policy] epoch #89 | Saved
2021-06-04 13:25:34 | [train_policy] epoch #89 | Time 74.85 s
2021-06-04 13:25:34 | [train_policy] epoch #89 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.2873
Evaluation/AverageDiscountedReturn         -47.46
Evaluation/AverageReturn                   -47.46
Evaluation/CompletionRate                    0
Evaluation/Iteration                        89
Evaluation/MaxReturn                       -34.2688
Evaluation/MinReturn                       -78.6716
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.55723
Extras/EpisodeRewardMean                   -47.3054
LinearFeatureBaseline/ExplainedVariance      0.869735
PolicyExecTime                               0.226984
ProcessExecTime                              0.0316179
TotalEnvSteps                            91080
policy/Entropy                               1.06167
policy/KL                                    0.00947723
policy/KLBefore                              0
policy/LossAfter                            -0.0159315
policy/LossBefore                            1.41355e-09
policy/Perplexity                            2.89119
policy/dLoss                                 0.0159315
---------------------------------------  ---------------
2021-06-04 13:25:34 | [train_policy] epoch #90 | Obtaining samples for iteration 90...
2021-06-04 13:25:35 | [train_policy] epoch #90 | Logging diagnostics...
2021-06-04 13:25:35 | [train_policy] epoch #90 | Optimizing policy...
2021-06-04 13:25:35 | [train_policy] epoch #90 | Computing loss before
2021-06-04 13:25:35 | [train_policy] epoch #90 | Computing KL before
2021-06-04 13:25:35 | [train_policy] epoch #90 | Optimizing
2021-06-04 13:25:35 | [train_policy] epoch #90 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:35 | [train_policy] epoch #90 | computing loss before
2021-06-04 13:25:35 | [train_policy] epoch #90 | computing gradient
2021-06-04 13:25:35 | [train_policy] epoch #90 | gradient computed
2021-06-04 13:25:35 | [train_policy] epoch #90 | computing descent direction
2021-06-04 13:25:35 | [train_policy] epoch #90 | descent direction computed
2021-06-04 13:25:35 | [train_policy] epoch #90 | backtrack iters: 0
2021-06-04 13:25:35 | [train_policy] epoch #90 | optimization finished
2021-06-04 13:25:35 | [train_policy] epoch #90 | Computing KL after
2021-06-04 13:25:35 | [train_policy] epoch #90 | Computing loss after
2021-06-04 13:25:35 | [train_policy] epoch #90 | Fitting baseline...
2021-06-04 13:25:35 | [train_policy] epoch #90 | Saving snapshot...
2021-06-04 13:25:35 | [train_policy] epoch #90 | Saved
2021-06-04 13:25:35 | [train_policy] epoch #90 | Time 75.65 s
2021-06-04 13:25:35 | [train_policy] epoch #90 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285519
Evaluation/AverageDiscountedReturn         -47.4441
Evaluation/AverageReturn                   -47.4441
Evaluation/CompletionRate                    0
Evaluation/Iteration                        90
Evaluation/MaxReturn                       -37.7522
Evaluation/MinReturn                       -65.8392
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.65475
Extras/EpisodeRewardMean                   -47.6464
LinearFeatureBaseline/ExplainedVariance      0.950403
PolicyExecTime                               0.230692
ProcessExecTime                              0.0313489
TotalEnvSteps                            92092
policy/Entropy                               1.06378
policy/KL                                    0.00994642
policy/KLBefore                              0
policy/LossAfter                            -0.0242724
policy/LossBefore                           -5.18301e-09
policy/Perplexity                            2.8973
policy/dLoss                                 0.0242724
---------------------------------------  ---------------
2021-06-04 13:25:35 | [train_policy] epoch #91 | Obtaining samples for iteration 91...
2021-06-04 13:25:36 | [train_policy] epoch #91 | Logging diagnostics...
2021-06-04 13:25:36 | [train_policy] epoch #91 | Optimizing policy...
2021-06-04 13:25:36 | [train_policy] epoch #91 | Computing loss before
2021-06-04 13:25:36 | [train_policy] epoch #91 | Computing KL before
2021-06-04 13:25:36 | [train_policy] epoch #91 | Optimizing
2021-06-04 13:25:36 | [train_policy] epoch #91 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:36 | [train_policy] epoch #91 | computing loss before
2021-06-04 13:25:36 | [train_policy] epoch #91 | computing gradient
2021-06-04 13:25:36 | [train_policy] epoch #91 | gradient computed
2021-06-04 13:25:36 | [train_policy] epoch #91 | computing descent direction
2021-06-04 13:25:36 | [train_policy] epoch #91 | descent direction computed
2021-06-04 13:25:36 | [train_policy] epoch #91 | backtrack iters: 1
2021-06-04 13:25:36 | [train_policy] epoch #91 | optimization finished
2021-06-04 13:25:36 | [train_policy] epoch #91 | Computing KL after
2021-06-04 13:25:36 | [train_policy] epoch #91 | Computing loss after
2021-06-04 13:25:36 | [train_policy] epoch #91 | Fitting baseline...
2021-06-04 13:25:36 | [train_policy] epoch #91 | Saving snapshot...
2021-06-04 13:25:36 | [train_policy] epoch #91 | Saved
2021-06-04 13:25:36 | [train_policy] epoch #91 | Time 76.45 s
2021-06-04 13:25:36 | [train_policy] epoch #91 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.289682
Evaluation/AverageDiscountedReturn         -46.8215
Evaluation/AverageReturn                   -46.8215
Evaluation/CompletionRate                    0
Evaluation/Iteration                        91
Evaluation/MaxReturn                       -34.809
Evaluation/MinReturn                       -81.4589
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.72516
Extras/EpisodeRewardMean                   -46.88
LinearFeatureBaseline/ExplainedVariance      0.89894
PolicyExecTime                               0.231572
ProcessExecTime                              0.0319514
TotalEnvSteps                            93104
policy/Entropy                               1.04012
policy/KL                                    0.00694469
policy/KLBefore                              0
policy/LossAfter                            -0.0239906
policy/LossBefore                            8.48129e-09
policy/Perplexity                            2.82956
policy/dLoss                                 0.0239906
---------------------------------------  ---------------
2021-06-04 13:25:36 | [train_policy] epoch #92 | Obtaining samples for iteration 92...
2021-06-04 13:25:37 | [train_policy] epoch #92 | Logging diagnostics...
2021-06-04 13:25:37 | [train_policy] epoch #92 | Optimizing policy...
2021-06-04 13:25:37 | [train_policy] epoch #92 | Computing loss before
2021-06-04 13:25:37 | [train_policy] epoch #92 | Computing KL before
2021-06-04 13:25:37 | [train_policy] epoch #92 | Optimizing
2021-06-04 13:25:37 | [train_policy] epoch #92 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:37 | [train_policy] epoch #92 | computing loss before
2021-06-04 13:25:37 | [train_policy] epoch #92 | computing gradient
2021-06-04 13:25:37 | [train_policy] epoch #92 | gradient computed
2021-06-04 13:25:37 | [train_policy] epoch #92 | computing descent direction
2021-06-04 13:25:37 | [train_policy] epoch #92 | descent direction computed
2021-06-04 13:25:37 | [train_policy] epoch #92 | backtrack iters: 1
2021-06-04 13:25:37 | [train_policy] epoch #92 | optimization finished
2021-06-04 13:25:37 | [train_policy] epoch #92 | Computing KL after
2021-06-04 13:25:37 | [train_policy] epoch #92 | Computing loss after
2021-06-04 13:25:37 | [train_policy] epoch #92 | Fitting baseline...
2021-06-04 13:25:37 | [train_policy] epoch #92 | Saving snapshot...
2021-06-04 13:25:37 | [train_policy] epoch #92 | Saved
2021-06-04 13:25:37 | [train_policy] epoch #92 | Time 77.26 s
2021-06-04 13:25:37 | [train_policy] epoch #92 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285793
Evaluation/AverageDiscountedReturn         -70.5658
Evaluation/AverageReturn                   -70.5658
Evaluation/CompletionRate                    0
Evaluation/Iteration                        92
Evaluation/MaxReturn                       -34.5381
Evaluation/MinReturn                     -2062.26
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.035
Extras/EpisodeRewardMean                   -69.0263
LinearFeatureBaseline/ExplainedVariance      0.0126813
PolicyExecTime                               0.228519
ProcessExecTime                              0.0314054
TotalEnvSteps                            94116
policy/Entropy                               1.02445
policy/KL                                    0.00755205
policy/KLBefore                              0
policy/LossAfter                            -0.0166233
policy/LossBefore                           -1.27219e-08
policy/Perplexity                            2.78557
policy/dLoss                                 0.0166233
---------------------------------------  ---------------
2021-06-04 13:25:37 | [train_policy] epoch #93 | Obtaining samples for iteration 93...
2021-06-04 13:25:38 | [train_policy] epoch #93 | Logging diagnostics...
2021-06-04 13:25:38 | [train_policy] epoch #93 | Optimizing policy...
2021-06-04 13:25:38 | [train_policy] epoch #93 | Computing loss before
2021-06-04 13:25:38 | [train_policy] epoch #93 | Computing KL before
2021-06-04 13:25:38 | [train_policy] epoch #93 | Optimizing
2021-06-04 13:25:38 | [train_policy] epoch #93 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:38 | [train_policy] epoch #93 | computing loss before
2021-06-04 13:25:38 | [train_policy] epoch #93 | computing gradient
2021-06-04 13:25:38 | [train_policy] epoch #93 | gradient computed
2021-06-04 13:25:38 | [train_policy] epoch #93 | computing descent direction
2021-06-04 13:25:38 | [train_policy] epoch #93 | descent direction computed
2021-06-04 13:25:38 | [train_policy] epoch #93 | backtrack iters: 1
2021-06-04 13:25:38 | [train_policy] epoch #93 | optimization finished
2021-06-04 13:25:38 | [train_policy] epoch #93 | Computing KL after
2021-06-04 13:25:38 | [train_policy] epoch #93 | Computing loss after
2021-06-04 13:25:38 | [train_policy] epoch #93 | Fitting baseline...
2021-06-04 13:25:38 | [train_policy] epoch #93 | Saving snapshot...
2021-06-04 13:25:38 | [train_policy] epoch #93 | Saved
2021-06-04 13:25:38 | [train_policy] epoch #93 | Time 78.07 s
2021-06-04 13:25:38 | [train_policy] epoch #93 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                  0.285882
Evaluation/AverageDiscountedReturn         -49.359
Evaluation/AverageReturn                   -49.359
Evaluation/CompletionRate                    0
Evaluation/Iteration                        93
Evaluation/MaxReturn                       -35.5239
Evaluation/MinReturn                      -246.602
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        21.419
Extras/EpisodeRewardMean                   -70.2248
LinearFeatureBaseline/ExplainedVariance    -16.0284
PolicyExecTime                               0.240414
ProcessExecTime                              0.0314405
TotalEnvSteps                            95128
policy/Entropy                               1.00469
policy/KL                                    0.00671006
policy/KLBefore                              0
policy/LossAfter                            -0.0199245
policy/LossBefore                           -2.54439e-08
policy/Perplexity                            2.73107
policy/dLoss                                 0.0199245
---------------------------------------  ---------------
2021-06-04 13:25:38 | [train_policy] epoch #94 | Obtaining samples for iteration 94...
2021-06-04 13:25:38 | [train_policy] epoch #94 | Logging diagnostics...
2021-06-04 13:25:38 | [train_policy] epoch #94 | Optimizing policy...
2021-06-04 13:25:38 | [train_policy] epoch #94 | Computing loss before
2021-06-04 13:25:38 | [train_policy] epoch #94 | Computing KL before
2021-06-04 13:25:38 | [train_policy] epoch #94 | Optimizing
2021-06-04 13:25:38 | [train_policy] epoch #94 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:38 | [train_policy] epoch #94 | computing loss before
2021-06-04 13:25:38 | [train_policy] epoch #94 | computing gradient
2021-06-04 13:25:38 | [train_policy] epoch #94 | gradient computed
2021-06-04 13:25:38 | [train_policy] epoch #94 | computing descent direction
2021-06-04 13:25:38 | [train_policy] epoch #94 | descent direction computed
2021-06-04 13:25:38 | [train_policy] epoch #94 | backtrack iters: 0
2021-06-04 13:25:38 | [train_policy] epoch #94 | optimization finished
2021-06-04 13:25:38 | [train_policy] epoch #94 | Computing KL after
2021-06-04 13:25:38 | [train_policy] epoch #94 | Computing loss after
2021-06-04 13:25:38 | [train_policy] epoch #94 | Fitting baseline...
2021-06-04 13:25:38 | [train_policy] epoch #94 | Saving snapshot...
2021-06-04 13:25:38 | [train_policy] epoch #94 | Saved
2021-06-04 13:25:38 | [train_policy] epoch #94 | Time 78.86 s
2021-06-04 13:25:38 | [train_policy] epoch #94 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.287625
Evaluation/AverageDiscountedReturn         -46.422
Evaluation/AverageReturn                   -46.422
Evaluation/CompletionRate                    0
Evaluation/Iteration                        94
Evaluation/MaxReturn                       -32.8915
Evaluation/MinReturn                       -64.8774
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.25544
Extras/EpisodeRewardMean                   -46.5042
LinearFeatureBaseline/ExplainedVariance      0.95303
PolicyExecTime                               0.225703
ProcessExecTime                              0.0316336
TotalEnvSteps                            96140
policy/Entropy                               1.00173
policy/KL                                    0.00955357
policy/KLBefore                              0
policy/LossAfter                            -0.0205315
policy/LossBefore                           -1.86117e-08
policy/Perplexity                            2.72298
policy/dLoss                                 0.0205315
---------------------------------------  ---------------
2021-06-04 13:25:38 | [train_policy] epoch #95 | Obtaining samples for iteration 95...
2021-06-04 13:25:39 | [train_policy] epoch #95 | Logging diagnostics...
2021-06-04 13:25:39 | [train_policy] epoch #95 | Optimizing policy...
2021-06-04 13:25:39 | [train_policy] epoch #95 | Computing loss before
2021-06-04 13:25:39 | [train_policy] epoch #95 | Computing KL before
2021-06-04 13:25:39 | [train_policy] epoch #95 | Optimizing
2021-06-04 13:25:39 | [train_policy] epoch #95 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:39 | [train_policy] epoch #95 | computing loss before
2021-06-04 13:25:39 | [train_policy] epoch #95 | computing gradient
2021-06-04 13:25:39 | [train_policy] epoch #95 | gradient computed
2021-06-04 13:25:39 | [train_policy] epoch #95 | computing descent direction
2021-06-04 13:25:39 | [train_policy] epoch #95 | descent direction computed
2021-06-04 13:25:39 | [train_policy] epoch #95 | backtrack iters: 1
2021-06-04 13:25:39 | [train_policy] epoch #95 | optimization finished
2021-06-04 13:25:39 | [train_policy] epoch #95 | Computing KL after
2021-06-04 13:25:39 | [train_policy] epoch #95 | Computing loss after
2021-06-04 13:25:39 | [train_policy] epoch #95 | Fitting baseline...
2021-06-04 13:25:39 | [train_policy] epoch #95 | Saving snapshot...
2021-06-04 13:25:39 | [train_policy] epoch #95 | Saved
2021-06-04 13:25:39 | [train_policy] epoch #95 | Time 79.66 s
2021-06-04 13:25:39 | [train_policy] epoch #95 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285834
Evaluation/AverageDiscountedReturn         -46.2319
Evaluation/AverageReturn                   -46.2319
Evaluation/CompletionRate                    0
Evaluation/Iteration                        95
Evaluation/MaxReturn                       -35.3566
Evaluation/MinReturn                      -104.463
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.60974
Extras/EpisodeRewardMean                   -46.3227
LinearFeatureBaseline/ExplainedVariance      0.846269
PolicyExecTime                               0.22685
ProcessExecTime                              0.0316727
TotalEnvSteps                            97152
policy/Entropy                               0.999772
policy/KL                                    0.00649117
policy/KLBefore                              0
policy/LossAfter                            -0.0193877
policy/LossBefore                            9.30586e-09
policy/Perplexity                            2.71766
policy/dLoss                                 0.0193877
---------------------------------------  ---------------
2021-06-04 13:25:39 | [train_policy] epoch #96 | Obtaining samples for iteration 96...
2021-06-04 13:25:40 | [train_policy] epoch #96 | Logging diagnostics...
2021-06-04 13:25:40 | [train_policy] epoch #96 | Optimizing policy...
2021-06-04 13:25:40 | [train_policy] epoch #96 | Computing loss before
2021-06-04 13:25:40 | [train_policy] epoch #96 | Computing KL before
2021-06-04 13:25:40 | [train_policy] epoch #96 | Optimizing
2021-06-04 13:25:40 | [train_policy] epoch #96 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:40 | [train_policy] epoch #96 | computing loss before
2021-06-04 13:25:40 | [train_policy] epoch #96 | computing gradient
2021-06-04 13:25:40 | [train_policy] epoch #96 | gradient computed
2021-06-04 13:25:40 | [train_policy] epoch #96 | computing descent direction
2021-06-04 13:25:40 | [train_policy] epoch #96 | descent direction computed
2021-06-04 13:25:40 | [train_policy] epoch #96 | backtrack iters: 1
2021-06-04 13:25:40 | [train_policy] epoch #96 | optimization finished
2021-06-04 13:25:40 | [train_policy] epoch #96 | Computing KL after
2021-06-04 13:25:40 | [train_policy] epoch #96 | Computing loss after
2021-06-04 13:25:40 | [train_policy] epoch #96 | Fitting baseline...
2021-06-04 13:25:40 | [train_policy] epoch #96 | Saving snapshot...
2021-06-04 13:25:40 | [train_policy] epoch #96 | Saved
2021-06-04 13:25:40 | [train_policy] epoch #96 | Time 80.48 s
2021-06-04 13:25:40 | [train_policy] epoch #96 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                  0.30735
Evaluation/AverageDiscountedReturn         -46.7567
Evaluation/AverageReturn                   -46.7567
Evaluation/CompletionRate                    0
Evaluation/Iteration                        96
Evaluation/MaxReturn                       -35.0427
Evaluation/MinReturn                       -64.1803
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.37077
Extras/EpisodeRewardMean                   -46.4472
LinearFeatureBaseline/ExplainedVariance      0.947726
PolicyExecTime                               0.232604
ProcessExecTime                              0.0339215
TotalEnvSteps                            98164
policy/Entropy                               0.956764
policy/KL                                    0.00675648
policy/KLBefore                              0
policy/LossAfter                            -0.0208751
policy/LossBefore                           -9.42366e-10
policy/Perplexity                            2.60326
policy/dLoss                                 0.0208751
---------------------------------------  ---------------
2021-06-04 13:25:40 | [train_policy] epoch #97 | Obtaining samples for iteration 97...
2021-06-04 13:25:41 | [train_policy] epoch #97 | Logging diagnostics...
2021-06-04 13:25:41 | [train_policy] epoch #97 | Optimizing policy...
2021-06-04 13:25:41 | [train_policy] epoch #97 | Computing loss before
2021-06-04 13:25:41 | [train_policy] epoch #97 | Computing KL before
2021-06-04 13:25:41 | [train_policy] epoch #97 | Optimizing
2021-06-04 13:25:41 | [train_policy] epoch #97 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:41 | [train_policy] epoch #97 | computing loss before
2021-06-04 13:25:41 | [train_policy] epoch #97 | computing gradient
2021-06-04 13:25:41 | [train_policy] epoch #97 | gradient computed
2021-06-04 13:25:41 | [train_policy] epoch #97 | computing descent direction
2021-06-04 13:25:41 | [train_policy] epoch #97 | descent direction computed
2021-06-04 13:25:41 | [train_policy] epoch #97 | backtrack iters: 1
2021-06-04 13:25:41 | [train_policy] epoch #97 | optimization finished
2021-06-04 13:25:41 | [train_policy] epoch #97 | Computing KL after
2021-06-04 13:25:41 | [train_policy] epoch #97 | Computing loss after
2021-06-04 13:25:41 | [train_policy] epoch #97 | Fitting baseline...
2021-06-04 13:25:41 | [train_policy] epoch #97 | Saving snapshot...
2021-06-04 13:25:41 | [train_policy] epoch #97 | Saved
2021-06-04 13:25:41 | [train_policy] epoch #97 | Time 81.30 s
2021-06-04 13:25:41 | [train_policy] epoch #97 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                  0.292476
Evaluation/AverageDiscountedReturn         -50.0296
Evaluation/AverageReturn                   -50.0296
Evaluation/CompletionRate                    0
Evaluation/Iteration                        97
Evaluation/MaxReturn                       -32.9807
Evaluation/MinReturn                      -246.359
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        23.2916
Extras/EpisodeRewardMean                   -49.7122
LinearFeatureBaseline/ExplainedVariance      0.671622
PolicyExecTime                               0.23419
ProcessExecTime                              0.032222
TotalEnvSteps                            99176
policy/Entropy                               0.933668
policy/KL                                    0.00663224
policy/KLBefore                              0
policy/LossAfter                            -0.0213335
policy/LossBefore                            1.17796e-08
policy/Perplexity                            2.54382
policy/dLoss                                 0.0213335
---------------------------------------  ---------------
2021-06-04 13:25:41 | [train_policy] epoch #98 | Obtaining samples for iteration 98...
2021-06-04 13:25:42 | [train_policy] epoch #98 | Logging diagnostics...
2021-06-04 13:25:42 | [train_policy] epoch #98 | Optimizing policy...
2021-06-04 13:25:42 | [train_policy] epoch #98 | Computing loss before
2021-06-04 13:25:42 | [train_policy] epoch #98 | Computing KL before
2021-06-04 13:25:42 | [train_policy] epoch #98 | Optimizing
2021-06-04 13:25:42 | [train_policy] epoch #98 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:42 | [train_policy] epoch #98 | computing loss before
2021-06-04 13:25:42 | [train_policy] epoch #98 | computing gradient
2021-06-04 13:25:42 | [train_policy] epoch #98 | gradient computed
2021-06-04 13:25:42 | [train_policy] epoch #98 | computing descent direction
2021-06-04 13:25:42 | [train_policy] epoch #98 | descent direction computed
2021-06-04 13:25:42 | [train_policy] epoch #98 | backtrack iters: 1
2021-06-04 13:25:42 | [train_policy] epoch #98 | optimization finished
2021-06-04 13:25:42 | [train_policy] epoch #98 | Computing KL after
2021-06-04 13:25:42 | [train_policy] epoch #98 | Computing loss after
2021-06-04 13:25:42 | [train_policy] epoch #98 | Fitting baseline...
2021-06-04 13:25:42 | [train_policy] epoch #98 | Saving snapshot...
2021-06-04 13:25:42 | [train_policy] epoch #98 | Saved
2021-06-04 13:25:42 | [train_policy] epoch #98 | Time 82.11 s
2021-06-04 13:25:42 | [train_policy] epoch #98 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285814
Evaluation/AverageDiscountedReturn          -46.2733
Evaluation/AverageReturn                    -46.2733
Evaluation/CompletionRate                     0
Evaluation/Iteration                         98
Evaluation/MaxReturn                        -32.3115
Evaluation/MinReturn                        -62.1535
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.82998
Extras/EpisodeRewardMean                    -46.9397
LinearFeatureBaseline/ExplainedVariance       0.9114
PolicyExecTime                                0.23049
ProcessExecTime                               0.0314596
TotalEnvSteps                            100188
policy/Entropy                                0.950987
policy/KL                                     0.00686897
policy/KLBefore                               0
policy/LossAfter                             -0.0166084
policy/LossBefore                             4.18175e-09
policy/Perplexity                             2.58826
policy/dLoss                                  0.0166084
---------------------------------------  ----------------
2021-06-04 13:25:42 | [train_policy] epoch #99 | Obtaining samples for iteration 99...
2021-06-04 13:25:42 | [train_policy] epoch #99 | Logging diagnostics...
2021-06-04 13:25:42 | [train_policy] epoch #99 | Optimizing policy...
2021-06-04 13:25:42 | [train_policy] epoch #99 | Computing loss before
2021-06-04 13:25:42 | [train_policy] epoch #99 | Computing KL before
2021-06-04 13:25:42 | [train_policy] epoch #99 | Optimizing
2021-06-04 13:25:42 | [train_policy] epoch #99 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:42 | [train_policy] epoch #99 | computing loss before
2021-06-04 13:25:42 | [train_policy] epoch #99 | computing gradient
2021-06-04 13:25:42 | [train_policy] epoch #99 | gradient computed
2021-06-04 13:25:42 | [train_policy] epoch #99 | computing descent direction
2021-06-04 13:25:42 | [train_policy] epoch #99 | descent direction computed
2021-06-04 13:25:42 | [train_policy] epoch #99 | backtrack iters: 1
2021-06-04 13:25:42 | [train_policy] epoch #99 | optimization finished
2021-06-04 13:25:42 | [train_policy] epoch #99 | Computing KL after
2021-06-04 13:25:42 | [train_policy] epoch #99 | Computing loss after
2021-06-04 13:25:42 | [train_policy] epoch #99 | Fitting baseline...
2021-06-04 13:25:42 | [train_policy] epoch #99 | Saving snapshot...
2021-06-04 13:25:43 | [train_policy] epoch #99 | Saved
2021-06-04 13:25:43 | [train_policy] epoch #99 | Time 82.91 s
2021-06-04 13:25:43 | [train_policy] epoch #99 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285201
Evaluation/AverageDiscountedReturn          -45.2882
Evaluation/AverageReturn                    -45.2882
Evaluation/CompletionRate                     0
Evaluation/Iteration                         99
Evaluation/MaxReturn                        -33.9597
Evaluation/MinReturn                        -60.5841
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.0284
Extras/EpisodeRewardMean                    -45.427
LinearFeatureBaseline/ExplainedVariance       0.948382
PolicyExecTime                                0.237636
ProcessExecTime                               0.0312724
TotalEnvSteps                            101200
policy/Entropy                                0.910041
policy/KL                                     0.00676404
policy/KLBefore                               0
policy/LossAfter                             -0.0264817
policy/LossBefore                             6.83215e-09
policy/Perplexity                             2.48442
policy/dLoss                                  0.0264817
---------------------------------------  ----------------
2021-06-04 13:25:43 | [train_policy] epoch #100 | Obtaining samples for iteration 100...
2021-06-04 13:25:43 | [train_policy] epoch #100 | Logging diagnostics...
2021-06-04 13:25:43 | [train_policy] epoch #100 | Optimizing policy...
2021-06-04 13:25:43 | [train_policy] epoch #100 | Computing loss before
2021-06-04 13:25:43 | [train_policy] epoch #100 | Computing KL before
2021-06-04 13:25:43 | [train_policy] epoch #100 | Optimizing
2021-06-04 13:25:43 | [train_policy] epoch #100 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:43 | [train_policy] epoch #100 | computing loss before
2021-06-04 13:25:43 | [train_policy] epoch #100 | computing gradient
2021-06-04 13:25:43 | [train_policy] epoch #100 | gradient computed
2021-06-04 13:25:43 | [train_policy] epoch #100 | computing descent direction
2021-06-04 13:25:43 | [train_policy] epoch #100 | descent direction computed
2021-06-04 13:25:43 | [train_policy] epoch #100 | backtrack iters: 1
2021-06-04 13:25:43 | [train_policy] epoch #100 | optimization finished
2021-06-04 13:25:43 | [train_policy] epoch #100 | Computing KL after
2021-06-04 13:25:43 | [train_policy] epoch #100 | Computing loss after
2021-06-04 13:25:43 | [train_policy] epoch #100 | Fitting baseline...
2021-06-04 13:25:43 | [train_policy] epoch #100 | Saving snapshot...
2021-06-04 13:25:43 | [train_policy] epoch #100 | Saved
2021-06-04 13:25:43 | [train_policy] epoch #100 | Time 83.72 s
2021-06-04 13:25:43 | [train_policy] epoch #100 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286874
Evaluation/AverageDiscountedReturn          -68.9232
Evaluation/AverageReturn                    -68.9232
Evaluation/CompletionRate                     0
Evaluation/Iteration                        100
Evaluation/MaxReturn                        -32.8949
Evaluation/MinReturn                      -2056.02
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.393
Extras/EpisodeRewardMean                    -66.7627
LinearFeatureBaseline/ExplainedVariance       0.00855974
PolicyExecTime                                0.232396
ProcessExecTime                               0.0316236
TotalEnvSteps                            102212
policy/Entropy                                0.933774
policy/KL                                     0.00675097
policy/KLBefore                               0
policy/LossAfter                             -0.0146882
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             2.54409
policy/dLoss                                  0.0146882
---------------------------------------  ----------------
2021-06-04 13:25:43 | [train_policy] epoch #101 | Obtaining samples for iteration 101...
2021-06-04 13:25:44 | [train_policy] epoch #101 | Logging diagnostics...
2021-06-04 13:25:44 | [train_policy] epoch #101 | Optimizing policy...
2021-06-04 13:25:44 | [train_policy] epoch #101 | Computing loss before
2021-06-04 13:25:44 | [train_policy] epoch #101 | Computing KL before
2021-06-04 13:25:44 | [train_policy] epoch #101 | Optimizing
2021-06-04 13:25:44 | [train_policy] epoch #101 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:44 | [train_policy] epoch #101 | computing loss before
2021-06-04 13:25:44 | [train_policy] epoch #101 | computing gradient
2021-06-04 13:25:44 | [train_policy] epoch #101 | gradient computed
2021-06-04 13:25:44 | [train_policy] epoch #101 | computing descent direction
2021-06-04 13:25:44 | [train_policy] epoch #101 | descent direction computed
2021-06-04 13:25:44 | [train_policy] epoch #101 | backtrack iters: 0
2021-06-04 13:25:44 | [train_policy] epoch #101 | optimization finished
2021-06-04 13:25:44 | [train_policy] epoch #101 | Computing KL after
2021-06-04 13:25:44 | [train_policy] epoch #101 | Computing loss after
2021-06-04 13:25:44 | [train_policy] epoch #101 | Fitting baseline...
2021-06-04 13:25:44 | [train_policy] epoch #101 | Saving snapshot...
2021-06-04 13:25:44 | [train_policy] epoch #101 | Saved
2021-06-04 13:25:44 | [train_policy] epoch #101 | Time 84.51 s
2021-06-04 13:25:44 | [train_policy] epoch #101 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286318
Evaluation/AverageDiscountedReturn          -46.8029
Evaluation/AverageReturn                    -46.8029
Evaluation/CompletionRate                     0
Evaluation/Iteration                        101
Evaluation/MaxReturn                        -34.0767
Evaluation/MinReturn                       -179.183
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         15.1455
Extras/EpisodeRewardMean                    -67.2937
LinearFeatureBaseline/ExplainedVariance      -6.93988
PolicyExecTime                                0.226163
ProcessExecTime                               0.0313699
TotalEnvSteps                            103224
policy/Entropy                                0.99682
policy/KL                                     0.00900949
policy/KLBefore                               0
policy/LossAfter                             -0.0293979
policy/LossBefore                             3.15693e-08
policy/Perplexity                             2.70965
policy/dLoss                                  0.0293979
---------------------------------------  ----------------
2021-06-04 13:25:44 | [train_policy] epoch #102 | Obtaining samples for iteration 102...
2021-06-04 13:25:45 | [train_policy] epoch #102 | Logging diagnostics...
2021-06-04 13:25:45 | [train_policy] epoch #102 | Optimizing policy...
2021-06-04 13:25:45 | [train_policy] epoch #102 | Computing loss before
2021-06-04 13:25:45 | [train_policy] epoch #102 | Computing KL before
2021-06-04 13:25:45 | [train_policy] epoch #102 | Optimizing
2021-06-04 13:25:45 | [train_policy] epoch #102 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:45 | [train_policy] epoch #102 | computing loss before
2021-06-04 13:25:45 | [train_policy] epoch #102 | computing gradient
2021-06-04 13:25:45 | [train_policy] epoch #102 | gradient computed
2021-06-04 13:25:45 | [train_policy] epoch #102 | computing descent direction
2021-06-04 13:25:45 | [train_policy] epoch #102 | descent direction computed
2021-06-04 13:25:45 | [train_policy] epoch #102 | backtrack iters: 0
2021-06-04 13:25:45 | [train_policy] epoch #102 | optimization finished
2021-06-04 13:25:45 | [train_policy] epoch #102 | Computing KL after
2021-06-04 13:25:45 | [train_policy] epoch #102 | Computing loss after
2021-06-04 13:25:45 | [train_policy] epoch #102 | Fitting baseline...
2021-06-04 13:25:45 | [train_policy] epoch #102 | Saving snapshot...
2021-06-04 13:25:45 | [train_policy] epoch #102 | Saved
2021-06-04 13:25:45 | [train_policy] epoch #102 | Time 85.31 s
2021-06-04 13:25:45 | [train_policy] epoch #102 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.288157
Evaluation/AverageDiscountedReturn          -69.3857
Evaluation/AverageReturn                    -69.3857
Evaluation/CompletionRate                     0
Evaluation/Iteration                        102
Evaluation/MaxReturn                        -37.0707
Evaluation/MinReturn                      -2061.13
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.938
Extras/EpisodeRewardMean                    -67.3318
LinearFeatureBaseline/ExplainedVariance       0.0272235
PolicyExecTime                                0.236312
ProcessExecTime                               0.0315719
TotalEnvSteps                            104236
policy/Entropy                                1.01626
policy/KL                                     0.00957611
policy/KLBefore                               0
policy/LossAfter                             -0.0146899
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             2.76284
policy/dLoss                                  0.0146899
---------------------------------------  ---------------
2021-06-04 13:25:45 | [train_policy] epoch #103 | Obtaining samples for iteration 103...
2021-06-04 13:25:46 | [train_policy] epoch #103 | Logging diagnostics...
2021-06-04 13:25:46 | [train_policy] epoch #103 | Optimizing policy...
2021-06-04 13:25:46 | [train_policy] epoch #103 | Computing loss before
2021-06-04 13:25:46 | [train_policy] epoch #103 | Computing KL before
2021-06-04 13:25:46 | [train_policy] epoch #103 | Optimizing
2021-06-04 13:25:46 | [train_policy] epoch #103 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:46 | [train_policy] epoch #103 | computing loss before
2021-06-04 13:25:46 | [train_policy] epoch #103 | computing gradient
2021-06-04 13:25:46 | [train_policy] epoch #103 | gradient computed
2021-06-04 13:25:46 | [train_policy] epoch #103 | computing descent direction
2021-06-04 13:25:46 | [train_policy] epoch #103 | descent direction computed
2021-06-04 13:25:46 | [train_policy] epoch #103 | backtrack iters: 0
2021-06-04 13:25:46 | [train_policy] epoch #103 | optimization finished
2021-06-04 13:25:46 | [train_policy] epoch #103 | Computing KL after
2021-06-04 13:25:46 | [train_policy] epoch #103 | Computing loss after
2021-06-04 13:25:46 | [train_policy] epoch #103 | Fitting baseline...
2021-06-04 13:25:46 | [train_policy] epoch #103 | Saving snapshot...
2021-06-04 13:25:46 | [train_policy] epoch #103 | Saved
2021-06-04 13:25:46 | [train_policy] epoch #103 | Time 86.10 s
2021-06-04 13:25:46 | [train_policy] epoch #103 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288722
Evaluation/AverageDiscountedReturn         -157.25
Evaluation/AverageReturn                   -157.25
Evaluation/CompletionRate                     0
Evaluation/Iteration                        103
Evaluation/MaxReturn                        -35.0904
Evaluation/MinReturn                      -2064.04
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        456.463
Extras/EpisodeRewardMean                   -148.209
LinearFeatureBaseline/ExplainedVariance       0.146552
PolicyExecTime                                0.230669
ProcessExecTime                               0.0317585
TotalEnvSteps                            105248
policy/Entropy                                1.06112
policy/KL                                     0.00952749
policy/KLBefore                               0
policy/LossAfter                             -0.0247702
policy/LossBefore                             3.29828e-09
policy/Perplexity                             2.88961
policy/dLoss                                  0.0247702
---------------------------------------  ----------------
2021-06-04 13:25:46 | [train_policy] epoch #104 | Obtaining samples for iteration 104...
2021-06-04 13:25:46 | [train_policy] epoch #104 | Logging diagnostics...
2021-06-04 13:25:46 | [train_policy] epoch #104 | Optimizing policy...
2021-06-04 13:25:46 | [train_policy] epoch #104 | Computing loss before
2021-06-04 13:25:46 | [train_policy] epoch #104 | Computing KL before
2021-06-04 13:25:46 | [train_policy] epoch #104 | Optimizing
2021-06-04 13:25:46 | [train_policy] epoch #104 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:46 | [train_policy] epoch #104 | computing loss before
2021-06-04 13:25:46 | [train_policy] epoch #104 | computing gradient
2021-06-04 13:25:46 | [train_policy] epoch #104 | gradient computed
2021-06-04 13:25:46 | [train_policy] epoch #104 | computing descent direction
2021-06-04 13:25:46 | [train_policy] epoch #104 | descent direction computed
2021-06-04 13:25:46 | [train_policy] epoch #104 | backtrack iters: 0
2021-06-04 13:25:46 | [train_policy] epoch #104 | optimization finished
2021-06-04 13:25:46 | [train_policy] epoch #104 | Computing KL after
2021-06-04 13:25:46 | [train_policy] epoch #104 | Computing loss after
2021-06-04 13:25:46 | [train_policy] epoch #104 | Fitting baseline...
2021-06-04 13:25:46 | [train_policy] epoch #104 | Saving snapshot...
2021-06-04 13:25:46 | [train_policy] epoch #104 | Saved
2021-06-04 13:25:46 | [train_policy] epoch #104 | Time 86.88 s
2021-06-04 13:25:46 | [train_policy] epoch #104 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285521
Evaluation/AverageDiscountedReturn          -68.8007
Evaluation/AverageReturn                    -68.8007
Evaluation/CompletionRate                     0
Evaluation/Iteration                        104
Evaluation/MaxReturn                        -34.1953
Evaluation/MinReturn                      -2060.49
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.876
Extras/EpisodeRewardMean                    -87.6037
LinearFeatureBaseline/ExplainedVariance      -0.291677
PolicyExecTime                                0.222413
ProcessExecTime                               0.0314441
TotalEnvSteps                            106260
policy/Entropy                                1.07387
policy/KL                                     0.00961584
policy/KLBefore                               0
policy/LossAfter                             -0.0268319
policy/LossBefore                             1.86117e-08
policy/Perplexity                             2.92669
policy/dLoss                                  0.0268319
---------------------------------------  ----------------
2021-06-04 13:25:47 | [train_policy] epoch #105 | Obtaining samples for iteration 105...
2021-06-04 13:25:47 | [train_policy] epoch #105 | Logging diagnostics...
2021-06-04 13:25:47 | [train_policy] epoch #105 | Optimizing policy...
2021-06-04 13:25:47 | [train_policy] epoch #105 | Computing loss before
2021-06-04 13:25:47 | [train_policy] epoch #105 | Computing KL before
2021-06-04 13:25:47 | [train_policy] epoch #105 | Optimizing
2021-06-04 13:25:47 | [train_policy] epoch #105 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:47 | [train_policy] epoch #105 | computing loss before
2021-06-04 13:25:47 | [train_policy] epoch #105 | computing gradient
2021-06-04 13:25:47 | [train_policy] epoch #105 | gradient computed
2021-06-04 13:25:47 | [train_policy] epoch #105 | computing descent direction
2021-06-04 13:25:47 | [train_policy] epoch #105 | descent direction computed
2021-06-04 13:25:47 | [train_policy] epoch #105 | backtrack iters: 0
2021-06-04 13:25:47 | [train_policy] epoch #105 | optimization finished
2021-06-04 13:25:47 | [train_policy] epoch #105 | Computing KL after
2021-06-04 13:25:47 | [train_policy] epoch #105 | Computing loss after
2021-06-04 13:25:47 | [train_policy] epoch #105 | Fitting baseline...
2021-06-04 13:25:47 | [train_policy] epoch #105 | Saving snapshot...
2021-06-04 13:25:47 | [train_policy] epoch #105 | Saved
2021-06-04 13:25:47 | [train_policy] epoch #105 | Time 87.68 s
2021-06-04 13:25:47 | [train_policy] epoch #105 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.287414
Evaluation/AverageDiscountedReturn          -47.1782
Evaluation/AverageReturn                    -47.1782
Evaluation/CompletionRate                     0
Evaluation/Iteration                        105
Evaluation/MaxReturn                        -33.7609
Evaluation/MinReturn                        -64.8099
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.16635
Extras/EpisodeRewardMean                    -67.258
LinearFeatureBaseline/ExplainedVariance     -20.4147
PolicyExecTime                                0.230952
ProcessExecTime                               0.0315449
TotalEnvSteps                            107272
policy/Entropy                                1.04671
policy/KL                                     0.00960864
policy/KLBefore                               0
policy/LossAfter                             -0.0229784
policy/LossBefore                             2.7093e-08
policy/Perplexity                             2.84827
policy/dLoss                                  0.0229784
---------------------------------------  ---------------
2021-06-04 13:25:47 | [train_policy] epoch #106 | Obtaining samples for iteration 106...
2021-06-04 13:25:48 | [train_policy] epoch #106 | Logging diagnostics...
2021-06-04 13:25:48 | [train_policy] epoch #106 | Optimizing policy...
2021-06-04 13:25:48 | [train_policy] epoch #106 | Computing loss before
2021-06-04 13:25:48 | [train_policy] epoch #106 | Computing KL before
2021-06-04 13:25:48 | [train_policy] epoch #106 | Optimizing
2021-06-04 13:25:48 | [train_policy] epoch #106 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:48 | [train_policy] epoch #106 | computing loss before
2021-06-04 13:25:48 | [train_policy] epoch #106 | computing gradient
2021-06-04 13:25:48 | [train_policy] epoch #106 | gradient computed
2021-06-04 13:25:48 | [train_policy] epoch #106 | computing descent direction
2021-06-04 13:25:48 | [train_policy] epoch #106 | descent direction computed
2021-06-04 13:25:48 | [train_policy] epoch #106 | backtrack iters: 1
2021-06-04 13:25:48 | [train_policy] epoch #106 | optimization finished
2021-06-04 13:25:48 | [train_policy] epoch #106 | Computing KL after
2021-06-04 13:25:48 | [train_policy] epoch #106 | Computing loss after
2021-06-04 13:25:48 | [train_policy] epoch #106 | Fitting baseline...
2021-06-04 13:25:48 | [train_policy] epoch #106 | Saving snapshot...
2021-06-04 13:25:48 | [train_policy] epoch #106 | Saved
2021-06-04 13:25:48 | [train_policy] epoch #106 | Time 88.46 s
2021-06-04 13:25:48 | [train_policy] epoch #106 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285522
Evaluation/AverageDiscountedReturn          -96.8843
Evaluation/AverageReturn                    -96.8843
Evaluation/CompletionRate                     0
Evaluation/Iteration                        106
Evaluation/MaxReturn                        -34.1303
Evaluation/MinReturn                      -2061.42
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        297.38
Extras/EpisodeRewardMean                    -92.7082
LinearFeatureBaseline/ExplainedVariance       0.00703284
PolicyExecTime                                0.216131
ProcessExecTime                               0.0313475
TotalEnvSteps                            108284
policy/Entropy                                1.05608
policy/KL                                     0.00686316
policy/KLBefore                               0
policy/LossAfter                             -0.0218389
policy/LossBefore                             7.06774e-09
policy/Perplexity                             2.87508
policy/dLoss                                  0.0218389
---------------------------------------  ----------------
2021-06-04 13:25:48 | [train_policy] epoch #107 | Obtaining samples for iteration 107...
2021-06-04 13:25:49 | [train_policy] epoch #107 | Logging diagnostics...
2021-06-04 13:25:49 | [train_policy] epoch #107 | Optimizing policy...
2021-06-04 13:25:49 | [train_policy] epoch #107 | Computing loss before
2021-06-04 13:25:49 | [train_policy] epoch #107 | Computing KL before
2021-06-04 13:25:49 | [train_policy] epoch #107 | Optimizing
2021-06-04 13:25:49 | [train_policy] epoch #107 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:49 | [train_policy] epoch #107 | computing loss before
2021-06-04 13:25:49 | [train_policy] epoch #107 | computing gradient
2021-06-04 13:25:49 | [train_policy] epoch #107 | gradient computed
2021-06-04 13:25:49 | [train_policy] epoch #107 | computing descent direction
2021-06-04 13:25:49 | [train_policy] epoch #107 | descent direction computed
2021-06-04 13:25:49 | [train_policy] epoch #107 | backtrack iters: 1
2021-06-04 13:25:49 | [train_policy] epoch #107 | optimization finished
2021-06-04 13:25:49 | [train_policy] epoch #107 | Computing KL after
2021-06-04 13:25:49 | [train_policy] epoch #107 | Computing loss after
2021-06-04 13:25:49 | [train_policy] epoch #107 | Fitting baseline...
2021-06-04 13:25:49 | [train_policy] epoch #107 | Saving snapshot...
2021-06-04 13:25:49 | [train_policy] epoch #107 | Saved
2021-06-04 13:25:49 | [train_policy] epoch #107 | Time 89.26 s
2021-06-04 13:25:49 | [train_policy] epoch #107 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285949
Evaluation/AverageDiscountedReturn          -47.3058
Evaluation/AverageReturn                    -47.3058
Evaluation/CompletionRate                     0
Evaluation/Iteration                        107
Evaluation/MaxReturn                        -36.9328
Evaluation/MinReturn                       -115.693
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.32938
Extras/EpisodeRewardMean                    -47.165
LinearFeatureBaseline/ExplainedVariance     -45.7642
PolicyExecTime                                0.214436
ProcessExecTime                               0.0314813
TotalEnvSteps                            109296
policy/Entropy                                1.03221
policy/KL                                     0.00664996
policy/KLBefore                               0
policy/LossAfter                             -0.0181528
policy/LossBefore                             1.64914e-09
policy/Perplexity                             2.80726
policy/dLoss                                  0.0181528
---------------------------------------  ----------------
2021-06-04 13:25:49 | [train_policy] epoch #108 | Obtaining samples for iteration 108...
2021-06-04 13:25:49 | [train_policy] epoch #108 | Logging diagnostics...
2021-06-04 13:25:49 | [train_policy] epoch #108 | Optimizing policy...
2021-06-04 13:25:49 | [train_policy] epoch #108 | Computing loss before
2021-06-04 13:25:50 | [train_policy] epoch #108 | Computing KL before
2021-06-04 13:25:50 | [train_policy] epoch #108 | Optimizing
2021-06-04 13:25:50 | [train_policy] epoch #108 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:50 | [train_policy] epoch #108 | computing loss before
2021-06-04 13:25:50 | [train_policy] epoch #108 | computing gradient
2021-06-04 13:25:50 | [train_policy] epoch #108 | gradient computed
2021-06-04 13:25:50 | [train_policy] epoch #108 | computing descent direction
2021-06-04 13:25:50 | [train_policy] epoch #108 | descent direction computed
2021-06-04 13:25:50 | [train_policy] epoch #108 | backtrack iters: 0
2021-06-04 13:25:50 | [train_policy] epoch #108 | optimization finished
2021-06-04 13:25:50 | [train_policy] epoch #108 | Computing KL after
2021-06-04 13:25:50 | [train_policy] epoch #108 | Computing loss after
2021-06-04 13:25:50 | [train_policy] epoch #108 | Fitting baseline...
2021-06-04 13:25:50 | [train_policy] epoch #108 | Saving snapshot...
2021-06-04 13:25:50 | [train_policy] epoch #108 | Saved
2021-06-04 13:25:50 | [train_policy] epoch #108 | Time 90.04 s
2021-06-04 13:25:50 | [train_policy] epoch #108 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.28465
Evaluation/AverageDiscountedReturn          -47.5862
Evaluation/AverageReturn                    -47.5862
Evaluation/CompletionRate                     0
Evaluation/Iteration                        108
Evaluation/MaxReturn                        -37.0933
Evaluation/MinReturn                        -67.458
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.41677
Extras/EpisodeRewardMean                    -47.4622
LinearFeatureBaseline/ExplainedVariance       0.847627
PolicyExecTime                                0.213655
ProcessExecTime                               0.0312266
TotalEnvSteps                            110308
policy/Entropy                                0.992229
policy/KL                                     0.00998803
policy/KLBefore                               0
policy/LossAfter                             -0.0328588
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             2.69724
policy/dLoss                                  0.0328588
---------------------------------------  ----------------
2021-06-04 13:25:50 | [train_policy] epoch #109 | Obtaining samples for iteration 109...
2021-06-04 13:25:50 | [train_policy] epoch #109 | Logging diagnostics...
2021-06-04 13:25:50 | [train_policy] epoch #109 | Optimizing policy...
2021-06-04 13:25:50 | [train_policy] epoch #109 | Computing loss before
2021-06-04 13:25:50 | [train_policy] epoch #109 | Computing KL before
2021-06-04 13:25:50 | [train_policy] epoch #109 | Optimizing
2021-06-04 13:25:50 | [train_policy] epoch #109 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:50 | [train_policy] epoch #109 | computing loss before
2021-06-04 13:25:50 | [train_policy] epoch #109 | computing gradient
2021-06-04 13:25:50 | [train_policy] epoch #109 | gradient computed
2021-06-04 13:25:50 | [train_policy] epoch #109 | computing descent direction
2021-06-04 13:25:50 | [train_policy] epoch #109 | descent direction computed
2021-06-04 13:25:50 | [train_policy] epoch #109 | backtrack iters: 0
2021-06-04 13:25:50 | [train_policy] epoch #109 | optimization finished
2021-06-04 13:25:50 | [train_policy] epoch #109 | Computing KL after
2021-06-04 13:25:50 | [train_policy] epoch #109 | Computing loss after
2021-06-04 13:25:50 | [train_policy] epoch #109 | Fitting baseline...
2021-06-04 13:25:50 | [train_policy] epoch #109 | Saving snapshot...
2021-06-04 13:25:50 | [train_policy] epoch #109 | Saved
2021-06-04 13:25:50 | [train_policy] epoch #109 | Time 90.84 s
2021-06-04 13:25:50 | [train_policy] epoch #109 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.289967
Evaluation/AverageDiscountedReturn          -49.195
Evaluation/AverageReturn                    -49.195
Evaluation/CompletionRate                     0
Evaluation/Iteration                        109
Evaluation/MaxReturn                        -35.9874
Evaluation/MinReturn                       -177.268
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         16.5418
Extras/EpisodeRewardMean                    -49.1416
LinearFeatureBaseline/ExplainedVariance       0.638079
PolicyExecTime                                0.234369
ProcessExecTime                               0.0319672
TotalEnvSteps                            111320
policy/Entropy                                0.987511
policy/KL                                     0.00969542
policy/KLBefore                               0
policy/LossAfter                             -0.0161423
policy/LossBefore                             1.88473e-09
policy/Perplexity                             2.68454
policy/dLoss                                  0.0161423
---------------------------------------  ----------------
2021-06-04 13:25:50 | [train_policy] epoch #110 | Obtaining samples for iteration 110...
2021-06-04 13:25:51 | [train_policy] epoch #110 | Logging diagnostics...
2021-06-04 13:25:51 | [train_policy] epoch #110 | Optimizing policy...
2021-06-04 13:25:51 | [train_policy] epoch #110 | Computing loss before
2021-06-04 13:25:51 | [train_policy] epoch #110 | Computing KL before
2021-06-04 13:25:51 | [train_policy] epoch #110 | Optimizing
2021-06-04 13:25:51 | [train_policy] epoch #110 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:51 | [train_policy] epoch #110 | computing loss before
2021-06-04 13:25:51 | [train_policy] epoch #110 | computing gradient
2021-06-04 13:25:51 | [train_policy] epoch #110 | gradient computed
2021-06-04 13:25:51 | [train_policy] epoch #110 | computing descent direction
2021-06-04 13:25:51 | [train_policy] epoch #110 | descent direction computed
2021-06-04 13:25:51 | [train_policy] epoch #110 | backtrack iters: 0
2021-06-04 13:25:51 | [train_policy] epoch #110 | optimization finished
2021-06-04 13:25:51 | [train_policy] epoch #110 | Computing KL after
2021-06-04 13:25:51 | [train_policy] epoch #110 | Computing loss after
2021-06-04 13:25:51 | [train_policy] epoch #110 | Fitting baseline...
2021-06-04 13:25:51 | [train_policy] epoch #110 | Saving snapshot...
2021-06-04 13:25:51 | [train_policy] epoch #110 | Saved
2021-06-04 13:25:51 | [train_policy] epoch #110 | Time 91.65 s
2021-06-04 13:25:51 | [train_policy] epoch #110 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287768
Evaluation/AverageDiscountedReturn          -47.6653
Evaluation/AverageReturn                    -47.6653
Evaluation/CompletionRate                     0
Evaluation/Iteration                        110
Evaluation/MaxReturn                        -35.3029
Evaluation/MinReturn                       -103.099
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.23344
Extras/EpisodeRewardMean                    -47.5931
LinearFeatureBaseline/ExplainedVariance       0.85061
PolicyExecTime                                0.233045
ProcessExecTime                               0.0315928
TotalEnvSteps                            112332
policy/Entropy                                1.00842
policy/KL                                     0.00888978
policy/KLBefore                               0
policy/LossAfter                             -0.0250356
policy/LossBefore                             2.35591e-10
policy/Perplexity                             2.74126
policy/dLoss                                  0.0250356
---------------------------------------  ----------------
2021-06-04 13:25:51 | [train_policy] epoch #111 | Obtaining samples for iteration 111...
2021-06-04 13:25:52 | [train_policy] epoch #111 | Logging diagnostics...
2021-06-04 13:25:52 | [train_policy] epoch #111 | Optimizing policy...
2021-06-04 13:25:52 | [train_policy] epoch #111 | Computing loss before
2021-06-04 13:25:52 | [train_policy] epoch #111 | Computing KL before
2021-06-04 13:25:52 | [train_policy] epoch #111 | Optimizing
2021-06-04 13:25:52 | [train_policy] epoch #111 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:52 | [train_policy] epoch #111 | computing loss before
2021-06-04 13:25:52 | [train_policy] epoch #111 | computing gradient
2021-06-04 13:25:52 | [train_policy] epoch #111 | gradient computed
2021-06-04 13:25:52 | [train_policy] epoch #111 | computing descent direction
2021-06-04 13:25:52 | [train_policy] epoch #111 | descent direction computed
2021-06-04 13:25:52 | [train_policy] epoch #111 | backtrack iters: 0
2021-06-04 13:25:52 | [train_policy] epoch #111 | optimization finished
2021-06-04 13:25:52 | [train_policy] epoch #111 | Computing KL after
2021-06-04 13:25:52 | [train_policy] epoch #111 | Computing loss after
2021-06-04 13:25:52 | [train_policy] epoch #111 | Fitting baseline...
2021-06-04 13:25:52 | [train_policy] epoch #111 | Saving snapshot...
2021-06-04 13:25:52 | [train_policy] epoch #111 | Saved
2021-06-04 13:25:52 | [train_policy] epoch #111 | Time 92.46 s
2021-06-04 13:25:52 | [train_policy] epoch #111 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.290809
Evaluation/AverageDiscountedReturn          -48.1205
Evaluation/AverageReturn                    -48.1205
Evaluation/CompletionRate                     0
Evaluation/Iteration                        111
Evaluation/MaxReturn                        -36.8698
Evaluation/MinReturn                        -89.7336
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.36196
Extras/EpisodeRewardMean                    -47.7627
LinearFeatureBaseline/ExplainedVariance       0.907045
PolicyExecTime                                0.238573
ProcessExecTime                               0.0319002
TotalEnvSteps                            113344
policy/Entropy                                1.01284
policy/KL                                     0.00952422
policy/KLBefore                               0
policy/LossAfter                             -0.015899
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             2.75341
policy/dLoss                                  0.015899
---------------------------------------  ----------------
2021-06-04 13:25:52 | [train_policy] epoch #112 | Obtaining samples for iteration 112...
2021-06-04 13:25:53 | [train_policy] epoch #112 | Logging diagnostics...
2021-06-04 13:25:53 | [train_policy] epoch #112 | Optimizing policy...
2021-06-04 13:25:53 | [train_policy] epoch #112 | Computing loss before
2021-06-04 13:25:53 | [train_policy] epoch #112 | Computing KL before
2021-06-04 13:25:53 | [train_policy] epoch #112 | Optimizing
2021-06-04 13:25:53 | [train_policy] epoch #112 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:53 | [train_policy] epoch #112 | computing loss before
2021-06-04 13:25:53 | [train_policy] epoch #112 | computing gradient
2021-06-04 13:25:53 | [train_policy] epoch #112 | gradient computed
2021-06-04 13:25:53 | [train_policy] epoch #112 | computing descent direction
2021-06-04 13:25:53 | [train_policy] epoch #112 | descent direction computed
2021-06-04 13:25:53 | [train_policy] epoch #112 | backtrack iters: 1
2021-06-04 13:25:53 | [train_policy] epoch #112 | optimization finished
2021-06-04 13:25:53 | [train_policy] epoch #112 | Computing KL after
2021-06-04 13:25:53 | [train_policy] epoch #112 | Computing loss after
2021-06-04 13:25:53 | [train_policy] epoch #112 | Fitting baseline...
2021-06-04 13:25:53 | [train_policy] epoch #112 | Saving snapshot...
2021-06-04 13:25:53 | [train_policy] epoch #112 | Saved
2021-06-04 13:25:53 | [train_policy] epoch #112 | Time 93.27 s
2021-06-04 13:25:53 | [train_policy] epoch #112 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.289373
Evaluation/AverageDiscountedReturn          -50.1961
Evaluation/AverageReturn                    -50.1961
Evaluation/CompletionRate                     0
Evaluation/Iteration                        112
Evaluation/MaxReturn                        -33.5521
Evaluation/MinReturn                       -335.226
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         31.2635
Extras/EpisodeRewardMean                    -50.0836
LinearFeatureBaseline/ExplainedVariance       0.233878
PolicyExecTime                                0.230916
ProcessExecTime                               0.0318704
TotalEnvSteps                            114356
policy/Entropy                                1.00507
policy/KL                                     0.0064153
policy/KLBefore                               0
policy/LossAfter                             -0.0208224
policy/LossBefore                            -1.64914e-08
policy/Perplexity                             2.73209
policy/dLoss                                  0.0208224
---------------------------------------  ----------------
2021-06-04 13:25:53 | [train_policy] epoch #113 | Obtaining samples for iteration 113...
2021-06-04 13:25:54 | [train_policy] epoch #113 | Logging diagnostics...
2021-06-04 13:25:54 | [train_policy] epoch #113 | Optimizing policy...
2021-06-04 13:25:54 | [train_policy] epoch #113 | Computing loss before
2021-06-04 13:25:54 | [train_policy] epoch #113 | Computing KL before
2021-06-04 13:25:54 | [train_policy] epoch #113 | Optimizing
2021-06-04 13:25:54 | [train_policy] epoch #113 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:54 | [train_policy] epoch #113 | computing loss before
2021-06-04 13:25:54 | [train_policy] epoch #113 | computing gradient
2021-06-04 13:25:54 | [train_policy] epoch #113 | gradient computed
2021-06-04 13:25:54 | [train_policy] epoch #113 | computing descent direction
2021-06-04 13:25:54 | [train_policy] epoch #113 | descent direction computed
2021-06-04 13:25:54 | [train_policy] epoch #113 | backtrack iters: 1
2021-06-04 13:25:54 | [train_policy] epoch #113 | optimization finished
2021-06-04 13:25:54 | [train_policy] epoch #113 | Computing KL after
2021-06-04 13:25:54 | [train_policy] epoch #113 | Computing loss after
2021-06-04 13:25:54 | [train_policy] epoch #113 | Fitting baseline...
2021-06-04 13:25:54 | [train_policy] epoch #113 | Saving snapshot...
2021-06-04 13:25:54 | [train_policy] epoch #113 | Saved
2021-06-04 13:25:54 | [train_policy] epoch #113 | Time 94.06 s
2021-06-04 13:25:54 | [train_policy] epoch #113 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285717
Evaluation/AverageDiscountedReturn          -47.7012
Evaluation/AverageReturn                    -47.7012
Evaluation/CompletionRate                     0
Evaluation/Iteration                        113
Evaluation/MaxReturn                        -35.6141
Evaluation/MinReturn                        -97.7347
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.64144
Extras/EpisodeRewardMean                    -48.1015
LinearFeatureBaseline/ExplainedVariance       0.429971
PolicyExecTime                                0.223296
ProcessExecTime                               0.031323
TotalEnvSteps                            115368
policy/Entropy                                0.996868
policy/KL                                     0.00641884
policy/KLBefore                               0
policy/LossAfter                             -0.0192789
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             2.70978
policy/dLoss                                  0.0192788
---------------------------------------  ----------------
2021-06-04 13:25:54 | [train_policy] epoch #114 | Obtaining samples for iteration 114...
2021-06-04 13:25:54 | [train_policy] epoch #114 | Logging diagnostics...
2021-06-04 13:25:54 | [train_policy] epoch #114 | Optimizing policy...
2021-06-04 13:25:54 | [train_policy] epoch #114 | Computing loss before
2021-06-04 13:25:54 | [train_policy] epoch #114 | Computing KL before
2021-06-04 13:25:54 | [train_policy] epoch #114 | Optimizing
2021-06-04 13:25:54 | [train_policy] epoch #114 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:54 | [train_policy] epoch #114 | computing loss before
2021-06-04 13:25:54 | [train_policy] epoch #114 | computing gradient
2021-06-04 13:25:54 | [train_policy] epoch #114 | gradient computed
2021-06-04 13:25:54 | [train_policy] epoch #114 | computing descent direction
2021-06-04 13:25:54 | [train_policy] epoch #114 | descent direction computed
2021-06-04 13:25:54 | [train_policy] epoch #114 | backtrack iters: 1
2021-06-04 13:25:54 | [train_policy] epoch #114 | optimization finished
2021-06-04 13:25:54 | [train_policy] epoch #114 | Computing KL after
2021-06-04 13:25:54 | [train_policy] epoch #114 | Computing loss after
2021-06-04 13:25:54 | [train_policy] epoch #114 | Fitting baseline...
2021-06-04 13:25:54 | [train_policy] epoch #114 | Saving snapshot...
2021-06-04 13:25:54 | [train_policy] epoch #114 | Saved
2021-06-04 13:25:54 | [train_policy] epoch #114 | Time 94.85 s
2021-06-04 13:25:54 | [train_policy] epoch #114 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284752
Evaluation/AverageDiscountedReturn          -47.3602
Evaluation/AverageReturn                    -47.3602
Evaluation/CompletionRate                     0
Evaluation/Iteration                        114
Evaluation/MaxReturn                        -36.985
Evaluation/MinReturn                        -89.4296
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.25898
Extras/EpisodeRewardMean                    -47.5267
LinearFeatureBaseline/ExplainedVariance       0.885067
PolicyExecTime                                0.228554
ProcessExecTime                               0.0312505
TotalEnvSteps                            116380
policy/Entropy                                0.995638
policy/KL                                     0.00685264
policy/KLBefore                               0
policy/LossAfter                             -0.0167578
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             2.70645
policy/dLoss                                  0.0167578
---------------------------------------  ----------------
2021-06-04 13:25:54 | [train_policy] epoch #115 | Obtaining samples for iteration 115...
2021-06-04 13:25:55 | [train_policy] epoch #115 | Logging diagnostics...
2021-06-04 13:25:55 | [train_policy] epoch #115 | Optimizing policy...
2021-06-04 13:25:55 | [train_policy] epoch #115 | Computing loss before
2021-06-04 13:25:55 | [train_policy] epoch #115 | Computing KL before
2021-06-04 13:25:55 | [train_policy] epoch #115 | Optimizing
2021-06-04 13:25:55 | [train_policy] epoch #115 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:55 | [train_policy] epoch #115 | computing loss before
2021-06-04 13:25:55 | [train_policy] epoch #115 | computing gradient
2021-06-04 13:25:55 | [train_policy] epoch #115 | gradient computed
2021-06-04 13:25:55 | [train_policy] epoch #115 | computing descent direction
2021-06-04 13:25:55 | [train_policy] epoch #115 | descent direction computed
2021-06-04 13:25:55 | [train_policy] epoch #115 | backtrack iters: 1
2021-06-04 13:25:55 | [train_policy] epoch #115 | optimization finished
2021-06-04 13:25:55 | [train_policy] epoch #115 | Computing KL after
2021-06-04 13:25:55 | [train_policy] epoch #115 | Computing loss after
2021-06-04 13:25:55 | [train_policy] epoch #115 | Fitting baseline...
2021-06-04 13:25:55 | [train_policy] epoch #115 | Saving snapshot...
2021-06-04 13:25:55 | [train_policy] epoch #115 | Saved
2021-06-04 13:25:55 | [train_policy] epoch #115 | Time 95.65 s
2021-06-04 13:25:55 | [train_policy] epoch #115 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.286918
Evaluation/AverageDiscountedReturn          -90.8776
Evaluation/AverageReturn                    -90.8776
Evaluation/CompletionRate                     0
Evaluation/Iteration                        115
Evaluation/MaxReturn                        -34.3465
Evaluation/MinReturn                      -2061.37
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        293.807
Extras/EpisodeRewardMean                    -87.4545
LinearFeatureBaseline/ExplainedVariance       0.0110277
PolicyExecTime                                0.219322
ProcessExecTime                               0.0315602
TotalEnvSteps                            117392
policy/Entropy                                0.977488
policy/KL                                     0.00751922
policy/KLBefore                               0
policy/LossAfter                             -0.0262491
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             2.65777
policy/dLoss                                  0.0262491
---------------------------------------  ---------------
2021-06-04 13:25:55 | [train_policy] epoch #116 | Obtaining samples for iteration 116...
2021-06-04 13:25:56 | [train_policy] epoch #116 | Logging diagnostics...
2021-06-04 13:25:56 | [train_policy] epoch #116 | Optimizing policy...
2021-06-04 13:25:56 | [train_policy] epoch #116 | Computing loss before
2021-06-04 13:25:56 | [train_policy] epoch #116 | Computing KL before
2021-06-04 13:25:56 | [train_policy] epoch #116 | Optimizing
2021-06-04 13:25:56 | [train_policy] epoch #116 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:56 | [train_policy] epoch #116 | computing loss before
2021-06-04 13:25:56 | [train_policy] epoch #116 | computing gradient
2021-06-04 13:25:56 | [train_policy] epoch #116 | gradient computed
2021-06-04 13:25:56 | [train_policy] epoch #116 | computing descent direction
2021-06-04 13:25:56 | [train_policy] epoch #116 | descent direction computed
2021-06-04 13:25:56 | [train_policy] epoch #116 | backtrack iters: 1
2021-06-04 13:25:56 | [train_policy] epoch #116 | optimization finished
2021-06-04 13:25:56 | [train_policy] epoch #116 | Computing KL after
2021-06-04 13:25:56 | [train_policy] epoch #116 | Computing loss after
2021-06-04 13:25:56 | [train_policy] epoch #116 | Fitting baseline...
2021-06-04 13:25:56 | [train_policy] epoch #116 | Saving snapshot...
2021-06-04 13:25:56 | [train_policy] epoch #116 | Saved
2021-06-04 13:25:56 | [train_policy] epoch #116 | Time 96.45 s
2021-06-04 13:25:56 | [train_policy] epoch #116 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284915
Evaluation/AverageDiscountedReturn          -47.128
Evaluation/AverageReturn                    -47.128
Evaluation/CompletionRate                     0
Evaluation/Iteration                        116
Evaluation/MaxReturn                        -33.302
Evaluation/MinReturn                        -95.1863
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.32258
Extras/EpisodeRewardMean                    -47.245
LinearFeatureBaseline/ExplainedVariance     -72.3356
PolicyExecTime                                0.226948
ProcessExecTime                               0.0313768
TotalEnvSteps                            118404
policy/Entropy                                0.975664
policy/KL                                     0.00665107
policy/KLBefore                               0
policy/LossAfter                             -0.0182058
policy/LossBefore                            -7.24444e-09
policy/Perplexity                             2.65293
policy/dLoss                                  0.0182058
---------------------------------------  ----------------
2021-06-04 13:25:56 | [train_policy] epoch #117 | Obtaining samples for iteration 117...
2021-06-04 13:25:57 | [train_policy] epoch #117 | Logging diagnostics...
2021-06-04 13:25:57 | [train_policy] epoch #117 | Optimizing policy...
2021-06-04 13:25:57 | [train_policy] epoch #117 | Computing loss before
2021-06-04 13:25:57 | [train_policy] epoch #117 | Computing KL before
2021-06-04 13:25:57 | [train_policy] epoch #117 | Optimizing
2021-06-04 13:25:57 | [train_policy] epoch #117 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:57 | [train_policy] epoch #117 | computing loss before
2021-06-04 13:25:57 | [train_policy] epoch #117 | computing gradient
2021-06-04 13:25:57 | [train_policy] epoch #117 | gradient computed
2021-06-04 13:25:57 | [train_policy] epoch #117 | computing descent direction
2021-06-04 13:25:57 | [train_policy] epoch #117 | descent direction computed
2021-06-04 13:25:57 | [train_policy] epoch #117 | backtrack iters: 0
2021-06-04 13:25:57 | [train_policy] epoch #117 | optimization finished
2021-06-04 13:25:57 | [train_policy] epoch #117 | Computing KL after
2021-06-04 13:25:57 | [train_policy] epoch #117 | Computing loss after
2021-06-04 13:25:57 | [train_policy] epoch #117 | Fitting baseline...
2021-06-04 13:25:57 | [train_policy] epoch #117 | Saving snapshot...
2021-06-04 13:25:57 | [train_policy] epoch #117 | Saved
2021-06-04 13:25:57 | [train_policy] epoch #117 | Time 97.25 s
2021-06-04 13:25:57 | [train_policy] epoch #117 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284772
Evaluation/AverageDiscountedReturn          -46.3079
Evaluation/AverageReturn                    -46.3079
Evaluation/CompletionRate                     0
Evaluation/Iteration                        117
Evaluation/MaxReturn                        -34.5347
Evaluation/MinReturn                        -86.4234
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.26978
Extras/EpisodeRewardMean                    -46.2804
LinearFeatureBaseline/ExplainedVariance       0.920628
PolicyExecTime                                0.231638
ProcessExecTime                               0.0312696
TotalEnvSteps                            119416
policy/Entropy                                1.01875
policy/KL                                     0.00879891
policy/KLBefore                               0
policy/LossAfter                             -0.0162476
policy/LossBefore                             1.17796e-09
policy/Perplexity                             2.76974
policy/dLoss                                  0.0162476
---------------------------------------  ----------------
2021-06-04 13:25:57 | [train_policy] epoch #118 | Obtaining samples for iteration 118...
2021-06-04 13:25:57 | [train_policy] epoch #118 | Logging diagnostics...
2021-06-04 13:25:57 | [train_policy] epoch #118 | Optimizing policy...
2021-06-04 13:25:57 | [train_policy] epoch #118 | Computing loss before
2021-06-04 13:25:58 | [train_policy] epoch #118 | Computing KL before
2021-06-04 13:25:58 | [train_policy] epoch #118 | Optimizing
2021-06-04 13:25:58 | [train_policy] epoch #118 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:58 | [train_policy] epoch #118 | computing loss before
2021-06-04 13:25:58 | [train_policy] epoch #118 | computing gradient
2021-06-04 13:25:58 | [train_policy] epoch #118 | gradient computed
2021-06-04 13:25:58 | [train_policy] epoch #118 | computing descent direction
2021-06-04 13:25:58 | [train_policy] epoch #118 | descent direction computed
2021-06-04 13:25:58 | [train_policy] epoch #118 | backtrack iters: 1
2021-06-04 13:25:58 | [train_policy] epoch #118 | optimization finished
2021-06-04 13:25:58 | [train_policy] epoch #118 | Computing KL after
2021-06-04 13:25:58 | [train_policy] epoch #118 | Computing loss after
2021-06-04 13:25:58 | [train_policy] epoch #118 | Fitting baseline...
2021-06-04 13:25:58 | [train_policy] epoch #118 | Saving snapshot...
2021-06-04 13:25:58 | [train_policy] epoch #118 | Saved
2021-06-04 13:25:58 | [train_policy] epoch #118 | Time 98.04 s
2021-06-04 13:25:58 | [train_policy] epoch #118 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285722
Evaluation/AverageDiscountedReturn          -46.7039
Evaluation/AverageReturn                    -46.7039
Evaluation/CompletionRate                     0
Evaluation/Iteration                        118
Evaluation/MaxReturn                        -37.4385
Evaluation/MinReturn                        -78.7116
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.15206
Extras/EpisodeRewardMean                    -46.5115
LinearFeatureBaseline/ExplainedVariance       0.932581
PolicyExecTime                                0.225514
ProcessExecTime                               0.0313334
TotalEnvSteps                            120428
policy/Entropy                                1.00417
policy/KL                                     0.00670042
policy/KLBefore                               0
policy/LossAfter                             -0.0146616
policy/LossBefore                             1.13084e-08
policy/Perplexity                             2.72964
policy/dLoss                                  0.0146616
---------------------------------------  ----------------
2021-06-04 13:25:58 | [train_policy] epoch #119 | Obtaining samples for iteration 119...
2021-06-04 13:25:58 | [train_policy] epoch #119 | Logging diagnostics...
2021-06-04 13:25:58 | [train_policy] epoch #119 | Optimizing policy...
2021-06-04 13:25:58 | [train_policy] epoch #119 | Computing loss before
2021-06-04 13:25:58 | [train_policy] epoch #119 | Computing KL before
2021-06-04 13:25:58 | [train_policy] epoch #119 | Optimizing
2021-06-04 13:25:58 | [train_policy] epoch #119 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:58 | [train_policy] epoch #119 | computing loss before
2021-06-04 13:25:58 | [train_policy] epoch #119 | computing gradient
2021-06-04 13:25:58 | [train_policy] epoch #119 | gradient computed
2021-06-04 13:25:58 | [train_policy] epoch #119 | computing descent direction
2021-06-04 13:25:58 | [train_policy] epoch #119 | descent direction computed
2021-06-04 13:25:58 | [train_policy] epoch #119 | backtrack iters: 1
2021-06-04 13:25:58 | [train_policy] epoch #119 | optimization finished
2021-06-04 13:25:58 | [train_policy] epoch #119 | Computing KL after
2021-06-04 13:25:58 | [train_policy] epoch #119 | Computing loss after
2021-06-04 13:25:58 | [train_policy] epoch #119 | Fitting baseline...
2021-06-04 13:25:58 | [train_policy] epoch #119 | Saving snapshot...
2021-06-04 13:25:58 | [train_policy] epoch #119 | Saved
2021-06-04 13:25:58 | [train_policy] epoch #119 | Time 98.84 s
2021-06-04 13:25:58 | [train_policy] epoch #119 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288454
Evaluation/AverageDiscountedReturn          -47.1229
Evaluation/AverageReturn                    -47.1229
Evaluation/CompletionRate                     0
Evaluation/Iteration                        119
Evaluation/MaxReturn                        -37.2293
Evaluation/MinReturn                        -64.6948
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.0855
Extras/EpisodeRewardMean                    -47.1283
LinearFeatureBaseline/ExplainedVariance       0.948895
PolicyExecTime                                0.224398
ProcessExecTime                               0.0316865
TotalEnvSteps                            121440
policy/Entropy                                1.00284
policy/KL                                     0.00672092
policy/KLBefore                               0
policy/LossAfter                             -0.0188964
policy/LossBefore                            -2.34414e-08
policy/Perplexity                             2.72602
policy/dLoss                                  0.0188963
---------------------------------------  ----------------
2021-06-04 13:25:58 | [train_policy] epoch #120 | Obtaining samples for iteration 120...
2021-06-04 13:25:59 | [train_policy] epoch #120 | Logging diagnostics...
2021-06-04 13:25:59 | [train_policy] epoch #120 | Optimizing policy...
2021-06-04 13:25:59 | [train_policy] epoch #120 | Computing loss before
2021-06-04 13:25:59 | [train_policy] epoch #120 | Computing KL before
2021-06-04 13:25:59 | [train_policy] epoch #120 | Optimizing
2021-06-04 13:25:59 | [train_policy] epoch #120 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:25:59 | [train_policy] epoch #120 | computing loss before
2021-06-04 13:25:59 | [train_policy] epoch #120 | computing gradient
2021-06-04 13:25:59 | [train_policy] epoch #120 | gradient computed
2021-06-04 13:25:59 | [train_policy] epoch #120 | computing descent direction
2021-06-04 13:25:59 | [train_policy] epoch #120 | descent direction computed
2021-06-04 13:25:59 | [train_policy] epoch #120 | backtrack iters: 1
2021-06-04 13:25:59 | [train_policy] epoch #120 | optimization finished
2021-06-04 13:25:59 | [train_policy] epoch #120 | Computing KL after
2021-06-04 13:25:59 | [train_policy] epoch #120 | Computing loss after
2021-06-04 13:25:59 | [train_policy] epoch #120 | Fitting baseline...
2021-06-04 13:25:59 | [train_policy] epoch #120 | Saving snapshot...
2021-06-04 13:25:59 | [train_policy] epoch #120 | Saved
2021-06-04 13:25:59 | [train_policy] epoch #120 | Time 99.63 s
2021-06-04 13:25:59 | [train_policy] epoch #120 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284616
Evaluation/AverageDiscountedReturn          -47.3174
Evaluation/AverageReturn                    -47.3174
Evaluation/CompletionRate                     0
Evaluation/Iteration                        120
Evaluation/MaxReturn                        -36.7286
Evaluation/MinReturn                       -103.617
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.67303
Extras/EpisodeRewardMean                    -47.2425
LinearFeatureBaseline/ExplainedVariance       0.847005
PolicyExecTime                                0.227443
ProcessExecTime                               0.0312696
TotalEnvSteps                            122452
policy/Entropy                                1.00395
policy/KL                                     0.00698415
policy/KLBefore                               0
policy/LossAfter                             -0.0206264
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             2.72905
policy/dLoss                                  0.0206264
---------------------------------------  ----------------
2021-06-04 13:25:59 | [train_policy] epoch #121 | Obtaining samples for iteration 121...
2021-06-04 13:26:00 | [train_policy] epoch #121 | Logging diagnostics...
2021-06-04 13:26:00 | [train_policy] epoch #121 | Optimizing policy...
2021-06-04 13:26:00 | [train_policy] epoch #121 | Computing loss before
2021-06-04 13:26:00 | [train_policy] epoch #121 | Computing KL before
2021-06-04 13:26:00 | [train_policy] epoch #121 | Optimizing
2021-06-04 13:26:00 | [train_policy] epoch #121 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:00 | [train_policy] epoch #121 | computing loss before
2021-06-04 13:26:00 | [train_policy] epoch #121 | computing gradient
2021-06-04 13:26:00 | [train_policy] epoch #121 | gradient computed
2021-06-04 13:26:00 | [train_policy] epoch #121 | computing descent direction
2021-06-04 13:26:00 | [train_policy] epoch #121 | descent direction computed
2021-06-04 13:26:00 | [train_policy] epoch #121 | backtrack iters: 1
2021-06-04 13:26:00 | [train_policy] epoch #121 | optimization finished
2021-06-04 13:26:00 | [train_policy] epoch #121 | Computing KL after
2021-06-04 13:26:00 | [train_policy] epoch #121 | Computing loss after
2021-06-04 13:26:00 | [train_policy] epoch #121 | Fitting baseline...
2021-06-04 13:26:00 | [train_policy] epoch #121 | Saving snapshot...
2021-06-04 13:26:00 | [train_policy] epoch #121 | Saved
2021-06-04 13:26:00 | [train_policy] epoch #121 | Time 100.46 s
2021-06-04 13:26:00 | [train_policy] epoch #121 | EpochTime 0.81 s
---------------------------------------  ----------------
EnvExecTime                                   0.30974
Evaluation/AverageDiscountedReturn          -71.7816
Evaluation/AverageReturn                    -71.7816
Evaluation/CompletionRate                     0
Evaluation/Iteration                        121
Evaluation/MaxReturn                        -37.3706
Evaluation/MinReturn                      -2061.59
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.624
Extras/EpisodeRewardMean                    -70.2221
LinearFeatureBaseline/ExplainedVariance       0.0149599
PolicyExecTime                                0.236413
ProcessExecTime                               0.0342999
TotalEnvSteps                            123464
policy/Entropy                                1.01065
policy/KL                                     0.00718012
policy/KLBefore                               0
policy/LossAfter                             -0.0169918
policy/LossBefore                             6.12538e-09
policy/Perplexity                             2.74739
policy/dLoss                                  0.0169918
---------------------------------------  ----------------
2021-06-04 13:26:00 | [train_policy] epoch #122 | Obtaining samples for iteration 122...
2021-06-04 13:26:01 | [train_policy] epoch #122 | Logging diagnostics...
2021-06-04 13:26:01 | [train_policy] epoch #122 | Optimizing policy...
2021-06-04 13:26:01 | [train_policy] epoch #122 | Computing loss before
2021-06-04 13:26:01 | [train_policy] epoch #122 | Computing KL before
2021-06-04 13:26:01 | [train_policy] epoch #122 | Optimizing
2021-06-04 13:26:01 | [train_policy] epoch #122 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:01 | [train_policy] epoch #122 | computing loss before
2021-06-04 13:26:01 | [train_policy] epoch #122 | computing gradient
2021-06-04 13:26:01 | [train_policy] epoch #122 | gradient computed
2021-06-04 13:26:01 | [train_policy] epoch #122 | computing descent direction
2021-06-04 13:26:01 | [train_policy] epoch #122 | descent direction computed
2021-06-04 13:26:01 | [train_policy] epoch #122 | backtrack iters: 1
2021-06-04 13:26:01 | [train_policy] epoch #122 | optimization finished
2021-06-04 13:26:01 | [train_policy] epoch #122 | Computing KL after
2021-06-04 13:26:01 | [train_policy] epoch #122 | Computing loss after
2021-06-04 13:26:01 | [train_policy] epoch #122 | Fitting baseline...
2021-06-04 13:26:01 | [train_policy] epoch #122 | Saving snapshot...
2021-06-04 13:26:01 | [train_policy] epoch #122 | Saved
2021-06-04 13:26:01 | [train_policy] epoch #122 | Time 101.27 s
2021-06-04 13:26:01 | [train_policy] epoch #122 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288223
Evaluation/AverageDiscountedReturn          -45.7456
Evaluation/AverageReturn                    -45.7456
Evaluation/CompletionRate                     0
Evaluation/Iteration                        122
Evaluation/MaxReturn                        -34.0547
Evaluation/MinReturn                        -76.335
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.47955
Extras/EpisodeRewardMean                    -48.3555
LinearFeatureBaseline/ExplainedVariance     -10.901
PolicyExecTime                                0.232071
ProcessExecTime                               0.0317059
TotalEnvSteps                            124476
policy/Entropy                                1.00588
policy/KL                                     0.00643276
policy/KLBefore                               0
policy/LossAfter                             -0.0256452
policy/LossBefore                             3.76946e-09
policy/Perplexity                             2.73432
policy/dLoss                                  0.0256452
---------------------------------------  ----------------
2021-06-04 13:26:01 | [train_policy] epoch #123 | Obtaining samples for iteration 123...
2021-06-04 13:26:02 | [train_policy] epoch #123 | Logging diagnostics...
2021-06-04 13:26:02 | [train_policy] epoch #123 | Optimizing policy...
2021-06-04 13:26:02 | [train_policy] epoch #123 | Computing loss before
2021-06-04 13:26:02 | [train_policy] epoch #123 | Computing KL before
2021-06-04 13:26:02 | [train_policy] epoch #123 | Optimizing
2021-06-04 13:26:02 | [train_policy] epoch #123 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:02 | [train_policy] epoch #123 | computing loss before
2021-06-04 13:26:02 | [train_policy] epoch #123 | computing gradient
2021-06-04 13:26:02 | [train_policy] epoch #123 | gradient computed
2021-06-04 13:26:02 | [train_policy] epoch #123 | computing descent direction
2021-06-04 13:26:02 | [train_policy] epoch #123 | descent direction computed
2021-06-04 13:26:02 | [train_policy] epoch #123 | backtrack iters: 1
2021-06-04 13:26:02 | [train_policy] epoch #123 | optimization finished
2021-06-04 13:26:02 | [train_policy] epoch #123 | Computing KL after
2021-06-04 13:26:02 | [train_policy] epoch #123 | Computing loss after
2021-06-04 13:26:02 | [train_policy] epoch #123 | Fitting baseline...
2021-06-04 13:26:02 | [train_policy] epoch #123 | Saving snapshot...
2021-06-04 13:26:02 | [train_policy] epoch #123 | Saved
2021-06-04 13:26:02 | [train_policy] epoch #123 | Time 102.09 s
2021-06-04 13:26:02 | [train_policy] epoch #123 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.289277
Evaluation/AverageDiscountedReturn          -48.6146
Evaluation/AverageReturn                    -48.6146
Evaluation/CompletionRate                     0
Evaluation/Iteration                        123
Evaluation/MaxReturn                        -35.7859
Evaluation/MinReturn                        -92.0986
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.33285
Extras/EpisodeRewardMean                    -48.2438
LinearFeatureBaseline/ExplainedVariance       0.888554
PolicyExecTime                                0.248535
ProcessExecTime                               0.0317054
TotalEnvSteps                            125488
policy/Entropy                                0.985658
policy/KL                                     0.00666372
policy/KLBefore                               0
policy/LossAfter                             -0.0154644
policy/LossBefore                            -7.77452e-09
policy/Perplexity                             2.67957
policy/dLoss                                  0.0154644
---------------------------------------  ----------------
2021-06-04 13:26:02 | [train_policy] epoch #124 | Obtaining samples for iteration 124...
2021-06-04 13:26:02 | [train_policy] epoch #124 | Logging diagnostics...
2021-06-04 13:26:02 | [train_policy] epoch #124 | Optimizing policy...
2021-06-04 13:26:02 | [train_policy] epoch #124 | Computing loss before
2021-06-04 13:26:02 | [train_policy] epoch #124 | Computing KL before
2021-06-04 13:26:02 | [train_policy] epoch #124 | Optimizing
2021-06-04 13:26:02 | [train_policy] epoch #124 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:02 | [train_policy] epoch #124 | computing loss before
2021-06-04 13:26:02 | [train_policy] epoch #124 | computing gradient
2021-06-04 13:26:02 | [train_policy] epoch #124 | gradient computed
2021-06-04 13:26:02 | [train_policy] epoch #124 | computing descent direction
2021-06-04 13:26:02 | [train_policy] epoch #124 | descent direction computed
2021-06-04 13:26:02 | [train_policy] epoch #124 | backtrack iters: 0
2021-06-04 13:26:02 | [train_policy] epoch #124 | optimization finished
2021-06-04 13:26:02 | [train_policy] epoch #124 | Computing KL after
2021-06-04 13:26:02 | [train_policy] epoch #124 | Computing loss after
2021-06-04 13:26:02 | [train_policy] epoch #124 | Fitting baseline...
2021-06-04 13:26:02 | [train_policy] epoch #124 | Saving snapshot...
2021-06-04 13:26:02 | [train_policy] epoch #124 | Saved
2021-06-04 13:26:02 | [train_policy] epoch #124 | Time 102.87 s
2021-06-04 13:26:02 | [train_policy] epoch #124 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28488
Evaluation/AverageDiscountedReturn          -46.2757
Evaluation/AverageReturn                    -46.2757
Evaluation/CompletionRate                     0
Evaluation/Iteration                        124
Evaluation/MaxReturn                        -34.9002
Evaluation/MinReturn                        -78.3537
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.56835
Extras/EpisodeRewardMean                    -46.609
LinearFeatureBaseline/ExplainedVariance       0.923395
PolicyExecTime                                0.212756
ProcessExecTime                               0.0313334
TotalEnvSteps                            126500
policy/Entropy                                0.976871
policy/KL                                     0.0099948
policy/KLBefore                               0
policy/LossAfter                             -0.0250721
policy/LossBefore                            -1.12495e-08
policy/Perplexity                             2.65613
policy/dLoss                                  0.0250721
---------------------------------------  ----------------
2021-06-04 13:26:02 | [train_policy] epoch #125 | Obtaining samples for iteration 125...
2021-06-04 13:26:03 | [train_policy] epoch #125 | Logging diagnostics...
2021-06-04 13:26:03 | [train_policy] epoch #125 | Optimizing policy...
2021-06-04 13:26:03 | [train_policy] epoch #125 | Computing loss before
2021-06-04 13:26:03 | [train_policy] epoch #125 | Computing KL before
2021-06-04 13:26:03 | [train_policy] epoch #125 | Optimizing
2021-06-04 13:26:03 | [train_policy] epoch #125 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:03 | [train_policy] epoch #125 | computing loss before
2021-06-04 13:26:03 | [train_policy] epoch #125 | computing gradient
2021-06-04 13:26:03 | [train_policy] epoch #125 | gradient computed
2021-06-04 13:26:03 | [train_policy] epoch #125 | computing descent direction
2021-06-04 13:26:03 | [train_policy] epoch #125 | descent direction computed
2021-06-04 13:26:03 | [train_policy] epoch #125 | backtrack iters: 1
2021-06-04 13:26:03 | [train_policy] epoch #125 | optimization finished
2021-06-04 13:26:03 | [train_policy] epoch #125 | Computing KL after
2021-06-04 13:26:03 | [train_policy] epoch #125 | Computing loss after
2021-06-04 13:26:03 | [train_policy] epoch #125 | Fitting baseline...
2021-06-04 13:26:03 | [train_policy] epoch #125 | Saving snapshot...
2021-06-04 13:26:03 | [train_policy] epoch #125 | Saved
2021-06-04 13:26:03 | [train_policy] epoch #125 | Time 103.68 s
2021-06-04 13:26:03 | [train_policy] epoch #125 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.293666
Evaluation/AverageDiscountedReturn          -47.1295
Evaluation/AverageReturn                    -47.1295
Evaluation/CompletionRate                     0
Evaluation/Iteration                        125
Evaluation/MaxReturn                        -33.4576
Evaluation/MinReturn                        -79.1161
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.73988
Extras/EpisodeRewardMean                    -47.036
LinearFeatureBaseline/ExplainedVariance       0.942362
PolicyExecTime                                0.237741
ProcessExecTime                               0.032115
TotalEnvSteps                            127512
policy/Entropy                                0.932879
policy/KL                                     0.00677525
policy/KLBefore                               0
policy/LossAfter                             -0.0181863
policy/LossBefore                             2.8271e-09
policy/Perplexity                             2.54182
policy/dLoss                                  0.0181863
---------------------------------------  ---------------
2021-06-04 13:26:03 | [train_policy] epoch #126 | Obtaining samples for iteration 126...
2021-06-04 13:26:04 | [train_policy] epoch #126 | Logging diagnostics...
2021-06-04 13:26:04 | [train_policy] epoch #126 | Optimizing policy...
2021-06-04 13:26:04 | [train_policy] epoch #126 | Computing loss before
2021-06-04 13:26:04 | [train_policy] epoch #126 | Computing KL before
2021-06-04 13:26:04 | [train_policy] epoch #126 | Optimizing
2021-06-04 13:26:04 | [train_policy] epoch #126 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:04 | [train_policy] epoch #126 | computing loss before
2021-06-04 13:26:04 | [train_policy] epoch #126 | computing gradient
2021-06-04 13:26:04 | [train_policy] epoch #126 | gradient computed
2021-06-04 13:26:04 | [train_policy] epoch #126 | computing descent direction
2021-06-04 13:26:04 | [train_policy] epoch #126 | descent direction computed
2021-06-04 13:26:04 | [train_policy] epoch #126 | backtrack iters: 1
2021-06-04 13:26:04 | [train_policy] epoch #126 | optimization finished
2021-06-04 13:26:04 | [train_policy] epoch #126 | Computing KL after
2021-06-04 13:26:04 | [train_policy] epoch #126 | Computing loss after
2021-06-04 13:26:04 | [train_policy] epoch #126 | Fitting baseline...
2021-06-04 13:26:04 | [train_policy] epoch #126 | Saving snapshot...
2021-06-04 13:26:04 | [train_policy] epoch #126 | Saved
2021-06-04 13:26:04 | [train_policy] epoch #126 | Time 104.49 s
2021-06-04 13:26:04 | [train_policy] epoch #126 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285266
Evaluation/AverageDiscountedReturn          -46.0966
Evaluation/AverageReturn                    -46.0966
Evaluation/CompletionRate                     0
Evaluation/Iteration                        126
Evaluation/MaxReturn                        -33.7869
Evaluation/MinReturn                        -75.3972
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.87637
Extras/EpisodeRewardMean                    -46.2497
LinearFeatureBaseline/ExplainedVariance       0.927543
PolicyExecTime                                0.23259
ProcessExecTime                               0.0312889
TotalEnvSteps                            128524
policy/Entropy                                0.930785
policy/KL                                     0.00648672
policy/KLBefore                               0
policy/LossAfter                             -0.0135233
policy/LossBefore                            -7.77452e-09
policy/Perplexity                             2.5365
policy/dLoss                                  0.0135233
---------------------------------------  ----------------
2021-06-04 13:26:04 | [train_policy] epoch #127 | Obtaining samples for iteration 127...
2021-06-04 13:26:05 | [train_policy] epoch #127 | Logging diagnostics...
2021-06-04 13:26:05 | [train_policy] epoch #127 | Optimizing policy...
2021-06-04 13:26:05 | [train_policy] epoch #127 | Computing loss before
2021-06-04 13:26:05 | [train_policy] epoch #127 | Computing KL before
2021-06-04 13:26:05 | [train_policy] epoch #127 | Optimizing
2021-06-04 13:26:05 | [train_policy] epoch #127 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:05 | [train_policy] epoch #127 | computing loss before
2021-06-04 13:26:05 | [train_policy] epoch #127 | computing gradient
2021-06-04 13:26:05 | [train_policy] epoch #127 | gradient computed
2021-06-04 13:26:05 | [train_policy] epoch #127 | computing descent direction
2021-06-04 13:26:05 | [train_policy] epoch #127 | descent direction computed
2021-06-04 13:26:05 | [train_policy] epoch #127 | backtrack iters: 0
2021-06-04 13:26:05 | [train_policy] epoch #127 | optimization finished
2021-06-04 13:26:05 | [train_policy] epoch #127 | Computing KL after
2021-06-04 13:26:05 | [train_policy] epoch #127 | Computing loss after
2021-06-04 13:26:05 | [train_policy] epoch #127 | Fitting baseline...
2021-06-04 13:26:05 | [train_policy] epoch #127 | Saving snapshot...
2021-06-04 13:26:05 | [train_policy] epoch #127 | Saved
2021-06-04 13:26:05 | [train_policy] epoch #127 | Time 105.29 s
2021-06-04 13:26:05 | [train_policy] epoch #127 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288399
Evaluation/AverageDiscountedReturn          -45.3598
Evaluation/AverageReturn                    -45.3598
Evaluation/CompletionRate                     0
Evaluation/Iteration                        127
Evaluation/MaxReturn                        -33.0009
Evaluation/MinReturn                        -60.3604
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.43981
Extras/EpisodeRewardMean                    -45.4755
LinearFeatureBaseline/ExplainedVariance       0.945626
PolicyExecTime                                0.232414
ProcessExecTime                               0.0317593
TotalEnvSteps                            129536
policy/Entropy                                0.938049
policy/KL                                     0.00985088
policy/KLBefore                               0
policy/LossAfter                             -0.0183165
policy/LossBefore                            -8.01011e-09
policy/Perplexity                             2.55499
policy/dLoss                                  0.0183165
---------------------------------------  ----------------
2021-06-04 13:26:05 | [train_policy] epoch #128 | Obtaining samples for iteration 128...
2021-06-04 13:26:06 | [train_policy] epoch #128 | Logging diagnostics...
2021-06-04 13:26:06 | [train_policy] epoch #128 | Optimizing policy...
2021-06-04 13:26:06 | [train_policy] epoch #128 | Computing loss before
2021-06-04 13:26:06 | [train_policy] epoch #128 | Computing KL before
2021-06-04 13:26:06 | [train_policy] epoch #128 | Optimizing
2021-06-04 13:26:06 | [train_policy] epoch #128 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:06 | [train_policy] epoch #128 | computing loss before
2021-06-04 13:26:06 | [train_policy] epoch #128 | computing gradient
2021-06-04 13:26:06 | [train_policy] epoch #128 | gradient computed
2021-06-04 13:26:06 | [train_policy] epoch #128 | computing descent direction
2021-06-04 13:26:06 | [train_policy] epoch #128 | descent direction computed
2021-06-04 13:26:06 | [train_policy] epoch #128 | backtrack iters: 1
2021-06-04 13:26:06 | [train_policy] epoch #128 | optimization finished
2021-06-04 13:26:06 | [train_policy] epoch #128 | Computing KL after
2021-06-04 13:26:06 | [train_policy] epoch #128 | Computing loss after
2021-06-04 13:26:06 | [train_policy] epoch #128 | Fitting baseline...
2021-06-04 13:26:06 | [train_policy] epoch #128 | Saving snapshot...
2021-06-04 13:26:06 | [train_policy] epoch #128 | Saved
2021-06-04 13:26:06 | [train_policy] epoch #128 | Time 106.09 s
2021-06-04 13:26:06 | [train_policy] epoch #128 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285311
Evaluation/AverageDiscountedReturn          -47.1105
Evaluation/AverageReturn                    -47.1105
Evaluation/CompletionRate                     0
Evaluation/Iteration                        128
Evaluation/MaxReturn                        -33.3583
Evaluation/MinReturn                       -136.794
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.2053
Extras/EpisodeRewardMean                    -46.83
LinearFeatureBaseline/ExplainedVariance       0.72079
PolicyExecTime                                0.230403
ProcessExecTime                               0.0313294
TotalEnvSteps                            130548
policy/Entropy                                0.899374
policy/KL                                     0.00665956
policy/KLBefore                               0
policy/LossAfter                             -0.013921
policy/LossBefore                             2.27346e-08
policy/Perplexity                             2.45806
policy/dLoss                                  0.013921
---------------------------------------  ----------------
2021-06-04 13:26:06 | [train_policy] epoch #129 | Obtaining samples for iteration 129...
2021-06-04 13:26:06 | [train_policy] epoch #129 | Logging diagnostics...
2021-06-04 13:26:06 | [train_policy] epoch #129 | Optimizing policy...
2021-06-04 13:26:06 | [train_policy] epoch #129 | Computing loss before
2021-06-04 13:26:06 | [train_policy] epoch #129 | Computing KL before
2021-06-04 13:26:06 | [train_policy] epoch #129 | Optimizing
2021-06-04 13:26:06 | [train_policy] epoch #129 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:06 | [train_policy] epoch #129 | computing loss before
2021-06-04 13:26:06 | [train_policy] epoch #129 | computing gradient
2021-06-04 13:26:06 | [train_policy] epoch #129 | gradient computed
2021-06-04 13:26:06 | [train_policy] epoch #129 | computing descent direction
2021-06-04 13:26:06 | [train_policy] epoch #129 | descent direction computed
2021-06-04 13:26:06 | [train_policy] epoch #129 | backtrack iters: 1
2021-06-04 13:26:06 | [train_policy] epoch #129 | optimization finished
2021-06-04 13:26:06 | [train_policy] epoch #129 | Computing KL after
2021-06-04 13:26:06 | [train_policy] epoch #129 | Computing loss after
2021-06-04 13:26:06 | [train_policy] epoch #129 | Fitting baseline...
2021-06-04 13:26:06 | [train_policy] epoch #129 | Saving snapshot...
2021-06-04 13:26:07 | [train_policy] epoch #129 | Saved
2021-06-04 13:26:07 | [train_policy] epoch #129 | Time 106.90 s
2021-06-04 13:26:07 | [train_policy] epoch #129 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.288028
Evaluation/AverageDiscountedReturn          -46.5714
Evaluation/AverageReturn                    -46.5714
Evaluation/CompletionRate                     0
Evaluation/Iteration                        129
Evaluation/MaxReturn                        -36.0542
Evaluation/MinReturn                        -88.8988
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.08469
Extras/EpisodeRewardMean                    -46.3151
LinearFeatureBaseline/ExplainedVariance       0.875624
PolicyExecTime                                0.229626
ProcessExecTime                               0.0316389
TotalEnvSteps                            131560
policy/Entropy                                0.898722
policy/KL                                     0.00708596
policy/KLBefore                               0
policy/LossAfter                             -0.0181484
policy/LossBefore                             4.24065e-09
policy/Perplexity                             2.45646
policy/dLoss                                  0.0181484
---------------------------------------  ----------------
2021-06-04 13:26:07 | [train_policy] epoch #130 | Obtaining samples for iteration 130...
2021-06-04 13:26:07 | [train_policy] epoch #130 | Logging diagnostics...
2021-06-04 13:26:07 | [train_policy] epoch #130 | Optimizing policy...
2021-06-04 13:26:07 | [train_policy] epoch #130 | Computing loss before
2021-06-04 13:26:07 | [train_policy] epoch #130 | Computing KL before
2021-06-04 13:26:07 | [train_policy] epoch #130 | Optimizing
2021-06-04 13:26:07 | [train_policy] epoch #130 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:07 | [train_policy] epoch #130 | computing loss before
2021-06-04 13:26:07 | [train_policy] epoch #130 | computing gradient
2021-06-04 13:26:07 | [train_policy] epoch #130 | gradient computed
2021-06-04 13:26:07 | [train_policy] epoch #130 | computing descent direction
2021-06-04 13:26:07 | [train_policy] epoch #130 | descent direction computed
2021-06-04 13:26:07 | [train_policy] epoch #130 | backtrack iters: 1
2021-06-04 13:26:07 | [train_policy] epoch #130 | optimization finished
2021-06-04 13:26:07 | [train_policy] epoch #130 | Computing KL after
2021-06-04 13:26:07 | [train_policy] epoch #130 | Computing loss after
2021-06-04 13:26:07 | [train_policy] epoch #130 | Fitting baseline...
2021-06-04 13:26:07 | [train_policy] epoch #130 | Saving snapshot...
2021-06-04 13:26:07 | [train_policy] epoch #130 | Saved
2021-06-04 13:26:07 | [train_policy] epoch #130 | Time 107.72 s
2021-06-04 13:26:07 | [train_policy] epoch #130 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.289444
Evaluation/AverageDiscountedReturn          -46.1294
Evaluation/AverageReturn                    -46.1294
Evaluation/CompletionRate                     0
Evaluation/Iteration                        130
Evaluation/MaxReturn                        -35.8771
Evaluation/MinReturn                        -83.9519
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.19007
Extras/EpisodeRewardMean                    -46.0302
LinearFeatureBaseline/ExplainedVariance       0.917075
PolicyExecTime                                0.238913
ProcessExecTime                               0.0317426
TotalEnvSteps                            132572
policy/Entropy                                0.82622
policy/KL                                     0.00676756
policy/KLBefore                               0
policy/LossAfter                             -0.0187824
policy/LossBefore                            -8.83468e-09
policy/Perplexity                             2.28467
policy/dLoss                                  0.0187824
---------------------------------------  ----------------
2021-06-04 13:26:07 | [train_policy] epoch #131 | Obtaining samples for iteration 131...
2021-06-04 13:26:08 | [train_policy] epoch #131 | Logging diagnostics...
2021-06-04 13:26:08 | [train_policy] epoch #131 | Optimizing policy...
2021-06-04 13:26:08 | [train_policy] epoch #131 | Computing loss before
2021-06-04 13:26:08 | [train_policy] epoch #131 | Computing KL before
2021-06-04 13:26:08 | [train_policy] epoch #131 | Optimizing
2021-06-04 13:26:08 | [train_policy] epoch #131 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:08 | [train_policy] epoch #131 | computing loss before
2021-06-04 13:26:08 | [train_policy] epoch #131 | computing gradient
2021-06-04 13:26:08 | [train_policy] epoch #131 | gradient computed
2021-06-04 13:26:08 | [train_policy] epoch #131 | computing descent direction
2021-06-04 13:26:08 | [train_policy] epoch #131 | descent direction computed
2021-06-04 13:26:08 | [train_policy] epoch #131 | backtrack iters: 1
2021-06-04 13:26:08 | [train_policy] epoch #131 | optimization finished
2021-06-04 13:26:08 | [train_policy] epoch #131 | Computing KL after
2021-06-04 13:26:08 | [train_policy] epoch #131 | Computing loss after
2021-06-04 13:26:08 | [train_policy] epoch #131 | Fitting baseline...
2021-06-04 13:26:08 | [train_policy] epoch #131 | Saving snapshot...
2021-06-04 13:26:08 | [train_policy] epoch #131 | Saved
2021-06-04 13:26:08 | [train_policy] epoch #131 | Time 108.51 s
2021-06-04 13:26:08 | [train_policy] epoch #131 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285954
Evaluation/AverageDiscountedReturn          -46.7849
Evaluation/AverageReturn                    -46.7849
Evaluation/CompletionRate                     0
Evaluation/Iteration                        131
Evaluation/MaxReturn                        -36.5071
Evaluation/MinReturn                       -107.832
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2696
Extras/EpisodeRewardMean                    -46.8976
LinearFeatureBaseline/ExplainedVariance       0.783305
PolicyExecTime                                0.219676
ProcessExecTime                               0.0313549
TotalEnvSteps                            133584
policy/Entropy                                0.81751
policy/KL                                     0.00740397
policy/KLBefore                               0
policy/LossAfter                             -0.0222428
policy/LossBefore                             1.31931e-08
policy/Perplexity                             2.26485
policy/dLoss                                  0.0222429
---------------------------------------  ----------------
2021-06-04 13:26:08 | [train_policy] epoch #132 | Obtaining samples for iteration 132...
2021-06-04 13:26:09 | [train_policy] epoch #132 | Logging diagnostics...
2021-06-04 13:26:09 | [train_policy] epoch #132 | Optimizing policy...
2021-06-04 13:26:09 | [train_policy] epoch #132 | Computing loss before
2021-06-04 13:26:09 | [train_policy] epoch #132 | Computing KL before
2021-06-04 13:26:09 | [train_policy] epoch #132 | Optimizing
2021-06-04 13:26:09 | [train_policy] epoch #132 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:09 | [train_policy] epoch #132 | computing loss before
2021-06-04 13:26:09 | [train_policy] epoch #132 | computing gradient
2021-06-04 13:26:09 | [train_policy] epoch #132 | gradient computed
2021-06-04 13:26:09 | [train_policy] epoch #132 | computing descent direction
2021-06-04 13:26:09 | [train_policy] epoch #132 | descent direction computed
2021-06-04 13:26:09 | [train_policy] epoch #132 | backtrack iters: 1
2021-06-04 13:26:09 | [train_policy] epoch #132 | optimization finished
2021-06-04 13:26:09 | [train_policy] epoch #132 | Computing KL after
2021-06-04 13:26:09 | [train_policy] epoch #132 | Computing loss after
2021-06-04 13:26:09 | [train_policy] epoch #132 | Fitting baseline...
2021-06-04 13:26:09 | [train_policy] epoch #132 | Saving snapshot...
2021-06-04 13:26:09 | [train_policy] epoch #132 | Saved
2021-06-04 13:26:09 | [train_policy] epoch #132 | Time 109.32 s
2021-06-04 13:26:09 | [train_policy] epoch #132 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.2887
Evaluation/AverageDiscountedReturn          -45.9247
Evaluation/AverageReturn                    -45.9247
Evaluation/CompletionRate                     0
Evaluation/Iteration                        132
Evaluation/MaxReturn                        -31.2744
Evaluation/MinReturn                        -66.0059
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.05139
Extras/EpisodeRewardMean                    -45.7841
LinearFeatureBaseline/ExplainedVariance       0.913686
PolicyExecTime                                0.233644
ProcessExecTime                               0.031507
TotalEnvSteps                            134596
policy/Entropy                                0.776736
policy/KL                                     0.00669477
policy/KLBefore                               0
policy/LossAfter                             -0.0216001
policy/LossBefore                            -1.64914e-09
policy/Perplexity                             2.17436
policy/dLoss                                  0.0216001
---------------------------------------  ----------------
2021-06-04 13:26:09 | [train_policy] epoch #133 | Obtaining samples for iteration 133...
2021-06-04 13:26:10 | [train_policy] epoch #133 | Logging diagnostics...
2021-06-04 13:26:10 | [train_policy] epoch #133 | Optimizing policy...
2021-06-04 13:26:10 | [train_policy] epoch #133 | Computing loss before
2021-06-04 13:26:10 | [train_policy] epoch #133 | Computing KL before
2021-06-04 13:26:10 | [train_policy] epoch #133 | Optimizing
2021-06-04 13:26:10 | [train_policy] epoch #133 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:10 | [train_policy] epoch #133 | computing loss before
2021-06-04 13:26:10 | [train_policy] epoch #133 | computing gradient
2021-06-04 13:26:10 | [train_policy] epoch #133 | gradient computed
2021-06-04 13:26:10 | [train_policy] epoch #133 | computing descent direction
2021-06-04 13:26:10 | [train_policy] epoch #133 | descent direction computed
2021-06-04 13:26:10 | [train_policy] epoch #133 | backtrack iters: 0
2021-06-04 13:26:10 | [train_policy] epoch #133 | optimization finished
2021-06-04 13:26:10 | [train_policy] epoch #133 | Computing KL after
2021-06-04 13:26:10 | [train_policy] epoch #133 | Computing loss after
2021-06-04 13:26:10 | [train_policy] epoch #133 | Fitting baseline...
2021-06-04 13:26:10 | [train_policy] epoch #133 | Saving snapshot...
2021-06-04 13:26:10 | [train_policy] epoch #133 | Saved
2021-06-04 13:26:10 | [train_policy] epoch #133 | Time 110.10 s
2021-06-04 13:26:10 | [train_policy] epoch #133 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285882
Evaluation/AverageDiscountedReturn          -45.0869
Evaluation/AverageReturn                    -45.0869
Evaluation/CompletionRate                     0
Evaluation/Iteration                        133
Evaluation/MaxReturn                        -33.1946
Evaluation/MinReturn                        -65.7851
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.70528
Extras/EpisodeRewardMean                    -45.4992
LinearFeatureBaseline/ExplainedVariance       0.943148
PolicyExecTime                                0.220663
ProcessExecTime                               0.0313585
TotalEnvSteps                            135608
policy/Entropy                                0.775712
policy/KL                                     0.00998376
policy/KLBefore                               0
policy/LossAfter                             -0.0155271
policy/LossBefore                             3.53387e-09
policy/Perplexity                             2.17214
policy/dLoss                                  0.0155271
---------------------------------------  ----------------
2021-06-04 13:26:10 | [train_policy] epoch #134 | Obtaining samples for iteration 134...
2021-06-04 13:26:10 | [train_policy] epoch #134 | Logging diagnostics...
2021-06-04 13:26:10 | [train_policy] epoch #134 | Optimizing policy...
2021-06-04 13:26:10 | [train_policy] epoch #134 | Computing loss before
2021-06-04 13:26:10 | [train_policy] epoch #134 | Computing KL before
2021-06-04 13:26:10 | [train_policy] epoch #134 | Optimizing
2021-06-04 13:26:10 | [train_policy] epoch #134 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:10 | [train_policy] epoch #134 | computing loss before
2021-06-04 13:26:10 | [train_policy] epoch #134 | computing gradient
2021-06-04 13:26:10 | [train_policy] epoch #134 | gradient computed
2021-06-04 13:26:10 | [train_policy] epoch #134 | computing descent direction
2021-06-04 13:26:10 | [train_policy] epoch #134 | descent direction computed
2021-06-04 13:26:10 | [train_policy] epoch #134 | backtrack iters: 0
2021-06-04 13:26:10 | [train_policy] epoch #134 | optimization finished
2021-06-04 13:26:10 | [train_policy] epoch #134 | Computing KL after
2021-06-04 13:26:10 | [train_policy] epoch #134 | Computing loss after
2021-06-04 13:26:10 | [train_policy] epoch #134 | Fitting baseline...
2021-06-04 13:26:10 | [train_policy] epoch #134 | Saving snapshot...
2021-06-04 13:26:11 | [train_policy] epoch #134 | Saved
2021-06-04 13:26:11 | [train_policy] epoch #134 | Time 110.91 s
2021-06-04 13:26:11 | [train_policy] epoch #134 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.294003
Evaluation/AverageDiscountedReturn          -47.8684
Evaluation/AverageReturn                    -47.8684
Evaluation/CompletionRate                     0
Evaluation/Iteration                        134
Evaluation/MaxReturn                        -34.0277
Evaluation/MinReturn                        -94.0481
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.77859
Extras/EpisodeRewardMean                    -47.4715
LinearFeatureBaseline/ExplainedVariance       0.841394
PolicyExecTime                                0.228922
ProcessExecTime                               0.0322249
TotalEnvSteps                            136620
policy/Entropy                                0.785596
policy/KL                                     0.00963648
policy/KLBefore                               0
policy/LossAfter                             -0.0210095
policy/LossBefore                             1.08372e-08
policy/Perplexity                             2.19371
policy/dLoss                                  0.0210096
---------------------------------------  ----------------
2021-06-04 13:26:11 | [train_policy] epoch #135 | Obtaining samples for iteration 135...
2021-06-04 13:26:11 | [train_policy] epoch #135 | Logging diagnostics...
2021-06-04 13:26:11 | [train_policy] epoch #135 | Optimizing policy...
2021-06-04 13:26:11 | [train_policy] epoch #135 | Computing loss before
2021-06-04 13:26:11 | [train_policy] epoch #135 | Computing KL before
2021-06-04 13:26:11 | [train_policy] epoch #135 | Optimizing
2021-06-04 13:26:11 | [train_policy] epoch #135 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:11 | [train_policy] epoch #135 | computing loss before
2021-06-04 13:26:11 | [train_policy] epoch #135 | computing gradient
2021-06-04 13:26:11 | [train_policy] epoch #135 | gradient computed
2021-06-04 13:26:11 | [train_policy] epoch #135 | computing descent direction
2021-06-04 13:26:11 | [train_policy] epoch #135 | descent direction computed
2021-06-04 13:26:11 | [train_policy] epoch #135 | backtrack iters: 0
2021-06-04 13:26:11 | [train_policy] epoch #135 | optimization finished
2021-06-04 13:26:11 | [train_policy] epoch #135 | Computing KL after
2021-06-04 13:26:11 | [train_policy] epoch #135 | Computing loss after
2021-06-04 13:26:11 | [train_policy] epoch #135 | Fitting baseline...
2021-06-04 13:26:11 | [train_policy] epoch #135 | Saving snapshot...
2021-06-04 13:26:11 | [train_policy] epoch #135 | Saved
2021-06-04 13:26:11 | [train_policy] epoch #135 | Time 111.70 s
2021-06-04 13:26:11 | [train_policy] epoch #135 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28774
Evaluation/AverageDiscountedReturn          -46.2955
Evaluation/AverageReturn                    -46.2955
Evaluation/CompletionRate                     0
Evaluation/Iteration                        135
Evaluation/MaxReturn                        -33.0938
Evaluation/MinReturn                        -65.5917
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.22206
Extras/EpisodeRewardMean                    -46.59
LinearFeatureBaseline/ExplainedVariance       0.936747
PolicyExecTime                                0.236233
ProcessExecTime                               0.031637
TotalEnvSteps                            137632
policy/Entropy                                0.781721
policy/KL                                     0.00996374
policy/KLBefore                               0
policy/LossAfter                             -0.0154613
policy/LossBefore                            -2.83888e-08
policy/Perplexity                             2.18523
policy/dLoss                                  0.0154613
---------------------------------------  ----------------
2021-06-04 13:26:11 | [train_policy] epoch #136 | Obtaining samples for iteration 136...
2021-06-04 13:26:12 | [train_policy] epoch #136 | Logging diagnostics...
2021-06-04 13:26:12 | [train_policy] epoch #136 | Optimizing policy...
2021-06-04 13:26:12 | [train_policy] epoch #136 | Computing loss before
2021-06-04 13:26:12 | [train_policy] epoch #136 | Computing KL before
2021-06-04 13:26:12 | [train_policy] epoch #136 | Optimizing
2021-06-04 13:26:12 | [train_policy] epoch #136 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:12 | [train_policy] epoch #136 | computing loss before
2021-06-04 13:26:12 | [train_policy] epoch #136 | computing gradient
2021-06-04 13:26:12 | [train_policy] epoch #136 | gradient computed
2021-06-04 13:26:12 | [train_policy] epoch #136 | computing descent direction
2021-06-04 13:26:12 | [train_policy] epoch #136 | descent direction computed
2021-06-04 13:26:12 | [train_policy] epoch #136 | backtrack iters: 1
2021-06-04 13:26:12 | [train_policy] epoch #136 | optimization finished
2021-06-04 13:26:12 | [train_policy] epoch #136 | Computing KL after
2021-06-04 13:26:12 | [train_policy] epoch #136 | Computing loss after
2021-06-04 13:26:12 | [train_policy] epoch #136 | Fitting baseline...
2021-06-04 13:26:12 | [train_policy] epoch #136 | Saving snapshot...
2021-06-04 13:26:12 | [train_policy] epoch #136 | Saved
2021-06-04 13:26:12 | [train_policy] epoch #136 | Time 112.48 s
2021-06-04 13:26:12 | [train_policy] epoch #136 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284783
Evaluation/AverageDiscountedReturn          -45.156
Evaluation/AverageReturn                    -45.156
Evaluation/CompletionRate                     0
Evaluation/Iteration                        136
Evaluation/MaxReturn                        -32.4122
Evaluation/MinReturn                        -64.3204
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.31713
Extras/EpisodeRewardMean                    -45.313
LinearFeatureBaseline/ExplainedVariance       0.947946
PolicyExecTime                                0.215722
ProcessExecTime                               0.0312643
TotalEnvSteps                            138644
policy/Entropy                                0.80528
policy/KL                                     0.00682563
policy/KLBefore                               0
policy/LossAfter                             -0.015697
policy/LossBefore                             1.71982e-08
policy/Perplexity                             2.23732
policy/dLoss                                  0.015697
---------------------------------------  ----------------
2021-06-04 13:26:12 | [train_policy] epoch #137 | Obtaining samples for iteration 137...
2021-06-04 13:26:13 | [train_policy] epoch #137 | Logging diagnostics...
2021-06-04 13:26:13 | [train_policy] epoch #137 | Optimizing policy...
2021-06-04 13:26:13 | [train_policy] epoch #137 | Computing loss before
2021-06-04 13:26:13 | [train_policy] epoch #137 | Computing KL before
2021-06-04 13:26:13 | [train_policy] epoch #137 | Optimizing
2021-06-04 13:26:13 | [train_policy] epoch #137 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:13 | [train_policy] epoch #137 | computing loss before
2021-06-04 13:26:13 | [train_policy] epoch #137 | computing gradient
2021-06-04 13:26:13 | [train_policy] epoch #137 | gradient computed
2021-06-04 13:26:13 | [train_policy] epoch #137 | computing descent direction
2021-06-04 13:26:13 | [train_policy] epoch #137 | descent direction computed
2021-06-04 13:26:13 | [train_policy] epoch #137 | backtrack iters: 1
2021-06-04 13:26:13 | [train_policy] epoch #137 | optimization finished
2021-06-04 13:26:13 | [train_policy] epoch #137 | Computing KL after
2021-06-04 13:26:13 | [train_policy] epoch #137 | Computing loss after
2021-06-04 13:26:13 | [train_policy] epoch #137 | Fitting baseline...
2021-06-04 13:26:13 | [train_policy] epoch #137 | Saving snapshot...
2021-06-04 13:26:13 | [train_policy] epoch #137 | Saved
2021-06-04 13:26:13 | [train_policy] epoch #137 | Time 113.28 s
2021-06-04 13:26:13 | [train_policy] epoch #137 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286698
Evaluation/AverageDiscountedReturn          -44.9208
Evaluation/AverageReturn                    -44.9208
Evaluation/CompletionRate                     0
Evaluation/Iteration                        137
Evaluation/MaxReturn                        -31.6461
Evaluation/MinReturn                        -60.5975
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.69568
Extras/EpisodeRewardMean                    -44.7425
LinearFeatureBaseline/ExplainedVariance       0.945195
PolicyExecTime                                0.23281
ProcessExecTime                               0.0313823
TotalEnvSteps                            139656
policy/Entropy                                0.772706
policy/KL                                     0.00760733
policy/KLBefore                               0
policy/LossAfter                             -0.0202581
policy/LossBefore                             5.18301e-09
policy/Perplexity                             2.16562
policy/dLoss                                  0.0202581
---------------------------------------  ----------------
2021-06-04 13:26:13 | [train_policy] epoch #138 | Obtaining samples for iteration 138...
2021-06-04 13:26:14 | [train_policy] epoch #138 | Logging diagnostics...
2021-06-04 13:26:14 | [train_policy] epoch #138 | Optimizing policy...
2021-06-04 13:26:14 | [train_policy] epoch #138 | Computing loss before
2021-06-04 13:26:14 | [train_policy] epoch #138 | Computing KL before
2021-06-04 13:26:14 | [train_policy] epoch #138 | Optimizing
2021-06-04 13:26:14 | [train_policy] epoch #138 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:14 | [train_policy] epoch #138 | computing loss before
2021-06-04 13:26:14 | [train_policy] epoch #138 | computing gradient
2021-06-04 13:26:14 | [train_policy] epoch #138 | gradient computed
2021-06-04 13:26:14 | [train_policy] epoch #138 | computing descent direction
2021-06-04 13:26:14 | [train_policy] epoch #138 | descent direction computed
2021-06-04 13:26:14 | [train_policy] epoch #138 | backtrack iters: 1
2021-06-04 13:26:14 | [train_policy] epoch #138 | optimization finished
2021-06-04 13:26:14 | [train_policy] epoch #138 | Computing KL after
2021-06-04 13:26:14 | [train_policy] epoch #138 | Computing loss after
2021-06-04 13:26:14 | [train_policy] epoch #138 | Fitting baseline...
2021-06-04 13:26:14 | [train_policy] epoch #138 | Saving snapshot...
2021-06-04 13:26:14 | [train_policy] epoch #138 | Saved
2021-06-04 13:26:14 | [train_policy] epoch #138 | Time 114.09 s
2021-06-04 13:26:14 | [train_policy] epoch #138 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284979
Evaluation/AverageDiscountedReturn          -44.6061
Evaluation/AverageReturn                    -44.6061
Evaluation/CompletionRate                     0
Evaluation/Iteration                        138
Evaluation/MaxReturn                        -32.2532
Evaluation/MinReturn                        -66.6468
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.91939
Extras/EpisodeRewardMean                    -44.4755
LinearFeatureBaseline/ExplainedVariance       0.928081
PolicyExecTime                                0.228812
ProcessExecTime                               0.0313513
TotalEnvSteps                            140668
policy/Entropy                                0.773161
policy/KL                                     0.00640332
policy/KLBefore                               0
policy/LossAfter                             -0.0140275
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             2.1666
policy/dLoss                                  0.0140275
---------------------------------------  ----------------
2021-06-04 13:26:14 | [train_policy] epoch #139 | Obtaining samples for iteration 139...
2021-06-04 13:26:14 | [train_policy] epoch #139 | Logging diagnostics...
2021-06-04 13:26:14 | [train_policy] epoch #139 | Optimizing policy...
2021-06-04 13:26:14 | [train_policy] epoch #139 | Computing loss before
2021-06-04 13:26:14 | [train_policy] epoch #139 | Computing KL before
2021-06-04 13:26:14 | [train_policy] epoch #139 | Optimizing
2021-06-04 13:26:14 | [train_policy] epoch #139 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:14 | [train_policy] epoch #139 | computing loss before
2021-06-04 13:26:14 | [train_policy] epoch #139 | computing gradient
2021-06-04 13:26:14 | [train_policy] epoch #139 | gradient computed
2021-06-04 13:26:14 | [train_policy] epoch #139 | computing descent direction
2021-06-04 13:26:14 | [train_policy] epoch #139 | descent direction computed
2021-06-04 13:26:14 | [train_policy] epoch #139 | backtrack iters: 1
2021-06-04 13:26:14 | [train_policy] epoch #139 | optimization finished
2021-06-04 13:26:14 | [train_policy] epoch #139 | Computing KL after
2021-06-04 13:26:14 | [train_policy] epoch #139 | Computing loss after
2021-06-04 13:26:14 | [train_policy] epoch #139 | Fitting baseline...
2021-06-04 13:26:14 | [train_policy] epoch #139 | Saving snapshot...
2021-06-04 13:26:14 | [train_policy] epoch #139 | Saved
2021-06-04 13:26:14 | [train_policy] epoch #139 | Time 114.88 s
2021-06-04 13:26:14 | [train_policy] epoch #139 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287886
Evaluation/AverageDiscountedReturn          -46.2358
Evaluation/AverageReturn                    -46.2358
Evaluation/CompletionRate                     0
Evaluation/Iteration                        139
Evaluation/MaxReturn                        -34.7147
Evaluation/MinReturn                        -66.0441
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.92709
Extras/EpisodeRewardMean                    -46.2185
LinearFeatureBaseline/ExplainedVariance       0.912531
PolicyExecTime                                0.217376
ProcessExecTime                               0.0316222
TotalEnvSteps                            141680
policy/Entropy                                0.715694
policy/KL                                     0.00675211
policy/KLBefore                               0
policy/LossAfter                             -0.0198375
policy/LossBefore                             9.65925e-09
policy/Perplexity                             2.04561
policy/dLoss                                  0.0198375
---------------------------------------  ----------------
2021-06-04 13:26:15 | [train_policy] epoch #140 | Obtaining samples for iteration 140...
2021-06-04 13:26:15 | [train_policy] epoch #140 | Logging diagnostics...
2021-06-04 13:26:15 | [train_policy] epoch #140 | Optimizing policy...
2021-06-04 13:26:15 | [train_policy] epoch #140 | Computing loss before
2021-06-04 13:26:15 | [train_policy] epoch #140 | Computing KL before
2021-06-04 13:26:15 | [train_policy] epoch #140 | Optimizing
2021-06-04 13:26:15 | [train_policy] epoch #140 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:15 | [train_policy] epoch #140 | computing loss before
2021-06-04 13:26:15 | [train_policy] epoch #140 | computing gradient
2021-06-04 13:26:15 | [train_policy] epoch #140 | gradient computed
2021-06-04 13:26:15 | [train_policy] epoch #140 | computing descent direction
2021-06-04 13:26:15 | [train_policy] epoch #140 | descent direction computed
2021-06-04 13:26:15 | [train_policy] epoch #140 | backtrack iters: 1
2021-06-04 13:26:15 | [train_policy] epoch #140 | optimization finished
2021-06-04 13:26:15 | [train_policy] epoch #140 | Computing KL after
2021-06-04 13:26:15 | [train_policy] epoch #140 | Computing loss after
2021-06-04 13:26:15 | [train_policy] epoch #140 | Fitting baseline...
2021-06-04 13:26:15 | [train_policy] epoch #140 | Saving snapshot...
2021-06-04 13:26:15 | [train_policy] epoch #140 | Saved
2021-06-04 13:26:15 | [train_policy] epoch #140 | Time 115.69 s
2021-06-04 13:26:15 | [train_policy] epoch #140 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287454
Evaluation/AverageDiscountedReturn          -45.0954
Evaluation/AverageReturn                    -45.0954
Evaluation/CompletionRate                     0
Evaluation/Iteration                        140
Evaluation/MaxReturn                        -34.2988
Evaluation/MinReturn                        -65.014
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.8823
Extras/EpisodeRewardMean                    -45.4413
LinearFeatureBaseline/ExplainedVariance       0.924821
PolicyExecTime                                0.234475
ProcessExecTime                               0.0314858
TotalEnvSteps                            142692
policy/Entropy                                0.688157
policy/KL                                     0.00741986
policy/KLBefore                               0
policy/LossAfter                             -0.0206592
policy/LossBefore                             6.59656e-09
policy/Perplexity                             1.99004
policy/dLoss                                  0.0206592
---------------------------------------  ----------------
2021-06-04 13:26:15 | [train_policy] epoch #141 | Obtaining samples for iteration 141...
2021-06-04 13:26:16 | [train_policy] epoch #141 | Logging diagnostics...
2021-06-04 13:26:16 | [train_policy] epoch #141 | Optimizing policy...
2021-06-04 13:26:16 | [train_policy] epoch #141 | Computing loss before
2021-06-04 13:26:16 | [train_policy] epoch #141 | Computing KL before
2021-06-04 13:26:16 | [train_policy] epoch #141 | Optimizing
2021-06-04 13:26:16 | [train_policy] epoch #141 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:16 | [train_policy] epoch #141 | computing loss before
2021-06-04 13:26:16 | [train_policy] epoch #141 | computing gradient
2021-06-04 13:26:16 | [train_policy] epoch #141 | gradient computed
2021-06-04 13:26:16 | [train_policy] epoch #141 | computing descent direction
2021-06-04 13:26:16 | [train_policy] epoch #141 | descent direction computed
2021-06-04 13:26:16 | [train_policy] epoch #141 | backtrack iters: 1
2021-06-04 13:26:16 | [train_policy] epoch #141 | optimization finished
2021-06-04 13:26:16 | [train_policy] epoch #141 | Computing KL after
2021-06-04 13:26:16 | [train_policy] epoch #141 | Computing loss after
2021-06-04 13:26:16 | [train_policy] epoch #141 | Fitting baseline...
2021-06-04 13:26:16 | [train_policy] epoch #141 | Saving snapshot...
2021-06-04 13:26:16 | [train_policy] epoch #141 | Saved
2021-06-04 13:26:16 | [train_policy] epoch #141 | Time 116.49 s
2021-06-04 13:26:16 | [train_policy] epoch #141 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285525
Evaluation/AverageDiscountedReturn          -46.5228
Evaluation/AverageReturn                    -46.5228
Evaluation/CompletionRate                     0
Evaluation/Iteration                        141
Evaluation/MaxReturn                        -36.1114
Evaluation/MinReturn                       -130.333
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.759
Extras/EpisodeRewardMean                    -46.2102
LinearFeatureBaseline/ExplainedVariance       0.728901
PolicyExecTime                                0.228127
ProcessExecTime                               0.0314305
TotalEnvSteps                            143704
policy/Entropy                                0.676999
policy/KL                                     0.0076515
policy/KLBefore                               0
policy/LossAfter                             -0.0190227
policy/LossBefore                             4.94742e-09
policy/Perplexity                             1.96796
policy/dLoss                                  0.0190227
---------------------------------------  ----------------
2021-06-04 13:26:16 | [train_policy] epoch #142 | Obtaining samples for iteration 142...
2021-06-04 13:26:17 | [train_policy] epoch #142 | Logging diagnostics...
2021-06-04 13:26:17 | [train_policy] epoch #142 | Optimizing policy...
2021-06-04 13:26:17 | [train_policy] epoch #142 | Computing loss before
2021-06-04 13:26:17 | [train_policy] epoch #142 | Computing KL before
2021-06-04 13:26:17 | [train_policy] epoch #142 | Optimizing
2021-06-04 13:26:17 | [train_policy] epoch #142 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:17 | [train_policy] epoch #142 | computing loss before
2021-06-04 13:26:17 | [train_policy] epoch #142 | computing gradient
2021-06-04 13:26:17 | [train_policy] epoch #142 | gradient computed
2021-06-04 13:26:17 | [train_policy] epoch #142 | computing descent direction
2021-06-04 13:26:17 | [train_policy] epoch #142 | descent direction computed
2021-06-04 13:26:17 | [train_policy] epoch #142 | backtrack iters: 0
2021-06-04 13:26:17 | [train_policy] epoch #142 | optimization finished
2021-06-04 13:26:17 | [train_policy] epoch #142 | Computing KL after
2021-06-04 13:26:17 | [train_policy] epoch #142 | Computing loss after
2021-06-04 13:26:17 | [train_policy] epoch #142 | Fitting baseline...
2021-06-04 13:26:17 | [train_policy] epoch #142 | Saving snapshot...
2021-06-04 13:26:17 | [train_policy] epoch #142 | Saved
2021-06-04 13:26:17 | [train_policy] epoch #142 | Time 117.29 s
2021-06-04 13:26:17 | [train_policy] epoch #142 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288386
Evaluation/AverageDiscountedReturn          -46.1387
Evaluation/AverageReturn                    -46.1387
Evaluation/CompletionRate                     0
Evaluation/Iteration                        142
Evaluation/MaxReturn                        -35.6355
Evaluation/MinReturn                        -80.0538
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.98725
Extras/EpisodeRewardMean                    -45.8783
LinearFeatureBaseline/ExplainedVariance       0.865186
PolicyExecTime                                0.232507
ProcessExecTime                               0.0316868
TotalEnvSteps                            144716
policy/Entropy                                0.668532
policy/KL                                     0.009586
policy/KLBefore                               0
policy/LossAfter                             -0.0232635
policy/LossBefore                             1.69626e-08
policy/Perplexity                             1.95137
policy/dLoss                                  0.0232635
---------------------------------------  ----------------
2021-06-04 13:26:17 | [train_policy] epoch #143 | Obtaining samples for iteration 143...
2021-06-04 13:26:18 | [train_policy] epoch #143 | Logging diagnostics...
2021-06-04 13:26:18 | [train_policy] epoch #143 | Optimizing policy...
2021-06-04 13:26:18 | [train_policy] epoch #143 | Computing loss before
2021-06-04 13:26:18 | [train_policy] epoch #143 | Computing KL before
2021-06-04 13:26:18 | [train_policy] epoch #143 | Optimizing
2021-06-04 13:26:18 | [train_policy] epoch #143 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:18 | [train_policy] epoch #143 | computing loss before
2021-06-04 13:26:18 | [train_policy] epoch #143 | computing gradient
2021-06-04 13:26:18 | [train_policy] epoch #143 | gradient computed
2021-06-04 13:26:18 | [train_policy] epoch #143 | computing descent direction
2021-06-04 13:26:18 | [train_policy] epoch #143 | descent direction computed
2021-06-04 13:26:18 | [train_policy] epoch #143 | backtrack iters: 1
2021-06-04 13:26:18 | [train_policy] epoch #143 | optimization finished
2021-06-04 13:26:18 | [train_policy] epoch #143 | Computing KL after
2021-06-04 13:26:18 | [train_policy] epoch #143 | Computing loss after
2021-06-04 13:26:18 | [train_policy] epoch #143 | Fitting baseline...
2021-06-04 13:26:18 | [train_policy] epoch #143 | Saving snapshot...
2021-06-04 13:26:18 | [train_policy] epoch #143 | Saved
2021-06-04 13:26:18 | [train_policy] epoch #143 | Time 118.09 s
2021-06-04 13:26:18 | [train_policy] epoch #143 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286313
Evaluation/AverageDiscountedReturn          -46.5026
Evaluation/AverageReturn                    -46.5026
Evaluation/CompletionRate                     0
Evaluation/Iteration                        143
Evaluation/MaxReturn                        -32.0758
Evaluation/MinReturn                       -136.234
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.831
Extras/EpisodeRewardMean                    -46.4131
LinearFeatureBaseline/ExplainedVariance       0.720556
PolicyExecTime                                0.228195
ProcessExecTime                               0.0314949
TotalEnvSteps                            145728
policy/Entropy                                0.666069
policy/KL                                     0.00646412
policy/KLBefore                               0
policy/LossAfter                             -0.0166087
policy/LossBefore                            -2.59151e-09
policy/Perplexity                             1.94657
policy/dLoss                                  0.0166087
---------------------------------------  ----------------
2021-06-04 13:26:18 | [train_policy] epoch #144 | Obtaining samples for iteration 144...
2021-06-04 13:26:18 | [train_policy] epoch #144 | Logging diagnostics...
2021-06-04 13:26:18 | [train_policy] epoch #144 | Optimizing policy...
2021-06-04 13:26:18 | [train_policy] epoch #144 | Computing loss before
2021-06-04 13:26:18 | [train_policy] epoch #144 | Computing KL before
2021-06-04 13:26:18 | [train_policy] epoch #144 | Optimizing
2021-06-04 13:26:18 | [train_policy] epoch #144 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:18 | [train_policy] epoch #144 | computing loss before
2021-06-04 13:26:18 | [train_policy] epoch #144 | computing gradient
2021-06-04 13:26:18 | [train_policy] epoch #144 | gradient computed
2021-06-04 13:26:18 | [train_policy] epoch #144 | computing descent direction
2021-06-04 13:26:18 | [train_policy] epoch #144 | descent direction computed
2021-06-04 13:26:18 | [train_policy] epoch #144 | backtrack iters: 1
2021-06-04 13:26:18 | [train_policy] epoch #144 | optimization finished
2021-06-04 13:26:18 | [train_policy] epoch #144 | Computing KL after
2021-06-04 13:26:18 | [train_policy] epoch #144 | Computing loss after
2021-06-04 13:26:18 | [train_policy] epoch #144 | Fitting baseline...
2021-06-04 13:26:18 | [train_policy] epoch #144 | Saving snapshot...
2021-06-04 13:26:18 | [train_policy] epoch #144 | Saved
2021-06-04 13:26:18 | [train_policy] epoch #144 | Time 118.89 s
2021-06-04 13:26:18 | [train_policy] epoch #144 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286636
Evaluation/AverageDiscountedReturn          -45.0135
Evaluation/AverageReturn                    -45.0135
Evaluation/CompletionRate                     0
Evaluation/Iteration                        144
Evaluation/MaxReturn                        -35.2633
Evaluation/MinReturn                        -60.5353
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.11917
Extras/EpisodeRewardMean                    -44.8998
LinearFeatureBaseline/ExplainedVariance       0.893057
PolicyExecTime                                0.225456
ProcessExecTime                               0.0315115
TotalEnvSteps                            146740
policy/Entropy                                0.662541
policy/KL                                     0.00759416
policy/KLBefore                               0
policy/LossAfter                             -0.0106555
policy/LossBefore                             9.89484e-09
policy/Perplexity                             1.93971
policy/dLoss                                  0.0106555
---------------------------------------  ----------------
2021-06-04 13:26:19 | [train_policy] epoch #145 | Obtaining samples for iteration 145...
2021-06-04 13:26:19 | [train_policy] epoch #145 | Logging diagnostics...
2021-06-04 13:26:19 | [train_policy] epoch #145 | Optimizing policy...
2021-06-04 13:26:19 | [train_policy] epoch #145 | Computing loss before
2021-06-04 13:26:19 | [train_policy] epoch #145 | Computing KL before
2021-06-04 13:26:19 | [train_policy] epoch #145 | Optimizing
2021-06-04 13:26:19 | [train_policy] epoch #145 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:19 | [train_policy] epoch #145 | computing loss before
2021-06-04 13:26:19 | [train_policy] epoch #145 | computing gradient
2021-06-04 13:26:19 | [train_policy] epoch #145 | gradient computed
2021-06-04 13:26:19 | [train_policy] epoch #145 | computing descent direction
2021-06-04 13:26:19 | [train_policy] epoch #145 | descent direction computed
2021-06-04 13:26:19 | [train_policy] epoch #145 | backtrack iters: 1
2021-06-04 13:26:19 | [train_policy] epoch #145 | optimization finished
2021-06-04 13:26:19 | [train_policy] epoch #145 | Computing KL after
2021-06-04 13:26:19 | [train_policy] epoch #145 | Computing loss after
2021-06-04 13:26:19 | [train_policy] epoch #145 | Fitting baseline...
2021-06-04 13:26:19 | [train_policy] epoch #145 | Saving snapshot...
2021-06-04 13:26:19 | [train_policy] epoch #145 | Saved
2021-06-04 13:26:19 | [train_policy] epoch #145 | Time 119.68 s
2021-06-04 13:26:19 | [train_policy] epoch #145 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284749
Evaluation/AverageDiscountedReturn          -44.5527
Evaluation/AverageReturn                    -44.5527
Evaluation/CompletionRate                     0
Evaluation/Iteration                        145
Evaluation/MaxReturn                        -31.444
Evaluation/MinReturn                        -65.9965
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.43749
Extras/EpisodeRewardMean                    -44.6641
LinearFeatureBaseline/ExplainedVariance       0.934854
PolicyExecTime                                0.228339
ProcessExecTime                               0.0312462
TotalEnvSteps                            147752
policy/Entropy                                0.64869
policy/KL                                     0.00799023
policy/KLBefore                               0
policy/LossAfter                             -0.0179393
policy/LossBefore                             3.29828e-09
policy/Perplexity                             1.91303
policy/dLoss                                  0.0179393
---------------------------------------  ----------------
2021-06-04 13:26:19 | [train_policy] epoch #146 | Obtaining samples for iteration 146...
2021-06-04 13:26:20 | [train_policy] epoch #146 | Logging diagnostics...
2021-06-04 13:26:20 | [train_policy] epoch #146 | Optimizing policy...
2021-06-04 13:26:20 | [train_policy] epoch #146 | Computing loss before
2021-06-04 13:26:20 | [train_policy] epoch #146 | Computing KL before
2021-06-04 13:26:20 | [train_policy] epoch #146 | Optimizing
2021-06-04 13:26:20 | [train_policy] epoch #146 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:20 | [train_policy] epoch #146 | computing loss before
2021-06-04 13:26:20 | [train_policy] epoch #146 | computing gradient
2021-06-04 13:26:20 | [train_policy] epoch #146 | gradient computed
2021-06-04 13:26:20 | [train_policy] epoch #146 | computing descent direction
2021-06-04 13:26:20 | [train_policy] epoch #146 | descent direction computed
2021-06-04 13:26:20 | [train_policy] epoch #146 | backtrack iters: 1
2021-06-04 13:26:20 | [train_policy] epoch #146 | optimization finished
2021-06-04 13:26:20 | [train_policy] epoch #146 | Computing KL after
2021-06-04 13:26:20 | [train_policy] epoch #146 | Computing loss after
2021-06-04 13:26:20 | [train_policy] epoch #146 | Fitting baseline...
2021-06-04 13:26:20 | [train_policy] epoch #146 | Saving snapshot...
2021-06-04 13:26:20 | [train_policy] epoch #146 | Saved
2021-06-04 13:26:20 | [train_policy] epoch #146 | Time 120.50 s
2021-06-04 13:26:20 | [train_policy] epoch #146 | EpochTime 0.81 s
---------------------------------------  ----------------
EnvExecTime                                   0.310108
Evaluation/AverageDiscountedReturn          -68.2721
Evaluation/AverageReturn                    -68.2721
Evaluation/CompletionRate                     0
Evaluation/Iteration                        146
Evaluation/MaxReturn                        -35.4503
Evaluation/MinReturn                      -2060.52
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.132
Extras/EpisodeRewardMean                    -66.2199
LinearFeatureBaseline/ExplainedVariance       0.0113328
PolicyExecTime                                0.232169
ProcessExecTime                               0.0337255
TotalEnvSteps                            148764
policy/Entropy                                0.633569
policy/KL                                     0.00684975
policy/KLBefore                               0
policy/LossAfter                             -0.0214926
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             1.88432
policy/dLoss                                  0.0214926
---------------------------------------  ----------------
2021-06-04 13:26:20 | [train_policy] epoch #147 | Obtaining samples for iteration 147...
2021-06-04 13:26:21 | [train_policy] epoch #147 | Logging diagnostics...
2021-06-04 13:26:21 | [train_policy] epoch #147 | Optimizing policy...
2021-06-04 13:26:21 | [train_policy] epoch #147 | Computing loss before
2021-06-04 13:26:21 | [train_policy] epoch #147 | Computing KL before
2021-06-04 13:26:21 | [train_policy] epoch #147 | Optimizing
2021-06-04 13:26:21 | [train_policy] epoch #147 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:21 | [train_policy] epoch #147 | computing loss before
2021-06-04 13:26:21 | [train_policy] epoch #147 | computing gradient
2021-06-04 13:26:21 | [train_policy] epoch #147 | gradient computed
2021-06-04 13:26:21 | [train_policy] epoch #147 | computing descent direction
2021-06-04 13:26:21 | [train_policy] epoch #147 | descent direction computed
2021-06-04 13:26:21 | [train_policy] epoch #147 | backtrack iters: 1
2021-06-04 13:26:21 | [train_policy] epoch #147 | optimization finished
2021-06-04 13:26:21 | [train_policy] epoch #147 | Computing KL after
2021-06-04 13:26:21 | [train_policy] epoch #147 | Computing loss after
2021-06-04 13:26:21 | [train_policy] epoch #147 | Fitting baseline...
2021-06-04 13:26:21 | [train_policy] epoch #147 | Saving snapshot...
2021-06-04 13:26:21 | [train_policy] epoch #147 | Saved
2021-06-04 13:26:21 | [train_policy] epoch #147 | Time 121.31 s
2021-06-04 13:26:21 | [train_policy] epoch #147 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288518
Evaluation/AverageDiscountedReturn          -46.3039
Evaluation/AverageReturn                    -46.3039
Evaluation/CompletionRate                     0
Evaluation/Iteration                        147
Evaluation/MaxReturn                        -32.7616
Evaluation/MinReturn                       -193.157
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         16.5326
Extras/EpisodeRewardMean                    -46.3021
LinearFeatureBaseline/ExplainedVariance     -11.4776
PolicyExecTime                                0.224273
ProcessExecTime                               0.0317552
TotalEnvSteps                            149776
policy/Entropy                                0.612263
policy/KL                                     0.00650134
policy/KLBefore                               0
policy/LossAfter                             -0.0175152
policy/LossBefore                            -2.59151e-09
policy/Perplexity                             1.8446
policy/dLoss                                  0.0175152
---------------------------------------  ----------------
2021-06-04 13:26:21 | [train_policy] epoch #148 | Obtaining samples for iteration 148...
2021-06-04 13:26:22 | [train_policy] epoch #148 | Logging diagnostics...
2021-06-04 13:26:22 | [train_policy] epoch #148 | Optimizing policy...
2021-06-04 13:26:22 | [train_policy] epoch #148 | Computing loss before
2021-06-04 13:26:22 | [train_policy] epoch #148 | Computing KL before
2021-06-04 13:26:22 | [train_policy] epoch #148 | Optimizing
2021-06-04 13:26:22 | [train_policy] epoch #148 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:22 | [train_policy] epoch #148 | computing loss before
2021-06-04 13:26:22 | [train_policy] epoch #148 | computing gradient
2021-06-04 13:26:22 | [train_policy] epoch #148 | gradient computed
2021-06-04 13:26:22 | [train_policy] epoch #148 | computing descent direction
2021-06-04 13:26:22 | [train_policy] epoch #148 | descent direction computed
2021-06-04 13:26:22 | [train_policy] epoch #148 | backtrack iters: 1
2021-06-04 13:26:22 | [train_policy] epoch #148 | optimization finished
2021-06-04 13:26:22 | [train_policy] epoch #148 | Computing KL after
2021-06-04 13:26:22 | [train_policy] epoch #148 | Computing loss after
2021-06-04 13:26:22 | [train_policy] epoch #148 | Fitting baseline...
2021-06-04 13:26:22 | [train_policy] epoch #148 | Saving snapshot...
2021-06-04 13:26:22 | [train_policy] epoch #148 | Saved
2021-06-04 13:26:22 | [train_policy] epoch #148 | Time 122.12 s
2021-06-04 13:26:22 | [train_policy] epoch #148 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287447
Evaluation/AverageDiscountedReturn          -66.2757
Evaluation/AverageReturn                    -66.2757
Evaluation/CompletionRate                     0
Evaluation/Iteration                        148
Evaluation/MaxReturn                        -33.3212
Evaluation/MinReturn                      -2062.13
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.346
Extras/EpisodeRewardMean                    -64.5906
LinearFeatureBaseline/ExplainedVariance       0.010726
PolicyExecTime                                0.234183
ProcessExecTime                               0.0314271
TotalEnvSteps                            150788
policy/Entropy                                0.600888
policy/KL                                     0.00651926
policy/KLBefore                               0
policy/LossAfter                             -0.0264252
policy/LossBefore                             3.76946e-09
policy/Perplexity                             1.82374
policy/dLoss                                  0.0264252
---------------------------------------  ----------------
2021-06-04 13:26:22 | [train_policy] epoch #149 | Obtaining samples for iteration 149...
2021-06-04 13:26:22 | [train_policy] epoch #149 | Logging diagnostics...
2021-06-04 13:26:22 | [train_policy] epoch #149 | Optimizing policy...
2021-06-04 13:26:22 | [train_policy] epoch #149 | Computing loss before
2021-06-04 13:26:22 | [train_policy] epoch #149 | Computing KL before
2021-06-04 13:26:22 | [train_policy] epoch #149 | Optimizing
2021-06-04 13:26:22 | [train_policy] epoch #149 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:22 | [train_policy] epoch #149 | computing loss before
2021-06-04 13:26:22 | [train_policy] epoch #149 | computing gradient
2021-06-04 13:26:22 | [train_policy] epoch #149 | gradient computed
2021-06-04 13:26:22 | [train_policy] epoch #149 | computing descent direction
2021-06-04 13:26:22 | [train_policy] epoch #149 | descent direction computed
2021-06-04 13:26:22 | [train_policy] epoch #149 | backtrack iters: 1
2021-06-04 13:26:22 | [train_policy] epoch #149 | optimization finished
2021-06-04 13:26:22 | [train_policy] epoch #149 | Computing KL after
2021-06-04 13:26:22 | [train_policy] epoch #149 | Computing loss after
2021-06-04 13:26:22 | [train_policy] epoch #149 | Fitting baseline...
2021-06-04 13:26:23 | [train_policy] epoch #149 | Saving snapshot...
2021-06-04 13:26:23 | [train_policy] epoch #149 | Saved
2021-06-04 13:26:23 | [train_policy] epoch #149 | Time 122.93 s
2021-06-04 13:26:23 | [train_policy] epoch #149 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285748
Evaluation/AverageDiscountedReturn          -65.8376
Evaluation/AverageReturn                    -65.8376
Evaluation/CompletionRate                     0
Evaluation/Iteration                        149
Evaluation/MaxReturn                        -32.1135
Evaluation/MinReturn                      -2056.09
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.74
Extras/EpisodeRewardMean                    -84.3461
LinearFeatureBaseline/ExplainedVariance       0.0355159
PolicyExecTime                                0.228357
ProcessExecTime                               0.0312781
TotalEnvSteps                            151800
policy/Entropy                                0.609936
policy/KL                                     0.00660261
policy/KLBefore                               0
policy/LossAfter                             -0.028414
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             1.84031
policy/dLoss                                  0.028414
---------------------------------------  ----------------
2021-06-04 13:26:23 | [train_policy] epoch #150 | Obtaining samples for iteration 150...
2021-06-04 13:26:23 | [train_policy] epoch #150 | Logging diagnostics...
2021-06-04 13:26:23 | [train_policy] epoch #150 | Optimizing policy...
2021-06-04 13:26:23 | [train_policy] epoch #150 | Computing loss before
2021-06-04 13:26:23 | [train_policy] epoch #150 | Computing KL before
2021-06-04 13:26:23 | [train_policy] epoch #150 | Optimizing
2021-06-04 13:26:23 | [train_policy] epoch #150 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:23 | [train_policy] epoch #150 | computing loss before
2021-06-04 13:26:23 | [train_policy] epoch #150 | computing gradient
2021-06-04 13:26:23 | [train_policy] epoch #150 | gradient computed
2021-06-04 13:26:23 | [train_policy] epoch #150 | computing descent direction
2021-06-04 13:26:23 | [train_policy] epoch #150 | descent direction computed
2021-06-04 13:26:23 | [train_policy] epoch #150 | backtrack iters: 1
2021-06-04 13:26:23 | [train_policy] epoch #150 | optimization finished
2021-06-04 13:26:23 | [train_policy] epoch #150 | Computing KL after
2021-06-04 13:26:23 | [train_policy] epoch #150 | Computing loss after
2021-06-04 13:26:23 | [train_policy] epoch #150 | Fitting baseline...
2021-06-04 13:26:23 | [train_policy] epoch #150 | Saving snapshot...
2021-06-04 13:26:23 | [train_policy] epoch #150 | Saved
2021-06-04 13:26:23 | [train_policy] epoch #150 | Time 123.71 s
2021-06-04 13:26:23 | [train_policy] epoch #150 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285155
Evaluation/AverageDiscountedReturn          -45.294
Evaluation/AverageReturn                    -45.294
Evaluation/CompletionRate                     0
Evaluation/Iteration                        150
Evaluation/MaxReturn                        -33.1331
Evaluation/MinReturn                        -82.505
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.78654
Extras/EpisodeRewardMean                    -45.0105
LinearFeatureBaseline/ExplainedVariance     -16.4692
PolicyExecTime                                0.218821
ProcessExecTime                               0.0312731
TotalEnvSteps                            152812
policy/Entropy                                0.593644
policy/KL                                     0.00649222
policy/KLBefore                               0
policy/LossAfter                             -0.0279422
policy/LossBefore                             1.88473e-09
policy/Perplexity                             1.81057
policy/dLoss                                  0.0279422
---------------------------------------  ----------------
2021-06-04 13:26:23 | [train_policy] epoch #151 | Obtaining samples for iteration 151...
2021-06-04 13:26:24 | [train_policy] epoch #151 | Logging diagnostics...
2021-06-04 13:26:24 | [train_policy] epoch #151 | Optimizing policy...
2021-06-04 13:26:24 | [train_policy] epoch #151 | Computing loss before
2021-06-04 13:26:24 | [train_policy] epoch #151 | Computing KL before
2021-06-04 13:26:24 | [train_policy] epoch #151 | Optimizing
2021-06-04 13:26:24 | [train_policy] epoch #151 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:24 | [train_policy] epoch #151 | computing loss before
2021-06-04 13:26:24 | [train_policy] epoch #151 | computing gradient
2021-06-04 13:26:24 | [train_policy] epoch #151 | gradient computed
2021-06-04 13:26:24 | [train_policy] epoch #151 | computing descent direction
2021-06-04 13:26:24 | [train_policy] epoch #151 | descent direction computed
2021-06-04 13:26:24 | [train_policy] epoch #151 | backtrack iters: 1
2021-06-04 13:26:24 | [train_policy] epoch #151 | optimization finished
2021-06-04 13:26:24 | [train_policy] epoch #151 | Computing KL after
2021-06-04 13:26:24 | [train_policy] epoch #151 | Computing loss after
2021-06-04 13:26:24 | [train_policy] epoch #151 | Fitting baseline...
2021-06-04 13:26:24 | [train_policy] epoch #151 | Saving snapshot...
2021-06-04 13:26:24 | [train_policy] epoch #151 | Saved
2021-06-04 13:26:24 | [train_policy] epoch #151 | Time 124.50 s
2021-06-04 13:26:24 | [train_policy] epoch #151 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288135
Evaluation/AverageDiscountedReturn          -45.5158
Evaluation/AverageReturn                    -45.5158
Evaluation/CompletionRate                     0
Evaluation/Iteration                        151
Evaluation/MaxReturn                        -34.9483
Evaluation/MinReturn                        -64.388
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.65995
Extras/EpisodeRewardMean                    -46.0793
LinearFeatureBaseline/ExplainedVariance       0.92279
PolicyExecTime                                0.227063
ProcessExecTime                               0.03175
TotalEnvSteps                            153824
policy/Entropy                                0.551716
policy/KL                                     0.00793897
policy/KLBefore                               0
policy/LossAfter                             -0.0154678
policy/LossBefore                             1.06016e-08
policy/Perplexity                             1.73623
policy/dLoss                                  0.0154678
---------------------------------------  ----------------
2021-06-04 13:26:24 | [train_policy] epoch #152 | Obtaining samples for iteration 152...
2021-06-04 13:26:25 | [train_policy] epoch #152 | Logging diagnostics...
2021-06-04 13:26:25 | [train_policy] epoch #152 | Optimizing policy...
2021-06-04 13:26:25 | [train_policy] epoch #152 | Computing loss before
2021-06-04 13:26:25 | [train_policy] epoch #152 | Computing KL before
2021-06-04 13:26:25 | [train_policy] epoch #152 | Optimizing
2021-06-04 13:26:25 | [train_policy] epoch #152 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:25 | [train_policy] epoch #152 | computing loss before
2021-06-04 13:26:25 | [train_policy] epoch #152 | computing gradient
2021-06-04 13:26:25 | [train_policy] epoch #152 | gradient computed
2021-06-04 13:26:25 | [train_policy] epoch #152 | computing descent direction
2021-06-04 13:26:25 | [train_policy] epoch #152 | descent direction computed
2021-06-04 13:26:25 | [train_policy] epoch #152 | backtrack iters: 1
2021-06-04 13:26:25 | [train_policy] epoch #152 | optimization finished
2021-06-04 13:26:25 | [train_policy] epoch #152 | Computing KL after
2021-06-04 13:26:25 | [train_policy] epoch #152 | Computing loss after
2021-06-04 13:26:25 | [train_policy] epoch #152 | Fitting baseline...
2021-06-04 13:26:25 | [train_policy] epoch #152 | Saving snapshot...
2021-06-04 13:26:25 | [train_policy] epoch #152 | Saved
2021-06-04 13:26:25 | [train_policy] epoch #152 | Time 125.31 s
2021-06-04 13:26:25 | [train_policy] epoch #152 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28597
Evaluation/AverageDiscountedReturn          -65.5847
Evaluation/AverageReturn                    -65.5847
Evaluation/CompletionRate                     0
Evaluation/Iteration                        152
Evaluation/MaxReturn                        -34.7055
Evaluation/MinReturn                      -2061.75
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.308
Extras/EpisodeRewardMean                    -64.3661
LinearFeatureBaseline/ExplainedVariance       0.0105364
PolicyExecTime                                0.217626
ProcessExecTime                               0.0313778
TotalEnvSteps                            154836
policy/Entropy                                0.5476
policy/KL                                     0.00658868
policy/KLBefore                               0
policy/LossAfter                             -0.0389038
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             1.7291
policy/dLoss                                  0.0389038
---------------------------------------  ----------------
2021-06-04 13:26:25 | [train_policy] epoch #153 | Obtaining samples for iteration 153...
2021-06-04 13:26:26 | [train_policy] epoch #153 | Logging diagnostics...
2021-06-04 13:26:26 | [train_policy] epoch #153 | Optimizing policy...
2021-06-04 13:26:26 | [train_policy] epoch #153 | Computing loss before
2021-06-04 13:26:26 | [train_policy] epoch #153 | Computing KL before
2021-06-04 13:26:26 | [train_policy] epoch #153 | Optimizing
2021-06-04 13:26:26 | [train_policy] epoch #153 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:26 | [train_policy] epoch #153 | computing loss before
2021-06-04 13:26:26 | [train_policy] epoch #153 | computing gradient
2021-06-04 13:26:26 | [train_policy] epoch #153 | gradient computed
2021-06-04 13:26:26 | [train_policy] epoch #153 | computing descent direction
2021-06-04 13:26:26 | [train_policy] epoch #153 | descent direction computed
2021-06-04 13:26:26 | [train_policy] epoch #153 | backtrack iters: 1
2021-06-04 13:26:26 | [train_policy] epoch #153 | optimization finished
2021-06-04 13:26:26 | [train_policy] epoch #153 | Computing KL after
2021-06-04 13:26:26 | [train_policy] epoch #153 | Computing loss after
2021-06-04 13:26:26 | [train_policy] epoch #153 | Fitting baseline...
2021-06-04 13:26:26 | [train_policy] epoch #153 | Saving snapshot...
2021-06-04 13:26:26 | [train_policy] epoch #153 | Saved
2021-06-04 13:26:26 | [train_policy] epoch #153 | Time 126.10 s
2021-06-04 13:26:26 | [train_policy] epoch #153 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288126
Evaluation/AverageDiscountedReturn          -45.5113
Evaluation/AverageReturn                    -45.5113
Evaluation/CompletionRate                     0
Evaluation/Iteration                        153
Evaluation/MaxReturn                        -31.7119
Evaluation/MinReturn                        -64.0604
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.17174
Extras/EpisodeRewardMean                    -45.3993
LinearFeatureBaseline/ExplainedVariance     -62.2438
PolicyExecTime                                0.226798
ProcessExecTime                               0.031656
TotalEnvSteps                            155848
policy/Entropy                                0.528737
policy/KL                                     0.00668352
policy/KLBefore                               0
policy/LossAfter                             -0.0210441
policy/LossBefore                             2.49727e-08
policy/Perplexity                             1.69679
policy/dLoss                                  0.0210441
---------------------------------------  ----------------
2021-06-04 13:26:26 | [train_policy] epoch #154 | Obtaining samples for iteration 154...
2021-06-04 13:26:26 | [train_policy] epoch #154 | Logging diagnostics...
2021-06-04 13:26:26 | [train_policy] epoch #154 | Optimizing policy...
2021-06-04 13:26:26 | [train_policy] epoch #154 | Computing loss before
2021-06-04 13:26:26 | [train_policy] epoch #154 | Computing KL before
2021-06-04 13:26:26 | [train_policy] epoch #154 | Optimizing
2021-06-04 13:26:26 | [train_policy] epoch #154 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:26 | [train_policy] epoch #154 | computing loss before
2021-06-04 13:26:26 | [train_policy] epoch #154 | computing gradient
2021-06-04 13:26:26 | [train_policy] epoch #154 | gradient computed
2021-06-04 13:26:26 | [train_policy] epoch #154 | computing descent direction
2021-06-04 13:26:26 | [train_policy] epoch #154 | descent direction computed
2021-06-04 13:26:26 | [train_policy] epoch #154 | backtrack iters: 1
2021-06-04 13:26:26 | [train_policy] epoch #154 | optimization finished
2021-06-04 13:26:26 | [train_policy] epoch #154 | Computing KL after
2021-06-04 13:26:26 | [train_policy] epoch #154 | Computing loss after
2021-06-04 13:26:26 | [train_policy] epoch #154 | Fitting baseline...
2021-06-04 13:26:26 | [train_policy] epoch #154 | Saving snapshot...
2021-06-04 13:26:26 | [train_policy] epoch #154 | Saved
2021-06-04 13:26:26 | [train_policy] epoch #154 | Time 126.89 s
2021-06-04 13:26:26 | [train_policy] epoch #154 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285248
Evaluation/AverageDiscountedReturn          -44.8465
Evaluation/AverageReturn                    -44.8465
Evaluation/CompletionRate                     0
Evaluation/Iteration                        154
Evaluation/MaxReturn                        -34.9064
Evaluation/MinReturn                        -63.9161
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.01313
Extras/EpisodeRewardMean                    -44.7391
LinearFeatureBaseline/ExplainedVariance       0.942784
PolicyExecTime                                0.209836
ProcessExecTime                               0.0313764
TotalEnvSteps                            156860
policy/Entropy                                0.534063
policy/KL                                     0.00673505
policy/KLBefore                               0
policy/LossAfter                             -0.0178246
policy/LossBefore                             1.86117e-08
policy/Perplexity                             1.70585
policy/dLoss                                  0.0178246
---------------------------------------  ----------------
2021-06-04 13:26:27 | [train_policy] epoch #155 | Obtaining samples for iteration 155...
2021-06-04 13:26:27 | [train_policy] epoch #155 | Logging diagnostics...
2021-06-04 13:26:27 | [train_policy] epoch #155 | Optimizing policy...
2021-06-04 13:26:27 | [train_policy] epoch #155 | Computing loss before
2021-06-04 13:26:27 | [train_policy] epoch #155 | Computing KL before
2021-06-04 13:26:27 | [train_policy] epoch #155 | Optimizing
2021-06-04 13:26:27 | [train_policy] epoch #155 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:26:27 | [train_policy] epoch #155 | computing loss before
2021-06-04 13:26:27 | [train_policy] epoch #155 | computing gradient
2021-06-04 13:26:27 | [train_policy] epoch #155 | gradient computed
2021-06-04 13:26:27 | [train_policy] epoch #155 | computing descent direction
2021-06-04 13:26:27 | [train_policy] epoch #155 | descent direction computed
2021-06-04 13:26:27 | [train_policy] epoch #155 | backtrack iters: 0
2021-06-04 13:26:27 | [train_policy] epoch #155 | optimization finished
2021-06-04 13:26:27 | [train_policy] epoch #155 | Computing KL after
2021-06-04 13:26:27 | [train_policy] epoch #155 | Computing loss after
2021-06-04 13:26:27 | [train_policy] epoch #155 | Fitting baseline...
2021-06-04 13:26:27 | [train_policy] epoch #155 | Saving snapshot...
2021-06-04 13:26:27 | [train_policy] epoch #155 | Saved
2021-06-04 13:26:27 | [train_policy] epoch #155 | Time 127.69 s
2021-06-04 13:26:27 | [train_policy] epoch #155 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.287459
Evaluation/AverageDiscountedReturn          -45.6875
Evaluation/AverageReturn                    -45.6875
Evaluation/CompletionRate                     0
Evaluation/Iteration                        155
Evaluation/MaxReturn                        -31.6236
Evaluation/MinReturn                        -64.4888
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.22457
Extras/EpisodeRewardMean                    -45.3509
LinearFeatureBaseline/ExplainedVariance       0.935073
PolicyExecTime                                0.228019
ProcessExecTime                               0.0314848
TotalEnvSteps                            157872
policy/Entropy                                0.488956
policy/KL                                     0.00982178
policy/KLBefore                               0
policy/LossAfter                             -0.0159109
policy/LossBefore                             1.0366e-08
policy/Perplexity                             1.63061
policy/dLoss                                  0.0159109
---------------------------------------  ---------------
2021-06-04 13:47:46 | [train_policy] Logging to ../models/adni_split0_TRPO_11_7.0_delta_fixed_1.0_1000_1000_2.0_fixed_full_1.0_1.0_11_old_MMSE_32
2021-06-04 13:47:46 | [train_policy] Setting seed to 1
2021-06-04 13:47:48 | [train_policy] Obtaining samples...
2021-06-04 13:47:48 | [train_policy] epoch #0 | Obtaining samples for iteration 0...
2021-06-04 13:47:49 | [train_policy] epoch #0 | Logging diagnostics...
2021-06-04 13:47:49 | [train_policy] epoch #0 | Optimizing policy...
2021-06-04 13:47:49 | [train_policy] epoch #0 | Computing loss before
2021-06-04 13:47:49 | [train_policy] epoch #0 | Computing KL before
2021-06-04 13:47:49 | [train_policy] epoch #0 | Optimizing
2021-06-04 13:47:49 | [train_policy] epoch #0 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:49 | [train_policy] epoch #0 | computing loss before
2021-06-04 13:47:49 | [train_policy] epoch #0 | computing gradient
2021-06-04 13:47:50 | [train_policy] epoch #0 | gradient computed
2021-06-04 13:47:50 | [train_policy] epoch #0 | computing descent direction
2021-06-04 13:47:51 | [train_policy] epoch #0 | descent direction computed
2021-06-04 13:47:51 | [train_policy] epoch #0 | backtrack iters: 0
2021-06-04 13:47:51 | [train_policy] epoch #0 | optimization finished
2021-06-04 13:47:51 | [train_policy] epoch #0 | Computing KL after
2021-06-04 13:47:51 | [train_policy] epoch #0 | Computing loss after
2021-06-04 13:47:52 | [train_policy] epoch #0 | Fitting baseline...
2021-06-04 13:47:52 | [train_policy] epoch #0 | Saving snapshot...
2021-06-04 13:47:52 | [train_policy] epoch #0 | Saved
2021-06-04 13:47:52 | [train_policy] epoch #0 | Time 3.85 s
2021-06-04 13:47:52 | [train_policy] epoch #0 | EpochTime 3.85 s
---------------------------------------  ----------------
EnvExecTime                                   0.688308
Evaluation/AverageDiscountedReturn        -5122.66
Evaluation/AverageReturn                  -5122.66
Evaluation/CompletionRate                     0
Evaluation/Iteration                          0
Evaluation/MaxReturn                        -46.1096
Evaluation/MinReturn                     -22000
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       6669.7
Extras/EpisodeRewardMean                  -5122.66
LinearFeatureBaseline/ExplainedVariance      -2.73945e-09
PolicyExecTime                                0.475126
ProcessExecTime                               0.0748963
TotalEnvSteps                              1012
policy/Entropy                                2.83575
policy/KL                                     0.00934829
policy/KLBefore                               0
policy/LossAfter                             -0.0567007
policy/LossBefore                            -7.89231e-09
policy/Perplexity                            17.0431
policy/dLoss                                  0.0567007
---------------------------------------  ----------------
2021-06-04 13:47:52 | [train_policy] epoch #1 | Obtaining samples for iteration 1...
2021-06-04 13:47:52 | [train_policy] epoch #1 | Logging diagnostics...
2021-06-04 13:47:52 | [train_policy] epoch #1 | Optimizing policy...
2021-06-04 13:47:52 | [train_policy] epoch #1 | Computing loss before
2021-06-04 13:47:52 | [train_policy] epoch #1 | Computing KL before
2021-06-04 13:47:52 | [train_policy] epoch #1 | Optimizing
2021-06-04 13:47:52 | [train_policy] epoch #1 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:52 | [train_policy] epoch #1 | computing loss before
2021-06-04 13:47:52 | [train_policy] epoch #1 | computing gradient
2021-06-04 13:47:52 | [train_policy] epoch #1 | gradient computed
2021-06-04 13:47:52 | [train_policy] epoch #1 | computing descent direction
2021-06-04 13:47:52 | [train_policy] epoch #1 | descent direction computed
2021-06-04 13:47:52 | [train_policy] epoch #1 | backtrack iters: 1
2021-06-04 13:47:52 | [train_policy] epoch #1 | optimization finished
2021-06-04 13:47:52 | [train_policy] epoch #1 | Computing KL after
2021-06-04 13:47:52 | [train_policy] epoch #1 | Computing loss after
2021-06-04 13:47:52 | [train_policy] epoch #1 | Fitting baseline...
2021-06-04 13:47:52 | [train_policy] epoch #1 | Saving snapshot...
2021-06-04 13:47:52 | [train_policy] epoch #1 | Saved
2021-06-04 13:47:52 | [train_policy] epoch #1 | Time 4.64 s
2021-06-04 13:47:52 | [train_policy] epoch #1 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.282666
Evaluation/AverageDiscountedReturn        -3683.34
Evaluation/AverageReturn                  -3683.34
Evaluation/CompletionRate                     0
Evaluation/Iteration                          1
Evaluation/MaxReturn                        -46.6597
Evaluation/MinReturn                     -21882.7
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       5979.86
Extras/EpisodeRewardMean                  -3905.58
LinearFeatureBaseline/ExplainedVariance       0.207244
PolicyExecTime                                0.233349
ProcessExecTime                               0.0309207
TotalEnvSteps                              2024
policy/Entropy                                2.81225
policy/KL                                     0.0067576
policy/KLBefore                               0
policy/LossAfter                             -0.0501936
policy/LossBefore                            -7.06774e-09
policy/Perplexity                            16.6473
policy/dLoss                                  0.0501936
---------------------------------------  ----------------
2021-06-04 13:47:52 | [train_policy] epoch #2 | Obtaining samples for iteration 2...
2021-06-04 13:47:53 | [train_policy] epoch #2 | Logging diagnostics...
2021-06-04 13:47:53 | [train_policy] epoch #2 | Optimizing policy...
2021-06-04 13:47:53 | [train_policy] epoch #2 | Computing loss before
2021-06-04 13:47:53 | [train_policy] epoch #2 | Computing KL before
2021-06-04 13:47:53 | [train_policy] epoch #2 | Optimizing
2021-06-04 13:47:53 | [train_policy] epoch #2 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:53 | [train_policy] epoch #2 | computing loss before
2021-06-04 13:47:53 | [train_policy] epoch #2 | computing gradient
2021-06-04 13:47:53 | [train_policy] epoch #2 | gradient computed
2021-06-04 13:47:53 | [train_policy] epoch #2 | computing descent direction
2021-06-04 13:47:53 | [train_policy] epoch #2 | descent direction computed
2021-06-04 13:47:53 | [train_policy] epoch #2 | backtrack iters: 1
2021-06-04 13:47:53 | [train_policy] epoch #2 | optimization finished
2021-06-04 13:47:53 | [train_policy] epoch #2 | Computing KL after
2021-06-04 13:47:53 | [train_policy] epoch #2 | Computing loss after
2021-06-04 13:47:53 | [train_policy] epoch #2 | Fitting baseline...
2021-06-04 13:47:53 | [train_policy] epoch #2 | Saving snapshot...
2021-06-04 13:47:53 | [train_policy] epoch #2 | Saved
2021-06-04 13:47:53 | [train_policy] epoch #2 | Time 5.44 s
2021-06-04 13:47:53 | [train_policy] epoch #2 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286197
Evaluation/AverageDiscountedReturn        -1294.8
Evaluation/AverageReturn                  -1294.8
Evaluation/CompletionRate                     0
Evaluation/Iteration                          2
Evaluation/MaxReturn                        -44.869
Evaluation/MinReturn                     -14289.2
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       2628.34
Extras/EpisodeRewardMean                  -1564.55
LinearFeatureBaseline/ExplainedVariance      -0.248466
PolicyExecTime                                0.232279
ProcessExecTime                               0.03125
TotalEnvSteps                              3036
policy/Entropy                                2.81041
policy/KL                                     0.00681839
policy/KLBefore                               0
policy/LossAfter                             -0.0387484
policy/LossBefore                             2.23812e-08
policy/Perplexity                            16.6168
policy/dLoss                                  0.0387485
---------------------------------------  ----------------
2021-06-04 13:47:53 | [train_policy] epoch #3 | Obtaining samples for iteration 3...
2021-06-04 13:47:54 | [train_policy] epoch #3 | Logging diagnostics...
2021-06-04 13:47:54 | [train_policy] epoch #3 | Optimizing policy...
2021-06-04 13:47:54 | [train_policy] epoch #3 | Computing loss before
2021-06-04 13:47:54 | [train_policy] epoch #3 | Computing KL before
2021-06-04 13:47:54 | [train_policy] epoch #3 | Optimizing
2021-06-04 13:47:54 | [train_policy] epoch #3 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:54 | [train_policy] epoch #3 | computing loss before
2021-06-04 13:47:54 | [train_policy] epoch #3 | computing gradient
2021-06-04 13:47:54 | [train_policy] epoch #3 | gradient computed
2021-06-04 13:47:54 | [train_policy] epoch #3 | computing descent direction
2021-06-04 13:47:54 | [train_policy] epoch #3 | descent direction computed
2021-06-04 13:47:54 | [train_policy] epoch #3 | backtrack iters: 0
2021-06-04 13:47:54 | [train_policy] epoch #3 | optimization finished
2021-06-04 13:47:54 | [train_policy] epoch #3 | Computing KL after
2021-06-04 13:47:54 | [train_policy] epoch #3 | Computing loss after
2021-06-04 13:47:54 | [train_policy] epoch #3 | Fitting baseline...
2021-06-04 13:47:54 | [train_policy] epoch #3 | Saving snapshot...
2021-06-04 13:47:54 | [train_policy] epoch #3 | Saved
2021-06-04 13:47:54 | [train_policy] epoch #3 | Time 6.21 s
2021-06-04 13:47:54 | [train_policy] epoch #3 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.283615
Evaluation/AverageDiscountedReturn         -744.612
Evaluation/AverageReturn                   -744.612
Evaluation/CompletionRate                     0
Evaluation/Iteration                          3
Evaluation/MaxReturn                        -39.7719
Evaluation/MinReturn                     -12077
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       1983.87
Extras/EpisodeRewardMean                   -770.41
LinearFeatureBaseline/ExplainedVariance       0.148907
PolicyExecTime                                0.215273
ProcessExecTime                               0.0310869
TotalEnvSteps                              4048
policy/Entropy                                2.78259
policy/KL                                     0.00985825
policy/KLBefore                               0
policy/LossAfter                             -0.0420206
policy/LossBefore                            -5.18301e-09
policy/Perplexity                            16.1608
policy/dLoss                                  0.0420206
---------------------------------------  ----------------
2021-06-04 13:47:54 | [train_policy] epoch #4 | Obtaining samples for iteration 4...
2021-06-04 13:47:55 | [train_policy] epoch #4 | Logging diagnostics...
2021-06-04 13:47:55 | [train_policy] epoch #4 | Optimizing policy...
2021-06-04 13:47:55 | [train_policy] epoch #4 | Computing loss before
2021-06-04 13:47:55 | [train_policy] epoch #4 | Computing KL before
2021-06-04 13:47:55 | [train_policy] epoch #4 | Optimizing
2021-06-04 13:47:55 | [train_policy] epoch #4 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:55 | [train_policy] epoch #4 | computing loss before
2021-06-04 13:47:55 | [train_policy] epoch #4 | computing gradient
2021-06-04 13:47:55 | [train_policy] epoch #4 | gradient computed
2021-06-04 13:47:55 | [train_policy] epoch #4 | computing descent direction
2021-06-04 13:47:55 | [train_policy] epoch #4 | descent direction computed
2021-06-04 13:47:55 | [train_policy] epoch #4 | backtrack iters: 0
2021-06-04 13:47:55 | [train_policy] epoch #4 | optimization finished
2021-06-04 13:47:55 | [train_policy] epoch #4 | Computing KL after
2021-06-04 13:47:55 | [train_policy] epoch #4 | Computing loss after
2021-06-04 13:47:55 | [train_policy] epoch #4 | Fitting baseline...
2021-06-04 13:47:55 | [train_policy] epoch #4 | Saving snapshot...
2021-06-04 13:47:55 | [train_policy] epoch #4 | Saved
2021-06-04 13:47:55 | [train_policy] epoch #4 | Time 6.98 s
2021-06-04 13:47:55 | [train_policy] epoch #4 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                  0.28326
Evaluation/AverageDiscountedReturn        -288.814
Evaluation/AverageReturn                  -288.814
Evaluation/CompletionRate                    0
Evaluation/Iteration                         4
Evaluation/MaxReturn                       -38.0542
Evaluation/MinReturn                     -6051.79
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       876.987
Extras/EpisodeRewardMean                  -290.61
LinearFeatureBaseline/ExplainedVariance     -0.239738
PolicyExecTime                               0.221417
ProcessExecTime                              0.0311024
TotalEnvSteps                             5060
policy/Entropy                               2.74427
policy/KL                                    0.00955932
policy/KLBefore                              0
policy/LossAfter                            -0.0190431
policy/LossBefore                            1.81405e-08
policy/Perplexity                           15.5533
policy/dLoss                                 0.0190431
---------------------------------------  ---------------
2021-06-04 13:47:55 | [train_policy] epoch #5 | Obtaining samples for iteration 5...
2021-06-04 13:47:55 | [train_policy] epoch #5 | Logging diagnostics...
2021-06-04 13:47:55 | [train_policy] epoch #5 | Optimizing policy...
2021-06-04 13:47:55 | [train_policy] epoch #5 | Computing loss before
2021-06-04 13:47:55 | [train_policy] epoch #5 | Computing KL before
2021-06-04 13:47:55 | [train_policy] epoch #5 | Optimizing
2021-06-04 13:47:55 | [train_policy] epoch #5 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:55 | [train_policy] epoch #5 | computing loss before
2021-06-04 13:47:55 | [train_policy] epoch #5 | computing gradient
2021-06-04 13:47:55 | [train_policy] epoch #5 | gradient computed
2021-06-04 13:47:55 | [train_policy] epoch #5 | computing descent direction
2021-06-04 13:47:56 | [train_policy] epoch #5 | descent direction computed
2021-06-04 13:47:56 | [train_policy] epoch #5 | backtrack iters: 1
2021-06-04 13:47:56 | [train_policy] epoch #5 | optimization finished
2021-06-04 13:47:56 | [train_policy] epoch #5 | Computing KL after
2021-06-04 13:47:56 | [train_policy] epoch #5 | Computing loss after
2021-06-04 13:47:56 | [train_policy] epoch #5 | Fitting baseline...
2021-06-04 13:47:56 | [train_policy] epoch #5 | Saving snapshot...
2021-06-04 13:47:56 | [train_policy] epoch #5 | Saved
2021-06-04 13:47:56 | [train_policy] epoch #5 | Time 7.78 s
2021-06-04 13:47:56 | [train_policy] epoch #5 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285128
Evaluation/AverageDiscountedReturn         -506.516
Evaluation/AverageReturn                   -506.516
Evaluation/CompletionRate                     0
Evaluation/Iteration                          5
Evaluation/MaxReturn                        -44.9822
Evaluation/MinReturn                     -12431.9
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       1745.84
Extras/EpisodeRewardMean                   -471.339
LinearFeatureBaseline/ExplainedVariance       0.0344033
PolicyExecTime                                0.236902
ProcessExecTime                               0.0312355
TotalEnvSteps                              6072
policy/Entropy                                2.69098
policy/KL                                     0.00695014
policy/KLBefore                               0
policy/LossAfter                             -0.0366384
policy/LossBefore                            -1.06016e-08
policy/Perplexity                            14.746
policy/dLoss                                  0.0366384
---------------------------------------  ----------------
2021-06-04 13:47:56 | [train_policy] epoch #6 | Obtaining samples for iteration 6...
2021-06-04 13:47:56 | [train_policy] epoch #6 | Logging diagnostics...
2021-06-04 13:47:56 | [train_policy] epoch #6 | Optimizing policy...
2021-06-04 13:47:56 | [train_policy] epoch #6 | Computing loss before
2021-06-04 13:47:56 | [train_policy] epoch #6 | Computing KL before
2021-06-04 13:47:56 | [train_policy] epoch #6 | Optimizing
2021-06-04 13:47:56 | [train_policy] epoch #6 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:56 | [train_policy] epoch #6 | computing loss before
2021-06-04 13:47:56 | [train_policy] epoch #6 | computing gradient
2021-06-04 13:47:56 | [train_policy] epoch #6 | gradient computed
2021-06-04 13:47:56 | [train_policy] epoch #6 | computing descent direction
2021-06-04 13:47:56 | [train_policy] epoch #6 | descent direction computed
2021-06-04 13:47:56 | [train_policy] epoch #6 | backtrack iters: 0
2021-06-04 13:47:56 | [train_policy] epoch #6 | optimization finished
2021-06-04 13:47:56 | [train_policy] epoch #6 | Computing KL after
2021-06-04 13:47:56 | [train_policy] epoch #6 | Computing loss after
2021-06-04 13:47:56 | [train_policy] epoch #6 | Fitting baseline...
2021-06-04 13:47:56 | [train_policy] epoch #6 | Saving snapshot...
2021-06-04 13:47:56 | [train_policy] epoch #6 | Saved
2021-06-04 13:47:56 | [train_policy] epoch #6 | Time 8.57 s
2021-06-04 13:47:56 | [train_policy] epoch #6 | EpochTime 0.77 s
---------------------------------------  --------------
EnvExecTime                                  0.286928
Evaluation/AverageDiscountedReturn        -105.083
Evaluation/AverageReturn                  -105.083
Evaluation/CompletionRate                    0
Evaluation/Iteration                         6
Evaluation/MaxReturn                       -45.1769
Evaluation/MinReturn                     -2550.05
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       269.606
Extras/EpisodeRewardMean                  -112.693
LinearFeatureBaseline/ExplainedVariance     -7.88974
PolicyExecTime                               0.225856
ProcessExecTime                              0.0314491
TotalEnvSteps                             7084
policy/Entropy                               2.68803
policy/KL                                    0.00845201
policy/KLBefore                              0
policy/LossAfter                            -0.0229809
policy/LossBefore                            3.8637e-08
policy/Perplexity                           14.7027
policy/dLoss                                 0.022981
---------------------------------------  --------------
2021-06-04 13:47:56 | [train_policy] epoch #7 | Obtaining samples for iteration 7...
2021-06-04 13:47:57 | [train_policy] epoch #7 | Logging diagnostics...
2021-06-04 13:47:57 | [train_policy] epoch #7 | Optimizing policy...
2021-06-04 13:47:57 | [train_policy] epoch #7 | Computing loss before
2021-06-04 13:47:57 | [train_policy] epoch #7 | Computing KL before
2021-06-04 13:47:57 | [train_policy] epoch #7 | Optimizing
2021-06-04 13:47:57 | [train_policy] epoch #7 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:57 | [train_policy] epoch #7 | computing loss before
2021-06-04 13:47:57 | [train_policy] epoch #7 | computing gradient
2021-06-04 13:47:57 | [train_policy] epoch #7 | gradient computed
2021-06-04 13:47:57 | [train_policy] epoch #7 | computing descent direction
2021-06-04 13:47:57 | [train_policy] epoch #7 | descent direction computed
2021-06-04 13:47:57 | [train_policy] epoch #7 | backtrack iters: 1
2021-06-04 13:47:57 | [train_policy] epoch #7 | optimization finished
2021-06-04 13:47:57 | [train_policy] epoch #7 | Computing KL after
2021-06-04 13:47:57 | [train_policy] epoch #7 | Computing loss after
2021-06-04 13:47:57 | [train_policy] epoch #7 | Fitting baseline...
2021-06-04 13:47:57 | [train_policy] epoch #7 | Saving snapshot...
2021-06-04 13:47:57 | [train_policy] epoch #7 | Saved
2021-06-04 13:47:57 | [train_policy] epoch #7 | Time 9.38 s
2021-06-04 13:47:57 | [train_policy] epoch #7 | EpochTime 0.79 s
---------------------------------------  --------------
EnvExecTime                                  0.284056
Evaluation/AverageDiscountedReturn        -173.397
Evaluation/AverageReturn                  -173.397
Evaluation/CompletionRate                    0
Evaluation/Iteration                         7
Evaluation/MaxReturn                       -48.3632
Evaluation/MinReturn                     -2932.42
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       430.828
Extras/EpisodeRewardMean                  -164.712
LinearFeatureBaseline/ExplainedVariance      0.0131095
PolicyExecTime                               0.238894
ProcessExecTime                              0.0311697
TotalEnvSteps                             8096
policy/Entropy                               2.661
policy/KL                                    0.00663461
policy/KLBefore                              0
policy/LossAfter                            -0.0328874
policy/LossBefore                           -5.6542e-09
policy/Perplexity                           14.3105
policy/dLoss                                 0.0328874
---------------------------------------  --------------
2021-06-04 13:47:57 | [train_policy] epoch #8 | Obtaining samples for iteration 8...
2021-06-04 13:47:58 | [train_policy] epoch #8 | Logging diagnostics...
2021-06-04 13:47:58 | [train_policy] epoch #8 | Optimizing policy...
2021-06-04 13:47:58 | [train_policy] epoch #8 | Computing loss before
2021-06-04 13:47:58 | [train_policy] epoch #8 | Computing KL before
2021-06-04 13:47:58 | [train_policy] epoch #8 | Optimizing
2021-06-04 13:47:58 | [train_policy] epoch #8 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:58 | [train_policy] epoch #8 | computing loss before
2021-06-04 13:47:58 | [train_policy] epoch #8 | computing gradient
2021-06-04 13:47:58 | [train_policy] epoch #8 | gradient computed
2021-06-04 13:47:58 | [train_policy] epoch #8 | computing descent direction
2021-06-04 13:47:58 | [train_policy] epoch #8 | descent direction computed
2021-06-04 13:47:58 | [train_policy] epoch #8 | backtrack iters: 1
2021-06-04 13:47:58 | [train_policy] epoch #8 | optimization finished
2021-06-04 13:47:58 | [train_policy] epoch #8 | Computing KL after
2021-06-04 13:47:58 | [train_policy] epoch #8 | Computing loss after
2021-06-04 13:47:58 | [train_policy] epoch #8 | Fitting baseline...
2021-06-04 13:47:58 | [train_policy] epoch #8 | Saving snapshot...
2021-06-04 13:47:58 | [train_policy] epoch #8 | Saved
2021-06-04 13:47:58 | [train_policy] epoch #8 | Time 10.17 s
2021-06-04 13:47:58 | [train_policy] epoch #8 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286039
Evaluation/AverageDiscountedReturn        -113.322
Evaluation/AverageReturn                  -113.322
Evaluation/CompletionRate                    0
Evaluation/Iteration                         8
Evaluation/MaxReturn                       -50.6686
Evaluation/MinReturn                     -2135.54
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       295.714
Extras/EpisodeRewardMean                  -110.03
LinearFeatureBaseline/ExplainedVariance     -0.0616328
PolicyExecTime                               0.224729
ProcessExecTime                              0.0315125
TotalEnvSteps                             9108
policy/Entropy                               2.62293
policy/KL                                    0.00730207
policy/KLBefore                              0
policy/LossAfter                            -0.0301082
policy/LossBefore                           -6.59656e-09
policy/Perplexity                           13.776
policy/dLoss                                 0.0301082
---------------------------------------  ---------------
2021-06-04 13:47:58 | [train_policy] epoch #9 | Obtaining samples for iteration 9...
2021-06-04 13:47:59 | [train_policy] epoch #9 | Logging diagnostics...
2021-06-04 13:47:59 | [train_policy] epoch #9 | Optimizing policy...
2021-06-04 13:47:59 | [train_policy] epoch #9 | Computing loss before
2021-06-04 13:47:59 | [train_policy] epoch #9 | Computing KL before
2021-06-04 13:47:59 | [train_policy] epoch #9 | Optimizing
2021-06-04 13:47:59 | [train_policy] epoch #9 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:59 | [train_policy] epoch #9 | computing loss before
2021-06-04 13:47:59 | [train_policy] epoch #9 | computing gradient
2021-06-04 13:47:59 | [train_policy] epoch #9 | gradient computed
2021-06-04 13:47:59 | [train_policy] epoch #9 | computing descent direction
2021-06-04 13:47:59 | [train_policy] epoch #9 | descent direction computed
2021-06-04 13:47:59 | [train_policy] epoch #9 | backtrack iters: 1
2021-06-04 13:47:59 | [train_policy] epoch #9 | optimization finished
2021-06-04 13:47:59 | [train_policy] epoch #9 | Computing KL after
2021-06-04 13:47:59 | [train_policy] epoch #9 | Computing loss after
2021-06-04 13:47:59 | [train_policy] epoch #9 | Fitting baseline...
2021-06-04 13:47:59 | [train_policy] epoch #9 | Saving snapshot...
2021-06-04 13:47:59 | [train_policy] epoch #9 | Saved
2021-06-04 13:47:59 | [train_policy] epoch #9 | Time 10.95 s
2021-06-04 13:47:59 | [train_policy] epoch #9 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.284679
Evaluation/AverageDiscountedReturn         -66.6334
Evaluation/AverageReturn                   -66.6334
Evaluation/CompletionRate                    0
Evaluation/Iteration                         9
Evaluation/MaxReturn                       -42.8101
Evaluation/MinReturn                       -81.8643
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.00272
Extras/EpisodeRewardMean                   -66.6743
LinearFeatureBaseline/ExplainedVariance     -0.979874
PolicyExecTime                               0.211482
ProcessExecTime                              0.0312643
TotalEnvSteps                            10120
policy/Entropy                               2.62769
policy/KL                                    0.00949231
policy/KLBefore                              0
policy/LossAfter                            -0.0296194
policy/LossBefore                            1.22508e-08
policy/Perplexity                           13.8417
policy/dLoss                                 0.0296194
---------------------------------------  ---------------
2021-06-04 13:47:59 | [train_policy] epoch #10 | Obtaining samples for iteration 10...
2021-06-04 13:47:59 | [train_policy] epoch #10 | Logging diagnostics...
2021-06-04 13:47:59 | [train_policy] epoch #10 | Optimizing policy...
2021-06-04 13:47:59 | [train_policy] epoch #10 | Computing loss before
2021-06-04 13:47:59 | [train_policy] epoch #10 | Computing KL before
2021-06-04 13:47:59 | [train_policy] epoch #10 | Optimizing
2021-06-04 13:47:59 | [train_policy] epoch #10 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:47:59 | [train_policy] epoch #10 | computing loss before
2021-06-04 13:47:59 | [train_policy] epoch #10 | computing gradient
2021-06-04 13:47:59 | [train_policy] epoch #10 | gradient computed
2021-06-04 13:47:59 | [train_policy] epoch #10 | computing descent direction
2021-06-04 13:47:59 | [train_policy] epoch #10 | descent direction computed
2021-06-04 13:47:59 | [train_policy] epoch #10 | backtrack iters: 0
2021-06-04 13:47:59 | [train_policy] epoch #10 | optimization finished
2021-06-04 13:47:59 | [train_policy] epoch #10 | Computing KL after
2021-06-04 13:47:59 | [train_policy] epoch #10 | Computing loss after
2021-06-04 13:47:59 | [train_policy] epoch #10 | Fitting baseline...
2021-06-04 13:47:59 | [train_policy] epoch #10 | Saving snapshot...
2021-06-04 13:48:00 | [train_policy] epoch #10 | Saved
2021-06-04 13:48:00 | [train_policy] epoch #10 | Time 11.73 s
2021-06-04 13:48:00 | [train_policy] epoch #10 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                  0.289255
Evaluation/AverageDiscountedReturn         -94.7807
Evaluation/AverageReturn                   -94.7807
Evaluation/CompletionRate                    0
Evaluation/Iteration                        10
Evaluation/MaxReturn                       -50.3663
Evaluation/MinReturn                     -2060.71
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       212.895
Extras/EpisodeRewardMean                   -92.9078
LinearFeatureBaseline/ExplainedVariance      0.0697027
PolicyExecTime                               0.211757
ProcessExecTime                              0.0317144
TotalEnvSteps                            11132
policy/Entropy                               2.60775
policy/KL                                    0.00888305
policy/KLBefore                              0
policy/LossAfter                            -0.056827
policy/LossBefore                            8.48129e-09
policy/Perplexity                           13.5685
policy/dLoss                                 0.056827
---------------------------------------  ---------------
2021-06-04 13:48:00 | [train_policy] epoch #11 | Obtaining samples for iteration 11...
2021-06-04 13:48:00 | [train_policy] epoch #11 | Logging diagnostics...
2021-06-04 13:48:00 | [train_policy] epoch #11 | Optimizing policy...
2021-06-04 13:48:00 | [train_policy] epoch #11 | Computing loss before
2021-06-04 13:48:00 | [train_policy] epoch #11 | Computing KL before
2021-06-04 13:48:00 | [train_policy] epoch #11 | Optimizing
2021-06-04 13:48:00 | [train_policy] epoch #11 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:00 | [train_policy] epoch #11 | computing loss before
2021-06-04 13:48:00 | [train_policy] epoch #11 | computing gradient
2021-06-04 13:48:00 | [train_policy] epoch #11 | gradient computed
2021-06-04 13:48:00 | [train_policy] epoch #11 | computing descent direction
2021-06-04 13:48:00 | [train_policy] epoch #11 | descent direction computed
2021-06-04 13:48:00 | [train_policy] epoch #11 | backtrack iters: 1
2021-06-04 13:48:00 | [train_policy] epoch #11 | optimization finished
2021-06-04 13:48:00 | [train_policy] epoch #11 | Computing KL after
2021-06-04 13:48:00 | [train_policy] epoch #11 | Computing loss after
2021-06-04 13:48:00 | [train_policy] epoch #11 | Fitting baseline...
2021-06-04 13:48:00 | [train_policy] epoch #11 | Saving snapshot...
2021-06-04 13:48:00 | [train_policy] epoch #11 | Saved
2021-06-04 13:48:00 | [train_policy] epoch #11 | Time 12.51 s
2021-06-04 13:48:00 | [train_policy] epoch #11 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.292223
Evaluation/AverageDiscountedReturn         -71.5651
Evaluation/AverageReturn                   -71.5651
Evaluation/CompletionRate                    0
Evaluation/Iteration                        11
Evaluation/MaxReturn                       -46.4773
Evaluation/MinReturn                      -644.529
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        60.485
Extras/EpisodeRewardMean                   -70.9443
LinearFeatureBaseline/ExplainedVariance      0.107507
PolicyExecTime                               0.216048
ProcessExecTime                              0.0320678
TotalEnvSteps                            12144
policy/Entropy                               2.59793
policy/KL                                    0.00786002
policy/KLBefore                              0
policy/LossAfter                            -0.0261813
policy/LossBefore                           -2.40303e-08
policy/Perplexity                           13.436
policy/dLoss                                 0.0261813
---------------------------------------  ---------------
2021-06-04 13:48:00 | [train_policy] epoch #12 | Obtaining samples for iteration 12...
2021-06-04 13:48:01 | [train_policy] epoch #12 | Logging diagnostics...
2021-06-04 13:48:01 | [train_policy] epoch #12 | Optimizing policy...
2021-06-04 13:48:01 | [train_policy] epoch #12 | Computing loss before
2021-06-04 13:48:01 | [train_policy] epoch #12 | Computing KL before
2021-06-04 13:48:01 | [train_policy] epoch #12 | Optimizing
2021-06-04 13:48:01 | [train_policy] epoch #12 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:01 | [train_policy] epoch #12 | computing loss before
2021-06-04 13:48:01 | [train_policy] epoch #12 | computing gradient
2021-06-04 13:48:01 | [train_policy] epoch #12 | gradient computed
2021-06-04 13:48:01 | [train_policy] epoch #12 | computing descent direction
2021-06-04 13:48:01 | [train_policy] epoch #12 | descent direction computed
2021-06-04 13:48:01 | [train_policy] epoch #12 | backtrack iters: 1
2021-06-04 13:48:01 | [train_policy] epoch #12 | optimization finished
2021-06-04 13:48:01 | [train_policy] epoch #12 | Computing KL after
2021-06-04 13:48:01 | [train_policy] epoch #12 | Computing loss after
2021-06-04 13:48:01 | [train_policy] epoch #12 | Fitting baseline...
2021-06-04 13:48:01 | [train_policy] epoch #12 | Saving snapshot...
2021-06-04 13:48:01 | [train_policy] epoch #12 | Saved
2021-06-04 13:48:01 | [train_policy] epoch #12 | Time 13.31 s
2021-06-04 13:48:01 | [train_policy] epoch #12 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.293415
Evaluation/AverageDiscountedReturn         -77.9967
Evaluation/AverageReturn                   -77.9967
Evaluation/CompletionRate                    0
Evaluation/Iteration                        12
Evaluation/MaxReturn                       -50.2932
Evaluation/MinReturn                     -1151.39
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       113.429
Extras/EpisodeRewardMean                   -76.861
LinearFeatureBaseline/ExplainedVariance      0.220383
PolicyExecTime                               0.231426
ProcessExecTime                              0.0313487
TotalEnvSteps                            13156
policy/Entropy                               2.56392
policy/KL                                    0.00857441
policy/KLBefore                              0
policy/LossAfter                            -0.024485
policy/LossBefore                           -5.6542e-09
policy/Perplexity                           12.9866
policy/dLoss                                 0.024485
---------------------------------------  --------------
2021-06-04 13:48:01 | [train_policy] epoch #13 | Obtaining samples for iteration 13...
2021-06-04 13:48:02 | [train_policy] epoch #13 | Logging diagnostics...
2021-06-04 13:48:02 | [train_policy] epoch #13 | Optimizing policy...
2021-06-04 13:48:02 | [train_policy] epoch #13 | Computing loss before
2021-06-04 13:48:02 | [train_policy] epoch #13 | Computing KL before
2021-06-04 13:48:02 | [train_policy] epoch #13 | Optimizing
2021-06-04 13:48:02 | [train_policy] epoch #13 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:02 | [train_policy] epoch #13 | computing loss before
2021-06-04 13:48:02 | [train_policy] epoch #13 | computing gradient
2021-06-04 13:48:02 | [train_policy] epoch #13 | gradient computed
2021-06-04 13:48:02 | [train_policy] epoch #13 | computing descent direction
2021-06-04 13:48:02 | [train_policy] epoch #13 | descent direction computed
2021-06-04 13:48:02 | [train_policy] epoch #13 | backtrack iters: 0
2021-06-04 13:48:02 | [train_policy] epoch #13 | optimization finished
2021-06-04 13:48:02 | [train_policy] epoch #13 | Computing KL after
2021-06-04 13:48:02 | [train_policy] epoch #13 | Computing loss after
2021-06-04 13:48:02 | [train_policy] epoch #13 | Fitting baseline...
2021-06-04 13:48:02 | [train_policy] epoch #13 | Saving snapshot...
2021-06-04 13:48:02 | [train_policy] epoch #13 | Saved
2021-06-04 13:48:02 | [train_policy] epoch #13 | Time 14.10 s
2021-06-04 13:48:02 | [train_policy] epoch #13 | EpochTime 0.76 s
---------------------------------------  --------------
EnvExecTime                                  0.287055
Evaluation/AverageDiscountedReturn         -71.4876
Evaluation/AverageReturn                   -71.4876
Evaluation/CompletionRate                    0
Evaluation/Iteration                        13
Evaluation/MaxReturn                       -49.6466
Evaluation/MinReturn                      -850.647
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        81.9406
Extras/EpisodeRewardMean                   -70.9407
LinearFeatureBaseline/ExplainedVariance      0.2435
PolicyExecTime                               0.219408
ProcessExecTime                              0.0313952
TotalEnvSteps                            14168
policy/Entropy                               2.55372
policy/KL                                    0.00688058
policy/KLBefore                              0
policy/LossAfter                            -0.0444095
policy/LossBefore                            3.4043e-08
policy/Perplexity                           12.8548
policy/dLoss                                 0.0444096
---------------------------------------  --------------
2021-06-04 13:48:02 | [train_policy] epoch #14 | Obtaining samples for iteration 14...
2021-06-04 13:48:03 | [train_policy] epoch #14 | Logging diagnostics...
2021-06-04 13:48:03 | [train_policy] epoch #14 | Optimizing policy...
2021-06-04 13:48:03 | [train_policy] epoch #14 | Computing loss before
2021-06-04 13:48:03 | [train_policy] epoch #14 | Computing KL before
2021-06-04 13:48:03 | [train_policy] epoch #14 | Optimizing
2021-06-04 13:48:03 | [train_policy] epoch #14 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:03 | [train_policy] epoch #14 | computing loss before
2021-06-04 13:48:03 | [train_policy] epoch #14 | computing gradient
2021-06-04 13:48:03 | [train_policy] epoch #14 | gradient computed
2021-06-04 13:48:03 | [train_policy] epoch #14 | computing descent direction
2021-06-04 13:48:03 | [train_policy] epoch #14 | descent direction computed
2021-06-04 13:48:03 | [train_policy] epoch #14 | backtrack iters: 1
2021-06-04 13:48:03 | [train_policy] epoch #14 | optimization finished
2021-06-04 13:48:03 | [train_policy] epoch #14 | Computing KL after
2021-06-04 13:48:03 | [train_policy] epoch #14 | Computing loss after
2021-06-04 13:48:03 | [train_policy] epoch #14 | Fitting baseline...
2021-06-04 13:48:03 | [train_policy] epoch #14 | Saving snapshot...
2021-06-04 13:48:03 | [train_policy] epoch #14 | Saved
2021-06-04 13:48:03 | [train_policy] epoch #14 | Time 14.90 s
2021-06-04 13:48:03 | [train_policy] epoch #14 | EpochTime 0.77 s
---------------------------------------  --------------
EnvExecTime                                  0.283268
Evaluation/AverageDiscountedReturn         -66.5627
Evaluation/AverageReturn                   -66.5627
Evaluation/CompletionRate                    0
Evaluation/Iteration                        14
Evaluation/MaxReturn                       -48.0745
Evaluation/MinReturn                      -186.857
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        18.8422
Extras/EpisodeRewardMean                   -66.4039
LinearFeatureBaseline/ExplainedVariance      0.44959
PolicyExecTime                               0.214931
ProcessExecTime                              0.0310326
TotalEnvSteps                            15180
policy/Entropy                               2.48757
policy/KL                                    0.00710883
policy/KLBefore                              0
policy/LossAfter                            -0.0180786
policy/LossBefore                           -0
policy/Perplexity                           12.032
policy/dLoss                                 0.0180786
---------------------------------------  --------------
2021-06-04 13:48:03 | [train_policy] epoch #15 | Obtaining samples for iteration 15...
2021-06-04 13:48:03 | [train_policy] epoch #15 | Logging diagnostics...
2021-06-04 13:48:03 | [train_policy] epoch #15 | Optimizing policy...
2021-06-04 13:48:03 | [train_policy] epoch #15 | Computing loss before
2021-06-04 13:48:03 | [train_policy] epoch #15 | Computing KL before
2021-06-04 13:48:03 | [train_policy] epoch #15 | Optimizing
2021-06-04 13:48:03 | [train_policy] epoch #15 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:03 | [train_policy] epoch #15 | computing loss before
2021-06-04 13:48:03 | [train_policy] epoch #15 | computing gradient
2021-06-04 13:48:03 | [train_policy] epoch #15 | gradient computed
2021-06-04 13:48:03 | [train_policy] epoch #15 | computing descent direction
2021-06-04 13:48:03 | [train_policy] epoch #15 | descent direction computed
2021-06-04 13:48:03 | [train_policy] epoch #15 | backtrack iters: 0
2021-06-04 13:48:03 | [train_policy] epoch #15 | optimization finished
2021-06-04 13:48:03 | [train_policy] epoch #15 | Computing KL after
2021-06-04 13:48:03 | [train_policy] epoch #15 | Computing loss after
2021-06-04 13:48:03 | [train_policy] epoch #15 | Fitting baseline...
2021-06-04 13:48:03 | [train_policy] epoch #15 | Saving snapshot...
2021-06-04 13:48:03 | [train_policy] epoch #15 | Saved
2021-06-04 13:48:03 | [train_policy] epoch #15 | Time 15.70 s
2021-06-04 13:48:03 | [train_policy] epoch #15 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285774
Evaluation/AverageDiscountedReturn         -61.7612
Evaluation/AverageReturn                   -61.7612
Evaluation/CompletionRate                    0
Evaluation/Iteration                        15
Evaluation/MaxReturn                       -47.4019
Evaluation/MinReturn                       -79.3068
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.07376
Extras/EpisodeRewardMean                   -62.9212
LinearFeatureBaseline/ExplainedVariance      0.938586
PolicyExecTime                               0.230771
ProcessExecTime                              0.0313342
TotalEnvSteps                            16192
policy/Entropy                               2.47716
policy/KL                                    0.00972453
policy/KLBefore                              0
policy/LossAfter                            -0.0443576
policy/LossBefore                            5.27725e-08
policy/Perplexity                           11.9074
policy/dLoss                                 0.0443577
---------------------------------------  ---------------
2021-06-04 13:48:04 | [train_policy] epoch #16 | Obtaining samples for iteration 16...
2021-06-04 13:48:04 | [train_policy] epoch #16 | Logging diagnostics...
2021-06-04 13:48:04 | [train_policy] epoch #16 | Optimizing policy...
2021-06-04 13:48:04 | [train_policy] epoch #16 | Computing loss before
2021-06-04 13:48:04 | [train_policy] epoch #16 | Computing KL before
2021-06-04 13:48:04 | [train_policy] epoch #16 | Optimizing
2021-06-04 13:48:04 | [train_policy] epoch #16 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:04 | [train_policy] epoch #16 | computing loss before
2021-06-04 13:48:04 | [train_policy] epoch #16 | computing gradient
2021-06-04 13:48:04 | [train_policy] epoch #16 | gradient computed
2021-06-04 13:48:04 | [train_policy] epoch #16 | computing descent direction
2021-06-04 13:48:04 | [train_policy] epoch #16 | descent direction computed
2021-06-04 13:48:04 | [train_policy] epoch #16 | backtrack iters: 1
2021-06-04 13:48:04 | [train_policy] epoch #16 | optimization finished
2021-06-04 13:48:04 | [train_policy] epoch #16 | Computing KL after
2021-06-04 13:48:04 | [train_policy] epoch #16 | Computing loss after
2021-06-04 13:48:04 | [train_policy] epoch #16 | Fitting baseline...
2021-06-04 13:48:04 | [train_policy] epoch #16 | Saving snapshot...
2021-06-04 13:48:04 | [train_policy] epoch #16 | Saved
2021-06-04 13:48:04 | [train_policy] epoch #16 | Time 16.51 s
2021-06-04 13:48:04 | [train_policy] epoch #16 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.286344
Evaluation/AverageDiscountedReturn         -60.6705
Evaluation/AverageReturn                   -60.6705
Evaluation/CompletionRate                    0
Evaluation/Iteration                        16
Evaluation/MaxReturn                       -44.7885
Evaluation/MinReturn                       -75.6125
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.56826
Extras/EpisodeRewardMean                   -60.6918
LinearFeatureBaseline/ExplainedVariance      0.96074
PolicyExecTime                               0.235557
ProcessExecTime                              0.0316255
TotalEnvSteps                            17204
policy/Entropy                               2.48309
policy/KL                                    0.00654399
policy/KLBefore                              0
policy/LossAfter                            -0.0432797
policy/LossBefore                           -1.69626e-08
policy/Perplexity                           11.9782
policy/dLoss                                 0.0432797
---------------------------------------  ---------------
2021-06-04 13:48:04 | [train_policy] epoch #17 | Obtaining samples for iteration 17...
2021-06-04 13:48:05 | [train_policy] epoch #17 | Logging diagnostics...
2021-06-04 13:48:05 | [train_policy] epoch #17 | Optimizing policy...
2021-06-04 13:48:05 | [train_policy] epoch #17 | Computing loss before
2021-06-04 13:48:05 | [train_policy] epoch #17 | Computing KL before
2021-06-04 13:48:05 | [train_policy] epoch #17 | Optimizing
2021-06-04 13:48:05 | [train_policy] epoch #17 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:05 | [train_policy] epoch #17 | computing loss before
2021-06-04 13:48:05 | [train_policy] epoch #17 | computing gradient
2021-06-04 13:48:05 | [train_policy] epoch #17 | gradient computed
2021-06-04 13:48:05 | [train_policy] epoch #17 | computing descent direction
2021-06-04 13:48:05 | [train_policy] epoch #17 | descent direction computed
2021-06-04 13:48:05 | [train_policy] epoch #17 | backtrack iters: 0
2021-06-04 13:48:05 | [train_policy] epoch #17 | optimization finished
2021-06-04 13:48:05 | [train_policy] epoch #17 | Computing KL after
2021-06-04 13:48:05 | [train_policy] epoch #17 | Computing loss after
2021-06-04 13:48:05 | [train_policy] epoch #17 | Fitting baseline...
2021-06-04 13:48:05 | [train_policy] epoch #17 | Saving snapshot...
2021-06-04 13:48:05 | [train_policy] epoch #17 | Saved
2021-06-04 13:48:05 | [train_policy] epoch #17 | Time 17.28 s
2021-06-04 13:48:05 | [train_policy] epoch #17 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                  0.285074
Evaluation/AverageDiscountedReturn         -81.7775
Evaluation/AverageReturn                   -81.7775
Evaluation/CompletionRate                    0
Evaluation/Iteration                        17
Evaluation/MaxReturn                       -46.6136
Evaluation/MinReturn                     -2056.15
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       207.095
Extras/EpisodeRewardMean                   -79.9119
LinearFeatureBaseline/ExplainedVariance      0.0962828
PolicyExecTime                               0.21088
ProcessExecTime                              0.0312524
TotalEnvSteps                            18216
policy/Entropy                               2.47591
policy/KL                                    0.0099051
policy/KLBefore                              0
policy/LossAfter                            -0.0440909
policy/LossBefore                            1.31931e-08
policy/Perplexity                           11.8925
policy/dLoss                                 0.0440909
---------------------------------------  ---------------
2021-06-04 13:48:05 | [train_policy] epoch #18 | Obtaining samples for iteration 18...
2021-06-04 13:48:06 | [train_policy] epoch #18 | Logging diagnostics...
2021-06-04 13:48:06 | [train_policy] epoch #18 | Optimizing policy...
2021-06-04 13:48:06 | [train_policy] epoch #18 | Computing loss before
2021-06-04 13:48:06 | [train_policy] epoch #18 | Computing KL before
2021-06-04 13:48:06 | [train_policy] epoch #18 | Optimizing
2021-06-04 13:48:06 | [train_policy] epoch #18 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:06 | [train_policy] epoch #18 | computing loss before
2021-06-04 13:48:06 | [train_policy] epoch #18 | computing gradient
2021-06-04 13:48:06 | [train_policy] epoch #18 | gradient computed
2021-06-04 13:48:06 | [train_policy] epoch #18 | computing descent direction
2021-06-04 13:48:06 | [train_policy] epoch #18 | descent direction computed
2021-06-04 13:48:06 | [train_policy] epoch #18 | backtrack iters: 0
2021-06-04 13:48:06 | [train_policy] epoch #18 | optimization finished
2021-06-04 13:48:06 | [train_policy] epoch #18 | Computing KL after
2021-06-04 13:48:06 | [train_policy] epoch #18 | Computing loss after
2021-06-04 13:48:06 | [train_policy] epoch #18 | Fitting baseline...
2021-06-04 13:48:06 | [train_policy] epoch #18 | Saving snapshot...
2021-06-04 13:48:06 | [train_policy] epoch #18 | Saved
2021-06-04 13:48:06 | [train_policy] epoch #18 | Time 18.08 s
2021-06-04 13:48:06 | [train_policy] epoch #18 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285561
Evaluation/AverageDiscountedReturn         -82.9594
Evaluation/AverageReturn                   -82.9594
Evaluation/CompletionRate                    0
Evaluation/Iteration                        18
Evaluation/MaxReturn                       -42.9473
Evaluation/MinReturn                     -2047.9
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       206.711
Extras/EpisodeRewardMean                   -80.8634
LinearFeatureBaseline/ExplainedVariance      0.0538834
PolicyExecTime                               0.230185
ProcessExecTime                              0.031173
TotalEnvSteps                            19228
policy/Entropy                               2.41438
policy/KL                                    0.00942015
policy/KLBefore                              0
policy/LossAfter                            -0.0258373
policy/LossBefore                            7.06774e-09
policy/Perplexity                           11.1828
policy/dLoss                                 0.0258373
---------------------------------------  ---------------
2021-06-04 13:48:06 | [train_policy] epoch #19 | Obtaining samples for iteration 19...
2021-06-04 13:48:07 | [train_policy] epoch #19 | Logging diagnostics...
2021-06-04 13:48:07 | [train_policy] epoch #19 | Optimizing policy...
2021-06-04 13:48:07 | [train_policy] epoch #19 | Computing loss before
2021-06-04 13:48:07 | [train_policy] epoch #19 | Computing KL before
2021-06-04 13:48:07 | [train_policy] epoch #19 | Optimizing
2021-06-04 13:48:07 | [train_policy] epoch #19 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:07 | [train_policy] epoch #19 | computing loss before
2021-06-04 13:48:07 | [train_policy] epoch #19 | computing gradient
2021-06-04 13:48:07 | [train_policy] epoch #19 | gradient computed
2021-06-04 13:48:07 | [train_policy] epoch #19 | computing descent direction
2021-06-04 13:48:07 | [train_policy] epoch #19 | descent direction computed
2021-06-04 13:48:07 | [train_policy] epoch #19 | backtrack iters: 1
2021-06-04 13:48:07 | [train_policy] epoch #19 | optimization finished
2021-06-04 13:48:07 | [train_policy] epoch #19 | Computing KL after
2021-06-04 13:48:07 | [train_policy] epoch #19 | Computing loss after
2021-06-04 13:48:07 | [train_policy] epoch #19 | Fitting baseline...
2021-06-04 13:48:07 | [train_policy] epoch #19 | Saving snapshot...
2021-06-04 13:48:07 | [train_policy] epoch #19 | Saved
2021-06-04 13:48:07 | [train_policy] epoch #19 | Time 18.89 s
2021-06-04 13:48:07 | [train_policy] epoch #19 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.289682
Evaluation/AverageDiscountedReturn         -58.0678
Evaluation/AverageReturn                   -58.0678
Evaluation/CompletionRate                    0
Evaluation/Iteration                        19
Evaluation/MaxReturn                       -39.4737
Evaluation/MinReturn                       -94.5962
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.14259
Extras/EpisodeRewardMean                   -57.9234
LinearFeatureBaseline/ExplainedVariance      0.558672
PolicyExecTime                               0.232583
ProcessExecTime                              0.0316653
TotalEnvSteps                            20240
policy/Entropy                               2.40713
policy/KL                                    0.00715282
policy/KLBefore                              0
policy/LossAfter                            -0.0262864
policy/LossBefore                           -1.64914e-08
policy/Perplexity                           11.1021
policy/dLoss                                 0.0262864
---------------------------------------  ---------------
2021-06-04 13:48:07 | [train_policy] epoch #20 | Obtaining samples for iteration 20...
2021-06-04 13:48:07 | [train_policy] epoch #20 | Logging diagnostics...
2021-06-04 13:48:07 | [train_policy] epoch #20 | Optimizing policy...
2021-06-04 13:48:07 | [train_policy] epoch #20 | Computing loss before
2021-06-04 13:48:07 | [train_policy] epoch #20 | Computing KL before
2021-06-04 13:48:07 | [train_policy] epoch #20 | Optimizing
2021-06-04 13:48:07 | [train_policy] epoch #20 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:07 | [train_policy] epoch #20 | computing loss before
2021-06-04 13:48:07 | [train_policy] epoch #20 | computing gradient
2021-06-04 13:48:07 | [train_policy] epoch #20 | gradient computed
2021-06-04 13:48:07 | [train_policy] epoch #20 | computing descent direction
2021-06-04 13:48:07 | [train_policy] epoch #20 | descent direction computed
2021-06-04 13:48:07 | [train_policy] epoch #20 | backtrack iters: 1
2021-06-04 13:48:07 | [train_policy] epoch #20 | optimization finished
2021-06-04 13:48:07 | [train_policy] epoch #20 | Computing KL after
2021-06-04 13:48:07 | [train_policy] epoch #20 | Computing loss after
2021-06-04 13:48:07 | [train_policy] epoch #20 | Fitting baseline...
2021-06-04 13:48:07 | [train_policy] epoch #20 | Saving snapshot...
2021-06-04 13:48:07 | [train_policy] epoch #20 | Saved
2021-06-04 13:48:07 | [train_policy] epoch #20 | Time 19.69 s
2021-06-04 13:48:07 | [train_policy] epoch #20 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.284151
Evaluation/AverageDiscountedReturn         -58.8253
Evaluation/AverageReturn                   -58.8253
Evaluation/CompletionRate                    0
Evaluation/Iteration                        20
Evaluation/MaxReturn                       -42.0908
Evaluation/MinReturn                      -107.256
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.92338
Extras/EpisodeRewardMean                   -59.2438
LinearFeatureBaseline/ExplainedVariance      0.928403
PolicyExecTime                               0.217776
ProcessExecTime                              0.031131
TotalEnvSteps                            21252
policy/Entropy                               2.39113
policy/KL                                    0.00668978
policy/KLBefore                              0
policy/LossAfter                            -0.0306618
policy/LossBefore                           -5.18301e-09
policy/Perplexity                           10.9259
policy/dLoss                                 0.0306618
---------------------------------------  ---------------
2021-06-04 13:48:07 | [train_policy] epoch #21 | Obtaining samples for iteration 21...
2021-06-04 13:48:08 | [train_policy] epoch #21 | Logging diagnostics...
2021-06-04 13:48:08 | [train_policy] epoch #21 | Optimizing policy...
2021-06-04 13:48:08 | [train_policy] epoch #21 | Computing loss before
2021-06-04 13:48:08 | [train_policy] epoch #21 | Computing KL before
2021-06-04 13:48:08 | [train_policy] epoch #21 | Optimizing
2021-06-04 13:48:08 | [train_policy] epoch #21 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:08 | [train_policy] epoch #21 | computing loss before
2021-06-04 13:48:08 | [train_policy] epoch #21 | computing gradient
2021-06-04 13:48:08 | [train_policy] epoch #21 | gradient computed
2021-06-04 13:48:08 | [train_policy] epoch #21 | computing descent direction
2021-06-04 13:48:08 | [train_policy] epoch #21 | descent direction computed
2021-06-04 13:48:08 | [train_policy] epoch #21 | backtrack iters: 1
2021-06-04 13:48:08 | [train_policy] epoch #21 | optimization finished
2021-06-04 13:48:08 | [train_policy] epoch #21 | Computing KL after
2021-06-04 13:48:08 | [train_policy] epoch #21 | Computing loss after
2021-06-04 13:48:08 | [train_policy] epoch #21 | Fitting baseline...
2021-06-04 13:48:08 | [train_policy] epoch #21 | Saving snapshot...
2021-06-04 13:48:08 | [train_policy] epoch #21 | Saved
2021-06-04 13:48:08 | [train_policy] epoch #21 | Time 20.49 s
2021-06-04 13:48:08 | [train_policy] epoch #21 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285455
Evaluation/AverageDiscountedReturn         -77.715
Evaluation/AverageReturn                   -77.715
Evaluation/CompletionRate                    0
Evaluation/Iteration                        21
Evaluation/MaxReturn                       -45.6608
Evaluation/MinReturn                     -2043.32
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       206.215
Extras/EpisodeRewardMean                   -75.9258
LinearFeatureBaseline/ExplainedVariance      0.0193748
PolicyExecTime                               0.231543
ProcessExecTime                              0.0312555
TotalEnvSteps                            22264
policy/Entropy                               2.35727
policy/KL                                    0.00648399
policy/KLBefore                              0
policy/LossAfter                            -0.0292512
policy/LossBefore                           -1.60202e-08
policy/Perplexity                           10.5621
policy/dLoss                                 0.0292512
---------------------------------------  ---------------
2021-06-04 13:48:08 | [train_policy] epoch #22 | Obtaining samples for iteration 22...
2021-06-04 13:48:09 | [train_policy] epoch #22 | Logging diagnostics...
2021-06-04 13:48:09 | [train_policy] epoch #22 | Optimizing policy...
2021-06-04 13:48:09 | [train_policy] epoch #22 | Computing loss before
2021-06-04 13:48:09 | [train_policy] epoch #22 | Computing KL before
2021-06-04 13:48:09 | [train_policy] epoch #22 | Optimizing
2021-06-04 13:48:09 | [train_policy] epoch #22 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:09 | [train_policy] epoch #22 | computing loss before
2021-06-04 13:48:09 | [train_policy] epoch #22 | computing gradient
2021-06-04 13:48:09 | [train_policy] epoch #22 | gradient computed
2021-06-04 13:48:09 | [train_policy] epoch #22 | computing descent direction
2021-06-04 13:48:09 | [train_policy] epoch #22 | descent direction computed
2021-06-04 13:48:09 | [train_policy] epoch #22 | backtrack iters: 1
2021-06-04 13:48:09 | [train_policy] epoch #22 | optimization finished
2021-06-04 13:48:09 | [train_policy] epoch #22 | Computing KL after
2021-06-04 13:48:09 | [train_policy] epoch #22 | Computing loss after
2021-06-04 13:48:09 | [train_policy] epoch #22 | Fitting baseline...
2021-06-04 13:48:09 | [train_policy] epoch #22 | Saving snapshot...
2021-06-04 13:48:09 | [train_policy] epoch #22 | Saved
2021-06-04 13:48:09 | [train_policy] epoch #22 | Time 21.27 s
2021-06-04 13:48:09 | [train_policy] epoch #22 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.28831
Evaluation/AverageDiscountedReturn        -102.335
Evaluation/AverageReturn                  -102.335
Evaluation/CompletionRate                    0
Evaluation/Iteration                        22
Evaluation/MaxReturn                       -42.6
Evaluation/MinReturn                     -4040.84
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       413.457
Extras/EpisodeRewardMean                   -99.2739
LinearFeatureBaseline/ExplainedVariance      0.0120122
PolicyExecTime                               0.209062
ProcessExecTime                              0.0313392
TotalEnvSteps                            23276
policy/Entropy                               2.30829
policy/KL                                    0.00706849
policy/KLBefore                              0
policy/LossAfter                            -0.022293
policy/LossBefore                           -3.76946e-09
policy/Perplexity                           10.0572
policy/dLoss                                 0.022293
---------------------------------------  ---------------
2021-06-04 13:48:09 | [train_policy] epoch #23 | Obtaining samples for iteration 23...
2021-06-04 13:48:10 | [train_policy] epoch #23 | Logging diagnostics...
2021-06-04 13:48:10 | [train_policy] epoch #23 | Optimizing policy...
2021-06-04 13:48:10 | [train_policy] epoch #23 | Computing loss before
2021-06-04 13:48:10 | [train_policy] epoch #23 | Computing KL before
2021-06-04 13:48:10 | [train_policy] epoch #23 | Optimizing
2021-06-04 13:48:10 | [train_policy] epoch #23 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:10 | [train_policy] epoch #23 | computing loss before
2021-06-04 13:48:10 | [train_policy] epoch #23 | computing gradient
2021-06-04 13:48:10 | [train_policy] epoch #23 | gradient computed
2021-06-04 13:48:10 | [train_policy] epoch #23 | computing descent direction
2021-06-04 13:48:10 | [train_policy] epoch #23 | descent direction computed
2021-06-04 13:48:10 | [train_policy] epoch #23 | backtrack iters: 0
2021-06-04 13:48:10 | [train_policy] epoch #23 | optimization finished
2021-06-04 13:48:10 | [train_policy] epoch #23 | Computing KL after
2021-06-04 13:48:10 | [train_policy] epoch #23 | Computing loss after
2021-06-04 13:48:10 | [train_policy] epoch #23 | Fitting baseline...
2021-06-04 13:48:10 | [train_policy] epoch #23 | Saving snapshot...
2021-06-04 13:48:10 | [train_policy] epoch #23 | Saved
2021-06-04 13:48:10 | [train_policy] epoch #23 | Time 22.07 s
2021-06-04 13:48:10 | [train_policy] epoch #23 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285207
Evaluation/AverageDiscountedReturn         -79.0956
Evaluation/AverageReturn                   -79.0956
Evaluation/CompletionRate                    0
Evaluation/Iteration                        23
Evaluation/MaxReturn                       -45.4471
Evaluation/MinReturn                     -2066.87
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.464
Extras/EpisodeRewardMean                   -77.2436
LinearFeatureBaseline/ExplainedVariance     -0.0386644
PolicyExecTime                               0.226908
ProcessExecTime                              0.0312436
TotalEnvSteps                            24288
policy/Entropy                               2.30969
policy/KL                                    0.00912523
policy/KLBefore                              0
policy/LossAfter                            -0.0199611
policy/LossBefore                           -7.06774e-09
policy/Perplexity                           10.0713
policy/dLoss                                 0.0199611
---------------------------------------  ---------------
2021-06-04 13:48:10 | [train_policy] epoch #24 | Obtaining samples for iteration 24...
2021-06-04 13:48:10 | [train_policy] epoch #24 | Logging diagnostics...
2021-06-04 13:48:10 | [train_policy] epoch #24 | Optimizing policy...
2021-06-04 13:48:10 | [train_policy] epoch #24 | Computing loss before
2021-06-04 13:48:10 | [train_policy] epoch #24 | Computing KL before
2021-06-04 13:48:10 | [train_policy] epoch #24 | Optimizing
2021-06-04 13:48:10 | [train_policy] epoch #24 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:10 | [train_policy] epoch #24 | computing loss before
2021-06-04 13:48:10 | [train_policy] epoch #24 | computing gradient
2021-06-04 13:48:11 | [train_policy] epoch #24 | gradient computed
2021-06-04 13:48:11 | [train_policy] epoch #24 | computing descent direction
2021-06-04 13:48:11 | [train_policy] epoch #24 | descent direction computed
2021-06-04 13:48:11 | [train_policy] epoch #24 | backtrack iters: 1
2021-06-04 13:48:11 | [train_policy] epoch #24 | optimization finished
2021-06-04 13:48:11 | [train_policy] epoch #24 | Computing KL after
2021-06-04 13:48:11 | [train_policy] epoch #24 | Computing loss after
2021-06-04 13:48:11 | [train_policy] epoch #24 | Fitting baseline...
2021-06-04 13:48:11 | [train_policy] epoch #24 | Saving snapshot...
2021-06-04 13:48:11 | [train_policy] epoch #24 | Saved
2021-06-04 13:48:11 | [train_policy] epoch #24 | Time 22.85 s
2021-06-04 13:48:11 | [train_policy] epoch #24 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.288905
Evaluation/AverageDiscountedReturn         -64.6677
Evaluation/AverageReturn                   -64.6677
Evaluation/CompletionRate                    0
Evaluation/Iteration                        24
Evaluation/MaxReturn                       -39.4503
Evaluation/MinReturn                      -737.726
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        70.8538
Extras/EpisodeRewardMean                   -64.0908
LinearFeatureBaseline/ExplainedVariance     -1.78657
PolicyExecTime                               0.209747
ProcessExecTime                              0.0314326
TotalEnvSteps                            25300
policy/Entropy                               2.28613
policy/KL                                    0.00652985
policy/KLBefore                              0
policy/LossAfter                            -0.0271886
policy/LossBefore                           -1.27219e-08
policy/Perplexity                            9.83675
policy/dLoss                                 0.0271886
---------------------------------------  ---------------
2021-06-04 13:48:11 | [train_policy] epoch #25 | Obtaining samples for iteration 25...
2021-06-04 13:48:11 | [train_policy] epoch #25 | Logging diagnostics...
2021-06-04 13:48:11 | [train_policy] epoch #25 | Optimizing policy...
2021-06-04 13:48:11 | [train_policy] epoch #25 | Computing loss before
2021-06-04 13:48:11 | [train_policy] epoch #25 | Computing KL before
2021-06-04 13:48:11 | [train_policy] epoch #25 | Optimizing
2021-06-04 13:48:11 | [train_policy] epoch #25 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:11 | [train_policy] epoch #25 | computing loss before
2021-06-04 13:48:11 | [train_policy] epoch #25 | computing gradient
2021-06-04 13:48:11 | [train_policy] epoch #25 | gradient computed
2021-06-04 13:48:11 | [train_policy] epoch #25 | computing descent direction
2021-06-04 13:48:11 | [train_policy] epoch #25 | descent direction computed
2021-06-04 13:48:11 | [train_policy] epoch #25 | backtrack iters: 0
2021-06-04 13:48:11 | [train_policy] epoch #25 | optimization finished
2021-06-04 13:48:11 | [train_policy] epoch #25 | Computing KL after
2021-06-04 13:48:11 | [train_policy] epoch #25 | Computing loss after
2021-06-04 13:48:11 | [train_policy] epoch #25 | Fitting baseline...
2021-06-04 13:48:11 | [train_policy] epoch #25 | Saving snapshot...
2021-06-04 13:48:11 | [train_policy] epoch #25 | Saved
2021-06-04 13:48:11 | [train_policy] epoch #25 | Time 23.66 s
2021-06-04 13:48:11 | [train_policy] epoch #25 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.285026
Evaluation/AverageDiscountedReturn         -57.1635
Evaluation/AverageReturn                   -57.1635
Evaluation/CompletionRate                    0
Evaluation/Iteration                        25
Evaluation/MaxReturn                       -45.3084
Evaluation/MinReturn                       -71.5883
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.566
Extras/EpisodeRewardMean                   -56.843
LinearFeatureBaseline/ExplainedVariance      0.18263
PolicyExecTime                               0.235479
ProcessExecTime                              0.0311692
TotalEnvSteps                            26312
policy/Entropy                               2.26039
policy/KL                                    0.00971134
policy/KLBefore                              0
policy/LossAfter                            -0.0259345
policy/LossBefore                           -2.92133e-08
policy/Perplexity                            9.58685
policy/dLoss                                 0.0259345
---------------------------------------  ---------------
2021-06-04 13:48:11 | [train_policy] epoch #26 | Obtaining samples for iteration 26...
2021-06-04 13:48:12 | [train_policy] epoch #26 | Logging diagnostics...
2021-06-04 13:48:12 | [train_policy] epoch #26 | Optimizing policy...
2021-06-04 13:48:12 | [train_policy] epoch #26 | Computing loss before
2021-06-04 13:48:12 | [train_policy] epoch #26 | Computing KL before
2021-06-04 13:48:12 | [train_policy] epoch #26 | Optimizing
2021-06-04 13:48:12 | [train_policy] epoch #26 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:12 | [train_policy] epoch #26 | computing loss before
2021-06-04 13:48:12 | [train_policy] epoch #26 | computing gradient
2021-06-04 13:48:12 | [train_policy] epoch #26 | gradient computed
2021-06-04 13:48:12 | [train_policy] epoch #26 | computing descent direction
2021-06-04 13:48:12 | [train_policy] epoch #26 | descent direction computed
2021-06-04 13:48:12 | [train_policy] epoch #26 | backtrack iters: 1
2021-06-04 13:48:12 | [train_policy] epoch #26 | optimization finished
2021-06-04 13:48:12 | [train_policy] epoch #26 | Computing KL after
2021-06-04 13:48:12 | [train_policy] epoch #26 | Computing loss after
2021-06-04 13:48:12 | [train_policy] epoch #26 | Fitting baseline...
2021-06-04 13:48:12 | [train_policy] epoch #26 | Saving snapshot...
2021-06-04 13:48:12 | [train_policy] epoch #26 | Saved
2021-06-04 13:48:12 | [train_policy] epoch #26 | Time 24.47 s
2021-06-04 13:48:12 | [train_policy] epoch #26 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.284723
Evaluation/AverageDiscountedReturn         -56.9706
Evaluation/AverageReturn                   -56.9706
Evaluation/CompletionRate                    0
Evaluation/Iteration                        26
Evaluation/MaxReturn                       -44.9717
Evaluation/MinReturn                       -71.8147
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.54105
Extras/EpisodeRewardMean                   -57.0968
LinearFeatureBaseline/ExplainedVariance      0.970825
PolicyExecTime                               0.228313
ProcessExecTime                              0.031184
TotalEnvSteps                            27324
policy/Entropy                               2.26352
policy/KL                                    0.00646922
policy/KLBefore                              0
policy/LossAfter                            -0.0408292
policy/LossBefore                            1.64914e-08
policy/Perplexity                            9.61689
policy/dLoss                                 0.0408293
---------------------------------------  ---------------
2021-06-04 13:48:12 | [train_policy] epoch #27 | Obtaining samples for iteration 27...
2021-06-04 13:48:13 | [train_policy] epoch #27 | Logging diagnostics...
2021-06-04 13:48:13 | [train_policy] epoch #27 | Optimizing policy...
2021-06-04 13:48:13 | [train_policy] epoch #27 | Computing loss before
2021-06-04 13:48:13 | [train_policy] epoch #27 | Computing KL before
2021-06-04 13:48:13 | [train_policy] epoch #27 | Optimizing
2021-06-04 13:48:13 | [train_policy] epoch #27 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:13 | [train_policy] epoch #27 | computing loss before
2021-06-04 13:48:13 | [train_policy] epoch #27 | computing gradient
2021-06-04 13:48:13 | [train_policy] epoch #27 | gradient computed
2021-06-04 13:48:13 | [train_policy] epoch #27 | computing descent direction
2021-06-04 13:48:13 | [train_policy] epoch #27 | descent direction computed
2021-06-04 13:48:13 | [train_policy] epoch #27 | backtrack iters: 1
2021-06-04 13:48:13 | [train_policy] epoch #27 | optimization finished
2021-06-04 13:48:13 | [train_policy] epoch #27 | Computing KL after
2021-06-04 13:48:13 | [train_policy] epoch #27 | Computing loss after
2021-06-04 13:48:13 | [train_policy] epoch #27 | Fitting baseline...
2021-06-04 13:48:13 | [train_policy] epoch #27 | Saving snapshot...
2021-06-04 13:48:13 | [train_policy] epoch #27 | Saved
2021-06-04 13:48:13 | [train_policy] epoch #27 | Time 25.25 s
2021-06-04 13:48:13 | [train_policy] epoch #27 | EpochTime 0.76 s
---------------------------------------  --------------
EnvExecTime                                  0.287627
Evaluation/AverageDiscountedReturn         -58.5456
Evaluation/AverageReturn                   -58.5456
Evaluation/CompletionRate                    0
Evaluation/Iteration                        27
Evaluation/MaxReturn                       -42.8457
Evaluation/MinReturn                      -354.704
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        31.5914
Extras/EpisodeRewardMean                   -58.493
LinearFeatureBaseline/ExplainedVariance      0.582467
PolicyExecTime                               0.220905
ProcessExecTime                              0.0312963
TotalEnvSteps                            28336
policy/Entropy                               2.22289
policy/KL                                    0.00657903
policy/KLBefore                              0
policy/LossAfter                            -0.0313658
policy/LossBefore                            2.8271e-09
policy/Perplexity                            9.234
policy/dLoss                                 0.0313658
---------------------------------------  --------------
2021-06-04 13:48:13 | [train_policy] epoch #28 | Obtaining samples for iteration 28...
2021-06-04 13:48:14 | [train_policy] epoch #28 | Logging diagnostics...
2021-06-04 13:48:14 | [train_policy] epoch #28 | Optimizing policy...
2021-06-04 13:48:14 | [train_policy] epoch #28 | Computing loss before
2021-06-04 13:48:14 | [train_policy] epoch #28 | Computing KL before
2021-06-04 13:48:14 | [train_policy] epoch #28 | Optimizing
2021-06-04 13:48:14 | [train_policy] epoch #28 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:14 | [train_policy] epoch #28 | computing loss before
2021-06-04 13:48:14 | [train_policy] epoch #28 | computing gradient
2021-06-04 13:48:14 | [train_policy] epoch #28 | gradient computed
2021-06-04 13:48:14 | [train_policy] epoch #28 | computing descent direction
2021-06-04 13:48:14 | [train_policy] epoch #28 | descent direction computed
2021-06-04 13:48:14 | [train_policy] epoch #28 | backtrack iters: 3
2021-06-04 13:48:14 | [train_policy] epoch #28 | optimization finished
2021-06-04 13:48:14 | [train_policy] epoch #28 | Computing KL after
2021-06-04 13:48:14 | [train_policy] epoch #28 | Computing loss after
2021-06-04 13:48:14 | [train_policy] epoch #28 | Fitting baseline...
2021-06-04 13:48:14 | [train_policy] epoch #28 | Saving snapshot...
2021-06-04 13:48:14 | [train_policy] epoch #28 | Saved
2021-06-04 13:48:14 | [train_policy] epoch #28 | Time 26.06 s
2021-06-04 13:48:14 | [train_policy] epoch #28 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.283826
Evaluation/AverageDiscountedReturn         -78.6511
Evaluation/AverageReturn                   -78.6511
Evaluation/CompletionRate                    0
Evaluation/Iteration                        28
Evaluation/MaxReturn                       -41.8843
Evaluation/MinReturn                     -2051.89
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       207.306
Extras/EpisodeRewardMean                   -76.798
LinearFeatureBaseline/ExplainedVariance      0.00755768
PolicyExecTime                               0.231585
ProcessExecTime                              0.0311389
TotalEnvSteps                            29348
policy/Entropy                               2.20139
policy/KL                                    0.00516466
policy/KLBefore                              0
policy/LossAfter                            -0.0147895
policy/LossBefore                            3.76946e-09
policy/Perplexity                            9.03761
policy/dLoss                                 0.0147895
---------------------------------------  ---------------
2021-06-04 13:48:14 | [train_policy] epoch #29 | Obtaining samples for iteration 29...
2021-06-04 13:48:14 | [train_policy] epoch #29 | Logging diagnostics...
2021-06-04 13:48:14 | [train_policy] epoch #29 | Optimizing policy...
2021-06-04 13:48:14 | [train_policy] epoch #29 | Computing loss before
2021-06-04 13:48:14 | [train_policy] epoch #29 | Computing KL before
2021-06-04 13:48:14 | [train_policy] epoch #29 | Optimizing
2021-06-04 13:48:14 | [train_policy] epoch #29 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:14 | [train_policy] epoch #29 | computing loss before
2021-06-04 13:48:14 | [train_policy] epoch #29 | computing gradient
2021-06-04 13:48:14 | [train_policy] epoch #29 | gradient computed
2021-06-04 13:48:15 | [train_policy] epoch #29 | computing descent direction
2021-06-04 13:48:15 | [train_policy] epoch #29 | descent direction computed
2021-06-04 13:48:15 | [train_policy] epoch #29 | backtrack iters: 1
2021-06-04 13:48:15 | [train_policy] epoch #29 | optimization finished
2021-06-04 13:48:15 | [train_policy] epoch #29 | Computing KL after
2021-06-04 13:48:15 | [train_policy] epoch #29 | Computing loss after
2021-06-04 13:48:15 | [train_policy] epoch #29 | Fitting baseline...
2021-06-04 13:48:15 | [train_policy] epoch #29 | Saving snapshot...
2021-06-04 13:48:15 | [train_policy] epoch #29 | Saved
2021-06-04 13:48:15 | [train_policy] epoch #29 | Time 26.85 s
2021-06-04 13:48:15 | [train_policy] epoch #29 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286585
Evaluation/AverageDiscountedReturn         -57.7593
Evaluation/AverageReturn                   -57.7593
Evaluation/CompletionRate                    0
Evaluation/Iteration                        29
Evaluation/MaxReturn                       -45.0881
Evaluation/MinReturn                      -162.181
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        12.769
Extras/EpisodeRewardMean                   -58.853
LinearFeatureBaseline/ExplainedVariance     -3.20481
PolicyExecTime                               0.226063
ProcessExecTime                              0.031491
TotalEnvSteps                            30360
policy/Entropy                               2.19116
policy/KL                                    0.00653703
policy/KLBefore                              0
policy/LossAfter                            -0.0215886
policy/LossBefore                           -1.41355e-09
policy/Perplexity                            8.9456
policy/dLoss                                 0.0215886
---------------------------------------  ---------------
2021-06-04 13:48:15 | [train_policy] epoch #30 | Obtaining samples for iteration 30...
2021-06-04 13:48:15 | [train_policy] epoch #30 | Logging diagnostics...
2021-06-04 13:48:15 | [train_policy] epoch #30 | Optimizing policy...
2021-06-04 13:48:15 | [train_policy] epoch #30 | Computing loss before
2021-06-04 13:48:15 | [train_policy] epoch #30 | Computing KL before
2021-06-04 13:48:15 | [train_policy] epoch #30 | Optimizing
2021-06-04 13:48:15 | [train_policy] epoch #30 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:15 | [train_policy] epoch #30 | computing loss before
2021-06-04 13:48:15 | [train_policy] epoch #30 | computing gradient
2021-06-04 13:48:15 | [train_policy] epoch #30 | gradient computed
2021-06-04 13:48:15 | [train_policy] epoch #30 | computing descent direction
2021-06-04 13:48:15 | [train_policy] epoch #30 | descent direction computed
2021-06-04 13:48:15 | [train_policy] epoch #30 | backtrack iters: 1
2021-06-04 13:48:15 | [train_policy] epoch #30 | optimization finished
2021-06-04 13:48:15 | [train_policy] epoch #30 | Computing KL after
2021-06-04 13:48:15 | [train_policy] epoch #30 | Computing loss after
2021-06-04 13:48:15 | [train_policy] epoch #30 | Fitting baseline...
2021-06-04 13:48:15 | [train_policy] epoch #30 | Saving snapshot...
2021-06-04 13:48:15 | [train_policy] epoch #30 | Saved
2021-06-04 13:48:15 | [train_policy] epoch #30 | Time 27.65 s
2021-06-04 13:48:15 | [train_policy] epoch #30 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.286982
Evaluation/AverageDiscountedReturn         -55.7071
Evaluation/AverageReturn                   -55.7071
Evaluation/CompletionRate                    0
Evaluation/Iteration                        30
Evaluation/MaxReturn                       -38.6816
Evaluation/MinReturn                       -84.408
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.23965
Extras/EpisodeRewardMean                   -56.0373
LinearFeatureBaseline/ExplainedVariance      0.874826
PolicyExecTime                               0.232657
ProcessExecTime                              0.0312672
TotalEnvSteps                            31372
policy/Entropy                               2.17471
policy/KL                                    0.00657753
policy/KLBefore                              0
policy/LossAfter                            -0.0366919
policy/LossBefore                           -1.41355e-09
policy/Perplexity                            8.79963
policy/dLoss                                 0.0366919
---------------------------------------  ---------------
2021-06-04 13:48:15 | [train_policy] epoch #31 | Obtaining samples for iteration 31...
2021-06-04 13:48:16 | [train_policy] epoch #31 | Logging diagnostics...
2021-06-04 13:48:16 | [train_policy] epoch #31 | Optimizing policy...
2021-06-04 13:48:16 | [train_policy] epoch #31 | Computing loss before
2021-06-04 13:48:16 | [train_policy] epoch #31 | Computing KL before
2021-06-04 13:48:16 | [train_policy] epoch #31 | Optimizing
2021-06-04 13:48:16 | [train_policy] epoch #31 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:16 | [train_policy] epoch #31 | computing loss before
2021-06-04 13:48:16 | [train_policy] epoch #31 | computing gradient
2021-06-04 13:48:16 | [train_policy] epoch #31 | gradient computed
2021-06-04 13:48:16 | [train_policy] epoch #31 | computing descent direction
2021-06-04 13:48:16 | [train_policy] epoch #31 | descent direction computed
2021-06-04 13:48:16 | [train_policy] epoch #31 | backtrack iters: 0
2021-06-04 13:48:16 | [train_policy] epoch #31 | optimization finished
2021-06-04 13:48:16 | [train_policy] epoch #31 | Computing KL after
2021-06-04 13:48:16 | [train_policy] epoch #31 | Computing loss after
2021-06-04 13:48:16 | [train_policy] epoch #31 | Fitting baseline...
2021-06-04 13:48:16 | [train_policy] epoch #31 | Saving snapshot...
2021-06-04 13:48:16 | [train_policy] epoch #31 | Saved
2021-06-04 13:48:16 | [train_policy] epoch #31 | Time 28.43 s
2021-06-04 13:48:16 | [train_policy] epoch #31 | EpochTime 0.77 s
---------------------------------------  --------------
EnvExecTime                                  0.285791
Evaluation/AverageDiscountedReturn         -54.5575
Evaluation/AverageReturn                   -54.5575
Evaluation/CompletionRate                    0
Evaluation/Iteration                        31
Evaluation/MaxReturn                       -39.6922
Evaluation/MinReturn                       -71.4153
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.13817
Extras/EpisodeRewardMean                   -54.4558
LinearFeatureBaseline/ExplainedVariance      0.952196
PolicyExecTime                               0.228262
ProcessExecTime                              0.0314107
TotalEnvSteps                            32384
policy/Entropy                               2.16167
policy/KL                                    0.00978353
policy/KLBefore                              0
policy/LossAfter                            -0.0342906
policy/LossBefore                            1.5549e-08
policy/Perplexity                            8.68565
policy/dLoss                                 0.0342907
---------------------------------------  --------------
2021-06-04 13:48:16 | [train_policy] epoch #32 | Obtaining samples for iteration 32...
2021-06-04 13:48:17 | [train_policy] epoch #32 | Logging diagnostics...
2021-06-04 13:48:17 | [train_policy] epoch #32 | Optimizing policy...
2021-06-04 13:48:17 | [train_policy] epoch #32 | Computing loss before
2021-06-04 13:48:17 | [train_policy] epoch #32 | Computing KL before
2021-06-04 13:48:17 | [train_policy] epoch #32 | Optimizing
2021-06-04 13:48:17 | [train_policy] epoch #32 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:17 | [train_policy] epoch #32 | computing loss before
2021-06-04 13:48:17 | [train_policy] epoch #32 | computing gradient
2021-06-04 13:48:17 | [train_policy] epoch #32 | gradient computed
2021-06-04 13:48:17 | [train_policy] epoch #32 | computing descent direction
2021-06-04 13:48:17 | [train_policy] epoch #32 | descent direction computed
2021-06-04 13:48:17 | [train_policy] epoch #32 | backtrack iters: 1
2021-06-04 13:48:17 | [train_policy] epoch #32 | optimization finished
2021-06-04 13:48:17 | [train_policy] epoch #32 | Computing KL after
2021-06-04 13:48:17 | [train_policy] epoch #32 | Computing loss after
2021-06-04 13:48:17 | [train_policy] epoch #32 | Fitting baseline...
2021-06-04 13:48:17 | [train_policy] epoch #32 | Saving snapshot...
2021-06-04 13:48:17 | [train_policy] epoch #32 | Saved
2021-06-04 13:48:17 | [train_policy] epoch #32 | Time 29.23 s
2021-06-04 13:48:17 | [train_policy] epoch #32 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.284134
Evaluation/AverageDiscountedReturn         -63.2787
Evaluation/AverageReturn                   -63.2787
Evaluation/CompletionRate                    0
Evaluation/Iteration                        32
Evaluation/MaxReturn                       -38.8925
Evaluation/MinReturn                      -781.36
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        76.7263
Extras/EpisodeRewardMean                   -62.5008
LinearFeatureBaseline/ExplainedVariance      0.0617628
PolicyExecTime                               0.228248
ProcessExecTime                              0.0311682
TotalEnvSteps                            33396
policy/Entropy                               2.12613
policy/KL                                    0.00680928
policy/KLBefore                              0
policy/LossAfter                            -0.014721
policy/LossBefore                            1.5549e-08
policy/Perplexity                            8.38236
policy/dLoss                                 0.014721
---------------------------------------  --------------
2021-06-04 13:48:17 | [train_policy] epoch #33 | Obtaining samples for iteration 33...
2021-06-04 13:48:18 | [train_policy] epoch #33 | Logging diagnostics...
2021-06-04 13:48:18 | [train_policy] epoch #33 | Optimizing policy...
2021-06-04 13:48:18 | [train_policy] epoch #33 | Computing loss before
2021-06-04 13:48:18 | [train_policy] epoch #33 | Computing KL before
2021-06-04 13:48:18 | [train_policy] epoch #33 | Optimizing
2021-06-04 13:48:18 | [train_policy] epoch #33 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:18 | [train_policy] epoch #33 | computing loss before
2021-06-04 13:48:18 | [train_policy] epoch #33 | computing gradient
2021-06-04 13:48:18 | [train_policy] epoch #33 | gradient computed
2021-06-04 13:48:18 | [train_policy] epoch #33 | computing descent direction
2021-06-04 13:48:18 | [train_policy] epoch #33 | descent direction computed
2021-06-04 13:48:18 | [train_policy] epoch #33 | backtrack iters: 1
2021-06-04 13:48:18 | [train_policy] epoch #33 | optimization finished
2021-06-04 13:48:18 | [train_policy] epoch #33 | Computing KL after
2021-06-04 13:48:18 | [train_policy] epoch #33 | Computing loss after
2021-06-04 13:48:18 | [train_policy] epoch #33 | Fitting baseline...
2021-06-04 13:48:18 | [train_policy] epoch #33 | Saving snapshot...
2021-06-04 13:48:18 | [train_policy] epoch #33 | Saved
2021-06-04 13:48:18 | [train_policy] epoch #33 | Time 30.03 s
2021-06-04 13:48:18 | [train_policy] epoch #33 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.286277
Evaluation/AverageDiscountedReturn         -54.7629
Evaluation/AverageReturn                   -54.7629
Evaluation/CompletionRate                    0
Evaluation/Iteration                        33
Evaluation/MaxReturn                       -41.7541
Evaluation/MinReturn                       -67.6026
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.28883
Extras/EpisodeRewardMean                   -54.3307
LinearFeatureBaseline/ExplainedVariance      0.168956
PolicyExecTime                               0.228875
ProcessExecTime                              0.0314093
TotalEnvSteps                            34408
policy/Entropy                               2.117
policy/KL                                    0.0064306
policy/KLBefore                              0
policy/LossAfter                            -0.0157609
policy/LossBefore                           -9.18807e-09
policy/Perplexity                            8.30619
policy/dLoss                                 0.0157609
---------------------------------------  ---------------
2021-06-04 13:48:18 | [train_policy] epoch #34 | Obtaining samples for iteration 34...
2021-06-04 13:48:18 | [train_policy] epoch #34 | Logging diagnostics...
2021-06-04 13:48:18 | [train_policy] epoch #34 | Optimizing policy...
2021-06-04 13:48:18 | [train_policy] epoch #34 | Computing loss before
2021-06-04 13:48:18 | [train_policy] epoch #34 | Computing KL before
2021-06-04 13:48:18 | [train_policy] epoch #34 | Optimizing
2021-06-04 13:48:18 | [train_policy] epoch #34 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:18 | [train_policy] epoch #34 | computing loss before
2021-06-04 13:48:18 | [train_policy] epoch #34 | computing gradient
2021-06-04 13:48:18 | [train_policy] epoch #34 | gradient computed
2021-06-04 13:48:18 | [train_policy] epoch #34 | computing descent direction
2021-06-04 13:48:19 | [train_policy] epoch #34 | descent direction computed
2021-06-04 13:48:19 | [train_policy] epoch #34 | backtrack iters: 1
2021-06-04 13:48:19 | [train_policy] epoch #34 | optimization finished
2021-06-04 13:48:19 | [train_policy] epoch #34 | Computing KL after
2021-06-04 13:48:19 | [train_policy] epoch #34 | Computing loss after
2021-06-04 13:48:19 | [train_policy] epoch #34 | Fitting baseline...
2021-06-04 13:48:19 | [train_policy] epoch #34 | Saving snapshot...
2021-06-04 13:48:19 | [train_policy] epoch #34 | Saved
2021-06-04 13:48:19 | [train_policy] epoch #34 | Time 30.81 s
2021-06-04 13:48:19 | [train_policy] epoch #34 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286774
Evaluation/AverageDiscountedReturn         -54.469
Evaluation/AverageReturn                   -54.469
Evaluation/CompletionRate                    0
Evaluation/Iteration                        34
Evaluation/MaxReturn                       -37.9706
Evaluation/MinReturn                       -72.5525
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.65753
Extras/EpisodeRewardMean                   -54.5735
LinearFeatureBaseline/ExplainedVariance      0.960442
PolicyExecTime                               0.223137
ProcessExecTime                              0.0313909
TotalEnvSteps                            35420
policy/Entropy                               2.13787
policy/KL                                    0.0066692
policy/KLBefore                              0
policy/LossAfter                            -0.033877
policy/LossBefore                            3.29828e-09
policy/Perplexity                            8.48133
policy/dLoss                                 0.033877
---------------------------------------  ---------------
2021-06-04 13:48:19 | [train_policy] epoch #35 | Obtaining samples for iteration 35...
2021-06-04 13:48:19 | [train_policy] epoch #35 | Logging diagnostics...
2021-06-04 13:48:19 | [train_policy] epoch #35 | Optimizing policy...
2021-06-04 13:48:19 | [train_policy] epoch #35 | Computing loss before
2021-06-04 13:48:19 | [train_policy] epoch #35 | Computing KL before
2021-06-04 13:48:19 | [train_policy] epoch #35 | Optimizing
2021-06-04 13:48:19 | [train_policy] epoch #35 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:19 | [train_policy] epoch #35 | computing loss before
2021-06-04 13:48:19 | [train_policy] epoch #35 | computing gradient
2021-06-04 13:48:19 | [train_policy] epoch #35 | gradient computed
2021-06-04 13:48:19 | [train_policy] epoch #35 | computing descent direction
2021-06-04 13:48:19 | [train_policy] epoch #35 | descent direction computed
2021-06-04 13:48:19 | [train_policy] epoch #35 | backtrack iters: 1
2021-06-04 13:48:19 | [train_policy] epoch #35 | optimization finished
2021-06-04 13:48:19 | [train_policy] epoch #35 | Computing KL after
2021-06-04 13:48:19 | [train_policy] epoch #35 | Computing loss after
2021-06-04 13:48:19 | [train_policy] epoch #35 | Fitting baseline...
2021-06-04 13:48:19 | [train_policy] epoch #35 | Saving snapshot...
2021-06-04 13:48:19 | [train_policy] epoch #35 | Saved
2021-06-04 13:48:19 | [train_policy] epoch #35 | Time 31.61 s
2021-06-04 13:48:19 | [train_policy] epoch #35 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.288803
Evaluation/AverageDiscountedReturn         -53.1066
Evaluation/AverageReturn                   -53.1066
Evaluation/CompletionRate                    0
Evaluation/Iteration                        35
Evaluation/MaxReturn                       -45.3346
Evaluation/MinReturn                       -74.9752
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.71357
Extras/EpisodeRewardMean                   -53.3004
LinearFeatureBaseline/ExplainedVariance      0.955272
PolicyExecTime                               0.231765
ProcessExecTime                              0.0315938
TotalEnvSteps                            36432
policy/Entropy                               2.13948
policy/KL                                    0.00646827
policy/KLBefore                              0
policy/LossAfter                            -0.034601
policy/LossBefore                           -1.36643e-08
policy/Perplexity                            8.49504
policy/dLoss                                 0.034601
---------------------------------------  ---------------
2021-06-04 13:48:19 | [train_policy] epoch #36 | Obtaining samples for iteration 36...
2021-06-04 13:48:20 | [train_policy] epoch #36 | Logging diagnostics...
2021-06-04 13:48:20 | [train_policy] epoch #36 | Optimizing policy...
2021-06-04 13:48:20 | [train_policy] epoch #36 | Computing loss before
2021-06-04 13:48:20 | [train_policy] epoch #36 | Computing KL before
2021-06-04 13:48:20 | [train_policy] epoch #36 | Optimizing
2021-06-04 13:48:20 | [train_policy] epoch #36 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:20 | [train_policy] epoch #36 | computing loss before
2021-06-04 13:48:20 | [train_policy] epoch #36 | computing gradient
2021-06-04 13:48:20 | [train_policy] epoch #36 | gradient computed
2021-06-04 13:48:20 | [train_policy] epoch #36 | computing descent direction
2021-06-04 13:48:20 | [train_policy] epoch #36 | descent direction computed
2021-06-04 13:48:20 | [train_policy] epoch #36 | backtrack iters: 1
2021-06-04 13:48:20 | [train_policy] epoch #36 | optimization finished
2021-06-04 13:48:20 | [train_policy] epoch #36 | Computing KL after
2021-06-04 13:48:20 | [train_policy] epoch #36 | Computing loss after
2021-06-04 13:48:20 | [train_policy] epoch #36 | Fitting baseline...
2021-06-04 13:48:20 | [train_policy] epoch #36 | Saving snapshot...
2021-06-04 13:48:20 | [train_policy] epoch #36 | Saved
2021-06-04 13:48:20 | [train_policy] epoch #36 | Time 32.40 s
2021-06-04 13:48:20 | [train_policy] epoch #36 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.28539
Evaluation/AverageDiscountedReturn         -52.9547
Evaluation/AverageReturn                   -52.9547
Evaluation/CompletionRate                    0
Evaluation/Iteration                        36
Evaluation/MaxReturn                       -37.2652
Evaluation/MinReturn                       -73.1587
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.8029
Extras/EpisodeRewardMean                   -52.7592
LinearFeatureBaseline/ExplainedVariance      0.961864
PolicyExecTime                               0.226049
ProcessExecTime                              0.0312116
TotalEnvSteps                            37444
policy/Entropy                               2.13207
policy/KL                                    0.00666044
policy/KLBefore                              0
policy/LossAfter                            -0.0280581
policy/LossBefore                           -1.41355e-08
policy/Perplexity                            8.43232
policy/dLoss                                 0.0280581
---------------------------------------  ---------------
2021-06-04 13:48:20 | [train_policy] epoch #37 | Obtaining samples for iteration 37...
2021-06-04 13:48:21 | [train_policy] epoch #37 | Logging diagnostics...
2021-06-04 13:48:21 | [train_policy] epoch #37 | Optimizing policy...
2021-06-04 13:48:21 | [train_policy] epoch #37 | Computing loss before
2021-06-04 13:48:21 | [train_policy] epoch #37 | Computing KL before
2021-06-04 13:48:21 | [train_policy] epoch #37 | Optimizing
2021-06-04 13:48:21 | [train_policy] epoch #37 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:21 | [train_policy] epoch #37 | computing loss before
2021-06-04 13:48:21 | [train_policy] epoch #37 | computing gradient
2021-06-04 13:48:21 | [train_policy] epoch #37 | gradient computed
2021-06-04 13:48:21 | [train_policy] epoch #37 | computing descent direction
2021-06-04 13:48:21 | [train_policy] epoch #37 | descent direction computed
2021-06-04 13:48:21 | [train_policy] epoch #37 | backtrack iters: 1
2021-06-04 13:48:21 | [train_policy] epoch #37 | optimization finished
2021-06-04 13:48:21 | [train_policy] epoch #37 | Computing KL after
2021-06-04 13:48:21 | [train_policy] epoch #37 | Computing loss after
2021-06-04 13:48:21 | [train_policy] epoch #37 | Fitting baseline...
2021-06-04 13:48:21 | [train_policy] epoch #37 | Saving snapshot...
2021-06-04 13:48:21 | [train_policy] epoch #37 | Saved
2021-06-04 13:48:21 | [train_policy] epoch #37 | Time 33.20 s
2021-06-04 13:48:21 | [train_policy] epoch #37 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.286402
Evaluation/AverageDiscountedReturn         -52.8721
Evaluation/AverageReturn                   -52.8721
Evaluation/CompletionRate                    0
Evaluation/Iteration                        37
Evaluation/MaxReturn                       -39.8667
Evaluation/MinReturn                       -97.0365
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.63511
Extras/EpisodeRewardMean                   -52.8055
LinearFeatureBaseline/ExplainedVariance      0.889803
PolicyExecTime                               0.234201
ProcessExecTime                              0.0313559
TotalEnvSteps                            38456
policy/Entropy                               2.10399
policy/KL                                    0.00723354
policy/KLBefore                              0
policy/LossAfter                            -0.0252242
policy/LossBefore                           -6.59656e-09
policy/Perplexity                            8.19886
policy/dLoss                                 0.0252242
---------------------------------------  ---------------
2021-06-04 13:48:21 | [train_policy] epoch #38 | Obtaining samples for iteration 38...
2021-06-04 13:48:22 | [train_policy] epoch #38 | Logging diagnostics...
2021-06-04 13:48:22 | [train_policy] epoch #38 | Optimizing policy...
2021-06-04 13:48:22 | [train_policy] epoch #38 | Computing loss before
2021-06-04 13:48:22 | [train_policy] epoch #38 | Computing KL before
2021-06-04 13:48:22 | [train_policy] epoch #38 | Optimizing
2021-06-04 13:48:22 | [train_policy] epoch #38 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:22 | [train_policy] epoch #38 | computing loss before
2021-06-04 13:48:22 | [train_policy] epoch #38 | computing gradient
2021-06-04 13:48:22 | [train_policy] epoch #38 | gradient computed
2021-06-04 13:48:22 | [train_policy] epoch #38 | computing descent direction
2021-06-04 13:48:22 | [train_policy] epoch #38 | descent direction computed
2021-06-04 13:48:22 | [train_policy] epoch #38 | backtrack iters: 1
2021-06-04 13:48:22 | [train_policy] epoch #38 | optimization finished
2021-06-04 13:48:22 | [train_policy] epoch #38 | Computing KL after
2021-06-04 13:48:22 | [train_policy] epoch #38 | Computing loss after
2021-06-04 13:48:22 | [train_policy] epoch #38 | Fitting baseline...
2021-06-04 13:48:22 | [train_policy] epoch #38 | Saving snapshot...
2021-06-04 13:48:22 | [train_policy] epoch #38 | Saved
2021-06-04 13:48:22 | [train_policy] epoch #38 | Time 33.99 s
2021-06-04 13:48:22 | [train_policy] epoch #38 | EpochTime 0.76 s
---------------------------------------  --------------
EnvExecTime                                  0.287028
Evaluation/AverageDiscountedReturn         -94.8927
Evaluation/AverageReturn                   -94.8927
Evaluation/CompletionRate                    0
Evaluation/Iteration                        38
Evaluation/MaxReturn                       -39.3914
Evaluation/MinReturn                     -4058.57
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       415.537
Extras/EpisodeRewardMean                   -91.1222
LinearFeatureBaseline/ExplainedVariance      0.00521326
PolicyExecTime                               0.212926
ProcessExecTime                              0.0313137
TotalEnvSteps                            39468
policy/Entropy                               2.12027
policy/KL                                    0.00689534
policy/KLBefore                              0
policy/LossAfter                            -0.0262496
policy/LossBefore                           -1.0366e-08
policy/Perplexity                            8.33338
policy/dLoss                                 0.0262496
---------------------------------------  --------------
2021-06-04 13:48:22 | [train_policy] epoch #39 | Obtaining samples for iteration 39...
2021-06-04 13:48:22 | [train_policy] epoch #39 | Logging diagnostics...
2021-06-04 13:48:22 | [train_policy] epoch #39 | Optimizing policy...
2021-06-04 13:48:22 | [train_policy] epoch #39 | Computing loss before
2021-06-04 13:48:22 | [train_policy] epoch #39 | Computing KL before
2021-06-04 13:48:22 | [train_policy] epoch #39 | Optimizing
2021-06-04 13:48:22 | [train_policy] epoch #39 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:22 | [train_policy] epoch #39 | computing loss before
2021-06-04 13:48:22 | [train_policy] epoch #39 | computing gradient
2021-06-04 13:48:22 | [train_policy] epoch #39 | gradient computed
2021-06-04 13:48:22 | [train_policy] epoch #39 | computing descent direction
2021-06-04 13:48:22 | [train_policy] epoch #39 | descent direction computed
2021-06-04 13:48:23 | [train_policy] epoch #39 | backtrack iters: 0
2021-06-04 13:48:23 | [train_policy] epoch #39 | optimization finished
2021-06-04 13:48:23 | [train_policy] epoch #39 | Computing KL after
2021-06-04 13:48:23 | [train_policy] epoch #39 | Computing loss after
2021-06-04 13:48:23 | [train_policy] epoch #39 | Fitting baseline...
2021-06-04 13:48:23 | [train_policy] epoch #39 | Saving snapshot...
2021-06-04 13:48:23 | [train_policy] epoch #39 | Saved
2021-06-04 13:48:23 | [train_policy] epoch #39 | Time 34.77 s
2021-06-04 13:48:23 | [train_policy] epoch #39 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286495
Evaluation/AverageDiscountedReturn         -70.0389
Evaluation/AverageReturn                   -70.0389
Evaluation/CompletionRate                    0
Evaluation/Iteration                        39
Evaluation/MaxReturn                       -39.5686
Evaluation/MinReturn                     -1478.97
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       148.425
Extras/EpisodeRewardMean                  -108.502
LinearFeatureBaseline/ExplainedVariance     -2.38259
PolicyExecTime                               0.228092
ProcessExecTime                              0.0314333
TotalEnvSteps                            40480
policy/Entropy                               2.10124
policy/KL                                    0.00985164
policy/KLBefore                              0
policy/LossAfter                            -0.0348701
policy/LossBefore                            2.35591e-10
policy/Perplexity                            8.17627
policy/dLoss                                 0.0348701
---------------------------------------  ---------------
2021-06-04 13:48:23 | [train_policy] epoch #40 | Obtaining samples for iteration 40...
2021-06-04 13:48:23 | [train_policy] epoch #40 | Logging diagnostics...
2021-06-04 13:48:23 | [train_policy] epoch #40 | Optimizing policy...
2021-06-04 13:48:23 | [train_policy] epoch #40 | Computing loss before
2021-06-04 13:48:23 | [train_policy] epoch #40 | Computing KL before
2021-06-04 13:48:23 | [train_policy] epoch #40 | Optimizing
2021-06-04 13:48:23 | [train_policy] epoch #40 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:23 | [train_policy] epoch #40 | computing loss before
2021-06-04 13:48:23 | [train_policy] epoch #40 | computing gradient
2021-06-04 13:48:23 | [train_policy] epoch #40 | gradient computed
2021-06-04 13:48:23 | [train_policy] epoch #40 | computing descent direction
2021-06-04 13:48:23 | [train_policy] epoch #40 | descent direction computed
2021-06-04 13:48:23 | [train_policy] epoch #40 | backtrack iters: 1
2021-06-04 13:48:23 | [train_policy] epoch #40 | optimization finished
2021-06-04 13:48:23 | [train_policy] epoch #40 | Computing KL after
2021-06-04 13:48:23 | [train_policy] epoch #40 | Computing loss after
2021-06-04 13:48:23 | [train_policy] epoch #40 | Fitting baseline...
2021-06-04 13:48:23 | [train_policy] epoch #40 | Saving snapshot...
2021-06-04 13:48:23 | [train_policy] epoch #40 | Saved
2021-06-04 13:48:23 | [train_policy] epoch #40 | Time 35.56 s
2021-06-04 13:48:23 | [train_policy] epoch #40 | EpochTime 0.77 s
---------------------------------------  --------------
EnvExecTime                                  0.285004
Evaluation/AverageDiscountedReturn        -101.002
Evaluation/AverageReturn                  -101.002
Evaluation/CompletionRate                    0
Evaluation/Iteration                        40
Evaluation/MaxReturn                       -39.4545
Evaluation/MinReturn                     -2066.59
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       296.432
Extras/EpisodeRewardMean                   -97.0306
LinearFeatureBaseline/ExplainedVariance     -0.154581
PolicyExecTime                               0.226757
ProcessExecTime                              0.0312557
TotalEnvSteps                            41492
policy/Entropy                               2.09268
policy/KL                                    0.00643462
policy/KLBefore                              0
policy/LossAfter                            -0.0352083
policy/LossBefore                           -5.4186e-09
policy/Perplexity                            8.10659
policy/dLoss                                 0.0352083
---------------------------------------  --------------
2021-06-04 13:48:23 | [train_policy] epoch #41 | Obtaining samples for iteration 41...
2021-06-04 13:48:24 | [train_policy] epoch #41 | Logging diagnostics...
2021-06-04 13:48:24 | [train_policy] epoch #41 | Optimizing policy...
2021-06-04 13:48:24 | [train_policy] epoch #41 | Computing loss before
2021-06-04 13:48:24 | [train_policy] epoch #41 | Computing KL before
2021-06-04 13:48:24 | [train_policy] epoch #41 | Optimizing
2021-06-04 13:48:24 | [train_policy] epoch #41 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:24 | [train_policy] epoch #41 | computing loss before
2021-06-04 13:48:24 | [train_policy] epoch #41 | computing gradient
2021-06-04 13:48:24 | [train_policy] epoch #41 | gradient computed
2021-06-04 13:48:24 | [train_policy] epoch #41 | computing descent direction
2021-06-04 13:48:24 | [train_policy] epoch #41 | descent direction computed
2021-06-04 13:48:24 | [train_policy] epoch #41 | backtrack iters: 0
2021-06-04 13:48:24 | [train_policy] epoch #41 | optimization finished
2021-06-04 13:48:24 | [train_policy] epoch #41 | Computing KL after
2021-06-04 13:48:24 | [train_policy] epoch #41 | Computing loss after
2021-06-04 13:48:24 | [train_policy] epoch #41 | Fitting baseline...
2021-06-04 13:48:24 | [train_policy] epoch #41 | Saving snapshot...
2021-06-04 13:48:24 | [train_policy] epoch #41 | Saved
2021-06-04 13:48:24 | [train_policy] epoch #41 | Time 36.35 s
2021-06-04 13:48:24 | [train_policy] epoch #41 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.284135
Evaluation/AverageDiscountedReturn         -53.5639
Evaluation/AverageReturn                   -53.5639
Evaluation/CompletionRate                    0
Evaluation/Iteration                        41
Evaluation/MaxReturn                       -41.673
Evaluation/MinReturn                      -130.141
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        10.0365
Extras/EpisodeRewardMean                   -53.2404
LinearFeatureBaseline/ExplainedVariance    -42.8555
PolicyExecTime                               0.229597
ProcessExecTime                              0.0310693
TotalEnvSteps                            42504
policy/Entropy                               2.11726
policy/KL                                    0.00969652
policy/KLBefore                              0
policy/LossAfter                            -0.024753
policy/LossBefore                            3.06269e-08
policy/Perplexity                            8.30831
policy/dLoss                                 0.024753
---------------------------------------  ---------------
2021-06-04 13:48:24 | [train_policy] epoch #42 | Obtaining samples for iteration 42...
2021-06-04 13:48:25 | [train_policy] epoch #42 | Logging diagnostics...
2021-06-04 13:48:25 | [train_policy] epoch #42 | Optimizing policy...
2021-06-04 13:48:25 | [train_policy] epoch #42 | Computing loss before
2021-06-04 13:48:25 | [train_policy] epoch #42 | Computing KL before
2021-06-04 13:48:25 | [train_policy] epoch #42 | Optimizing
2021-06-04 13:48:25 | [train_policy] epoch #42 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:25 | [train_policy] epoch #42 | computing loss before
2021-06-04 13:48:25 | [train_policy] epoch #42 | computing gradient
2021-06-04 13:48:25 | [train_policy] epoch #42 | gradient computed
2021-06-04 13:48:25 | [train_policy] epoch #42 | computing descent direction
2021-06-04 13:48:25 | [train_policy] epoch #42 | descent direction computed
2021-06-04 13:48:25 | [train_policy] epoch #42 | backtrack iters: 1
2021-06-04 13:48:25 | [train_policy] epoch #42 | optimization finished
2021-06-04 13:48:25 | [train_policy] epoch #42 | Computing KL after
2021-06-04 13:48:25 | [train_policy] epoch #42 | Computing loss after
2021-06-04 13:48:25 | [train_policy] epoch #42 | Fitting baseline...
2021-06-04 13:48:25 | [train_policy] epoch #42 | Saving snapshot...
2021-06-04 13:48:25 | [train_policy] epoch #42 | Saved
2021-06-04 13:48:25 | [train_policy] epoch #42 | Time 37.15 s
2021-06-04 13:48:25 | [train_policy] epoch #42 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285217
Evaluation/AverageDiscountedReturn         -52.7179
Evaluation/AverageReturn                   -52.7179
Evaluation/CompletionRate                    0
Evaluation/Iteration                        42
Evaluation/MaxReturn                       -37.5331
Evaluation/MinReturn                       -80.5897
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.50429
Extras/EpisodeRewardMean                   -53.4802
LinearFeatureBaseline/ExplainedVariance      0.937468
PolicyExecTime                               0.226714
ProcessExecTime                              0.0312452
TotalEnvSteps                            43516
policy/Entropy                               2.064
policy/KL                                    0.00710048
policy/KLBefore                              0
policy/LossAfter                            -0.0219265
policy/LossBefore                           -7.30334e-09
policy/Perplexity                            7.87741
policy/dLoss                                 0.0219265
---------------------------------------  ---------------
2021-06-04 13:48:25 | [train_policy] epoch #43 | Obtaining samples for iteration 43...
2021-06-04 13:48:26 | [train_policy] epoch #43 | Logging diagnostics...
2021-06-04 13:48:26 | [train_policy] epoch #43 | Optimizing policy...
2021-06-04 13:48:26 | [train_policy] epoch #43 | Computing loss before
2021-06-04 13:48:26 | [train_policy] epoch #43 | Computing KL before
2021-06-04 13:48:26 | [train_policy] epoch #43 | Optimizing
2021-06-04 13:48:26 | [train_policy] epoch #43 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:26 | [train_policy] epoch #43 | computing loss before
2021-06-04 13:48:26 | [train_policy] epoch #43 | computing gradient
2021-06-04 13:48:26 | [train_policy] epoch #43 | gradient computed
2021-06-04 13:48:26 | [train_policy] epoch #43 | computing descent direction
2021-06-04 13:48:26 | [train_policy] epoch #43 | descent direction computed
2021-06-04 13:48:26 | [train_policy] epoch #43 | backtrack iters: 1
2021-06-04 13:48:26 | [train_policy] epoch #43 | optimization finished
2021-06-04 13:48:26 | [train_policy] epoch #43 | Computing KL after
2021-06-04 13:48:26 | [train_policy] epoch #43 | Computing loss after
2021-06-04 13:48:26 | [train_policy] epoch #43 | Fitting baseline...
2021-06-04 13:48:26 | [train_policy] epoch #43 | Saving snapshot...
2021-06-04 13:48:26 | [train_policy] epoch #43 | Saved
2021-06-04 13:48:26 | [train_policy] epoch #43 | Time 37.92 s
2021-06-04 13:48:26 | [train_policy] epoch #43 | EpochTime 0.75 s
---------------------------------------  --------------
EnvExecTime                                  0.286872
Evaluation/AverageDiscountedReturn         -52.3317
Evaluation/AverageReturn                   -52.3317
Evaluation/CompletionRate                    0
Evaluation/Iteration                        43
Evaluation/MaxReturn                       -38.6294
Evaluation/MinReturn                      -113.864
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        10.3463
Extras/EpisodeRewardMean                   -52.301
LinearFeatureBaseline/ExplainedVariance      0.735105
PolicyExecTime                               0.208704
ProcessExecTime                              0.0314109
TotalEnvSteps                            44528
policy/Entropy                               1.99854
policy/KL                                    0.00685616
policy/KLBefore                              0
policy/LossAfter                            -0.0226473
policy/LossBefore                           -1.0366e-08
policy/Perplexity                            7.37827
policy/dLoss                                 0.0226473
---------------------------------------  --------------
2021-06-04 13:48:26 | [train_policy] epoch #44 | Obtaining samples for iteration 44...
2021-06-04 13:48:26 | [train_policy] epoch #44 | Logging diagnostics...
2021-06-04 13:48:26 | [train_policy] epoch #44 | Optimizing policy...
2021-06-04 13:48:26 | [train_policy] epoch #44 | Computing loss before
2021-06-04 13:48:26 | [train_policy] epoch #44 | Computing KL before
2021-06-04 13:48:26 | [train_policy] epoch #44 | Optimizing
2021-06-04 13:48:26 | [train_policy] epoch #44 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:26 | [train_policy] epoch #44 | computing loss before
2021-06-04 13:48:26 | [train_policy] epoch #44 | computing gradient
2021-06-04 13:48:26 | [train_policy] epoch #44 | gradient computed
2021-06-04 13:48:26 | [train_policy] epoch #44 | computing descent direction
2021-06-04 13:48:26 | [train_policy] epoch #44 | descent direction computed
2021-06-04 13:48:26 | [train_policy] epoch #44 | backtrack iters: 0
2021-06-04 13:48:26 | [train_policy] epoch #44 | optimization finished
2021-06-04 13:48:26 | [train_policy] epoch #44 | Computing KL after
2021-06-04 13:48:26 | [train_policy] epoch #44 | Computing loss after
2021-06-04 13:48:26 | [train_policy] epoch #44 | Fitting baseline...
2021-06-04 13:48:26 | [train_policy] epoch #44 | Saving snapshot...
2021-06-04 13:48:26 | [train_policy] epoch #44 | Saved
2021-06-04 13:48:26 | [train_policy] epoch #44 | Time 38.70 s
2021-06-04 13:48:26 | [train_policy] epoch #44 | EpochTime 0.77 s
---------------------------------------  --------------
EnvExecTime                                  0.28894
Evaluation/AverageDiscountedReturn         -52.4745
Evaluation/AverageReturn                   -52.4745
Evaluation/CompletionRate                    0
Evaluation/Iteration                        44
Evaluation/MaxReturn                       -36.411
Evaluation/MinReturn                       -88.9906
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.74252
Extras/EpisodeRewardMean                   -52.4573
LinearFeatureBaseline/ExplainedVariance      0.873182
PolicyExecTime                               0.223869
ProcessExecTime                              0.0318794
TotalEnvSteps                            45540
policy/Entropy                               1.96288
policy/KL                                    0.00891012
policy/KLBefore                              0
policy/LossAfter                            -0.0198739
policy/LossBefore                           -1.5549e-08
policy/Perplexity                            7.11979
policy/dLoss                                 0.0198738
---------------------------------------  --------------
2021-06-04 13:48:27 | [train_policy] epoch #45 | Obtaining samples for iteration 45...
2021-06-04 13:48:27 | [train_policy] epoch #45 | Logging diagnostics...
2021-06-04 13:48:27 | [train_policy] epoch #45 | Optimizing policy...
2021-06-04 13:48:27 | [train_policy] epoch #45 | Computing loss before
2021-06-04 13:48:27 | [train_policy] epoch #45 | Computing KL before
2021-06-04 13:48:27 | [train_policy] epoch #45 | Optimizing
2021-06-04 13:48:27 | [train_policy] epoch #45 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:27 | [train_policy] epoch #45 | computing loss before
2021-06-04 13:48:27 | [train_policy] epoch #45 | computing gradient
2021-06-04 13:48:27 | [train_policy] epoch #45 | gradient computed
2021-06-04 13:48:27 | [train_policy] epoch #45 | computing descent direction
2021-06-04 13:48:27 | [train_policy] epoch #45 | descent direction computed
2021-06-04 13:48:27 | [train_policy] epoch #45 | backtrack iters: 0
2021-06-04 13:48:27 | [train_policy] epoch #45 | optimization finished
2021-06-04 13:48:27 | [train_policy] epoch #45 | Computing KL after
2021-06-04 13:48:27 | [train_policy] epoch #45 | Computing loss after
2021-06-04 13:48:27 | [train_policy] epoch #45 | Fitting baseline...
2021-06-04 13:48:27 | [train_policy] epoch #45 | Saving snapshot...
2021-06-04 13:48:27 | [train_policy] epoch #45 | Saved
2021-06-04 13:48:27 | [train_policy] epoch #45 | Time 39.49 s
2021-06-04 13:48:27 | [train_policy] epoch #45 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285076
Evaluation/AverageDiscountedReturn         -51.4886
Evaluation/AverageReturn                   -51.4886
Evaluation/CompletionRate                    0
Evaluation/Iteration                        45
Evaluation/MaxReturn                       -39.0268
Evaluation/MinReturn                       -70.0793
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.02459
Extras/EpisodeRewardMean                   -51.7147
LinearFeatureBaseline/ExplainedVariance      0.951928
PolicyExecTime                               0.234377
ProcessExecTime                              0.0311844
TotalEnvSteps                            46552
policy/Entropy                               1.9487
policy/KL                                    0.00861981
policy/KLBefore                              0
policy/LossAfter                            -0.0270694
policy/LossBefore                            7.53893e-09
policy/Perplexity                            7.01954
policy/dLoss                                 0.0270694
---------------------------------------  ---------------
2021-06-04 13:48:27 | [train_policy] epoch #46 | Obtaining samples for iteration 46...
2021-06-04 13:48:28 | [train_policy] epoch #46 | Logging diagnostics...
2021-06-04 13:48:28 | [train_policy] epoch #46 | Optimizing policy...
2021-06-04 13:48:28 | [train_policy] epoch #46 | Computing loss before
2021-06-04 13:48:28 | [train_policy] epoch #46 | Computing KL before
2021-06-04 13:48:28 | [train_policy] epoch #46 | Optimizing
2021-06-04 13:48:28 | [train_policy] epoch #46 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:28 | [train_policy] epoch #46 | computing loss before
2021-06-04 13:48:28 | [train_policy] epoch #46 | computing gradient
2021-06-04 13:48:28 | [train_policy] epoch #46 | gradient computed
2021-06-04 13:48:28 | [train_policy] epoch #46 | computing descent direction
2021-06-04 13:48:28 | [train_policy] epoch #46 | descent direction computed
2021-06-04 13:48:28 | [train_policy] epoch #46 | backtrack iters: 1
2021-06-04 13:48:28 | [train_policy] epoch #46 | optimization finished
2021-06-04 13:48:28 | [train_policy] epoch #46 | Computing KL after
2021-06-04 13:48:28 | [train_policy] epoch #46 | Computing loss after
2021-06-04 13:48:28 | [train_policy] epoch #46 | Fitting baseline...
2021-06-04 13:48:28 | [train_policy] epoch #46 | Saving snapshot...
2021-06-04 13:48:28 | [train_policy] epoch #46 | Saved
2021-06-04 13:48:28 | [train_policy] epoch #46 | Time 40.28 s
2021-06-04 13:48:28 | [train_policy] epoch #46 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.287104
Evaluation/AverageDiscountedReturn         -82.3815
Evaluation/AverageReturn                   -82.3815
Evaluation/CompletionRate                    0
Evaluation/Iteration                        46
Evaluation/MaxReturn                       -41.4237
Evaluation/MinReturn                     -2060.29
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       217.015
Extras/EpisodeRewardMean                   -80.1439
LinearFeatureBaseline/ExplainedVariance      0.0102246
PolicyExecTime                               0.220233
ProcessExecTime                              0.0312974
TotalEnvSteps                            47564
policy/Entropy                               1.91133
policy/KL                                    0.0064497
policy/KLBefore                              0
policy/LossAfter                            -0.0119162
policy/LossBefore                            1.31931e-08
policy/Perplexity                            6.76206
policy/dLoss                                 0.0119162
---------------------------------------  ---------------
2021-06-04 13:48:28 | [train_policy] epoch #47 | Obtaining samples for iteration 47...
2021-06-04 13:48:29 | [train_policy] epoch #47 | Logging diagnostics...
2021-06-04 13:48:29 | [train_policy] epoch #47 | Optimizing policy...
2021-06-04 13:48:29 | [train_policy] epoch #47 | Computing loss before
2021-06-04 13:48:29 | [train_policy] epoch #47 | Computing KL before
2021-06-04 13:48:29 | [train_policy] epoch #47 | Optimizing
2021-06-04 13:48:29 | [train_policy] epoch #47 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:29 | [train_policy] epoch #47 | computing loss before
2021-06-04 13:48:29 | [train_policy] epoch #47 | computing gradient
2021-06-04 13:48:29 | [train_policy] epoch #47 | gradient computed
2021-06-04 13:48:29 | [train_policy] epoch #47 | computing descent direction
2021-06-04 13:48:29 | [train_policy] epoch #47 | descent direction computed
2021-06-04 13:48:29 | [train_policy] epoch #47 | backtrack iters: 1
2021-06-04 13:48:29 | [train_policy] epoch #47 | optimization finished
2021-06-04 13:48:29 | [train_policy] epoch #47 | Computing KL after
2021-06-04 13:48:29 | [train_policy] epoch #47 | Computing loss after
2021-06-04 13:48:29 | [train_policy] epoch #47 | Fitting baseline...
2021-06-04 13:48:29 | [train_policy] epoch #47 | Saving snapshot...
2021-06-04 13:48:29 | [train_policy] epoch #47 | Saved
2021-06-04 13:48:29 | [train_policy] epoch #47 | Time 41.07 s
2021-06-04 13:48:29 | [train_policy] epoch #47 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.287886
Evaluation/AverageDiscountedReturn         -51.9274
Evaluation/AverageReturn                   -51.9274
Evaluation/CompletionRate                    0
Evaluation/Iteration                        47
Evaluation/MaxReturn                       -40.9871
Evaluation/MinReturn                       -70.1573
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.1439
Extras/EpisodeRewardMean                   -51.7983
LinearFeatureBaseline/ExplainedVariance    -20.5446
PolicyExecTime                               0.218885
ProcessExecTime                              0.0315447
TotalEnvSteps                            48576
policy/Entropy                               1.90882
policy/KL                                    0.00662061
policy/KLBefore                              0
policy/LossAfter                            -0.0201158
policy/LossBefore                           -5.88979e-09
policy/Perplexity                            6.74511
policy/dLoss                                 0.0201158
---------------------------------------  ---------------
2021-06-04 13:48:29 | [train_policy] epoch #48 | Obtaining samples for iteration 48...
2021-06-04 13:48:29 | [train_policy] epoch #48 | Logging diagnostics...
2021-06-04 13:48:29 | [train_policy] epoch #48 | Optimizing policy...
2021-06-04 13:48:29 | [train_policy] epoch #48 | Computing loss before
2021-06-04 13:48:29 | [train_policy] epoch #48 | Computing KL before
2021-06-04 13:48:29 | [train_policy] epoch #48 | Optimizing
2021-06-04 13:48:29 | [train_policy] epoch #48 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:29 | [train_policy] epoch #48 | computing loss before
2021-06-04 13:48:30 | [train_policy] epoch #48 | computing gradient
2021-06-04 13:48:30 | [train_policy] epoch #48 | gradient computed
2021-06-04 13:48:30 | [train_policy] epoch #48 | computing descent direction
2021-06-04 13:48:30 | [train_policy] epoch #48 | descent direction computed
2021-06-04 13:48:30 | [train_policy] epoch #48 | backtrack iters: 1
2021-06-04 13:48:30 | [train_policy] epoch #48 | optimization finished
2021-06-04 13:48:30 | [train_policy] epoch #48 | Computing KL after
2021-06-04 13:48:30 | [train_policy] epoch #48 | Computing loss after
2021-06-04 13:48:30 | [train_policy] epoch #48 | Fitting baseline...
2021-06-04 13:48:30 | [train_policy] epoch #48 | Saving snapshot...
2021-06-04 13:48:30 | [train_policy] epoch #48 | Saved
2021-06-04 13:48:30 | [train_policy] epoch #48 | Time 41.86 s
2021-06-04 13:48:30 | [train_policy] epoch #48 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285393
Evaluation/AverageDiscountedReturn         -73.6736
Evaluation/AverageReturn                   -73.6736
Evaluation/CompletionRate                    0
Evaluation/Iteration                        48
Evaluation/MaxReturn                       -38.5195
Evaluation/MinReturn                     -2067.87
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.145
Extras/EpisodeRewardMean                   -71.4964
LinearFeatureBaseline/ExplainedVariance      0.0101164
PolicyExecTime                               0.226414
ProcessExecTime                              0.0312686
TotalEnvSteps                            49588
policy/Entropy                               1.89845
policy/KL                                    0.00685991
policy/KLBefore                              0
policy/LossAfter                            -0.0181343
policy/LossBefore                           -8.01011e-09
policy/Perplexity                            6.67554
policy/dLoss                                 0.0181343
---------------------------------------  ---------------
2021-06-04 13:48:30 | [train_policy] epoch #49 | Obtaining samples for iteration 49...
2021-06-04 13:48:30 | [train_policy] epoch #49 | Logging diagnostics...
2021-06-04 13:48:30 | [train_policy] epoch #49 | Optimizing policy...
2021-06-04 13:48:30 | [train_policy] epoch #49 | Computing loss before
2021-06-04 13:48:30 | [train_policy] epoch #49 | Computing KL before
2021-06-04 13:48:30 | [train_policy] epoch #49 | Optimizing
2021-06-04 13:48:30 | [train_policy] epoch #49 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:30 | [train_policy] epoch #49 | computing loss before
2021-06-04 13:48:30 | [train_policy] epoch #49 | computing gradient
2021-06-04 13:48:30 | [train_policy] epoch #49 | gradient computed
2021-06-04 13:48:30 | [train_policy] epoch #49 | computing descent direction
2021-06-04 13:48:30 | [train_policy] epoch #49 | descent direction computed
2021-06-04 13:48:30 | [train_policy] epoch #49 | backtrack iters: 1
2021-06-04 13:48:30 | [train_policy] epoch #49 | optimization finished
2021-06-04 13:48:30 | [train_policy] epoch #49 | Computing KL after
2021-06-04 13:48:30 | [train_policy] epoch #49 | Computing loss after
2021-06-04 13:48:30 | [train_policy] epoch #49 | Fitting baseline...
2021-06-04 13:48:30 | [train_policy] epoch #49 | Saving snapshot...
2021-06-04 13:48:30 | [train_policy] epoch #49 | Saved
2021-06-04 13:48:30 | [train_policy] epoch #49 | Time 42.63 s
2021-06-04 13:48:30 | [train_policy] epoch #49 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.285468
Evaluation/AverageDiscountedReturn         -50.6799
Evaluation/AverageReturn                   -50.6799
Evaluation/CompletionRate                    0
Evaluation/Iteration                        49
Evaluation/MaxReturn                       -39.8902
Evaluation/MinReturn                       -70.6952
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.51689
Extras/EpisodeRewardMean                   -50.9517
LinearFeatureBaseline/ExplainedVariance    -47.3925
PolicyExecTime                               0.216668
ProcessExecTime                              0.0312212
TotalEnvSteps                            50600
policy/Entropy                               1.89061
policy/KL                                    0.00652779
policy/KLBefore                              0
policy/LossAfter                            -0.0193667
policy/LossBefore                           -6.59656e-09
policy/Perplexity                            6.62338
policy/dLoss                                 0.0193666
---------------------------------------  ---------------
2021-06-04 13:48:30 | [train_policy] epoch #50 | Obtaining samples for iteration 50...
2021-06-04 13:48:31 | [train_policy] epoch #50 | Logging diagnostics...
2021-06-04 13:48:31 | [train_policy] epoch #50 | Optimizing policy...
2021-06-04 13:48:31 | [train_policy] epoch #50 | Computing loss before
2021-06-04 13:48:31 | [train_policy] epoch #50 | Computing KL before
2021-06-04 13:48:31 | [train_policy] epoch #50 | Optimizing
2021-06-04 13:48:31 | [train_policy] epoch #50 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:31 | [train_policy] epoch #50 | computing loss before
2021-06-04 13:48:31 | [train_policy] epoch #50 | computing gradient
2021-06-04 13:48:31 | [train_policy] epoch #50 | gradient computed
2021-06-04 13:48:31 | [train_policy] epoch #50 | computing descent direction
2021-06-04 13:48:31 | [train_policy] epoch #50 | descent direction computed
2021-06-04 13:48:31 | [train_policy] epoch #50 | backtrack iters: 0
2021-06-04 13:48:31 | [train_policy] epoch #50 | optimization finished
2021-06-04 13:48:31 | [train_policy] epoch #50 | Computing KL after
2021-06-04 13:48:31 | [train_policy] epoch #50 | Computing loss after
2021-06-04 13:48:31 | [train_policy] epoch #50 | Fitting baseline...
2021-06-04 13:48:31 | [train_policy] epoch #50 | Saving snapshot...
2021-06-04 13:48:31 | [train_policy] epoch #50 | Saved
2021-06-04 13:48:31 | [train_policy] epoch #50 | Time 43.43 s
2021-06-04 13:48:31 | [train_policy] epoch #50 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.290038
Evaluation/AverageDiscountedReturn         -50.7621
Evaluation/AverageReturn                   -50.7621
Evaluation/CompletionRate                    0
Evaluation/Iteration                        50
Evaluation/MaxReturn                       -36.6961
Evaluation/MinReturn                       -74.2224
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.56702
Extras/EpisodeRewardMean                   -50.822
LinearFeatureBaseline/ExplainedVariance      0.952985
PolicyExecTime                               0.228105
ProcessExecTime                              0.0317023
TotalEnvSteps                            51612
policy/Entropy                               1.90656
policy/KL                                    0.00971858
policy/KLBefore                              0
policy/LossAfter                            -0.0234037
policy/LossBefore                           -1.41355e-08
policy/Perplexity                            6.72989
policy/dLoss                                 0.0234037
---------------------------------------  ---------------
2021-06-04 13:48:31 | [train_policy] epoch #51 | Obtaining samples for iteration 51...
2021-06-04 13:48:32 | [train_policy] epoch #51 | Logging diagnostics...
2021-06-04 13:48:32 | [train_policy] epoch #51 | Optimizing policy...
2021-06-04 13:48:32 | [train_policy] epoch #51 | Computing loss before
2021-06-04 13:48:32 | [train_policy] epoch #51 | Computing KL before
2021-06-04 13:48:32 | [train_policy] epoch #51 | Optimizing
2021-06-04 13:48:32 | [train_policy] epoch #51 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:32 | [train_policy] epoch #51 | computing loss before
2021-06-04 13:48:32 | [train_policy] epoch #51 | computing gradient
2021-06-04 13:48:32 | [train_policy] epoch #51 | gradient computed
2021-06-04 13:48:32 | [train_policy] epoch #51 | computing descent direction
2021-06-04 13:48:32 | [train_policy] epoch #51 | descent direction computed
2021-06-04 13:48:32 | [train_policy] epoch #51 | backtrack iters: 1
2021-06-04 13:48:32 | [train_policy] epoch #51 | optimization finished
2021-06-04 13:48:32 | [train_policy] epoch #51 | Computing KL after
2021-06-04 13:48:32 | [train_policy] epoch #51 | Computing loss after
2021-06-04 13:48:32 | [train_policy] epoch #51 | Fitting baseline...
2021-06-04 13:48:32 | [train_policy] epoch #51 | Saving snapshot...
2021-06-04 13:48:32 | [train_policy] epoch #51 | Saved
2021-06-04 13:48:32 | [train_policy] epoch #51 | Time 44.22 s
2021-06-04 13:48:32 | [train_policy] epoch #51 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285808
Evaluation/AverageDiscountedReturn         -52.9843
Evaluation/AverageReturn                   -52.9843
Evaluation/CompletionRate                    0
Evaluation/Iteration                        51
Evaluation/MaxReturn                       -35.8583
Evaluation/MinReturn                      -258.056
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        22.6214
Extras/EpisodeRewardMean                   -53.2739
LinearFeatureBaseline/ExplainedVariance      0.477141
PolicyExecTime                               0.229715
ProcessExecTime                              0.0312731
TotalEnvSteps                            52624
policy/Entropy                               1.88388
policy/KL                                    0.00647304
policy/KLBefore                              0
policy/LossAfter                            -0.0179144
policy/LossBefore                           -1.27219e-08
policy/Perplexity                            6.57901
policy/dLoss                                 0.0179144
---------------------------------------  ---------------
2021-06-04 13:48:32 | [train_policy] epoch #52 | Obtaining samples for iteration 52...
2021-06-04 13:48:33 | [train_policy] epoch #52 | Logging diagnostics...
2021-06-04 13:48:33 | [train_policy] epoch #52 | Optimizing policy...
2021-06-04 13:48:33 | [train_policy] epoch #52 | Computing loss before
2021-06-04 13:48:33 | [train_policy] epoch #52 | Computing KL before
2021-06-04 13:48:33 | [train_policy] epoch #52 | Optimizing
2021-06-04 13:48:33 | [train_policy] epoch #52 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:33 | [train_policy] epoch #52 | computing loss before
2021-06-04 13:48:33 | [train_policy] epoch #52 | computing gradient
2021-06-04 13:48:33 | [train_policy] epoch #52 | gradient computed
2021-06-04 13:48:33 | [train_policy] epoch #52 | computing descent direction
2021-06-04 13:48:33 | [train_policy] epoch #52 | descent direction computed
2021-06-04 13:48:33 | [train_policy] epoch #52 | backtrack iters: 0
2021-06-04 13:48:33 | [train_policy] epoch #52 | optimization finished
2021-06-04 13:48:33 | [train_policy] epoch #52 | Computing KL after
2021-06-04 13:48:33 | [train_policy] epoch #52 | Computing loss after
2021-06-04 13:48:33 | [train_policy] epoch #52 | Fitting baseline...
2021-06-04 13:48:33 | [train_policy] epoch #52 | Saving snapshot...
2021-06-04 13:48:33 | [train_policy] epoch #52 | Saved
2021-06-04 13:48:33 | [train_policy] epoch #52 | Time 44.99 s
2021-06-04 13:48:33 | [train_policy] epoch #52 | EpochTime 0.76 s
---------------------------------------  --------------
EnvExecTime                                  0.28531
Evaluation/AverageDiscountedReturn         -52.9896
Evaluation/AverageReturn                   -52.9896
Evaluation/CompletionRate                    0
Evaluation/Iteration                        52
Evaluation/MaxReturn                       -36.2974
Evaluation/MinReturn                      -288.859
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        25.6211
Extras/EpisodeRewardMean                   -52.6893
LinearFeatureBaseline/ExplainedVariance      0.564965
PolicyExecTime                               0.21333
ProcessExecTime                              0.0312655
TotalEnvSteps                            53636
policy/Entropy                               1.89225
policy/KL                                    0.00959447
policy/KLBefore                              0
policy/LossAfter                            -0.0162147
policy/LossBefore                            1.5549e-08
policy/Perplexity                            6.63426
policy/dLoss                                 0.0162147
---------------------------------------  --------------
2021-06-04 13:48:33 | [train_policy] epoch #53 | Obtaining samples for iteration 53...
2021-06-04 13:48:33 | [train_policy] epoch #53 | Logging diagnostics...
2021-06-04 13:48:33 | [train_policy] epoch #53 | Optimizing policy...
2021-06-04 13:48:33 | [train_policy] epoch #53 | Computing loss before
2021-06-04 13:48:33 | [train_policy] epoch #53 | Computing KL before
2021-06-04 13:48:33 | [train_policy] epoch #53 | Optimizing
2021-06-04 13:48:33 | [train_policy] epoch #53 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:33 | [train_policy] epoch #53 | computing loss before
2021-06-04 13:48:33 | [train_policy] epoch #53 | computing gradient
2021-06-04 13:48:33 | [train_policy] epoch #53 | gradient computed
2021-06-04 13:48:33 | [train_policy] epoch #53 | computing descent direction
2021-06-04 13:48:33 | [train_policy] epoch #53 | descent direction computed
2021-06-04 13:48:34 | [train_policy] epoch #53 | backtrack iters: 0
2021-06-04 13:48:34 | [train_policy] epoch #53 | optimization finished
2021-06-04 13:48:34 | [train_policy] epoch #53 | Computing KL after
2021-06-04 13:48:34 | [train_policy] epoch #53 | Computing loss after
2021-06-04 13:48:34 | [train_policy] epoch #53 | Fitting baseline...
2021-06-04 13:48:34 | [train_policy] epoch #53 | Saving snapshot...
2021-06-04 13:48:34 | [train_policy] epoch #53 | Saved
2021-06-04 13:48:34 | [train_policy] epoch #53 | Time 45.78 s
2021-06-04 13:48:34 | [train_policy] epoch #53 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.283817
Evaluation/AverageDiscountedReturn         -95.4055
Evaluation/AverageReturn                   -95.4055
Evaluation/CompletionRate                    0
Evaluation/Iteration                        53
Evaluation/MaxReturn                       -39.6341
Evaluation/MinReturn                     -2062.91
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       293.228
Extras/EpisodeRewardMean                   -92.0995
LinearFeatureBaseline/ExplainedVariance      0.0108166
PolicyExecTime                               0.228698
ProcessExecTime                              0.0311198
TotalEnvSteps                            54648
policy/Entropy                               1.91805
policy/KL                                    0.00985996
policy/KLBefore                              0
policy/LossAfter                            -0.0170196
policy/LossBefore                            4.71183e-10
policy/Perplexity                            6.80766
policy/dLoss                                 0.0170196
---------------------------------------  ---------------
2021-06-04 13:48:34 | [train_policy] epoch #54 | Obtaining samples for iteration 54...
2021-06-04 13:48:34 | [train_policy] epoch #54 | Logging diagnostics...
2021-06-04 13:48:34 | [train_policy] epoch #54 | Optimizing policy...
2021-06-04 13:48:34 | [train_policy] epoch #54 | Computing loss before
2021-06-04 13:48:34 | [train_policy] epoch #54 | Computing KL before
2021-06-04 13:48:34 | [train_policy] epoch #54 | Optimizing
2021-06-04 13:48:34 | [train_policy] epoch #54 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:34 | [train_policy] epoch #54 | computing loss before
2021-06-04 13:48:34 | [train_policy] epoch #54 | computing gradient
2021-06-04 13:48:34 | [train_policy] epoch #54 | gradient computed
2021-06-04 13:48:34 | [train_policy] epoch #54 | computing descent direction
2021-06-04 13:48:34 | [train_policy] epoch #54 | descent direction computed
2021-06-04 13:48:34 | [train_policy] epoch #54 | backtrack iters: 1
2021-06-04 13:48:34 | [train_policy] epoch #54 | optimization finished
2021-06-04 13:48:34 | [train_policy] epoch #54 | Computing KL after
2021-06-04 13:48:34 | [train_policy] epoch #54 | Computing loss after
2021-06-04 13:48:34 | [train_policy] epoch #54 | Fitting baseline...
2021-06-04 13:48:34 | [train_policy] epoch #54 | Saving snapshot...
2021-06-04 13:48:34 | [train_policy] epoch #54 | Saved
2021-06-04 13:48:34 | [train_policy] epoch #54 | Time 46.56 s
2021-06-04 13:48:34 | [train_policy] epoch #54 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286778
Evaluation/AverageDiscountedReturn         -52.7548
Evaluation/AverageReturn                   -52.7548
Evaluation/CompletionRate                    0
Evaluation/Iteration                        54
Evaluation/MaxReturn                       -39.6813
Evaluation/MinReturn                       -91.1762
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.80247
Extras/EpisodeRewardMean                   -72.5848
LinearFeatureBaseline/ExplainedVariance    -34.8335
PolicyExecTime                               0.226036
ProcessExecTime                              0.0313525
TotalEnvSteps                            55660
policy/Entropy                               1.9233
policy/KL                                    0.00683224
policy/KLBefore                              0
policy/LossAfter                            -0.0134258
policy/LossBefore                            2.63862e-08
policy/Perplexity                            6.84349
policy/dLoss                                 0.0134258
---------------------------------------  ---------------
2021-06-04 13:48:34 | [train_policy] epoch #55 | Obtaining samples for iteration 55...
2021-06-04 13:48:35 | [train_policy] epoch #55 | Logging diagnostics...
2021-06-04 13:48:35 | [train_policy] epoch #55 | Optimizing policy...
2021-06-04 13:48:35 | [train_policy] epoch #55 | Computing loss before
2021-06-04 13:48:35 | [train_policy] epoch #55 | Computing KL before
2021-06-04 13:48:35 | [train_policy] epoch #55 | Optimizing
2021-06-04 13:48:35 | [train_policy] epoch #55 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:35 | [train_policy] epoch #55 | computing loss before
2021-06-04 13:48:35 | [train_policy] epoch #55 | computing gradient
2021-06-04 13:48:35 | [train_policy] epoch #55 | gradient computed
2021-06-04 13:48:35 | [train_policy] epoch #55 | computing descent direction
2021-06-04 13:48:35 | [train_policy] epoch #55 | descent direction computed
2021-06-04 13:48:35 | [train_policy] epoch #55 | backtrack iters: 1
2021-06-04 13:48:35 | [train_policy] epoch #55 | optimization finished
2021-06-04 13:48:35 | [train_policy] epoch #55 | Computing KL after
2021-06-04 13:48:35 | [train_policy] epoch #55 | Computing loss after
2021-06-04 13:48:35 | [train_policy] epoch #55 | Fitting baseline...
2021-06-04 13:48:35 | [train_policy] epoch #55 | Saving snapshot...
2021-06-04 13:48:35 | [train_policy] epoch #55 | Saved
2021-06-04 13:48:35 | [train_policy] epoch #55 | Time 47.36 s
2021-06-04 13:48:35 | [train_policy] epoch #55 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285377
Evaluation/AverageDiscountedReturn         -50.1717
Evaluation/AverageReturn                   -50.1717
Evaluation/CompletionRate                    0
Evaluation/Iteration                        55
Evaluation/MaxReturn                       -40.6154
Evaluation/MinReturn                       -80.6735
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.96913
Extras/EpisodeRewardMean                   -50.4984
LinearFeatureBaseline/ExplainedVariance      0.935854
PolicyExecTime                               0.231177
ProcessExecTime                              0.0311289
TotalEnvSteps                            56672
policy/Entropy                               1.9016
policy/KL                                    0.00663156
policy/KLBefore                              0
policy/LossAfter                            -0.0231512
policy/LossBefore                           -1.69626e-08
policy/Perplexity                            6.69663
policy/dLoss                                 0.0231511
---------------------------------------  ---------------
2021-06-04 13:48:35 | [train_policy] epoch #56 | Obtaining samples for iteration 56...
2021-06-04 13:48:36 | [train_policy] epoch #56 | Logging diagnostics...
2021-06-04 13:48:36 | [train_policy] epoch #56 | Optimizing policy...
2021-06-04 13:48:36 | [train_policy] epoch #56 | Computing loss before
2021-06-04 13:48:36 | [train_policy] epoch #56 | Computing KL before
2021-06-04 13:48:36 | [train_policy] epoch #56 | Optimizing
2021-06-04 13:48:36 | [train_policy] epoch #56 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:36 | [train_policy] epoch #56 | computing loss before
2021-06-04 13:48:36 | [train_policy] epoch #56 | computing gradient
2021-06-04 13:48:36 | [train_policy] epoch #56 | gradient computed
2021-06-04 13:48:36 | [train_policy] epoch #56 | computing descent direction
2021-06-04 13:48:36 | [train_policy] epoch #56 | descent direction computed
2021-06-04 13:48:36 | [train_policy] epoch #56 | backtrack iters: 1
2021-06-04 13:48:36 | [train_policy] epoch #56 | optimization finished
2021-06-04 13:48:36 | [train_policy] epoch #56 | Computing KL after
2021-06-04 13:48:36 | [train_policy] epoch #56 | Computing loss after
2021-06-04 13:48:36 | [train_policy] epoch #56 | Fitting baseline...
2021-06-04 13:48:36 | [train_policy] epoch #56 | Saving snapshot...
2021-06-04 13:48:36 | [train_policy] epoch #56 | Saved
2021-06-04 13:48:36 | [train_policy] epoch #56 | Time 48.15 s
2021-06-04 13:48:36 | [train_policy] epoch #56 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.286258
Evaluation/AverageDiscountedReturn         -50.2214
Evaluation/AverageReturn                   -50.2214
Evaluation/CompletionRate                    0
Evaluation/Iteration                        56
Evaluation/MaxReturn                       -35.8467
Evaluation/MinReturn                       -70.2005
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.62718
Extras/EpisodeRewardMean                   -50.368
LinearFeatureBaseline/ExplainedVariance      0.953184
PolicyExecTime                               0.220668
ProcessExecTime                              0.0313554
TotalEnvSteps                            57684
policy/Entropy                               1.87152
policy/KL                                    0.0065363
policy/KLBefore                              0
policy/LossAfter                            -0.0210513
policy/LossBefore                            4.71183e-09
policy/Perplexity                            6.49819
policy/dLoss                                 0.0210514
---------------------------------------  ---------------
2021-06-04 13:48:36 | [train_policy] epoch #57 | Obtaining samples for iteration 57...
2021-06-04 13:48:37 | [train_policy] epoch #57 | Logging diagnostics...
2021-06-04 13:48:37 | [train_policy] epoch #57 | Optimizing policy...
2021-06-04 13:48:37 | [train_policy] epoch #57 | Computing loss before
2021-06-04 13:48:37 | [train_policy] epoch #57 | Computing KL before
2021-06-04 13:48:37 | [train_policy] epoch #57 | Optimizing
2021-06-04 13:48:37 | [train_policy] epoch #57 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:37 | [train_policy] epoch #57 | computing loss before
2021-06-04 13:48:37 | [train_policy] epoch #57 | computing gradient
2021-06-04 13:48:37 | [train_policy] epoch #57 | gradient computed
2021-06-04 13:48:37 | [train_policy] epoch #57 | computing descent direction
2021-06-04 13:48:37 | [train_policy] epoch #57 | descent direction computed
2021-06-04 13:48:37 | [train_policy] epoch #57 | backtrack iters: 1
2021-06-04 13:48:37 | [train_policy] epoch #57 | optimization finished
2021-06-04 13:48:37 | [train_policy] epoch #57 | Computing KL after
2021-06-04 13:48:37 | [train_policy] epoch #57 | Computing loss after
2021-06-04 13:48:37 | [train_policy] epoch #57 | Fitting baseline...
2021-06-04 13:48:37 | [train_policy] epoch #57 | Saving snapshot...
2021-06-04 13:48:37 | [train_policy] epoch #57 | Saved
2021-06-04 13:48:37 | [train_policy] epoch #57 | Time 48.95 s
2021-06-04 13:48:37 | [train_policy] epoch #57 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286037
Evaluation/AverageDiscountedReturn         -50.9204
Evaluation/AverageReturn                   -50.9204
Evaluation/CompletionRate                    0
Evaluation/Iteration                        57
Evaluation/MaxReturn                       -38.5103
Evaluation/MinReturn                      -126.441
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        10.1668
Extras/EpisodeRewardMean                   -50.7554
LinearFeatureBaseline/ExplainedVariance      0.854982
PolicyExecTime                               0.226107
ProcessExecTime                              0.031446
TotalEnvSteps                            58696
policy/Entropy                               1.78371
policy/KL                                    0.0065686
policy/KLBefore                              0
policy/LossAfter                            -0.0231907
policy/LossBefore                           -2.26168e-08
policy/Perplexity                            5.95191
policy/dLoss                                 0.0231907
---------------------------------------  ---------------
2021-06-04 13:48:37 | [train_policy] epoch #58 | Obtaining samples for iteration 58...
2021-06-04 13:48:37 | [train_policy] epoch #58 | Logging diagnostics...
2021-06-04 13:48:37 | [train_policy] epoch #58 | Optimizing policy...
2021-06-04 13:48:37 | [train_policy] epoch #58 | Computing loss before
2021-06-04 13:48:37 | [train_policy] epoch #58 | Computing KL before
2021-06-04 13:48:37 | [train_policy] epoch #58 | Optimizing
2021-06-04 13:48:37 | [train_policy] epoch #58 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:37 | [train_policy] epoch #58 | computing loss before
2021-06-04 13:48:37 | [train_policy] epoch #58 | computing gradient
2021-06-04 13:48:37 | [train_policy] epoch #58 | gradient computed
2021-06-04 13:48:37 | [train_policy] epoch #58 | computing descent direction
2021-06-04 13:48:37 | [train_policy] epoch #58 | descent direction computed
2021-06-04 13:48:37 | [train_policy] epoch #58 | backtrack iters: 1
2021-06-04 13:48:37 | [train_policy] epoch #58 | optimization finished
2021-06-04 13:48:37 | [train_policy] epoch #58 | Computing KL after
2021-06-04 13:48:37 | [train_policy] epoch #58 | Computing loss after
2021-06-04 13:48:37 | [train_policy] epoch #58 | Fitting baseline...
2021-06-04 13:48:37 | [train_policy] epoch #58 | Saving snapshot...
2021-06-04 13:48:38 | [train_policy] epoch #58 | Saved
2021-06-04 13:48:38 | [train_policy] epoch #58 | Time 49.74 s
2021-06-04 13:48:38 | [train_policy] epoch #58 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.284447
Evaluation/AverageDiscountedReturn         -50.7583
Evaluation/AverageReturn                   -50.7583
Evaluation/CompletionRate                    0
Evaluation/Iteration                        58
Evaluation/MaxReturn                       -39.8438
Evaluation/MinReturn                       -85.4648
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.85593
Extras/EpisodeRewardMean                   -50.8841
LinearFeatureBaseline/ExplainedVariance      0.915412
PolicyExecTime                               0.23266
ProcessExecTime                              0.0311077
TotalEnvSteps                            59708
policy/Entropy                               1.73918
policy/KL                                    0.00681671
policy/KLBefore                              0
policy/LossAfter                            -0.0182698
policy/LossBefore                            2.59151e-08
policy/Perplexity                            5.69269
policy/dLoss                                 0.0182699
---------------------------------------  ---------------
2021-06-04 13:48:38 | [train_policy] epoch #59 | Obtaining samples for iteration 59...
2021-06-04 13:48:38 | [train_policy] epoch #59 | Logging diagnostics...
2021-06-04 13:48:38 | [train_policy] epoch #59 | Optimizing policy...
2021-06-04 13:48:38 | [train_policy] epoch #59 | Computing loss before
2021-06-04 13:48:38 | [train_policy] epoch #59 | Computing KL before
2021-06-04 13:48:38 | [train_policy] epoch #59 | Optimizing
2021-06-04 13:48:38 | [train_policy] epoch #59 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:38 | [train_policy] epoch #59 | computing loss before
2021-06-04 13:48:38 | [train_policy] epoch #59 | computing gradient
2021-06-04 13:48:38 | [train_policy] epoch #59 | gradient computed
2021-06-04 13:48:38 | [train_policy] epoch #59 | computing descent direction
2021-06-04 13:48:38 | [train_policy] epoch #59 | descent direction computed
2021-06-04 13:48:38 | [train_policy] epoch #59 | backtrack iters: 1
2021-06-04 13:48:38 | [train_policy] epoch #59 | optimization finished
2021-06-04 13:48:38 | [train_policy] epoch #59 | Computing KL after
2021-06-04 13:48:38 | [train_policy] epoch #59 | Computing loss after
2021-06-04 13:48:38 | [train_policy] epoch #59 | Fitting baseline...
2021-06-04 13:48:38 | [train_policy] epoch #59 | Saving snapshot...
2021-06-04 13:48:38 | [train_policy] epoch #59 | Saved
2021-06-04 13:48:38 | [train_policy] epoch #59 | Time 50.53 s
2021-06-04 13:48:38 | [train_policy] epoch #59 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.283599
Evaluation/AverageDiscountedReturn         -70.9614
Evaluation/AverageReturn                   -70.9614
Evaluation/CompletionRate                    0
Evaluation/Iteration                        59
Evaluation/MaxReturn                       -36.2658
Evaluation/MinReturn                     -2055.54
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.118
Extras/EpisodeRewardMean                   -69.2593
LinearFeatureBaseline/ExplainedVariance      0.0110335
PolicyExecTime                               0.224669
ProcessExecTime                              0.0310504
TotalEnvSteps                            60720
policy/Entropy                               1.74197
policy/KL                                    0.00731781
policy/KLBefore                              0
policy/LossAfter                            -0.022642
policy/LossBefore                            7.53893e-09
policy/Perplexity                            5.7086
policy/dLoss                                 0.022642
---------------------------------------  ---------------
2021-06-04 13:48:38 | [train_policy] epoch #60 | Obtaining samples for iteration 60...
2021-06-04 13:48:39 | [train_policy] epoch #60 | Logging diagnostics...
2021-06-04 13:48:39 | [train_policy] epoch #60 | Optimizing policy...
2021-06-04 13:48:39 | [train_policy] epoch #60 | Computing loss before
2021-06-04 13:48:39 | [train_policy] epoch #60 | Computing KL before
2021-06-04 13:48:39 | [train_policy] epoch #60 | Optimizing
2021-06-04 13:48:39 | [train_policy] epoch #60 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:39 | [train_policy] epoch #60 | computing loss before
2021-06-04 13:48:39 | [train_policy] epoch #60 | computing gradient
2021-06-04 13:48:39 | [train_policy] epoch #60 | gradient computed
2021-06-04 13:48:39 | [train_policy] epoch #60 | computing descent direction
2021-06-04 13:48:39 | [train_policy] epoch #60 | descent direction computed
2021-06-04 13:48:39 | [train_policy] epoch #60 | backtrack iters: 1
2021-06-04 13:48:39 | [train_policy] epoch #60 | optimization finished
2021-06-04 13:48:39 | [train_policy] epoch #60 | Computing KL after
2021-06-04 13:48:39 | [train_policy] epoch #60 | Computing loss after
2021-06-04 13:48:39 | [train_policy] epoch #60 | Fitting baseline...
2021-06-04 13:48:39 | [train_policy] epoch #60 | Saving snapshot...
2021-06-04 13:48:39 | [train_policy] epoch #60 | Saved
2021-06-04 13:48:39 | [train_policy] epoch #60 | Time 51.31 s
2021-06-04 13:48:39 | [train_policy] epoch #60 | EpochTime 0.76 s
---------------------------------------  --------------
EnvExecTime                                  0.285355
Evaluation/AverageDiscountedReturn         -56.3315
Evaluation/AverageReturn                   -56.3315
Evaluation/CompletionRate                    0
Evaluation/Iteration                        60
Evaluation/MaxReturn                       -36.5408
Evaluation/MinReturn                      -387.171
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        37.0888
Extras/EpisodeRewardMean                   -55.7276
LinearFeatureBaseline/ExplainedVariance    -12.5335
PolicyExecTime                               0.217984
ProcessExecTime                              0.0319734
TotalEnvSteps                            61732
policy/Entropy                               1.7227
policy/KL                                    0.00673758
policy/KLBefore                              0
policy/LossAfter                            -0.0282832
policy/LossBefore                           -2.8271e-09
policy/Perplexity                            5.59963
policy/dLoss                                 0.0282832
---------------------------------------  --------------
2021-06-04 13:48:39 | [train_policy] epoch #61 | Obtaining samples for iteration 61...
2021-06-04 13:48:40 | [train_policy] epoch #61 | Logging diagnostics...
2021-06-04 13:48:40 | [train_policy] epoch #61 | Optimizing policy...
2021-06-04 13:48:40 | [train_policy] epoch #61 | Computing loss before
2021-06-04 13:48:40 | [train_policy] epoch #61 | Computing KL before
2021-06-04 13:48:40 | [train_policy] epoch #61 | Optimizing
2021-06-04 13:48:40 | [train_policy] epoch #61 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:40 | [train_policy] epoch #61 | computing loss before
2021-06-04 13:48:40 | [train_policy] epoch #61 | computing gradient
2021-06-04 13:48:40 | [train_policy] epoch #61 | gradient computed
2021-06-04 13:48:40 | [train_policy] epoch #61 | computing descent direction
2021-06-04 13:48:40 | [train_policy] epoch #61 | descent direction computed
2021-06-04 13:48:40 | [train_policy] epoch #61 | backtrack iters: 1
2021-06-04 13:48:40 | [train_policy] epoch #61 | optimization finished
2021-06-04 13:48:40 | [train_policy] epoch #61 | Computing KL after
2021-06-04 13:48:40 | [train_policy] epoch #61 | Computing loss after
2021-06-04 13:48:40 | [train_policy] epoch #61 | Fitting baseline...
2021-06-04 13:48:40 | [train_policy] epoch #61 | Saving snapshot...
2021-06-04 13:48:40 | [train_policy] epoch #61 | Saved
2021-06-04 13:48:40 | [train_policy] epoch #61 | Time 52.10 s
2021-06-04 13:48:40 | [train_policy] epoch #61 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286553
Evaluation/AverageDiscountedReturn         -71.1811
Evaluation/AverageReturn                   -71.1811
Evaluation/CompletionRate                    0
Evaluation/Iteration                        61
Evaluation/MaxReturn                       -38.3289
Evaluation/MinReturn                     -2065.68
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.161
Extras/EpisodeRewardMean                   -69.4647
LinearFeatureBaseline/ExplainedVariance      0.00948439
PolicyExecTime                               0.225083
ProcessExecTime                              0.0312023
TotalEnvSteps                            62744
policy/Entropy                               1.71743
policy/KL                                    0.00634915
policy/KLBefore                              0
policy/LossAfter                            -0.021809
policy/LossBefore                           -1.41355e-09
policy/Perplexity                            5.57022
policy/dLoss                                 0.021809
---------------------------------------  ---------------
2021-06-04 13:48:40 | [train_policy] epoch #62 | Obtaining samples for iteration 62...
2021-06-04 13:48:41 | [train_policy] epoch #62 | Logging diagnostics...
2021-06-04 13:48:41 | [train_policy] epoch #62 | Optimizing policy...
2021-06-04 13:48:41 | [train_policy] epoch #62 | Computing loss before
2021-06-04 13:48:41 | [train_policy] epoch #62 | Computing KL before
2021-06-04 13:48:41 | [train_policy] epoch #62 | Optimizing
2021-06-04 13:48:41 | [train_policy] epoch #62 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:41 | [train_policy] epoch #62 | computing loss before
2021-06-04 13:48:41 | [train_policy] epoch #62 | computing gradient
2021-06-04 13:48:41 | [train_policy] epoch #62 | gradient computed
2021-06-04 13:48:41 | [train_policy] epoch #62 | computing descent direction
2021-06-04 13:48:41 | [train_policy] epoch #62 | descent direction computed
2021-06-04 13:48:41 | [train_policy] epoch #62 | backtrack iters: 0
2021-06-04 13:48:41 | [train_policy] epoch #62 | optimization finished
2021-06-04 13:48:41 | [train_policy] epoch #62 | Computing KL after
2021-06-04 13:48:41 | [train_policy] epoch #62 | Computing loss after
2021-06-04 13:48:41 | [train_policy] epoch #62 | Fitting baseline...
2021-06-04 13:48:41 | [train_policy] epoch #62 | Saving snapshot...
2021-06-04 13:48:41 | [train_policy] epoch #62 | Saved
2021-06-04 13:48:41 | [train_policy] epoch #62 | Time 52.89 s
2021-06-04 13:48:41 | [train_policy] epoch #62 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286651
Evaluation/AverageDiscountedReturn         -50.3821
Evaluation/AverageReturn                   -50.3821
Evaluation/CompletionRate                    0
Evaluation/Iteration                        62
Evaluation/MaxReturn                       -36.9847
Evaluation/MinReturn                      -155.581
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        12.5129
Extras/EpisodeRewardMean                   -49.7807
LinearFeatureBaseline/ExplainedVariance    -19.4057
PolicyExecTime                               0.229422
ProcessExecTime                              0.0314326
TotalEnvSteps                            63756
policy/Entropy                               1.67388
policy/KL                                    0.00993922
policy/KLBefore                              0
policy/LossAfter                            -0.022234
policy/LossBefore                           -9.42366e-10
policy/Perplexity                            5.33282
policy/dLoss                                 0.022234
---------------------------------------  ---------------
2021-06-04 13:48:41 | [train_policy] epoch #63 | Obtaining samples for iteration 63...
2021-06-04 13:48:41 | [train_policy] epoch #63 | Logging diagnostics...
2021-06-04 13:48:41 | [train_policy] epoch #63 | Optimizing policy...
2021-06-04 13:48:41 | [train_policy] epoch #63 | Computing loss before
2021-06-04 13:48:41 | [train_policy] epoch #63 | Computing KL before
2021-06-04 13:48:41 | [train_policy] epoch #63 | Optimizing
2021-06-04 13:48:41 | [train_policy] epoch #63 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:41 | [train_policy] epoch #63 | computing loss before
2021-06-04 13:48:41 | [train_policy] epoch #63 | computing gradient
2021-06-04 13:48:41 | [train_policy] epoch #63 | gradient computed
2021-06-04 13:48:41 | [train_policy] epoch #63 | computing descent direction
2021-06-04 13:48:41 | [train_policy] epoch #63 | descent direction computed
2021-06-04 13:48:41 | [train_policy] epoch #63 | backtrack iters: 0
2021-06-04 13:48:41 | [train_policy] epoch #63 | optimization finished
2021-06-04 13:48:41 | [train_policy] epoch #63 | Computing KL after
2021-06-04 13:48:41 | [train_policy] epoch #63 | Computing loss after
2021-06-04 13:48:41 | [train_policy] epoch #63 | Fitting baseline...
2021-06-04 13:48:41 | [train_policy] epoch #63 | Saving snapshot...
2021-06-04 13:48:41 | [train_policy] epoch #63 | Saved
2021-06-04 13:48:41 | [train_policy] epoch #63 | Time 53.65 s
2021-06-04 13:48:41 | [train_policy] epoch #63 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                  0.288417
Evaluation/AverageDiscountedReturn         -50.3206
Evaluation/AverageReturn                   -50.3206
Evaluation/CompletionRate                    0
Evaluation/Iteration                        63
Evaluation/MaxReturn                       -39.5049
Evaluation/MinReturn                       -67.8315
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.81338
Extras/EpisodeRewardMean                   -50.146
LinearFeatureBaseline/ExplainedVariance      0.94887
PolicyExecTime                               0.21303
ProcessExecTime                              0.0316067
TotalEnvSteps                            64768
policy/Entropy                               1.68559
policy/KL                                    0.00992086
policy/KLBefore                              0
policy/LossAfter                            -0.024427
policy/LossBefore                            9.89484e-09
policy/Perplexity                            5.39565
policy/dLoss                                 0.024427
---------------------------------------  ---------------
2021-06-04 13:48:41 | [train_policy] epoch #64 | Obtaining samples for iteration 64...
2021-06-04 13:48:42 | [train_policy] epoch #64 | Logging diagnostics...
2021-06-04 13:48:42 | [train_policy] epoch #64 | Optimizing policy...
2021-06-04 13:48:42 | [train_policy] epoch #64 | Computing loss before
2021-06-04 13:48:42 | [train_policy] epoch #64 | Computing KL before
2021-06-04 13:48:42 | [train_policy] epoch #64 | Optimizing
2021-06-04 13:48:42 | [train_policy] epoch #64 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:42 | [train_policy] epoch #64 | computing loss before
2021-06-04 13:48:42 | [train_policy] epoch #64 | computing gradient
2021-06-04 13:48:42 | [train_policy] epoch #64 | gradient computed
2021-06-04 13:48:42 | [train_policy] epoch #64 | computing descent direction
2021-06-04 13:48:42 | [train_policy] epoch #64 | descent direction computed
2021-06-04 13:48:42 | [train_policy] epoch #64 | backtrack iters: 1
2021-06-04 13:48:42 | [train_policy] epoch #64 | optimization finished
2021-06-04 13:48:42 | [train_policy] epoch #64 | Computing KL after
2021-06-04 13:48:42 | [train_policy] epoch #64 | Computing loss after
2021-06-04 13:48:42 | [train_policy] epoch #64 | Fitting baseline...
2021-06-04 13:48:42 | [train_policy] epoch #64 | Saving snapshot...
2021-06-04 13:48:42 | [train_policy] epoch #64 | Saved
2021-06-04 13:48:42 | [train_policy] epoch #64 | Time 54.45 s
2021-06-04 13:48:42 | [train_policy] epoch #64 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285196
Evaluation/AverageDiscountedReturn         -72.7114
Evaluation/AverageReturn                   -72.7114
Evaluation/CompletionRate                    0
Evaluation/Iteration                        64
Evaluation/MaxReturn                       -37.5104
Evaluation/MinReturn                     -2040.31
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       206.582
Extras/EpisodeRewardMean                   -71.1629
LinearFeatureBaseline/ExplainedVariance      0.0131533
PolicyExecTime                               0.227547
ProcessExecTime                              0.0312288
TotalEnvSteps                            65780
policy/Entropy                               1.6123
policy/KL                                    0.00665039
policy/KLBefore                              0
policy/LossAfter                            -0.0130114
policy/LossBefore                            2.92133e-08
policy/Perplexity                            5.01431
policy/dLoss                                 0.0130115
---------------------------------------  ---------------
2021-06-04 13:48:42 | [train_policy] epoch #65 | Obtaining samples for iteration 65...
2021-06-04 13:48:43 | [train_policy] epoch #65 | Logging diagnostics...
2021-06-04 13:48:43 | [train_policy] epoch #65 | Optimizing policy...
2021-06-04 13:48:43 | [train_policy] epoch #65 | Computing loss before
2021-06-04 13:48:43 | [train_policy] epoch #65 | Computing KL before
2021-06-04 13:48:43 | [train_policy] epoch #65 | Optimizing
2021-06-04 13:48:43 | [train_policy] epoch #65 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:43 | [train_policy] epoch #65 | computing loss before
2021-06-04 13:48:43 | [train_policy] epoch #65 | computing gradient
2021-06-04 13:48:43 | [train_policy] epoch #65 | gradient computed
2021-06-04 13:48:43 | [train_policy] epoch #65 | computing descent direction
2021-06-04 13:48:43 | [train_policy] epoch #65 | descent direction computed
2021-06-04 13:48:43 | [train_policy] epoch #65 | backtrack iters: 0
2021-06-04 13:48:43 | [train_policy] epoch #65 | optimization finished
2021-06-04 13:48:43 | [train_policy] epoch #65 | Computing KL after
2021-06-04 13:48:43 | [train_policy] epoch #65 | Computing loss after
2021-06-04 13:48:43 | [train_policy] epoch #65 | Fitting baseline...
2021-06-04 13:48:43 | [train_policy] epoch #65 | Saving snapshot...
2021-06-04 13:48:43 | [train_policy] epoch #65 | Saved
2021-06-04 13:48:43 | [train_policy] epoch #65 | Time 55.25 s
2021-06-04 13:48:43 | [train_policy] epoch #65 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.290484
Evaluation/AverageDiscountedReturn         -50.2803
Evaluation/AverageReturn                   -50.2803
Evaluation/CompletionRate                    0
Evaluation/Iteration                        65
Evaluation/MaxReturn                       -35.2763
Evaluation/MinReturn                       -88.9744
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.22922
Extras/EpisodeRewardMean                   -70.3427
LinearFeatureBaseline/ExplainedVariance     -5.31385
PolicyExecTime                               0.235656
ProcessExecTime                              0.0318742
TotalEnvSteps                            66792
policy/Entropy                               1.61155
policy/KL                                    0.00962137
policy/KLBefore                              0
policy/LossAfter                            -0.0289403
policy/LossBefore                            2.92133e-08
policy/Perplexity                            5.01058
policy/dLoss                                 0.0289404
---------------------------------------  ---------------
2021-06-04 13:48:43 | [train_policy] epoch #66 | Obtaining samples for iteration 66...
2021-06-04 13:48:44 | [train_policy] epoch #66 | Logging diagnostics...
2021-06-04 13:48:44 | [train_policy] epoch #66 | Optimizing policy...
2021-06-04 13:48:44 | [train_policy] epoch #66 | Computing loss before
2021-06-04 13:48:44 | [train_policy] epoch #66 | Computing KL before
2021-06-04 13:48:44 | [train_policy] epoch #66 | Optimizing
2021-06-04 13:48:44 | [train_policy] epoch #66 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:44 | [train_policy] epoch #66 | computing loss before
2021-06-04 13:48:44 | [train_policy] epoch #66 | computing gradient
2021-06-04 13:48:44 | [train_policy] epoch #66 | gradient computed
2021-06-04 13:48:44 | [train_policy] epoch #66 | computing descent direction
2021-06-04 13:48:44 | [train_policy] epoch #66 | descent direction computed
2021-06-04 13:48:44 | [train_policy] epoch #66 | backtrack iters: 1
2021-06-04 13:48:44 | [train_policy] epoch #66 | optimization finished
2021-06-04 13:48:44 | [train_policy] epoch #66 | Computing KL after
2021-06-04 13:48:44 | [train_policy] epoch #66 | Computing loss after
2021-06-04 13:48:44 | [train_policy] epoch #66 | Fitting baseline...
2021-06-04 13:48:44 | [train_policy] epoch #66 | Saving snapshot...
2021-06-04 13:48:44 | [train_policy] epoch #66 | Saved
2021-06-04 13:48:44 | [train_policy] epoch #66 | Time 56.04 s
2021-06-04 13:48:44 | [train_policy] epoch #66 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285909
Evaluation/AverageDiscountedReturn         -50.7362
Evaluation/AverageReturn                   -50.7362
Evaluation/CompletionRate                    0
Evaluation/Iteration                        66
Evaluation/MaxReturn                       -39.0056
Evaluation/MinReturn                       -83.5375
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.0498
Extras/EpisodeRewardMean                   -50.6601
LinearFeatureBaseline/ExplainedVariance      0.900971
PolicyExecTime                               0.225832
ProcessExecTime                              0.0312908
TotalEnvSteps                            67804
policy/Entropy                               1.53856
policy/KL                                    0.00674286
policy/KLBefore                              0
policy/LossAfter                            -0.0173445
policy/LossBefore                           -1.41355e-08
policy/Perplexity                            4.65786
policy/dLoss                                 0.0173445
---------------------------------------  ---------------
2021-06-04 13:48:44 | [train_policy] epoch #67 | Obtaining samples for iteration 67...
2021-06-04 13:48:44 | [train_policy] epoch #67 | Logging diagnostics...
2021-06-04 13:48:44 | [train_policy] epoch #67 | Optimizing policy...
2021-06-04 13:48:44 | [train_policy] epoch #67 | Computing loss before
2021-06-04 13:48:44 | [train_policy] epoch #67 | Computing KL before
2021-06-04 13:48:44 | [train_policy] epoch #67 | Optimizing
2021-06-04 13:48:44 | [train_policy] epoch #67 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:44 | [train_policy] epoch #67 | computing loss before
2021-06-04 13:48:44 | [train_policy] epoch #67 | computing gradient
2021-06-04 13:48:44 | [train_policy] epoch #67 | gradient computed
2021-06-04 13:48:44 | [train_policy] epoch #67 | computing descent direction
2021-06-04 13:48:45 | [train_policy] epoch #67 | descent direction computed
2021-06-04 13:48:45 | [train_policy] epoch #67 | backtrack iters: 0
2021-06-04 13:48:45 | [train_policy] epoch #67 | optimization finished
2021-06-04 13:48:45 | [train_policy] epoch #67 | Computing KL after
2021-06-04 13:48:45 | [train_policy] epoch #67 | Computing loss after
2021-06-04 13:48:45 | [train_policy] epoch #67 | Fitting baseline...
2021-06-04 13:48:45 | [train_policy] epoch #67 | Saving snapshot...
2021-06-04 13:48:45 | [train_policy] epoch #67 | Saved
2021-06-04 13:48:45 | [train_policy] epoch #67 | Time 56.81 s
2021-06-04 13:48:45 | [train_policy] epoch #67 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.28883
Evaluation/AverageDiscountedReturn         -49.5454
Evaluation/AverageReturn                   -49.5454
Evaluation/CompletionRate                    0
Evaluation/Iteration                        67
Evaluation/MaxReturn                       -35.7078
Evaluation/MinReturn                       -73.0099
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.23708
Extras/EpisodeRewardMean                   -49.6554
LinearFeatureBaseline/ExplainedVariance      0.929609
PolicyExecTime                               0.222427
ProcessExecTime                              0.0315557
TotalEnvSteps                            68816
policy/Entropy                               1.5042
policy/KL                                    0.00995473
policy/KLBefore                              0
policy/LossAfter                            -0.0216148
policy/LossBefore                           -4.71183e-09
policy/Perplexity                            4.50055
policy/dLoss                                 0.0216148
---------------------------------------  ---------------
2021-06-04 13:48:45 | [train_policy] epoch #68 | Obtaining samples for iteration 68...
2021-06-04 13:48:45 | [train_policy] epoch #68 | Logging diagnostics...
2021-06-04 13:48:45 | [train_policy] epoch #68 | Optimizing policy...
2021-06-04 13:48:45 | [train_policy] epoch #68 | Computing loss before
2021-06-04 13:48:45 | [train_policy] epoch #68 | Computing KL before
2021-06-04 13:48:45 | [train_policy] epoch #68 | Optimizing
2021-06-04 13:48:45 | [train_policy] epoch #68 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:45 | [train_policy] epoch #68 | computing loss before
2021-06-04 13:48:45 | [train_policy] epoch #68 | computing gradient
2021-06-04 13:48:45 | [train_policy] epoch #68 | gradient computed
2021-06-04 13:48:45 | [train_policy] epoch #68 | computing descent direction
2021-06-04 13:48:45 | [train_policy] epoch #68 | descent direction computed
2021-06-04 13:48:45 | [train_policy] epoch #68 | backtrack iters: 1
2021-06-04 13:48:45 | [train_policy] epoch #68 | optimization finished
2021-06-04 13:48:45 | [train_policy] epoch #68 | Computing KL after
2021-06-04 13:48:45 | [train_policy] epoch #68 | Computing loss after
2021-06-04 13:48:45 | [train_policy] epoch #68 | Fitting baseline...
2021-06-04 13:48:45 | [train_policy] epoch #68 | Saving snapshot...
2021-06-04 13:48:45 | [train_policy] epoch #68 | Saved
2021-06-04 13:48:45 | [train_policy] epoch #68 | Time 57.61 s
2021-06-04 13:48:45 | [train_policy] epoch #68 | EpochTime 0.77 s
---------------------------------------  --------------
EnvExecTime                                  0.284324
Evaluation/AverageDiscountedReturn         -51.0315
Evaluation/AverageReturn                   -51.0315
Evaluation/CompletionRate                    0
Evaluation/Iteration                        68
Evaluation/MaxReturn                       -35.45
Evaluation/MinReturn                      -156.538
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        12.9336
Extras/EpisodeRewardMean                   -50.867
LinearFeatureBaseline/ExplainedVariance      0.836207
PolicyExecTime                               0.230106
ProcessExecTime                              0.0311074
TotalEnvSteps                            69828
policy/Entropy                               1.44599
policy/KL                                    0.00688838
policy/KLBefore                              0
policy/LossAfter                            -0.0223138
policy/LossBefore                           -0
policy/Perplexity                            4.24605
policy/dLoss                                 0.0223138
---------------------------------------  --------------
2021-06-04 13:48:45 | [train_policy] epoch #69 | Obtaining samples for iteration 69...
2021-06-04 13:48:46 | [train_policy] epoch #69 | Logging diagnostics...
2021-06-04 13:48:46 | [train_policy] epoch #69 | Optimizing policy...
2021-06-04 13:48:46 | [train_policy] epoch #69 | Computing loss before
2021-06-04 13:48:46 | [train_policy] epoch #69 | Computing KL before
2021-06-04 13:48:46 | [train_policy] epoch #69 | Optimizing
2021-06-04 13:48:46 | [train_policy] epoch #69 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:46 | [train_policy] epoch #69 | computing loss before
2021-06-04 13:48:46 | [train_policy] epoch #69 | computing gradient
2021-06-04 13:48:46 | [train_policy] epoch #69 | gradient computed
2021-06-04 13:48:46 | [train_policy] epoch #69 | computing descent direction
2021-06-04 13:48:46 | [train_policy] epoch #69 | descent direction computed
2021-06-04 13:48:46 | [train_policy] epoch #69 | backtrack iters: 1
2021-06-04 13:48:46 | [train_policy] epoch #69 | optimization finished
2021-06-04 13:48:46 | [train_policy] epoch #69 | Computing KL after
2021-06-04 13:48:46 | [train_policy] epoch #69 | Computing loss after
2021-06-04 13:48:46 | [train_policy] epoch #69 | Fitting baseline...
2021-06-04 13:48:46 | [train_policy] epoch #69 | Saving snapshot...
2021-06-04 13:48:46 | [train_policy] epoch #69 | Saved
2021-06-04 13:48:46 | [train_policy] epoch #69 | Time 58.39 s
2021-06-04 13:48:46 | [train_policy] epoch #69 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286642
Evaluation/AverageDiscountedReturn         -48.4566
Evaluation/AverageReturn                   -48.4566
Evaluation/CompletionRate                    0
Evaluation/Iteration                        69
Evaluation/MaxReturn                       -38.9495
Evaluation/MinReturn                       -75.1747
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.43801
Extras/EpisodeRewardMean                   -48.6328
LinearFeatureBaseline/ExplainedVariance      0.934278
PolicyExecTime                               0.211133
ProcessExecTime                              0.0314605
TotalEnvSteps                            70840
policy/Entropy                               1.40714
policy/KL                                    0.00652661
policy/KLBefore                              0
policy/LossAfter                            -0.0178227
policy/LossBefore                            7.06774e-10
policy/Perplexity                            4.08427
policy/dLoss                                 0.0178227
---------------------------------------  ---------------
2021-06-04 13:48:46 | [train_policy] epoch #70 | Obtaining samples for iteration 70...
2021-06-04 13:48:47 | [train_policy] epoch #70 | Logging diagnostics...
2021-06-04 13:48:47 | [train_policy] epoch #70 | Optimizing policy...
2021-06-04 13:48:47 | [train_policy] epoch #70 | Computing loss before
2021-06-04 13:48:47 | [train_policy] epoch #70 | Computing KL before
2021-06-04 13:48:47 | [train_policy] epoch #70 | Optimizing
2021-06-04 13:48:47 | [train_policy] epoch #70 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:47 | [train_policy] epoch #70 | computing loss before
2021-06-04 13:48:47 | [train_policy] epoch #70 | computing gradient
2021-06-04 13:48:47 | [train_policy] epoch #70 | gradient computed
2021-06-04 13:48:47 | [train_policy] epoch #70 | computing descent direction
2021-06-04 13:48:47 | [train_policy] epoch #70 | descent direction computed
2021-06-04 13:48:47 | [train_policy] epoch #70 | backtrack iters: 1
2021-06-04 13:48:47 | [train_policy] epoch #70 | optimization finished
2021-06-04 13:48:47 | [train_policy] epoch #70 | Computing KL after
2021-06-04 13:48:47 | [train_policy] epoch #70 | Computing loss after
2021-06-04 13:48:47 | [train_policy] epoch #70 | Fitting baseline...
2021-06-04 13:48:47 | [train_policy] epoch #70 | Saving snapshot...
2021-06-04 13:48:47 | [train_policy] epoch #70 | Saved
2021-06-04 13:48:47 | [train_policy] epoch #70 | Time 59.19 s
2021-06-04 13:48:47 | [train_policy] epoch #70 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285289
Evaluation/AverageDiscountedReturn         -49.0475
Evaluation/AverageReturn                   -49.0475
Evaluation/CompletionRate                    0
Evaluation/Iteration                        70
Evaluation/MaxReturn                       -37.5414
Evaluation/MinReturn                       -78.1167
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.55488
Extras/EpisodeRewardMean                   -49.0658
LinearFeatureBaseline/ExplainedVariance      0.938823
PolicyExecTime                               0.224838
ProcessExecTime                              0.0312054
TotalEnvSteps                            71852
policy/Entropy                               1.37821
policy/KL                                    0.00657617
policy/KLBefore                              0
policy/LossAfter                            -0.0202055
policy/LossBefore                            4.47624e-09
policy/Perplexity                            3.9678
policy/dLoss                                 0.0202055
---------------------------------------  ---------------
2021-06-04 13:48:47 | [train_policy] epoch #71 | Obtaining samples for iteration 71...
2021-06-04 13:48:48 | [train_policy] epoch #71 | Logging diagnostics...
2021-06-04 13:48:48 | [train_policy] epoch #71 | Optimizing policy...
2021-06-04 13:48:48 | [train_policy] epoch #71 | Computing loss before
2021-06-04 13:48:48 | [train_policy] epoch #71 | Computing KL before
2021-06-04 13:48:48 | [train_policy] epoch #71 | Optimizing
2021-06-04 13:48:48 | [train_policy] epoch #71 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:48 | [train_policy] epoch #71 | computing loss before
2021-06-04 13:48:48 | [train_policy] epoch #71 | computing gradient
2021-06-04 13:48:48 | [train_policy] epoch #71 | gradient computed
2021-06-04 13:48:48 | [train_policy] epoch #71 | computing descent direction
2021-06-04 13:48:48 | [train_policy] epoch #71 | descent direction computed
2021-06-04 13:48:48 | [train_policy] epoch #71 | backtrack iters: 1
2021-06-04 13:48:48 | [train_policy] epoch #71 | optimization finished
2021-06-04 13:48:48 | [train_policy] epoch #71 | Computing KL after
2021-06-04 13:48:48 | [train_policy] epoch #71 | Computing loss after
2021-06-04 13:48:48 | [train_policy] epoch #71 | Fitting baseline...
2021-06-04 13:48:48 | [train_policy] epoch #71 | Saving snapshot...
2021-06-04 13:48:48 | [train_policy] epoch #71 | Saved
2021-06-04 13:48:48 | [train_policy] epoch #71 | Time 59.96 s
2021-06-04 13:48:48 | [train_policy] epoch #71 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                  0.284682
Evaluation/AverageDiscountedReturn         -49.7286
Evaluation/AverageReturn                   -49.7286
Evaluation/CompletionRate                    0
Evaluation/Iteration                        71
Evaluation/MaxReturn                       -38.1792
Evaluation/MinReturn                       -68.6388
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.93432
Extras/EpisodeRewardMean                   -49.2601
LinearFeatureBaseline/ExplainedVariance      0.940198
PolicyExecTime                               0.208498
ProcessExecTime                              0.0312481
TotalEnvSteps                            72864
policy/Entropy                               1.35795
policy/KL                                    0.00661616
policy/KLBefore                              0
policy/LossAfter                            -0.0211843
policy/LossBefore                           -5.18301e-09
policy/Perplexity                            3.88821
policy/dLoss                                 0.0211843
---------------------------------------  ---------------
2021-06-04 13:48:48 | [train_policy] epoch #72 | Obtaining samples for iteration 72...
2021-06-04 13:48:48 | [train_policy] epoch #72 | Logging diagnostics...
2021-06-04 13:48:48 | [train_policy] epoch #72 | Optimizing policy...
2021-06-04 13:48:48 | [train_policy] epoch #72 | Computing loss before
2021-06-04 13:48:48 | [train_policy] epoch #72 | Computing KL before
2021-06-04 13:48:48 | [train_policy] epoch #72 | Optimizing
2021-06-04 13:48:48 | [train_policy] epoch #72 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:48 | [train_policy] epoch #72 | computing loss before
2021-06-04 13:48:48 | [train_policy] epoch #72 | computing gradient
2021-06-04 13:48:48 | [train_policy] epoch #72 | gradient computed
2021-06-04 13:48:48 | [train_policy] epoch #72 | computing descent direction
2021-06-04 13:48:48 | [train_policy] epoch #72 | descent direction computed
2021-06-04 13:48:48 | [train_policy] epoch #72 | backtrack iters: 1
2021-06-04 13:48:48 | [train_policy] epoch #72 | optimization finished
2021-06-04 13:48:48 | [train_policy] epoch #72 | Computing KL after
2021-06-04 13:48:48 | [train_policy] epoch #72 | Computing loss after
2021-06-04 13:48:49 | [train_policy] epoch #72 | Fitting baseline...
2021-06-04 13:48:49 | [train_policy] epoch #72 | Saving snapshot...
2021-06-04 13:48:49 | [train_policy] epoch #72 | Saved
2021-06-04 13:48:49 | [train_policy] epoch #72 | Time 60.75 s
2021-06-04 13:48:49 | [train_policy] epoch #72 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.2851
Evaluation/AverageDiscountedReturn         -71.6838
Evaluation/AverageReturn                   -71.6838
Evaluation/CompletionRate                    0
Evaluation/Iteration                        72
Evaluation/MaxReturn                       -37.8826
Evaluation/MinReturn                     -2062.81
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.843
Extras/EpisodeRewardMean                   -69.7126
LinearFeatureBaseline/ExplainedVariance      0.0113047
PolicyExecTime                               0.230635
ProcessExecTime                              0.0312166
TotalEnvSteps                            73876
policy/Entropy                               1.34605
policy/KL                                    0.00633825
policy/KLBefore                              0
policy/LossAfter                            -0.0178267
policy/LossBefore                            2.35591e-09
policy/Perplexity                            3.84221
policy/dLoss                                 0.0178267
---------------------------------------  ---------------
2021-06-04 13:48:49 | [train_policy] epoch #73 | Obtaining samples for iteration 73...
2021-06-04 13:48:49 | [train_policy] epoch #73 | Logging diagnostics...
2021-06-04 13:48:49 | [train_policy] epoch #73 | Optimizing policy...
2021-06-04 13:48:49 | [train_policy] epoch #73 | Computing loss before
2021-06-04 13:48:49 | [train_policy] epoch #73 | Computing KL before
2021-06-04 13:48:49 | [train_policy] epoch #73 | Optimizing
2021-06-04 13:48:49 | [train_policy] epoch #73 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:49 | [train_policy] epoch #73 | computing loss before
2021-06-04 13:48:49 | [train_policy] epoch #73 | computing gradient
2021-06-04 13:48:49 | [train_policy] epoch #73 | gradient computed
2021-06-04 13:48:49 | [train_policy] epoch #73 | computing descent direction
2021-06-04 13:48:49 | [train_policy] epoch #73 | descent direction computed
2021-06-04 13:48:49 | [train_policy] epoch #73 | backtrack iters: 1
2021-06-04 13:48:49 | [train_policy] epoch #73 | optimization finished
2021-06-04 13:48:49 | [train_policy] epoch #73 | Computing KL after
2021-06-04 13:48:49 | [train_policy] epoch #73 | Computing loss after
2021-06-04 13:48:49 | [train_policy] epoch #73 | Fitting baseline...
2021-06-04 13:48:49 | [train_policy] epoch #73 | Saving snapshot...
2021-06-04 13:48:49 | [train_policy] epoch #73 | Saved
2021-06-04 13:48:49 | [train_policy] epoch #73 | Time 61.56 s
2021-06-04 13:48:49 | [train_policy] epoch #73 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285759
Evaluation/AverageDiscountedReturn         -49.3651
Evaluation/AverageReturn                   -49.3651
Evaluation/CompletionRate                    0
Evaluation/Iteration                        73
Evaluation/MaxReturn                       -39.0232
Evaluation/MinReturn                       -83.4032
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.89602
Extras/EpisodeRewardMean                   -49.3703
LinearFeatureBaseline/ExplainedVariance    -13.9946
PolicyExecTime                               0.23547
ProcessExecTime                              0.0311155
TotalEnvSteps                            74888
policy/Entropy                               1.35504
policy/KL                                    0.00732592
policy/KLBefore                              0
policy/LossAfter                            -0.0203218
policy/LossBefore                            3.62811e-08
policy/Perplexity                            3.87691
policy/dLoss                                 0.0203219
---------------------------------------  ---------------
2021-06-04 13:48:49 | [train_policy] epoch #74 | Obtaining samples for iteration 74...
2021-06-04 13:48:50 | [train_policy] epoch #74 | Logging diagnostics...
2021-06-04 13:48:50 | [train_policy] epoch #74 | Optimizing policy...
2021-06-04 13:48:50 | [train_policy] epoch #74 | Computing loss before
2021-06-04 13:48:50 | [train_policy] epoch #74 | Computing KL before
2021-06-04 13:48:50 | [train_policy] epoch #74 | Optimizing
2021-06-04 13:48:50 | [train_policy] epoch #74 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:50 | [train_policy] epoch #74 | computing loss before
2021-06-04 13:48:50 | [train_policy] epoch #74 | computing gradient
2021-06-04 13:48:50 | [train_policy] epoch #74 | gradient computed
2021-06-04 13:48:50 | [train_policy] epoch #74 | computing descent direction
2021-06-04 13:48:50 | [train_policy] epoch #74 | descent direction computed
2021-06-04 13:48:50 | [train_policy] epoch #74 | backtrack iters: 1
2021-06-04 13:48:50 | [train_policy] epoch #74 | optimization finished
2021-06-04 13:48:50 | [train_policy] epoch #74 | Computing KL after
2021-06-04 13:48:50 | [train_policy] epoch #74 | Computing loss after
2021-06-04 13:48:50 | [train_policy] epoch #74 | Fitting baseline...
2021-06-04 13:48:50 | [train_policy] epoch #74 | Saving snapshot...
2021-06-04 13:48:50 | [train_policy] epoch #74 | Saved
2021-06-04 13:48:50 | [train_policy] epoch #74 | Time 62.34 s
2021-06-04 13:48:50 | [train_policy] epoch #74 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286892
Evaluation/AverageDiscountedReturn         -48.071
Evaluation/AverageReturn                   -48.071
Evaluation/CompletionRate                    0
Evaluation/Iteration                        74
Evaluation/MaxReturn                       -36.3175
Evaluation/MinReturn                       -66.7551
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.81514
Extras/EpisodeRewardMean                   -47.925
LinearFeatureBaseline/ExplainedVariance      0.959824
PolicyExecTime                               0.221265
ProcessExecTime                              0.0313656
TotalEnvSteps                            75900
policy/Entropy                               1.3193
policy/KL                                    0.00688277
policy/KLBefore                              0
policy/LossAfter                            -0.0187299
policy/LossBefore                            8.95248e-09
policy/Perplexity                            3.74081
policy/dLoss                                 0.0187299
---------------------------------------  ---------------
2021-06-04 13:48:50 | [train_policy] epoch #75 | Obtaining samples for iteration 75...
2021-06-04 13:48:51 | [train_policy] epoch #75 | Logging diagnostics...
2021-06-04 13:48:51 | [train_policy] epoch #75 | Optimizing policy...
2021-06-04 13:48:51 | [train_policy] epoch #75 | Computing loss before
2021-06-04 13:48:51 | [train_policy] epoch #75 | Computing KL before
2021-06-04 13:48:51 | [train_policy] epoch #75 | Optimizing
2021-06-04 13:48:51 | [train_policy] epoch #75 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:51 | [train_policy] epoch #75 | computing loss before
2021-06-04 13:48:51 | [train_policy] epoch #75 | computing gradient
2021-06-04 13:48:51 | [train_policy] epoch #75 | gradient computed
2021-06-04 13:48:51 | [train_policy] epoch #75 | computing descent direction
2021-06-04 13:48:51 | [train_policy] epoch #75 | descent direction computed
2021-06-04 13:48:51 | [train_policy] epoch #75 | backtrack iters: 0
2021-06-04 13:48:51 | [train_policy] epoch #75 | optimization finished
2021-06-04 13:48:51 | [train_policy] epoch #75 | Computing KL after
2021-06-04 13:48:51 | [train_policy] epoch #75 | Computing loss after
2021-06-04 13:48:51 | [train_policy] epoch #75 | Fitting baseline...
2021-06-04 13:48:51 | [train_policy] epoch #75 | Saving snapshot...
2021-06-04 13:48:51 | [train_policy] epoch #75 | Saved
2021-06-04 13:48:51 | [train_policy] epoch #75 | Time 63.14 s
2021-06-04 13:48:51 | [train_policy] epoch #75 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.28858
Evaluation/AverageDiscountedReturn         -50.647
Evaluation/AverageReturn                   -50.647
Evaluation/CompletionRate                    0
Evaluation/Iteration                        75
Evaluation/MaxReturn                       -39.1092
Evaluation/MinReturn                       -76.1205
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.05378
Extras/EpisodeRewardMean                   -50.6196
LinearFeatureBaseline/ExplainedVariance      0.921799
PolicyExecTime                               0.228852
ProcessExecTime                              0.0315833
TotalEnvSteps                            76912
policy/Entropy                               1.30494
policy/KL                                    0.00945602
policy/KLBefore                              0
policy/LossAfter                            -0.024185
policy/LossBefore                           -7.06774e-09
policy/Perplexity                            3.68748
policy/dLoss                                 0.0241849
---------------------------------------  ---------------
2021-06-04 13:48:51 | [train_policy] epoch #76 | Obtaining samples for iteration 76...
2021-06-04 13:48:52 | [train_policy] epoch #76 | Logging diagnostics...
2021-06-04 13:48:52 | [train_policy] epoch #76 | Optimizing policy...
2021-06-04 13:48:52 | [train_policy] epoch #76 | Computing loss before
2021-06-04 13:48:52 | [train_policy] epoch #76 | Computing KL before
2021-06-04 13:48:52 | [train_policy] epoch #76 | Optimizing
2021-06-04 13:48:52 | [train_policy] epoch #76 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:52 | [train_policy] epoch #76 | computing loss before
2021-06-04 13:48:52 | [train_policy] epoch #76 | computing gradient
2021-06-04 13:48:52 | [train_policy] epoch #76 | gradient computed
2021-06-04 13:48:52 | [train_policy] epoch #76 | computing descent direction
2021-06-04 13:48:52 | [train_policy] epoch #76 | descent direction computed
2021-06-04 13:48:52 | [train_policy] epoch #76 | backtrack iters: 1
2021-06-04 13:48:52 | [train_policy] epoch #76 | optimization finished
2021-06-04 13:48:52 | [train_policy] epoch #76 | Computing KL after
2021-06-04 13:48:52 | [train_policy] epoch #76 | Computing loss after
2021-06-04 13:48:52 | [train_policy] epoch #76 | Fitting baseline...
2021-06-04 13:48:52 | [train_policy] epoch #76 | Saving snapshot...
2021-06-04 13:48:52 | [train_policy] epoch #76 | Saved
2021-06-04 13:48:52 | [train_policy] epoch #76 | Time 63.94 s
2021-06-04 13:48:52 | [train_policy] epoch #76 | EpochTime 0.78 s
---------------------------------------  --------------
EnvExecTime                                  0.286199
Evaluation/AverageDiscountedReturn         -48.2329
Evaluation/AverageReturn                   -48.2329
Evaluation/CompletionRate                    0
Evaluation/Iteration                        76
Evaluation/MaxReturn                       -37.6933
Evaluation/MinReturn                       -63.7557
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.12086
Extras/EpisodeRewardMean                   -48.2831
LinearFeatureBaseline/ExplainedVariance      0.938041
PolicyExecTime                               0.228885
ProcessExecTime                              0.0315039
TotalEnvSteps                            77924
policy/Entropy                               1.2567
policy/KL                                    0.0065971
policy/KLBefore                              0
policy/LossAfter                            -0.0190231
policy/LossBefore                           -5.6542e-09
policy/Perplexity                            3.51381
policy/dLoss                                 0.0190231
---------------------------------------  --------------
2021-06-04 13:48:52 | [train_policy] epoch #77 | Obtaining samples for iteration 77...
2021-06-04 13:48:52 | [train_policy] epoch #77 | Logging diagnostics...
2021-06-04 13:48:52 | [train_policy] epoch #77 | Optimizing policy...
2021-06-04 13:48:52 | [train_policy] epoch #77 | Computing loss before
2021-06-04 13:48:52 | [train_policy] epoch #77 | Computing KL before
2021-06-04 13:48:52 | [train_policy] epoch #77 | Optimizing
2021-06-04 13:48:52 | [train_policy] epoch #77 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:52 | [train_policy] epoch #77 | computing loss before
2021-06-04 13:48:52 | [train_policy] epoch #77 | computing gradient
2021-06-04 13:48:52 | [train_policy] epoch #77 | gradient computed
2021-06-04 13:48:52 | [train_policy] epoch #77 | computing descent direction
2021-06-04 13:48:52 | [train_policy] epoch #77 | descent direction computed
2021-06-04 13:48:52 | [train_policy] epoch #77 | backtrack iters: 0
2021-06-04 13:48:52 | [train_policy] epoch #77 | optimization finished
2021-06-04 13:48:52 | [train_policy] epoch #77 | Computing KL after
2021-06-04 13:48:52 | [train_policy] epoch #77 | Computing loss after
2021-06-04 13:48:52 | [train_policy] epoch #77 | Fitting baseline...
2021-06-04 13:48:52 | [train_policy] epoch #77 | Saving snapshot...
2021-06-04 13:48:53 | [train_policy] epoch #77 | Saved
2021-06-04 13:48:53 | [train_policy] epoch #77 | Time 64.72 s
2021-06-04 13:48:53 | [train_policy] epoch #77 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286347
Evaluation/AverageDiscountedReturn         -48.9388
Evaluation/AverageReturn                   -48.9388
Evaluation/CompletionRate                    0
Evaluation/Iteration                        77
Evaluation/MaxReturn                       -37.4582
Evaluation/MinReturn                       -80.5037
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.90484
Extras/EpisodeRewardMean                   -48.8692
LinearFeatureBaseline/ExplainedVariance      0.935479
PolicyExecTime                               0.229071
ProcessExecTime                              0.0313163
TotalEnvSteps                            78936
policy/Entropy                               1.25279
policy/KL                                    0.00900088
policy/KLBefore                              0
policy/LossAfter                            -0.0253507
policy/LossBefore                            9.89484e-09
policy/Perplexity                            3.50009
policy/dLoss                                 0.0253507
---------------------------------------  ---------------
2021-06-04 13:48:53 | [train_policy] epoch #78 | Obtaining samples for iteration 78...
2021-06-04 13:48:53 | [train_policy] epoch #78 | Logging diagnostics...
2021-06-04 13:48:53 | [train_policy] epoch #78 | Optimizing policy...
2021-06-04 13:48:53 | [train_policy] epoch #78 | Computing loss before
2021-06-04 13:48:53 | [train_policy] epoch #78 | Computing KL before
2021-06-04 13:48:53 | [train_policy] epoch #78 | Optimizing
2021-06-04 13:48:53 | [train_policy] epoch #78 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:53 | [train_policy] epoch #78 | computing loss before
2021-06-04 13:48:53 | [train_policy] epoch #78 | computing gradient
2021-06-04 13:48:53 | [train_policy] epoch #78 | gradient computed
2021-06-04 13:48:53 | [train_policy] epoch #78 | computing descent direction
2021-06-04 13:48:53 | [train_policy] epoch #78 | descent direction computed
2021-06-04 13:48:53 | [train_policy] epoch #78 | backtrack iters: 0
2021-06-04 13:48:53 | [train_policy] epoch #78 | optimization finished
2021-06-04 13:48:53 | [train_policy] epoch #78 | Computing KL after
2021-06-04 13:48:53 | [train_policy] epoch #78 | Computing loss after
2021-06-04 13:48:53 | [train_policy] epoch #78 | Fitting baseline...
2021-06-04 13:48:53 | [train_policy] epoch #78 | Saving snapshot...
2021-06-04 13:48:53 | [train_policy] epoch #78 | Saved
2021-06-04 13:48:53 | [train_policy] epoch #78 | Time 65.54 s
2021-06-04 13:48:53 | [train_policy] epoch #78 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.285387
Evaluation/AverageDiscountedReturn         -48.8277
Evaluation/AverageReturn                   -48.8277
Evaluation/CompletionRate                    0
Evaluation/Iteration                        78
Evaluation/MaxReturn                       -37.4141
Evaluation/MinReturn                       -90.9943
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.17807
Extras/EpisodeRewardMean                   -48.798
LinearFeatureBaseline/ExplainedVariance      0.872543
PolicyExecTime                               0.242864
ProcessExecTime                              0.0317354
TotalEnvSteps                            79948
policy/Entropy                               1.2618
policy/KL                                    0.00865924
policy/KLBefore                              0
policy/LossAfter                            -0.0188579
policy/LossBefore                            1.88473e-09
policy/Perplexity                            3.53176
policy/dLoss                                 0.0188579
---------------------------------------  ---------------
2021-06-04 13:48:53 | [train_policy] epoch #79 | Obtaining samples for iteration 79...
2021-06-04 13:48:54 | [train_policy] epoch #79 | Logging diagnostics...
2021-06-04 13:48:54 | [train_policy] epoch #79 | Optimizing policy...
2021-06-04 13:48:54 | [train_policy] epoch #79 | Computing loss before
2021-06-04 13:48:54 | [train_policy] epoch #79 | Computing KL before
2021-06-04 13:48:54 | [train_policy] epoch #79 | Optimizing
2021-06-04 13:48:54 | [train_policy] epoch #79 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:54 | [train_policy] epoch #79 | computing loss before
2021-06-04 13:48:54 | [train_policy] epoch #79 | computing gradient
2021-06-04 13:48:54 | [train_policy] epoch #79 | gradient computed
2021-06-04 13:48:54 | [train_policy] epoch #79 | computing descent direction
2021-06-04 13:48:54 | [train_policy] epoch #79 | descent direction computed
2021-06-04 13:48:54 | [train_policy] epoch #79 | backtrack iters: 1
2021-06-04 13:48:54 | [train_policy] epoch #79 | optimization finished
2021-06-04 13:48:54 | [train_policy] epoch #79 | Computing KL after
2021-06-04 13:48:54 | [train_policy] epoch #79 | Computing loss after
2021-06-04 13:48:54 | [train_policy] epoch #79 | Fitting baseline...
2021-06-04 13:48:54 | [train_policy] epoch #79 | Saving snapshot...
2021-06-04 13:48:54 | [train_policy] epoch #79 | Saved
2021-06-04 13:48:54 | [train_policy] epoch #79 | Time 66.34 s
2021-06-04 13:48:54 | [train_policy] epoch #79 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285498
Evaluation/AverageDiscountedReturn         -69.1847
Evaluation/AverageReturn                   -69.1847
Evaluation/CompletionRate                    0
Evaluation/Iteration                        79
Evaluation/MaxReturn                       -37.7455
Evaluation/MinReturn                     -2063.21
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.121
Extras/EpisodeRewardMean                   -67.7377
LinearFeatureBaseline/ExplainedVariance      0.0173458
PolicyExecTime                               0.226892
ProcessExecTime                              0.031167
TotalEnvSteps                            80960
policy/Entropy                               1.27762
policy/KL                                    0.00695495
policy/KLBefore                              0
policy/LossAfter                            -0.0217299
policy/LossBefore                           -1.08372e-08
policy/Perplexity                            3.5881
policy/dLoss                                 0.0217298
---------------------------------------  ---------------
2021-06-04 13:48:54 | [train_policy] epoch #80 | Obtaining samples for iteration 80...
2021-06-04 13:48:55 | [train_policy] epoch #80 | Logging diagnostics...
2021-06-04 13:48:55 | [train_policy] epoch #80 | Optimizing policy...
2021-06-04 13:48:55 | [train_policy] epoch #80 | Computing loss before
2021-06-04 13:48:55 | [train_policy] epoch #80 | Computing KL before
2021-06-04 13:48:55 | [train_policy] epoch #80 | Optimizing
2021-06-04 13:48:55 | [train_policy] epoch #80 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:55 | [train_policy] epoch #80 | computing loss before
2021-06-04 13:48:55 | [train_policy] epoch #80 | computing gradient
2021-06-04 13:48:55 | [train_policy] epoch #80 | gradient computed
2021-06-04 13:48:55 | [train_policy] epoch #80 | computing descent direction
2021-06-04 13:48:55 | [train_policy] epoch #80 | descent direction computed
2021-06-04 13:48:55 | [train_policy] epoch #80 | backtrack iters: 1
2021-06-04 13:48:55 | [train_policy] epoch #80 | optimization finished
2021-06-04 13:48:55 | [train_policy] epoch #80 | Computing KL after
2021-06-04 13:48:55 | [train_policy] epoch #80 | Computing loss after
2021-06-04 13:48:55 | [train_policy] epoch #80 | Fitting baseline...
2021-06-04 13:48:55 | [train_policy] epoch #80 | Saving snapshot...
2021-06-04 13:48:55 | [train_policy] epoch #80 | Saved
2021-06-04 13:48:55 | [train_policy] epoch #80 | Time 67.14 s
2021-06-04 13:48:55 | [train_policy] epoch #80 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.287019
Evaluation/AverageDiscountedReturn         -49.132
Evaluation/AverageReturn                   -49.132
Evaluation/CompletionRate                    0
Evaluation/Iteration                        80
Evaluation/MaxReturn                       -39.8411
Evaluation/MinReturn                       -70.161
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.81621
Extras/EpisodeRewardMean                   -49.0309
LinearFeatureBaseline/ExplainedVariance    -36.0624
PolicyExecTime                               0.222265
ProcessExecTime                              0.0313039
TotalEnvSteps                            81972
policy/Entropy                               1.27263
policy/KL                                    0.00681887
policy/KLBefore                              0
policy/LossAfter                            -0.0248747
policy/LossBefore                            1.18974e-08
policy/Perplexity                            3.57023
policy/dLoss                                 0.0248747
---------------------------------------  ---------------
2021-06-04 13:48:55 | [train_policy] epoch #81 | Obtaining samples for iteration 81...
2021-06-04 13:48:56 | [train_policy] epoch #81 | Logging diagnostics...
2021-06-04 13:48:56 | [train_policy] epoch #81 | Optimizing policy...
2021-06-04 13:48:56 | [train_policy] epoch #81 | Computing loss before
2021-06-04 13:48:56 | [train_policy] epoch #81 | Computing KL before
2021-06-04 13:48:56 | [train_policy] epoch #81 | Optimizing
2021-06-04 13:48:56 | [train_policy] epoch #81 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:56 | [train_policy] epoch #81 | computing loss before
2021-06-04 13:48:56 | [train_policy] epoch #81 | computing gradient
2021-06-04 13:48:56 | [train_policy] epoch #81 | gradient computed
2021-06-04 13:48:56 | [train_policy] epoch #81 | computing descent direction
2021-06-04 13:48:56 | [train_policy] epoch #81 | descent direction computed
2021-06-04 13:48:56 | [train_policy] epoch #81 | backtrack iters: 1
2021-06-04 13:48:56 | [train_policy] epoch #81 | optimization finished
2021-06-04 13:48:56 | [train_policy] epoch #81 | Computing KL after
2021-06-04 13:48:56 | [train_policy] epoch #81 | Computing loss after
2021-06-04 13:48:56 | [train_policy] epoch #81 | Fitting baseline...
2021-06-04 13:48:56 | [train_policy] epoch #81 | Saving snapshot...
2021-06-04 13:48:56 | [train_policy] epoch #81 | Saved
2021-06-04 13:48:56 | [train_policy] epoch #81 | Time 67.96 s
2021-06-04 13:48:56 | [train_policy] epoch #81 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.288115
Evaluation/AverageDiscountedReturn         -47.6815
Evaluation/AverageReturn                   -47.6815
Evaluation/CompletionRate                    0
Evaluation/Iteration                        81
Evaluation/MaxReturn                       -34.4884
Evaluation/MinReturn                       -83.3448
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.55275
Extras/EpisodeRewardMean                   -47.7708
LinearFeatureBaseline/ExplainedVariance      0.919692
PolicyExecTime                               0.238433
ProcessExecTime                              0.0316606
TotalEnvSteps                            82984
policy/Entropy                               1.23086
policy/KL                                    0.00745686
policy/KLBefore                              0
policy/LossAfter                            -0.022247
policy/LossBefore                           -2.02609e-08
policy/Perplexity                            3.42419
policy/dLoss                                 0.022247
---------------------------------------  ---------------
2021-06-04 13:48:56 | [train_policy] epoch #82 | Obtaining samples for iteration 82...
2021-06-04 13:48:56 | [train_policy] epoch #82 | Logging diagnostics...
2021-06-04 13:48:56 | [train_policy] epoch #82 | Optimizing policy...
2021-06-04 13:48:56 | [train_policy] epoch #82 | Computing loss before
2021-06-04 13:48:56 | [train_policy] epoch #82 | Computing KL before
2021-06-04 13:48:56 | [train_policy] epoch #82 | Optimizing
2021-06-04 13:48:56 | [train_policy] epoch #82 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:56 | [train_policy] epoch #82 | computing loss before
2021-06-04 13:48:56 | [train_policy] epoch #82 | computing gradient
2021-06-04 13:48:56 | [train_policy] epoch #82 | gradient computed
2021-06-04 13:48:56 | [train_policy] epoch #82 | computing descent direction
2021-06-04 13:48:56 | [train_policy] epoch #82 | descent direction computed
2021-06-04 13:48:56 | [train_policy] epoch #82 | backtrack iters: 1
2021-06-04 13:48:56 | [train_policy] epoch #82 | optimization finished
2021-06-04 13:48:56 | [train_policy] epoch #82 | Computing KL after
2021-06-04 13:48:56 | [train_policy] epoch #82 | Computing loss after
2021-06-04 13:48:56 | [train_policy] epoch #82 | Fitting baseline...
2021-06-04 13:48:57 | [train_policy] epoch #82 | Saving snapshot...
2021-06-04 13:48:57 | [train_policy] epoch #82 | Saved
2021-06-04 13:48:57 | [train_policy] epoch #82 | Time 68.75 s
2021-06-04 13:48:57 | [train_policy] epoch #82 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.284344
Evaluation/AverageDiscountedReturn         -69.9151
Evaluation/AverageReturn                   -69.9151
Evaluation/CompletionRate                    0
Evaluation/Iteration                        82
Evaluation/MaxReturn                       -37.9531
Evaluation/MinReturn                     -2061.88
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.915
Extras/EpisodeRewardMean                   -68.0115
LinearFeatureBaseline/ExplainedVariance      0.0119242
PolicyExecTime                               0.221309
ProcessExecTime                              0.0311196
TotalEnvSteps                            83996
policy/Entropy                               1.24129
policy/KL                                    0.0067582
policy/KLBefore                              0
policy/LossAfter                            -0.0178228
policy/LossBefore                           -1.22508e-08
policy/Perplexity                            3.46009
policy/dLoss                                 0.0178228
---------------------------------------  ---------------
2021-06-04 13:48:57 | [train_policy] epoch #83 | Obtaining samples for iteration 83...
2021-06-04 13:48:57 | [train_policy] epoch #83 | Logging diagnostics...
2021-06-04 13:48:57 | [train_policy] epoch #83 | Optimizing policy...
2021-06-04 13:48:57 | [train_policy] epoch #83 | Computing loss before
2021-06-04 13:48:57 | [train_policy] epoch #83 | Computing KL before
2021-06-04 13:48:57 | [train_policy] epoch #83 | Optimizing
2021-06-04 13:48:57 | [train_policy] epoch #83 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:57 | [train_policy] epoch #83 | computing loss before
2021-06-04 13:48:57 | [train_policy] epoch #83 | computing gradient
2021-06-04 13:48:57 | [train_policy] epoch #83 | gradient computed
2021-06-04 13:48:57 | [train_policy] epoch #83 | computing descent direction
2021-06-04 13:48:57 | [train_policy] epoch #83 | descent direction computed
2021-06-04 13:48:57 | [train_policy] epoch #83 | backtrack iters: 0
2021-06-04 13:48:57 | [train_policy] epoch #83 | optimization finished
2021-06-04 13:48:57 | [train_policy] epoch #83 | Computing KL after
2021-06-04 13:48:57 | [train_policy] epoch #83 | Computing loss after
2021-06-04 13:48:57 | [train_policy] epoch #83 | Fitting baseline...
2021-06-04 13:48:57 | [train_policy] epoch #83 | Saving snapshot...
2021-06-04 13:48:57 | [train_policy] epoch #83 | Saved
2021-06-04 13:48:57 | [train_policy] epoch #83 | Time 69.57 s
2021-06-04 13:48:57 | [train_policy] epoch #83 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                  0.288732
Evaluation/AverageDiscountedReturn         -47.3695
Evaluation/AverageReturn                   -47.3695
Evaluation/CompletionRate                    0
Evaluation/Iteration                        83
Evaluation/MaxReturn                       -35.0826
Evaluation/MinReturn                       -66.1355
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.24952
Extras/EpisodeRewardMean                   -47.7836
LinearFeatureBaseline/ExplainedVariance    -23.4146
PolicyExecTime                               0.23953
ProcessExecTime                              0.0315948
TotalEnvSteps                            85008
policy/Entropy                               1.20062
policy/KL                                    0.00925045
policy/KLBefore                              0
policy/LossAfter                            -0.0158442
policy/LossBefore                           -2.80354e-08
policy/Perplexity                            3.32218
policy/dLoss                                 0.0158441
---------------------------------------  ---------------
2021-06-04 13:48:57 | [train_policy] epoch #84 | Obtaining samples for iteration 84...
2021-06-04 13:48:58 | [train_policy] epoch #84 | Logging diagnostics...
2021-06-04 13:48:58 | [train_policy] epoch #84 | Optimizing policy...
2021-06-04 13:48:58 | [train_policy] epoch #84 | Computing loss before
2021-06-04 13:48:58 | [train_policy] epoch #84 | Computing KL before
2021-06-04 13:48:58 | [train_policy] epoch #84 | Optimizing
2021-06-04 13:48:58 | [train_policy] epoch #84 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:58 | [train_policy] epoch #84 | computing loss before
2021-06-04 13:48:58 | [train_policy] epoch #84 | computing gradient
2021-06-04 13:48:58 | [train_policy] epoch #84 | gradient computed
2021-06-04 13:48:58 | [train_policy] epoch #84 | computing descent direction
2021-06-04 13:48:58 | [train_policy] epoch #84 | descent direction computed
2021-06-04 13:48:58 | [train_policy] epoch #84 | backtrack iters: 1
2021-06-04 13:48:58 | [train_policy] epoch #84 | optimization finished
2021-06-04 13:48:58 | [train_policy] epoch #84 | Computing KL after
2021-06-04 13:48:58 | [train_policy] epoch #84 | Computing loss after
2021-06-04 13:48:58 | [train_policy] epoch #84 | Fitting baseline...
2021-06-04 13:48:58 | [train_policy] epoch #84 | Saving snapshot...
2021-06-04 13:48:58 | [train_policy] epoch #84 | Saved
2021-06-04 13:48:58 | [train_policy] epoch #84 | Time 70.35 s
2021-06-04 13:48:58 | [train_policy] epoch #84 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.286608
Evaluation/AverageDiscountedReturn         -69.33
Evaluation/AverageReturn                   -69.33
Evaluation/CompletionRate                    0
Evaluation/Iteration                        84
Evaluation/MaxReturn                       -37.0301
Evaluation/MinReturn                     -2060.64
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.937
Extras/EpisodeRewardMean                   -67.7944
LinearFeatureBaseline/ExplainedVariance      0.0106149
PolicyExecTime                               0.206956
ProcessExecTime                              0.0313609
TotalEnvSteps                            86020
policy/Entropy                               1.17408
policy/KL                                    0.00636246
policy/KLBefore                              0
policy/LossAfter                            -0.0125822
policy/LossBefore                            1.69626e-08
policy/Perplexity                            3.23517
policy/dLoss                                 0.0125822
---------------------------------------  ---------------
2021-06-04 13:48:58 | [train_policy] epoch #85 | Obtaining samples for iteration 85...
2021-06-04 13:48:59 | [train_policy] epoch #85 | Logging diagnostics...
2021-06-04 13:48:59 | [train_policy] epoch #85 | Optimizing policy...
2021-06-04 13:48:59 | [train_policy] epoch #85 | Computing loss before
2021-06-04 13:48:59 | [train_policy] epoch #85 | Computing KL before
2021-06-04 13:48:59 | [train_policy] epoch #85 | Optimizing
2021-06-04 13:48:59 | [train_policy] epoch #85 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:48:59 | [train_policy] epoch #85 | computing loss before
2021-06-04 13:48:59 | [train_policy] epoch #85 | computing gradient
2021-06-04 13:48:59 | [train_policy] epoch #85 | gradient computed
2021-06-04 13:48:59 | [train_policy] epoch #85 | computing descent direction
2021-06-04 13:48:59 | [train_policy] epoch #85 | descent direction computed
2021-06-04 13:48:59 | [train_policy] epoch #85 | backtrack iters: 1
2021-06-04 13:48:59 | [train_policy] epoch #85 | optimization finished
2021-06-04 13:48:59 | [train_policy] epoch #85 | Computing KL after
2021-06-04 13:48:59 | [train_policy] epoch #85 | Computing loss after
2021-06-04 13:48:59 | [train_policy] epoch #85 | Fitting baseline...
2021-06-04 13:48:59 | [train_policy] epoch #85 | Saving snapshot...
2021-06-04 13:48:59 | [train_policy] epoch #85 | Saved
2021-06-04 13:48:59 | [train_policy] epoch #85 | Time 71.14 s
2021-06-04 13:48:59 | [train_policy] epoch #85 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.286672
Evaluation/AverageDiscountedReturn         -69.2627
Evaluation/AverageReturn                   -69.2627
Evaluation/CompletionRate                    0
Evaluation/Iteration                        85
Evaluation/MaxReturn                       -36.6009
Evaluation/MinReturn                     -2053.79
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.112
Extras/EpisodeRewardMean                   -67.4421
LinearFeatureBaseline/ExplainedVariance      0.0178969
PolicyExecTime                               0.21843
ProcessExecTime                              0.0315163
TotalEnvSteps                            87032
policy/Entropy                               1.15966
policy/KL                                    0.00809843
policy/KLBefore                              0
policy/LossAfter                            -0.0254754
policy/LossBefore                            1.83761e-08
policy/Perplexity                            3.18885
policy/dLoss                                 0.0254754
---------------------------------------  ---------------
2021-06-04 13:48:59 | [train_policy] epoch #86 | Obtaining samples for iteration 86...
2021-06-04 13:49:00 | [train_policy] epoch #86 | Logging diagnostics...
2021-06-04 13:49:00 | [train_policy] epoch #86 | Optimizing policy...
2021-06-04 13:49:00 | [train_policy] epoch #86 | Computing loss before
2021-06-04 13:49:00 | [train_policy] epoch #86 | Computing KL before
2021-06-04 13:49:00 | [train_policy] epoch #86 | Optimizing
2021-06-04 13:49:00 | [train_policy] epoch #86 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:00 | [train_policy] epoch #86 | computing loss before
2021-06-04 13:49:00 | [train_policy] epoch #86 | computing gradient
2021-06-04 13:49:00 | [train_policy] epoch #86 | gradient computed
2021-06-04 13:49:00 | [train_policy] epoch #86 | computing descent direction
2021-06-04 13:49:00 | [train_policy] epoch #86 | descent direction computed
2021-06-04 13:49:00 | [train_policy] epoch #86 | backtrack iters: 0
2021-06-04 13:49:00 | [train_policy] epoch #86 | optimization finished
2021-06-04 13:49:00 | [train_policy] epoch #86 | Computing KL after
2021-06-04 13:49:00 | [train_policy] epoch #86 | Computing loss after
2021-06-04 13:49:00 | [train_policy] epoch #86 | Fitting baseline...
2021-06-04 13:49:00 | [train_policy] epoch #86 | Saving snapshot...
2021-06-04 13:49:00 | [train_policy] epoch #86 | Saved
2021-06-04 13:49:00 | [train_policy] epoch #86 | Time 71.93 s
2021-06-04 13:49:00 | [train_policy] epoch #86 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285845
Evaluation/AverageDiscountedReturn         -48.126
Evaluation/AverageReturn                   -48.126
Evaluation/CompletionRate                    0
Evaluation/Iteration                        86
Evaluation/MaxReturn                       -36.4155
Evaluation/MinReturn                       -86.3143
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.86098
Extras/EpisodeRewardMean                   -48.2809
LinearFeatureBaseline/ExplainedVariance    -27.5661
PolicyExecTime                               0.227588
ProcessExecTime                              0.0312567
TotalEnvSteps                            88044
policy/Entropy                               1.18183
policy/KL                                    0.00973035
policy/KLBefore                              0
policy/LossAfter                            -0.0329929
policy/LossBefore                            7.30334e-09
policy/Perplexity                            3.26033
policy/dLoss                                 0.0329929
---------------------------------------  ---------------
2021-06-04 13:49:00 | [train_policy] epoch #87 | Obtaining samples for iteration 87...
2021-06-04 13:49:00 | [train_policy] epoch #87 | Logging diagnostics...
2021-06-04 13:49:00 | [train_policy] epoch #87 | Optimizing policy...
2021-06-04 13:49:00 | [train_policy] epoch #87 | Computing loss before
2021-06-04 13:49:00 | [train_policy] epoch #87 | Computing KL before
2021-06-04 13:49:00 | [train_policy] epoch #87 | Optimizing
2021-06-04 13:49:00 | [train_policy] epoch #87 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:00 | [train_policy] epoch #87 | computing loss before
2021-06-04 13:49:00 | [train_policy] epoch #87 | computing gradient
2021-06-04 13:49:00 | [train_policy] epoch #87 | gradient computed
2021-06-04 13:49:00 | [train_policy] epoch #87 | computing descent direction
2021-06-04 13:49:00 | [train_policy] epoch #87 | descent direction computed
2021-06-04 13:49:00 | [train_policy] epoch #87 | backtrack iters: 0
2021-06-04 13:49:00 | [train_policy] epoch #87 | optimization finished
2021-06-04 13:49:00 | [train_policy] epoch #87 | Computing KL after
2021-06-04 13:49:00 | [train_policy] epoch #87 | Computing loss after
2021-06-04 13:49:00 | [train_policy] epoch #87 | Fitting baseline...
2021-06-04 13:49:00 | [train_policy] epoch #87 | Saving snapshot...
2021-06-04 13:49:00 | [train_policy] epoch #87 | Saved
2021-06-04 13:49:00 | [train_policy] epoch #87 | Time 72.72 s
2021-06-04 13:49:00 | [train_policy] epoch #87 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                  0.288032
Evaluation/AverageDiscountedReturn         -47.7416
Evaluation/AverageReturn                   -47.7416
Evaluation/CompletionRate                    0
Evaluation/Iteration                        87
Evaluation/MaxReturn                       -37.0833
Evaluation/MinReturn                       -61.4966
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.33082
Extras/EpisodeRewardMean                   -47.7334
LinearFeatureBaseline/ExplainedVariance      0.947509
PolicyExecTime                               0.21802
ProcessExecTime                              0.0315223
TotalEnvSteps                            89056
policy/Entropy                               1.12273
policy/KL                                    0.00992271
policy/KLBefore                              0
policy/LossAfter                            -0.0205207
policy/LossBefore                           -2.35591e-10
policy/Perplexity                            3.07325
policy/dLoss                                 0.0205207
---------------------------------------  ---------------
2021-06-04 13:49:01 | [train_policy] epoch #88 | Obtaining samples for iteration 88...
2021-06-04 13:49:01 | [train_policy] epoch #88 | Logging diagnostics...
2021-06-04 13:49:01 | [train_policy] epoch #88 | Optimizing policy...
2021-06-04 13:49:01 | [train_policy] epoch #88 | Computing loss before
2021-06-04 13:49:01 | [train_policy] epoch #88 | Computing KL before
2021-06-04 13:49:01 | [train_policy] epoch #88 | Optimizing
2021-06-04 13:49:01 | [train_policy] epoch #88 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:01 | [train_policy] epoch #88 | computing loss before
2021-06-04 13:49:01 | [train_policy] epoch #88 | computing gradient
2021-06-04 13:49:01 | [train_policy] epoch #88 | gradient computed
2021-06-04 13:49:01 | [train_policy] epoch #88 | computing descent direction
2021-06-04 13:49:01 | [train_policy] epoch #88 | descent direction computed
2021-06-04 13:49:01 | [train_policy] epoch #88 | backtrack iters: 1
2021-06-04 13:49:01 | [train_policy] epoch #88 | optimization finished
2021-06-04 13:49:01 | [train_policy] epoch #88 | Computing KL after
2021-06-04 13:49:01 | [train_policy] epoch #88 | Computing loss after
2021-06-04 13:49:01 | [train_policy] epoch #88 | Fitting baseline...
2021-06-04 13:49:01 | [train_policy] epoch #88 | Saving snapshot...
2021-06-04 13:49:01 | [train_policy] epoch #88 | Saved
2021-06-04 13:49:01 | [train_policy] epoch #88 | Time 73.51 s
2021-06-04 13:49:01 | [train_policy] epoch #88 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.285387
Evaluation/AverageDiscountedReturn         -50.2919
Evaluation/AverageReturn                   -50.2919
Evaluation/CompletionRate                    0
Evaluation/Iteration                        88
Evaluation/MaxReturn                       -38.6094
Evaluation/MinReturn                      -218.789
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        19.9227
Extras/EpisodeRewardMean                   -50.4252
LinearFeatureBaseline/ExplainedVariance      0.367753
PolicyExecTime                               0.216801
ProcessExecTime                              0.0319452
TotalEnvSteps                            90068
policy/Entropy                               1.06094
policy/KL                                    0.00671496
policy/KLBefore                              0
policy/LossAfter                            -0.0147474
policy/LossBefore                           -4.82963e-09
policy/Perplexity                            2.88908
policy/dLoss                                 0.0147474
---------------------------------------  ---------------
2021-06-04 13:49:01 | [train_policy] epoch #89 | Obtaining samples for iteration 89...
2021-06-04 13:49:02 | [train_policy] epoch #89 | Logging diagnostics...
2021-06-04 13:49:02 | [train_policy] epoch #89 | Optimizing policy...
2021-06-04 13:49:02 | [train_policy] epoch #89 | Computing loss before
2021-06-04 13:49:02 | [train_policy] epoch #89 | Computing KL before
2021-06-04 13:49:02 | [train_policy] epoch #89 | Optimizing
2021-06-04 13:49:02 | [train_policy] epoch #89 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:02 | [train_policy] epoch #89 | computing loss before
2021-06-04 13:49:02 | [train_policy] epoch #89 | computing gradient
2021-06-04 13:49:02 | [train_policy] epoch #89 | gradient computed
2021-06-04 13:49:02 | [train_policy] epoch #89 | computing descent direction
2021-06-04 13:49:02 | [train_policy] epoch #89 | descent direction computed
2021-06-04 13:49:02 | [train_policy] epoch #89 | backtrack iters: 0
2021-06-04 13:49:02 | [train_policy] epoch #89 | optimization finished
2021-06-04 13:49:02 | [train_policy] epoch #89 | Computing KL after
2021-06-04 13:49:02 | [train_policy] epoch #89 | Computing loss after
2021-06-04 13:49:02 | [train_policy] epoch #89 | Fitting baseline...
2021-06-04 13:49:02 | [train_policy] epoch #89 | Saving snapshot...
2021-06-04 13:49:02 | [train_policy] epoch #89 | Saved
2021-06-04 13:49:02 | [train_policy] epoch #89 | Time 74.31 s
2021-06-04 13:49:02 | [train_policy] epoch #89 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.292202
Evaluation/AverageDiscountedReturn         -47.46
Evaluation/AverageReturn                   -47.46
Evaluation/CompletionRate                    0
Evaluation/Iteration                        89
Evaluation/MaxReturn                       -34.2688
Evaluation/MinReturn                       -78.6716
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.55723
Extras/EpisodeRewardMean                   -47.3054
LinearFeatureBaseline/ExplainedVariance      0.869735
PolicyExecTime                               0.229993
ProcessExecTime                              0.0320079
TotalEnvSteps                            91080
policy/Entropy                               1.06167
policy/KL                                    0.00947723
policy/KLBefore                              0
policy/LossAfter                            -0.0159315
policy/LossBefore                            1.41355e-09
policy/Perplexity                            2.89119
policy/dLoss                                 0.0159315
---------------------------------------  ---------------
2021-06-04 13:49:02 | [train_policy] epoch #90 | Obtaining samples for iteration 90...
2021-06-04 13:49:03 | [train_policy] epoch #90 | Logging diagnostics...
2021-06-04 13:49:03 | [train_policy] epoch #90 | Optimizing policy...
2021-06-04 13:49:03 | [train_policy] epoch #90 | Computing loss before
2021-06-04 13:49:03 | [train_policy] epoch #90 | Computing KL before
2021-06-04 13:49:03 | [train_policy] epoch #90 | Optimizing
2021-06-04 13:49:03 | [train_policy] epoch #90 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:03 | [train_policy] epoch #90 | computing loss before
2021-06-04 13:49:03 | [train_policy] epoch #90 | computing gradient
2021-06-04 13:49:03 | [train_policy] epoch #90 | gradient computed
2021-06-04 13:49:03 | [train_policy] epoch #90 | computing descent direction
2021-06-04 13:49:03 | [train_policy] epoch #90 | descent direction computed
2021-06-04 13:49:03 | [train_policy] epoch #90 | backtrack iters: 0
2021-06-04 13:49:03 | [train_policy] epoch #90 | optimization finished
2021-06-04 13:49:03 | [train_policy] epoch #90 | Computing KL after
2021-06-04 13:49:03 | [train_policy] epoch #90 | Computing loss after
2021-06-04 13:49:03 | [train_policy] epoch #90 | Fitting baseline...
2021-06-04 13:49:03 | [train_policy] epoch #90 | Saving snapshot...
2021-06-04 13:49:03 | [train_policy] epoch #90 | Saved
2021-06-04 13:49:03 | [train_policy] epoch #90 | Time 75.11 s
2021-06-04 13:49:03 | [train_policy] epoch #90 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.288032
Evaluation/AverageDiscountedReturn         -47.4441
Evaluation/AverageReturn                   -47.4441
Evaluation/CompletionRate                    0
Evaluation/Iteration                        90
Evaluation/MaxReturn                       -37.7522
Evaluation/MinReturn                       -65.8392
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.65475
Extras/EpisodeRewardMean                   -47.6464
LinearFeatureBaseline/ExplainedVariance      0.950403
PolicyExecTime                               0.228646
ProcessExecTime                              0.0315483
TotalEnvSteps                            92092
policy/Entropy                               1.06378
policy/KL                                    0.00994642
policy/KLBefore                              0
policy/LossAfter                            -0.0242724
policy/LossBefore                           -5.18301e-09
policy/Perplexity                            2.8973
policy/dLoss                                 0.0242724
---------------------------------------  ---------------
2021-06-04 13:49:03 | [train_policy] epoch #91 | Obtaining samples for iteration 91...
2021-06-04 13:49:04 | [train_policy] epoch #91 | Logging diagnostics...
2021-06-04 13:49:04 | [train_policy] epoch #91 | Optimizing policy...
2021-06-04 13:49:04 | [train_policy] epoch #91 | Computing loss before
2021-06-04 13:49:04 | [train_policy] epoch #91 | Computing KL before
2021-06-04 13:49:04 | [train_policy] epoch #91 | Optimizing
2021-06-04 13:49:04 | [train_policy] epoch #91 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:04 | [train_policy] epoch #91 | computing loss before
2021-06-04 13:49:04 | [train_policy] epoch #91 | computing gradient
2021-06-04 13:49:04 | [train_policy] epoch #91 | gradient computed
2021-06-04 13:49:04 | [train_policy] epoch #91 | computing descent direction
2021-06-04 13:49:04 | [train_policy] epoch #91 | descent direction computed
2021-06-04 13:49:04 | [train_policy] epoch #91 | backtrack iters: 1
2021-06-04 13:49:04 | [train_policy] epoch #91 | optimization finished
2021-06-04 13:49:04 | [train_policy] epoch #91 | Computing KL after
2021-06-04 13:49:04 | [train_policy] epoch #91 | Computing loss after
2021-06-04 13:49:04 | [train_policy] epoch #91 | Fitting baseline...
2021-06-04 13:49:04 | [train_policy] epoch #91 | Saving snapshot...
2021-06-04 13:49:04 | [train_policy] epoch #91 | Saved
2021-06-04 13:49:04 | [train_policy] epoch #91 | Time 75.91 s
2021-06-04 13:49:04 | [train_policy] epoch #91 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.28489
Evaluation/AverageDiscountedReturn         -46.8215
Evaluation/AverageReturn                   -46.8215
Evaluation/CompletionRate                    0
Evaluation/Iteration                        91
Evaluation/MaxReturn                       -34.809
Evaluation/MinReturn                       -81.4589
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.72516
Extras/EpisodeRewardMean                   -46.88
LinearFeatureBaseline/ExplainedVariance      0.89894
PolicyExecTime                               0.228789
ProcessExecTime                              0.0311284
TotalEnvSteps                            93104
policy/Entropy                               1.04012
policy/KL                                    0.00694469
policy/KLBefore                              0
policy/LossAfter                            -0.0239906
policy/LossBefore                            8.48129e-09
policy/Perplexity                            2.82956
policy/dLoss                                 0.0239906
---------------------------------------  ---------------
2021-06-04 13:49:04 | [train_policy] epoch #92 | Obtaining samples for iteration 92...
2021-06-04 13:49:04 | [train_policy] epoch #92 | Logging diagnostics...
2021-06-04 13:49:04 | [train_policy] epoch #92 | Optimizing policy...
2021-06-04 13:49:04 | [train_policy] epoch #92 | Computing loss before
2021-06-04 13:49:04 | [train_policy] epoch #92 | Computing KL before
2021-06-04 13:49:04 | [train_policy] epoch #92 | Optimizing
2021-06-04 13:49:04 | [train_policy] epoch #92 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:04 | [train_policy] epoch #92 | computing loss before
2021-06-04 13:49:04 | [train_policy] epoch #92 | computing gradient
2021-06-04 13:49:04 | [train_policy] epoch #92 | gradient computed
2021-06-04 13:49:04 | [train_policy] epoch #92 | computing descent direction
2021-06-04 13:49:04 | [train_policy] epoch #92 | descent direction computed
2021-06-04 13:49:04 | [train_policy] epoch #92 | backtrack iters: 1
2021-06-04 13:49:04 | [train_policy] epoch #92 | optimization finished
2021-06-04 13:49:04 | [train_policy] epoch #92 | Computing KL after
2021-06-04 13:49:04 | [train_policy] epoch #92 | Computing loss after
2021-06-04 13:49:04 | [train_policy] epoch #92 | Fitting baseline...
2021-06-04 13:49:04 | [train_policy] epoch #92 | Saving snapshot...
2021-06-04 13:49:04 | [train_policy] epoch #92 | Saved
2021-06-04 13:49:04 | [train_policy] epoch #92 | Time 76.70 s
2021-06-04 13:49:04 | [train_policy] epoch #92 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.284442
Evaluation/AverageDiscountedReturn         -70.5658
Evaluation/AverageReturn                   -70.5658
Evaluation/CompletionRate                    0
Evaluation/Iteration                        92
Evaluation/MaxReturn                       -34.5381
Evaluation/MinReturn                     -2062.26
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.035
Extras/EpisodeRewardMean                   -69.0263
LinearFeatureBaseline/ExplainedVariance      0.0126813
PolicyExecTime                               0.224524
ProcessExecTime                              0.0311348
TotalEnvSteps                            94116
policy/Entropy                               1.02445
policy/KL                                    0.00755205
policy/KLBefore                              0
policy/LossAfter                            -0.0166233
policy/LossBefore                           -1.27219e-08
policy/Perplexity                            2.78557
policy/dLoss                                 0.0166233
---------------------------------------  ---------------
2021-06-04 13:49:05 | [train_policy] epoch #93 | Obtaining samples for iteration 93...
2021-06-04 13:49:05 | [train_policy] epoch #93 | Logging diagnostics...
2021-06-04 13:49:05 | [train_policy] epoch #93 | Optimizing policy...
2021-06-04 13:49:05 | [train_policy] epoch #93 | Computing loss before
2021-06-04 13:49:05 | [train_policy] epoch #93 | Computing KL before
2021-06-04 13:49:05 | [train_policy] epoch #93 | Optimizing
2021-06-04 13:49:05 | [train_policy] epoch #93 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:05 | [train_policy] epoch #93 | computing loss before
2021-06-04 13:49:05 | [train_policy] epoch #93 | computing gradient
2021-06-04 13:49:05 | [train_policy] epoch #93 | gradient computed
2021-06-04 13:49:05 | [train_policy] epoch #93 | computing descent direction
2021-06-04 13:49:05 | [train_policy] epoch #93 | descent direction computed
2021-06-04 13:49:05 | [train_policy] epoch #93 | backtrack iters: 1
2021-06-04 13:49:05 | [train_policy] epoch #93 | optimization finished
2021-06-04 13:49:05 | [train_policy] epoch #93 | Computing KL after
2021-06-04 13:49:05 | [train_policy] epoch #93 | Computing loss after
2021-06-04 13:49:05 | [train_policy] epoch #93 | Fitting baseline...
2021-06-04 13:49:05 | [train_policy] epoch #93 | Saving snapshot...
2021-06-04 13:49:05 | [train_policy] epoch #93 | Saved
2021-06-04 13:49:05 | [train_policy] epoch #93 | Time 77.50 s
2021-06-04 13:49:05 | [train_policy] epoch #93 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.284491
Evaluation/AverageDiscountedReturn         -49.359
Evaluation/AverageReturn                   -49.359
Evaluation/CompletionRate                    0
Evaluation/Iteration                        93
Evaluation/MaxReturn                       -35.5239
Evaluation/MinReturn                      -246.602
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        21.419
Extras/EpisodeRewardMean                   -70.2248
LinearFeatureBaseline/ExplainedVariance    -16.0284
PolicyExecTime                               0.229609
ProcessExecTime                              0.0310245
TotalEnvSteps                            95128
policy/Entropy                               1.00469
policy/KL                                    0.00671006
policy/KLBefore                              0
policy/LossAfter                            -0.0199245
policy/LossBefore                           -2.54439e-08
policy/Perplexity                            2.73107
policy/dLoss                                 0.0199245
---------------------------------------  ---------------
2021-06-04 13:49:05 | [train_policy] epoch #94 | Obtaining samples for iteration 94...
2021-06-04 13:49:06 | [train_policy] epoch #94 | Logging diagnostics...
2021-06-04 13:49:06 | [train_policy] epoch #94 | Optimizing policy...
2021-06-04 13:49:06 | [train_policy] epoch #94 | Computing loss before
2021-06-04 13:49:06 | [train_policy] epoch #94 | Computing KL before
2021-06-04 13:49:06 | [train_policy] epoch #94 | Optimizing
2021-06-04 13:49:06 | [train_policy] epoch #94 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:06 | [train_policy] epoch #94 | computing loss before
2021-06-04 13:49:06 | [train_policy] epoch #94 | computing gradient
2021-06-04 13:49:06 | [train_policy] epoch #94 | gradient computed
2021-06-04 13:49:06 | [train_policy] epoch #94 | computing descent direction
2021-06-04 13:49:06 | [train_policy] epoch #94 | descent direction computed
2021-06-04 13:49:06 | [train_policy] epoch #94 | backtrack iters: 0
2021-06-04 13:49:06 | [train_policy] epoch #94 | optimization finished
2021-06-04 13:49:06 | [train_policy] epoch #94 | Computing KL after
2021-06-04 13:49:06 | [train_policy] epoch #94 | Computing loss after
2021-06-04 13:49:06 | [train_policy] epoch #94 | Fitting baseline...
2021-06-04 13:49:06 | [train_policy] epoch #94 | Saving snapshot...
2021-06-04 13:49:06 | [train_policy] epoch #94 | Saved
2021-06-04 13:49:06 | [train_policy] epoch #94 | Time 78.29 s
2021-06-04 13:49:06 | [train_policy] epoch #94 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                  0.284612
Evaluation/AverageDiscountedReturn         -46.422
Evaluation/AverageReturn                   -46.422
Evaluation/CompletionRate                    0
Evaluation/Iteration                        94
Evaluation/MaxReturn                       -32.8915
Evaluation/MinReturn                       -64.8774
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.25544
Extras/EpisodeRewardMean                   -46.5042
LinearFeatureBaseline/ExplainedVariance      0.95303
PolicyExecTime                               0.22179
ProcessExecTime                              0.0311313
TotalEnvSteps                            96140
policy/Entropy                               1.00173
policy/KL                                    0.00955357
policy/KLBefore                              0
policy/LossAfter                            -0.0205315
policy/LossBefore                           -1.86117e-08
policy/Perplexity                            2.72298
policy/dLoss                                 0.0205315
---------------------------------------  ---------------
2021-06-04 13:49:06 | [train_policy] epoch #95 | Obtaining samples for iteration 95...
2021-06-04 13:49:07 | [train_policy] epoch #95 | Logging diagnostics...
2021-06-04 13:49:07 | [train_policy] epoch #95 | Optimizing policy...
2021-06-04 13:49:07 | [train_policy] epoch #95 | Computing loss before
2021-06-04 13:49:07 | [train_policy] epoch #95 | Computing KL before
2021-06-04 13:49:07 | [train_policy] epoch #95 | Optimizing
2021-06-04 13:49:07 | [train_policy] epoch #95 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:07 | [train_policy] epoch #95 | computing loss before
2021-06-04 13:49:07 | [train_policy] epoch #95 | computing gradient
2021-06-04 13:49:07 | [train_policy] epoch #95 | gradient computed
2021-06-04 13:49:07 | [train_policy] epoch #95 | computing descent direction
2021-06-04 13:49:07 | [train_policy] epoch #95 | descent direction computed
2021-06-04 13:49:07 | [train_policy] epoch #95 | backtrack iters: 1
2021-06-04 13:49:07 | [train_policy] epoch #95 | optimization finished
2021-06-04 13:49:07 | [train_policy] epoch #95 | Computing KL after
2021-06-04 13:49:07 | [train_policy] epoch #95 | Computing loss after
2021-06-04 13:49:07 | [train_policy] epoch #95 | Fitting baseline...
2021-06-04 13:49:07 | [train_policy] epoch #95 | Saving snapshot...
2021-06-04 13:49:07 | [train_policy] epoch #95 | Saved
2021-06-04 13:49:07 | [train_policy] epoch #95 | Time 79.10 s
2021-06-04 13:49:07 | [train_policy] epoch #95 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.285311
Evaluation/AverageDiscountedReturn         -46.2319
Evaluation/AverageReturn                   -46.2319
Evaluation/CompletionRate                    0
Evaluation/Iteration                        95
Evaluation/MaxReturn                       -35.3566
Evaluation/MinReturn                      -104.463
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.60974
Extras/EpisodeRewardMean                   -46.3227
LinearFeatureBaseline/ExplainedVariance      0.846269
PolicyExecTime                               0.231951
ProcessExecTime                              0.0312133
TotalEnvSteps                            97152
policy/Entropy                               0.999772
policy/KL                                    0.00649117
policy/KLBefore                              0
policy/LossAfter                            -0.0193877
policy/LossBefore                            9.30586e-09
policy/Perplexity                            2.71766
policy/dLoss                                 0.0193877
---------------------------------------  ---------------
2021-06-04 13:49:07 | [train_policy] epoch #96 | Obtaining samples for iteration 96...
2021-06-04 13:49:08 | [train_policy] epoch #96 | Logging diagnostics...
2021-06-04 13:49:08 | [train_policy] epoch #96 | Optimizing policy...
2021-06-04 13:49:08 | [train_policy] epoch #96 | Computing loss before
2021-06-04 13:49:08 | [train_policy] epoch #96 | Computing KL before
2021-06-04 13:49:08 | [train_policy] epoch #96 | Optimizing
2021-06-04 13:49:08 | [train_policy] epoch #96 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:08 | [train_policy] epoch #96 | computing loss before
2021-06-04 13:49:08 | [train_policy] epoch #96 | computing gradient
2021-06-04 13:49:08 | [train_policy] epoch #96 | gradient computed
2021-06-04 13:49:08 | [train_policy] epoch #96 | computing descent direction
2021-06-04 13:49:08 | [train_policy] epoch #96 | descent direction computed
2021-06-04 13:49:08 | [train_policy] epoch #96 | backtrack iters: 1
2021-06-04 13:49:08 | [train_policy] epoch #96 | optimization finished
2021-06-04 13:49:08 | [train_policy] epoch #96 | Computing KL after
2021-06-04 13:49:08 | [train_policy] epoch #96 | Computing loss after
2021-06-04 13:49:08 | [train_policy] epoch #96 | Fitting baseline...
2021-06-04 13:49:08 | [train_policy] epoch #96 | Saving snapshot...
2021-06-04 13:49:08 | [train_policy] epoch #96 | Saved
2021-06-04 13:49:08 | [train_policy] epoch #96 | Time 79.90 s
2021-06-04 13:49:08 | [train_policy] epoch #96 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                  0.284769
Evaluation/AverageDiscountedReturn         -46.7567
Evaluation/AverageReturn                   -46.7567
Evaluation/CompletionRate                    0
Evaluation/Iteration                        96
Evaluation/MaxReturn                       -35.0427
Evaluation/MinReturn                       -64.1803
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.37077
Extras/EpisodeRewardMean                   -46.4472
LinearFeatureBaseline/ExplainedVariance      0.947726
PolicyExecTime                               0.230212
ProcessExecTime                              0.031179
TotalEnvSteps                            98164
policy/Entropy                               0.956764
policy/KL                                    0.00675648
policy/KLBefore                              0
policy/LossAfter                            -0.0208751
policy/LossBefore                           -9.42366e-10
policy/Perplexity                            2.60326
policy/dLoss                                 0.0208751
---------------------------------------  ---------------
2021-06-04 13:49:08 | [train_policy] epoch #97 | Obtaining samples for iteration 97...
2021-06-04 13:49:08 | [train_policy] epoch #97 | Logging diagnostics...
2021-06-04 13:49:08 | [train_policy] epoch #97 | Optimizing policy...
2021-06-04 13:49:08 | [train_policy] epoch #97 | Computing loss before
2021-06-04 13:49:08 | [train_policy] epoch #97 | Computing KL before
2021-06-04 13:49:08 | [train_policy] epoch #97 | Optimizing
2021-06-04 13:49:08 | [train_policy] epoch #97 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:08 | [train_policy] epoch #97 | computing loss before
2021-06-04 13:49:08 | [train_policy] epoch #97 | computing gradient
2021-06-04 13:49:08 | [train_policy] epoch #97 | gradient computed
2021-06-04 13:49:08 | [train_policy] epoch #97 | computing descent direction
2021-06-04 13:49:08 | [train_policy] epoch #97 | descent direction computed
2021-06-04 13:49:08 | [train_policy] epoch #97 | backtrack iters: 1
2021-06-04 13:49:08 | [train_policy] epoch #97 | optimization finished
2021-06-04 13:49:08 | [train_policy] epoch #97 | Computing KL after
2021-06-04 13:49:08 | [train_policy] epoch #97 | Computing loss after
2021-06-04 13:49:08 | [train_policy] epoch #97 | Fitting baseline...
2021-06-04 13:49:08 | [train_policy] epoch #97 | Saving snapshot...
2021-06-04 13:49:09 | [train_policy] epoch #97 | Saved
2021-06-04 13:49:09 | [train_policy] epoch #97 | Time 80.72 s
2021-06-04 13:49:09 | [train_policy] epoch #97 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                  0.294234
Evaluation/AverageDiscountedReturn         -50.0296
Evaluation/AverageReturn                   -50.0296
Evaluation/CompletionRate                    0
Evaluation/Iteration                        97
Evaluation/MaxReturn                       -32.9807
Evaluation/MinReturn                      -246.359
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        23.2916
Extras/EpisodeRewardMean                   -49.7122
LinearFeatureBaseline/ExplainedVariance      0.671622
PolicyExecTime                               0.234919
ProcessExecTime                              0.0318975
TotalEnvSteps                            99176
policy/Entropy                               0.933668
policy/KL                                    0.00663224
policy/KLBefore                              0
policy/LossAfter                            -0.0213335
policy/LossBefore                            1.17796e-08
policy/Perplexity                            2.54382
policy/dLoss                                 0.0213335
---------------------------------------  ---------------
2021-06-04 13:49:09 | [train_policy] epoch #98 | Obtaining samples for iteration 98...
2021-06-04 13:49:09 | [train_policy] epoch #98 | Logging diagnostics...
2021-06-04 13:49:09 | [train_policy] epoch #98 | Optimizing policy...
2021-06-04 13:49:09 | [train_policy] epoch #98 | Computing loss before
2021-06-04 13:49:09 | [train_policy] epoch #98 | Computing KL before
2021-06-04 13:49:09 | [train_policy] epoch #98 | Optimizing
2021-06-04 13:49:09 | [train_policy] epoch #98 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:09 | [train_policy] epoch #98 | computing loss before
2021-06-04 13:49:09 | [train_policy] epoch #98 | computing gradient
2021-06-04 13:49:09 | [train_policy] epoch #98 | gradient computed
2021-06-04 13:49:09 | [train_policy] epoch #98 | computing descent direction
2021-06-04 13:49:09 | [train_policy] epoch #98 | descent direction computed
2021-06-04 13:49:09 | [train_policy] epoch #98 | backtrack iters: 1
2021-06-04 13:49:09 | [train_policy] epoch #98 | optimization finished
2021-06-04 13:49:09 | [train_policy] epoch #98 | Computing KL after
2021-06-04 13:49:09 | [train_policy] epoch #98 | Computing loss after
2021-06-04 13:49:09 | [train_policy] epoch #98 | Fitting baseline...
2021-06-04 13:49:09 | [train_policy] epoch #98 | Saving snapshot...
2021-06-04 13:49:09 | [train_policy] epoch #98 | Saved
2021-06-04 13:49:09 | [train_policy] epoch #98 | Time 81.51 s
2021-06-04 13:49:09 | [train_policy] epoch #98 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283727
Evaluation/AverageDiscountedReturn          -46.2733
Evaluation/AverageReturn                    -46.2733
Evaluation/CompletionRate                     0
Evaluation/Iteration                         98
Evaluation/MaxReturn                        -32.3115
Evaluation/MinReturn                        -62.1535
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.82998
Extras/EpisodeRewardMean                    -46.9397
LinearFeatureBaseline/ExplainedVariance       0.9114
PolicyExecTime                                0.21256
ProcessExecTime                               0.0311394
TotalEnvSteps                            100188
policy/Entropy                                0.950987
policy/KL                                     0.00686897
policy/KLBefore                               0
policy/LossAfter                             -0.0166084
policy/LossBefore                             4.18175e-09
policy/Perplexity                             2.58826
policy/dLoss                                  0.0166084
---------------------------------------  ----------------
2021-06-04 13:49:09 | [train_policy] epoch #99 | Obtaining samples for iteration 99...
2021-06-04 13:49:10 | [train_policy] epoch #99 | Logging diagnostics...
2021-06-04 13:49:10 | [train_policy] epoch #99 | Optimizing policy...
2021-06-04 13:49:10 | [train_policy] epoch #99 | Computing loss before
2021-06-04 13:49:10 | [train_policy] epoch #99 | Computing KL before
2021-06-04 13:49:10 | [train_policy] epoch #99 | Optimizing
2021-06-04 13:49:10 | [train_policy] epoch #99 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:10 | [train_policy] epoch #99 | computing loss before
2021-06-04 13:49:10 | [train_policy] epoch #99 | computing gradient
2021-06-04 13:49:10 | [train_policy] epoch #99 | gradient computed
2021-06-04 13:49:10 | [train_policy] epoch #99 | computing descent direction
2021-06-04 13:49:10 | [train_policy] epoch #99 | descent direction computed
2021-06-04 13:49:10 | [train_policy] epoch #99 | backtrack iters: 1
2021-06-04 13:49:10 | [train_policy] epoch #99 | optimization finished
2021-06-04 13:49:10 | [train_policy] epoch #99 | Computing KL after
2021-06-04 13:49:10 | [train_policy] epoch #99 | Computing loss after
2021-06-04 13:49:10 | [train_policy] epoch #99 | Fitting baseline...
2021-06-04 13:49:10 | [train_policy] epoch #99 | Saving snapshot...
2021-06-04 13:49:10 | [train_policy] epoch #99 | Saved
2021-06-04 13:49:10 | [train_policy] epoch #99 | Time 82.31 s
2021-06-04 13:49:10 | [train_policy] epoch #99 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.292207
Evaluation/AverageDiscountedReturn          -45.2882
Evaluation/AverageReturn                    -45.2882
Evaluation/CompletionRate                     0
Evaluation/Iteration                         99
Evaluation/MaxReturn                        -33.9597
Evaluation/MinReturn                        -60.5841
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.0284
Extras/EpisodeRewardMean                    -45.427
LinearFeatureBaseline/ExplainedVariance       0.948382
PolicyExecTime                                0.223523
ProcessExecTime                               0.0319726
TotalEnvSteps                            101200
policy/Entropy                                0.910041
policy/KL                                     0.00676404
policy/KLBefore                               0
policy/LossAfter                             -0.0264817
policy/LossBefore                             6.83215e-09
policy/Perplexity                             2.48442
policy/dLoss                                  0.0264817
---------------------------------------  ----------------
2021-06-04 13:49:10 | [train_policy] epoch #100 | Obtaining samples for iteration 100...
2021-06-04 13:49:11 | [train_policy] epoch #100 | Logging diagnostics...
2021-06-04 13:49:11 | [train_policy] epoch #100 | Optimizing policy...
2021-06-04 13:49:11 | [train_policy] epoch #100 | Computing loss before
2021-06-04 13:49:11 | [train_policy] epoch #100 | Computing KL before
2021-06-04 13:49:11 | [train_policy] epoch #100 | Optimizing
2021-06-04 13:49:11 | [train_policy] epoch #100 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:11 | [train_policy] epoch #100 | computing loss before
2021-06-04 13:49:11 | [train_policy] epoch #100 | computing gradient
2021-06-04 13:49:11 | [train_policy] epoch #100 | gradient computed
2021-06-04 13:49:11 | [train_policy] epoch #100 | computing descent direction
2021-06-04 13:49:11 | [train_policy] epoch #100 | descent direction computed
2021-06-04 13:49:11 | [train_policy] epoch #100 | backtrack iters: 1
2021-06-04 13:49:11 | [train_policy] epoch #100 | optimization finished
2021-06-04 13:49:11 | [train_policy] epoch #100 | Computing KL after
2021-06-04 13:49:11 | [train_policy] epoch #100 | Computing loss after
2021-06-04 13:49:11 | [train_policy] epoch #100 | Fitting baseline...
2021-06-04 13:49:11 | [train_policy] epoch #100 | Saving snapshot...
2021-06-04 13:49:11 | [train_policy] epoch #100 | Saved
2021-06-04 13:49:11 | [train_policy] epoch #100 | Time 83.11 s
2021-06-04 13:49:11 | [train_policy] epoch #100 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284665
Evaluation/AverageDiscountedReturn          -68.9232
Evaluation/AverageReturn                    -68.9232
Evaluation/CompletionRate                     0
Evaluation/Iteration                        100
Evaluation/MaxReturn                        -32.8949
Evaluation/MinReturn                      -2056.02
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.393
Extras/EpisodeRewardMean                    -66.7627
LinearFeatureBaseline/ExplainedVariance       0.00855974
PolicyExecTime                                0.229759
ProcessExecTime                               0.031076
TotalEnvSteps                            102212
policy/Entropy                                0.933774
policy/KL                                     0.00675097
policy/KLBefore                               0
policy/LossAfter                             -0.0146882
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             2.54409
policy/dLoss                                  0.0146882
---------------------------------------  ----------------
2021-06-04 13:49:11 | [train_policy] epoch #101 | Obtaining samples for iteration 101...
2021-06-04 13:49:12 | [train_policy] epoch #101 | Logging diagnostics...
2021-06-04 13:49:12 | [train_policy] epoch #101 | Optimizing policy...
2021-06-04 13:49:12 | [train_policy] epoch #101 | Computing loss before
2021-06-04 13:49:12 | [train_policy] epoch #101 | Computing KL before
2021-06-04 13:49:12 | [train_policy] epoch #101 | Optimizing
2021-06-04 13:49:12 | [train_policy] epoch #101 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:12 | [train_policy] epoch #101 | computing loss before
2021-06-04 13:49:12 | [train_policy] epoch #101 | computing gradient
2021-06-04 13:49:12 | [train_policy] epoch #101 | gradient computed
2021-06-04 13:49:12 | [train_policy] epoch #101 | computing descent direction
2021-06-04 13:49:12 | [train_policy] epoch #101 | descent direction computed
2021-06-04 13:49:12 | [train_policy] epoch #101 | backtrack iters: 0
2021-06-04 13:49:12 | [train_policy] epoch #101 | optimization finished
2021-06-04 13:49:12 | [train_policy] epoch #101 | Computing KL after
2021-06-04 13:49:12 | [train_policy] epoch #101 | Computing loss after
2021-06-04 13:49:12 | [train_policy] epoch #101 | Fitting baseline...
2021-06-04 13:49:12 | [train_policy] epoch #101 | Saving snapshot...
2021-06-04 13:49:12 | [train_policy] epoch #101 | Saved
2021-06-04 13:49:12 | [train_policy] epoch #101 | Time 83.91 s
2021-06-04 13:49:12 | [train_policy] epoch #101 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284033
Evaluation/AverageDiscountedReturn          -46.8029
Evaluation/AverageReturn                    -46.8029
Evaluation/CompletionRate                     0
Evaluation/Iteration                        101
Evaluation/MaxReturn                        -34.0767
Evaluation/MinReturn                       -179.183
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         15.1455
Extras/EpisodeRewardMean                    -67.2937
LinearFeatureBaseline/ExplainedVariance      -6.93988
PolicyExecTime                                0.227823
ProcessExecTime                               0.0310857
TotalEnvSteps                            103224
policy/Entropy                                0.99682
policy/KL                                     0.00900949
policy/KLBefore                               0
policy/LossAfter                             -0.0293979
policy/LossBefore                             3.15693e-08
policy/Perplexity                             2.70965
policy/dLoss                                  0.0293979
---------------------------------------  ----------------
2021-06-04 13:49:12 | [train_policy] epoch #102 | Obtaining samples for iteration 102...
2021-06-04 13:49:12 | [train_policy] epoch #102 | Logging diagnostics...
2021-06-04 13:49:12 | [train_policy] epoch #102 | Optimizing policy...
2021-06-04 13:49:12 | [train_policy] epoch #102 | Computing loss before
2021-06-04 13:49:12 | [train_policy] epoch #102 | Computing KL before
2021-06-04 13:49:12 | [train_policy] epoch #102 | Optimizing
2021-06-04 13:49:12 | [train_policy] epoch #102 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:12 | [train_policy] epoch #102 | computing loss before
2021-06-04 13:49:12 | [train_policy] epoch #102 | computing gradient
2021-06-04 13:49:12 | [train_policy] epoch #102 | gradient computed
2021-06-04 13:49:12 | [train_policy] epoch #102 | computing descent direction
2021-06-04 13:49:12 | [train_policy] epoch #102 | descent direction computed
2021-06-04 13:49:12 | [train_policy] epoch #102 | backtrack iters: 0
2021-06-04 13:49:12 | [train_policy] epoch #102 | optimization finished
2021-06-04 13:49:12 | [train_policy] epoch #102 | Computing KL after
2021-06-04 13:49:12 | [train_policy] epoch #102 | Computing loss after
2021-06-04 13:49:12 | [train_policy] epoch #102 | Fitting baseline...
2021-06-04 13:49:12 | [train_policy] epoch #102 | Saving snapshot...
2021-06-04 13:49:12 | [train_policy] epoch #102 | Saved
2021-06-04 13:49:12 | [train_policy] epoch #102 | Time 84.68 s
2021-06-04 13:49:12 | [train_policy] epoch #102 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                   0.286453
Evaluation/AverageDiscountedReturn          -69.3857
Evaluation/AverageReturn                    -69.3857
Evaluation/CompletionRate                     0
Evaluation/Iteration                        102
Evaluation/MaxReturn                        -37.0707
Evaluation/MinReturn                      -2061.13
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.938
Extras/EpisodeRewardMean                    -67.3318
LinearFeatureBaseline/ExplainedVariance       0.0272235
PolicyExecTime                                0.211386
ProcessExecTime                               0.0312886
TotalEnvSteps                            104236
policy/Entropy                                1.01626
policy/KL                                     0.00957611
policy/KLBefore                               0
policy/LossAfter                             -0.0146899
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             2.76284
policy/dLoss                                  0.0146899
---------------------------------------  ---------------
2021-06-04 13:49:12 | [train_policy] epoch #103 | Obtaining samples for iteration 103...
2021-06-04 13:49:13 | [train_policy] epoch #103 | Logging diagnostics...
2021-06-04 13:49:13 | [train_policy] epoch #103 | Optimizing policy...
2021-06-04 13:49:13 | [train_policy] epoch #103 | Computing loss before
2021-06-04 13:49:13 | [train_policy] epoch #103 | Computing KL before
2021-06-04 13:49:13 | [train_policy] epoch #103 | Optimizing
2021-06-04 13:49:13 | [train_policy] epoch #103 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:13 | [train_policy] epoch #103 | computing loss before
2021-06-04 13:49:13 | [train_policy] epoch #103 | computing gradient
2021-06-04 13:49:13 | [train_policy] epoch #103 | gradient computed
2021-06-04 13:49:13 | [train_policy] epoch #103 | computing descent direction
2021-06-04 13:49:13 | [train_policy] epoch #103 | descent direction computed
2021-06-04 13:49:13 | [train_policy] epoch #103 | backtrack iters: 0
2021-06-04 13:49:13 | [train_policy] epoch #103 | optimization finished
2021-06-04 13:49:13 | [train_policy] epoch #103 | Computing KL after
2021-06-04 13:49:13 | [train_policy] epoch #103 | Computing loss after
2021-06-04 13:49:13 | [train_policy] epoch #103 | Fitting baseline...
2021-06-04 13:49:13 | [train_policy] epoch #103 | Saving snapshot...
2021-06-04 13:49:13 | [train_policy] epoch #103 | Saved
2021-06-04 13:49:13 | [train_policy] epoch #103 | Time 85.49 s
2021-06-04 13:49:13 | [train_policy] epoch #103 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286822
Evaluation/AverageDiscountedReturn         -157.25
Evaluation/AverageReturn                   -157.25
Evaluation/CompletionRate                     0
Evaluation/Iteration                        103
Evaluation/MaxReturn                        -35.0904
Evaluation/MinReturn                      -2064.04
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        456.463
Extras/EpisodeRewardMean                   -148.209
LinearFeatureBaseline/ExplainedVariance       0.146552
PolicyExecTime                                0.224797
ProcessExecTime                               0.0312927
TotalEnvSteps                            105248
policy/Entropy                                1.06112
policy/KL                                     0.00952749
policy/KLBefore                               0
policy/LossAfter                             -0.0247702
policy/LossBefore                             3.29828e-09
policy/Perplexity                             2.88961
policy/dLoss                                  0.0247702
---------------------------------------  ----------------
2021-06-04 13:49:13 | [train_policy] epoch #104 | Obtaining samples for iteration 104...
2021-06-04 13:49:14 | [train_policy] epoch #104 | Logging diagnostics...
2021-06-04 13:49:14 | [train_policy] epoch #104 | Optimizing policy...
2021-06-04 13:49:14 | [train_policy] epoch #104 | Computing loss before
2021-06-04 13:49:14 | [train_policy] epoch #104 | Computing KL before
2021-06-04 13:49:14 | [train_policy] epoch #104 | Optimizing
2021-06-04 13:49:14 | [train_policy] epoch #104 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:14 | [train_policy] epoch #104 | computing loss before
2021-06-04 13:49:14 | [train_policy] epoch #104 | computing gradient
2021-06-04 13:49:14 | [train_policy] epoch #104 | gradient computed
2021-06-04 13:49:14 | [train_policy] epoch #104 | computing descent direction
2021-06-04 13:49:14 | [train_policy] epoch #104 | descent direction computed
2021-06-04 13:49:14 | [train_policy] epoch #104 | backtrack iters: 0
2021-06-04 13:49:14 | [train_policy] epoch #104 | optimization finished
2021-06-04 13:49:14 | [train_policy] epoch #104 | Computing KL after
2021-06-04 13:49:14 | [train_policy] epoch #104 | Computing loss after
2021-06-04 13:49:14 | [train_policy] epoch #104 | Fitting baseline...
2021-06-04 13:49:14 | [train_policy] epoch #104 | Saving snapshot...
2021-06-04 13:49:14 | [train_policy] epoch #104 | Saved
2021-06-04 13:49:14 | [train_policy] epoch #104 | Time 86.29 s
2021-06-04 13:49:14 | [train_policy] epoch #104 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284905
Evaluation/AverageDiscountedReturn          -68.8007
Evaluation/AverageReturn                    -68.8007
Evaluation/CompletionRate                     0
Evaluation/Iteration                        104
Evaluation/MaxReturn                        -34.1953
Evaluation/MinReturn                      -2060.49
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.876
Extras/EpisodeRewardMean                    -87.6037
LinearFeatureBaseline/ExplainedVariance      -0.291677
PolicyExecTime                                0.228629
ProcessExecTime                               0.0312486
TotalEnvSteps                            106260
policy/Entropy                                1.07387
policy/KL                                     0.00961584
policy/KLBefore                               0
policy/LossAfter                             -0.0268319
policy/LossBefore                             1.86117e-08
policy/Perplexity                             2.92669
policy/dLoss                                  0.0268319
---------------------------------------  ----------------
2021-06-04 13:49:14 | [train_policy] epoch #105 | Obtaining samples for iteration 105...
2021-06-04 13:49:15 | [train_policy] epoch #105 | Logging diagnostics...
2021-06-04 13:49:15 | [train_policy] epoch #105 | Optimizing policy...
2021-06-04 13:49:15 | [train_policy] epoch #105 | Computing loss before
2021-06-04 13:49:15 | [train_policy] epoch #105 | Computing KL before
2021-06-04 13:49:15 | [train_policy] epoch #105 | Optimizing
2021-06-04 13:49:15 | [train_policy] epoch #105 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:15 | [train_policy] epoch #105 | computing loss before
2021-06-04 13:49:15 | [train_policy] epoch #105 | computing gradient
2021-06-04 13:49:15 | [train_policy] epoch #105 | gradient computed
2021-06-04 13:49:15 | [train_policy] epoch #105 | computing descent direction
2021-06-04 13:49:15 | [train_policy] epoch #105 | descent direction computed
2021-06-04 13:49:15 | [train_policy] epoch #105 | backtrack iters: 0
2021-06-04 13:49:15 | [train_policy] epoch #105 | optimization finished
2021-06-04 13:49:15 | [train_policy] epoch #105 | Computing KL after
2021-06-04 13:49:15 | [train_policy] epoch #105 | Computing loss after
2021-06-04 13:49:15 | [train_policy] epoch #105 | Fitting baseline...
2021-06-04 13:49:15 | [train_policy] epoch #105 | Saving snapshot...
2021-06-04 13:49:15 | [train_policy] epoch #105 | Saved
2021-06-04 13:49:15 | [train_policy] epoch #105 | Time 87.07 s
2021-06-04 13:49:15 | [train_policy] epoch #105 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.284467
Evaluation/AverageDiscountedReturn          -47.1782
Evaluation/AverageReturn                    -47.1782
Evaluation/CompletionRate                     0
Evaluation/Iteration                        105
Evaluation/MaxReturn                        -33.7609
Evaluation/MinReturn                        -64.8099
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.16635
Extras/EpisodeRewardMean                    -67.258
LinearFeatureBaseline/ExplainedVariance     -20.4147
PolicyExecTime                                0.220803
ProcessExecTime                               0.0312197
TotalEnvSteps                            107272
policy/Entropy                                1.04671
policy/KL                                     0.00960864
policy/KLBefore                               0
policy/LossAfter                             -0.0229784
policy/LossBefore                             2.7093e-08
policy/Perplexity                             2.84827
policy/dLoss                                  0.0229784
---------------------------------------  ---------------
2021-06-04 13:49:15 | [train_policy] epoch #106 | Obtaining samples for iteration 106...
2021-06-04 13:49:16 | [train_policy] epoch #106 | Logging diagnostics...
2021-06-04 13:49:16 | [train_policy] epoch #106 | Optimizing policy...
2021-06-04 13:49:16 | [train_policy] epoch #106 | Computing loss before
2021-06-04 13:49:16 | [train_policy] epoch #106 | Computing KL before
2021-06-04 13:49:16 | [train_policy] epoch #106 | Optimizing
2021-06-04 13:49:16 | [train_policy] epoch #106 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:16 | [train_policy] epoch #106 | computing loss before
2021-06-04 13:49:16 | [train_policy] epoch #106 | computing gradient
2021-06-04 13:49:16 | [train_policy] epoch #106 | gradient computed
2021-06-04 13:49:16 | [train_policy] epoch #106 | computing descent direction
2021-06-04 13:49:16 | [train_policy] epoch #106 | descent direction computed
2021-06-04 13:49:16 | [train_policy] epoch #106 | backtrack iters: 1
2021-06-04 13:49:16 | [train_policy] epoch #106 | optimization finished
2021-06-04 13:49:16 | [train_policy] epoch #106 | Computing KL after
2021-06-04 13:49:16 | [train_policy] epoch #106 | Computing loss after
2021-06-04 13:49:16 | [train_policy] epoch #106 | Fitting baseline...
2021-06-04 13:49:16 | [train_policy] epoch #106 | Saving snapshot...
2021-06-04 13:49:16 | [train_policy] epoch #106 | Saved
2021-06-04 13:49:16 | [train_policy] epoch #106 | Time 87.88 s
2021-06-04 13:49:16 | [train_policy] epoch #106 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286538
Evaluation/AverageDiscountedReturn          -96.8843
Evaluation/AverageReturn                    -96.8843
Evaluation/CompletionRate                     0
Evaluation/Iteration                        106
Evaluation/MaxReturn                        -34.1303
Evaluation/MinReturn                      -2061.42
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        297.38
Extras/EpisodeRewardMean                    -92.7082
LinearFeatureBaseline/ExplainedVariance       0.00703284
PolicyExecTime                                0.228945
ProcessExecTime                               0.0312614
TotalEnvSteps                            108284
policy/Entropy                                1.05608
policy/KL                                     0.00686316
policy/KLBefore                               0
policy/LossAfter                             -0.0218389
policy/LossBefore                             7.06774e-09
policy/Perplexity                             2.87508
policy/dLoss                                  0.0218389
---------------------------------------  ----------------
2021-06-04 13:49:16 | [train_policy] epoch #107 | Obtaining samples for iteration 107...
2021-06-04 13:49:16 | [train_policy] epoch #107 | Logging diagnostics...
2021-06-04 13:49:16 | [train_policy] epoch #107 | Optimizing policy...
2021-06-04 13:49:16 | [train_policy] epoch #107 | Computing loss before
2021-06-04 13:49:16 | [train_policy] epoch #107 | Computing KL before
2021-06-04 13:49:16 | [train_policy] epoch #107 | Optimizing
2021-06-04 13:49:16 | [train_policy] epoch #107 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:16 | [train_policy] epoch #107 | computing loss before
2021-06-04 13:49:16 | [train_policy] epoch #107 | computing gradient
2021-06-04 13:49:16 | [train_policy] epoch #107 | gradient computed
2021-06-04 13:49:16 | [train_policy] epoch #107 | computing descent direction
2021-06-04 13:49:16 | [train_policy] epoch #107 | descent direction computed
2021-06-04 13:49:16 | [train_policy] epoch #107 | backtrack iters: 1
2021-06-04 13:49:16 | [train_policy] epoch #107 | optimization finished
2021-06-04 13:49:16 | [train_policy] epoch #107 | Computing KL after
2021-06-04 13:49:16 | [train_policy] epoch #107 | Computing loss after
2021-06-04 13:49:16 | [train_policy] epoch #107 | Fitting baseline...
2021-06-04 13:49:16 | [train_policy] epoch #107 | Saving snapshot...
2021-06-04 13:49:16 | [train_policy] epoch #107 | Saved
2021-06-04 13:49:16 | [train_policy] epoch #107 | Time 88.69 s
2021-06-04 13:49:16 | [train_policy] epoch #107 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286144
Evaluation/AverageDiscountedReturn          -47.3058
Evaluation/AverageReturn                    -47.3058
Evaluation/CompletionRate                     0
Evaluation/Iteration                        107
Evaluation/MaxReturn                        -36.9328
Evaluation/MinReturn                       -115.693
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.32938
Extras/EpisodeRewardMean                    -47.165
LinearFeatureBaseline/ExplainedVariance     -45.7642
PolicyExecTime                                0.231755
ProcessExecTime                               0.03124
TotalEnvSteps                            109296
policy/Entropy                                1.03221
policy/KL                                     0.00664996
policy/KLBefore                               0
policy/LossAfter                             -0.0181528
policy/LossBefore                             1.64914e-09
policy/Perplexity                             2.80726
policy/dLoss                                  0.0181528
---------------------------------------  ----------------
2021-06-04 13:49:16 | [train_policy] epoch #108 | Obtaining samples for iteration 108...
2021-06-04 13:49:17 | [train_policy] epoch #108 | Logging diagnostics...
2021-06-04 13:49:17 | [train_policy] epoch #108 | Optimizing policy...
2021-06-04 13:49:17 | [train_policy] epoch #108 | Computing loss before
2021-06-04 13:49:17 | [train_policy] epoch #108 | Computing KL before
2021-06-04 13:49:17 | [train_policy] epoch #108 | Optimizing
2021-06-04 13:49:17 | [train_policy] epoch #108 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:17 | [train_policy] epoch #108 | computing loss before
2021-06-04 13:49:17 | [train_policy] epoch #108 | computing gradient
2021-06-04 13:49:17 | [train_policy] epoch #108 | gradient computed
2021-06-04 13:49:17 | [train_policy] epoch #108 | computing descent direction
2021-06-04 13:49:17 | [train_policy] epoch #108 | descent direction computed
2021-06-04 13:49:17 | [train_policy] epoch #108 | backtrack iters: 0
2021-06-04 13:49:17 | [train_policy] epoch #108 | optimization finished
2021-06-04 13:49:17 | [train_policy] epoch #108 | Computing KL after
2021-06-04 13:49:17 | [train_policy] epoch #108 | Computing loss after
2021-06-04 13:49:17 | [train_policy] epoch #108 | Fitting baseline...
2021-06-04 13:49:17 | [train_policy] epoch #108 | Saving snapshot...
2021-06-04 13:49:17 | [train_policy] epoch #108 | Saved
2021-06-04 13:49:17 | [train_policy] epoch #108 | Time 89.50 s
2021-06-04 13:49:17 | [train_policy] epoch #108 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284733
Evaluation/AverageDiscountedReturn          -47.5862
Evaluation/AverageReturn                    -47.5862
Evaluation/CompletionRate                     0
Evaluation/Iteration                        108
Evaluation/MaxReturn                        -37.0933
Evaluation/MinReturn                        -67.458
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.41677
Extras/EpisodeRewardMean                    -47.4622
LinearFeatureBaseline/ExplainedVariance       0.847627
PolicyExecTime                                0.244
ProcessExecTime                               0.031214
TotalEnvSteps                            110308
policy/Entropy                                0.992229
policy/KL                                     0.00998803
policy/KLBefore                               0
policy/LossAfter                             -0.0328588
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             2.69724
policy/dLoss                                  0.0328588
---------------------------------------  ----------------
2021-06-04 13:49:17 | [train_policy] epoch #109 | Obtaining samples for iteration 109...
2021-06-04 13:49:18 | [train_policy] epoch #109 | Logging diagnostics...
2021-06-04 13:49:18 | [train_policy] epoch #109 | Optimizing policy...
2021-06-04 13:49:18 | [train_policy] epoch #109 | Computing loss before
2021-06-04 13:49:18 | [train_policy] epoch #109 | Computing KL before
2021-06-04 13:49:18 | [train_policy] epoch #109 | Optimizing
2021-06-04 13:49:18 | [train_policy] epoch #109 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:18 | [train_policy] epoch #109 | computing loss before
2021-06-04 13:49:18 | [train_policy] epoch #109 | computing gradient
2021-06-04 13:49:18 | [train_policy] epoch #109 | gradient computed
2021-06-04 13:49:18 | [train_policy] epoch #109 | computing descent direction
2021-06-04 13:49:18 | [train_policy] epoch #109 | descent direction computed
2021-06-04 13:49:18 | [train_policy] epoch #109 | backtrack iters: 0
2021-06-04 13:49:18 | [train_policy] epoch #109 | optimization finished
2021-06-04 13:49:18 | [train_policy] epoch #109 | Computing KL after
2021-06-04 13:49:18 | [train_policy] epoch #109 | Computing loss after
2021-06-04 13:49:18 | [train_policy] epoch #109 | Fitting baseline...
2021-06-04 13:49:18 | [train_policy] epoch #109 | Saving snapshot...
2021-06-04 13:49:18 | [train_policy] epoch #109 | Saved
2021-06-04 13:49:18 | [train_policy] epoch #109 | Time 90.27 s
2021-06-04 13:49:18 | [train_policy] epoch #109 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.283689
Evaluation/AverageDiscountedReturn          -49.195
Evaluation/AverageReturn                    -49.195
Evaluation/CompletionRate                     0
Evaluation/Iteration                        109
Evaluation/MaxReturn                        -35.9874
Evaluation/MinReturn                       -177.268
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         16.5418
Extras/EpisodeRewardMean                    -49.1416
LinearFeatureBaseline/ExplainedVariance       0.638079
PolicyExecTime                                0.207828
ProcessExecTime                               0.0310502
TotalEnvSteps                            111320
policy/Entropy                                0.987511
policy/KL                                     0.00969542
policy/KLBefore                               0
policy/LossAfter                             -0.0161423
policy/LossBefore                             1.88473e-09
policy/Perplexity                             2.68454
policy/dLoss                                  0.0161423
---------------------------------------  ----------------
2021-06-04 13:49:18 | [train_policy] epoch #110 | Obtaining samples for iteration 110...
2021-06-04 13:49:19 | [train_policy] epoch #110 | Logging diagnostics...
2021-06-04 13:49:19 | [train_policy] epoch #110 | Optimizing policy...
2021-06-04 13:49:19 | [train_policy] epoch #110 | Computing loss before
2021-06-04 13:49:19 | [train_policy] epoch #110 | Computing KL before
2021-06-04 13:49:19 | [train_policy] epoch #110 | Optimizing
2021-06-04 13:49:19 | [train_policy] epoch #110 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:19 | [train_policy] epoch #110 | computing loss before
2021-06-04 13:49:19 | [train_policy] epoch #110 | computing gradient
2021-06-04 13:49:19 | [train_policy] epoch #110 | gradient computed
2021-06-04 13:49:19 | [train_policy] epoch #110 | computing descent direction
2021-06-04 13:49:19 | [train_policy] epoch #110 | descent direction computed
2021-06-04 13:49:19 | [train_policy] epoch #110 | backtrack iters: 0
2021-06-04 13:49:19 | [train_policy] epoch #110 | optimization finished
2021-06-04 13:49:19 | [train_policy] epoch #110 | Computing KL after
2021-06-04 13:49:19 | [train_policy] epoch #110 | Computing loss after
2021-06-04 13:49:19 | [train_policy] epoch #110 | Fitting baseline...
2021-06-04 13:49:19 | [train_policy] epoch #110 | Saving snapshot...
2021-06-04 13:49:19 | [train_policy] epoch #110 | Saved
2021-06-04 13:49:19 | [train_policy] epoch #110 | Time 91.08 s
2021-06-04 13:49:19 | [train_policy] epoch #110 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.289124
Evaluation/AverageDiscountedReturn          -47.6653
Evaluation/AverageReturn                    -47.6653
Evaluation/CompletionRate                     0
Evaluation/Iteration                        110
Evaluation/MaxReturn                        -35.3029
Evaluation/MinReturn                       -103.099
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.23344
Extras/EpisodeRewardMean                    -47.5931
LinearFeatureBaseline/ExplainedVariance       0.85061
PolicyExecTime                                0.239152
ProcessExecTime                               0.0315959
TotalEnvSteps                            112332
policy/Entropy                                1.00842
policy/KL                                     0.00888978
policy/KLBefore                               0
policy/LossAfter                             -0.0250356
policy/LossBefore                             2.35591e-10
policy/Perplexity                             2.74126
policy/dLoss                                  0.0250356
---------------------------------------  ----------------
2021-06-04 13:49:19 | [train_policy] epoch #111 | Obtaining samples for iteration 111...
2021-06-04 13:49:20 | [train_policy] epoch #111 | Logging diagnostics...
2021-06-04 13:49:20 | [train_policy] epoch #111 | Optimizing policy...
2021-06-04 13:49:20 | [train_policy] epoch #111 | Computing loss before
2021-06-04 13:49:20 | [train_policy] epoch #111 | Computing KL before
2021-06-04 13:49:20 | [train_policy] epoch #111 | Optimizing
2021-06-04 13:49:20 | [train_policy] epoch #111 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:20 | [train_policy] epoch #111 | computing loss before
2021-06-04 13:49:20 | [train_policy] epoch #111 | computing gradient
2021-06-04 13:49:20 | [train_policy] epoch #111 | gradient computed
2021-06-04 13:49:20 | [train_policy] epoch #111 | computing descent direction
2021-06-04 13:49:20 | [train_policy] epoch #111 | descent direction computed
2021-06-04 13:49:20 | [train_policy] epoch #111 | backtrack iters: 0
2021-06-04 13:49:20 | [train_policy] epoch #111 | optimization finished
2021-06-04 13:49:20 | [train_policy] epoch #111 | Computing KL after
2021-06-04 13:49:20 | [train_policy] epoch #111 | Computing loss after
2021-06-04 13:49:20 | [train_policy] epoch #111 | Fitting baseline...
2021-06-04 13:49:20 | [train_policy] epoch #111 | Saving snapshot...
2021-06-04 13:49:20 | [train_policy] epoch #111 | Saved
2021-06-04 13:49:20 | [train_policy] epoch #111 | Time 91.88 s
2021-06-04 13:49:20 | [train_policy] epoch #111 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284957
Evaluation/AverageDiscountedReturn          -48.1205
Evaluation/AverageReturn                    -48.1205
Evaluation/CompletionRate                     0
Evaluation/Iteration                        111
Evaluation/MaxReturn                        -36.8698
Evaluation/MinReturn                        -89.7336
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.36196
Extras/EpisodeRewardMean                    -47.7627
LinearFeatureBaseline/ExplainedVariance       0.907045
PolicyExecTime                                0.23416
ProcessExecTime                               0.0312028
TotalEnvSteps                            113344
policy/Entropy                                1.01284
policy/KL                                     0.00952422
policy/KLBefore                               0
policy/LossAfter                             -0.015899
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             2.75341
policy/dLoss                                  0.015899
---------------------------------------  ----------------
2021-06-04 13:49:20 | [train_policy] epoch #112 | Obtaining samples for iteration 112...
2021-06-04 13:49:20 | [train_policy] epoch #112 | Logging diagnostics...
2021-06-04 13:49:20 | [train_policy] epoch #112 | Optimizing policy...
2021-06-04 13:49:20 | [train_policy] epoch #112 | Computing loss before
2021-06-04 13:49:20 | [train_policy] epoch #112 | Computing KL before
2021-06-04 13:49:20 | [train_policy] epoch #112 | Optimizing
2021-06-04 13:49:20 | [train_policy] epoch #112 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:20 | [train_policy] epoch #112 | computing loss before
2021-06-04 13:49:20 | [train_policy] epoch #112 | computing gradient
2021-06-04 13:49:20 | [train_policy] epoch #112 | gradient computed
2021-06-04 13:49:20 | [train_policy] epoch #112 | computing descent direction
2021-06-04 13:49:20 | [train_policy] epoch #112 | descent direction computed
2021-06-04 13:49:20 | [train_policy] epoch #112 | backtrack iters: 1
2021-06-04 13:49:20 | [train_policy] epoch #112 | optimization finished
2021-06-04 13:49:20 | [train_policy] epoch #112 | Computing KL after
2021-06-04 13:49:20 | [train_policy] epoch #112 | Computing loss after
2021-06-04 13:49:20 | [train_policy] epoch #112 | Fitting baseline...
2021-06-04 13:49:20 | [train_policy] epoch #112 | Saving snapshot...
2021-06-04 13:49:20 | [train_policy] epoch #112 | Saved
2021-06-04 13:49:20 | [train_policy] epoch #112 | Time 92.68 s
2021-06-04 13:49:20 | [train_policy] epoch #112 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288081
Evaluation/AverageDiscountedReturn          -50.1961
Evaluation/AverageReturn                    -50.1961
Evaluation/CompletionRate                     0
Evaluation/Iteration                        112
Evaluation/MaxReturn                        -33.5521
Evaluation/MinReturn                       -335.226
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         31.2635
Extras/EpisodeRewardMean                    -50.0836
LinearFeatureBaseline/ExplainedVariance       0.233878
PolicyExecTime                                0.233032
ProcessExecTime                               0.0314045
TotalEnvSteps                            114356
policy/Entropy                                1.00507
policy/KL                                     0.0064153
policy/KLBefore                               0
policy/LossAfter                             -0.0208224
policy/LossBefore                            -1.64914e-08
policy/Perplexity                             2.73209
policy/dLoss                                  0.0208224
---------------------------------------  ----------------
2021-06-04 13:49:20 | [train_policy] epoch #113 | Obtaining samples for iteration 113...
2021-06-04 13:49:21 | [train_policy] epoch #113 | Logging diagnostics...
2021-06-04 13:49:21 | [train_policy] epoch #113 | Optimizing policy...
2021-06-04 13:49:21 | [train_policy] epoch #113 | Computing loss before
2021-06-04 13:49:21 | [train_policy] epoch #113 | Computing KL before
2021-06-04 13:49:21 | [train_policy] epoch #113 | Optimizing
2021-06-04 13:49:21 | [train_policy] epoch #113 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:21 | [train_policy] epoch #113 | computing loss before
2021-06-04 13:49:21 | [train_policy] epoch #113 | computing gradient
2021-06-04 13:49:21 | [train_policy] epoch #113 | gradient computed
2021-06-04 13:49:21 | [train_policy] epoch #113 | computing descent direction
2021-06-04 13:49:21 | [train_policy] epoch #113 | descent direction computed
2021-06-04 13:49:21 | [train_policy] epoch #113 | backtrack iters: 1
2021-06-04 13:49:21 | [train_policy] epoch #113 | optimization finished
2021-06-04 13:49:21 | [train_policy] epoch #113 | Computing KL after
2021-06-04 13:49:21 | [train_policy] epoch #113 | Computing loss after
2021-06-04 13:49:21 | [train_policy] epoch #113 | Fitting baseline...
2021-06-04 13:49:21 | [train_policy] epoch #113 | Saving snapshot...
2021-06-04 13:49:21 | [train_policy] epoch #113 | Saved
2021-06-04 13:49:21 | [train_policy] epoch #113 | Time 93.48 s
2021-06-04 13:49:21 | [train_policy] epoch #113 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285241
Evaluation/AverageDiscountedReturn          -47.7012
Evaluation/AverageReturn                    -47.7012
Evaluation/CompletionRate                     0
Evaluation/Iteration                        113
Evaluation/MaxReturn                        -35.6141
Evaluation/MinReturn                        -97.7347
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.64144
Extras/EpisodeRewardMean                    -48.1015
LinearFeatureBaseline/ExplainedVariance       0.429971
PolicyExecTime                                0.225101
ProcessExecTime                               0.0312269
TotalEnvSteps                            115368
policy/Entropy                                0.996868
policy/KL                                     0.00641884
policy/KLBefore                               0
policy/LossAfter                             -0.0192789
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             2.70978
policy/dLoss                                  0.0192788
---------------------------------------  ----------------
2021-06-04 13:49:21 | [train_policy] epoch #114 | Obtaining samples for iteration 114...
2021-06-04 13:49:22 | [train_policy] epoch #114 | Logging diagnostics...
2021-06-04 13:49:22 | [train_policy] epoch #114 | Optimizing policy...
2021-06-04 13:49:22 | [train_policy] epoch #114 | Computing loss before
2021-06-04 13:49:22 | [train_policy] epoch #114 | Computing KL before
2021-06-04 13:49:22 | [train_policy] epoch #114 | Optimizing
2021-06-04 13:49:22 | [train_policy] epoch #114 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:22 | [train_policy] epoch #114 | computing loss before
2021-06-04 13:49:22 | [train_policy] epoch #114 | computing gradient
2021-06-04 13:49:22 | [train_policy] epoch #114 | gradient computed
2021-06-04 13:49:22 | [train_policy] epoch #114 | computing descent direction
2021-06-04 13:49:22 | [train_policy] epoch #114 | descent direction computed
2021-06-04 13:49:22 | [train_policy] epoch #114 | backtrack iters: 1
2021-06-04 13:49:22 | [train_policy] epoch #114 | optimization finished
2021-06-04 13:49:22 | [train_policy] epoch #114 | Computing KL after
2021-06-04 13:49:22 | [train_policy] epoch #114 | Computing loss after
2021-06-04 13:49:22 | [train_policy] epoch #114 | Fitting baseline...
2021-06-04 13:49:22 | [train_policy] epoch #114 | Saving snapshot...
2021-06-04 13:49:22 | [train_policy] epoch #114 | Saved
2021-06-04 13:49:22 | [train_policy] epoch #114 | Time 94.28 s
2021-06-04 13:49:22 | [train_policy] epoch #114 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285291
Evaluation/AverageDiscountedReturn          -47.3602
Evaluation/AverageReturn                    -47.3602
Evaluation/CompletionRate                     0
Evaluation/Iteration                        114
Evaluation/MaxReturn                        -36.985
Evaluation/MinReturn                        -89.4296
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.25898
Extras/EpisodeRewardMean                    -47.5267
LinearFeatureBaseline/ExplainedVariance       0.885067
PolicyExecTime                                0.228894
ProcessExecTime                               0.0311477
TotalEnvSteps                            116380
policy/Entropy                                0.995638
policy/KL                                     0.00685264
policy/KLBefore                               0
policy/LossAfter                             -0.0167578
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             2.70645
policy/dLoss                                  0.0167578
---------------------------------------  ----------------
2021-06-04 13:49:22 | [train_policy] epoch #115 | Obtaining samples for iteration 115...
2021-06-04 13:49:23 | [train_policy] epoch #115 | Logging diagnostics...
2021-06-04 13:49:23 | [train_policy] epoch #115 | Optimizing policy...
2021-06-04 13:49:23 | [train_policy] epoch #115 | Computing loss before
2021-06-04 13:49:23 | [train_policy] epoch #115 | Computing KL before
2021-06-04 13:49:23 | [train_policy] epoch #115 | Optimizing
2021-06-04 13:49:23 | [train_policy] epoch #115 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:23 | [train_policy] epoch #115 | computing loss before
2021-06-04 13:49:23 | [train_policy] epoch #115 | computing gradient
2021-06-04 13:49:23 | [train_policy] epoch #115 | gradient computed
2021-06-04 13:49:23 | [train_policy] epoch #115 | computing descent direction
2021-06-04 13:49:23 | [train_policy] epoch #115 | descent direction computed
2021-06-04 13:49:23 | [train_policy] epoch #115 | backtrack iters: 1
2021-06-04 13:49:23 | [train_policy] epoch #115 | optimization finished
2021-06-04 13:49:23 | [train_policy] epoch #115 | Computing KL after
2021-06-04 13:49:23 | [train_policy] epoch #115 | Computing loss after
2021-06-04 13:49:23 | [train_policy] epoch #115 | Fitting baseline...
2021-06-04 13:49:23 | [train_policy] epoch #115 | Saving snapshot...
2021-06-04 13:49:23 | [train_policy] epoch #115 | Saved
2021-06-04 13:49:23 | [train_policy] epoch #115 | Time 95.09 s
2021-06-04 13:49:23 | [train_policy] epoch #115 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.285775
Evaluation/AverageDiscountedReturn          -90.8776
Evaluation/AverageReturn                    -90.8776
Evaluation/CompletionRate                     0
Evaluation/Iteration                        115
Evaluation/MaxReturn                        -34.3465
Evaluation/MinReturn                      -2061.37
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        293.807
Extras/EpisodeRewardMean                    -87.4545
LinearFeatureBaseline/ExplainedVariance       0.0110277
PolicyExecTime                                0.234417
ProcessExecTime                               0.0312037
TotalEnvSteps                            117392
policy/Entropy                                0.977488
policy/KL                                     0.00751922
policy/KLBefore                               0
policy/LossAfter                             -0.0262491
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             2.65777
policy/dLoss                                  0.0262491
---------------------------------------  ---------------
2021-06-04 13:49:23 | [train_policy] epoch #116 | Obtaining samples for iteration 116...
2021-06-04 13:49:24 | [train_policy] epoch #116 | Logging diagnostics...
2021-06-04 13:49:24 | [train_policy] epoch #116 | Optimizing policy...
2021-06-04 13:49:24 | [train_policy] epoch #116 | Computing loss before
2021-06-04 13:49:24 | [train_policy] epoch #116 | Computing KL before
2021-06-04 13:49:24 | [train_policy] epoch #116 | Optimizing
2021-06-04 13:49:24 | [train_policy] epoch #116 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:24 | [train_policy] epoch #116 | computing loss before
2021-06-04 13:49:24 | [train_policy] epoch #116 | computing gradient
2021-06-04 13:49:24 | [train_policy] epoch #116 | gradient computed
2021-06-04 13:49:24 | [train_policy] epoch #116 | computing descent direction
2021-06-04 13:49:24 | [train_policy] epoch #116 | descent direction computed
2021-06-04 13:49:24 | [train_policy] epoch #116 | backtrack iters: 1
2021-06-04 13:49:24 | [train_policy] epoch #116 | optimization finished
2021-06-04 13:49:24 | [train_policy] epoch #116 | Computing KL after
2021-06-04 13:49:24 | [train_policy] epoch #116 | Computing loss after
2021-06-04 13:49:24 | [train_policy] epoch #116 | Fitting baseline...
2021-06-04 13:49:24 | [train_policy] epoch #116 | Saving snapshot...
2021-06-04 13:49:24 | [train_policy] epoch #116 | Saved
2021-06-04 13:49:24 | [train_policy] epoch #116 | Time 95.88 s
2021-06-04 13:49:24 | [train_policy] epoch #116 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285058
Evaluation/AverageDiscountedReturn          -47.128
Evaluation/AverageReturn                    -47.128
Evaluation/CompletionRate                     0
Evaluation/Iteration                        116
Evaluation/MaxReturn                        -33.302
Evaluation/MinReturn                        -95.1863
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.32258
Extras/EpisodeRewardMean                    -47.245
LinearFeatureBaseline/ExplainedVariance     -72.3356
PolicyExecTime                                0.210671
ProcessExecTime                               0.0312445
TotalEnvSteps                            118404
policy/Entropy                                0.975664
policy/KL                                     0.00665107
policy/KLBefore                               0
policy/LossAfter                             -0.0182058
policy/LossBefore                            -7.24444e-09
policy/Perplexity                             2.65293
policy/dLoss                                  0.0182058
---------------------------------------  ----------------
2021-06-04 13:49:24 | [train_policy] epoch #117 | Obtaining samples for iteration 117...
2021-06-04 13:49:24 | [train_policy] epoch #117 | Logging diagnostics...
2021-06-04 13:49:24 | [train_policy] epoch #117 | Optimizing policy...
2021-06-04 13:49:24 | [train_policy] epoch #117 | Computing loss before
2021-06-04 13:49:24 | [train_policy] epoch #117 | Computing KL before
2021-06-04 13:49:24 | [train_policy] epoch #117 | Optimizing
2021-06-04 13:49:24 | [train_policy] epoch #117 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:24 | [train_policy] epoch #117 | computing loss before
2021-06-04 13:49:24 | [train_policy] epoch #117 | computing gradient
2021-06-04 13:49:24 | [train_policy] epoch #117 | gradient computed
2021-06-04 13:49:24 | [train_policy] epoch #117 | computing descent direction
2021-06-04 13:49:24 | [train_policy] epoch #117 | descent direction computed
2021-06-04 13:49:24 | [train_policy] epoch #117 | backtrack iters: 0
2021-06-04 13:49:24 | [train_policy] epoch #117 | optimization finished
2021-06-04 13:49:24 | [train_policy] epoch #117 | Computing KL after
2021-06-04 13:49:24 | [train_policy] epoch #117 | Computing loss after
2021-06-04 13:49:24 | [train_policy] epoch #117 | Fitting baseline...
2021-06-04 13:49:24 | [train_policy] epoch #117 | Saving snapshot...
2021-06-04 13:49:24 | [train_policy] epoch #117 | Saved
2021-06-04 13:49:24 | [train_policy] epoch #117 | Time 96.66 s
2021-06-04 13:49:24 | [train_policy] epoch #117 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283835
Evaluation/AverageDiscountedReturn          -46.3079
Evaluation/AverageReturn                    -46.3079
Evaluation/CompletionRate                     0
Evaluation/Iteration                        117
Evaluation/MaxReturn                        -34.5347
Evaluation/MinReturn                        -86.4234
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.26978
Extras/EpisodeRewardMean                    -46.2804
LinearFeatureBaseline/ExplainedVariance       0.920628
PolicyExecTime                                0.22029
ProcessExecTime                               0.0310161
TotalEnvSteps                            119416
policy/Entropy                                1.01875
policy/KL                                     0.00879891
policy/KLBefore                               0
policy/LossAfter                             -0.0162476
policy/LossBefore                             1.17796e-09
policy/Perplexity                             2.76974
policy/dLoss                                  0.0162476
---------------------------------------  ----------------
2021-06-04 13:49:24 | [train_policy] epoch #118 | Obtaining samples for iteration 118...
2021-06-04 13:49:25 | [train_policy] epoch #118 | Logging diagnostics...
2021-06-04 13:49:25 | [train_policy] epoch #118 | Optimizing policy...
2021-06-04 13:49:25 | [train_policy] epoch #118 | Computing loss before
2021-06-04 13:49:25 | [train_policy] epoch #118 | Computing KL before
2021-06-04 13:49:25 | [train_policy] epoch #118 | Optimizing
2021-06-04 13:49:25 | [train_policy] epoch #118 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:25 | [train_policy] epoch #118 | computing loss before
2021-06-04 13:49:25 | [train_policy] epoch #118 | computing gradient
2021-06-04 13:49:25 | [train_policy] epoch #118 | gradient computed
2021-06-04 13:49:25 | [train_policy] epoch #118 | computing descent direction
2021-06-04 13:49:25 | [train_policy] epoch #118 | descent direction computed
2021-06-04 13:49:25 | [train_policy] epoch #118 | backtrack iters: 1
2021-06-04 13:49:25 | [train_policy] epoch #118 | optimization finished
2021-06-04 13:49:25 | [train_policy] epoch #118 | Computing KL after
2021-06-04 13:49:25 | [train_policy] epoch #118 | Computing loss after
2021-06-04 13:49:25 | [train_policy] epoch #118 | Fitting baseline...
2021-06-04 13:49:25 | [train_policy] epoch #118 | Saving snapshot...
2021-06-04 13:49:25 | [train_policy] epoch #118 | Saved
2021-06-04 13:49:25 | [train_policy] epoch #118 | Time 97.46 s
2021-06-04 13:49:25 | [train_policy] epoch #118 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285349
Evaluation/AverageDiscountedReturn          -46.7039
Evaluation/AverageReturn                    -46.7039
Evaluation/CompletionRate                     0
Evaluation/Iteration                        118
Evaluation/MaxReturn                        -37.4385
Evaluation/MinReturn                        -78.7116
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.15206
Extras/EpisodeRewardMean                    -46.5115
LinearFeatureBaseline/ExplainedVariance       0.932581
PolicyExecTime                                0.228505
ProcessExecTime                               0.0311525
TotalEnvSteps                            120428
policy/Entropy                                1.00417
policy/KL                                     0.00670042
policy/KLBefore                               0
policy/LossAfter                             -0.0146616
policy/LossBefore                             1.13084e-08
policy/Perplexity                             2.72964
policy/dLoss                                  0.0146616
---------------------------------------  ----------------
2021-06-04 13:49:25 | [train_policy] epoch #119 | Obtaining samples for iteration 119...
2021-06-04 13:49:26 | [train_policy] epoch #119 | Logging diagnostics...
2021-06-04 13:49:26 | [train_policy] epoch #119 | Optimizing policy...
2021-06-04 13:49:26 | [train_policy] epoch #119 | Computing loss before
2021-06-04 13:49:26 | [train_policy] epoch #119 | Computing KL before
2021-06-04 13:49:26 | [train_policy] epoch #119 | Optimizing
2021-06-04 13:49:26 | [train_policy] epoch #119 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:26 | [train_policy] epoch #119 | computing loss before
2021-06-04 13:49:26 | [train_policy] epoch #119 | computing gradient
2021-06-04 13:49:26 | [train_policy] epoch #119 | gradient computed
2021-06-04 13:49:26 | [train_policy] epoch #119 | computing descent direction
2021-06-04 13:49:26 | [train_policy] epoch #119 | descent direction computed
2021-06-04 13:49:26 | [train_policy] epoch #119 | backtrack iters: 1
2021-06-04 13:49:26 | [train_policy] epoch #119 | optimization finished
2021-06-04 13:49:26 | [train_policy] epoch #119 | Computing KL after
2021-06-04 13:49:26 | [train_policy] epoch #119 | Computing loss after
2021-06-04 13:49:26 | [train_policy] epoch #119 | Fitting baseline...
2021-06-04 13:49:26 | [train_policy] epoch #119 | Saving snapshot...
2021-06-04 13:49:26 | [train_policy] epoch #119 | Saved
2021-06-04 13:49:26 | [train_policy] epoch #119 | Time 98.27 s
2021-06-04 13:49:26 | [train_policy] epoch #119 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286936
Evaluation/AverageDiscountedReturn          -47.1229
Evaluation/AverageReturn                    -47.1229
Evaluation/CompletionRate                     0
Evaluation/Iteration                        119
Evaluation/MaxReturn                        -37.2293
Evaluation/MinReturn                        -64.6948
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.0855
Extras/EpisodeRewardMean                    -47.1283
LinearFeatureBaseline/ExplainedVariance       0.948895
PolicyExecTime                                0.233786
ProcessExecTime                               0.031424
TotalEnvSteps                            121440
policy/Entropy                                1.00284
policy/KL                                     0.00672092
policy/KLBefore                               0
policy/LossAfter                             -0.0188964
policy/LossBefore                            -2.34414e-08
policy/Perplexity                             2.72602
policy/dLoss                                  0.0188963
---------------------------------------  ----------------
2021-06-04 13:49:26 | [train_policy] epoch #120 | Obtaining samples for iteration 120...
2021-06-04 13:49:27 | [train_policy] epoch #120 | Logging diagnostics...
2021-06-04 13:49:27 | [train_policy] epoch #120 | Optimizing policy...
2021-06-04 13:49:27 | [train_policy] epoch #120 | Computing loss before
2021-06-04 13:49:27 | [train_policy] epoch #120 | Computing KL before
2021-06-04 13:49:27 | [train_policy] epoch #120 | Optimizing
2021-06-04 13:49:27 | [train_policy] epoch #120 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:27 | [train_policy] epoch #120 | computing loss before
2021-06-04 13:49:27 | [train_policy] epoch #120 | computing gradient
2021-06-04 13:49:27 | [train_policy] epoch #120 | gradient computed
2021-06-04 13:49:27 | [train_policy] epoch #120 | computing descent direction
2021-06-04 13:49:27 | [train_policy] epoch #120 | descent direction computed
2021-06-04 13:49:27 | [train_policy] epoch #120 | backtrack iters: 1
2021-06-04 13:49:27 | [train_policy] epoch #120 | optimization finished
2021-06-04 13:49:27 | [train_policy] epoch #120 | Computing KL after
2021-06-04 13:49:27 | [train_policy] epoch #120 | Computing loss after
2021-06-04 13:49:27 | [train_policy] epoch #120 | Fitting baseline...
2021-06-04 13:49:27 | [train_policy] epoch #120 | Saving snapshot...
2021-06-04 13:49:27 | [train_policy] epoch #120 | Saved
2021-06-04 13:49:27 | [train_policy] epoch #120 | Time 99.07 s
2021-06-04 13:49:27 | [train_policy] epoch #120 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284725
Evaluation/AverageDiscountedReturn          -47.3174
Evaluation/AverageReturn                    -47.3174
Evaluation/CompletionRate                     0
Evaluation/Iteration                        120
Evaluation/MaxReturn                        -36.7286
Evaluation/MinReturn                       -103.617
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.67303
Extras/EpisodeRewardMean                    -47.2425
LinearFeatureBaseline/ExplainedVariance       0.847005
PolicyExecTime                                0.227582
ProcessExecTime                               0.0312183
TotalEnvSteps                            122452
policy/Entropy                                1.00395
policy/KL                                     0.00698415
policy/KLBefore                               0
policy/LossAfter                             -0.0206264
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             2.72905
policy/dLoss                                  0.0206264
---------------------------------------  ----------------
2021-06-04 13:49:27 | [train_policy] epoch #121 | Obtaining samples for iteration 121...
2021-06-04 13:49:28 | [train_policy] epoch #121 | Logging diagnostics...
2021-06-04 13:49:28 | [train_policy] epoch #121 | Optimizing policy...
2021-06-04 13:49:28 | [train_policy] epoch #121 | Computing loss before
2021-06-04 13:49:28 | [train_policy] epoch #121 | Computing KL before
2021-06-04 13:49:28 | [train_policy] epoch #121 | Optimizing
2021-06-04 13:49:28 | [train_policy] epoch #121 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:28 | [train_policy] epoch #121 | computing loss before
2021-06-04 13:49:28 | [train_policy] epoch #121 | computing gradient
2021-06-04 13:49:28 | [train_policy] epoch #121 | gradient computed
2021-06-04 13:49:28 | [train_policy] epoch #121 | computing descent direction
2021-06-04 13:49:28 | [train_policy] epoch #121 | descent direction computed
2021-06-04 13:49:28 | [train_policy] epoch #121 | backtrack iters: 1
2021-06-04 13:49:28 | [train_policy] epoch #121 | optimization finished
2021-06-04 13:49:28 | [train_policy] epoch #121 | Computing KL after
2021-06-04 13:49:28 | [train_policy] epoch #121 | Computing loss after
2021-06-04 13:49:28 | [train_policy] epoch #121 | Fitting baseline...
2021-06-04 13:49:28 | [train_policy] epoch #121 | Saving snapshot...
2021-06-04 13:49:28 | [train_policy] epoch #121 | Saved
2021-06-04 13:49:28 | [train_policy] epoch #121 | Time 99.88 s
2021-06-04 13:49:28 | [train_policy] epoch #121 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287349
Evaluation/AverageDiscountedReturn          -71.7816
Evaluation/AverageReturn                    -71.7816
Evaluation/CompletionRate                     0
Evaluation/Iteration                        121
Evaluation/MaxReturn                        -37.3706
Evaluation/MinReturn                      -2061.59
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.624
Extras/EpisodeRewardMean                    -70.2221
LinearFeatureBaseline/ExplainedVariance       0.0149599
PolicyExecTime                                0.227963
ProcessExecTime                               0.0314307
TotalEnvSteps                            123464
policy/Entropy                                1.01065
policy/KL                                     0.00718012
policy/KLBefore                               0
policy/LossAfter                             -0.0169918
policy/LossBefore                             6.12538e-09
policy/Perplexity                             2.74739
policy/dLoss                                  0.0169918
---------------------------------------  ----------------
2021-06-04 13:49:28 | [train_policy] epoch #122 | Obtaining samples for iteration 122...
2021-06-04 13:49:28 | [train_policy] epoch #122 | Logging diagnostics...
2021-06-04 13:49:28 | [train_policy] epoch #122 | Optimizing policy...
2021-06-04 13:49:28 | [train_policy] epoch #122 | Computing loss before
2021-06-04 13:49:28 | [train_policy] epoch #122 | Computing KL before
2021-06-04 13:49:28 | [train_policy] epoch #122 | Optimizing
2021-06-04 13:49:28 | [train_policy] epoch #122 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:28 | [train_policy] epoch #122 | computing loss before
2021-06-04 13:49:28 | [train_policy] epoch #122 | computing gradient
2021-06-04 13:49:28 | [train_policy] epoch #122 | gradient computed
2021-06-04 13:49:28 | [train_policy] epoch #122 | computing descent direction
2021-06-04 13:49:28 | [train_policy] epoch #122 | descent direction computed
2021-06-04 13:49:28 | [train_policy] epoch #122 | backtrack iters: 1
2021-06-04 13:49:28 | [train_policy] epoch #122 | optimization finished
2021-06-04 13:49:28 | [train_policy] epoch #122 | Computing KL after
2021-06-04 13:49:28 | [train_policy] epoch #122 | Computing loss after
2021-06-04 13:49:28 | [train_policy] epoch #122 | Fitting baseline...
2021-06-04 13:49:28 | [train_policy] epoch #122 | Saving snapshot...
2021-06-04 13:49:28 | [train_policy] epoch #122 | Saved
2021-06-04 13:49:28 | [train_policy] epoch #122 | Time 100.68 s
2021-06-04 13:49:28 | [train_policy] epoch #122 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284565
Evaluation/AverageDiscountedReturn          -45.7456
Evaluation/AverageReturn                    -45.7456
Evaluation/CompletionRate                     0
Evaluation/Iteration                        122
Evaluation/MaxReturn                        -34.0547
Evaluation/MinReturn                        -76.335
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.47955
Extras/EpisodeRewardMean                    -48.3555
LinearFeatureBaseline/ExplainedVariance     -10.901
PolicyExecTime                                0.229913
ProcessExecTime                               0.0311475
TotalEnvSteps                            124476
policy/Entropy                                1.00588
policy/KL                                     0.00643276
policy/KLBefore                               0
policy/LossAfter                             -0.0256452
policy/LossBefore                             3.76946e-09
policy/Perplexity                             2.73432
policy/dLoss                                  0.0256452
---------------------------------------  ----------------
2021-06-04 13:49:28 | [train_policy] epoch #123 | Obtaining samples for iteration 123...
2021-06-04 13:49:29 | [train_policy] epoch #123 | Logging diagnostics...
2021-06-04 13:49:29 | [train_policy] epoch #123 | Optimizing policy...
2021-06-04 13:49:29 | [train_policy] epoch #123 | Computing loss before
2021-06-04 13:49:29 | [train_policy] epoch #123 | Computing KL before
2021-06-04 13:49:29 | [train_policy] epoch #123 | Optimizing
2021-06-04 13:49:29 | [train_policy] epoch #123 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:29 | [train_policy] epoch #123 | computing loss before
2021-06-04 13:49:29 | [train_policy] epoch #123 | computing gradient
2021-06-04 13:49:29 | [train_policy] epoch #123 | gradient computed
2021-06-04 13:49:29 | [train_policy] epoch #123 | computing descent direction
2021-06-04 13:49:29 | [train_policy] epoch #123 | descent direction computed
2021-06-04 13:49:29 | [train_policy] epoch #123 | backtrack iters: 1
2021-06-04 13:49:29 | [train_policy] epoch #123 | optimization finished
2021-06-04 13:49:29 | [train_policy] epoch #123 | Computing KL after
2021-06-04 13:49:29 | [train_policy] epoch #123 | Computing loss after
2021-06-04 13:49:29 | [train_policy] epoch #123 | Fitting baseline...
2021-06-04 13:49:29 | [train_policy] epoch #123 | Saving snapshot...
2021-06-04 13:49:29 | [train_policy] epoch #123 | Saved
2021-06-04 13:49:29 | [train_policy] epoch #123 | Time 101.48 s
2021-06-04 13:49:29 | [train_policy] epoch #123 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284437
Evaluation/AverageDiscountedReturn          -48.6146
Evaluation/AverageReturn                    -48.6146
Evaluation/CompletionRate                     0
Evaluation/Iteration                        123
Evaluation/MaxReturn                        -35.7859
Evaluation/MinReturn                        -92.0986
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.33285
Extras/EpisodeRewardMean                    -48.2438
LinearFeatureBaseline/ExplainedVariance       0.888554
PolicyExecTime                                0.221962
ProcessExecTime                               0.0310912
TotalEnvSteps                            125488
policy/Entropy                                0.985658
policy/KL                                     0.00666372
policy/KLBefore                               0
policy/LossAfter                             -0.0154644
policy/LossBefore                            -7.77452e-09
policy/Perplexity                             2.67957
policy/dLoss                                  0.0154644
---------------------------------------  ----------------
2021-06-04 13:49:29 | [train_policy] epoch #124 | Obtaining samples for iteration 124...
2021-06-04 13:49:30 | [train_policy] epoch #124 | Logging diagnostics...
2021-06-04 13:49:30 | [train_policy] epoch #124 | Optimizing policy...
2021-06-04 13:49:30 | [train_policy] epoch #124 | Computing loss before
2021-06-04 13:49:30 | [train_policy] epoch #124 | Computing KL before
2021-06-04 13:49:30 | [train_policy] epoch #124 | Optimizing
2021-06-04 13:49:30 | [train_policy] epoch #124 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:30 | [train_policy] epoch #124 | computing loss before
2021-06-04 13:49:30 | [train_policy] epoch #124 | computing gradient
2021-06-04 13:49:30 | [train_policy] epoch #124 | gradient computed
2021-06-04 13:49:30 | [train_policy] epoch #124 | computing descent direction
2021-06-04 13:49:30 | [train_policy] epoch #124 | descent direction computed
2021-06-04 13:49:30 | [train_policy] epoch #124 | backtrack iters: 0
2021-06-04 13:49:30 | [train_policy] epoch #124 | optimization finished
2021-06-04 13:49:30 | [train_policy] epoch #124 | Computing KL after
2021-06-04 13:49:30 | [train_policy] epoch #124 | Computing loss after
2021-06-04 13:49:30 | [train_policy] epoch #124 | Fitting baseline...
2021-06-04 13:49:30 | [train_policy] epoch #124 | Saving snapshot...
2021-06-04 13:49:30 | [train_policy] epoch #124 | Saved
2021-06-04 13:49:30 | [train_policy] epoch #124 | Time 102.26 s
2021-06-04 13:49:30 | [train_policy] epoch #124 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.285244
Evaluation/AverageDiscountedReturn          -46.2757
Evaluation/AverageReturn                    -46.2757
Evaluation/CompletionRate                     0
Evaluation/Iteration                        124
Evaluation/MaxReturn                        -34.9002
Evaluation/MinReturn                        -78.3537
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.56835
Extras/EpisodeRewardMean                    -46.609
LinearFeatureBaseline/ExplainedVariance       0.923395
PolicyExecTime                                0.212925
ProcessExecTime                               0.0312293
TotalEnvSteps                            126500
policy/Entropy                                0.976871
policy/KL                                     0.0099948
policy/KLBefore                               0
policy/LossAfter                             -0.0250721
policy/LossBefore                            -1.12495e-08
policy/Perplexity                             2.65613
policy/dLoss                                  0.0250721
---------------------------------------  ----------------
2021-06-04 13:49:30 | [train_policy] epoch #125 | Obtaining samples for iteration 125...
2021-06-04 13:49:31 | [train_policy] epoch #125 | Logging diagnostics...
2021-06-04 13:49:31 | [train_policy] epoch #125 | Optimizing policy...
2021-06-04 13:49:31 | [train_policy] epoch #125 | Computing loss before
2021-06-04 13:49:31 | [train_policy] epoch #125 | Computing KL before
2021-06-04 13:49:31 | [train_policy] epoch #125 | Optimizing
2021-06-04 13:49:31 | [train_policy] epoch #125 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:31 | [train_policy] epoch #125 | computing loss before
2021-06-04 13:49:31 | [train_policy] epoch #125 | computing gradient
2021-06-04 13:49:31 | [train_policy] epoch #125 | gradient computed
2021-06-04 13:49:31 | [train_policy] epoch #125 | computing descent direction
2021-06-04 13:49:31 | [train_policy] epoch #125 | descent direction computed
2021-06-04 13:49:31 | [train_policy] epoch #125 | backtrack iters: 1
2021-06-04 13:49:31 | [train_policy] epoch #125 | optimization finished
2021-06-04 13:49:31 | [train_policy] epoch #125 | Computing KL after
2021-06-04 13:49:31 | [train_policy] epoch #125 | Computing loss after
2021-06-04 13:49:31 | [train_policy] epoch #125 | Fitting baseline...
2021-06-04 13:49:31 | [train_policy] epoch #125 | Saving snapshot...
2021-06-04 13:49:31 | [train_policy] epoch #125 | Saved
2021-06-04 13:49:31 | [train_policy] epoch #125 | Time 103.06 s
2021-06-04 13:49:31 | [train_policy] epoch #125 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.284224
Evaluation/AverageDiscountedReturn          -47.1295
Evaluation/AverageReturn                    -47.1295
Evaluation/CompletionRate                     0
Evaluation/Iteration                        125
Evaluation/MaxReturn                        -33.4576
Evaluation/MinReturn                        -79.1161
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.73988
Extras/EpisodeRewardMean                    -47.036
LinearFeatureBaseline/ExplainedVariance       0.942362
PolicyExecTime                                0.230379
ProcessExecTime                               0.0311043
TotalEnvSteps                            127512
policy/Entropy                                0.932879
policy/KL                                     0.00677525
policy/KLBefore                               0
policy/LossAfter                             -0.0181863
policy/LossBefore                             2.8271e-09
policy/Perplexity                             2.54182
policy/dLoss                                  0.0181863
---------------------------------------  ---------------
2021-06-04 13:49:31 | [train_policy] epoch #126 | Obtaining samples for iteration 126...
2021-06-04 13:49:31 | [train_policy] epoch #126 | Logging diagnostics...
2021-06-04 13:49:31 | [train_policy] epoch #126 | Optimizing policy...
2021-06-04 13:49:31 | [train_policy] epoch #126 | Computing loss before
2021-06-04 13:49:31 | [train_policy] epoch #126 | Computing KL before
2021-06-04 13:49:31 | [train_policy] epoch #126 | Optimizing
2021-06-04 13:49:31 | [train_policy] epoch #126 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:31 | [train_policy] epoch #126 | computing loss before
2021-06-04 13:49:31 | [train_policy] epoch #126 | computing gradient
2021-06-04 13:49:31 | [train_policy] epoch #126 | gradient computed
2021-06-04 13:49:31 | [train_policy] epoch #126 | computing descent direction
2021-06-04 13:49:32 | [train_policy] epoch #126 | descent direction computed
2021-06-04 13:49:32 | [train_policy] epoch #126 | backtrack iters: 1
2021-06-04 13:49:32 | [train_policy] epoch #126 | optimization finished
2021-06-04 13:49:32 | [train_policy] epoch #126 | Computing KL after
2021-06-04 13:49:32 | [train_policy] epoch #126 | Computing loss after
2021-06-04 13:49:32 | [train_policy] epoch #126 | Fitting baseline...
2021-06-04 13:49:32 | [train_policy] epoch #126 | Saving snapshot...
2021-06-04 13:49:32 | [train_policy] epoch #126 | Saved
2021-06-04 13:49:32 | [train_policy] epoch #126 | Time 103.85 s
2021-06-04 13:49:32 | [train_policy] epoch #126 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288487
Evaluation/AverageDiscountedReturn          -46.0966
Evaluation/AverageReturn                    -46.0966
Evaluation/CompletionRate                     0
Evaluation/Iteration                        126
Evaluation/MaxReturn                        -33.7869
Evaluation/MinReturn                        -75.3972
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.87637
Extras/EpisodeRewardMean                    -46.2497
LinearFeatureBaseline/ExplainedVariance       0.927543
PolicyExecTime                                0.226085
ProcessExecTime                               0.0314946
TotalEnvSteps                            128524
policy/Entropy                                0.930785
policy/KL                                     0.00648672
policy/KLBefore                               0
policy/LossAfter                             -0.0135233
policy/LossBefore                            -7.77452e-09
policy/Perplexity                             2.5365
policy/dLoss                                  0.0135233
---------------------------------------  ----------------
2021-06-04 13:49:32 | [train_policy] epoch #127 | Obtaining samples for iteration 127...
2021-06-04 13:49:32 | [train_policy] epoch #127 | Logging diagnostics...
2021-06-04 13:49:32 | [train_policy] epoch #127 | Optimizing policy...
2021-06-04 13:49:32 | [train_policy] epoch #127 | Computing loss before
2021-06-04 13:49:32 | [train_policy] epoch #127 | Computing KL before
2021-06-04 13:49:32 | [train_policy] epoch #127 | Optimizing
2021-06-04 13:49:32 | [train_policy] epoch #127 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:32 | [train_policy] epoch #127 | computing loss before
2021-06-04 13:49:32 | [train_policy] epoch #127 | computing gradient
2021-06-04 13:49:32 | [train_policy] epoch #127 | gradient computed
2021-06-04 13:49:32 | [train_policy] epoch #127 | computing descent direction
2021-06-04 13:49:32 | [train_policy] epoch #127 | descent direction computed
2021-06-04 13:49:32 | [train_policy] epoch #127 | backtrack iters: 0
2021-06-04 13:49:32 | [train_policy] epoch #127 | optimization finished
2021-06-04 13:49:32 | [train_policy] epoch #127 | Computing KL after
2021-06-04 13:49:32 | [train_policy] epoch #127 | Computing loss after
2021-06-04 13:49:32 | [train_policy] epoch #127 | Fitting baseline...
2021-06-04 13:49:32 | [train_policy] epoch #127 | Saving snapshot...
2021-06-04 13:49:32 | [train_policy] epoch #127 | Saved
2021-06-04 13:49:32 | [train_policy] epoch #127 | Time 104.64 s
2021-06-04 13:49:32 | [train_policy] epoch #127 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.286296
Evaluation/AverageDiscountedReturn          -45.3598
Evaluation/AverageReturn                    -45.3598
Evaluation/CompletionRate                     0
Evaluation/Iteration                        127
Evaluation/MaxReturn                        -33.0009
Evaluation/MinReturn                        -60.3604
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.43981
Extras/EpisodeRewardMean                    -45.4755
LinearFeatureBaseline/ExplainedVariance       0.945626
PolicyExecTime                                0.223288
ProcessExecTime                               0.0312927
TotalEnvSteps                            129536
policy/Entropy                                0.938049
policy/KL                                     0.00985088
policy/KLBefore                               0
policy/LossAfter                             -0.0183165
policy/LossBefore                            -8.01011e-09
policy/Perplexity                             2.55499
policy/dLoss                                  0.0183165
---------------------------------------  ----------------
2021-06-04 13:49:32 | [train_policy] epoch #128 | Obtaining samples for iteration 128...
2021-06-04 13:49:33 | [train_policy] epoch #128 | Logging diagnostics...
2021-06-04 13:49:33 | [train_policy] epoch #128 | Optimizing policy...
2021-06-04 13:49:33 | [train_policy] epoch #128 | Computing loss before
2021-06-04 13:49:33 | [train_policy] epoch #128 | Computing KL before
2021-06-04 13:49:33 | [train_policy] epoch #128 | Optimizing
2021-06-04 13:49:33 | [train_policy] epoch #128 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:33 | [train_policy] epoch #128 | computing loss before
2021-06-04 13:49:33 | [train_policy] epoch #128 | computing gradient
2021-06-04 13:49:33 | [train_policy] epoch #128 | gradient computed
2021-06-04 13:49:33 | [train_policy] epoch #128 | computing descent direction
2021-06-04 13:49:33 | [train_policy] epoch #128 | descent direction computed
2021-06-04 13:49:33 | [train_policy] epoch #128 | backtrack iters: 1
2021-06-04 13:49:33 | [train_policy] epoch #128 | optimization finished
2021-06-04 13:49:33 | [train_policy] epoch #128 | Computing KL after
2021-06-04 13:49:33 | [train_policy] epoch #128 | Computing loss after
2021-06-04 13:49:33 | [train_policy] epoch #128 | Fitting baseline...
2021-06-04 13:49:33 | [train_policy] epoch #128 | Saving snapshot...
2021-06-04 13:49:33 | [train_policy] epoch #128 | Saved
2021-06-04 13:49:33 | [train_policy] epoch #128 | Time 105.44 s
2021-06-04 13:49:33 | [train_policy] epoch #128 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285241
Evaluation/AverageDiscountedReturn          -47.1105
Evaluation/AverageReturn                    -47.1105
Evaluation/CompletionRate                     0
Evaluation/Iteration                        128
Evaluation/MaxReturn                        -33.3583
Evaluation/MinReturn                       -136.794
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.2053
Extras/EpisodeRewardMean                    -46.83
LinearFeatureBaseline/ExplainedVariance       0.72079
PolicyExecTime                                0.229898
ProcessExecTime                               0.031234
TotalEnvSteps                            130548
policy/Entropy                                0.899374
policy/KL                                     0.00665956
policy/KLBefore                               0
policy/LossAfter                             -0.013921
policy/LossBefore                             2.27346e-08
policy/Perplexity                             2.45806
policy/dLoss                                  0.013921
---------------------------------------  ----------------
2021-06-04 13:49:33 | [train_policy] epoch #129 | Obtaining samples for iteration 129...
2021-06-04 13:49:34 | [train_policy] epoch #129 | Logging diagnostics...
2021-06-04 13:49:34 | [train_policy] epoch #129 | Optimizing policy...
2021-06-04 13:49:34 | [train_policy] epoch #129 | Computing loss before
2021-06-04 13:49:34 | [train_policy] epoch #129 | Computing KL before
2021-06-04 13:49:34 | [train_policy] epoch #129 | Optimizing
2021-06-04 13:49:34 | [train_policy] epoch #129 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:34 | [train_policy] epoch #129 | computing loss before
2021-06-04 13:49:34 | [train_policy] epoch #129 | computing gradient
2021-06-04 13:49:34 | [train_policy] epoch #129 | gradient computed
2021-06-04 13:49:34 | [train_policy] epoch #129 | computing descent direction
2021-06-04 13:49:34 | [train_policy] epoch #129 | descent direction computed
2021-06-04 13:49:34 | [train_policy] epoch #129 | backtrack iters: 1
2021-06-04 13:49:34 | [train_policy] epoch #129 | optimization finished
2021-06-04 13:49:34 | [train_policy] epoch #129 | Computing KL after
2021-06-04 13:49:34 | [train_policy] epoch #129 | Computing loss after
2021-06-04 13:49:34 | [train_policy] epoch #129 | Fitting baseline...
2021-06-04 13:49:34 | [train_policy] epoch #129 | Saving snapshot...
2021-06-04 13:49:34 | [train_policy] epoch #129 | Saved
2021-06-04 13:49:34 | [train_policy] epoch #129 | Time 106.23 s
2021-06-04 13:49:34 | [train_policy] epoch #129 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28369
Evaluation/AverageDiscountedReturn          -46.5714
Evaluation/AverageReturn                    -46.5714
Evaluation/CompletionRate                     0
Evaluation/Iteration                        129
Evaluation/MaxReturn                        -36.0542
Evaluation/MinReturn                        -88.8988
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.08469
Extras/EpisodeRewardMean                    -46.3151
LinearFeatureBaseline/ExplainedVariance       0.875624
PolicyExecTime                                0.221894
ProcessExecTime                               0.0310032
TotalEnvSteps                            131560
policy/Entropy                                0.898722
policy/KL                                     0.00708596
policy/KLBefore                               0
policy/LossAfter                             -0.0181484
policy/LossBefore                             4.24065e-09
policy/Perplexity                             2.45646
policy/dLoss                                  0.0181484
---------------------------------------  ----------------
2021-06-04 13:49:34 | [train_policy] epoch #130 | Obtaining samples for iteration 130...
2021-06-04 13:49:35 | [train_policy] epoch #130 | Logging diagnostics...
2021-06-04 13:49:35 | [train_policy] epoch #130 | Optimizing policy...
2021-06-04 13:49:35 | [train_policy] epoch #130 | Computing loss before
2021-06-04 13:49:35 | [train_policy] epoch #130 | Computing KL before
2021-06-04 13:49:35 | [train_policy] epoch #130 | Optimizing
2021-06-04 13:49:35 | [train_policy] epoch #130 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:35 | [train_policy] epoch #130 | computing loss before
2021-06-04 13:49:35 | [train_policy] epoch #130 | computing gradient
2021-06-04 13:49:35 | [train_policy] epoch #130 | gradient computed
2021-06-04 13:49:35 | [train_policy] epoch #130 | computing descent direction
2021-06-04 13:49:35 | [train_policy] epoch #130 | descent direction computed
2021-06-04 13:49:35 | [train_policy] epoch #130 | backtrack iters: 1
2021-06-04 13:49:35 | [train_policy] epoch #130 | optimization finished
2021-06-04 13:49:35 | [train_policy] epoch #130 | Computing KL after
2021-06-04 13:49:35 | [train_policy] epoch #130 | Computing loss after
2021-06-04 13:49:35 | [train_policy] epoch #130 | Fitting baseline...
2021-06-04 13:49:35 | [train_policy] epoch #130 | Saving snapshot...
2021-06-04 13:49:35 | [train_policy] epoch #130 | Saved
2021-06-04 13:49:35 | [train_policy] epoch #130 | Time 107.05 s
2021-06-04 13:49:35 | [train_policy] epoch #130 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287027
Evaluation/AverageDiscountedReturn          -46.1294
Evaluation/AverageReturn                    -46.1294
Evaluation/CompletionRate                     0
Evaluation/Iteration                        130
Evaluation/MaxReturn                        -35.8771
Evaluation/MinReturn                        -83.9519
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.19007
Extras/EpisodeRewardMean                    -46.0302
LinearFeatureBaseline/ExplainedVariance       0.917075
PolicyExecTime                                0.228683
ProcessExecTime                               0.0313985
TotalEnvSteps                            132572
policy/Entropy                                0.82622
policy/KL                                     0.00676756
policy/KLBefore                               0
policy/LossAfter                             -0.0187824
policy/LossBefore                            -8.83468e-09
policy/Perplexity                             2.28467
policy/dLoss                                  0.0187824
---------------------------------------  ----------------
2021-06-04 13:49:35 | [train_policy] epoch #131 | Obtaining samples for iteration 131...
2021-06-04 13:49:35 | [train_policy] epoch #131 | Logging diagnostics...
2021-06-04 13:49:35 | [train_policy] epoch #131 | Optimizing policy...
2021-06-04 13:49:35 | [train_policy] epoch #131 | Computing loss before
2021-06-04 13:49:35 | [train_policy] epoch #131 | Computing KL before
2021-06-04 13:49:35 | [train_policy] epoch #131 | Optimizing
2021-06-04 13:49:35 | [train_policy] epoch #131 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:35 | [train_policy] epoch #131 | computing loss before
2021-06-04 13:49:35 | [train_policy] epoch #131 | computing gradient
2021-06-04 13:49:35 | [train_policy] epoch #131 | gradient computed
2021-06-04 13:49:35 | [train_policy] epoch #131 | computing descent direction
2021-06-04 13:49:36 | [train_policy] epoch #131 | descent direction computed
2021-06-04 13:49:36 | [train_policy] epoch #131 | backtrack iters: 1
2021-06-04 13:49:36 | [train_policy] epoch #131 | optimization finished
2021-06-04 13:49:36 | [train_policy] epoch #131 | Computing KL after
2021-06-04 13:49:36 | [train_policy] epoch #131 | Computing loss after
2021-06-04 13:49:36 | [train_policy] epoch #131 | Fitting baseline...
2021-06-04 13:49:36 | [train_policy] epoch #131 | Saving snapshot...
2021-06-04 13:49:36 | [train_policy] epoch #131 | Saved
2021-06-04 13:49:36 | [train_policy] epoch #131 | Time 107.84 s
2021-06-04 13:49:36 | [train_policy] epoch #131 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285329
Evaluation/AverageDiscountedReturn          -46.7849
Evaluation/AverageReturn                    -46.7849
Evaluation/CompletionRate                     0
Evaluation/Iteration                        131
Evaluation/MaxReturn                        -36.5071
Evaluation/MinReturn                       -107.832
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2696
Extras/EpisodeRewardMean                    -46.8976
LinearFeatureBaseline/ExplainedVariance       0.783305
PolicyExecTime                                0.228665
ProcessExecTime                               0.0312428
TotalEnvSteps                            133584
policy/Entropy                                0.81751
policy/KL                                     0.00740397
policy/KLBefore                               0
policy/LossAfter                             -0.0222428
policy/LossBefore                             1.31931e-08
policy/Perplexity                             2.26485
policy/dLoss                                  0.0222429
---------------------------------------  ----------------
2021-06-04 13:49:36 | [train_policy] epoch #132 | Obtaining samples for iteration 132...
2021-06-04 13:49:36 | [train_policy] epoch #132 | Logging diagnostics...
2021-06-04 13:49:36 | [train_policy] epoch #132 | Optimizing policy...
2021-06-04 13:49:36 | [train_policy] epoch #132 | Computing loss before
2021-06-04 13:49:36 | [train_policy] epoch #132 | Computing KL before
2021-06-04 13:49:36 | [train_policy] epoch #132 | Optimizing
2021-06-04 13:49:36 | [train_policy] epoch #132 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:36 | [train_policy] epoch #132 | computing loss before
2021-06-04 13:49:36 | [train_policy] epoch #132 | computing gradient
2021-06-04 13:49:36 | [train_policy] epoch #132 | gradient computed
2021-06-04 13:49:36 | [train_policy] epoch #132 | computing descent direction
2021-06-04 13:49:36 | [train_policy] epoch #132 | descent direction computed
2021-06-04 13:49:36 | [train_policy] epoch #132 | backtrack iters: 1
2021-06-04 13:49:36 | [train_policy] epoch #132 | optimization finished
2021-06-04 13:49:36 | [train_policy] epoch #132 | Computing KL after
2021-06-04 13:49:36 | [train_policy] epoch #132 | Computing loss after
2021-06-04 13:49:36 | [train_policy] epoch #132 | Fitting baseline...
2021-06-04 13:49:36 | [train_policy] epoch #132 | Saving snapshot...
2021-06-04 13:49:36 | [train_policy] epoch #132 | Saved
2021-06-04 13:49:36 | [train_policy] epoch #132 | Time 108.64 s
2021-06-04 13:49:36 | [train_policy] epoch #132 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284957
Evaluation/AverageDiscountedReturn          -45.9247
Evaluation/AverageReturn                    -45.9247
Evaluation/CompletionRate                     0
Evaluation/Iteration                        132
Evaluation/MaxReturn                        -31.2744
Evaluation/MinReturn                        -66.0059
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.05139
Extras/EpisodeRewardMean                    -45.7841
LinearFeatureBaseline/ExplainedVariance       0.913686
PolicyExecTime                                0.226705
ProcessExecTime                               0.0310087
TotalEnvSteps                            134596
policy/Entropy                                0.776736
policy/KL                                     0.00669477
policy/KLBefore                               0
policy/LossAfter                             -0.0216001
policy/LossBefore                            -1.64914e-09
policy/Perplexity                             2.17436
policy/dLoss                                  0.0216001
---------------------------------------  ----------------
2021-06-04 13:49:36 | [train_policy] epoch #133 | Obtaining samples for iteration 133...
2021-06-04 13:49:37 | [train_policy] epoch #133 | Logging diagnostics...
2021-06-04 13:49:37 | [train_policy] epoch #133 | Optimizing policy...
2021-06-04 13:49:37 | [train_policy] epoch #133 | Computing loss before
2021-06-04 13:49:37 | [train_policy] epoch #133 | Computing KL before
2021-06-04 13:49:37 | [train_policy] epoch #133 | Optimizing
2021-06-04 13:49:37 | [train_policy] epoch #133 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:37 | [train_policy] epoch #133 | computing loss before
2021-06-04 13:49:37 | [train_policy] epoch #133 | computing gradient
2021-06-04 13:49:37 | [train_policy] epoch #133 | gradient computed
2021-06-04 13:49:37 | [train_policy] epoch #133 | computing descent direction
2021-06-04 13:49:37 | [train_policy] epoch #133 | descent direction computed
2021-06-04 13:49:37 | [train_policy] epoch #133 | backtrack iters: 0
2021-06-04 13:49:37 | [train_policy] epoch #133 | optimization finished
2021-06-04 13:49:37 | [train_policy] epoch #133 | Computing KL after
2021-06-04 13:49:37 | [train_policy] epoch #133 | Computing loss after
2021-06-04 13:49:37 | [train_policy] epoch #133 | Fitting baseline...
2021-06-04 13:49:37 | [train_policy] epoch #133 | Saving snapshot...
2021-06-04 13:49:37 | [train_policy] epoch #133 | Saved
2021-06-04 13:49:37 | [train_policy] epoch #133 | Time 109.46 s
2021-06-04 13:49:37 | [train_policy] epoch #133 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287242
Evaluation/AverageDiscountedReturn          -45.0869
Evaluation/AverageReturn                    -45.0869
Evaluation/CompletionRate                     0
Evaluation/Iteration                        133
Evaluation/MaxReturn                        -33.1946
Evaluation/MinReturn                        -65.7851
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.70528
Extras/EpisodeRewardMean                    -45.4992
LinearFeatureBaseline/ExplainedVariance       0.943148
PolicyExecTime                                0.238134
ProcessExecTime                               0.0314159
TotalEnvSteps                            135608
policy/Entropy                                0.775712
policy/KL                                     0.00998376
policy/KLBefore                               0
policy/LossAfter                             -0.0155271
policy/LossBefore                             3.53387e-09
policy/Perplexity                             2.17214
policy/dLoss                                  0.0155271
---------------------------------------  ----------------
2021-06-04 13:49:37 | [train_policy] epoch #134 | Obtaining samples for iteration 134...
2021-06-04 13:49:38 | [train_policy] epoch #134 | Logging diagnostics...
2021-06-04 13:49:38 | [train_policy] epoch #134 | Optimizing policy...
2021-06-04 13:49:38 | [train_policy] epoch #134 | Computing loss before
2021-06-04 13:49:38 | [train_policy] epoch #134 | Computing KL before
2021-06-04 13:49:38 | [train_policy] epoch #134 | Optimizing
2021-06-04 13:49:38 | [train_policy] epoch #134 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:38 | [train_policy] epoch #134 | computing loss before
2021-06-04 13:49:38 | [train_policy] epoch #134 | computing gradient
2021-06-04 13:49:38 | [train_policy] epoch #134 | gradient computed
2021-06-04 13:49:38 | [train_policy] epoch #134 | computing descent direction
2021-06-04 13:49:38 | [train_policy] epoch #134 | descent direction computed
2021-06-04 13:49:38 | [train_policy] epoch #134 | backtrack iters: 0
2021-06-04 13:49:38 | [train_policy] epoch #134 | optimization finished
2021-06-04 13:49:38 | [train_policy] epoch #134 | Computing KL after
2021-06-04 13:49:38 | [train_policy] epoch #134 | Computing loss after
2021-06-04 13:49:38 | [train_policy] epoch #134 | Fitting baseline...
2021-06-04 13:49:38 | [train_policy] epoch #134 | Saving snapshot...
2021-06-04 13:49:38 | [train_policy] epoch #134 | Saved
2021-06-04 13:49:38 | [train_policy] epoch #134 | Time 110.25 s
2021-06-04 13:49:38 | [train_policy] epoch #134 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285402
Evaluation/AverageDiscountedReturn          -47.8684
Evaluation/AverageReturn                    -47.8684
Evaluation/CompletionRate                     0
Evaluation/Iteration                        134
Evaluation/MaxReturn                        -34.0277
Evaluation/MinReturn                        -94.0481
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.77859
Extras/EpisodeRewardMean                    -47.4715
LinearFeatureBaseline/ExplainedVariance       0.841394
PolicyExecTime                                0.226294
ProcessExecTime                               0.0312054
TotalEnvSteps                            136620
policy/Entropy                                0.785596
policy/KL                                     0.00963648
policy/KLBefore                               0
policy/LossAfter                             -0.0210095
policy/LossBefore                             1.08372e-08
policy/Perplexity                             2.19371
policy/dLoss                                  0.0210096
---------------------------------------  ----------------
2021-06-04 13:49:38 | [train_policy] epoch #135 | Obtaining samples for iteration 135...
2021-06-04 13:49:39 | [train_policy] epoch #135 | Logging diagnostics...
2021-06-04 13:49:39 | [train_policy] epoch #135 | Optimizing policy...
2021-06-04 13:49:39 | [train_policy] epoch #135 | Computing loss before
2021-06-04 13:49:39 | [train_policy] epoch #135 | Computing KL before
2021-06-04 13:49:39 | [train_policy] epoch #135 | Optimizing
2021-06-04 13:49:39 | [train_policy] epoch #135 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:39 | [train_policy] epoch #135 | computing loss before
2021-06-04 13:49:39 | [train_policy] epoch #135 | computing gradient
2021-06-04 13:49:39 | [train_policy] epoch #135 | gradient computed
2021-06-04 13:49:39 | [train_policy] epoch #135 | computing descent direction
2021-06-04 13:49:39 | [train_policy] epoch #135 | descent direction computed
2021-06-04 13:49:39 | [train_policy] epoch #135 | backtrack iters: 0
2021-06-04 13:49:39 | [train_policy] epoch #135 | optimization finished
2021-06-04 13:49:39 | [train_policy] epoch #135 | Computing KL after
2021-06-04 13:49:39 | [train_policy] epoch #135 | Computing loss after
2021-06-04 13:49:39 | [train_policy] epoch #135 | Fitting baseline...
2021-06-04 13:49:39 | [train_policy] epoch #135 | Saving snapshot...
2021-06-04 13:49:39 | [train_policy] epoch #135 | Saved
2021-06-04 13:49:39 | [train_policy] epoch #135 | Time 111.07 s
2021-06-04 13:49:39 | [train_policy] epoch #135 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287419
Evaluation/AverageDiscountedReturn          -46.2955
Evaluation/AverageReturn                    -46.2955
Evaluation/CompletionRate                     0
Evaluation/Iteration                        135
Evaluation/MaxReturn                        -33.0938
Evaluation/MinReturn                        -65.5917
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.22206
Extras/EpisodeRewardMean                    -46.59
LinearFeatureBaseline/ExplainedVariance       0.936747
PolicyExecTime                                0.23252
ProcessExecTime                               0.0313749
TotalEnvSteps                            137632
policy/Entropy                                0.781721
policy/KL                                     0.00996374
policy/KLBefore                               0
policy/LossAfter                             -0.0154613
policy/LossBefore                            -2.83888e-08
policy/Perplexity                             2.18523
policy/dLoss                                  0.0154613
---------------------------------------  ----------------
2021-06-04 13:49:39 | [train_policy] epoch #136 | Obtaining samples for iteration 136...
2021-06-04 13:49:39 | [train_policy] epoch #136 | Logging diagnostics...
2021-06-04 13:49:39 | [train_policy] epoch #136 | Optimizing policy...
2021-06-04 13:49:39 | [train_policy] epoch #136 | Computing loss before
2021-06-04 13:49:39 | [train_policy] epoch #136 | Computing KL before
2021-06-04 13:49:39 | [train_policy] epoch #136 | Optimizing
2021-06-04 13:49:39 | [train_policy] epoch #136 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:39 | [train_policy] epoch #136 | computing loss before
2021-06-04 13:49:39 | [train_policy] epoch #136 | computing gradient
2021-06-04 13:49:40 | [train_policy] epoch #136 | gradient computed
2021-06-04 13:49:40 | [train_policy] epoch #136 | computing descent direction
2021-06-04 13:49:40 | [train_policy] epoch #136 | descent direction computed
2021-06-04 13:49:40 | [train_policy] epoch #136 | backtrack iters: 1
2021-06-04 13:49:40 | [train_policy] epoch #136 | optimization finished
2021-06-04 13:49:40 | [train_policy] epoch #136 | Computing KL after
2021-06-04 13:49:40 | [train_policy] epoch #136 | Computing loss after
2021-06-04 13:49:40 | [train_policy] epoch #136 | Fitting baseline...
2021-06-04 13:49:40 | [train_policy] epoch #136 | Saving snapshot...
2021-06-04 13:49:40 | [train_policy] epoch #136 | Saved
2021-06-04 13:49:40 | [train_policy] epoch #136 | Time 111.86 s
2021-06-04 13:49:40 | [train_policy] epoch #136 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285543
Evaluation/AverageDiscountedReturn          -45.156
Evaluation/AverageReturn                    -45.156
Evaluation/CompletionRate                     0
Evaluation/Iteration                        136
Evaluation/MaxReturn                        -32.4122
Evaluation/MinReturn                        -64.3204
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.31713
Extras/EpisodeRewardMean                    -45.313
LinearFeatureBaseline/ExplainedVariance       0.947946
PolicyExecTime                                0.222072
ProcessExecTime                               0.0312541
TotalEnvSteps                            138644
policy/Entropy                                0.80528
policy/KL                                     0.00682563
policy/KLBefore                               0
policy/LossAfter                             -0.015697
policy/LossBefore                             1.71982e-08
policy/Perplexity                             2.23732
policy/dLoss                                  0.015697
---------------------------------------  ----------------
2021-06-04 13:49:40 | [train_policy] epoch #137 | Obtaining samples for iteration 137...
2021-06-04 13:49:40 | [train_policy] epoch #137 | Logging diagnostics...
2021-06-04 13:49:40 | [train_policy] epoch #137 | Optimizing policy...
2021-06-04 13:49:40 | [train_policy] epoch #137 | Computing loss before
2021-06-04 13:49:40 | [train_policy] epoch #137 | Computing KL before
2021-06-04 13:49:40 | [train_policy] epoch #137 | Optimizing
2021-06-04 13:49:40 | [train_policy] epoch #137 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:40 | [train_policy] epoch #137 | computing loss before
2021-06-04 13:49:40 | [train_policy] epoch #137 | computing gradient
2021-06-04 13:49:40 | [train_policy] epoch #137 | gradient computed
2021-06-04 13:49:40 | [train_policy] epoch #137 | computing descent direction
2021-06-04 13:49:40 | [train_policy] epoch #137 | descent direction computed
2021-06-04 13:49:40 | [train_policy] epoch #137 | backtrack iters: 1
2021-06-04 13:49:40 | [train_policy] epoch #137 | optimization finished
2021-06-04 13:49:40 | [train_policy] epoch #137 | Computing KL after
2021-06-04 13:49:40 | [train_policy] epoch #137 | Computing loss after
2021-06-04 13:49:40 | [train_policy] epoch #137 | Fitting baseline...
2021-06-04 13:49:40 | [train_policy] epoch #137 | Saving snapshot...
2021-06-04 13:49:40 | [train_policy] epoch #137 | Saved
2021-06-04 13:49:40 | [train_policy] epoch #137 | Time 112.66 s
2021-06-04 13:49:40 | [train_policy] epoch #137 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.289029
Evaluation/AverageDiscountedReturn          -44.9208
Evaluation/AverageReturn                    -44.9208
Evaluation/CompletionRate                     0
Evaluation/Iteration                        137
Evaluation/MaxReturn                        -31.6461
Evaluation/MinReturn                        -60.5975
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.69568
Extras/EpisodeRewardMean                    -44.7425
LinearFeatureBaseline/ExplainedVariance       0.945195
PolicyExecTime                                0.233598
ProcessExecTime                               0.031652
TotalEnvSteps                            139656
policy/Entropy                                0.772706
policy/KL                                     0.00760733
policy/KLBefore                               0
policy/LossAfter                             -0.0202581
policy/LossBefore                             5.18301e-09
policy/Perplexity                             2.16562
policy/dLoss                                  0.0202581
---------------------------------------  ----------------
2021-06-04 13:49:40 | [train_policy] epoch #138 | Obtaining samples for iteration 138...
2021-06-04 13:49:41 | [train_policy] epoch #138 | Logging diagnostics...
2021-06-04 13:49:41 | [train_policy] epoch #138 | Optimizing policy...
2021-06-04 13:49:41 | [train_policy] epoch #138 | Computing loss before
2021-06-04 13:49:41 | [train_policy] epoch #138 | Computing KL before
2021-06-04 13:49:41 | [train_policy] epoch #138 | Optimizing
2021-06-04 13:49:41 | [train_policy] epoch #138 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:41 | [train_policy] epoch #138 | computing loss before
2021-06-04 13:49:41 | [train_policy] epoch #138 | computing gradient
2021-06-04 13:49:41 | [train_policy] epoch #138 | gradient computed
2021-06-04 13:49:41 | [train_policy] epoch #138 | computing descent direction
2021-06-04 13:49:41 | [train_policy] epoch #138 | descent direction computed
2021-06-04 13:49:41 | [train_policy] epoch #138 | backtrack iters: 1
2021-06-04 13:49:41 | [train_policy] epoch #138 | optimization finished
2021-06-04 13:49:41 | [train_policy] epoch #138 | Computing KL after
2021-06-04 13:49:41 | [train_policy] epoch #138 | Computing loss after
2021-06-04 13:49:41 | [train_policy] epoch #138 | Fitting baseline...
2021-06-04 13:49:41 | [train_policy] epoch #138 | Saving snapshot...
2021-06-04 13:49:41 | [train_policy] epoch #138 | Saved
2021-06-04 13:49:41 | [train_policy] epoch #138 | Time 113.46 s
2021-06-04 13:49:41 | [train_policy] epoch #138 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28393
Evaluation/AverageDiscountedReturn          -44.6061
Evaluation/AverageReturn                    -44.6061
Evaluation/CompletionRate                     0
Evaluation/Iteration                        138
Evaluation/MaxReturn                        -32.2532
Evaluation/MinReturn                        -66.6468
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.91939
Extras/EpisodeRewardMean                    -44.4755
LinearFeatureBaseline/ExplainedVariance       0.928081
PolicyExecTime                                0.2316
ProcessExecTime                               0.0310051
TotalEnvSteps                            140668
policy/Entropy                                0.773161
policy/KL                                     0.00640332
policy/KLBefore                               0
policy/LossAfter                             -0.0140275
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             2.1666
policy/dLoss                                  0.0140275
---------------------------------------  ----------------
2021-06-04 13:49:41 | [train_policy] epoch #139 | Obtaining samples for iteration 139...
2021-06-04 13:49:42 | [train_policy] epoch #139 | Logging diagnostics...
2021-06-04 13:49:42 | [train_policy] epoch #139 | Optimizing policy...
2021-06-04 13:49:42 | [train_policy] epoch #139 | Computing loss before
2021-06-04 13:49:42 | [train_policy] epoch #139 | Computing KL before
2021-06-04 13:49:42 | [train_policy] epoch #139 | Optimizing
2021-06-04 13:49:42 | [train_policy] epoch #139 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:42 | [train_policy] epoch #139 | computing loss before
2021-06-04 13:49:42 | [train_policy] epoch #139 | computing gradient
2021-06-04 13:49:42 | [train_policy] epoch #139 | gradient computed
2021-06-04 13:49:42 | [train_policy] epoch #139 | computing descent direction
2021-06-04 13:49:42 | [train_policy] epoch #139 | descent direction computed
2021-06-04 13:49:42 | [train_policy] epoch #139 | backtrack iters: 1
2021-06-04 13:49:42 | [train_policy] epoch #139 | optimization finished
2021-06-04 13:49:42 | [train_policy] epoch #139 | Computing KL after
2021-06-04 13:49:42 | [train_policy] epoch #139 | Computing loss after
2021-06-04 13:49:42 | [train_policy] epoch #139 | Fitting baseline...
2021-06-04 13:49:42 | [train_policy] epoch #139 | Saving snapshot...
2021-06-04 13:49:42 | [train_policy] epoch #139 | Saved
2021-06-04 13:49:42 | [train_policy] epoch #139 | Time 114.27 s
2021-06-04 13:49:42 | [train_policy] epoch #139 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285857
Evaluation/AverageDiscountedReturn          -46.2358
Evaluation/AverageReturn                    -46.2358
Evaluation/CompletionRate                     0
Evaluation/Iteration                        139
Evaluation/MaxReturn                        -34.7147
Evaluation/MinReturn                        -66.0441
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.92709
Extras/EpisodeRewardMean                    -46.2185
LinearFeatureBaseline/ExplainedVariance       0.912531
PolicyExecTime                                0.227376
ProcessExecTime                               0.0313673
TotalEnvSteps                            141680
policy/Entropy                                0.715694
policy/KL                                     0.00675211
policy/KLBefore                               0
policy/LossAfter                             -0.0198375
policy/LossBefore                             9.65925e-09
policy/Perplexity                             2.04561
policy/dLoss                                  0.0198375
---------------------------------------  ----------------
2021-06-04 13:49:42 | [train_policy] epoch #140 | Obtaining samples for iteration 140...
2021-06-04 13:49:43 | [train_policy] epoch #140 | Logging diagnostics...
2021-06-04 13:49:43 | [train_policy] epoch #140 | Optimizing policy...
2021-06-04 13:49:43 | [train_policy] epoch #140 | Computing loss before
2021-06-04 13:49:43 | [train_policy] epoch #140 | Computing KL before
2021-06-04 13:49:43 | [train_policy] epoch #140 | Optimizing
2021-06-04 13:49:43 | [train_policy] epoch #140 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:43 | [train_policy] epoch #140 | computing loss before
2021-06-04 13:49:43 | [train_policy] epoch #140 | computing gradient
2021-06-04 13:49:43 | [train_policy] epoch #140 | gradient computed
2021-06-04 13:49:43 | [train_policy] epoch #140 | computing descent direction
2021-06-04 13:49:43 | [train_policy] epoch #140 | descent direction computed
2021-06-04 13:49:43 | [train_policy] epoch #140 | backtrack iters: 1
2021-06-04 13:49:43 | [train_policy] epoch #140 | optimization finished
2021-06-04 13:49:43 | [train_policy] epoch #140 | Computing KL after
2021-06-04 13:49:43 | [train_policy] epoch #140 | Computing loss after
2021-06-04 13:49:43 | [train_policy] epoch #140 | Fitting baseline...
2021-06-04 13:49:43 | [train_policy] epoch #140 | Saving snapshot...
2021-06-04 13:49:43 | [train_policy] epoch #140 | Saved
2021-06-04 13:49:43 | [train_policy] epoch #140 | Time 115.06 s
2021-06-04 13:49:43 | [train_policy] epoch #140 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.287464
Evaluation/AverageDiscountedReturn          -45.0954
Evaluation/AverageReturn                    -45.0954
Evaluation/CompletionRate                     0
Evaluation/Iteration                        140
Evaluation/MaxReturn                        -34.2988
Evaluation/MinReturn                        -65.014
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.8823
Extras/EpisodeRewardMean                    -45.4413
LinearFeatureBaseline/ExplainedVariance       0.924821
PolicyExecTime                                0.210127
ProcessExecTime                               0.0315151
TotalEnvSteps                            142692
policy/Entropy                                0.688157
policy/KL                                     0.00741986
policy/KLBefore                               0
policy/LossAfter                             -0.0206592
policy/LossBefore                             6.59656e-09
policy/Perplexity                             1.99004
policy/dLoss                                  0.0206592
---------------------------------------  ----------------
2021-06-04 13:49:43 | [train_policy] epoch #141 | Obtaining samples for iteration 141...
2021-06-04 13:49:43 | [train_policy] epoch #141 | Logging diagnostics...
2021-06-04 13:49:43 | [train_policy] epoch #141 | Optimizing policy...
2021-06-04 13:49:43 | [train_policy] epoch #141 | Computing loss before
2021-06-04 13:49:43 | [train_policy] epoch #141 | Computing KL before
2021-06-04 13:49:43 | [train_policy] epoch #141 | Optimizing
2021-06-04 13:49:43 | [train_policy] epoch #141 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:43 | [train_policy] epoch #141 | computing loss before
2021-06-04 13:49:43 | [train_policy] epoch #141 | computing gradient
2021-06-04 13:49:43 | [train_policy] epoch #141 | gradient computed
2021-06-04 13:49:43 | [train_policy] epoch #141 | computing descent direction
2021-06-04 13:49:44 | [train_policy] epoch #141 | descent direction computed
2021-06-04 13:49:44 | [train_policy] epoch #141 | backtrack iters: 1
2021-06-04 13:49:44 | [train_policy] epoch #141 | optimization finished
2021-06-04 13:49:44 | [train_policy] epoch #141 | Computing KL after
2021-06-04 13:49:44 | [train_policy] epoch #141 | Computing loss after
2021-06-04 13:49:44 | [train_policy] epoch #141 | Fitting baseline...
2021-06-04 13:49:44 | [train_policy] epoch #141 | Saving snapshot...
2021-06-04 13:49:44 | [train_policy] epoch #141 | Saved
2021-06-04 13:49:44 | [train_policy] epoch #141 | Time 115.84 s
2021-06-04 13:49:44 | [train_policy] epoch #141 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.283829
Evaluation/AverageDiscountedReturn          -46.5228
Evaluation/AverageReturn                    -46.5228
Evaluation/CompletionRate                     0
Evaluation/Iteration                        141
Evaluation/MaxReturn                        -36.1114
Evaluation/MinReturn                       -130.333
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.759
Extras/EpisodeRewardMean                    -46.2102
LinearFeatureBaseline/ExplainedVariance       0.728901
PolicyExecTime                                0.208206
ProcessExecTime                               0.0311098
TotalEnvSteps                            143704
policy/Entropy                                0.676999
policy/KL                                     0.0076515
policy/KLBefore                               0
policy/LossAfter                             -0.0190227
policy/LossBefore                             4.94742e-09
policy/Perplexity                             1.96796
policy/dLoss                                  0.0190227
---------------------------------------  ----------------
2021-06-04 13:49:44 | [train_policy] epoch #142 | Obtaining samples for iteration 142...
2021-06-04 13:49:44 | [train_policy] epoch #142 | Logging diagnostics...
2021-06-04 13:49:44 | [train_policy] epoch #142 | Optimizing policy...
2021-06-04 13:49:44 | [train_policy] epoch #142 | Computing loss before
2021-06-04 13:49:44 | [train_policy] epoch #142 | Computing KL before
2021-06-04 13:49:44 | [train_policy] epoch #142 | Optimizing
2021-06-04 13:49:44 | [train_policy] epoch #142 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:44 | [train_policy] epoch #142 | computing loss before
2021-06-04 13:49:44 | [train_policy] epoch #142 | computing gradient
2021-06-04 13:49:44 | [train_policy] epoch #142 | gradient computed
2021-06-04 13:49:44 | [train_policy] epoch #142 | computing descent direction
2021-06-04 13:49:44 | [train_policy] epoch #142 | descent direction computed
2021-06-04 13:49:44 | [train_policy] epoch #142 | backtrack iters: 0
2021-06-04 13:49:44 | [train_policy] epoch #142 | optimization finished
2021-06-04 13:49:44 | [train_policy] epoch #142 | Computing KL after
2021-06-04 13:49:44 | [train_policy] epoch #142 | Computing loss after
2021-06-04 13:49:44 | [train_policy] epoch #142 | Fitting baseline...
2021-06-04 13:49:44 | [train_policy] epoch #142 | Saving snapshot...
2021-06-04 13:49:44 | [train_policy] epoch #142 | Saved
2021-06-04 13:49:44 | [train_policy] epoch #142 | Time 116.63 s
2021-06-04 13:49:44 | [train_policy] epoch #142 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286778
Evaluation/AverageDiscountedReturn          -46.1387
Evaluation/AverageReturn                    -46.1387
Evaluation/CompletionRate                     0
Evaluation/Iteration                        142
Evaluation/MaxReturn                        -35.6355
Evaluation/MinReturn                        -80.0538
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.98725
Extras/EpisodeRewardMean                    -45.8783
LinearFeatureBaseline/ExplainedVariance       0.865186
PolicyExecTime                                0.22697
ProcessExecTime                               0.0313923
TotalEnvSteps                            144716
policy/Entropy                                0.668532
policy/KL                                     0.009586
policy/KLBefore                               0
policy/LossAfter                             -0.0232635
policy/LossBefore                             1.69626e-08
policy/Perplexity                             1.95137
policy/dLoss                                  0.0232635
---------------------------------------  ----------------
2021-06-04 13:49:44 | [train_policy] epoch #143 | Obtaining samples for iteration 143...
2021-06-04 13:49:45 | [train_policy] epoch #143 | Logging diagnostics...
2021-06-04 13:49:45 | [train_policy] epoch #143 | Optimizing policy...
2021-06-04 13:49:45 | [train_policy] epoch #143 | Computing loss before
2021-06-04 13:49:45 | [train_policy] epoch #143 | Computing KL before
2021-06-04 13:49:45 | [train_policy] epoch #143 | Optimizing
2021-06-04 13:49:45 | [train_policy] epoch #143 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:45 | [train_policy] epoch #143 | computing loss before
2021-06-04 13:49:45 | [train_policy] epoch #143 | computing gradient
2021-06-04 13:49:45 | [train_policy] epoch #143 | gradient computed
2021-06-04 13:49:45 | [train_policy] epoch #143 | computing descent direction
2021-06-04 13:49:45 | [train_policy] epoch #143 | descent direction computed
2021-06-04 13:49:45 | [train_policy] epoch #143 | backtrack iters: 1
2021-06-04 13:49:45 | [train_policy] epoch #143 | optimization finished
2021-06-04 13:49:45 | [train_policy] epoch #143 | Computing KL after
2021-06-04 13:49:45 | [train_policy] epoch #143 | Computing loss after
2021-06-04 13:49:45 | [train_policy] epoch #143 | Fitting baseline...
2021-06-04 13:49:45 | [train_policy] epoch #143 | Saving snapshot...
2021-06-04 13:49:45 | [train_policy] epoch #143 | Saved
2021-06-04 13:49:45 | [train_policy] epoch #143 | Time 117.43 s
2021-06-04 13:49:45 | [train_policy] epoch #143 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284506
Evaluation/AverageDiscountedReturn          -46.5026
Evaluation/AverageReturn                    -46.5026
Evaluation/CompletionRate                     0
Evaluation/Iteration                        143
Evaluation/MaxReturn                        -32.0758
Evaluation/MinReturn                       -136.234
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.831
Extras/EpisodeRewardMean                    -46.4131
LinearFeatureBaseline/ExplainedVariance       0.720556
PolicyExecTime                                0.230525
ProcessExecTime                               0.0309999
TotalEnvSteps                            145728
policy/Entropy                                0.666069
policy/KL                                     0.00646412
policy/KLBefore                               0
policy/LossAfter                             -0.0166087
policy/LossBefore                            -2.59151e-09
policy/Perplexity                             1.94657
policy/dLoss                                  0.0166087
---------------------------------------  ----------------
2021-06-04 13:49:45 | [train_policy] epoch #144 | Obtaining samples for iteration 144...
2021-06-04 13:49:46 | [train_policy] epoch #144 | Logging diagnostics...
2021-06-04 13:49:46 | [train_policy] epoch #144 | Optimizing policy...
2021-06-04 13:49:46 | [train_policy] epoch #144 | Computing loss before
2021-06-04 13:49:46 | [train_policy] epoch #144 | Computing KL before
2021-06-04 13:49:46 | [train_policy] epoch #144 | Optimizing
2021-06-04 13:49:46 | [train_policy] epoch #144 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:46 | [train_policy] epoch #144 | computing loss before
2021-06-04 13:49:46 | [train_policy] epoch #144 | computing gradient
2021-06-04 13:49:46 | [train_policy] epoch #144 | gradient computed
2021-06-04 13:49:46 | [train_policy] epoch #144 | computing descent direction
2021-06-04 13:49:46 | [train_policy] epoch #144 | descent direction computed
2021-06-04 13:49:46 | [train_policy] epoch #144 | backtrack iters: 1
2021-06-04 13:49:46 | [train_policy] epoch #144 | optimization finished
2021-06-04 13:49:46 | [train_policy] epoch #144 | Computing KL after
2021-06-04 13:49:46 | [train_policy] epoch #144 | Computing loss after
2021-06-04 13:49:46 | [train_policy] epoch #144 | Fitting baseline...
2021-06-04 13:49:46 | [train_policy] epoch #144 | Saving snapshot...
2021-06-04 13:49:46 | [train_policy] epoch #144 | Saved
2021-06-04 13:49:46 | [train_policy] epoch #144 | Time 118.24 s
2021-06-04 13:49:46 | [train_policy] epoch #144 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285559
Evaluation/AverageDiscountedReturn          -45.0135
Evaluation/AverageReturn                    -45.0135
Evaluation/CompletionRate                     0
Evaluation/Iteration                        144
Evaluation/MaxReturn                        -35.2633
Evaluation/MinReturn                        -60.5353
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.11917
Extras/EpisodeRewardMean                    -44.8998
LinearFeatureBaseline/ExplainedVariance       0.893057
PolicyExecTime                                0.227978
ProcessExecTime                               0.031184
TotalEnvSteps                            146740
policy/Entropy                                0.662541
policy/KL                                     0.00759416
policy/KLBefore                               0
policy/LossAfter                             -0.0106555
policy/LossBefore                             9.89484e-09
policy/Perplexity                             1.93971
policy/dLoss                                  0.0106555
---------------------------------------  ----------------
2021-06-04 13:49:46 | [train_policy] epoch #145 | Obtaining samples for iteration 145...
2021-06-04 13:49:47 | [train_policy] epoch #145 | Logging diagnostics...
2021-06-04 13:49:47 | [train_policy] epoch #145 | Optimizing policy...
2021-06-04 13:49:47 | [train_policy] epoch #145 | Computing loss before
2021-06-04 13:49:47 | [train_policy] epoch #145 | Computing KL before
2021-06-04 13:49:47 | [train_policy] epoch #145 | Optimizing
2021-06-04 13:49:47 | [train_policy] epoch #145 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:47 | [train_policy] epoch #145 | computing loss before
2021-06-04 13:49:47 | [train_policy] epoch #145 | computing gradient
2021-06-04 13:49:47 | [train_policy] epoch #145 | gradient computed
2021-06-04 13:49:47 | [train_policy] epoch #145 | computing descent direction
2021-06-04 13:49:47 | [train_policy] epoch #145 | descent direction computed
2021-06-04 13:49:47 | [train_policy] epoch #145 | backtrack iters: 1
2021-06-04 13:49:47 | [train_policy] epoch #145 | optimization finished
2021-06-04 13:49:47 | [train_policy] epoch #145 | Computing KL after
2021-06-04 13:49:47 | [train_policy] epoch #145 | Computing loss after
2021-06-04 13:49:47 | [train_policy] epoch #145 | Fitting baseline...
2021-06-04 13:49:47 | [train_policy] epoch #145 | Saving snapshot...
2021-06-04 13:49:47 | [train_policy] epoch #145 | Saved
2021-06-04 13:49:47 | [train_policy] epoch #145 | Time 119.03 s
2021-06-04 13:49:47 | [train_policy] epoch #145 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.286223
Evaluation/AverageDiscountedReturn          -44.5527
Evaluation/AverageReturn                    -44.5527
Evaluation/CompletionRate                     0
Evaluation/Iteration                        145
Evaluation/MaxReturn                        -31.444
Evaluation/MinReturn                        -65.9965
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.43749
Extras/EpisodeRewardMean                    -44.6641
LinearFeatureBaseline/ExplainedVariance       0.934854
PolicyExecTime                                0.216839
ProcessExecTime                               0.0313222
TotalEnvSteps                            147752
policy/Entropy                                0.64869
policy/KL                                     0.00799023
policy/KLBefore                               0
policy/LossAfter                             -0.0179393
policy/LossBefore                             3.29828e-09
policy/Perplexity                             1.91303
policy/dLoss                                  0.0179393
---------------------------------------  ----------------
2021-06-04 13:49:47 | [train_policy] epoch #146 | Obtaining samples for iteration 146...
2021-06-04 13:49:47 | [train_policy] epoch #146 | Logging diagnostics...
2021-06-04 13:49:47 | [train_policy] epoch #146 | Optimizing policy...
2021-06-04 13:49:47 | [train_policy] epoch #146 | Computing loss before
2021-06-04 13:49:47 | [train_policy] epoch #146 | Computing KL before
2021-06-04 13:49:47 | [train_policy] epoch #146 | Optimizing
2021-06-04 13:49:47 | [train_policy] epoch #146 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:47 | [train_policy] epoch #146 | computing loss before
2021-06-04 13:49:47 | [train_policy] epoch #146 | computing gradient
2021-06-04 13:49:47 | [train_policy] epoch #146 | gradient computed
2021-06-04 13:49:47 | [train_policy] epoch #146 | computing descent direction
2021-06-04 13:49:48 | [train_policy] epoch #146 | descent direction computed
2021-06-04 13:49:48 | [train_policy] epoch #146 | backtrack iters: 1
2021-06-04 13:49:48 | [train_policy] epoch #146 | optimization finished
2021-06-04 13:49:48 | [train_policy] epoch #146 | Computing KL after
2021-06-04 13:49:48 | [train_policy] epoch #146 | Computing loss after
2021-06-04 13:49:48 | [train_policy] epoch #146 | Fitting baseline...
2021-06-04 13:49:48 | [train_policy] epoch #146 | Saving snapshot...
2021-06-04 13:49:48 | [train_policy] epoch #146 | Saved
2021-06-04 13:49:48 | [train_policy] epoch #146 | Time 119.83 s
2021-06-04 13:49:48 | [train_policy] epoch #146 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286366
Evaluation/AverageDiscountedReturn          -68.2721
Evaluation/AverageReturn                    -68.2721
Evaluation/CompletionRate                     0
Evaluation/Iteration                        146
Evaluation/MaxReturn                        -35.4503
Evaluation/MinReturn                      -2060.52
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.132
Extras/EpisodeRewardMean                    -66.2199
LinearFeatureBaseline/ExplainedVariance       0.0113328
PolicyExecTime                                0.234059
ProcessExecTime                               0.0313044
TotalEnvSteps                            148764
policy/Entropy                                0.633569
policy/KL                                     0.00684975
policy/KLBefore                               0
policy/LossAfter                             -0.0214926
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             1.88432
policy/dLoss                                  0.0214926
---------------------------------------  ----------------
2021-06-04 13:49:48 | [train_policy] epoch #147 | Obtaining samples for iteration 147...
2021-06-04 13:49:48 | [train_policy] epoch #147 | Logging diagnostics...
2021-06-04 13:49:48 | [train_policy] epoch #147 | Optimizing policy...
2021-06-04 13:49:48 | [train_policy] epoch #147 | Computing loss before
2021-06-04 13:49:48 | [train_policy] epoch #147 | Computing KL before
2021-06-04 13:49:48 | [train_policy] epoch #147 | Optimizing
2021-06-04 13:49:48 | [train_policy] epoch #147 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:48 | [train_policy] epoch #147 | computing loss before
2021-06-04 13:49:48 | [train_policy] epoch #147 | computing gradient
2021-06-04 13:49:48 | [train_policy] epoch #147 | gradient computed
2021-06-04 13:49:48 | [train_policy] epoch #147 | computing descent direction
2021-06-04 13:49:48 | [train_policy] epoch #147 | descent direction computed
2021-06-04 13:49:48 | [train_policy] epoch #147 | backtrack iters: 1
2021-06-04 13:49:48 | [train_policy] epoch #147 | optimization finished
2021-06-04 13:49:48 | [train_policy] epoch #147 | Computing KL after
2021-06-04 13:49:48 | [train_policy] epoch #147 | Computing loss after
2021-06-04 13:49:48 | [train_policy] epoch #147 | Fitting baseline...
2021-06-04 13:49:48 | [train_policy] epoch #147 | Saving snapshot...
2021-06-04 13:49:48 | [train_policy] epoch #147 | Saved
2021-06-04 13:49:48 | [train_policy] epoch #147 | Time 120.62 s
2021-06-04 13:49:48 | [train_policy] epoch #147 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285014
Evaluation/AverageDiscountedReturn          -46.3039
Evaluation/AverageReturn                    -46.3039
Evaluation/CompletionRate                     0
Evaluation/Iteration                        147
Evaluation/MaxReturn                        -32.7616
Evaluation/MinReturn                       -193.157
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         16.5326
Extras/EpisodeRewardMean                    -46.3021
LinearFeatureBaseline/ExplainedVariance     -11.4776
PolicyExecTime                                0.224401
ProcessExecTime                               0.031152
TotalEnvSteps                            149776
policy/Entropy                                0.612263
policy/KL                                     0.00650134
policy/KLBefore                               0
policy/LossAfter                             -0.0175152
policy/LossBefore                            -2.59151e-09
policy/Perplexity                             1.8446
policy/dLoss                                  0.0175152
---------------------------------------  ----------------
2021-06-04 13:49:48 | [train_policy] epoch #148 | Obtaining samples for iteration 148...
2021-06-04 13:49:49 | [train_policy] epoch #148 | Logging diagnostics...
2021-06-04 13:49:49 | [train_policy] epoch #148 | Optimizing policy...
2021-06-04 13:49:49 | [train_policy] epoch #148 | Computing loss before
2021-06-04 13:49:49 | [train_policy] epoch #148 | Computing KL before
2021-06-04 13:49:49 | [train_policy] epoch #148 | Optimizing
2021-06-04 13:49:49 | [train_policy] epoch #148 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:49 | [train_policy] epoch #148 | computing loss before
2021-06-04 13:49:49 | [train_policy] epoch #148 | computing gradient
2021-06-04 13:49:49 | [train_policy] epoch #148 | gradient computed
2021-06-04 13:49:49 | [train_policy] epoch #148 | computing descent direction
2021-06-04 13:49:49 | [train_policy] epoch #148 | descent direction computed
2021-06-04 13:49:49 | [train_policy] epoch #148 | backtrack iters: 1
2021-06-04 13:49:49 | [train_policy] epoch #148 | optimization finished
2021-06-04 13:49:49 | [train_policy] epoch #148 | Computing KL after
2021-06-04 13:49:49 | [train_policy] epoch #148 | Computing loss after
2021-06-04 13:49:49 | [train_policy] epoch #148 | Fitting baseline...
2021-06-04 13:49:49 | [train_policy] epoch #148 | Saving snapshot...
2021-06-04 13:49:49 | [train_policy] epoch #148 | Saved
2021-06-04 13:49:49 | [train_policy] epoch #148 | Time 121.43 s
2021-06-04 13:49:49 | [train_policy] epoch #148 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287567
Evaluation/AverageDiscountedReturn          -66.2757
Evaluation/AverageReturn                    -66.2757
Evaluation/CompletionRate                     0
Evaluation/Iteration                        148
Evaluation/MaxReturn                        -33.3212
Evaluation/MinReturn                      -2062.13
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.346
Extras/EpisodeRewardMean                    -64.5906
LinearFeatureBaseline/ExplainedVariance       0.010726
PolicyExecTime                                0.229933
ProcessExecTime                               0.0314906
TotalEnvSteps                            150788
policy/Entropy                                0.600888
policy/KL                                     0.00651926
policy/KLBefore                               0
policy/LossAfter                             -0.0264252
policy/LossBefore                             3.76946e-09
policy/Perplexity                             1.82374
policy/dLoss                                  0.0264252
---------------------------------------  ----------------
2021-06-04 13:49:49 | [train_policy] epoch #149 | Obtaining samples for iteration 149...
2021-06-04 13:49:50 | [train_policy] epoch #149 | Logging diagnostics...
2021-06-04 13:49:50 | [train_policy] epoch #149 | Optimizing policy...
2021-06-04 13:49:50 | [train_policy] epoch #149 | Computing loss before
2021-06-04 13:49:50 | [train_policy] epoch #149 | Computing KL before
2021-06-04 13:49:50 | [train_policy] epoch #149 | Optimizing
2021-06-04 13:49:50 | [train_policy] epoch #149 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:50 | [train_policy] epoch #149 | computing loss before
2021-06-04 13:49:50 | [train_policy] epoch #149 | computing gradient
2021-06-04 13:49:50 | [train_policy] epoch #149 | gradient computed
2021-06-04 13:49:50 | [train_policy] epoch #149 | computing descent direction
2021-06-04 13:49:50 | [train_policy] epoch #149 | descent direction computed
2021-06-04 13:49:50 | [train_policy] epoch #149 | backtrack iters: 1
2021-06-04 13:49:50 | [train_policy] epoch #149 | optimization finished
2021-06-04 13:49:50 | [train_policy] epoch #149 | Computing KL after
2021-06-04 13:49:50 | [train_policy] epoch #149 | Computing loss after
2021-06-04 13:49:50 | [train_policy] epoch #149 | Fitting baseline...
2021-06-04 13:49:50 | [train_policy] epoch #149 | Saving snapshot...
2021-06-04 13:49:50 | [train_policy] epoch #149 | Saved
2021-06-04 13:49:50 | [train_policy] epoch #149 | Time 122.23 s
2021-06-04 13:49:50 | [train_policy] epoch #149 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28559
Evaluation/AverageDiscountedReturn          -65.8376
Evaluation/AverageReturn                    -65.8376
Evaluation/CompletionRate                     0
Evaluation/Iteration                        149
Evaluation/MaxReturn                        -32.1135
Evaluation/MinReturn                      -2056.09
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.74
Extras/EpisodeRewardMean                    -84.3461
LinearFeatureBaseline/ExplainedVariance       0.0355159
PolicyExecTime                                0.229298
ProcessExecTime                               0.0311596
TotalEnvSteps                            151800
policy/Entropy                                0.609936
policy/KL                                     0.00660261
policy/KLBefore                               0
policy/LossAfter                             -0.028414
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             1.84031
policy/dLoss                                  0.028414
---------------------------------------  ----------------
2021-06-04 13:49:50 | [train_policy] epoch #150 | Obtaining samples for iteration 150...
2021-06-04 13:49:51 | [train_policy] epoch #150 | Logging diagnostics...
2021-06-04 13:49:51 | [train_policy] epoch #150 | Optimizing policy...
2021-06-04 13:49:51 | [train_policy] epoch #150 | Computing loss before
2021-06-04 13:49:51 | [train_policy] epoch #150 | Computing KL before
2021-06-04 13:49:51 | [train_policy] epoch #150 | Optimizing
2021-06-04 13:49:51 | [train_policy] epoch #150 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:51 | [train_policy] epoch #150 | computing loss before
2021-06-04 13:49:51 | [train_policy] epoch #150 | computing gradient
2021-06-04 13:49:51 | [train_policy] epoch #150 | gradient computed
2021-06-04 13:49:51 | [train_policy] epoch #150 | computing descent direction
2021-06-04 13:49:51 | [train_policy] epoch #150 | descent direction computed
2021-06-04 13:49:51 | [train_policy] epoch #150 | backtrack iters: 1
2021-06-04 13:49:51 | [train_policy] epoch #150 | optimization finished
2021-06-04 13:49:51 | [train_policy] epoch #150 | Computing KL after
2021-06-04 13:49:51 | [train_policy] epoch #150 | Computing loss after
2021-06-04 13:49:51 | [train_policy] epoch #150 | Fitting baseline...
2021-06-04 13:49:51 | [train_policy] epoch #150 | Saving snapshot...
2021-06-04 13:49:51 | [train_policy] epoch #150 | Saved
2021-06-04 13:49:51 | [train_policy] epoch #150 | Time 123.05 s
2021-06-04 13:49:51 | [train_policy] epoch #150 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285356
Evaluation/AverageDiscountedReturn          -45.294
Evaluation/AverageReturn                    -45.294
Evaluation/CompletionRate                     0
Evaluation/Iteration                        150
Evaluation/MaxReturn                        -33.1331
Evaluation/MinReturn                        -82.505
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.78654
Extras/EpisodeRewardMean                    -45.0105
LinearFeatureBaseline/ExplainedVariance     -16.4692
PolicyExecTime                                0.235742
ProcessExecTime                               0.0312486
TotalEnvSteps                            152812
policy/Entropy                                0.593644
policy/KL                                     0.00649222
policy/KLBefore                               0
policy/LossAfter                             -0.0279422
policy/LossBefore                             1.88473e-09
policy/Perplexity                             1.81057
policy/dLoss                                  0.0279422
---------------------------------------  ----------------
2021-06-04 13:49:51 | [train_policy] epoch #151 | Obtaining samples for iteration 151...
2021-06-04 13:49:51 | [train_policy] epoch #151 | Logging diagnostics...
2021-06-04 13:49:51 | [train_policy] epoch #151 | Optimizing policy...
2021-06-04 13:49:51 | [train_policy] epoch #151 | Computing loss before
2021-06-04 13:49:51 | [train_policy] epoch #151 | Computing KL before
2021-06-04 13:49:51 | [train_policy] epoch #151 | Optimizing
2021-06-04 13:49:51 | [train_policy] epoch #151 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:51 | [train_policy] epoch #151 | computing loss before
2021-06-04 13:49:51 | [train_policy] epoch #151 | computing gradient
2021-06-04 13:49:51 | [train_policy] epoch #151 | gradient computed
2021-06-04 13:49:51 | [train_policy] epoch #151 | computing descent direction
2021-06-04 13:49:52 | [train_policy] epoch #151 | descent direction computed
2021-06-04 13:49:52 | [train_policy] epoch #151 | backtrack iters: 1
2021-06-04 13:49:52 | [train_policy] epoch #151 | optimization finished
2021-06-04 13:49:52 | [train_policy] epoch #151 | Computing KL after
2021-06-04 13:49:52 | [train_policy] epoch #151 | Computing loss after
2021-06-04 13:49:52 | [train_policy] epoch #151 | Fitting baseline...
2021-06-04 13:49:52 | [train_policy] epoch #151 | Saving snapshot...
2021-06-04 13:49:52 | [train_policy] epoch #151 | Saved
2021-06-04 13:49:52 | [train_policy] epoch #151 | Time 123.85 s
2021-06-04 13:49:52 | [train_policy] epoch #151 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284769
Evaluation/AverageDiscountedReturn          -45.5158
Evaluation/AverageReturn                    -45.5158
Evaluation/CompletionRate                     0
Evaluation/Iteration                        151
Evaluation/MaxReturn                        -34.9483
Evaluation/MinReturn                        -64.388
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.65995
Extras/EpisodeRewardMean                    -46.0793
LinearFeatureBaseline/ExplainedVariance       0.92279
PolicyExecTime                                0.229217
ProcessExecTime                               0.0311487
TotalEnvSteps                            153824
policy/Entropy                                0.551716
policy/KL                                     0.00793897
policy/KLBefore                               0
policy/LossAfter                             -0.0154678
policy/LossBefore                             1.06016e-08
policy/Perplexity                             1.73623
policy/dLoss                                  0.0154678
---------------------------------------  ----------------
2021-06-04 13:49:52 | [train_policy] epoch #152 | Obtaining samples for iteration 152...
2021-06-04 13:49:52 | [train_policy] epoch #152 | Logging diagnostics...
2021-06-04 13:49:52 | [train_policy] epoch #152 | Optimizing policy...
2021-06-04 13:49:52 | [train_policy] epoch #152 | Computing loss before
2021-06-04 13:49:52 | [train_policy] epoch #152 | Computing KL before
2021-06-04 13:49:52 | [train_policy] epoch #152 | Optimizing
2021-06-04 13:49:52 | [train_policy] epoch #152 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:52 | [train_policy] epoch #152 | computing loss before
2021-06-04 13:49:52 | [train_policy] epoch #152 | computing gradient
2021-06-04 13:49:52 | [train_policy] epoch #152 | gradient computed
2021-06-04 13:49:52 | [train_policy] epoch #152 | computing descent direction
2021-06-04 13:49:52 | [train_policy] epoch #152 | descent direction computed
2021-06-04 13:49:52 | [train_policy] epoch #152 | backtrack iters: 1
2021-06-04 13:49:52 | [train_policy] epoch #152 | optimization finished
2021-06-04 13:49:52 | [train_policy] epoch #152 | Computing KL after
2021-06-04 13:49:52 | [train_policy] epoch #152 | Computing loss after
2021-06-04 13:49:52 | [train_policy] epoch #152 | Fitting baseline...
2021-06-04 13:49:52 | [train_policy] epoch #152 | Saving snapshot...
2021-06-04 13:49:52 | [train_policy] epoch #152 | Saved
2021-06-04 13:49:52 | [train_policy] epoch #152 | Time 124.64 s
2021-06-04 13:49:52 | [train_policy] epoch #152 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284161
Evaluation/AverageDiscountedReturn          -65.5847
Evaluation/AverageReturn                    -65.5847
Evaluation/CompletionRate                     0
Evaluation/Iteration                        152
Evaluation/MaxReturn                        -34.7055
Evaluation/MinReturn                      -2061.75
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.308
Extras/EpisodeRewardMean                    -64.3661
LinearFeatureBaseline/ExplainedVariance       0.0105364
PolicyExecTime                                0.223196
ProcessExecTime                               0.0311205
TotalEnvSteps                            154836
policy/Entropy                                0.5476
policy/KL                                     0.00658868
policy/KLBefore                               0
policy/LossAfter                             -0.0389038
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             1.7291
policy/dLoss                                  0.0389038
---------------------------------------  ----------------
2021-06-04 13:49:52 | [train_policy] epoch #153 | Obtaining samples for iteration 153...
2021-06-04 13:49:53 | [train_policy] epoch #153 | Logging diagnostics...
2021-06-04 13:49:53 | [train_policy] epoch #153 | Optimizing policy...
2021-06-04 13:49:53 | [train_policy] epoch #153 | Computing loss before
2021-06-04 13:49:53 | [train_policy] epoch #153 | Computing KL before
2021-06-04 13:49:53 | [train_policy] epoch #153 | Optimizing
2021-06-04 13:49:53 | [train_policy] epoch #153 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:53 | [train_policy] epoch #153 | computing loss before
2021-06-04 13:49:53 | [train_policy] epoch #153 | computing gradient
2021-06-04 13:49:53 | [train_policy] epoch #153 | gradient computed
2021-06-04 13:49:53 | [train_policy] epoch #153 | computing descent direction
2021-06-04 13:49:53 | [train_policy] epoch #153 | descent direction computed
2021-06-04 13:49:53 | [train_policy] epoch #153 | backtrack iters: 1
2021-06-04 13:49:53 | [train_policy] epoch #153 | optimization finished
2021-06-04 13:49:53 | [train_policy] epoch #153 | Computing KL after
2021-06-04 13:49:53 | [train_policy] epoch #153 | Computing loss after
2021-06-04 13:49:53 | [train_policy] epoch #153 | Fitting baseline...
2021-06-04 13:49:53 | [train_policy] epoch #153 | Saving snapshot...
2021-06-04 13:49:53 | [train_policy] epoch #153 | Saved
2021-06-04 13:49:53 | [train_policy] epoch #153 | Time 125.45 s
2021-06-04 13:49:53 | [train_policy] epoch #153 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283959
Evaluation/AverageDiscountedReturn          -45.5113
Evaluation/AverageReturn                    -45.5113
Evaluation/CompletionRate                     0
Evaluation/Iteration                        153
Evaluation/MaxReturn                        -31.7119
Evaluation/MinReturn                        -64.0604
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.17174
Extras/EpisodeRewardMean                    -45.3993
LinearFeatureBaseline/ExplainedVariance     -62.2438
PolicyExecTime                                0.237081
ProcessExecTime                               0.0310166
TotalEnvSteps                            155848
policy/Entropy                                0.528737
policy/KL                                     0.00668352
policy/KLBefore                               0
policy/LossAfter                             -0.0210441
policy/LossBefore                             2.49727e-08
policy/Perplexity                             1.69679
policy/dLoss                                  0.0210441
---------------------------------------  ----------------
2021-06-04 13:49:53 | [train_policy] epoch #154 | Obtaining samples for iteration 154...
2021-06-04 13:49:54 | [train_policy] epoch #154 | Logging diagnostics...
2021-06-04 13:49:54 | [train_policy] epoch #154 | Optimizing policy...
2021-06-04 13:49:54 | [train_policy] epoch #154 | Computing loss before
2021-06-04 13:49:54 | [train_policy] epoch #154 | Computing KL before
2021-06-04 13:49:54 | [train_policy] epoch #154 | Optimizing
2021-06-04 13:49:54 | [train_policy] epoch #154 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:54 | [train_policy] epoch #154 | computing loss before
2021-06-04 13:49:54 | [train_policy] epoch #154 | computing gradient
2021-06-04 13:49:54 | [train_policy] epoch #154 | gradient computed
2021-06-04 13:49:54 | [train_policy] epoch #154 | computing descent direction
2021-06-04 13:49:54 | [train_policy] epoch #154 | descent direction computed
2021-06-04 13:49:54 | [train_policy] epoch #154 | backtrack iters: 1
2021-06-04 13:49:54 | [train_policy] epoch #154 | optimization finished
2021-06-04 13:49:54 | [train_policy] epoch #154 | Computing KL after
2021-06-04 13:49:54 | [train_policy] epoch #154 | Computing loss after
2021-06-04 13:49:54 | [train_policy] epoch #154 | Fitting baseline...
2021-06-04 13:49:54 | [train_policy] epoch #154 | Saving snapshot...
2021-06-04 13:49:54 | [train_policy] epoch #154 | Saved
2021-06-04 13:49:54 | [train_policy] epoch #154 | Time 126.26 s
2021-06-04 13:49:54 | [train_policy] epoch #154 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286773
Evaluation/AverageDiscountedReturn          -44.8465
Evaluation/AverageReturn                    -44.8465
Evaluation/CompletionRate                     0
Evaluation/Iteration                        154
Evaluation/MaxReturn                        -34.9064
Evaluation/MinReturn                        -63.9161
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.01313
Extras/EpisodeRewardMean                    -44.7391
LinearFeatureBaseline/ExplainedVariance       0.942784
PolicyExecTime                                0.227016
ProcessExecTime                               0.0313792
TotalEnvSteps                            156860
policy/Entropy                                0.534063
policy/KL                                     0.00673505
policy/KLBefore                               0
policy/LossAfter                             -0.0178246
policy/LossBefore                             1.86117e-08
policy/Perplexity                             1.70585
policy/dLoss                                  0.0178246
---------------------------------------  ----------------
2021-06-04 13:49:54 | [train_policy] epoch #155 | Obtaining samples for iteration 155...
2021-06-04 13:49:55 | [train_policy] epoch #155 | Logging diagnostics...
2021-06-04 13:49:55 | [train_policy] epoch #155 | Optimizing policy...
2021-06-04 13:49:55 | [train_policy] epoch #155 | Computing loss before
2021-06-04 13:49:55 | [train_policy] epoch #155 | Computing KL before
2021-06-04 13:49:55 | [train_policy] epoch #155 | Optimizing
2021-06-04 13:49:55 | [train_policy] epoch #155 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:55 | [train_policy] epoch #155 | computing loss before
2021-06-04 13:49:55 | [train_policy] epoch #155 | computing gradient
2021-06-04 13:49:55 | [train_policy] epoch #155 | gradient computed
2021-06-04 13:49:55 | [train_policy] epoch #155 | computing descent direction
2021-06-04 13:49:55 | [train_policy] epoch #155 | descent direction computed
2021-06-04 13:49:55 | [train_policy] epoch #155 | backtrack iters: 0
2021-06-04 13:49:55 | [train_policy] epoch #155 | optimization finished
2021-06-04 13:49:55 | [train_policy] epoch #155 | Computing KL after
2021-06-04 13:49:55 | [train_policy] epoch #155 | Computing loss after
2021-06-04 13:49:55 | [train_policy] epoch #155 | Fitting baseline...
2021-06-04 13:49:55 | [train_policy] epoch #155 | Saving snapshot...
2021-06-04 13:49:55 | [train_policy] epoch #155 | Saved
2021-06-04 13:49:55 | [train_policy] epoch #155 | Time 127.03 s
2021-06-04 13:49:55 | [train_policy] epoch #155 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                   0.284489
Evaluation/AverageDiscountedReturn          -45.6875
Evaluation/AverageReturn                    -45.6875
Evaluation/CompletionRate                     0
Evaluation/Iteration                        155
Evaluation/MaxReturn                        -31.6236
Evaluation/MinReturn                        -64.4888
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.22457
Extras/EpisodeRewardMean                    -45.3509
LinearFeatureBaseline/ExplainedVariance       0.935073
PolicyExecTime                                0.206777
ProcessExecTime                               0.0310118
TotalEnvSteps                            157872
policy/Entropy                                0.488956
policy/KL                                     0.00982178
policy/KLBefore                               0
policy/LossAfter                             -0.0159109
policy/LossBefore                             1.0366e-08
policy/Perplexity                             1.63061
policy/dLoss                                  0.0159109
---------------------------------------  ---------------
2021-06-04 13:49:55 | [train_policy] epoch #156 | Obtaining samples for iteration 156...
2021-06-04 13:49:55 | [train_policy] epoch #156 | Logging diagnostics...
2021-06-04 13:49:55 | [train_policy] epoch #156 | Optimizing policy...
2021-06-04 13:49:55 | [train_policy] epoch #156 | Computing loss before
2021-06-04 13:49:55 | [train_policy] epoch #156 | Computing KL before
2021-06-04 13:49:55 | [train_policy] epoch #156 | Optimizing
2021-06-04 13:49:55 | [train_policy] epoch #156 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:55 | [train_policy] epoch #156 | computing loss before
2021-06-04 13:49:55 | [train_policy] epoch #156 | computing gradient
2021-06-04 13:49:55 | [train_policy] epoch #156 | gradient computed
2021-06-04 13:49:55 | [train_policy] epoch #156 | computing descent direction
2021-06-04 13:49:56 | [train_policy] epoch #156 | descent direction computed
2021-06-04 13:49:56 | [train_policy] epoch #156 | backtrack iters: 0
2021-06-04 13:49:56 | [train_policy] epoch #156 | optimization finished
2021-06-04 13:49:56 | [train_policy] epoch #156 | Computing KL after
2021-06-04 13:49:56 | [train_policy] epoch #156 | Computing loss after
2021-06-04 13:49:56 | [train_policy] epoch #156 | Fitting baseline...
2021-06-04 13:49:56 | [train_policy] epoch #156 | Saving snapshot...
2021-06-04 13:49:56 | [train_policy] epoch #156 | Saved
2021-06-04 13:49:56 | [train_policy] epoch #156 | Time 127.82 s
2021-06-04 13:49:56 | [train_policy] epoch #156 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.286203
Evaluation/AverageDiscountedReturn          -46.586
Evaluation/AverageReturn                    -46.586
Evaluation/CompletionRate                     0
Evaluation/Iteration                        156
Evaluation/MaxReturn                        -32.7634
Evaluation/MinReturn                        -86.8964
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.05156
Extras/EpisodeRewardMean                    -46.3213
LinearFeatureBaseline/ExplainedVariance       0.865855
PolicyExecTime                                0.228259
ProcessExecTime                               0.0312331
TotalEnvSteps                            158884
policy/Entropy                                0.4769
policy/KL                                     0.00972101
policy/KLBefore                               0
policy/LossAfter                             -0.023871
policy/LossBefore                            -1.7905e-08
policy/Perplexity                             1.61107
policy/dLoss                                  0.023871
---------------------------------------  ---------------
2021-06-04 13:49:56 | [train_policy] epoch #157 | Obtaining samples for iteration 157...
2021-06-04 13:49:56 | [train_policy] epoch #157 | Logging diagnostics...
2021-06-04 13:49:56 | [train_policy] epoch #157 | Optimizing policy...
2021-06-04 13:49:56 | [train_policy] epoch #157 | Computing loss before
2021-06-04 13:49:56 | [train_policy] epoch #157 | Computing KL before
2021-06-04 13:49:56 | [train_policy] epoch #157 | Optimizing
2021-06-04 13:49:56 | [train_policy] epoch #157 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:56 | [train_policy] epoch #157 | computing loss before
2021-06-04 13:49:56 | [train_policy] epoch #157 | computing gradient
2021-06-04 13:49:56 | [train_policy] epoch #157 | gradient computed
2021-06-04 13:49:56 | [train_policy] epoch #157 | computing descent direction
2021-06-04 13:49:56 | [train_policy] epoch #157 | descent direction computed
2021-06-04 13:49:56 | [train_policy] epoch #157 | backtrack iters: 1
2021-06-04 13:49:56 | [train_policy] epoch #157 | optimization finished
2021-06-04 13:49:56 | [train_policy] epoch #157 | Computing KL after
2021-06-04 13:49:56 | [train_policy] epoch #157 | Computing loss after
2021-06-04 13:49:56 | [train_policy] epoch #157 | Fitting baseline...
2021-06-04 13:49:56 | [train_policy] epoch #157 | Saving snapshot...
2021-06-04 13:49:56 | [train_policy] epoch #157 | Saved
2021-06-04 13:49:56 | [train_policy] epoch #157 | Time 128.61 s
2021-06-04 13:49:56 | [train_policy] epoch #157 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286285
Evaluation/AverageDiscountedReturn          -44.6015
Evaluation/AverageReturn                    -44.6015
Evaluation/CompletionRate                     0
Evaluation/Iteration                        157
Evaluation/MaxReturn                        -31.4987
Evaluation/MinReturn                        -73.0395
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5921
Extras/EpisodeRewardMean                    -44.7604
LinearFeatureBaseline/ExplainedVariance       0.909978
PolicyExecTime                                0.219609
ProcessExecTime                               0.0313637
TotalEnvSteps                            159896
policy/Entropy                                0.44126
policy/KL                                     0.00668524
policy/KLBefore                               0
policy/LossAfter                             -0.0167056
policy/LossBefore                            -1.71982e-08
policy/Perplexity                             1.55467
policy/dLoss                                  0.0167056
---------------------------------------  ----------------
2021-06-04 13:49:56 | [train_policy] epoch #158 | Obtaining samples for iteration 158...
2021-06-04 13:49:57 | [train_policy] epoch #158 | Logging diagnostics...
2021-06-04 13:49:57 | [train_policy] epoch #158 | Optimizing policy...
2021-06-04 13:49:57 | [train_policy] epoch #158 | Computing loss before
2021-06-04 13:49:57 | [train_policy] epoch #158 | Computing KL before
2021-06-04 13:49:57 | [train_policy] epoch #158 | Optimizing
2021-06-04 13:49:57 | [train_policy] epoch #158 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:57 | [train_policy] epoch #158 | computing loss before
2021-06-04 13:49:57 | [train_policy] epoch #158 | computing gradient
2021-06-04 13:49:57 | [train_policy] epoch #158 | gradient computed
2021-06-04 13:49:57 | [train_policy] epoch #158 | computing descent direction
2021-06-04 13:49:57 | [train_policy] epoch #158 | descent direction computed
2021-06-04 13:49:57 | [train_policy] epoch #158 | backtrack iters: 1
2021-06-04 13:49:57 | [train_policy] epoch #158 | optimization finished
2021-06-04 13:49:57 | [train_policy] epoch #158 | Computing KL after
2021-06-04 13:49:57 | [train_policy] epoch #158 | Computing loss after
2021-06-04 13:49:57 | [train_policy] epoch #158 | Fitting baseline...
2021-06-04 13:49:57 | [train_policy] epoch #158 | Saving snapshot...
2021-06-04 13:49:57 | [train_policy] epoch #158 | Saved
2021-06-04 13:49:57 | [train_policy] epoch #158 | Time 129.41 s
2021-06-04 13:49:57 | [train_policy] epoch #158 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286588
Evaluation/AverageDiscountedReturn          -45.788
Evaluation/AverageReturn                    -45.788
Evaluation/CompletionRate                     0
Evaluation/Iteration                        158
Evaluation/MaxReturn                        -31.7526
Evaluation/MinReturn                        -97.299
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.43319
Extras/EpisodeRewardMean                    -45.9258
LinearFeatureBaseline/ExplainedVariance       0.856083
PolicyExecTime                                0.225208
ProcessExecTime                               0.0313723
TotalEnvSteps                            160908
policy/Entropy                                0.43454
policy/KL                                     0.00650422
policy/KLBefore                               0
policy/LossAfter                             -0.0269845
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             1.54425
policy/dLoss                                  0.0269845
---------------------------------------  ----------------
2021-06-04 13:49:57 | [train_policy] epoch #159 | Obtaining samples for iteration 159...
2021-06-04 13:49:58 | [train_policy] epoch #159 | Logging diagnostics...
2021-06-04 13:49:58 | [train_policy] epoch #159 | Optimizing policy...
2021-06-04 13:49:58 | [train_policy] epoch #159 | Computing loss before
2021-06-04 13:49:58 | [train_policy] epoch #159 | Computing KL before
2021-06-04 13:49:58 | [train_policy] epoch #159 | Optimizing
2021-06-04 13:49:58 | [train_policy] epoch #159 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:58 | [train_policy] epoch #159 | computing loss before
2021-06-04 13:49:58 | [train_policy] epoch #159 | computing gradient
2021-06-04 13:49:58 | [train_policy] epoch #159 | gradient computed
2021-06-04 13:49:58 | [train_policy] epoch #159 | computing descent direction
2021-06-04 13:49:58 | [train_policy] epoch #159 | descent direction computed
2021-06-04 13:49:58 | [train_policy] epoch #159 | backtrack iters: 0
2021-06-04 13:49:58 | [train_policy] epoch #159 | optimization finished
2021-06-04 13:49:58 | [train_policy] epoch #159 | Computing KL after
2021-06-04 13:49:58 | [train_policy] epoch #159 | Computing loss after
2021-06-04 13:49:58 | [train_policy] epoch #159 | Fitting baseline...
2021-06-04 13:49:58 | [train_policy] epoch #159 | Saving snapshot...
2021-06-04 13:49:58 | [train_policy] epoch #159 | Saved
2021-06-04 13:49:58 | [train_policy] epoch #159 | Time 130.21 s
2021-06-04 13:49:58 | [train_policy] epoch #159 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284075
Evaluation/AverageDiscountedReturn          -44.3601
Evaluation/AverageReturn                    -44.3601
Evaluation/CompletionRate                     0
Evaluation/Iteration                        159
Evaluation/MaxReturn                        -34.5392
Evaluation/MinReturn                        -90.067
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.55758
Extras/EpisodeRewardMean                    -44.3695
LinearFeatureBaseline/ExplainedVariance       0.894864
PolicyExecTime                                0.225163
ProcessExecTime                               0.0310638
TotalEnvSteps                            161920
policy/Entropy                                0.410062
policy/KL                                     0.00929326
policy/KLBefore                               0
policy/LossAfter                             -0.0204551
policy/LossBefore                             5.18301e-09
policy/Perplexity                             1.50691
policy/dLoss                                  0.0204551
---------------------------------------  ----------------
2021-06-04 13:49:58 | [train_policy] epoch #160 | Obtaining samples for iteration 160...
2021-06-04 13:49:59 | [train_policy] epoch #160 | Logging diagnostics...
2021-06-04 13:49:59 | [train_policy] epoch #160 | Optimizing policy...
2021-06-04 13:49:59 | [train_policy] epoch #160 | Computing loss before
2021-06-04 13:49:59 | [train_policy] epoch #160 | Computing KL before
2021-06-04 13:49:59 | [train_policy] epoch #160 | Optimizing
2021-06-04 13:49:59 | [train_policy] epoch #160 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:59 | [train_policy] epoch #160 | computing loss before
2021-06-04 13:49:59 | [train_policy] epoch #160 | computing gradient
2021-06-04 13:49:59 | [train_policy] epoch #160 | gradient computed
2021-06-04 13:49:59 | [train_policy] epoch #160 | computing descent direction
2021-06-04 13:49:59 | [train_policy] epoch #160 | descent direction computed
2021-06-04 13:49:59 | [train_policy] epoch #160 | backtrack iters: 1
2021-06-04 13:49:59 | [train_policy] epoch #160 | optimization finished
2021-06-04 13:49:59 | [train_policy] epoch #160 | Computing KL after
2021-06-04 13:49:59 | [train_policy] epoch #160 | Computing loss after
2021-06-04 13:49:59 | [train_policy] epoch #160 | Fitting baseline...
2021-06-04 13:49:59 | [train_policy] epoch #160 | Saving snapshot...
2021-06-04 13:49:59 | [train_policy] epoch #160 | Saved
2021-06-04 13:49:59 | [train_policy] epoch #160 | Time 131.02 s
2021-06-04 13:49:59 | [train_policy] epoch #160 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285753
Evaluation/AverageDiscountedReturn          -45.2273
Evaluation/AverageReturn                    -45.2273
Evaluation/CompletionRate                     0
Evaluation/Iteration                        160
Evaluation/MaxReturn                        -34.4358
Evaluation/MinReturn                        -61.8481
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.85104
Extras/EpisodeRewardMean                    -45.3043
LinearFeatureBaseline/ExplainedVariance       0.929792
PolicyExecTime                                0.231545
ProcessExecTime                               0.0313222
TotalEnvSteps                            162932
policy/Entropy                                0.363349
policy/KL                                     0.00668143
policy/KLBefore                               0
policy/LossAfter                             -0.019816
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             1.43814
policy/dLoss                                  0.019816
---------------------------------------  ----------------
2021-06-04 13:49:59 | [train_policy] epoch #161 | Obtaining samples for iteration 161...
2021-06-04 13:49:59 | [train_policy] epoch #161 | Logging diagnostics...
2021-06-04 13:49:59 | [train_policy] epoch #161 | Optimizing policy...
2021-06-04 13:49:59 | [train_policy] epoch #161 | Computing loss before
2021-06-04 13:49:59 | [train_policy] epoch #161 | Computing KL before
2021-06-04 13:49:59 | [train_policy] epoch #161 | Optimizing
2021-06-04 13:49:59 | [train_policy] epoch #161 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:49:59 | [train_policy] epoch #161 | computing loss before
2021-06-04 13:49:59 | [train_policy] epoch #161 | computing gradient
2021-06-04 13:49:59 | [train_policy] epoch #161 | gradient computed
2021-06-04 13:49:59 | [train_policy] epoch #161 | computing descent direction
2021-06-04 13:50:00 | [train_policy] epoch #161 | descent direction computed
2021-06-04 13:50:00 | [train_policy] epoch #161 | backtrack iters: 1
2021-06-04 13:50:00 | [train_policy] epoch #161 | optimization finished
2021-06-04 13:50:00 | [train_policy] epoch #161 | Computing KL after
2021-06-04 13:50:00 | [train_policy] epoch #161 | Computing loss after
2021-06-04 13:50:00 | [train_policy] epoch #161 | Fitting baseline...
2021-06-04 13:50:00 | [train_policy] epoch #161 | Saving snapshot...
2021-06-04 13:50:00 | [train_policy] epoch #161 | Saved
2021-06-04 13:50:00 | [train_policy] epoch #161 | Time 131.80 s
2021-06-04 13:50:00 | [train_policy] epoch #161 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28493
Evaluation/AverageDiscountedReturn          -88.0373
Evaluation/AverageReturn                    -88.0373
Evaluation/CompletionRate                     0
Evaluation/Iteration                        161
Evaluation/MaxReturn                        -31.9471
Evaluation/MinReturn                      -2062.35
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.316
Extras/EpisodeRewardMean                    -84.4521
LinearFeatureBaseline/ExplainedVariance       0.00969469
PolicyExecTime                                0.213707
ProcessExecTime                               0.0311882
TotalEnvSteps                            163944
policy/Entropy                                0.353625
policy/KL                                     0.00658256
policy/KLBefore                               0
policy/LossAfter                             -0.0267377
policy/LossBefore                             1.69626e-08
policy/Perplexity                             1.42422
policy/dLoss                                  0.0267378
---------------------------------------  ----------------
2021-06-04 13:50:00 | [train_policy] epoch #162 | Obtaining samples for iteration 162...
2021-06-04 13:50:00 | [train_policy] epoch #162 | Logging diagnostics...
2021-06-04 13:50:00 | [train_policy] epoch #162 | Optimizing policy...
2021-06-04 13:50:00 | [train_policy] epoch #162 | Computing loss before
2021-06-04 13:50:00 | [train_policy] epoch #162 | Computing KL before
2021-06-04 13:50:00 | [train_policy] epoch #162 | Optimizing
2021-06-04 13:50:00 | [train_policy] epoch #162 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:00 | [train_policy] epoch #162 | computing loss before
2021-06-04 13:50:00 | [train_policy] epoch #162 | computing gradient
2021-06-04 13:50:00 | [train_policy] epoch #162 | gradient computed
2021-06-04 13:50:00 | [train_policy] epoch #162 | computing descent direction
2021-06-04 13:50:00 | [train_policy] epoch #162 | descent direction computed
2021-06-04 13:50:00 | [train_policy] epoch #162 | backtrack iters: 1
2021-06-04 13:50:00 | [train_policy] epoch #162 | optimization finished
2021-06-04 13:50:00 | [train_policy] epoch #162 | Computing KL after
2021-06-04 13:50:00 | [train_policy] epoch #162 | Computing loss after
2021-06-04 13:50:00 | [train_policy] epoch #162 | Fitting baseline...
2021-06-04 13:50:00 | [train_policy] epoch #162 | Saving snapshot...
2021-06-04 13:50:00 | [train_policy] epoch #162 | Saved
2021-06-04 13:50:00 | [train_policy] epoch #162 | Time 132.64 s
2021-06-04 13:50:00 | [train_policy] epoch #162 | EpochTime 0.81 s
---------------------------------------  ----------------
EnvExecTime                                   0.316291
Evaluation/AverageDiscountedReturn          -44.8287
Evaluation/AverageReturn                    -44.8287
Evaluation/CompletionRate                     0
Evaluation/Iteration                        162
Evaluation/MaxReturn                        -33.9803
Evaluation/MinReturn                        -75.9106
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.15647
Extras/EpisodeRewardMean                    -44.6563
LinearFeatureBaseline/ExplainedVariance     -70.9278
PolicyExecTime                                0.232267
ProcessExecTime                               0.0344834
TotalEnvSteps                            164956
policy/Entropy                                0.389513
policy/KL                                     0.00654257
policy/KLBefore                               0
policy/LossAfter                             -0.0201278
policy/LossBefore                             2.14388e-08
policy/Perplexity                             1.47626
policy/dLoss                                  0.0201278
---------------------------------------  ----------------
2021-06-04 13:50:00 | [train_policy] epoch #163 | Obtaining samples for iteration 163...
2021-06-04 13:50:01 | [train_policy] epoch #163 | Logging diagnostics...
2021-06-04 13:50:01 | [train_policy] epoch #163 | Optimizing policy...
2021-06-04 13:50:01 | [train_policy] epoch #163 | Computing loss before
2021-06-04 13:50:01 | [train_policy] epoch #163 | Computing KL before
2021-06-04 13:50:01 | [train_policy] epoch #163 | Optimizing
2021-06-04 13:50:01 | [train_policy] epoch #163 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:01 | [train_policy] epoch #163 | computing loss before
2021-06-04 13:50:01 | [train_policy] epoch #163 | computing gradient
2021-06-04 13:50:01 | [train_policy] epoch #163 | gradient computed
2021-06-04 13:50:01 | [train_policy] epoch #163 | computing descent direction
2021-06-04 13:50:01 | [train_policy] epoch #163 | descent direction computed
2021-06-04 13:50:01 | [train_policy] epoch #163 | backtrack iters: 1
2021-06-04 13:50:01 | [train_policy] epoch #163 | optimization finished
2021-06-04 13:50:01 | [train_policy] epoch #163 | Computing KL after
2021-06-04 13:50:01 | [train_policy] epoch #163 | Computing loss after
2021-06-04 13:50:01 | [train_policy] epoch #163 | Fitting baseline...
2021-06-04 13:50:01 | [train_policy] epoch #163 | Saving snapshot...
2021-06-04 13:50:01 | [train_policy] epoch #163 | Saved
2021-06-04 13:50:01 | [train_policy] epoch #163 | Time 133.44 s
2021-06-04 13:50:01 | [train_policy] epoch #163 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286086
Evaluation/AverageDiscountedReturn          -45.4608
Evaluation/AverageReturn                    -45.4608
Evaluation/CompletionRate                     0
Evaluation/Iteration                        163
Evaluation/MaxReturn                        -32.8479
Evaluation/MinReturn                        -64.3381
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.80714
Extras/EpisodeRewardMean                    -45.6084
LinearFeatureBaseline/ExplainedVariance       0.924287
PolicyExecTime                                0.226382
ProcessExecTime                               0.0317385
TotalEnvSteps                            165968
policy/Entropy                                0.352281
policy/KL                                     0.00662388
policy/KLBefore                               0
policy/LossAfter                             -0.0288375
policy/LossBefore                             1.08372e-08
policy/Perplexity                             1.42231
policy/dLoss                                  0.0288375
---------------------------------------  ----------------
2021-06-04 13:50:01 | [train_policy] epoch #164 | Obtaining samples for iteration 164...
2021-06-04 13:50:02 | [train_policy] epoch #164 | Logging diagnostics...
2021-06-04 13:50:02 | [train_policy] epoch #164 | Optimizing policy...
2021-06-04 13:50:02 | [train_policy] epoch #164 | Computing loss before
2021-06-04 13:50:02 | [train_policy] epoch #164 | Computing KL before
2021-06-04 13:50:02 | [train_policy] epoch #164 | Optimizing
2021-06-04 13:50:02 | [train_policy] epoch #164 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:02 | [train_policy] epoch #164 | computing loss before
2021-06-04 13:50:02 | [train_policy] epoch #164 | computing gradient
2021-06-04 13:50:02 | [train_policy] epoch #164 | gradient computed
2021-06-04 13:50:02 | [train_policy] epoch #164 | computing descent direction
2021-06-04 13:50:02 | [train_policy] epoch #164 | descent direction computed
2021-06-04 13:50:02 | [train_policy] epoch #164 | backtrack iters: 1
2021-06-04 13:50:02 | [train_policy] epoch #164 | optimization finished
2021-06-04 13:50:02 | [train_policy] epoch #164 | Computing KL after
2021-06-04 13:50:02 | [train_policy] epoch #164 | Computing loss after
2021-06-04 13:50:02 | [train_policy] epoch #164 | Fitting baseline...
2021-06-04 13:50:02 | [train_policy] epoch #164 | Saving snapshot...
2021-06-04 13:50:02 | [train_policy] epoch #164 | Saved
2021-06-04 13:50:02 | [train_policy] epoch #164 | Time 134.24 s
2021-06-04 13:50:02 | [train_policy] epoch #164 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.291068
Evaluation/AverageDiscountedReturn          -44.6657
Evaluation/AverageReturn                    -44.6657
Evaluation/CompletionRate                     0
Evaluation/Iteration                        164
Evaluation/MaxReturn                        -30.6551
Evaluation/MinReturn                        -63.6902
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.88175
Extras/EpisodeRewardMean                    -44.5334
LinearFeatureBaseline/ExplainedVariance       0.93807
PolicyExecTime                                0.223937
ProcessExecTime                               0.0318081
TotalEnvSteps                            166980
policy/Entropy                                0.329664
policy/KL                                     0.0064801
policy/KLBefore                               0
policy/LossAfter                             -0.0188174
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             1.3905
policy/dLoss                                  0.0188174
---------------------------------------  ----------------
2021-06-04 13:50:02 | [train_policy] epoch #165 | Obtaining samples for iteration 165...
2021-06-04 13:50:03 | [train_policy] epoch #165 | Logging diagnostics...
2021-06-04 13:50:03 | [train_policy] epoch #165 | Optimizing policy...
2021-06-04 13:50:03 | [train_policy] epoch #165 | Computing loss before
2021-06-04 13:50:03 | [train_policy] epoch #165 | Computing KL before
2021-06-04 13:50:03 | [train_policy] epoch #165 | Optimizing
2021-06-04 13:50:03 | [train_policy] epoch #165 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:03 | [train_policy] epoch #165 | computing loss before
2021-06-04 13:50:03 | [train_policy] epoch #165 | computing gradient
2021-06-04 13:50:03 | [train_policy] epoch #165 | gradient computed
2021-06-04 13:50:03 | [train_policy] epoch #165 | computing descent direction
2021-06-04 13:50:03 | [train_policy] epoch #165 | descent direction computed
2021-06-04 13:50:03 | [train_policy] epoch #165 | backtrack iters: 0
2021-06-04 13:50:03 | [train_policy] epoch #165 | optimization finished
2021-06-04 13:50:03 | [train_policy] epoch #165 | Computing KL after
2021-06-04 13:50:03 | [train_policy] epoch #165 | Computing loss after
2021-06-04 13:50:03 | [train_policy] epoch #165 | Fitting baseline...
2021-06-04 13:50:03 | [train_policy] epoch #165 | Saving snapshot...
2021-06-04 13:50:03 | [train_policy] epoch #165 | Saved
2021-06-04 13:50:03 | [train_policy] epoch #165 | Time 135.03 s
2021-06-04 13:50:03 | [train_policy] epoch #165 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284736
Evaluation/AverageDiscountedReturn          -45.6364
Evaluation/AverageReturn                    -45.6364
Evaluation/CompletionRate                     0
Evaluation/Iteration                        165
Evaluation/MaxReturn                        -34.8511
Evaluation/MinReturn                        -84.7726
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.7777
Extras/EpisodeRewardMean                    -45.6458
LinearFeatureBaseline/ExplainedVariance       0.870819
PolicyExecTime                                0.222244
ProcessExecTime                               0.0311718
TotalEnvSteps                            167992
policy/Entropy                                0.300718
policy/KL                                     0.00975992
policy/KLBefore                               0
policy/LossAfter                             -0.0260035
policy/LossBefore                             4.94742e-09
policy/Perplexity                             1.35083
policy/dLoss                                  0.0260035
---------------------------------------  ----------------
2021-06-04 13:50:03 | [train_policy] epoch #166 | Obtaining samples for iteration 166...
2021-06-04 13:50:03 | [train_policy] epoch #166 | Logging diagnostics...
2021-06-04 13:50:03 | [train_policy] epoch #166 | Optimizing policy...
2021-06-04 13:50:03 | [train_policy] epoch #166 | Computing loss before
2021-06-04 13:50:03 | [train_policy] epoch #166 | Computing KL before
2021-06-04 13:50:03 | [train_policy] epoch #166 | Optimizing
2021-06-04 13:50:03 | [train_policy] epoch #166 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:03 | [train_policy] epoch #166 | computing loss before
2021-06-04 13:50:03 | [train_policy] epoch #166 | computing gradient
2021-06-04 13:50:03 | [train_policy] epoch #166 | gradient computed
2021-06-04 13:50:03 | [train_policy] epoch #166 | computing descent direction
2021-06-04 13:50:04 | [train_policy] epoch #166 | descent direction computed
2021-06-04 13:50:04 | [train_policy] epoch #166 | backtrack iters: 1
2021-06-04 13:50:04 | [train_policy] epoch #166 | optimization finished
2021-06-04 13:50:04 | [train_policy] epoch #166 | Computing KL after
2021-06-04 13:50:04 | [train_policy] epoch #166 | Computing loss after
2021-06-04 13:50:04 | [train_policy] epoch #166 | Fitting baseline...
2021-06-04 13:50:04 | [train_policy] epoch #166 | Saving snapshot...
2021-06-04 13:50:04 | [train_policy] epoch #166 | Saved
2021-06-04 13:50:04 | [train_policy] epoch #166 | Time 135.82 s
2021-06-04 13:50:04 | [train_policy] epoch #166 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284874
Evaluation/AverageDiscountedReturn          -66.5861
Evaluation/AverageReturn                    -66.5861
Evaluation/CompletionRate                     0
Evaluation/Iteration                        166
Evaluation/MaxReturn                        -31.8293
Evaluation/MinReturn                      -2061.48
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.251
Extras/EpisodeRewardMean                    -64.6036
LinearFeatureBaseline/ExplainedVariance       0.013421
PolicyExecTime                                0.227545
ProcessExecTime                               0.0312243
TotalEnvSteps                            169004
policy/Entropy                                0.29745
policy/KL                                     0.00714505
policy/KLBefore                               0
policy/LossAfter                             -0.0181984
policy/LossBefore                             3.72235e-08
policy/Perplexity                             1.34642
policy/dLoss                                  0.0181985
---------------------------------------  ----------------
2021-06-04 13:50:04 | [train_policy] epoch #167 | Obtaining samples for iteration 167...
2021-06-04 13:50:04 | [train_policy] epoch #167 | Logging diagnostics...
2021-06-04 13:50:04 | [train_policy] epoch #167 | Optimizing policy...
2021-06-04 13:50:04 | [train_policy] epoch #167 | Computing loss before
2021-06-04 13:50:04 | [train_policy] epoch #167 | Computing KL before
2021-06-04 13:50:04 | [train_policy] epoch #167 | Optimizing
2021-06-04 13:50:04 | [train_policy] epoch #167 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:04 | [train_policy] epoch #167 | computing loss before
2021-06-04 13:50:04 | [train_policy] epoch #167 | computing gradient
2021-06-04 13:50:04 | [train_policy] epoch #167 | gradient computed
2021-06-04 13:50:04 | [train_policy] epoch #167 | computing descent direction
2021-06-04 13:50:04 | [train_policy] epoch #167 | descent direction computed
2021-06-04 13:50:04 | [train_policy] epoch #167 | backtrack iters: 0
2021-06-04 13:50:04 | [train_policy] epoch #167 | optimization finished
2021-06-04 13:50:04 | [train_policy] epoch #167 | Computing KL after
2021-06-04 13:50:04 | [train_policy] epoch #167 | Computing loss after
2021-06-04 13:50:04 | [train_policy] epoch #167 | Fitting baseline...
2021-06-04 13:50:04 | [train_policy] epoch #167 | Saving snapshot...
2021-06-04 13:50:04 | [train_policy] epoch #167 | Saved
2021-06-04 13:50:04 | [train_policy] epoch #167 | Time 136.61 s
2021-06-04 13:50:04 | [train_policy] epoch #167 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.285012
Evaluation/AverageDiscountedReturn          -66.388
Evaluation/AverageReturn                    -66.388
Evaluation/CompletionRate                     0
Evaluation/Iteration                        167
Evaluation/MaxReturn                        -32.4405
Evaluation/MinReturn                      -2061.86
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.277
Extras/EpisodeRewardMean                    -64.3671
LinearFeatureBaseline/ExplainedVariance       0.13031
PolicyExecTime                                0.223416
ProcessExecTime                               0.0312109
TotalEnvSteps                            170016
policy/Entropy                                0.259724
policy/KL                                     0.00961048
policy/KLBefore                               0
policy/LossAfter                             -0.0300383
policy/LossBefore                            -0
policy/Perplexity                             1.29657
policy/dLoss                                  0.0300383
---------------------------------------  ---------------
2021-06-04 13:50:04 | [train_policy] epoch #168 | Obtaining samples for iteration 168...
2021-06-04 13:50:05 | [train_policy] epoch #168 | Logging diagnostics...
2021-06-04 13:50:05 | [train_policy] epoch #168 | Optimizing policy...
2021-06-04 13:50:05 | [train_policy] epoch #168 | Computing loss before
2021-06-04 13:50:05 | [train_policy] epoch #168 | Computing KL before
2021-06-04 13:50:05 | [train_policy] epoch #168 | Optimizing
2021-06-04 13:50:05 | [train_policy] epoch #168 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:05 | [train_policy] epoch #168 | computing loss before
2021-06-04 13:50:05 | [train_policy] epoch #168 | computing gradient
2021-06-04 13:50:05 | [train_policy] epoch #168 | gradient computed
2021-06-04 13:50:05 | [train_policy] epoch #168 | computing descent direction
2021-06-04 13:50:05 | [train_policy] epoch #168 | descent direction computed
2021-06-04 13:50:05 | [train_policy] epoch #168 | backtrack iters: 1
2021-06-04 13:50:05 | [train_policy] epoch #168 | optimization finished
2021-06-04 13:50:05 | [train_policy] epoch #168 | Computing KL after
2021-06-04 13:50:05 | [train_policy] epoch #168 | Computing loss after
2021-06-04 13:50:05 | [train_policy] epoch #168 | Fitting baseline...
2021-06-04 13:50:05 | [train_policy] epoch #168 | Saving snapshot...
2021-06-04 13:50:05 | [train_policy] epoch #168 | Saved
2021-06-04 13:50:05 | [train_policy] epoch #168 | Time 137.42 s
2021-06-04 13:50:05 | [train_policy] epoch #168 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.289243
Evaluation/AverageDiscountedReturn          -44.4572
Evaluation/AverageReturn                    -44.4572
Evaluation/CompletionRate                     0
Evaluation/Iteration                        168
Evaluation/MaxReturn                        -31.154
Evaluation/MinReturn                        -66.7574
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.99917
Extras/EpisodeRewardMean                    -44.2078
LinearFeatureBaseline/ExplainedVariance     -27.1731
PolicyExecTime                                0.229254
ProcessExecTime                               0.0315506
TotalEnvSteps                            171028
policy/Entropy                                0.249772
policy/KL                                     0.00654406
policy/KLBefore                               0
policy/LossAfter                             -0.026131
policy/LossBefore                            -1.16618e-08
policy/Perplexity                             1.28373
policy/dLoss                                  0.026131
---------------------------------------  ----------------
2021-06-04 13:50:05 | [train_policy] epoch #169 | Obtaining samples for iteration 169...
2021-06-04 13:50:06 | [train_policy] epoch #169 | Logging diagnostics...
2021-06-04 13:50:06 | [train_policy] epoch #169 | Optimizing policy...
2021-06-04 13:50:06 | [train_policy] epoch #169 | Computing loss before
2021-06-04 13:50:06 | [train_policy] epoch #169 | Computing KL before
2021-06-04 13:50:06 | [train_policy] epoch #169 | Optimizing
2021-06-04 13:50:06 | [train_policy] epoch #169 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:06 | [train_policy] epoch #169 | computing loss before
2021-06-04 13:50:06 | [train_policy] epoch #169 | computing gradient
2021-06-04 13:50:06 | [train_policy] epoch #169 | gradient computed
2021-06-04 13:50:06 | [train_policy] epoch #169 | computing descent direction
2021-06-04 13:50:06 | [train_policy] epoch #169 | descent direction computed
2021-06-04 13:50:06 | [train_policy] epoch #169 | backtrack iters: 1
2021-06-04 13:50:06 | [train_policy] epoch #169 | optimization finished
2021-06-04 13:50:06 | [train_policy] epoch #169 | Computing KL after
2021-06-04 13:50:06 | [train_policy] epoch #169 | Computing loss after
2021-06-04 13:50:06 | [train_policy] epoch #169 | Fitting baseline...
2021-06-04 13:50:06 | [train_policy] epoch #169 | Saving snapshot...
2021-06-04 13:50:06 | [train_policy] epoch #169 | Saved
2021-06-04 13:50:06 | [train_policy] epoch #169 | Time 138.23 s
2021-06-04 13:50:06 | [train_policy] epoch #169 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284364
Evaluation/AverageDiscountedReturn          -66.7753
Evaluation/AverageReturn                    -66.7753
Evaluation/CompletionRate                     0
Evaluation/Iteration                        169
Evaluation/MaxReturn                        -33.5023
Evaluation/MinReturn                      -2061.79
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.277
Extras/EpisodeRewardMean                    -64.7111
LinearFeatureBaseline/ExplainedVariance       0.0093506
PolicyExecTime                                0.226804
ProcessExecTime                               0.0311825
TotalEnvSteps                            172040
policy/Entropy                                0.218096
policy/KL                                     0.00652286
policy/KLBefore                               0
policy/LossAfter                             -0.0283103
policy/LossBefore                             8.48129e-09
policy/Perplexity                             1.24371
policy/dLoss                                  0.0283103
---------------------------------------  ----------------
2021-06-04 13:50:06 | [train_policy] epoch #170 | Obtaining samples for iteration 170...
2021-06-04 13:50:07 | [train_policy] epoch #170 | Logging diagnostics...
2021-06-04 13:50:07 | [train_policy] epoch #170 | Optimizing policy...
2021-06-04 13:50:07 | [train_policy] epoch #170 | Computing loss before
2021-06-04 13:50:07 | [train_policy] epoch #170 | Computing KL before
2021-06-04 13:50:07 | [train_policy] epoch #170 | Optimizing
2021-06-04 13:50:07 | [train_policy] epoch #170 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:07 | [train_policy] epoch #170 | computing loss before
2021-06-04 13:50:07 | [train_policy] epoch #170 | computing gradient
2021-06-04 13:50:07 | [train_policy] epoch #170 | gradient computed
2021-06-04 13:50:07 | [train_policy] epoch #170 | computing descent direction
2021-06-04 13:50:07 | [train_policy] epoch #170 | descent direction computed
2021-06-04 13:50:07 | [train_policy] epoch #170 | backtrack iters: 1
2021-06-04 13:50:07 | [train_policy] epoch #170 | optimization finished
2021-06-04 13:50:07 | [train_policy] epoch #170 | Computing KL after
2021-06-04 13:50:07 | [train_policy] epoch #170 | Computing loss after
2021-06-04 13:50:07 | [train_policy] epoch #170 | Fitting baseline...
2021-06-04 13:50:07 | [train_policy] epoch #170 | Saving snapshot...
2021-06-04 13:50:07 | [train_policy] epoch #170 | Saved
2021-06-04 13:50:07 | [train_policy] epoch #170 | Time 139.05 s
2021-06-04 13:50:07 | [train_policy] epoch #170 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.2851
Evaluation/AverageDiscountedReturn          -44.6424
Evaluation/AverageReturn                    -44.6424
Evaluation/CompletionRate                     0
Evaluation/Iteration                        170
Evaluation/MaxReturn                        -35.518
Evaluation/MinReturn                        -82.8682
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.9842
Extras/EpisodeRewardMean                    -44.564
LinearFeatureBaseline/ExplainedVariance     -32.5274
PolicyExecTime                                0.234592
ProcessExecTime                               0.0312054
TotalEnvSteps                            173052
policy/Entropy                                0.192907
policy/KL                                     0.00666749
policy/KLBefore                               0
policy/LossAfter                             -0.0200765
policy/LossBefore                            -2.68574e-08
policy/Perplexity                             1.21277
policy/dLoss                                  0.0200765
---------------------------------------  ----------------
2021-06-04 13:50:07 | [train_policy] epoch #171 | Obtaining samples for iteration 171...
2021-06-04 13:50:07 | [train_policy] epoch #171 | Logging diagnostics...
2021-06-04 13:50:07 | [train_policy] epoch #171 | Optimizing policy...
2021-06-04 13:50:07 | [train_policy] epoch #171 | Computing loss before
2021-06-04 13:50:07 | [train_policy] epoch #171 | Computing KL before
2021-06-04 13:50:07 | [train_policy] epoch #171 | Optimizing
2021-06-04 13:50:07 | [train_policy] epoch #171 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:07 | [train_policy] epoch #171 | computing loss before
2021-06-04 13:50:07 | [train_policy] epoch #171 | computing gradient
2021-06-04 13:50:07 | [train_policy] epoch #171 | gradient computed
2021-06-04 13:50:07 | [train_policy] epoch #171 | computing descent direction
2021-06-04 13:50:08 | [train_policy] epoch #171 | descent direction computed
2021-06-04 13:50:08 | [train_policy] epoch #171 | backtrack iters: 1
2021-06-04 13:50:08 | [train_policy] epoch #171 | optimization finished
2021-06-04 13:50:08 | [train_policy] epoch #171 | Computing KL after
2021-06-04 13:50:08 | [train_policy] epoch #171 | Computing loss after
2021-06-04 13:50:08 | [train_policy] epoch #171 | Fitting baseline...
2021-06-04 13:50:08 | [train_policy] epoch #171 | Saving snapshot...
2021-06-04 13:50:08 | [train_policy] epoch #171 | Saved
2021-06-04 13:50:08 | [train_policy] epoch #171 | Time 139.85 s
2021-06-04 13:50:08 | [train_policy] epoch #171 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286137
Evaluation/AverageDiscountedReturn          -66.0652
Evaluation/AverageReturn                    -66.0652
Evaluation/CompletionRate                     0
Evaluation/Iteration                        171
Evaluation/MaxReturn                        -32.942
Evaluation/MinReturn                      -2061.57
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.297
Extras/EpisodeRewardMean                    -64.7552
LinearFeatureBaseline/ExplainedVariance       0.0146075
PolicyExecTime                                0.226278
ProcessExecTime                               0.0312634
TotalEnvSteps                            174064
policy/Entropy                                0.218912
policy/KL                                     0.00679613
policy/KLBefore                               0
policy/LossAfter                             -0.0206859
policy/LossBefore                             4.24065e-09
policy/Perplexity                             1.24472
policy/dLoss                                  0.0206859
---------------------------------------  ----------------
2021-06-04 13:50:08 | [train_policy] epoch #172 | Obtaining samples for iteration 172...
2021-06-04 13:50:08 | [train_policy] epoch #172 | Logging diagnostics...
2021-06-04 13:50:08 | [train_policy] epoch #172 | Optimizing policy...
2021-06-04 13:50:08 | [train_policy] epoch #172 | Computing loss before
2021-06-04 13:50:08 | [train_policy] epoch #172 | Computing KL before
2021-06-04 13:50:08 | [train_policy] epoch #172 | Optimizing
2021-06-04 13:50:08 | [train_policy] epoch #172 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:08 | [train_policy] epoch #172 | computing loss before
2021-06-04 13:50:08 | [train_policy] epoch #172 | computing gradient
2021-06-04 13:50:08 | [train_policy] epoch #172 | gradient computed
2021-06-04 13:50:08 | [train_policy] epoch #172 | computing descent direction
2021-06-04 13:50:08 | [train_policy] epoch #172 | descent direction computed
2021-06-04 13:50:08 | [train_policy] epoch #172 | backtrack iters: 0
2021-06-04 13:50:08 | [train_policy] epoch #172 | optimization finished
2021-06-04 13:50:08 | [train_policy] epoch #172 | Computing KL after
2021-06-04 13:50:08 | [train_policy] epoch #172 | Computing loss after
2021-06-04 13:50:08 | [train_policy] epoch #172 | Fitting baseline...
2021-06-04 13:50:08 | [train_policy] epoch #172 | Saving snapshot...
2021-06-04 13:50:08 | [train_policy] epoch #172 | Saved
2021-06-04 13:50:08 | [train_policy] epoch #172 | Time 140.63 s
2021-06-04 13:50:08 | [train_policy] epoch #172 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284579
Evaluation/AverageDiscountedReturn          -44.3938
Evaluation/AverageReturn                    -44.3938
Evaluation/CompletionRate                     0
Evaluation/Iteration                        172
Evaluation/MaxReturn                        -35.0557
Evaluation/MinReturn                        -64.3531
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.21785
Extras/EpisodeRewardMean                    -44.3583
LinearFeatureBaseline/ExplainedVariance     -17.9996
PolicyExecTime                                0.213232
ProcessExecTime                               0.0311387
TotalEnvSteps                            175076
policy/Entropy                                0.236405
policy/KL                                     0.00936595
policy/KLBefore                               0
policy/LossAfter                             -0.0214072
policy/LossBefore                            -1.64914e-08
policy/Perplexity                             1.26669
policy/dLoss                                  0.0214071
---------------------------------------  ----------------
2021-06-04 13:50:08 | [train_policy] epoch #173 | Obtaining samples for iteration 173...
2021-06-04 13:50:09 | [train_policy] epoch #173 | Logging diagnostics...
2021-06-04 13:50:09 | [train_policy] epoch #173 | Optimizing policy...
2021-06-04 13:50:09 | [train_policy] epoch #173 | Computing loss before
2021-06-04 13:50:09 | [train_policy] epoch #173 | Computing KL before
2021-06-04 13:50:09 | [train_policy] epoch #173 | Optimizing
2021-06-04 13:50:09 | [train_policy] epoch #173 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:09 | [train_policy] epoch #173 | computing loss before
2021-06-04 13:50:09 | [train_policy] epoch #173 | computing gradient
2021-06-04 13:50:09 | [train_policy] epoch #173 | gradient computed
2021-06-04 13:50:09 | [train_policy] epoch #173 | computing descent direction
2021-06-04 13:50:09 | [train_policy] epoch #173 | descent direction computed
2021-06-04 13:50:09 | [train_policy] epoch #173 | backtrack iters: 0
2021-06-04 13:50:09 | [train_policy] epoch #173 | optimization finished
2021-06-04 13:50:09 | [train_policy] epoch #173 | Computing KL after
2021-06-04 13:50:09 | [train_policy] epoch #173 | Computing loss after
2021-06-04 13:50:09 | [train_policy] epoch #173 | Fitting baseline...
2021-06-04 13:50:09 | [train_policy] epoch #173 | Saving snapshot...
2021-06-04 13:50:09 | [train_policy] epoch #173 | Saved
2021-06-04 13:50:09 | [train_policy] epoch #173 | Time 141.43 s
2021-06-04 13:50:09 | [train_policy] epoch #173 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284779
Evaluation/AverageDiscountedReturn          -66.5157
Evaluation/AverageReturn                    -66.5157
Evaluation/CompletionRate                     0
Evaluation/Iteration                        173
Evaluation/MaxReturn                        -35.0918
Evaluation/MinReturn                      -2061.33
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.269
Extras/EpisodeRewardMean                    -64.7923
LinearFeatureBaseline/ExplainedVariance       0.0114437
PolicyExecTime                                0.228642
ProcessExecTime                               0.0311606
TotalEnvSteps                            176088
policy/Entropy                                0.281066
policy/KL                                     0.00998825
policy/KLBefore                               0
policy/LossAfter                             -0.0204894
policy/LossBefore                            -1.88473e-08
policy/Perplexity                             1.32454
policy/dLoss                                  0.0204894
---------------------------------------  ----------------
2021-06-04 13:50:09 | [train_policy] epoch #174 | Obtaining samples for iteration 174...
2021-06-04 13:50:10 | [train_policy] epoch #174 | Logging diagnostics...
2021-06-04 13:50:10 | [train_policy] epoch #174 | Optimizing policy...
2021-06-04 13:50:10 | [train_policy] epoch #174 | Computing loss before
2021-06-04 13:50:10 | [train_policy] epoch #174 | Computing KL before
2021-06-04 13:50:10 | [train_policy] epoch #174 | Optimizing
2021-06-04 13:50:10 | [train_policy] epoch #174 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:10 | [train_policy] epoch #174 | computing loss before
2021-06-04 13:50:10 | [train_policy] epoch #174 | computing gradient
2021-06-04 13:50:10 | [train_policy] epoch #174 | gradient computed
2021-06-04 13:50:10 | [train_policy] epoch #174 | computing descent direction
2021-06-04 13:50:10 | [train_policy] epoch #174 | descent direction computed
2021-06-04 13:50:10 | [train_policy] epoch #174 | backtrack iters: 0
2021-06-04 13:50:10 | [train_policy] epoch #174 | optimization finished
2021-06-04 13:50:10 | [train_policy] epoch #174 | Computing KL after
2021-06-04 13:50:10 | [train_policy] epoch #174 | Computing loss after
2021-06-04 13:50:10 | [train_policy] epoch #174 | Fitting baseline...
2021-06-04 13:50:10 | [train_policy] epoch #174 | Saving snapshot...
2021-06-04 13:50:10 | [train_policy] epoch #174 | Saved
2021-06-04 13:50:10 | [train_policy] epoch #174 | Time 142.22 s
2021-06-04 13:50:10 | [train_policy] epoch #174 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284771
Evaluation/AverageDiscountedReturn          -89.8227
Evaluation/AverageReturn                    -89.8227
Evaluation/CompletionRate                     0
Evaluation/Iteration                        174
Evaluation/MaxReturn                        -33.5767
Evaluation/MinReturn                      -2062.19
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        293.807
Extras/EpisodeRewardMean                    -86.1563
LinearFeatureBaseline/ExplainedVariance       0.117162
PolicyExecTime                                0.225415
ProcessExecTime                               0.0310922
TotalEnvSteps                            177100
policy/Entropy                                0.239909
policy/KL                                     0.00941875
policy/KLBefore                               0
policy/LossAfter                             -0.0229328
policy/LossBefore                             2.35591e-09
policy/Perplexity                             1.27113
policy/dLoss                                  0.0229328
---------------------------------------  ----------------
2021-06-04 13:50:10 | [train_policy] epoch #175 | Obtaining samples for iteration 175...
2021-06-04 13:50:11 | [train_policy] epoch #175 | Logging diagnostics...
2021-06-04 13:50:11 | [train_policy] epoch #175 | Optimizing policy...
2021-06-04 13:50:11 | [train_policy] epoch #175 | Computing loss before
2021-06-04 13:50:11 | [train_policy] epoch #175 | Computing KL before
2021-06-04 13:50:11 | [train_policy] epoch #175 | Optimizing
2021-06-04 13:50:11 | [train_policy] epoch #175 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:11 | [train_policy] epoch #175 | computing loss before
2021-06-04 13:50:11 | [train_policy] epoch #175 | computing gradient
2021-06-04 13:50:11 | [train_policy] epoch #175 | gradient computed
2021-06-04 13:50:11 | [train_policy] epoch #175 | computing descent direction
2021-06-04 13:50:11 | [train_policy] epoch #175 | descent direction computed
2021-06-04 13:50:11 | [train_policy] epoch #175 | backtrack iters: 0
2021-06-04 13:50:11 | [train_policy] epoch #175 | optimization finished
2021-06-04 13:50:11 | [train_policy] epoch #175 | Computing KL after
2021-06-04 13:50:11 | [train_policy] epoch #175 | Computing loss after
2021-06-04 13:50:11 | [train_policy] epoch #175 | Fitting baseline...
2021-06-04 13:50:11 | [train_policy] epoch #175 | Saving snapshot...
2021-06-04 13:50:11 | [train_policy] epoch #175 | Saved
2021-06-04 13:50:11 | [train_policy] epoch #175 | Time 143.01 s
2021-06-04 13:50:11 | [train_policy] epoch #175 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286739
Evaluation/AverageDiscountedReturn          -66.781
Evaluation/AverageReturn                    -66.781
Evaluation/CompletionRate                     0
Evaluation/Iteration                        175
Evaluation/MaxReturn                        -35.6807
Evaluation/MinReturn                      -2061.82
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.229
Extras/EpisodeRewardMean                    -85.0645
LinearFeatureBaseline/ExplainedVariance      -0.00213989
PolicyExecTime                                0.224474
ProcessExecTime                               0.0314052
TotalEnvSteps                            178112
policy/Entropy                                0.226144
policy/KL                                     0.00991394
policy/KLBefore                               0
policy/LossAfter                             -0.0187487
policy/LossBefore                             1.66092e-08
policy/Perplexity                             1.25376
policy/dLoss                                  0.0187487
---------------------------------------  ----------------
2021-06-04 13:50:11 | [train_policy] epoch #176 | Obtaining samples for iteration 176...
2021-06-04 13:50:11 | [train_policy] epoch #176 | Logging diagnostics...
2021-06-04 13:50:11 | [train_policy] epoch #176 | Optimizing policy...
2021-06-04 13:50:11 | [train_policy] epoch #176 | Computing loss before
2021-06-04 13:50:11 | [train_policy] epoch #176 | Computing KL before
2021-06-04 13:50:11 | [train_policy] epoch #176 | Optimizing
2021-06-04 13:50:11 | [train_policy] epoch #176 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:11 | [train_policy] epoch #176 | computing loss before
2021-06-04 13:50:11 | [train_policy] epoch #176 | computing gradient
2021-06-04 13:50:11 | [train_policy] epoch #176 | gradient computed
2021-06-04 13:50:11 | [train_policy] epoch #176 | computing descent direction
2021-06-04 13:50:12 | [train_policy] epoch #176 | descent direction computed
2021-06-04 13:50:12 | [train_policy] epoch #176 | backtrack iters: 1
2021-06-04 13:50:12 | [train_policy] epoch #176 | optimization finished
2021-06-04 13:50:12 | [train_policy] epoch #176 | Computing KL after
2021-06-04 13:50:12 | [train_policy] epoch #176 | Computing loss after
2021-06-04 13:50:12 | [train_policy] epoch #176 | Fitting baseline...
2021-06-04 13:50:12 | [train_policy] epoch #176 | Saving snapshot...
2021-06-04 13:50:12 | [train_policy] epoch #176 | Saved
2021-06-04 13:50:12 | [train_policy] epoch #176 | Time 143.79 s
2021-06-04 13:50:12 | [train_policy] epoch #176 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28412
Evaluation/AverageDiscountedReturn          -45.4531
Evaluation/AverageReturn                    -45.4531
Evaluation/CompletionRate                     0
Evaluation/Iteration                        176
Evaluation/MaxReturn                        -31.0439
Evaluation/MinReturn                        -64.423
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.27841
Extras/EpisodeRewardMean                    -45.2888
LinearFeatureBaseline/ExplainedVariance     -29.065
PolicyExecTime                                0.214958
ProcessExecTime                               0.0316272
TotalEnvSteps                            179124
policy/Entropy                                0.220261
policy/KL                                     0.00658132
policy/KLBefore                               0
policy/LossAfter                             -0.0219833
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             1.2464
policy/dLoss                                  0.0219833
---------------------------------------  ----------------
2021-06-04 13:50:12 | [train_policy] epoch #177 | Obtaining samples for iteration 177...
2021-06-04 13:50:12 | [train_policy] epoch #177 | Logging diagnostics...
2021-06-04 13:50:12 | [train_policy] epoch #177 | Optimizing policy...
2021-06-04 13:50:12 | [train_policy] epoch #177 | Computing loss before
2021-06-04 13:50:12 | [train_policy] epoch #177 | Computing KL before
2021-06-04 13:50:12 | [train_policy] epoch #177 | Optimizing
2021-06-04 13:50:12 | [train_policy] epoch #177 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:12 | [train_policy] epoch #177 | computing loss before
2021-06-04 13:50:12 | [train_policy] epoch #177 | computing gradient
2021-06-04 13:50:12 | [train_policy] epoch #177 | gradient computed
2021-06-04 13:50:12 | [train_policy] epoch #177 | computing descent direction
2021-06-04 13:50:12 | [train_policy] epoch #177 | descent direction computed
2021-06-04 13:50:12 | [train_policy] epoch #177 | backtrack iters: 0
2021-06-04 13:50:12 | [train_policy] epoch #177 | optimization finished
2021-06-04 13:50:12 | [train_policy] epoch #177 | Computing KL after
2021-06-04 13:50:12 | [train_policy] epoch #177 | Computing loss after
2021-06-04 13:50:12 | [train_policy] epoch #177 | Fitting baseline...
2021-06-04 13:50:12 | [train_policy] epoch #177 | Saving snapshot...
2021-06-04 13:50:12 | [train_policy] epoch #177 | Saved
2021-06-04 13:50:12 | [train_policy] epoch #177 | Time 144.59 s
2021-06-04 13:50:12 | [train_policy] epoch #177 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284993
Evaluation/AverageDiscountedReturn          -44.2234
Evaluation/AverageReturn                    -44.2234
Evaluation/CompletionRate                     0
Evaluation/Iteration                        177
Evaluation/MaxReturn                        -31.6563
Evaluation/MinReturn                        -64.1798
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.56775
Extras/EpisodeRewardMean                    -44.1449
LinearFeatureBaseline/ExplainedVariance       0.944053
PolicyExecTime                                0.229401
ProcessExecTime                               0.0311265
TotalEnvSteps                            180136
policy/Entropy                                0.269794
policy/KL                                     0.00949343
policy/KLBefore                               0
policy/LossAfter                             -0.0195504
policy/LossBefore                             9.65925e-09
policy/Perplexity                             1.3097
policy/dLoss                                  0.0195504
---------------------------------------  ----------------
2021-06-04 13:50:12 | [train_policy] epoch #178 | Obtaining samples for iteration 178...
2021-06-04 13:50:13 | [train_policy] epoch #178 | Logging diagnostics...
2021-06-04 13:50:13 | [train_policy] epoch #178 | Optimizing policy...
2021-06-04 13:50:13 | [train_policy] epoch #178 | Computing loss before
2021-06-04 13:50:13 | [train_policy] epoch #178 | Computing KL before
2021-06-04 13:50:13 | [train_policy] epoch #178 | Optimizing
2021-06-04 13:50:13 | [train_policy] epoch #178 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:13 | [train_policy] epoch #178 | computing loss before
2021-06-04 13:50:13 | [train_policy] epoch #178 | computing gradient
2021-06-04 13:50:13 | [train_policy] epoch #178 | gradient computed
2021-06-04 13:50:13 | [train_policy] epoch #178 | computing descent direction
2021-06-04 13:50:13 | [train_policy] epoch #178 | descent direction computed
2021-06-04 13:50:13 | [train_policy] epoch #178 | backtrack iters: 1
2021-06-04 13:50:13 | [train_policy] epoch #178 | optimization finished
2021-06-04 13:50:13 | [train_policy] epoch #178 | Computing KL after
2021-06-04 13:50:13 | [train_policy] epoch #178 | Computing loss after
2021-06-04 13:50:13 | [train_policy] epoch #178 | Fitting baseline...
2021-06-04 13:50:13 | [train_policy] epoch #178 | Saving snapshot...
2021-06-04 13:50:13 | [train_policy] epoch #178 | Saved
2021-06-04 13:50:13 | [train_policy] epoch #178 | Time 145.38 s
2021-06-04 13:50:13 | [train_policy] epoch #178 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285798
Evaluation/AverageDiscountedReturn          -45.2402
Evaluation/AverageReturn                    -45.2402
Evaluation/CompletionRate                     0
Evaluation/Iteration                        178
Evaluation/MaxReturn                        -31.207
Evaluation/MinReturn                       -108.337
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.73893
Extras/EpisodeRewardMean                    -45.2032
LinearFeatureBaseline/ExplainedVariance       0.811282
PolicyExecTime                                0.220264
ProcessExecTime                               0.0312712
TotalEnvSteps                            181148
policy/Entropy                                0.251946
policy/KL                                     0.00742948
policy/KLBefore                               0
policy/LossAfter                             -0.0272529
policy/LossBefore                             9.42366e-09
policy/Perplexity                             1.28653
policy/dLoss                                  0.0272529
---------------------------------------  ----------------
2021-06-04 13:50:13 | [train_policy] epoch #179 | Obtaining samples for iteration 179...
2021-06-04 13:50:14 | [train_policy] epoch #179 | Logging diagnostics...
2021-06-04 13:50:14 | [train_policy] epoch #179 | Optimizing policy...
2021-06-04 13:50:14 | [train_policy] epoch #179 | Computing loss before
2021-06-04 13:50:14 | [train_policy] epoch #179 | Computing KL before
2021-06-04 13:50:14 | [train_policy] epoch #179 | Optimizing
2021-06-04 13:50:14 | [train_policy] epoch #179 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:14 | [train_policy] epoch #179 | computing loss before
2021-06-04 13:50:14 | [train_policy] epoch #179 | computing gradient
2021-06-04 13:50:14 | [train_policy] epoch #179 | gradient computed
2021-06-04 13:50:14 | [train_policy] epoch #179 | computing descent direction
2021-06-04 13:50:14 | [train_policy] epoch #179 | descent direction computed
2021-06-04 13:50:14 | [train_policy] epoch #179 | backtrack iters: 1
2021-06-04 13:50:14 | [train_policy] epoch #179 | optimization finished
2021-06-04 13:50:14 | [train_policy] epoch #179 | Computing KL after
2021-06-04 13:50:14 | [train_policy] epoch #179 | Computing loss after
2021-06-04 13:50:14 | [train_policy] epoch #179 | Fitting baseline...
2021-06-04 13:50:14 | [train_policy] epoch #179 | Saving snapshot...
2021-06-04 13:50:14 | [train_policy] epoch #179 | Saved
2021-06-04 13:50:14 | [train_policy] epoch #179 | Time 146.18 s
2021-06-04 13:50:14 | [train_policy] epoch #179 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285831
Evaluation/AverageDiscountedReturn          -45.1251
Evaluation/AverageReturn                    -45.1251
Evaluation/CompletionRate                     0
Evaluation/Iteration                        179
Evaluation/MaxReturn                        -31.2813
Evaluation/MinReturn                       -116.076
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.0263
Extras/EpisodeRewardMean                    -45.3056
LinearFeatureBaseline/ExplainedVariance       0.784264
PolicyExecTime                                0.226123
ProcessExecTime                               0.0312812
TotalEnvSteps                            182160
policy/Entropy                                0.244551
policy/KL                                     0.00689063
policy/KLBefore                               0
policy/LossAfter                             -0.0253329
policy/LossBefore                             2.43837e-08
policy/Perplexity                             1.27705
policy/dLoss                                  0.025333
---------------------------------------  ----------------
2021-06-04 13:50:14 | [train_policy] epoch #180 | Obtaining samples for iteration 180...
2021-06-04 13:50:15 | [train_policy] epoch #180 | Logging diagnostics...
2021-06-04 13:50:15 | [train_policy] epoch #180 | Optimizing policy...
2021-06-04 13:50:15 | [train_policy] epoch #180 | Computing loss before
2021-06-04 13:50:15 | [train_policy] epoch #180 | Computing KL before
2021-06-04 13:50:15 | [train_policy] epoch #180 | Optimizing
2021-06-04 13:50:15 | [train_policy] epoch #180 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:15 | [train_policy] epoch #180 | computing loss before
2021-06-04 13:50:15 | [train_policy] epoch #180 | computing gradient
2021-06-04 13:50:15 | [train_policy] epoch #180 | gradient computed
2021-06-04 13:50:15 | [train_policy] epoch #180 | computing descent direction
2021-06-04 13:50:15 | [train_policy] epoch #180 | descent direction computed
2021-06-04 13:50:15 | [train_policy] epoch #180 | backtrack iters: 1
2021-06-04 13:50:15 | [train_policy] epoch #180 | optimization finished
2021-06-04 13:50:15 | [train_policy] epoch #180 | Computing KL after
2021-06-04 13:50:15 | [train_policy] epoch #180 | Computing loss after
2021-06-04 13:50:15 | [train_policy] epoch #180 | Fitting baseline...
2021-06-04 13:50:15 | [train_policy] epoch #180 | Saving snapshot...
2021-06-04 13:50:15 | [train_policy] epoch #180 | Saved
2021-06-04 13:50:15 | [train_policy] epoch #180 | Time 146.98 s
2021-06-04 13:50:15 | [train_policy] epoch #180 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285268
Evaluation/AverageDiscountedReturn          -45.0021
Evaluation/AverageReturn                    -45.0021
Evaluation/CompletionRate                     0
Evaluation/Iteration                        180
Evaluation/MaxReturn                        -31.8241
Evaluation/MinReturn                        -61.3908
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.25376
Extras/EpisodeRewardMean                    -44.7192
LinearFeatureBaseline/ExplainedVariance       0.901624
PolicyExecTime                                0.223963
ProcessExecTime                               0.031244
TotalEnvSteps                            183172
policy/Entropy                                0.170761
policy/KL                                     0.00671746
policy/KLBefore                               0
policy/LossAfter                             -0.0143055
policy/LossBefore                             1.75516e-08
policy/Perplexity                             1.18621
policy/dLoss                                  0.0143055
---------------------------------------  ----------------
2021-06-04 13:50:15 | [train_policy] epoch #181 | Obtaining samples for iteration 181...
2021-06-04 13:50:15 | [train_policy] epoch #181 | Logging diagnostics...
2021-06-04 13:50:15 | [train_policy] epoch #181 | Optimizing policy...
2021-06-04 13:50:15 | [train_policy] epoch #181 | Computing loss before
2021-06-04 13:50:15 | [train_policy] epoch #181 | Computing KL before
2021-06-04 13:50:15 | [train_policy] epoch #181 | Optimizing
2021-06-04 13:50:15 | [train_policy] epoch #181 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:15 | [train_policy] epoch #181 | computing loss before
2021-06-04 13:50:15 | [train_policy] epoch #181 | computing gradient
2021-06-04 13:50:15 | [train_policy] epoch #181 | gradient computed
2021-06-04 13:50:15 | [train_policy] epoch #181 | computing descent direction
2021-06-04 13:50:15 | [train_policy] epoch #181 | descent direction computed
2021-06-04 13:50:15 | [train_policy] epoch #181 | backtrack iters: 0
2021-06-04 13:50:15 | [train_policy] epoch #181 | optimization finished
2021-06-04 13:50:15 | [train_policy] epoch #181 | Computing KL after
2021-06-04 13:50:15 | [train_policy] epoch #181 | Computing loss after
2021-06-04 13:50:16 | [train_policy] epoch #181 | Fitting baseline...
2021-06-04 13:50:16 | [train_policy] epoch #181 | Saving snapshot...
2021-06-04 13:50:16 | [train_policy] epoch #181 | Saved
2021-06-04 13:50:16 | [train_policy] epoch #181 | Time 147.77 s
2021-06-04 13:50:16 | [train_policy] epoch #181 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285533
Evaluation/AverageDiscountedReturn          -45.0652
Evaluation/AverageReturn                    -45.0652
Evaluation/CompletionRate                     0
Evaluation/Iteration                        181
Evaluation/MaxReturn                        -33.7284
Evaluation/MinReturn                       -112.091
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.52698
Extras/EpisodeRewardMean                    -45.1672
LinearFeatureBaseline/ExplainedVariance       0.804532
PolicyExecTime                                0.223543
ProcessExecTime                               0.0312614
TotalEnvSteps                            184184
policy/Entropy                                0.210413
policy/KL                                     0.00956208
policy/KLBefore                               0
policy/LossAfter                             -0.0165361
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             1.23419
policy/dLoss                                  0.0165361
---------------------------------------  ----------------
2021-06-04 13:50:16 | [train_policy] epoch #182 | Obtaining samples for iteration 182...
2021-06-04 13:50:16 | [train_policy] epoch #182 | Logging diagnostics...
2021-06-04 13:50:16 | [train_policy] epoch #182 | Optimizing policy...
2021-06-04 13:50:16 | [train_policy] epoch #182 | Computing loss before
2021-06-04 13:50:16 | [train_policy] epoch #182 | Computing KL before
2021-06-04 13:50:16 | [train_policy] epoch #182 | Optimizing
2021-06-04 13:50:16 | [train_policy] epoch #182 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:16 | [train_policy] epoch #182 | computing loss before
2021-06-04 13:50:16 | [train_policy] epoch #182 | computing gradient
2021-06-04 13:50:16 | [train_policy] epoch #182 | gradient computed
2021-06-04 13:50:16 | [train_policy] epoch #182 | computing descent direction
2021-06-04 13:50:16 | [train_policy] epoch #182 | descent direction computed
2021-06-04 13:50:16 | [train_policy] epoch #182 | backtrack iters: 0
2021-06-04 13:50:16 | [train_policy] epoch #182 | optimization finished
2021-06-04 13:50:16 | [train_policy] epoch #182 | Computing KL after
2021-06-04 13:50:16 | [train_policy] epoch #182 | Computing loss after
2021-06-04 13:50:16 | [train_policy] epoch #182 | Fitting baseline...
2021-06-04 13:50:16 | [train_policy] epoch #182 | Saving snapshot...
2021-06-04 13:50:16 | [train_policy] epoch #182 | Saved
2021-06-04 13:50:16 | [train_policy] epoch #182 | Time 148.57 s
2021-06-04 13:50:16 | [train_policy] epoch #182 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287192
Evaluation/AverageDiscountedReturn          -67.8283
Evaluation/AverageReturn                    -67.8283
Evaluation/CompletionRate                     0
Evaluation/Iteration                        182
Evaluation/MaxReturn                        -29.9902
Evaluation/MinReturn                      -2062.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.198
Extras/EpisodeRewardMean                    -66.0299
LinearFeatureBaseline/ExplainedVariance       0.0164001
PolicyExecTime                                0.223581
ProcessExecTime                               0.0314455
TotalEnvSteps                            185196
policy/Entropy                                0.202536
policy/KL                                     0.0098185
policy/KLBefore                               0
policy/LossAfter                             -0.00926489
policy/LossBefore                            -1.13084e-08
policy/Perplexity                             1.2245
policy/dLoss                                  0.00926488
---------------------------------------  ----------------
2021-06-04 13:50:16 | [train_policy] epoch #183 | Obtaining samples for iteration 183...
2021-06-04 13:50:17 | [train_policy] epoch #183 | Logging diagnostics...
2021-06-04 13:50:17 | [train_policy] epoch #183 | Optimizing policy...
2021-06-04 13:50:17 | [train_policy] epoch #183 | Computing loss before
2021-06-04 13:50:17 | [train_policy] epoch #183 | Computing KL before
2021-06-04 13:50:17 | [train_policy] epoch #183 | Optimizing
2021-06-04 13:50:17 | [train_policy] epoch #183 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:17 | [train_policy] epoch #183 | computing loss before
2021-06-04 13:50:17 | [train_policy] epoch #183 | computing gradient
2021-06-04 13:50:17 | [train_policy] epoch #183 | gradient computed
2021-06-04 13:50:17 | [train_policy] epoch #183 | computing descent direction
2021-06-04 13:50:17 | [train_policy] epoch #183 | descent direction computed
2021-06-04 13:50:17 | [train_policy] epoch #183 | backtrack iters: 1
2021-06-04 13:50:17 | [train_policy] epoch #183 | optimization finished
2021-06-04 13:50:17 | [train_policy] epoch #183 | Computing KL after
2021-06-04 13:50:17 | [train_policy] epoch #183 | Computing loss after
2021-06-04 13:50:17 | [train_policy] epoch #183 | Fitting baseline...
2021-06-04 13:50:17 | [train_policy] epoch #183 | Saving snapshot...
2021-06-04 13:50:17 | [train_policy] epoch #183 | Saved
2021-06-04 13:50:17 | [train_policy] epoch #183 | Time 149.37 s
2021-06-04 13:50:17 | [train_policy] epoch #183 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28389
Evaluation/AverageDiscountedReturn          -43.8956
Evaluation/AverageReturn                    -43.8956
Evaluation/CompletionRate                     0
Evaluation/Iteration                        183
Evaluation/MaxReturn                        -31.1606
Evaluation/MinReturn                        -63.28
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.71984
Extras/EpisodeRewardMean                    -44.2082
LinearFeatureBaseline/ExplainedVariance     -14.4015
PolicyExecTime                                0.226727
ProcessExecTime                               0.0310471
TotalEnvSteps                            186208
policy/Entropy                                0.210065
policy/KL                                     0.00677512
policy/KLBefore                               0
policy/LossAfter                             -0.0148635
policy/LossBefore                            -4.16997e-08
policy/Perplexity                             1.23376
policy/dLoss                                  0.0148635
---------------------------------------  ----------------
2021-06-04 13:50:17 | [train_policy] epoch #184 | Obtaining samples for iteration 184...
2021-06-04 13:50:18 | [train_policy] epoch #184 | Logging diagnostics...
2021-06-04 13:50:18 | [train_policy] epoch #184 | Optimizing policy...
2021-06-04 13:50:18 | [train_policy] epoch #184 | Computing loss before
2021-06-04 13:50:18 | [train_policy] epoch #184 | Computing KL before
2021-06-04 13:50:18 | [train_policy] epoch #184 | Optimizing
2021-06-04 13:50:18 | [train_policy] epoch #184 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:18 | [train_policy] epoch #184 | computing loss before
2021-06-04 13:50:18 | [train_policy] epoch #184 | computing gradient
2021-06-04 13:50:18 | [train_policy] epoch #184 | gradient computed
2021-06-04 13:50:18 | [train_policy] epoch #184 | computing descent direction
2021-06-04 13:50:18 | [train_policy] epoch #184 | descent direction computed
2021-06-04 13:50:18 | [train_policy] epoch #184 | backtrack iters: 1
2021-06-04 13:50:18 | [train_policy] epoch #184 | optimization finished
2021-06-04 13:50:18 | [train_policy] epoch #184 | Computing KL after
2021-06-04 13:50:18 | [train_policy] epoch #184 | Computing loss after
2021-06-04 13:50:18 | [train_policy] epoch #184 | Fitting baseline...
2021-06-04 13:50:18 | [train_policy] epoch #184 | Saving snapshot...
2021-06-04 13:50:18 | [train_policy] epoch #184 | Saved
2021-06-04 13:50:18 | [train_policy] epoch #184 | Time 150.20 s
2021-06-04 13:50:18 | [train_policy] epoch #184 | EpochTime 0.81 s
---------------------------------------  ---------------
EnvExecTime                                   0.297182
Evaluation/AverageDiscountedReturn          -44.0902
Evaluation/AverageReturn                    -44.0902
Evaluation/CompletionRate                     0
Evaluation/Iteration                        184
Evaluation/MaxReturn                        -31.4304
Evaluation/MinReturn                       -117.946
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.86982
Extras/EpisodeRewardMean                    -44.2554
LinearFeatureBaseline/ExplainedVariance       0.773782
PolicyExecTime                                0.234882
ProcessExecTime                               0.0318973
TotalEnvSteps                            187220
policy/Entropy                                0.198938
policy/KL                                     0.00957542
policy/KLBefore                               0
policy/LossAfter                             -0.0283864
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             1.22011
policy/dLoss                                  0.0283864
---------------------------------------  ---------------
2021-06-04 13:50:18 | [train_policy] epoch #185 | Obtaining samples for iteration 185...
2021-06-04 13:50:19 | [train_policy] epoch #185 | Logging diagnostics...
2021-06-04 13:50:19 | [train_policy] epoch #185 | Optimizing policy...
2021-06-04 13:50:19 | [train_policy] epoch #185 | Computing loss before
2021-06-04 13:50:19 | [train_policy] epoch #185 | Computing KL before
2021-06-04 13:50:19 | [train_policy] epoch #185 | Optimizing
2021-06-04 13:50:19 | [train_policy] epoch #185 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:19 | [train_policy] epoch #185 | computing loss before
2021-06-04 13:50:19 | [train_policy] epoch #185 | computing gradient
2021-06-04 13:50:19 | [train_policy] epoch #185 | gradient computed
2021-06-04 13:50:19 | [train_policy] epoch #185 | computing descent direction
2021-06-04 13:50:19 | [train_policy] epoch #185 | descent direction computed
2021-06-04 13:50:19 | [train_policy] epoch #185 | backtrack iters: 1
2021-06-04 13:50:19 | [train_policy] epoch #185 | optimization finished
2021-06-04 13:50:19 | [train_policy] epoch #185 | Computing KL after
2021-06-04 13:50:19 | [train_policy] epoch #185 | Computing loss after
2021-06-04 13:50:19 | [train_policy] epoch #185 | Fitting baseline...
2021-06-04 13:50:19 | [train_policy] epoch #185 | Saving snapshot...
2021-06-04 13:50:19 | [train_policy] epoch #185 | Saved
2021-06-04 13:50:19 | [train_policy] epoch #185 | Time 151.00 s
2021-06-04 13:50:19 | [train_policy] epoch #185 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.287344
Evaluation/AverageDiscountedReturn          -67.2544
Evaluation/AverageReturn                    -67.2544
Evaluation/CompletionRate                     0
Evaluation/Iteration                        185
Evaluation/MaxReturn                        -31.0232
Evaluation/MinReturn                      -2061.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.222
Extras/EpisodeRewardMean                    -65.0953
LinearFeatureBaseline/ExplainedVariance       0.0176918
PolicyExecTime                                0.23349
ProcessExecTime                               0.0314741
TotalEnvSteps                            188232
policy/Entropy                                0.18943
policy/KL                                     0.00676649
policy/KLBefore                               0
policy/LossAfter                             -0.0247639
policy/LossBefore                             2.8271e-09
policy/Perplexity                             1.20856
policy/dLoss                                  0.0247639
---------------------------------------  ---------------
2021-06-04 13:50:19 | [train_policy] epoch #186 | Obtaining samples for iteration 186...
2021-06-04 13:50:19 | [train_policy] epoch #186 | Logging diagnostics...
2021-06-04 13:50:19 | [train_policy] epoch #186 | Optimizing policy...
2021-06-04 13:50:19 | [train_policy] epoch #186 | Computing loss before
2021-06-04 13:50:19 | [train_policy] epoch #186 | Computing KL before
2021-06-04 13:50:19 | [train_policy] epoch #186 | Optimizing
2021-06-04 13:50:19 | [train_policy] epoch #186 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:19 | [train_policy] epoch #186 | computing loss before
2021-06-04 13:50:19 | [train_policy] epoch #186 | computing gradient
2021-06-04 13:50:19 | [train_policy] epoch #186 | gradient computed
2021-06-04 13:50:19 | [train_policy] epoch #186 | computing descent direction
2021-06-04 13:50:20 | [train_policy] epoch #186 | descent direction computed
2021-06-04 13:50:20 | [train_policy] epoch #186 | backtrack iters: 1
2021-06-04 13:50:20 | [train_policy] epoch #186 | optimization finished
2021-06-04 13:50:20 | [train_policy] epoch #186 | Computing KL after
2021-06-04 13:50:20 | [train_policy] epoch #186 | Computing loss after
2021-06-04 13:50:20 | [train_policy] epoch #186 | Fitting baseline...
2021-06-04 13:50:20 | [train_policy] epoch #186 | Saving snapshot...
2021-06-04 13:50:20 | [train_policy] epoch #186 | Saved
2021-06-04 13:50:20 | [train_policy] epoch #186 | Time 151.81 s
2021-06-04 13:50:20 | [train_policy] epoch #186 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.2851
Evaluation/AverageDiscountedReturn          -43.4992
Evaluation/AverageReturn                    -43.4992
Evaluation/CompletionRate                     0
Evaluation/Iteration                        186
Evaluation/MaxReturn                        -31.3363
Evaluation/MinReturn                        -65.2737
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.5675
Extras/EpisodeRewardMean                    -43.7627
LinearFeatureBaseline/ExplainedVariance     -16.9979
PolicyExecTime                                0.234409
ProcessExecTime                               0.031384
TotalEnvSteps                            189244
policy/Entropy                                0.187037
policy/KL                                     0.00680757
policy/KLBefore                               0
policy/LossAfter                             -0.0189718
policy/LossBefore                             2.3088e-08
policy/Perplexity                             1.20567
policy/dLoss                                  0.0189718
---------------------------------------  ---------------
2021-06-04 13:50:20 | [train_policy] epoch #187 | Obtaining samples for iteration 187...
2021-06-04 13:50:20 | [train_policy] epoch #187 | Logging diagnostics...
2021-06-04 13:50:20 | [train_policy] epoch #187 | Optimizing policy...
2021-06-04 13:50:20 | [train_policy] epoch #187 | Computing loss before
2021-06-04 13:50:20 | [train_policy] epoch #187 | Computing KL before
2021-06-04 13:50:20 | [train_policy] epoch #187 | Optimizing
2021-06-04 13:50:20 | [train_policy] epoch #187 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:20 | [train_policy] epoch #187 | computing loss before
2021-06-04 13:50:20 | [train_policy] epoch #187 | computing gradient
2021-06-04 13:50:20 | [train_policy] epoch #187 | gradient computed
2021-06-04 13:50:20 | [train_policy] epoch #187 | computing descent direction
2021-06-04 13:50:20 | [train_policy] epoch #187 | descent direction computed
2021-06-04 13:50:20 | [train_policy] epoch #187 | backtrack iters: 1
2021-06-04 13:50:20 | [train_policy] epoch #187 | optimization finished
2021-06-04 13:50:20 | [train_policy] epoch #187 | Computing KL after
2021-06-04 13:50:20 | [train_policy] epoch #187 | Computing loss after
2021-06-04 13:50:20 | [train_policy] epoch #187 | Fitting baseline...
2021-06-04 13:50:20 | [train_policy] epoch #187 | Saving snapshot...
2021-06-04 13:50:20 | [train_policy] epoch #187 | Saved
2021-06-04 13:50:20 | [train_policy] epoch #187 | Time 152.61 s
2021-06-04 13:50:20 | [train_policy] epoch #187 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287192
Evaluation/AverageDiscountedReturn          -65.5109
Evaluation/AverageReturn                    -65.5109
Evaluation/CompletionRate                     0
Evaluation/Iteration                        187
Evaluation/MaxReturn                        -33.7788
Evaluation/MinReturn                      -2061.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.351
Extras/EpisodeRewardMean                    -63.4355
LinearFeatureBaseline/ExplainedVariance       0.0117896
PolicyExecTime                                0.228086
ProcessExecTime                               0.031414
TotalEnvSteps                            190256
policy/Entropy                                0.207163
policy/KL                                     0.00683569
policy/KLBefore                               0
policy/LossAfter                             -0.0177993
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             1.23018
policy/dLoss                                  0.0177993
---------------------------------------  ----------------
2021-06-04 13:50:20 | [train_policy] epoch #188 | Obtaining samples for iteration 188...
2021-06-04 13:50:21 | [train_policy] epoch #188 | Logging diagnostics...
2021-06-04 13:50:21 | [train_policy] epoch #188 | Optimizing policy...
2021-06-04 13:50:21 | [train_policy] epoch #188 | Computing loss before
2021-06-04 13:50:21 | [train_policy] epoch #188 | Computing KL before
2021-06-04 13:50:21 | [train_policy] epoch #188 | Optimizing
2021-06-04 13:50:21 | [train_policy] epoch #188 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:21 | [train_policy] epoch #188 | computing loss before
2021-06-04 13:50:21 | [train_policy] epoch #188 | computing gradient
2021-06-04 13:50:21 | [train_policy] epoch #188 | gradient computed
2021-06-04 13:50:21 | [train_policy] epoch #188 | computing descent direction
2021-06-04 13:50:21 | [train_policy] epoch #188 | descent direction computed
2021-06-04 13:50:21 | [train_policy] epoch #188 | backtrack iters: 0
2021-06-04 13:50:21 | [train_policy] epoch #188 | optimization finished
2021-06-04 13:50:21 | [train_policy] epoch #188 | Computing KL after
2021-06-04 13:50:21 | [train_policy] epoch #188 | Computing loss after
2021-06-04 13:50:21 | [train_policy] epoch #188 | Fitting baseline...
2021-06-04 13:50:21 | [train_policy] epoch #188 | Saving snapshot...
2021-06-04 13:50:21 | [train_policy] epoch #188 | Saved
2021-06-04 13:50:21 | [train_policy] epoch #188 | Time 153.40 s
2021-06-04 13:50:21 | [train_policy] epoch #188 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285322
Evaluation/AverageDiscountedReturn          -44.293
Evaluation/AverageReturn                    -44.293
Evaluation/CompletionRate                     0
Evaluation/Iteration                        188
Evaluation/MaxReturn                        -32.9737
Evaluation/MinReturn                        -64.5929
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.60151
Extras/EpisodeRewardMean                    -64.6557
LinearFeatureBaseline/ExplainedVariance     -29.9733
PolicyExecTime                                0.220209
ProcessExecTime                               0.0312479
TotalEnvSteps                            191268
policy/Entropy                                0.201377
policy/KL                                     0.00979278
policy/KLBefore                               0
policy/LossAfter                             -0.0229246
policy/LossBefore                            -2.96845e-08
policy/Perplexity                             1.22309
policy/dLoss                                  0.0229245
---------------------------------------  ----------------
2021-06-04 13:50:21 | [train_policy] epoch #189 | Obtaining samples for iteration 189...
2021-06-04 13:50:22 | [train_policy] epoch #189 | Logging diagnostics...
2021-06-04 13:50:22 | [train_policy] epoch #189 | Optimizing policy...
2021-06-04 13:50:22 | [train_policy] epoch #189 | Computing loss before
2021-06-04 13:50:22 | [train_policy] epoch #189 | Computing KL before
2021-06-04 13:50:22 | [train_policy] epoch #189 | Optimizing
2021-06-04 13:50:22 | [train_policy] epoch #189 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:22 | [train_policy] epoch #189 | computing loss before
2021-06-04 13:50:22 | [train_policy] epoch #189 | computing gradient
2021-06-04 13:50:22 | [train_policy] epoch #189 | gradient computed
2021-06-04 13:50:22 | [train_policy] epoch #189 | computing descent direction
2021-06-04 13:50:22 | [train_policy] epoch #189 | descent direction computed
2021-06-04 13:50:22 | [train_policy] epoch #189 | backtrack iters: 1
2021-06-04 13:50:22 | [train_policy] epoch #189 | optimization finished
2021-06-04 13:50:22 | [train_policy] epoch #189 | Computing KL after
2021-06-04 13:50:22 | [train_policy] epoch #189 | Computing loss after
2021-06-04 13:50:22 | [train_policy] epoch #189 | Fitting baseline...
2021-06-04 13:50:22 | [train_policy] epoch #189 | Saving snapshot...
2021-06-04 13:50:22 | [train_policy] epoch #189 | Saved
2021-06-04 13:50:22 | [train_policy] epoch #189 | Time 154.21 s
2021-06-04 13:50:22 | [train_policy] epoch #189 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285517
Evaluation/AverageDiscountedReturn          -45.8219
Evaluation/AverageReturn                    -45.8219
Evaluation/CompletionRate                     0
Evaluation/Iteration                        189
Evaluation/MaxReturn                        -30.8205
Evaluation/MinReturn                        -93.2808
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.73201
Extras/EpisodeRewardMean                    -45.7303
LinearFeatureBaseline/ExplainedVariance       0.88448
PolicyExecTime                                0.234119
ProcessExecTime                               0.0312099
TotalEnvSteps                            192280
policy/Entropy                                0.194669
policy/KL                                     0.00645667
policy/KLBefore                               0
policy/LossAfter                             -0.0159185
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             1.21491
policy/dLoss                                  0.0159185
---------------------------------------  ----------------
2021-06-04 13:50:22 | [train_policy] epoch #190 | Obtaining samples for iteration 190...
2021-06-04 13:50:23 | [train_policy] epoch #190 | Logging diagnostics...
2021-06-04 13:50:23 | [train_policy] epoch #190 | Optimizing policy...
2021-06-04 13:50:23 | [train_policy] epoch #190 | Computing loss before
2021-06-04 13:50:23 | [train_policy] epoch #190 | Computing KL before
2021-06-04 13:50:23 | [train_policy] epoch #190 | Optimizing
2021-06-04 13:50:23 | [train_policy] epoch #190 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:23 | [train_policy] epoch #190 | computing loss before
2021-06-04 13:50:23 | [train_policy] epoch #190 | computing gradient
2021-06-04 13:50:23 | [train_policy] epoch #190 | gradient computed
2021-06-04 13:50:23 | [train_policy] epoch #190 | computing descent direction
2021-06-04 13:50:23 | [train_policy] epoch #190 | descent direction computed
2021-06-04 13:50:23 | [train_policy] epoch #190 | backtrack iters: 1
2021-06-04 13:50:23 | [train_policy] epoch #190 | optimization finished
2021-06-04 13:50:23 | [train_policy] epoch #190 | Computing KL after
2021-06-04 13:50:23 | [train_policy] epoch #190 | Computing loss after
2021-06-04 13:50:23 | [train_policy] epoch #190 | Fitting baseline...
2021-06-04 13:50:23 | [train_policy] epoch #190 | Saving snapshot...
2021-06-04 13:50:23 | [train_policy] epoch #190 | Saved
2021-06-04 13:50:23 | [train_policy] epoch #190 | Time 155.02 s
2021-06-04 13:50:23 | [train_policy] epoch #190 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28535
Evaluation/AverageDiscountedReturn          -46.0213
Evaluation/AverageReturn                    -46.0213
Evaluation/CompletionRate                     0
Evaluation/Iteration                        190
Evaluation/MaxReturn                        -31.464
Evaluation/MinReturn                       -180.842
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         15.732
Extras/EpisodeRewardMean                    -45.8167
LinearFeatureBaseline/ExplainedVariance       0.520064
PolicyExecTime                                0.23315
ProcessExecTime                               0.031183
TotalEnvSteps                            193292
policy/Entropy                                0.19253
policy/KL                                     0.00674462
policy/KLBefore                               0
policy/LossAfter                             -0.0272687
policy/LossBefore                             6.12538e-09
policy/Perplexity                             1.21231
policy/dLoss                                  0.0272687
---------------------------------------  ----------------
2021-06-04 13:50:23 | [train_policy] epoch #191 | Obtaining samples for iteration 191...
2021-06-04 13:50:23 | [train_policy] epoch #191 | Logging diagnostics...
2021-06-04 13:50:23 | [train_policy] epoch #191 | Optimizing policy...
2021-06-04 13:50:23 | [train_policy] epoch #191 | Computing loss before
2021-06-04 13:50:23 | [train_policy] epoch #191 | Computing KL before
2021-06-04 13:50:23 | [train_policy] epoch #191 | Optimizing
2021-06-04 13:50:23 | [train_policy] epoch #191 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:23 | [train_policy] epoch #191 | computing loss before
2021-06-04 13:50:23 | [train_policy] epoch #191 | computing gradient
2021-06-04 13:50:23 | [train_policy] epoch #191 | gradient computed
2021-06-04 13:50:23 | [train_policy] epoch #191 | computing descent direction
2021-06-04 13:50:24 | [train_policy] epoch #191 | descent direction computed
2021-06-04 13:50:24 | [train_policy] epoch #191 | backtrack iters: 1
2021-06-04 13:50:24 | [train_policy] epoch #191 | optimization finished
2021-06-04 13:50:24 | [train_policy] epoch #191 | Computing KL after
2021-06-04 13:50:24 | [train_policy] epoch #191 | Computing loss after
2021-06-04 13:50:24 | [train_policy] epoch #191 | Fitting baseline...
2021-06-04 13:50:24 | [train_policy] epoch #191 | Saving snapshot...
2021-06-04 13:50:24 | [train_policy] epoch #191 | Saved
2021-06-04 13:50:24 | [train_policy] epoch #191 | Time 155.82 s
2021-06-04 13:50:24 | [train_policy] epoch #191 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285661
Evaluation/AverageDiscountedReturn          -67.6331
Evaluation/AverageReturn                    -67.6331
Evaluation/CompletionRate                     0
Evaluation/Iteration                        191
Evaluation/MaxReturn                        -32.7452
Evaluation/MinReturn                      -2062.25
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.334
Extras/EpisodeRewardMean                    -65.7126
LinearFeatureBaseline/ExplainedVariance       0.0207171
PolicyExecTime                                0.230852
ProcessExecTime                               0.03128
TotalEnvSteps                            194304
policy/Entropy                                0.196307
policy/KL                                     0.00748023
policy/KLBefore                               0
policy/LossAfter                             -0.00756718
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             1.2169
policy/dLoss                                  0.00756717
---------------------------------------  ----------------
2021-06-04 13:50:24 | [train_policy] epoch #192 | Obtaining samples for iteration 192...
2021-06-04 13:50:24 | [train_policy] epoch #192 | Logging diagnostics...
2021-06-04 13:50:24 | [train_policy] epoch #192 | Optimizing policy...
2021-06-04 13:50:24 | [train_policy] epoch #192 | Computing loss before
2021-06-04 13:50:24 | [train_policy] epoch #192 | Computing KL before
2021-06-04 13:50:24 | [train_policy] epoch #192 | Optimizing
2021-06-04 13:50:24 | [train_policy] epoch #192 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:24 | [train_policy] epoch #192 | computing loss before
2021-06-04 13:50:24 | [train_policy] epoch #192 | computing gradient
2021-06-04 13:50:24 | [train_policy] epoch #192 | gradient computed
2021-06-04 13:50:24 | [train_policy] epoch #192 | computing descent direction
2021-06-04 13:50:24 | [train_policy] epoch #192 | descent direction computed
2021-06-04 13:50:24 | [train_policy] epoch #192 | backtrack iters: 1
2021-06-04 13:50:24 | [train_policy] epoch #192 | optimization finished
2021-06-04 13:50:24 | [train_policy] epoch #192 | Computing KL after
2021-06-04 13:50:24 | [train_policy] epoch #192 | Computing loss after
2021-06-04 13:50:24 | [train_policy] epoch #192 | Fitting baseline...
2021-06-04 13:50:24 | [train_policy] epoch #192 | Saving snapshot...
2021-06-04 13:50:24 | [train_policy] epoch #192 | Saved
2021-06-04 13:50:24 | [train_policy] epoch #192 | Time 156.62 s
2021-06-04 13:50:24 | [train_policy] epoch #192 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28444
Evaluation/AverageDiscountedReturn          -44.5533
Evaluation/AverageReturn                    -44.5533
Evaluation/CompletionRate                     0
Evaluation/Iteration                        192
Evaluation/MaxReturn                        -33.1751
Evaluation/MinReturn                        -77.4307
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.60458
Extras/EpisodeRewardMean                    -44.4611
LinearFeatureBaseline/ExplainedVariance     -12.4306
PolicyExecTime                                0.230624
ProcessExecTime                               0.0310781
TotalEnvSteps                            195316
policy/Entropy                                0.186814
policy/KL                                     0.00649098
policy/KLBefore                               0
policy/LossAfter                             -0.0216375
policy/LossBefore                             6.59656e-09
policy/Perplexity                             1.2054
policy/dLoss                                  0.0216375
---------------------------------------  ----------------
2021-06-04 13:50:24 | [train_policy] epoch #193 | Obtaining samples for iteration 193...
2021-06-04 13:50:25 | [train_policy] epoch #193 | Logging diagnostics...
2021-06-04 13:50:25 | [train_policy] epoch #193 | Optimizing policy...
2021-06-04 13:50:25 | [train_policy] epoch #193 | Computing loss before
2021-06-04 13:50:25 | [train_policy] epoch #193 | Computing KL before
2021-06-04 13:50:25 | [train_policy] epoch #193 | Optimizing
2021-06-04 13:50:25 | [train_policy] epoch #193 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:25 | [train_policy] epoch #193 | computing loss before
2021-06-04 13:50:25 | [train_policy] epoch #193 | computing gradient
2021-06-04 13:50:25 | [train_policy] epoch #193 | gradient computed
2021-06-04 13:50:25 | [train_policy] epoch #193 | computing descent direction
2021-06-04 13:50:25 | [train_policy] epoch #193 | descent direction computed
2021-06-04 13:50:25 | [train_policy] epoch #193 | backtrack iters: 1
2021-06-04 13:50:25 | [train_policy] epoch #193 | optimization finished
2021-06-04 13:50:25 | [train_policy] epoch #193 | Computing KL after
2021-06-04 13:50:25 | [train_policy] epoch #193 | Computing loss after
2021-06-04 13:50:25 | [train_policy] epoch #193 | Fitting baseline...
2021-06-04 13:50:25 | [train_policy] epoch #193 | Saving snapshot...
2021-06-04 13:50:25 | [train_policy] epoch #193 | Saved
2021-06-04 13:50:25 | [train_policy] epoch #193 | Time 157.42 s
2021-06-04 13:50:25 | [train_policy] epoch #193 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285306
Evaluation/AverageDiscountedReturn          -44.936
Evaluation/AverageReturn                    -44.936
Evaluation/CompletionRate                     0
Evaluation/Iteration                        193
Evaluation/MaxReturn                        -31.8191
Evaluation/MinReturn                        -64.5521
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.97719
Extras/EpisodeRewardMean                    -44.7186
LinearFeatureBaseline/ExplainedVariance       0.925564
PolicyExecTime                                0.220886
ProcessExecTime                               0.0312073
TotalEnvSteps                            196328
policy/Entropy                                0.142841
policy/KL                                     0.0068423
policy/KLBefore                               0
policy/LossAfter                             -0.0159955
policy/LossBefore                             1.36643e-08
policy/Perplexity                             1.15355
policy/dLoss                                  0.0159956
---------------------------------------  ----------------
2021-06-04 13:50:25 | [train_policy] epoch #194 | Obtaining samples for iteration 194...
2021-06-04 13:50:26 | [train_policy] epoch #194 | Logging diagnostics...
2021-06-04 13:50:26 | [train_policy] epoch #194 | Optimizing policy...
2021-06-04 13:50:26 | [train_policy] epoch #194 | Computing loss before
2021-06-04 13:50:26 | [train_policy] epoch #194 | Computing KL before
2021-06-04 13:50:26 | [train_policy] epoch #194 | Optimizing
2021-06-04 13:50:26 | [train_policy] epoch #194 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:26 | [train_policy] epoch #194 | computing loss before
2021-06-04 13:50:26 | [train_policy] epoch #194 | computing gradient
2021-06-04 13:50:26 | [train_policy] epoch #194 | gradient computed
2021-06-04 13:50:26 | [train_policy] epoch #194 | computing descent direction
2021-06-04 13:50:26 | [train_policy] epoch #194 | descent direction computed
2021-06-04 13:50:26 | [train_policy] epoch #194 | backtrack iters: 0
2021-06-04 13:50:26 | [train_policy] epoch #194 | optimization finished
2021-06-04 13:50:26 | [train_policy] epoch #194 | Computing KL after
2021-06-04 13:50:26 | [train_policy] epoch #194 | Computing loss after
2021-06-04 13:50:26 | [train_policy] epoch #194 | Fitting baseline...
2021-06-04 13:50:26 | [train_policy] epoch #194 | Saving snapshot...
2021-06-04 13:50:26 | [train_policy] epoch #194 | Saved
2021-06-04 13:50:26 | [train_policy] epoch #194 | Time 158.21 s
2021-06-04 13:50:26 | [train_policy] epoch #194 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284295
Evaluation/AverageDiscountedReturn          -43.7124
Evaluation/AverageReturn                    -43.7124
Evaluation/CompletionRate                     0
Evaluation/Iteration                        194
Evaluation/MaxReturn                        -31.0813
Evaluation/MinReturn                        -67.6982
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.91601
Extras/EpisodeRewardMean                    -44.1498
LinearFeatureBaseline/ExplainedVariance       0.907442
PolicyExecTime                                0.228925
ProcessExecTime                               0.0311041
TotalEnvSteps                            197340
policy/Entropy                                0.109903
policy/KL                                     0.00954651
policy/KLBefore                               0
policy/LossAfter                             -0.0231232
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             1.11617
policy/dLoss                                  0.0231232
---------------------------------------  ----------------
2021-06-04 13:50:26 | [train_policy] epoch #195 | Obtaining samples for iteration 195...
2021-06-04 13:50:27 | [train_policy] epoch #195 | Logging diagnostics...
2021-06-04 13:50:27 | [train_policy] epoch #195 | Optimizing policy...
2021-06-04 13:50:27 | [train_policy] epoch #195 | Computing loss before
2021-06-04 13:50:27 | [train_policy] epoch #195 | Computing KL before
2021-06-04 13:50:27 | [train_policy] epoch #195 | Optimizing
2021-06-04 13:50:27 | [train_policy] epoch #195 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:27 | [train_policy] epoch #195 | computing loss before
2021-06-04 13:50:27 | [train_policy] epoch #195 | computing gradient
2021-06-04 13:50:27 | [train_policy] epoch #195 | gradient computed
2021-06-04 13:50:27 | [train_policy] epoch #195 | computing descent direction
2021-06-04 13:50:27 | [train_policy] epoch #195 | descent direction computed
2021-06-04 13:50:27 | [train_policy] epoch #195 | backtrack iters: 1
2021-06-04 13:50:27 | [train_policy] epoch #195 | optimization finished
2021-06-04 13:50:27 | [train_policy] epoch #195 | Computing KL after
2021-06-04 13:50:27 | [train_policy] epoch #195 | Computing loss after
2021-06-04 13:50:27 | [train_policy] epoch #195 | Fitting baseline...
2021-06-04 13:50:27 | [train_policy] epoch #195 | Saving snapshot...
2021-06-04 13:50:27 | [train_policy] epoch #195 | Saved
2021-06-04 13:50:27 | [train_policy] epoch #195 | Time 159.01 s
2021-06-04 13:50:27 | [train_policy] epoch #195 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283436
Evaluation/AverageDiscountedReturn          -68.0181
Evaluation/AverageReturn                    -68.0181
Evaluation/CompletionRate                     0
Evaluation/Iteration                        195
Evaluation/MaxReturn                        -33.9379
Evaluation/MinReturn                      -2062.6
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.58
Extras/EpisodeRewardMean                    -66.1023
LinearFeatureBaseline/ExplainedVariance       0.0116025
PolicyExecTime                                0.227108
ProcessExecTime                               0.0309844
TotalEnvSteps                            198352
policy/Entropy                                0.13094
policy/KL                                     0.00797749
policy/KLBefore                               0
policy/LossAfter                             -0.0202752
policy/LossBefore                             1.88473e-09
policy/Perplexity                             1.1399
policy/dLoss                                  0.0202752
---------------------------------------  ----------------
2021-06-04 13:50:27 | [train_policy] epoch #196 | Obtaining samples for iteration 196...
2021-06-04 13:50:27 | [train_policy] epoch #196 | Logging diagnostics...
2021-06-04 13:50:27 | [train_policy] epoch #196 | Optimizing policy...
2021-06-04 13:50:27 | [train_policy] epoch #196 | Computing loss before
2021-06-04 13:50:27 | [train_policy] epoch #196 | Computing KL before
2021-06-04 13:50:27 | [train_policy] epoch #196 | Optimizing
2021-06-04 13:50:27 | [train_policy] epoch #196 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:27 | [train_policy] epoch #196 | computing loss before
2021-06-04 13:50:27 | [train_policy] epoch #196 | computing gradient
2021-06-04 13:50:27 | [train_policy] epoch #196 | gradient computed
2021-06-04 13:50:27 | [train_policy] epoch #196 | computing descent direction
2021-06-04 13:50:28 | [train_policy] epoch #196 | descent direction computed
2021-06-04 13:50:28 | [train_policy] epoch #196 | backtrack iters: 1
2021-06-04 13:50:28 | [train_policy] epoch #196 | optimization finished
2021-06-04 13:50:28 | [train_policy] epoch #196 | Computing KL after
2021-06-04 13:50:28 | [train_policy] epoch #196 | Computing loss after
2021-06-04 13:50:28 | [train_policy] epoch #196 | Fitting baseline...
2021-06-04 13:50:28 | [train_policy] epoch #196 | Saving snapshot...
2021-06-04 13:50:28 | [train_policy] epoch #196 | Saved
2021-06-04 13:50:28 | [train_policy] epoch #196 | Time 159.83 s
2021-06-04 13:50:28 | [train_policy] epoch #196 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                   0.285392
Evaluation/AverageDiscountedReturn          -45.0303
Evaluation/AverageReturn                    -45.0303
Evaluation/CompletionRate                     0
Evaluation/Iteration                        196
Evaluation/MaxReturn                        -32.8131
Evaluation/MinReturn                        -81.0107
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.46608
Extras/EpisodeRewardMean                    -44.9619
LinearFeatureBaseline/ExplainedVariance     -18.4058
PolicyExecTime                                0.242702
ProcessExecTime                               0.0312252
TotalEnvSteps                            199364
policy/Entropy                                0.0743608
policy/KL                                     0.00672924
policy/KLBefore                               0
policy/LossAfter                             -0.015993
policy/LossBefore                            -3.8637e-08
policy/Perplexity                             1.0772
policy/dLoss                                  0.0159929
---------------------------------------  ---------------
2021-06-04 13:50:28 | [train_policy] epoch #197 | Obtaining samples for iteration 197...
2021-06-04 13:50:28 | [train_policy] epoch #197 | Logging diagnostics...
2021-06-04 13:50:28 | [train_policy] epoch #197 | Optimizing policy...
2021-06-04 13:50:28 | [train_policy] epoch #197 | Computing loss before
2021-06-04 13:50:28 | [train_policy] epoch #197 | Computing KL before
2021-06-04 13:50:28 | [train_policy] epoch #197 | Optimizing
2021-06-04 13:50:28 | [train_policy] epoch #197 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:28 | [train_policy] epoch #197 | computing loss before
2021-06-04 13:50:28 | [train_policy] epoch #197 | computing gradient
2021-06-04 13:50:28 | [train_policy] epoch #197 | gradient computed
2021-06-04 13:50:28 | [train_policy] epoch #197 | computing descent direction
2021-06-04 13:50:28 | [train_policy] epoch #197 | descent direction computed
2021-06-04 13:50:28 | [train_policy] epoch #197 | backtrack iters: 0
2021-06-04 13:50:28 | [train_policy] epoch #197 | optimization finished
2021-06-04 13:50:28 | [train_policy] epoch #197 | Computing KL after
2021-06-04 13:50:28 | [train_policy] epoch #197 | Computing loss after
2021-06-04 13:50:28 | [train_policy] epoch #197 | Fitting baseline...
2021-06-04 13:50:28 | [train_policy] epoch #197 | Saving snapshot...
2021-06-04 13:50:28 | [train_policy] epoch #197 | Saved
2021-06-04 13:50:28 | [train_policy] epoch #197 | Time 160.60 s
2021-06-04 13:50:28 | [train_policy] epoch #197 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284177
Evaluation/AverageDiscountedReturn          -44.7033
Evaluation/AverageReturn                    -44.7033
Evaluation/CompletionRate                     0
Evaluation/Iteration                        197
Evaluation/MaxReturn                        -34.7532
Evaluation/MinReturn                        -64.288
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.20869
Extras/EpisodeRewardMean                    -44.7395
LinearFeatureBaseline/ExplainedVariance       0.925577
PolicyExecTime                                0.206829
ProcessExecTime                               0.0310078
TotalEnvSteps                            200376
policy/Entropy                                0.0526576
policy/KL                                     0.00983758
policy/KLBefore                               0
policy/LossAfter                             -0.0196216
policy/LossBefore                            -1.34287e-08
policy/Perplexity                             1.05407
policy/dLoss                                  0.0196216
---------------------------------------  ----------------
2021-06-04 13:50:28 | [train_policy] epoch #198 | Obtaining samples for iteration 198...
2021-06-04 13:50:29 | [train_policy] epoch #198 | Logging diagnostics...
2021-06-04 13:50:29 | [train_policy] epoch #198 | Optimizing policy...
2021-06-04 13:50:29 | [train_policy] epoch #198 | Computing loss before
2021-06-04 13:50:29 | [train_policy] epoch #198 | Computing KL before
2021-06-04 13:50:29 | [train_policy] epoch #198 | Optimizing
2021-06-04 13:50:29 | [train_policy] epoch #198 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:29 | [train_policy] epoch #198 | computing loss before
2021-06-04 13:50:29 | [train_policy] epoch #198 | computing gradient
2021-06-04 13:50:29 | [train_policy] epoch #198 | gradient computed
2021-06-04 13:50:29 | [train_policy] epoch #198 | computing descent direction
2021-06-04 13:50:29 | [train_policy] epoch #198 | descent direction computed
2021-06-04 13:50:29 | [train_policy] epoch #198 | backtrack iters: 1
2021-06-04 13:50:29 | [train_policy] epoch #198 | optimization finished
2021-06-04 13:50:29 | [train_policy] epoch #198 | Computing KL after
2021-06-04 13:50:29 | [train_policy] epoch #198 | Computing loss after
2021-06-04 13:50:29 | [train_policy] epoch #198 | Fitting baseline...
2021-06-04 13:50:29 | [train_policy] epoch #198 | Saving snapshot...
2021-06-04 13:50:29 | [train_policy] epoch #198 | Saved
2021-06-04 13:50:29 | [train_policy] epoch #198 | Time 161.41 s
2021-06-04 13:50:29 | [train_policy] epoch #198 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284127
Evaluation/AverageDiscountedReturn          -44.4218
Evaluation/AverageReturn                    -44.4218
Evaluation/CompletionRate                     0
Evaluation/Iteration                        198
Evaluation/MaxReturn                        -31.7942
Evaluation/MinReturn                        -64.9054
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.10924
Extras/EpisodeRewardMean                    -44.4115
LinearFeatureBaseline/ExplainedVariance       0.928481
PolicyExecTime                                0.229996
ProcessExecTime                               0.0311561
TotalEnvSteps                            201388
policy/Entropy                                0.0256217
policy/KL                                     0.00653829
policy/KLBefore                               0
policy/LossAfter                             -0.012333
policy/LossBefore                            -4.12285e-09
policy/Perplexity                             1.02595
policy/dLoss                                  0.012333
---------------------------------------  ----------------
2021-06-04 13:50:29 | [train_policy] epoch #199 | Obtaining samples for iteration 199...
2021-06-04 13:50:30 | [train_policy] epoch #199 | Logging diagnostics...
2021-06-04 13:50:30 | [train_policy] epoch #199 | Optimizing policy...
2021-06-04 13:50:30 | [train_policy] epoch #199 | Computing loss before
2021-06-04 13:50:30 | [train_policy] epoch #199 | Computing KL before
2021-06-04 13:50:30 | [train_policy] epoch #199 | Optimizing
2021-06-04 13:50:30 | [train_policy] epoch #199 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:30 | [train_policy] epoch #199 | computing loss before
2021-06-04 13:50:30 | [train_policy] epoch #199 | computing gradient
2021-06-04 13:50:30 | [train_policy] epoch #199 | gradient computed
2021-06-04 13:50:30 | [train_policy] epoch #199 | computing descent direction
2021-06-04 13:50:30 | [train_policy] epoch #199 | descent direction computed
2021-06-04 13:50:30 | [train_policy] epoch #199 | backtrack iters: 0
2021-06-04 13:50:30 | [train_policy] epoch #199 | optimization finished
2021-06-04 13:50:30 | [train_policy] epoch #199 | Computing KL after
2021-06-04 13:50:30 | [train_policy] epoch #199 | Computing loss after
2021-06-04 13:50:30 | [train_policy] epoch #199 | Fitting baseline...
2021-06-04 13:50:30 | [train_policy] epoch #199 | Saving snapshot...
2021-06-04 13:50:30 | [train_policy] epoch #199 | Saved
2021-06-04 13:50:30 | [train_policy] epoch #199 | Time 162.19 s
2021-06-04 13:50:30 | [train_policy] epoch #199 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.285645
Evaluation/AverageDiscountedReturn          -44.3839
Evaluation/AverageReturn                    -44.3839
Evaluation/CompletionRate                     0
Evaluation/Iteration                        199
Evaluation/MaxReturn                        -30.9433
Evaluation/MinReturn                        -87.0566
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.06503
Extras/EpisodeRewardMean                    -44.345
LinearFeatureBaseline/ExplainedVariance       0.898428
PolicyExecTime                                0.217175
ProcessExecTime                               0.0311677
TotalEnvSteps                            202400
policy/Entropy                                0.017066
policy/KL                                     0.00963905
policy/KLBefore                               0
policy/LossAfter                             -0.0226778
policy/LossBefore                             1.1544e-08
policy/Perplexity                             1.01721
policy/dLoss                                  0.0226778
---------------------------------------  ---------------
2021-06-04 13:50:30 | [train_policy] epoch #200 | Obtaining samples for iteration 200...
2021-06-04 13:50:31 | [train_policy] epoch #200 | Logging diagnostics...
2021-06-04 13:50:31 | [train_policy] epoch #200 | Optimizing policy...
2021-06-04 13:50:31 | [train_policy] epoch #200 | Computing loss before
2021-06-04 13:50:31 | [train_policy] epoch #200 | Computing KL before
2021-06-04 13:50:31 | [train_policy] epoch #200 | Optimizing
2021-06-04 13:50:31 | [train_policy] epoch #200 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:31 | [train_policy] epoch #200 | computing loss before
2021-06-04 13:50:31 | [train_policy] epoch #200 | computing gradient
2021-06-04 13:50:31 | [train_policy] epoch #200 | gradient computed
2021-06-04 13:50:31 | [train_policy] epoch #200 | computing descent direction
2021-06-04 13:50:31 | [train_policy] epoch #200 | descent direction computed
2021-06-04 13:50:31 | [train_policy] epoch #200 | backtrack iters: 1
2021-06-04 13:50:31 | [train_policy] epoch #200 | optimization finished
2021-06-04 13:50:31 | [train_policy] epoch #200 | Computing KL after
2021-06-04 13:50:31 | [train_policy] epoch #200 | Computing loss after
2021-06-04 13:50:31 | [train_policy] epoch #200 | Fitting baseline...
2021-06-04 13:50:31 | [train_policy] epoch #200 | Saving snapshot...
2021-06-04 13:50:31 | [train_policy] epoch #200 | Saved
2021-06-04 13:50:31 | [train_policy] epoch #200 | Time 163.00 s
2021-06-04 13:50:31 | [train_policy] epoch #200 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28475
Evaluation/AverageDiscountedReturn          -89.1292
Evaluation/AverageReturn                    -89.1292
Evaluation/CompletionRate                     0
Evaluation/Iteration                        200
Evaluation/MaxReturn                        -32.9902
Evaluation/MinReturn                      -2063.31
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.57
Extras/EpisodeRewardMean                    -85.5084
LinearFeatureBaseline/ExplainedVariance       0.0126608
PolicyExecTime                                0.238876
ProcessExecTime                               0.0311401
TotalEnvSteps                            203412
policy/Entropy                                0.00984502
policy/KL                                     0.0073576
policy/KLBefore                               0
policy/LossAfter                             -0.0203461
policy/LossBefore                            -1.69626e-08
policy/Perplexity                             1.00989
policy/dLoss                                  0.0203461
---------------------------------------  ----------------
2021-06-04 13:50:31 | [train_policy] epoch #201 | Obtaining samples for iteration 201...
2021-06-04 13:50:31 | [train_policy] epoch #201 | Logging diagnostics...
2021-06-04 13:50:31 | [train_policy] epoch #201 | Optimizing policy...
2021-06-04 13:50:31 | [train_policy] epoch #201 | Computing loss before
2021-06-04 13:50:31 | [train_policy] epoch #201 | Computing KL before
2021-06-04 13:50:31 | [train_policy] epoch #201 | Optimizing
2021-06-04 13:50:31 | [train_policy] epoch #201 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:31 | [train_policy] epoch #201 | computing loss before
2021-06-04 13:50:31 | [train_policy] epoch #201 | computing gradient
2021-06-04 13:50:31 | [train_policy] epoch #201 | gradient computed
2021-06-04 13:50:31 | [train_policy] epoch #201 | computing descent direction
2021-06-04 13:50:31 | [train_policy] epoch #201 | descent direction computed
2021-06-04 13:50:32 | [train_policy] epoch #201 | backtrack iters: 1
2021-06-04 13:50:32 | [train_policy] epoch #201 | optimization finished
2021-06-04 13:50:32 | [train_policy] epoch #201 | Computing KL after
2021-06-04 13:50:32 | [train_policy] epoch #201 | Computing loss after
2021-06-04 13:50:32 | [train_policy] epoch #201 | Fitting baseline...
2021-06-04 13:50:32 | [train_policy] epoch #201 | Saving snapshot...
2021-06-04 13:50:32 | [train_policy] epoch #201 | Saved
2021-06-04 13:50:32 | [train_policy] epoch #201 | Time 163.77 s
2021-06-04 13:50:32 | [train_policy] epoch #201 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.287861
Evaluation/AverageDiscountedReturn          -88.3866
Evaluation/AverageReturn                    -88.3866
Evaluation/CompletionRate                     0
Evaluation/Iteration                        201
Evaluation/MaxReturn                        -32.5427
Evaluation/MinReturn                      -2061.91
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.272
Extras/EpisodeRewardMean                   -104.889
LinearFeatureBaseline/ExplainedVariance       0.14214
PolicyExecTime                                0.208527
ProcessExecTime                               0.0314116
TotalEnvSteps                            204424
policy/Entropy                                0.00575376
policy/KL                                     0.00656125
policy/KLBefore                               0
policy/LossAfter                             -0.0215708
policy/LossBefore                             1.36643e-08
policy/Perplexity                             1.00577
policy/dLoss                                  0.0215708
---------------------------------------  ----------------
2021-06-04 13:50:32 | [train_policy] epoch #202 | Obtaining samples for iteration 202...
2021-06-04 13:50:32 | [train_policy] epoch #202 | Logging diagnostics...
2021-06-04 13:50:32 | [train_policy] epoch #202 | Optimizing policy...
2021-06-04 13:50:32 | [train_policy] epoch #202 | Computing loss before
2021-06-04 13:50:32 | [train_policy] epoch #202 | Computing KL before
2021-06-04 13:50:32 | [train_policy] epoch #202 | Optimizing
2021-06-04 13:50:32 | [train_policy] epoch #202 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:32 | [train_policy] epoch #202 | computing loss before
2021-06-04 13:50:32 | [train_policy] epoch #202 | computing gradient
2021-06-04 13:50:32 | [train_policy] epoch #202 | gradient computed
2021-06-04 13:50:32 | [train_policy] epoch #202 | computing descent direction
2021-06-04 13:50:32 | [train_policy] epoch #202 | descent direction computed
2021-06-04 13:50:32 | [train_policy] epoch #202 | backtrack iters: 1
2021-06-04 13:50:32 | [train_policy] epoch #202 | optimization finished
2021-06-04 13:50:32 | [train_policy] epoch #202 | Computing KL after
2021-06-04 13:50:32 | [train_policy] epoch #202 | Computing loss after
2021-06-04 13:50:32 | [train_policy] epoch #202 | Fitting baseline...
2021-06-04 13:50:32 | [train_policy] epoch #202 | Saving snapshot...
2021-06-04 13:50:32 | [train_policy] epoch #202 | Saved
2021-06-04 13:50:32 | [train_policy] epoch #202 | Time 164.57 s
2021-06-04 13:50:32 | [train_policy] epoch #202 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285652
Evaluation/AverageDiscountedReturn          -44.1282
Evaluation/AverageReturn                    -44.1282
Evaluation/CompletionRate                     0
Evaluation/Iteration                        202
Evaluation/MaxReturn                        -30.8924
Evaluation/MinReturn                        -64.1791
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.83173
Extras/EpisodeRewardMean                    -43.7298
LinearFeatureBaseline/ExplainedVariance     -39.494
PolicyExecTime                                0.229224
ProcessExecTime                               0.0312431
TotalEnvSteps                            205436
policy/Entropy                               -0.0199738
policy/KL                                     0.00646169
policy/KLBefore                               0
policy/LossAfter                             -0.0214724
policy/LossBefore                            -4.14641e-08
policy/Perplexity                             0.980224
policy/dLoss                                  0.0214724
---------------------------------------  ----------------
2021-06-04 13:50:32 | [train_policy] epoch #203 | Obtaining samples for iteration 203...
2021-06-04 13:50:33 | [train_policy] epoch #203 | Logging diagnostics...
2021-06-04 13:50:33 | [train_policy] epoch #203 | Optimizing policy...
2021-06-04 13:50:33 | [train_policy] epoch #203 | Computing loss before
2021-06-04 13:50:33 | [train_policy] epoch #203 | Computing KL before
2021-06-04 13:50:33 | [train_policy] epoch #203 | Optimizing
2021-06-04 13:50:33 | [train_policy] epoch #203 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:33 | [train_policy] epoch #203 | computing loss before
2021-06-04 13:50:33 | [train_policy] epoch #203 | computing gradient
2021-06-04 13:50:33 | [train_policy] epoch #203 | gradient computed
2021-06-04 13:50:33 | [train_policy] epoch #203 | computing descent direction
2021-06-04 13:50:33 | [train_policy] epoch #203 | descent direction computed
2021-06-04 13:50:33 | [train_policy] epoch #203 | backtrack iters: 1
2021-06-04 13:50:33 | [train_policy] epoch #203 | optimization finished
2021-06-04 13:50:33 | [train_policy] epoch #203 | Computing KL after
2021-06-04 13:50:33 | [train_policy] epoch #203 | Computing loss after
2021-06-04 13:50:33 | [train_policy] epoch #203 | Fitting baseline...
2021-06-04 13:50:33 | [train_policy] epoch #203 | Saving snapshot...
2021-06-04 13:50:33 | [train_policy] epoch #203 | Saved
2021-06-04 13:50:33 | [train_policy] epoch #203 | Time 165.35 s
2021-06-04 13:50:33 | [train_policy] epoch #203 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283721
Evaluation/AverageDiscountedReturn          -66.2558
Evaluation/AverageReturn                    -66.2558
Evaluation/CompletionRate                     0
Evaluation/Iteration                        203
Evaluation/MaxReturn                        -33.9747
Evaluation/MinReturn                      -2062.66
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.397
Extras/EpisodeRewardMean                    -64.4234
LinearFeatureBaseline/ExplainedVariance       0.0109117
PolicyExecTime                                0.213917
ProcessExecTime                               0.0311
TotalEnvSteps                            206448
policy/Entropy                               -0.0125284
policy/KL                                     0.00747556
policy/KLBefore                               0
policy/LossAfter                             -0.0266745
policy/LossBefore                            -1.41355e-08
policy/Perplexity                             0.98755
policy/dLoss                                  0.0266745
---------------------------------------  ----------------
2021-06-04 13:50:33 | [train_policy] epoch #204 | Obtaining samples for iteration 204...
2021-06-04 13:50:34 | [train_policy] epoch #204 | Logging diagnostics...
2021-06-04 13:50:34 | [train_policy] epoch #204 | Optimizing policy...
2021-06-04 13:50:34 | [train_policy] epoch #204 | Computing loss before
2021-06-04 13:50:34 | [train_policy] epoch #204 | Computing KL before
2021-06-04 13:50:34 | [train_policy] epoch #204 | Optimizing
2021-06-04 13:50:34 | [train_policy] epoch #204 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:34 | [train_policy] epoch #204 | computing loss before
2021-06-04 13:50:34 | [train_policy] epoch #204 | computing gradient
2021-06-04 13:50:34 | [train_policy] epoch #204 | gradient computed
2021-06-04 13:50:34 | [train_policy] epoch #204 | computing descent direction
2021-06-04 13:50:34 | [train_policy] epoch #204 | descent direction computed
2021-06-04 13:50:34 | [train_policy] epoch #204 | backtrack iters: 1
2021-06-04 13:50:34 | [train_policy] epoch #204 | optimization finished
2021-06-04 13:50:34 | [train_policy] epoch #204 | Computing KL after
2021-06-04 13:50:34 | [train_policy] epoch #204 | Computing loss after
2021-06-04 13:50:34 | [train_policy] epoch #204 | Fitting baseline...
2021-06-04 13:50:34 | [train_policy] epoch #204 | Saving snapshot...
2021-06-04 13:50:34 | [train_policy] epoch #204 | Saved
2021-06-04 13:50:34 | [train_policy] epoch #204 | Time 166.14 s
2021-06-04 13:50:34 | [train_policy] epoch #204 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284337
Evaluation/AverageDiscountedReturn          -46.0217
Evaluation/AverageReturn                    -46.0217
Evaluation/CompletionRate                     0
Evaluation/Iteration                        204
Evaluation/MaxReturn                        -32.5152
Evaluation/MinReturn                       -119.948
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.7581
Extras/EpisodeRewardMean                    -45.9285
LinearFeatureBaseline/ExplainedVariance     -20.7079
PolicyExecTime                                0.226605
ProcessExecTime                               0.031112
TotalEnvSteps                            207460
policy/Entropy                               -0.0233781
policy/KL                                     0.00699147
policy/KLBefore                               0
policy/LossAfter                             -0.047434
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.976893
policy/dLoss                                  0.047434
---------------------------------------  ----------------
2021-06-04 13:50:34 | [train_policy] epoch #205 | Obtaining samples for iteration 205...
2021-06-04 13:50:35 | [train_policy] epoch #205 | Logging diagnostics...
2021-06-04 13:50:35 | [train_policy] epoch #205 | Optimizing policy...
2021-06-04 13:50:35 | [train_policy] epoch #205 | Computing loss before
2021-06-04 13:50:35 | [train_policy] epoch #205 | Computing KL before
2021-06-04 13:50:35 | [train_policy] epoch #205 | Optimizing
2021-06-04 13:50:35 | [train_policy] epoch #205 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:35 | [train_policy] epoch #205 | computing loss before
2021-06-04 13:50:35 | [train_policy] epoch #205 | computing gradient
2021-06-04 13:50:35 | [train_policy] epoch #205 | gradient computed
2021-06-04 13:50:35 | [train_policy] epoch #205 | computing descent direction
2021-06-04 13:50:35 | [train_policy] epoch #205 | descent direction computed
2021-06-04 13:50:35 | [train_policy] epoch #205 | backtrack iters: 1
2021-06-04 13:50:35 | [train_policy] epoch #205 | optimization finished
2021-06-04 13:50:35 | [train_policy] epoch #205 | Computing KL after
2021-06-04 13:50:35 | [train_policy] epoch #205 | Computing loss after
2021-06-04 13:50:35 | [train_policy] epoch #205 | Fitting baseline...
2021-06-04 13:50:35 | [train_policy] epoch #205 | Saving snapshot...
2021-06-04 13:50:35 | [train_policy] epoch #205 | Saved
2021-06-04 13:50:35 | [train_policy] epoch #205 | Time 166.92 s
2021-06-04 13:50:35 | [train_policy] epoch #205 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285213
Evaluation/AverageDiscountedReturn          -45.0193
Evaluation/AverageReturn                    -45.0193
Evaluation/CompletionRate                     0
Evaluation/Iteration                        205
Evaluation/MaxReturn                        -30.6541
Evaluation/MinReturn                        -66.1076
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.10997
Extras/EpisodeRewardMean                    -45.029
LinearFeatureBaseline/ExplainedVariance       0.880164
PolicyExecTime                                0.221026
ProcessExecTime                               0.0312469
TotalEnvSteps                            208472
policy/Entropy                               -0.0536263
policy/KL                                     0.00679587
policy/KLBefore                               0
policy/LossAfter                             -0.0149543
policy/LossBefore                            -3.62811e-08
policy/Perplexity                             0.947786
policy/dLoss                                  0.0149543
---------------------------------------  ----------------
2021-06-04 13:50:35 | [train_policy] epoch #206 | Obtaining samples for iteration 206...
2021-06-04 13:50:35 | [train_policy] epoch #206 | Logging diagnostics...
2021-06-04 13:50:35 | [train_policy] epoch #206 | Optimizing policy...
2021-06-04 13:50:35 | [train_policy] epoch #206 | Computing loss before
2021-06-04 13:50:35 | [train_policy] epoch #206 | Computing KL before
2021-06-04 13:50:35 | [train_policy] epoch #206 | Optimizing
2021-06-04 13:50:35 | [train_policy] epoch #206 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:35 | [train_policy] epoch #206 | computing loss before
2021-06-04 13:50:35 | [train_policy] epoch #206 | computing gradient
2021-06-04 13:50:35 | [train_policy] epoch #206 | gradient computed
2021-06-04 13:50:35 | [train_policy] epoch #206 | computing descent direction
2021-06-04 13:50:35 | [train_policy] epoch #206 | descent direction computed
2021-06-04 13:50:35 | [train_policy] epoch #206 | backtrack iters: 0
2021-06-04 13:50:35 | [train_policy] epoch #206 | optimization finished
2021-06-04 13:50:35 | [train_policy] epoch #206 | Computing KL after
2021-06-04 13:50:35 | [train_policy] epoch #206 | Computing loss after
2021-06-04 13:50:35 | [train_policy] epoch #206 | Fitting baseline...
2021-06-04 13:50:35 | [train_policy] epoch #206 | Saving snapshot...
2021-06-04 13:50:35 | [train_policy] epoch #206 | Saved
2021-06-04 13:50:35 | [train_policy] epoch #206 | Time 167.69 s
2021-06-04 13:50:35 | [train_policy] epoch #206 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.286472
Evaluation/AverageDiscountedReturn          -61.656
Evaluation/AverageReturn                    -61.656
Evaluation/CompletionRate                     0
Evaluation/Iteration                        206
Evaluation/MaxReturn                        -33.7392
Evaluation/MinReturn                      -1667.79
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        168.519
Extras/EpisodeRewardMean                    -60.4168
LinearFeatureBaseline/ExplainedVariance       0.0162117
PolicyExecTime                                0.212329
ProcessExecTime                               0.0314128
TotalEnvSteps                            209484
policy/Entropy                               -0.0565832
policy/KL                                     0.00981902
policy/KLBefore                               0
policy/LossAfter                             -0.0232473
policy/LossBefore                             1.41355e-08
policy/Perplexity                             0.944988
policy/dLoss                                  0.0232473
---------------------------------------  ----------------
2021-06-04 13:50:35 | [train_policy] epoch #207 | Obtaining samples for iteration 207...
2021-06-04 13:50:36 | [train_policy] epoch #207 | Logging diagnostics...
2021-06-04 13:50:36 | [train_policy] epoch #207 | Optimizing policy...
2021-06-04 13:50:36 | [train_policy] epoch #207 | Computing loss before
2021-06-04 13:50:36 | [train_policy] epoch #207 | Computing KL before
2021-06-04 13:50:36 | [train_policy] epoch #207 | Optimizing
2021-06-04 13:50:36 | [train_policy] epoch #207 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:36 | [train_policy] epoch #207 | computing loss before
2021-06-04 13:50:36 | [train_policy] epoch #207 | computing gradient
2021-06-04 13:50:36 | [train_policy] epoch #207 | gradient computed
2021-06-04 13:50:36 | [train_policy] epoch #207 | computing descent direction
2021-06-04 13:50:36 | [train_policy] epoch #207 | descent direction computed
2021-06-04 13:50:36 | [train_policy] epoch #207 | backtrack iters: 1
2021-06-04 13:50:36 | [train_policy] epoch #207 | optimization finished
2021-06-04 13:50:36 | [train_policy] epoch #207 | Computing KL after
2021-06-04 13:50:36 | [train_policy] epoch #207 | Computing loss after
2021-06-04 13:50:36 | [train_policy] epoch #207 | Fitting baseline...
2021-06-04 13:50:36 | [train_policy] epoch #207 | Saving snapshot...
2021-06-04 13:50:36 | [train_policy] epoch #207 | Saved
2021-06-04 13:50:36 | [train_policy] epoch #207 | Time 168.48 s
2021-06-04 13:50:36 | [train_policy] epoch #207 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283237
Evaluation/AverageDiscountedReturn          -46.7239
Evaluation/AverageReturn                    -46.7239
Evaluation/CompletionRate                     0
Evaluation/Iteration                        207
Evaluation/MaxReturn                        -33.9828
Evaluation/MinReturn                       -215.026
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         19.4631
Extras/EpisodeRewardMean                    -47.3251
LinearFeatureBaseline/ExplainedVariance      -5.96791
PolicyExecTime                                0.220908
ProcessExecTime                               0.030997
TotalEnvSteps                            210496
policy/Entropy                               -0.063314
policy/KL                                     0.00691723
policy/KLBefore                               0
policy/LossAfter                             -0.0178897
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.938649
policy/dLoss                                  0.0178897
---------------------------------------  ----------------
2021-06-04 13:50:36 | [train_policy] epoch #208 | Obtaining samples for iteration 208...
2021-06-04 13:50:37 | [train_policy] epoch #208 | Logging diagnostics...
2021-06-04 13:50:37 | [train_policy] epoch #208 | Optimizing policy...
2021-06-04 13:50:37 | [train_policy] epoch #208 | Computing loss before
2021-06-04 13:50:37 | [train_policy] epoch #208 | Computing KL before
2021-06-04 13:50:37 | [train_policy] epoch #208 | Optimizing
2021-06-04 13:50:37 | [train_policy] epoch #208 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:37 | [train_policy] epoch #208 | computing loss before
2021-06-04 13:50:37 | [train_policy] epoch #208 | computing gradient
2021-06-04 13:50:37 | [train_policy] epoch #208 | gradient computed
2021-06-04 13:50:37 | [train_policy] epoch #208 | computing descent direction
2021-06-04 13:50:37 | [train_policy] epoch #208 | descent direction computed
2021-06-04 13:50:37 | [train_policy] epoch #208 | backtrack iters: 0
2021-06-04 13:50:37 | [train_policy] epoch #208 | optimization finished
2021-06-04 13:50:37 | [train_policy] epoch #208 | Computing KL after
2021-06-04 13:50:37 | [train_policy] epoch #208 | Computing loss after
2021-06-04 13:50:37 | [train_policy] epoch #208 | Fitting baseline...
2021-06-04 13:50:37 | [train_policy] epoch #208 | Saving snapshot...
2021-06-04 13:50:37 | [train_policy] epoch #208 | Saved
2021-06-04 13:50:37 | [train_policy] epoch #208 | Time 169.25 s
2021-06-04 13:50:37 | [train_policy] epoch #208 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284774
Evaluation/AverageDiscountedReturn          -45.7851
Evaluation/AverageReturn                    -45.7851
Evaluation/CompletionRate                     0
Evaluation/Iteration                        208
Evaluation/MaxReturn                        -30.6727
Evaluation/MinReturn                        -65.5997
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.6696
Extras/EpisodeRewardMean                    -45.466
LinearFeatureBaseline/ExplainedVariance       0.813009
PolicyExecTime                                0.209186
ProcessExecTime                               0.0311732
TotalEnvSteps                            211508
policy/Entropy                               -0.0355961
policy/KL                                     0.00961359
policy/KLBefore                               0
policy/LossAfter                             -0.044595
policy/LossBefore                            -2.73286e-08
policy/Perplexity                             0.96503
policy/dLoss                                  0.044595
---------------------------------------  ----------------
2021-06-04 13:50:37 | [train_policy] epoch #209 | Obtaining samples for iteration 209...
2021-06-04 13:50:38 | [train_policy] epoch #209 | Logging diagnostics...
2021-06-04 13:50:38 | [train_policy] epoch #209 | Optimizing policy...
2021-06-04 13:50:38 | [train_policy] epoch #209 | Computing loss before
2021-06-04 13:50:38 | [train_policy] epoch #209 | Computing KL before
2021-06-04 13:50:38 | [train_policy] epoch #209 | Optimizing
2021-06-04 13:50:38 | [train_policy] epoch #209 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:38 | [train_policy] epoch #209 | computing loss before
2021-06-04 13:50:38 | [train_policy] epoch #209 | computing gradient
2021-06-04 13:50:38 | [train_policy] epoch #209 | gradient computed
2021-06-04 13:50:38 | [train_policy] epoch #209 | computing descent direction
2021-06-04 13:50:38 | [train_policy] epoch #209 | descent direction computed
2021-06-04 13:50:38 | [train_policy] epoch #209 | backtrack iters: 1
2021-06-04 13:50:38 | [train_policy] epoch #209 | optimization finished
2021-06-04 13:50:38 | [train_policy] epoch #209 | Computing KL after
2021-06-04 13:50:38 | [train_policy] epoch #209 | Computing loss after
2021-06-04 13:50:38 | [train_policy] epoch #209 | Fitting baseline...
2021-06-04 13:50:38 | [train_policy] epoch #209 | Saving snapshot...
2021-06-04 13:50:38 | [train_policy] epoch #209 | Saved
2021-06-04 13:50:38 | [train_policy] epoch #209 | Time 170.04 s
2021-06-04 13:50:38 | [train_policy] epoch #209 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.285393
Evaluation/AverageDiscountedReturn          -45.8016
Evaluation/AverageReturn                    -45.8016
Evaluation/CompletionRate                     0
Evaluation/Iteration                        209
Evaluation/MaxReturn                        -31.3418
Evaluation/MinReturn                        -81.2587
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77542
Extras/EpisodeRewardMean                    -45.9396
LinearFeatureBaseline/ExplainedVariance       0.89732
PolicyExecTime                                0.225373
ProcessExecTime                               0.0313463
TotalEnvSteps                            212520
policy/Entropy                               -0.0518396
policy/KL                                     0.00647082
policy/KLBefore                               0
policy/LossAfter                             -0.0175445
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.949481
policy/dLoss                                  0.0175445
---------------------------------------  ---------------
2021-06-04 13:50:38 | [train_policy] epoch #210 | Obtaining samples for iteration 210...
2021-06-04 13:50:38 | [train_policy] epoch #210 | Logging diagnostics...
2021-06-04 13:50:38 | [train_policy] epoch #210 | Optimizing policy...
2021-06-04 13:50:38 | [train_policy] epoch #210 | Computing loss before
2021-06-04 13:50:38 | [train_policy] epoch #210 | Computing KL before
2021-06-04 13:50:38 | [train_policy] epoch #210 | Optimizing
2021-06-04 13:50:38 | [train_policy] epoch #210 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:38 | [train_policy] epoch #210 | computing loss before
2021-06-04 13:50:38 | [train_policy] epoch #210 | computing gradient
2021-06-04 13:50:38 | [train_policy] epoch #210 | gradient computed
2021-06-04 13:50:38 | [train_policy] epoch #210 | computing descent direction
2021-06-04 13:50:39 | [train_policy] epoch #210 | descent direction computed
2021-06-04 13:50:39 | [train_policy] epoch #210 | backtrack iters: 0
2021-06-04 13:50:39 | [train_policy] epoch #210 | optimization finished
2021-06-04 13:50:39 | [train_policy] epoch #210 | Computing KL after
2021-06-04 13:50:39 | [train_policy] epoch #210 | Computing loss after
2021-06-04 13:50:39 | [train_policy] epoch #210 | Fitting baseline...
2021-06-04 13:50:39 | [train_policy] epoch #210 | Saving snapshot...
2021-06-04 13:50:39 | [train_policy] epoch #210 | Saved
2021-06-04 13:50:39 | [train_policy] epoch #210 | Time 170.82 s
2021-06-04 13:50:39 | [train_policy] epoch #210 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285883
Evaluation/AverageDiscountedReturn          -43.993
Evaluation/AverageReturn                    -43.993
Evaluation/CompletionRate                     0
Evaluation/Iteration                        210
Evaluation/MaxReturn                        -33.546
Evaluation/MinReturn                        -69.0348
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.59117
Extras/EpisodeRewardMean                    -44.0175
LinearFeatureBaseline/ExplainedVariance       0.910432
PolicyExecTime                                0.221151
ProcessExecTime                               0.0312538
TotalEnvSteps                            213532
policy/Entropy                               -0.02177
policy/KL                                     0.00932816
policy/KLBefore                               0
policy/LossAfter                             -0.019108
policy/LossBefore                             2.12032e-09
policy/Perplexity                             0.978465
policy/dLoss                                  0.019108
---------------------------------------  ----------------
2021-06-04 13:50:39 | [train_policy] epoch #211 | Obtaining samples for iteration 211...
2021-06-04 13:50:39 | [train_policy] epoch #211 | Logging diagnostics...
2021-06-04 13:50:39 | [train_policy] epoch #211 | Optimizing policy...
2021-06-04 13:50:39 | [train_policy] epoch #211 | Computing loss before
2021-06-04 13:50:39 | [train_policy] epoch #211 | Computing KL before
2021-06-04 13:50:39 | [train_policy] epoch #211 | Optimizing
2021-06-04 13:50:39 | [train_policy] epoch #211 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:39 | [train_policy] epoch #211 | computing loss before
2021-06-04 13:50:39 | [train_policy] epoch #211 | computing gradient
2021-06-04 13:50:39 | [train_policy] epoch #211 | gradient computed
2021-06-04 13:50:39 | [train_policy] epoch #211 | computing descent direction
2021-06-04 13:50:39 | [train_policy] epoch #211 | descent direction computed
2021-06-04 13:50:39 | [train_policy] epoch #211 | backtrack iters: 1
2021-06-04 13:50:39 | [train_policy] epoch #211 | optimization finished
2021-06-04 13:50:39 | [train_policy] epoch #211 | Computing KL after
2021-06-04 13:50:39 | [train_policy] epoch #211 | Computing loss after
2021-06-04 13:50:39 | [train_policy] epoch #211 | Fitting baseline...
2021-06-04 13:50:39 | [train_policy] epoch #211 | Saving snapshot...
2021-06-04 13:50:39 | [train_policy] epoch #211 | Saved
2021-06-04 13:50:39 | [train_policy] epoch #211 | Time 171.61 s
2021-06-04 13:50:39 | [train_policy] epoch #211 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28653
Evaluation/AverageDiscountedReturn          -45.7827
Evaluation/AverageReturn                    -45.7827
Evaluation/CompletionRate                     0
Evaluation/Iteration                        211
Evaluation/MaxReturn                        -34.887
Evaluation/MinReturn                        -92.1755
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.94925
Extras/EpisodeRewardMean                    -45.4484
LinearFeatureBaseline/ExplainedVariance       0.836054
PolicyExecTime                                0.232229
ProcessExecTime                               0.0312707
TotalEnvSteps                            214544
policy/Entropy                               -0.0326576
policy/KL                                     0.00650309
policy/KLBefore                               0
policy/LossAfter                             -0.0205572
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.96787
policy/dLoss                                  0.0205572
---------------------------------------  ----------------
2021-06-04 13:50:39 | [train_policy] epoch #212 | Obtaining samples for iteration 212...
2021-06-04 13:50:40 | [train_policy] epoch #212 | Logging diagnostics...
2021-06-04 13:50:40 | [train_policy] epoch #212 | Optimizing policy...
2021-06-04 13:50:40 | [train_policy] epoch #212 | Computing loss before
2021-06-04 13:50:40 | [train_policy] epoch #212 | Computing KL before
2021-06-04 13:50:40 | [train_policy] epoch #212 | Optimizing
2021-06-04 13:50:40 | [train_policy] epoch #212 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:40 | [train_policy] epoch #212 | computing loss before
2021-06-04 13:50:40 | [train_policy] epoch #212 | computing gradient
2021-06-04 13:50:40 | [train_policy] epoch #212 | gradient computed
2021-06-04 13:50:40 | [train_policy] epoch #212 | computing descent direction
2021-06-04 13:50:40 | [train_policy] epoch #212 | descent direction computed
2021-06-04 13:50:40 | [train_policy] epoch #212 | backtrack iters: 0
2021-06-04 13:50:40 | [train_policy] epoch #212 | optimization finished
2021-06-04 13:50:40 | [train_policy] epoch #212 | Computing KL after
2021-06-04 13:50:40 | [train_policy] epoch #212 | Computing loss after
2021-06-04 13:50:40 | [train_policy] epoch #212 | Fitting baseline...
2021-06-04 13:50:40 | [train_policy] epoch #212 | Saving snapshot...
2021-06-04 13:50:40 | [train_policy] epoch #212 | Saved
2021-06-04 13:50:40 | [train_policy] epoch #212 | Time 172.39 s
2021-06-04 13:50:40 | [train_policy] epoch #212 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284695
Evaluation/AverageDiscountedReturn          -45.0879
Evaluation/AverageReturn                    -45.0879
Evaluation/CompletionRate                     0
Evaluation/Iteration                        212
Evaluation/MaxReturn                        -31.7059
Evaluation/MinReturn                        -63.7619
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.65575
Extras/EpisodeRewardMean                    -45.7931
LinearFeatureBaseline/ExplainedVariance       0.907938
PolicyExecTime                                0.220067
ProcessExecTime                               0.03124
TotalEnvSteps                            215556
policy/Entropy                                0.00452018
policy/KL                                     0.0095794
policy/KLBefore                               0
policy/LossAfter                             -0.0206916
policy/LossBefore                            -1.18679e-08
policy/Perplexity                             1.00453
policy/dLoss                                  0.0206916
---------------------------------------  ----------------
2021-06-04 13:50:40 | [train_policy] epoch #213 | Obtaining samples for iteration 213...
2021-06-04 13:50:41 | [train_policy] epoch #213 | Logging diagnostics...
2021-06-04 13:50:41 | [train_policy] epoch #213 | Optimizing policy...
2021-06-04 13:50:41 | [train_policy] epoch #213 | Computing loss before
2021-06-04 13:50:41 | [train_policy] epoch #213 | Computing KL before
2021-06-04 13:50:41 | [train_policy] epoch #213 | Optimizing
2021-06-04 13:50:41 | [train_policy] epoch #213 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:41 | [train_policy] epoch #213 | computing loss before
2021-06-04 13:50:41 | [train_policy] epoch #213 | computing gradient
2021-06-04 13:50:41 | [train_policy] epoch #213 | gradient computed
2021-06-04 13:50:41 | [train_policy] epoch #213 | computing descent direction
2021-06-04 13:50:41 | [train_policy] epoch #213 | descent direction computed
2021-06-04 13:50:41 | [train_policy] epoch #213 | backtrack iters: 1
2021-06-04 13:50:41 | [train_policy] epoch #213 | optimization finished
2021-06-04 13:50:41 | [train_policy] epoch #213 | Computing KL after
2021-06-04 13:50:41 | [train_policy] epoch #213 | Computing loss after
2021-06-04 13:50:41 | [train_policy] epoch #213 | Fitting baseline...
2021-06-04 13:50:41 | [train_policy] epoch #213 | Saving snapshot...
2021-06-04 13:50:41 | [train_policy] epoch #213 | Saved
2021-06-04 13:50:41 | [train_policy] epoch #213 | Time 173.18 s
2021-06-04 13:50:41 | [train_policy] epoch #213 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287401
Evaluation/AverageDiscountedReturn          -44.7086
Evaluation/AverageReturn                    -44.7086
Evaluation/CompletionRate                     0
Evaluation/Iteration                        213
Evaluation/MaxReturn                        -32.832
Evaluation/MinReturn                        -63.9849
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.4895
Extras/EpisodeRewardMean                    -44.7547
LinearFeatureBaseline/ExplainedVariance       0.942461
PolicyExecTime                                0.227512
ProcessExecTime                               0.0311821
TotalEnvSteps                            216568
policy/Entropy                               -0.0146887
policy/KL                                     0.00649793
policy/KLBefore                               0
policy/LossAfter                             -0.0169913
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.985419
policy/dLoss                                  0.0169913
---------------------------------------  ----------------
2021-06-04 13:50:41 | [train_policy] epoch #214 | Obtaining samples for iteration 214...
2021-06-04 13:50:42 | [train_policy] epoch #214 | Logging diagnostics...
2021-06-04 13:50:42 | [train_policy] epoch #214 | Optimizing policy...
2021-06-04 13:50:42 | [train_policy] epoch #214 | Computing loss before
2021-06-04 13:50:42 | [train_policy] epoch #214 | Computing KL before
2021-06-04 13:50:42 | [train_policy] epoch #214 | Optimizing
2021-06-04 13:50:42 | [train_policy] epoch #214 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:42 | [train_policy] epoch #214 | computing loss before
2021-06-04 13:50:42 | [train_policy] epoch #214 | computing gradient
2021-06-04 13:50:42 | [train_policy] epoch #214 | gradient computed
2021-06-04 13:50:42 | [train_policy] epoch #214 | computing descent direction
2021-06-04 13:50:42 | [train_policy] epoch #214 | descent direction computed
2021-06-04 13:50:42 | [train_policy] epoch #214 | backtrack iters: 1
2021-06-04 13:50:42 | [train_policy] epoch #214 | optimization finished
2021-06-04 13:50:42 | [train_policy] epoch #214 | Computing KL after
2021-06-04 13:50:42 | [train_policy] epoch #214 | Computing loss after
2021-06-04 13:50:42 | [train_policy] epoch #214 | Fitting baseline...
2021-06-04 13:50:42 | [train_policy] epoch #214 | Saving snapshot...
2021-06-04 13:50:42 | [train_policy] epoch #214 | Saved
2021-06-04 13:50:42 | [train_policy] epoch #214 | Time 173.96 s
2021-06-04 13:50:42 | [train_policy] epoch #214 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284069
Evaluation/AverageDiscountedReturn          -45.4906
Evaluation/AverageReturn                    -45.4906
Evaluation/CompletionRate                     0
Evaluation/Iteration                        214
Evaluation/MaxReturn                        -29.5123
Evaluation/MinReturn                        -67.6109
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85771
Extras/EpisodeRewardMean                    -45.5863
LinearFeatureBaseline/ExplainedVariance       0.923272
PolicyExecTime                                0.228966
ProcessExecTime                               0.0311034
TotalEnvSteps                            217580
policy/Entropy                               -0.0252297
policy/KL                                     0.00640866
policy/KLBefore                               0
policy/LossAfter                             -0.01597
policy/LossBefore                             3.88726e-09
policy/Perplexity                             0.975086
policy/dLoss                                  0.01597
---------------------------------------  ----------------
2021-06-04 13:50:42 | [train_policy] epoch #215 | Obtaining samples for iteration 215...
2021-06-04 13:50:42 | [train_policy] epoch #215 | Logging diagnostics...
2021-06-04 13:50:42 | [train_policy] epoch #215 | Optimizing policy...
2021-06-04 13:50:42 | [train_policy] epoch #215 | Computing loss before
2021-06-04 13:50:42 | [train_policy] epoch #215 | Computing KL before
2021-06-04 13:50:42 | [train_policy] epoch #215 | Optimizing
2021-06-04 13:50:42 | [train_policy] epoch #215 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:42 | [train_policy] epoch #215 | computing loss before
2021-06-04 13:50:42 | [train_policy] epoch #215 | computing gradient
2021-06-04 13:50:42 | [train_policy] epoch #215 | gradient computed
2021-06-04 13:50:42 | [train_policy] epoch #215 | computing descent direction
2021-06-04 13:50:42 | [train_policy] epoch #215 | descent direction computed
2021-06-04 13:50:42 | [train_policy] epoch #215 | backtrack iters: 1
2021-06-04 13:50:42 | [train_policy] epoch #215 | optimization finished
2021-06-04 13:50:42 | [train_policy] epoch #215 | Computing KL after
2021-06-04 13:50:42 | [train_policy] epoch #215 | Computing loss after
2021-06-04 13:50:43 | [train_policy] epoch #215 | Fitting baseline...
2021-06-04 13:50:43 | [train_policy] epoch #215 | Saving snapshot...
2021-06-04 13:50:43 | [train_policy] epoch #215 | Saved
2021-06-04 13:50:43 | [train_policy] epoch #215 | Time 174.76 s
2021-06-04 13:50:43 | [train_policy] epoch #215 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288732
Evaluation/AverageDiscountedReturn          -44.4868
Evaluation/AverageReturn                    -44.4868
Evaluation/CompletionRate                     0
Evaluation/Iteration                        215
Evaluation/MaxReturn                        -31.8991
Evaluation/MinReturn                        -65.8182
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.21641
Extras/EpisodeRewardMean                    -44.6558
LinearFeatureBaseline/ExplainedVariance       0.901319
PolicyExecTime                                0.227962
ProcessExecTime                               0.0316501
TotalEnvSteps                            218592
policy/Entropy                               -0.045059
policy/KL                                     0.00670742
policy/KLBefore                               0
policy/LossAfter                             -0.0190242
policy/LossBefore                            -4.05217e-08
policy/Perplexity                             0.955941
policy/dLoss                                  0.0190242
---------------------------------------  ----------------
2021-06-04 13:50:43 | [train_policy] epoch #216 | Obtaining samples for iteration 216...
2021-06-04 13:50:43 | [train_policy] epoch #216 | Logging diagnostics...
2021-06-04 13:50:43 | [train_policy] epoch #216 | Optimizing policy...
2021-06-04 13:50:43 | [train_policy] epoch #216 | Computing loss before
2021-06-04 13:50:43 | [train_policy] epoch #216 | Computing KL before
2021-06-04 13:50:43 | [train_policy] epoch #216 | Optimizing
2021-06-04 13:50:43 | [train_policy] epoch #216 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:43 | [train_policy] epoch #216 | computing loss before
2021-06-04 13:50:43 | [train_policy] epoch #216 | computing gradient
2021-06-04 13:50:43 | [train_policy] epoch #216 | gradient computed
2021-06-04 13:50:43 | [train_policy] epoch #216 | computing descent direction
2021-06-04 13:50:43 | [train_policy] epoch #216 | descent direction computed
2021-06-04 13:50:43 | [train_policy] epoch #216 | backtrack iters: 1
2021-06-04 13:50:43 | [train_policy] epoch #216 | optimization finished
2021-06-04 13:50:43 | [train_policy] epoch #216 | Computing KL after
2021-06-04 13:50:43 | [train_policy] epoch #216 | Computing loss after
2021-06-04 13:50:43 | [train_policy] epoch #216 | Fitting baseline...
2021-06-04 13:50:43 | [train_policy] epoch #216 | Saving snapshot...
2021-06-04 13:50:43 | [train_policy] epoch #216 | Saved
2021-06-04 13:50:43 | [train_policy] epoch #216 | Time 175.55 s
2021-06-04 13:50:43 | [train_policy] epoch #216 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28468
Evaluation/AverageDiscountedReturn          -42.7951
Evaluation/AverageReturn                    -42.7951
Evaluation/CompletionRate                     0
Evaluation/Iteration                        216
Evaluation/MaxReturn                        -30.6992
Evaluation/MinReturn                        -64.1357
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.99884
Extras/EpisodeRewardMean                    -42.82
LinearFeatureBaseline/ExplainedVariance       0.9385
PolicyExecTime                                0.219686
ProcessExecTime                               0.0310936
TotalEnvSteps                            219604
policy/Entropy                               -0.0823109
policy/KL                                     0.00661792
policy/KLBefore                               0
policy/LossAfter                             -0.0129332
policy/LossBefore                            -1.97897e-08
policy/Perplexity                             0.920986
policy/dLoss                                  0.0129331
---------------------------------------  ----------------
2021-06-04 13:50:43 | [train_policy] epoch #217 | Obtaining samples for iteration 217...
2021-06-04 13:50:44 | [train_policy] epoch #217 | Logging diagnostics...
2021-06-04 13:50:44 | [train_policy] epoch #217 | Optimizing policy...
2021-06-04 13:50:44 | [train_policy] epoch #217 | Computing loss before
2021-06-04 13:50:44 | [train_policy] epoch #217 | Computing KL before
2021-06-04 13:50:44 | [train_policy] epoch #217 | Optimizing
2021-06-04 13:50:44 | [train_policy] epoch #217 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:44 | [train_policy] epoch #217 | computing loss before
2021-06-04 13:50:44 | [train_policy] epoch #217 | computing gradient
2021-06-04 13:50:44 | [train_policy] epoch #217 | gradient computed
2021-06-04 13:50:44 | [train_policy] epoch #217 | computing descent direction
2021-06-04 13:50:44 | [train_policy] epoch #217 | descent direction computed
2021-06-04 13:50:44 | [train_policy] epoch #217 | backtrack iters: 0
2021-06-04 13:50:44 | [train_policy] epoch #217 | optimization finished
2021-06-04 13:50:44 | [train_policy] epoch #217 | Computing KL after
2021-06-04 13:50:44 | [train_policy] epoch #217 | Computing loss after
2021-06-04 13:50:44 | [train_policy] epoch #217 | Fitting baseline...
2021-06-04 13:50:44 | [train_policy] epoch #217 | Saving snapshot...
2021-06-04 13:50:44 | [train_policy] epoch #217 | Saved
2021-06-04 13:50:44 | [train_policy] epoch #217 | Time 176.33 s
2021-06-04 13:50:44 | [train_policy] epoch #217 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285412
Evaluation/AverageDiscountedReturn          -43.9645
Evaluation/AverageReturn                    -43.9645
Evaluation/CompletionRate                     0
Evaluation/Iteration                        217
Evaluation/MaxReturn                        -32.0722
Evaluation/MinReturn                        -91.968
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.99237
Extras/EpisodeRewardMean                    -44.0316
LinearFeatureBaseline/ExplainedVariance       0.850923
PolicyExecTime                                0.223933
ProcessExecTime                               0.0312495
TotalEnvSteps                            220616
policy/Entropy                               -0.136519
policy/KL                                     0.00967256
policy/KLBefore                               0
policy/LossAfter                             -0.0217047
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.87239
policy/dLoss                                  0.0217047
---------------------------------------  ----------------
2021-06-04 13:50:44 | [train_policy] epoch #218 | Obtaining samples for iteration 218...
2021-06-04 13:50:45 | [train_policy] epoch #218 | Logging diagnostics...
2021-06-04 13:50:45 | [train_policy] epoch #218 | Optimizing policy...
2021-06-04 13:50:45 | [train_policy] epoch #218 | Computing loss before
2021-06-04 13:50:45 | [train_policy] epoch #218 | Computing KL before
2021-06-04 13:50:45 | [train_policy] epoch #218 | Optimizing
2021-06-04 13:50:45 | [train_policy] epoch #218 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:45 | [train_policy] epoch #218 | computing loss before
2021-06-04 13:50:45 | [train_policy] epoch #218 | computing gradient
2021-06-04 13:50:45 | [train_policy] epoch #218 | gradient computed
2021-06-04 13:50:45 | [train_policy] epoch #218 | computing descent direction
2021-06-04 13:50:45 | [train_policy] epoch #218 | descent direction computed
2021-06-04 13:50:45 | [train_policy] epoch #218 | backtrack iters: 1
2021-06-04 13:50:45 | [train_policy] epoch #218 | optimization finished
2021-06-04 13:50:45 | [train_policy] epoch #218 | Computing KL after
2021-06-04 13:50:45 | [train_policy] epoch #218 | Computing loss after
2021-06-04 13:50:45 | [train_policy] epoch #218 | Fitting baseline...
2021-06-04 13:50:45 | [train_policy] epoch #218 | Saving snapshot...
2021-06-04 13:50:45 | [train_policy] epoch #218 | Saved
2021-06-04 13:50:45 | [train_policy] epoch #218 | Time 177.11 s
2021-06-04 13:50:45 | [train_policy] epoch #218 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285995
Evaluation/AverageDiscountedReturn          -44.7247
Evaluation/AverageReturn                    -44.7247
Evaluation/CompletionRate                     0
Evaluation/Iteration                        218
Evaluation/MaxReturn                        -32.6786
Evaluation/MinReturn                        -92.3458
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.12774
Extras/EpisodeRewardMean                    -44.5025
LinearFeatureBaseline/ExplainedVariance       0.867408
PolicyExecTime                                0.213358
ProcessExecTime                               0.0311422
TotalEnvSteps                            221628
policy/Entropy                               -0.153139
policy/KL                                     0.00658906
policy/KLBefore                               0
policy/LossAfter                             -0.0191041
policy/LossBefore                            -1.20152e-08
policy/Perplexity                             0.858011
policy/dLoss                                  0.0191041
---------------------------------------  ----------------
2021-06-04 13:50:45 | [train_policy] epoch #219 | Obtaining samples for iteration 219...
2021-06-04 13:50:46 | [train_policy] epoch #219 | Logging diagnostics...
2021-06-04 13:50:46 | [train_policy] epoch #219 | Optimizing policy...
2021-06-04 13:50:46 | [train_policy] epoch #219 | Computing loss before
2021-06-04 13:50:46 | [train_policy] epoch #219 | Computing KL before
2021-06-04 13:50:46 | [train_policy] epoch #219 | Optimizing
2021-06-04 13:50:46 | [train_policy] epoch #219 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:46 | [train_policy] epoch #219 | computing loss before
2021-06-04 13:50:46 | [train_policy] epoch #219 | computing gradient
2021-06-04 13:50:46 | [train_policy] epoch #219 | gradient computed
2021-06-04 13:50:46 | [train_policy] epoch #219 | computing descent direction
2021-06-04 13:50:46 | [train_policy] epoch #219 | descent direction computed
2021-06-04 13:50:46 | [train_policy] epoch #219 | backtrack iters: 1
2021-06-04 13:50:46 | [train_policy] epoch #219 | optimization finished
2021-06-04 13:50:46 | [train_policy] epoch #219 | Computing KL after
2021-06-04 13:50:46 | [train_policy] epoch #219 | Computing loss after
2021-06-04 13:50:46 | [train_policy] epoch #219 | Fitting baseline...
2021-06-04 13:50:46 | [train_policy] epoch #219 | Saving snapshot...
2021-06-04 13:50:46 | [train_policy] epoch #219 | Saved
2021-06-04 13:50:46 | [train_policy] epoch #219 | Time 177.90 s
2021-06-04 13:50:46 | [train_policy] epoch #219 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.288633
Evaluation/AverageDiscountedReturn          -43.2715
Evaluation/AverageReturn                    -43.2715
Evaluation/CompletionRate                     0
Evaluation/Iteration                        219
Evaluation/MaxReturn                        -32.2778
Evaluation/MinReturn                        -61.2522
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.51205
Extras/EpisodeRewardMean                    -43.277
LinearFeatureBaseline/ExplainedVariance       0.909169
PolicyExecTime                                0.214945
ProcessExecTime                               0.03159
TotalEnvSteps                            222640
policy/Entropy                               -0.205287
policy/KL                                     0.00673122
policy/KLBefore                               0
policy/LossAfter                             -0.0150176
policy/LossBefore                             1.46067e-08
policy/Perplexity                             0.814413
policy/dLoss                                  0.0150176
---------------------------------------  ----------------
2021-06-04 13:50:46 | [train_policy] epoch #220 | Obtaining samples for iteration 220...
2021-06-04 13:50:46 | [train_policy] epoch #220 | Logging diagnostics...
2021-06-04 13:50:46 | [train_policy] epoch #220 | Optimizing policy...
2021-06-04 13:50:46 | [train_policy] epoch #220 | Computing loss before
2021-06-04 13:50:46 | [train_policy] epoch #220 | Computing KL before
2021-06-04 13:50:46 | [train_policy] epoch #220 | Optimizing
2021-06-04 13:50:46 | [train_policy] epoch #220 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:46 | [train_policy] epoch #220 | computing loss before
2021-06-04 13:50:46 | [train_policy] epoch #220 | computing gradient
2021-06-04 13:50:46 | [train_policy] epoch #220 | gradient computed
2021-06-04 13:50:46 | [train_policy] epoch #220 | computing descent direction
2021-06-04 13:50:46 | [train_policy] epoch #220 | descent direction computed
2021-06-04 13:50:46 | [train_policy] epoch #220 | backtrack iters: 1
2021-06-04 13:50:46 | [train_policy] epoch #220 | optimization finished
2021-06-04 13:50:46 | [train_policy] epoch #220 | Computing KL after
2021-06-04 13:50:46 | [train_policy] epoch #220 | Computing loss after
2021-06-04 13:50:46 | [train_policy] epoch #220 | Fitting baseline...
2021-06-04 13:50:46 | [train_policy] epoch #220 | Saving snapshot...
2021-06-04 13:50:46 | [train_policy] epoch #220 | Saved
2021-06-04 13:50:46 | [train_policy] epoch #220 | Time 178.67 s
2021-06-04 13:50:46 | [train_policy] epoch #220 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.285058
Evaluation/AverageDiscountedReturn          -44.3052
Evaluation/AverageReturn                    -44.3052
Evaluation/CompletionRate                     0
Evaluation/Iteration                        220
Evaluation/MaxReturn                        -30.732
Evaluation/MinReturn                        -64.711
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.29308
Extras/EpisodeRewardMean                    -44.3821
LinearFeatureBaseline/ExplainedVariance       0.92045
PolicyExecTime                                0.215263
ProcessExecTime                               0.0312614
TotalEnvSteps                            223652
policy/Entropy                               -0.235307
policy/KL                                     0.00647477
policy/KLBefore                               0
policy/LossAfter                             -0.0153718
policy/LossBefore                            -1.60202e-08
policy/Perplexity                             0.790328
policy/dLoss                                  0.0153717
---------------------------------------  ----------------
2021-06-04 13:50:46 | [train_policy] epoch #221 | Obtaining samples for iteration 221...
2021-06-04 13:50:47 | [train_policy] epoch #221 | Logging diagnostics...
2021-06-04 13:50:47 | [train_policy] epoch #221 | Optimizing policy...
2021-06-04 13:50:47 | [train_policy] epoch #221 | Computing loss before
2021-06-04 13:50:47 | [train_policy] epoch #221 | Computing KL before
2021-06-04 13:50:47 | [train_policy] epoch #221 | Optimizing
2021-06-04 13:50:47 | [train_policy] epoch #221 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:47 | [train_policy] epoch #221 | computing loss before
2021-06-04 13:50:47 | [train_policy] epoch #221 | computing gradient
2021-06-04 13:50:47 | [train_policy] epoch #221 | gradient computed
2021-06-04 13:50:47 | [train_policy] epoch #221 | computing descent direction
2021-06-04 13:50:47 | [train_policy] epoch #221 | descent direction computed
2021-06-04 13:50:47 | [train_policy] epoch #221 | backtrack iters: 0
2021-06-04 13:50:47 | [train_policy] epoch #221 | optimization finished
2021-06-04 13:50:47 | [train_policy] epoch #221 | Computing KL after
2021-06-04 13:50:47 | [train_policy] epoch #221 | Computing loss after
2021-06-04 13:50:47 | [train_policy] epoch #221 | Fitting baseline...
2021-06-04 13:50:47 | [train_policy] epoch #221 | Saving snapshot...
2021-06-04 13:50:47 | [train_policy] epoch #221 | Saved
2021-06-04 13:50:47 | [train_policy] epoch #221 | Time 179.45 s
2021-06-04 13:50:47 | [train_policy] epoch #221 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285863
Evaluation/AverageDiscountedReturn          -66.4106
Evaluation/AverageReturn                    -66.4106
Evaluation/CompletionRate                     0
Evaluation/Iteration                        221
Evaluation/MaxReturn                        -31.704
Evaluation/MinReturn                      -2062.04
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.375
Extras/EpisodeRewardMean                    -64.64
LinearFeatureBaseline/ExplainedVariance       0.0108062
PolicyExecTime                                0.224324
ProcessExecTime                               0.0313394
TotalEnvSteps                            224664
policy/Entropy                               -0.226279
policy/KL                                     0.00960252
policy/KLBefore                               0
policy/LossAfter                             -0.0239419
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.797496
policy/dLoss                                  0.0239419
---------------------------------------  ----------------
2021-06-04 13:50:47 | [train_policy] epoch #222 | Obtaining samples for iteration 222...
2021-06-04 13:50:48 | [train_policy] epoch #222 | Logging diagnostics...
2021-06-04 13:50:48 | [train_policy] epoch #222 | Optimizing policy...
2021-06-04 13:50:48 | [train_policy] epoch #222 | Computing loss before
2021-06-04 13:50:48 | [train_policy] epoch #222 | Computing KL before
2021-06-04 13:50:48 | [train_policy] epoch #222 | Optimizing
2021-06-04 13:50:48 | [train_policy] epoch #222 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:48 | [train_policy] epoch #222 | computing loss before
2021-06-04 13:50:48 | [train_policy] epoch #222 | computing gradient
2021-06-04 13:50:48 | [train_policy] epoch #222 | gradient computed
2021-06-04 13:50:48 | [train_policy] epoch #222 | computing descent direction
2021-06-04 13:50:48 | [train_policy] epoch #222 | descent direction computed
2021-06-04 13:50:48 | [train_policy] epoch #222 | backtrack iters: 0
2021-06-04 13:50:48 | [train_policy] epoch #222 | optimization finished
2021-06-04 13:50:48 | [train_policy] epoch #222 | Computing KL after
2021-06-04 13:50:48 | [train_policy] epoch #222 | Computing loss after
2021-06-04 13:50:48 | [train_policy] epoch #222 | Fitting baseline...
2021-06-04 13:50:48 | [train_policy] epoch #222 | Saving snapshot...
2021-06-04 13:50:48 | [train_policy] epoch #222 | Saved
2021-06-04 13:50:48 | [train_policy] epoch #222 | Time 180.25 s
2021-06-04 13:50:48 | [train_policy] epoch #222 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.286373
Evaluation/AverageDiscountedReturn          -65.2046
Evaluation/AverageReturn                    -65.2046
Evaluation/CompletionRate                     0
Evaluation/Iteration                        222
Evaluation/MaxReturn                        -30.2326
Evaluation/MinReturn                      -2062.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.437
Extras/EpisodeRewardMean                    -63.7512
LinearFeatureBaseline/ExplainedVariance       0.136704
PolicyExecTime                                0.234382
ProcessExecTime                               0.0313711
TotalEnvSteps                            225676
policy/Entropy                               -0.195479
policy/KL                                     0.00968959
policy/KLBefore                               0
policy/LossAfter                             -0.0170012
policy/LossBefore                            -0
policy/Perplexity                             0.82244
policy/dLoss                                  0.0170012
---------------------------------------  ---------------
2021-06-04 13:50:48 | [train_policy] epoch #223 | Obtaining samples for iteration 223...
2021-06-04 13:50:49 | [train_policy] epoch #223 | Logging diagnostics...
2021-06-04 13:50:49 | [train_policy] epoch #223 | Optimizing policy...
2021-06-04 13:50:49 | [train_policy] epoch #223 | Computing loss before
2021-06-04 13:50:49 | [train_policy] epoch #223 | Computing KL before
2021-06-04 13:50:49 | [train_policy] epoch #223 | Optimizing
2021-06-04 13:50:49 | [train_policy] epoch #223 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:49 | [train_policy] epoch #223 | computing loss before
2021-06-04 13:50:49 | [train_policy] epoch #223 | computing gradient
2021-06-04 13:50:49 | [train_policy] epoch #223 | gradient computed
2021-06-04 13:50:49 | [train_policy] epoch #223 | computing descent direction
2021-06-04 13:50:49 | [train_policy] epoch #223 | descent direction computed
2021-06-04 13:50:49 | [train_policy] epoch #223 | backtrack iters: 1
2021-06-04 13:50:49 | [train_policy] epoch #223 | optimization finished
2021-06-04 13:50:49 | [train_policy] epoch #223 | Computing KL after
2021-06-04 13:50:49 | [train_policy] epoch #223 | Computing loss after
2021-06-04 13:50:49 | [train_policy] epoch #223 | Fitting baseline...
2021-06-04 13:50:49 | [train_policy] epoch #223 | Saving snapshot...
2021-06-04 13:50:49 | [train_policy] epoch #223 | Saved
2021-06-04 13:50:49 | [train_policy] epoch #223 | Time 181.04 s
2021-06-04 13:50:49 | [train_policy] epoch #223 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.289416
Evaluation/AverageDiscountedReturn          -42.7339
Evaluation/AverageReturn                    -42.7339
Evaluation/CompletionRate                     0
Evaluation/Iteration                        223
Evaluation/MaxReturn                        -31.496
Evaluation/MinReturn                        -63.786
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.43991
Extras/EpisodeRewardMean                    -43.1542
LinearFeatureBaseline/ExplainedVariance     -80.9303
PolicyExecTime                                0.227578
ProcessExecTime                               0.0316343
TotalEnvSteps                            226688
policy/Entropy                               -0.221877
policy/KL                                     0.00655051
policy/KLBefore                               0
policy/LossAfter                             -0.0215132
policy/LossBefore                            -3.53387e-09
policy/Perplexity                             0.801014
policy/dLoss                                  0.0215132
---------------------------------------  ----------------
2021-06-04 13:50:49 | [train_policy] epoch #224 | Obtaining samples for iteration 224...
2021-06-04 13:50:49 | [train_policy] epoch #224 | Logging diagnostics...
2021-06-04 13:50:49 | [train_policy] epoch #224 | Optimizing policy...
2021-06-04 13:50:49 | [train_policy] epoch #224 | Computing loss before
2021-06-04 13:50:49 | [train_policy] epoch #224 | Computing KL before
2021-06-04 13:50:49 | [train_policy] epoch #224 | Optimizing
2021-06-04 13:50:49 | [train_policy] epoch #224 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:49 | [train_policy] epoch #224 | computing loss before
2021-06-04 13:50:49 | [train_policy] epoch #224 | computing gradient
2021-06-04 13:50:49 | [train_policy] epoch #224 | gradient computed
2021-06-04 13:50:49 | [train_policy] epoch #224 | computing descent direction
2021-06-04 13:50:50 | [train_policy] epoch #224 | descent direction computed
2021-06-04 13:50:50 | [train_policy] epoch #224 | backtrack iters: 0
2021-06-04 13:50:50 | [train_policy] epoch #224 | optimization finished
2021-06-04 13:50:50 | [train_policy] epoch #224 | Computing KL after
2021-06-04 13:50:50 | [train_policy] epoch #224 | Computing loss after
2021-06-04 13:50:50 | [train_policy] epoch #224 | Fitting baseline...
2021-06-04 13:50:50 | [train_policy] epoch #224 | Saving snapshot...
2021-06-04 13:50:50 | [train_policy] epoch #224 | Saved
2021-06-04 13:50:50 | [train_policy] epoch #224 | Time 181.82 s
2021-06-04 13:50:50 | [train_policy] epoch #224 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284416
Evaluation/AverageDiscountedReturn          -67.3775
Evaluation/AverageReturn                    -67.3775
Evaluation/CompletionRate                     0
Evaluation/Iteration                        224
Evaluation/MaxReturn                        -32.4348
Evaluation/MinReturn                      -2062.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.271
Extras/EpisodeRewardMean                    -65.1886
LinearFeatureBaseline/ExplainedVariance       0.0114832
PolicyExecTime                                0.218797
ProcessExecTime                               0.0311496
TotalEnvSteps                            227700
policy/Entropy                               -0.241058
policy/KL                                     0.00971366
policy/KLBefore                               0
policy/LossAfter                             -0.0287089
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.785796
policy/dLoss                                  0.0287089
---------------------------------------  ----------------
2021-06-04 13:50:50 | [train_policy] epoch #225 | Obtaining samples for iteration 225...
2021-06-04 13:50:50 | [train_policy] epoch #225 | Logging diagnostics...
2021-06-04 13:50:50 | [train_policy] epoch #225 | Optimizing policy...
2021-06-04 13:50:50 | [train_policy] epoch #225 | Computing loss before
2021-06-04 13:50:50 | [train_policy] epoch #225 | Computing KL before
2021-06-04 13:50:50 | [train_policy] epoch #225 | Optimizing
2021-06-04 13:50:50 | [train_policy] epoch #225 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:50 | [train_policy] epoch #225 | computing loss before
2021-06-04 13:50:50 | [train_policy] epoch #225 | computing gradient
2021-06-04 13:50:50 | [train_policy] epoch #225 | gradient computed
2021-06-04 13:50:50 | [train_policy] epoch #225 | computing descent direction
2021-06-04 13:50:50 | [train_policy] epoch #225 | descent direction computed
2021-06-04 13:50:50 | [train_policy] epoch #225 | backtrack iters: 0
2021-06-04 13:50:50 | [train_policy] epoch #225 | optimization finished
2021-06-04 13:50:50 | [train_policy] epoch #225 | Computing KL after
2021-06-04 13:50:50 | [train_policy] epoch #225 | Computing loss after
2021-06-04 13:50:50 | [train_policy] epoch #225 | Fitting baseline...
2021-06-04 13:50:50 | [train_policy] epoch #225 | Saving snapshot...
2021-06-04 13:50:50 | [train_policy] epoch #225 | Saved
2021-06-04 13:50:50 | [train_policy] epoch #225 | Time 182.62 s
2021-06-04 13:50:50 | [train_policy] epoch #225 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287352
Evaluation/AverageDiscountedReturn          -66.3227
Evaluation/AverageReturn                    -66.3227
Evaluation/CompletionRate                     0
Evaluation/Iteration                        225
Evaluation/MaxReturn                        -30.6378
Evaluation/MinReturn                      -2062.13
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.334
Extras/EpisodeRewardMean                    -64.8202
LinearFeatureBaseline/ExplainedVariance       0.0985296
PolicyExecTime                                0.228421
ProcessExecTime                               0.0316494
TotalEnvSteps                            228712
policy/Entropy                               -0.224296
policy/KL                                     0.00931766
policy/KLBefore                               0
policy/LossAfter                             -0.0258931
policy/LossBefore                            -2.12032e-09
policy/Perplexity                             0.799079
policy/dLoss                                  0.0258931
---------------------------------------  ----------------
2021-06-04 13:50:50 | [train_policy] epoch #226 | Obtaining samples for iteration 226...
2021-06-04 13:50:51 | [train_policy] epoch #226 | Logging diagnostics...
2021-06-04 13:50:51 | [train_policy] epoch #226 | Optimizing policy...
2021-06-04 13:50:51 | [train_policy] epoch #226 | Computing loss before
2021-06-04 13:50:51 | [train_policy] epoch #226 | Computing KL before
2021-06-04 13:50:51 | [train_policy] epoch #226 | Optimizing
2021-06-04 13:50:51 | [train_policy] epoch #226 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:51 | [train_policy] epoch #226 | computing loss before
2021-06-04 13:50:51 | [train_policy] epoch #226 | computing gradient
2021-06-04 13:50:51 | [train_policy] epoch #226 | gradient computed
2021-06-04 13:50:51 | [train_policy] epoch #226 | computing descent direction
2021-06-04 13:50:51 | [train_policy] epoch #226 | descent direction computed
2021-06-04 13:50:51 | [train_policy] epoch #226 | backtrack iters: 0
2021-06-04 13:50:51 | [train_policy] epoch #226 | optimization finished
2021-06-04 13:50:51 | [train_policy] epoch #226 | Computing KL after
2021-06-04 13:50:51 | [train_policy] epoch #226 | Computing loss after
2021-06-04 13:50:51 | [train_policy] epoch #226 | Fitting baseline...
2021-06-04 13:50:51 | [train_policy] epoch #226 | Saving snapshot...
2021-06-04 13:50:51 | [train_policy] epoch #226 | Saved
2021-06-04 13:50:51 | [train_policy] epoch #226 | Time 183.42 s
2021-06-04 13:50:51 | [train_policy] epoch #226 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285145
Evaluation/AverageDiscountedReturn          -44.6054
Evaluation/AverageReturn                    -44.6054
Evaluation/CompletionRate                     0
Evaluation/Iteration                        226
Evaluation/MaxReturn                        -29.8572
Evaluation/MinReturn                       -182.743
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         16.3149
Extras/EpisodeRewardMean                    -64.781
LinearFeatureBaseline/ExplainedVariance     -16.8505
PolicyExecTime                                0.231072
ProcessExecTime                               0.0313127
TotalEnvSteps                            229724
policy/Entropy                               -0.198969
policy/KL                                     0.00995011
policy/KLBefore                               0
policy/LossAfter                             -0.0383361
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.819575
policy/dLoss                                  0.0383361
---------------------------------------  ----------------
2021-06-04 13:50:51 | [train_policy] epoch #227 | Obtaining samples for iteration 227...
2021-06-04 13:50:52 | [train_policy] epoch #227 | Logging diagnostics...
2021-06-04 13:50:52 | [train_policy] epoch #227 | Optimizing policy...
2021-06-04 13:50:52 | [train_policy] epoch #227 | Computing loss before
2021-06-04 13:50:52 | [train_policy] epoch #227 | Computing KL before
2021-06-04 13:50:52 | [train_policy] epoch #227 | Optimizing
2021-06-04 13:50:52 | [train_policy] epoch #227 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:52 | [train_policy] epoch #227 | computing loss before
2021-06-04 13:50:52 | [train_policy] epoch #227 | computing gradient
2021-06-04 13:50:52 | [train_policy] epoch #227 | gradient computed
2021-06-04 13:50:52 | [train_policy] epoch #227 | computing descent direction
2021-06-04 13:50:52 | [train_policy] epoch #227 | descent direction computed
2021-06-04 13:50:52 | [train_policy] epoch #227 | backtrack iters: 0
2021-06-04 13:50:52 | [train_policy] epoch #227 | optimization finished
2021-06-04 13:50:52 | [train_policy] epoch #227 | Computing KL after
2021-06-04 13:50:52 | [train_policy] epoch #227 | Computing loss after
2021-06-04 13:50:52 | [train_policy] epoch #227 | Fitting baseline...
2021-06-04 13:50:52 | [train_policy] epoch #227 | Saving snapshot...
2021-06-04 13:50:52 | [train_policy] epoch #227 | Saved
2021-06-04 13:50:52 | [train_policy] epoch #227 | Time 184.20 s
2021-06-04 13:50:52 | [train_policy] epoch #227 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283826
Evaluation/AverageDiscountedReturn          -64.4777
Evaluation/AverageReturn                    -64.4777
Evaluation/CompletionRate                     0
Evaluation/Iteration                        227
Evaluation/MaxReturn                        -32.1788
Evaluation/MinReturn                      -2062.75
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.579
Extras/EpisodeRewardMean                    -62.6654
LinearFeatureBaseline/ExplainedVariance       0.0258501
PolicyExecTime                                0.220888
ProcessExecTime                               0.0311182
TotalEnvSteps                            230736
policy/Entropy                               -0.212056
policy/KL                                     0.00997209
policy/KLBefore                               0
policy/LossAfter                             -0.0312428
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.808919
policy/dLoss                                  0.0312428
---------------------------------------  ----------------
2021-06-04 13:50:52 | [train_policy] epoch #228 | Obtaining samples for iteration 228...
2021-06-04 13:50:53 | [train_policy] epoch #228 | Logging diagnostics...
2021-06-04 13:50:53 | [train_policy] epoch #228 | Optimizing policy...
2021-06-04 13:50:53 | [train_policy] epoch #228 | Computing loss before
2021-06-04 13:50:53 | [train_policy] epoch #228 | Computing KL before
2021-06-04 13:50:53 | [train_policy] epoch #228 | Optimizing
2021-06-04 13:50:53 | [train_policy] epoch #228 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:53 | [train_policy] epoch #228 | computing loss before
2021-06-04 13:50:53 | [train_policy] epoch #228 | computing gradient
2021-06-04 13:50:53 | [train_policy] epoch #228 | gradient computed
2021-06-04 13:50:53 | [train_policy] epoch #228 | computing descent direction
2021-06-04 13:50:53 | [train_policy] epoch #228 | descent direction computed
2021-06-04 13:50:53 | [train_policy] epoch #228 | backtrack iters: 0
2021-06-04 13:50:53 | [train_policy] epoch #228 | optimization finished
2021-06-04 13:50:53 | [train_policy] epoch #228 | Computing KL after
2021-06-04 13:50:53 | [train_policy] epoch #228 | Computing loss after
2021-06-04 13:50:53 | [train_policy] epoch #228 | Fitting baseline...
2021-06-04 13:50:53 | [train_policy] epoch #228 | Saving snapshot...
2021-06-04 13:50:53 | [train_policy] epoch #228 | Saved
2021-06-04 13:50:53 | [train_policy] epoch #228 | Time 184.99 s
2021-06-04 13:50:53 | [train_policy] epoch #228 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28744
Evaluation/AverageDiscountedReturn          -43.4096
Evaluation/AverageReturn                    -43.4096
Evaluation/CompletionRate                     0
Evaluation/Iteration                        228
Evaluation/MaxReturn                        -30.5583
Evaluation/MinReturn                        -65.3497
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.86805
Extras/EpisodeRewardMean                    -43.2225
LinearFeatureBaseline/ExplainedVariance     -37.2154
PolicyExecTime                                0.231013
ProcessExecTime                               0.0314267
TotalEnvSteps                            231748
policy/Entropy                               -0.199056
policy/KL                                     0.00988252
policy/KLBefore                               0
policy/LossAfter                             -0.0369323
policy/LossBefore                             8.71688e-09
policy/Perplexity                             0.819504
policy/dLoss                                  0.0369323
---------------------------------------  ----------------
2021-06-04 13:50:53 | [train_policy] epoch #229 | Obtaining samples for iteration 229...
2021-06-04 13:50:53 | [train_policy] epoch #229 | Logging diagnostics...
2021-06-04 13:50:53 | [train_policy] epoch #229 | Optimizing policy...
2021-06-04 13:50:53 | [train_policy] epoch #229 | Computing loss before
2021-06-04 13:50:53 | [train_policy] epoch #229 | Computing KL before
2021-06-04 13:50:53 | [train_policy] epoch #229 | Optimizing
2021-06-04 13:50:53 | [train_policy] epoch #229 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:53 | [train_policy] epoch #229 | computing loss before
2021-06-04 13:50:53 | [train_policy] epoch #229 | computing gradient
2021-06-04 13:50:53 | [train_policy] epoch #229 | gradient computed
2021-06-04 13:50:53 | [train_policy] epoch #229 | computing descent direction
2021-06-04 13:50:54 | [train_policy] epoch #229 | descent direction computed
2021-06-04 13:50:54 | [train_policy] epoch #229 | backtrack iters: 0
2021-06-04 13:50:54 | [train_policy] epoch #229 | optimization finished
2021-06-04 13:50:54 | [train_policy] epoch #229 | Computing KL after
2021-06-04 13:50:54 | [train_policy] epoch #229 | Computing loss after
2021-06-04 13:50:54 | [train_policy] epoch #229 | Fitting baseline...
2021-06-04 13:50:54 | [train_policy] epoch #229 | Saving snapshot...
2021-06-04 13:50:54 | [train_policy] epoch #229 | Saved
2021-06-04 13:50:54 | [train_policy] epoch #229 | Time 185.78 s
2021-06-04 13:50:54 | [train_policy] epoch #229 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285781
Evaluation/AverageDiscountedReturn          -86.5312
Evaluation/AverageReturn                    -86.5312
Evaluation/CompletionRate                     0
Evaluation/Iteration                        229
Evaluation/MaxReturn                        -30.9099
Evaluation/MinReturn                      -2062.16
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.581
Extras/EpisodeRewardMean                    -83.0855
LinearFeatureBaseline/ExplainedVariance       0.00908766
PolicyExecTime                                0.224965
ProcessExecTime                               0.0313389
TotalEnvSteps                            232760
policy/Entropy                               -0.163271
policy/KL                                     0.00967832
policy/KLBefore                               0
policy/LossAfter                             -0.0246498
policy/LossBefore                             1.76694e-08
policy/Perplexity                             0.849361
policy/dLoss                                  0.0246498
---------------------------------------  ----------------
2021-06-04 13:50:54 | [train_policy] epoch #230 | Obtaining samples for iteration 230...
2021-06-04 13:50:54 | [train_policy] epoch #230 | Logging diagnostics...
2021-06-04 13:50:54 | [train_policy] epoch #230 | Optimizing policy...
2021-06-04 13:50:54 | [train_policy] epoch #230 | Computing loss before
2021-06-04 13:50:54 | [train_policy] epoch #230 | Computing KL before
2021-06-04 13:50:54 | [train_policy] epoch #230 | Optimizing
2021-06-04 13:50:54 | [train_policy] epoch #230 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:54 | [train_policy] epoch #230 | computing loss before
2021-06-04 13:50:54 | [train_policy] epoch #230 | computing gradient
2021-06-04 13:50:54 | [train_policy] epoch #230 | gradient computed
2021-06-04 13:50:54 | [train_policy] epoch #230 | computing descent direction
2021-06-04 13:50:54 | [train_policy] epoch #230 | descent direction computed
2021-06-04 13:50:54 | [train_policy] epoch #230 | backtrack iters: 0
2021-06-04 13:50:54 | [train_policy] epoch #230 | optimization finished
2021-06-04 13:50:54 | [train_policy] epoch #230 | Computing KL after
2021-06-04 13:50:54 | [train_policy] epoch #230 | Computing loss after
2021-06-04 13:50:54 | [train_policy] epoch #230 | Fitting baseline...
2021-06-04 13:50:54 | [train_policy] epoch #230 | Saving snapshot...
2021-06-04 13:50:54 | [train_policy] epoch #230 | Saved
2021-06-04 13:50:54 | [train_policy] epoch #230 | Time 186.56 s
2021-06-04 13:50:54 | [train_policy] epoch #230 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283987
Evaluation/AverageDiscountedReturn          -66.4009
Evaluation/AverageReturn                    -66.4009
Evaluation/CompletionRate                     0
Evaluation/Iteration                        230
Evaluation/MaxReturn                        -30.1673
Evaluation/MinReturn                      -2062
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.321
Extras/EpisodeRewardMean                    -64.7484
LinearFeatureBaseline/ExplainedVariance      -0.00287268
PolicyExecTime                                0.225317
ProcessExecTime                               0.0311029
TotalEnvSteps                            233772
policy/Entropy                               -0.206637
policy/KL                                     0.00924741
policy/KLBefore                               0
policy/LossAfter                             -0.0185265
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.813315
policy/dLoss                                  0.0185265
---------------------------------------  ----------------
2021-06-04 13:50:54 | [train_policy] epoch #231 | Obtaining samples for iteration 231...
2021-06-04 13:50:55 | [train_policy] epoch #231 | Logging diagnostics...
2021-06-04 13:50:55 | [train_policy] epoch #231 | Optimizing policy...
2021-06-04 13:50:55 | [train_policy] epoch #231 | Computing loss before
2021-06-04 13:50:55 | [train_policy] epoch #231 | Computing KL before
2021-06-04 13:50:55 | [train_policy] epoch #231 | Optimizing
2021-06-04 13:50:55 | [train_policy] epoch #231 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:55 | [train_policy] epoch #231 | computing loss before
2021-06-04 13:50:55 | [train_policy] epoch #231 | computing gradient
2021-06-04 13:50:55 | [train_policy] epoch #231 | gradient computed
2021-06-04 13:50:55 | [train_policy] epoch #231 | computing descent direction
2021-06-04 13:50:55 | [train_policy] epoch #231 | descent direction computed
2021-06-04 13:50:55 | [train_policy] epoch #231 | backtrack iters: 1
2021-06-04 13:50:55 | [train_policy] epoch #231 | optimization finished
2021-06-04 13:50:55 | [train_policy] epoch #231 | Computing KL after
2021-06-04 13:50:55 | [train_policy] epoch #231 | Computing loss after
2021-06-04 13:50:55 | [train_policy] epoch #231 | Fitting baseline...
2021-06-04 13:50:55 | [train_policy] epoch #231 | Saving snapshot...
2021-06-04 13:50:55 | [train_policy] epoch #231 | Saved
2021-06-04 13:50:55 | [train_policy] epoch #231 | Time 187.36 s
2021-06-04 13:50:55 | [train_policy] epoch #231 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283448
Evaluation/AverageDiscountedReturn          -66.0088
Evaluation/AverageReturn                    -66.0088
Evaluation/CompletionRate                     0
Evaluation/Iteration                        231
Evaluation/MaxReturn                        -29.9013
Evaluation/MinReturn                      -2062.98
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.462
Extras/EpisodeRewardMean                    -64.4518
LinearFeatureBaseline/ExplainedVariance       0.100821
PolicyExecTime                                0.2335
ProcessExecTime                               0.0309317
TotalEnvSteps                            234784
policy/Entropy                               -0.195209
policy/KL                                     0.00766358
policy/KLBefore                               0
policy/LossAfter                             -0.0115101
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.822663
policy/dLoss                                  0.0115101
---------------------------------------  ----------------
2021-06-04 13:50:55 | [train_policy] epoch #232 | Obtaining samples for iteration 232...
2021-06-04 13:50:56 | [train_policy] epoch #232 | Logging diagnostics...
2021-06-04 13:50:56 | [train_policy] epoch #232 | Optimizing policy...
2021-06-04 13:50:56 | [train_policy] epoch #232 | Computing loss before
2021-06-04 13:50:56 | [train_policy] epoch #232 | Computing KL before
2021-06-04 13:50:56 | [train_policy] epoch #232 | Optimizing
2021-06-04 13:50:56 | [train_policy] epoch #232 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:56 | [train_policy] epoch #232 | computing loss before
2021-06-04 13:50:56 | [train_policy] epoch #232 | computing gradient
2021-06-04 13:50:56 | [train_policy] epoch #232 | gradient computed
2021-06-04 13:50:56 | [train_policy] epoch #232 | computing descent direction
2021-06-04 13:50:56 | [train_policy] epoch #232 | descent direction computed
2021-06-04 13:50:56 | [train_policy] epoch #232 | backtrack iters: 1
2021-06-04 13:50:56 | [train_policy] epoch #232 | optimization finished
2021-06-04 13:50:56 | [train_policy] epoch #232 | Computing KL after
2021-06-04 13:50:56 | [train_policy] epoch #232 | Computing loss after
2021-06-04 13:50:56 | [train_policy] epoch #232 | Fitting baseline...
2021-06-04 13:50:56 | [train_policy] epoch #232 | Saving snapshot...
2021-06-04 13:50:56 | [train_policy] epoch #232 | Saved
2021-06-04 13:50:56 | [train_policy] epoch #232 | Time 188.15 s
2021-06-04 13:50:56 | [train_policy] epoch #232 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284757
Evaluation/AverageDiscountedReturn          -88.1078
Evaluation/AverageReturn                    -88.1078
Evaluation/CompletionRate                     0
Evaluation/Iteration                        232
Evaluation/MaxReturn                        -31.6912
Evaluation/MinReturn                      -2064.33
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.635
Extras/EpisodeRewardMean                    -84.6404
LinearFeatureBaseline/ExplainedVariance       0.171978
PolicyExecTime                                0.228074
ProcessExecTime                               0.0313029
TotalEnvSteps                            235796
policy/Entropy                               -0.211543
policy/KL                                     0.0065454
policy/KLBefore                               0
policy/LossAfter                             -0.017193
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.809334
policy/dLoss                                  0.017193
---------------------------------------  ----------------
2021-06-04 13:50:56 | [train_policy] epoch #233 | Obtaining samples for iteration 233...
2021-06-04 13:50:57 | [train_policy] epoch #233 | Logging diagnostics...
2021-06-04 13:50:57 | [train_policy] epoch #233 | Optimizing policy...
2021-06-04 13:50:57 | [train_policy] epoch #233 | Computing loss before
2021-06-04 13:50:57 | [train_policy] epoch #233 | Computing KL before
2021-06-04 13:50:57 | [train_policy] epoch #233 | Optimizing
2021-06-04 13:50:57 | [train_policy] epoch #233 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:57 | [train_policy] epoch #233 | computing loss before
2021-06-04 13:50:57 | [train_policy] epoch #233 | computing gradient
2021-06-04 13:50:57 | [train_policy] epoch #233 | gradient computed
2021-06-04 13:50:57 | [train_policy] epoch #233 | computing descent direction
2021-06-04 13:50:57 | [train_policy] epoch #233 | descent direction computed
2021-06-04 13:50:57 | [train_policy] epoch #233 | backtrack iters: 0
2021-06-04 13:50:57 | [train_policy] epoch #233 | optimization finished
2021-06-04 13:50:57 | [train_policy] epoch #233 | Computing KL after
2021-06-04 13:50:57 | [train_policy] epoch #233 | Computing loss after
2021-06-04 13:50:57 | [train_policy] epoch #233 | Fitting baseline...
2021-06-04 13:50:57 | [train_policy] epoch #233 | Saving snapshot...
2021-06-04 13:50:57 | [train_policy] epoch #233 | Saved
2021-06-04 13:50:57 | [train_policy] epoch #233 | Time 188.94 s
2021-06-04 13:50:57 | [train_policy] epoch #233 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284837
Evaluation/AverageDiscountedReturn          -44.1908
Evaluation/AverageReturn                    -44.1908
Evaluation/CompletionRate                     0
Evaluation/Iteration                        233
Evaluation/MaxReturn                        -30.6361
Evaluation/MinReturn                        -64.4125
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.03444
Extras/EpisodeRewardMean                    -44.4316
LinearFeatureBaseline/ExplainedVariance     -80.8425
PolicyExecTime                                0.222999
ProcessExecTime                               0.0312171
TotalEnvSteps                            236808
policy/Entropy                               -0.202442
policy/KL                                     0.00962937
policy/KLBefore                               0
policy/LossAfter                             -0.0124278
policy/LossBefore                             2.49138e-08
policy/Perplexity                             0.816734
policy/dLoss                                  0.0124278
---------------------------------------  ----------------
2021-06-04 13:50:57 | [train_policy] epoch #234 | Obtaining samples for iteration 234...
2021-06-04 13:50:57 | [train_policy] epoch #234 | Logging diagnostics...
2021-06-04 13:50:57 | [train_policy] epoch #234 | Optimizing policy...
2021-06-04 13:50:57 | [train_policy] epoch #234 | Computing loss before
2021-06-04 13:50:57 | [train_policy] epoch #234 | Computing KL before
2021-06-04 13:50:57 | [train_policy] epoch #234 | Optimizing
2021-06-04 13:50:57 | [train_policy] epoch #234 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:57 | [train_policy] epoch #234 | computing loss before
2021-06-04 13:50:57 | [train_policy] epoch #234 | computing gradient
2021-06-04 13:50:57 | [train_policy] epoch #234 | gradient computed
2021-06-04 13:50:57 | [train_policy] epoch #234 | computing descent direction
2021-06-04 13:50:57 | [train_policy] epoch #234 | descent direction computed
2021-06-04 13:50:57 | [train_policy] epoch #234 | backtrack iters: 0
2021-06-04 13:50:57 | [train_policy] epoch #234 | optimization finished
2021-06-04 13:50:57 | [train_policy] epoch #234 | Computing KL after
2021-06-04 13:50:57 | [train_policy] epoch #234 | Computing loss after
2021-06-04 13:50:57 | [train_policy] epoch #234 | Fitting baseline...
2021-06-04 13:50:57 | [train_policy] epoch #234 | Saving snapshot...
2021-06-04 13:50:58 | [train_policy] epoch #234 | Saved
2021-06-04 13:50:58 | [train_policy] epoch #234 | Time 189.73 s
2021-06-04 13:50:58 | [train_policy] epoch #234 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285046
Evaluation/AverageDiscountedReturn          -45.0285
Evaluation/AverageReturn                    -45.0285
Evaluation/CompletionRate                     0
Evaluation/Iteration                        234
Evaluation/MaxReturn                        -31.526
Evaluation/MinReturn                        -64.5476
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.86281
Extras/EpisodeRewardMean                    -44.5123
LinearFeatureBaseline/ExplainedVariance       0.917402
PolicyExecTime                                0.226726
ProcessExecTime                               0.0311742
TotalEnvSteps                            237820
policy/Entropy                               -0.203979
policy/KL                                     0.00921195
policy/KLBefore                               0
policy/LossAfter                             -0.0233416
policy/LossBefore                             5.71309e-09
policy/Perplexity                             0.81548
policy/dLoss                                  0.0233416
---------------------------------------  ----------------
2021-06-04 13:50:58 | [train_policy] epoch #235 | Obtaining samples for iteration 235...
2021-06-04 13:50:58 | [train_policy] epoch #235 | Logging diagnostics...
2021-06-04 13:50:58 | [train_policy] epoch #235 | Optimizing policy...
2021-06-04 13:50:58 | [train_policy] epoch #235 | Computing loss before
2021-06-04 13:50:58 | [train_policy] epoch #235 | Computing KL before
2021-06-04 13:50:58 | [train_policy] epoch #235 | Optimizing
2021-06-04 13:50:58 | [train_policy] epoch #235 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:58 | [train_policy] epoch #235 | computing loss before
2021-06-04 13:50:58 | [train_policy] epoch #235 | computing gradient
2021-06-04 13:50:58 | [train_policy] epoch #235 | gradient computed
2021-06-04 13:50:58 | [train_policy] epoch #235 | computing descent direction
2021-06-04 13:50:58 | [train_policy] epoch #235 | descent direction computed
2021-06-04 13:50:58 | [train_policy] epoch #235 | backtrack iters: 1
2021-06-04 13:50:58 | [train_policy] epoch #235 | optimization finished
2021-06-04 13:50:58 | [train_policy] epoch #235 | Computing KL after
2021-06-04 13:50:58 | [train_policy] epoch #235 | Computing loss after
2021-06-04 13:50:58 | [train_policy] epoch #235 | Fitting baseline...
2021-06-04 13:50:58 | [train_policy] epoch #235 | Saving snapshot...
2021-06-04 13:50:58 | [train_policy] epoch #235 | Saved
2021-06-04 13:50:58 | [train_policy] epoch #235 | Time 190.53 s
2021-06-04 13:50:58 | [train_policy] epoch #235 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286098
Evaluation/AverageDiscountedReturn          -64.8878
Evaluation/AverageReturn                    -64.8878
Evaluation/CompletionRate                     0
Evaluation/Iteration                        235
Evaluation/MaxReturn                        -30.2483
Evaluation/MinReturn                      -2063.01
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.567
Extras/EpisodeRewardMean                    -63.0702
LinearFeatureBaseline/ExplainedVariance       0.0116094
PolicyExecTime                                0.228009
ProcessExecTime                               0.031374
TotalEnvSteps                            238832
policy/Entropy                               -0.159309
policy/KL                                     0.00695229
policy/KLBefore                               0
policy/LossAfter                             -0.0152269
policy/LossBefore                             1.22508e-08
policy/Perplexity                             0.852733
policy/dLoss                                  0.0152269
---------------------------------------  ----------------
2021-06-04 13:50:58 | [train_policy] epoch #236 | Obtaining samples for iteration 236...
2021-06-04 13:50:59 | [train_policy] epoch #236 | Logging diagnostics...
2021-06-04 13:50:59 | [train_policy] epoch #236 | Optimizing policy...
2021-06-04 13:50:59 | [train_policy] epoch #236 | Computing loss before
2021-06-04 13:50:59 | [train_policy] epoch #236 | Computing KL before
2021-06-04 13:50:59 | [train_policy] epoch #236 | Optimizing
2021-06-04 13:50:59 | [train_policy] epoch #236 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:50:59 | [train_policy] epoch #236 | computing loss before
2021-06-04 13:50:59 | [train_policy] epoch #236 | computing gradient
2021-06-04 13:50:59 | [train_policy] epoch #236 | gradient computed
2021-06-04 13:50:59 | [train_policy] epoch #236 | computing descent direction
2021-06-04 13:50:59 | [train_policy] epoch #236 | descent direction computed
2021-06-04 13:50:59 | [train_policy] epoch #236 | backtrack iters: 0
2021-06-04 13:50:59 | [train_policy] epoch #236 | optimization finished
2021-06-04 13:50:59 | [train_policy] epoch #236 | Computing KL after
2021-06-04 13:50:59 | [train_policy] epoch #236 | Computing loss after
2021-06-04 13:50:59 | [train_policy] epoch #236 | Fitting baseline...
2021-06-04 13:50:59 | [train_policy] epoch #236 | Saving snapshot...
2021-06-04 13:50:59 | [train_policy] epoch #236 | Saved
2021-06-04 13:50:59 | [train_policy] epoch #236 | Time 191.33 s
2021-06-04 13:50:59 | [train_policy] epoch #236 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283846
Evaluation/AverageDiscountedReturn          -45.362
Evaluation/AverageReturn                    -45.362
Evaluation/CompletionRate                     0
Evaluation/Iteration                        236
Evaluation/MaxReturn                        -32.8777
Evaluation/MinReturn                       -100.924
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.41605
Extras/EpisodeRewardMean                    -45.4457
LinearFeatureBaseline/ExplainedVariance     -27.9008
PolicyExecTime                                0.23395
ProcessExecTime                               0.0311334
TotalEnvSteps                            239844
policy/Entropy                               -0.133387
policy/KL                                     0.00988497
policy/KLBefore                               0
policy/LossAfter                             -0.0319087
policy/LossBefore                            -3.10981e-08
policy/Perplexity                             0.875126
policy/dLoss                                  0.0319087
---------------------------------------  ----------------
2021-06-04 13:50:59 | [train_policy] epoch #237 | Obtaining samples for iteration 237...
2021-06-04 13:51:00 | [train_policy] epoch #237 | Logging diagnostics...
2021-06-04 13:51:00 | [train_policy] epoch #237 | Optimizing policy...
2021-06-04 13:51:00 | [train_policy] epoch #237 | Computing loss before
2021-06-04 13:51:00 | [train_policy] epoch #237 | Computing KL before
2021-06-04 13:51:00 | [train_policy] epoch #237 | Optimizing
2021-06-04 13:51:00 | [train_policy] epoch #237 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:00 | [train_policy] epoch #237 | computing loss before
2021-06-04 13:51:00 | [train_policy] epoch #237 | computing gradient
2021-06-04 13:51:00 | [train_policy] epoch #237 | gradient computed
2021-06-04 13:51:00 | [train_policy] epoch #237 | computing descent direction
2021-06-04 13:51:00 | [train_policy] epoch #237 | descent direction computed
2021-06-04 13:51:00 | [train_policy] epoch #237 | backtrack iters: 0
2021-06-04 13:51:00 | [train_policy] epoch #237 | optimization finished
2021-06-04 13:51:00 | [train_policy] epoch #237 | Computing KL after
2021-06-04 13:51:00 | [train_policy] epoch #237 | Computing loss after
2021-06-04 13:51:00 | [train_policy] epoch #237 | Fitting baseline...
2021-06-04 13:51:00 | [train_policy] epoch #237 | Saving snapshot...
2021-06-04 13:51:00 | [train_policy] epoch #237 | Saved
2021-06-04 13:51:00 | [train_policy] epoch #237 | Time 192.12 s
2021-06-04 13:51:00 | [train_policy] epoch #237 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.290924
Evaluation/AverageDiscountedReturn          -43.8956
Evaluation/AverageReturn                    -43.8956
Evaluation/CompletionRate                     0
Evaluation/Iteration                        237
Evaluation/MaxReturn                        -29.9658
Evaluation/MinReturn                       -107.959
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.94191
Extras/EpisodeRewardMean                    -43.8526
LinearFeatureBaseline/ExplainedVariance       0.817634
PolicyExecTime                                0.225894
ProcessExecTime                               0.0318682
TotalEnvSteps                            240856
policy/Entropy                               -0.0925145
policy/KL                                     0.00961128
policy/KLBefore                               0
policy/LossAfter                             -0.0201375
policy/LossBefore                            -1.20152e-08
policy/Perplexity                             0.911636
policy/dLoss                                  0.0201375
---------------------------------------  ----------------
2021-06-04 13:51:00 | [train_policy] epoch #238 | Obtaining samples for iteration 238...
2021-06-04 13:51:01 | [train_policy] epoch #238 | Logging diagnostics...
2021-06-04 13:51:01 | [train_policy] epoch #238 | Optimizing policy...
2021-06-04 13:51:01 | [train_policy] epoch #238 | Computing loss before
2021-06-04 13:51:01 | [train_policy] epoch #238 | Computing KL before
2021-06-04 13:51:01 | [train_policy] epoch #238 | Optimizing
2021-06-04 13:51:01 | [train_policy] epoch #238 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:01 | [train_policy] epoch #238 | computing loss before
2021-06-04 13:51:01 | [train_policy] epoch #238 | computing gradient
2021-06-04 13:51:01 | [train_policy] epoch #238 | gradient computed
2021-06-04 13:51:01 | [train_policy] epoch #238 | computing descent direction
2021-06-04 13:51:01 | [train_policy] epoch #238 | descent direction computed
2021-06-04 13:51:01 | [train_policy] epoch #238 | backtrack iters: 1
2021-06-04 13:51:01 | [train_policy] epoch #238 | optimization finished
2021-06-04 13:51:01 | [train_policy] epoch #238 | Computing KL after
2021-06-04 13:51:01 | [train_policy] epoch #238 | Computing loss after
2021-06-04 13:51:01 | [train_policy] epoch #238 | Fitting baseline...
2021-06-04 13:51:01 | [train_policy] epoch #238 | Saving snapshot...
2021-06-04 13:51:01 | [train_policy] epoch #238 | Saved
2021-06-04 13:51:01 | [train_policy] epoch #238 | Time 192.91 s
2021-06-04 13:51:01 | [train_policy] epoch #238 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287265
Evaluation/AverageDiscountedReturn          -45.0487
Evaluation/AverageReturn                    -45.0487
Evaluation/CompletionRate                     0
Evaluation/Iteration                        238
Evaluation/MaxReturn                        -29.534
Evaluation/MinReturn                        -99.4126
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.58664
Extras/EpisodeRewardMean                    -44.9055
LinearFeatureBaseline/ExplainedVariance       0.852784
PolicyExecTime                                0.225869
ProcessExecTime                               0.0315423
TotalEnvSteps                            241868
policy/Entropy                               -0.148816
policy/KL                                     0.00667866
policy/KLBefore                               0
policy/LossAfter                             -0.0122769
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.861728
policy/dLoss                                  0.0122769
---------------------------------------  ----------------
2021-06-04 13:51:01 | [train_policy] epoch #239 | Obtaining samples for iteration 239...
2021-06-04 13:51:01 | [train_policy] epoch #239 | Logging diagnostics...
2021-06-04 13:51:01 | [train_policy] epoch #239 | Optimizing policy...
2021-06-04 13:51:01 | [train_policy] epoch #239 | Computing loss before
2021-06-04 13:51:01 | [train_policy] epoch #239 | Computing KL before
2021-06-04 13:51:01 | [train_policy] epoch #239 | Optimizing
2021-06-04 13:51:01 | [train_policy] epoch #239 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:01 | [train_policy] epoch #239 | computing loss before
2021-06-04 13:51:01 | [train_policy] epoch #239 | computing gradient
2021-06-04 13:51:01 | [train_policy] epoch #239 | gradient computed
2021-06-04 13:51:01 | [train_policy] epoch #239 | computing descent direction
2021-06-04 13:51:01 | [train_policy] epoch #239 | descent direction computed
2021-06-04 13:51:01 | [train_policy] epoch #239 | backtrack iters: 1
2021-06-04 13:51:01 | [train_policy] epoch #239 | optimization finished
2021-06-04 13:51:01 | [train_policy] epoch #239 | Computing KL after
2021-06-04 13:51:01 | [train_policy] epoch #239 | Computing loss after
2021-06-04 13:51:01 | [train_policy] epoch #239 | Fitting baseline...
2021-06-04 13:51:01 | [train_policy] epoch #239 | Saving snapshot...
2021-06-04 13:51:01 | [train_policy] epoch #239 | Saved
2021-06-04 13:51:01 | [train_policy] epoch #239 | Time 193.71 s
2021-06-04 13:51:01 | [train_policy] epoch #239 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.291849
Evaluation/AverageDiscountedReturn          -42.8045
Evaluation/AverageReturn                    -42.8045
Evaluation/CompletionRate                     0
Evaluation/Iteration                        239
Evaluation/MaxReturn                        -30.9915
Evaluation/MinReturn                        -63.7406
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.73091
Extras/EpisodeRewardMean                    -42.7415
LinearFeatureBaseline/ExplainedVariance       0.927434
PolicyExecTime                                0.219309
ProcessExecTime                               0.0321288
TotalEnvSteps                            242880
policy/Entropy                               -0.191291
policy/KL                                     0.00662438
policy/KLBefore                               0
policy/LossAfter                             -0.0165285
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.825892
policy/dLoss                                  0.0165284
---------------------------------------  ----------------
2021-06-04 13:51:02 | [train_policy] epoch #240 | Obtaining samples for iteration 240...
2021-06-04 13:51:02 | [train_policy] epoch #240 | Logging diagnostics...
2021-06-04 13:51:02 | [train_policy] epoch #240 | Optimizing policy...
2021-06-04 13:51:02 | [train_policy] epoch #240 | Computing loss before
2021-06-04 13:51:02 | [train_policy] epoch #240 | Computing KL before
2021-06-04 13:51:02 | [train_policy] epoch #240 | Optimizing
2021-06-04 13:51:02 | [train_policy] epoch #240 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:02 | [train_policy] epoch #240 | computing loss before
2021-06-04 13:51:02 | [train_policy] epoch #240 | computing gradient
2021-06-04 13:51:02 | [train_policy] epoch #240 | gradient computed
2021-06-04 13:51:02 | [train_policy] epoch #240 | computing descent direction
2021-06-04 13:51:02 | [train_policy] epoch #240 | descent direction computed
2021-06-04 13:51:02 | [train_policy] epoch #240 | backtrack iters: 0
2021-06-04 13:51:02 | [train_policy] epoch #240 | optimization finished
2021-06-04 13:51:02 | [train_policy] epoch #240 | Computing KL after
2021-06-04 13:51:02 | [train_policy] epoch #240 | Computing loss after
2021-06-04 13:51:02 | [train_policy] epoch #240 | Fitting baseline...
2021-06-04 13:51:02 | [train_policy] epoch #240 | Saving snapshot...
2021-06-04 13:51:02 | [train_policy] epoch #240 | Saved
2021-06-04 13:51:02 | [train_policy] epoch #240 | Time 194.49 s
2021-06-04 13:51:02 | [train_policy] epoch #240 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284103
Evaluation/AverageDiscountedReturn          -42.9272
Evaluation/AverageReturn                    -42.9272
Evaluation/CompletionRate                     0
Evaluation/Iteration                        240
Evaluation/MaxReturn                        -30.3951
Evaluation/MinReturn                        -61.0412
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.51098
Extras/EpisodeRewardMean                    -42.8514
LinearFeatureBaseline/ExplainedVariance       0.903376
PolicyExecTime                                0.217805
ProcessExecTime                               0.0309448
TotalEnvSteps                            243892
policy/Entropy                               -0.161136
policy/KL                                     0.00954373
policy/KLBefore                               0
policy/LossAfter                             -0.0232164
policy/LossBefore                            -3.15693e-08
policy/Perplexity                             0.851177
policy/dLoss                                  0.0232164
---------------------------------------  ----------------
2021-06-04 13:51:02 | [train_policy] epoch #241 | Obtaining samples for iteration 241...
2021-06-04 13:51:03 | [train_policy] epoch #241 | Logging diagnostics...
2021-06-04 13:51:03 | [train_policy] epoch #241 | Optimizing policy...
2021-06-04 13:51:03 | [train_policy] epoch #241 | Computing loss before
2021-06-04 13:51:03 | [train_policy] epoch #241 | Computing KL before
2021-06-04 13:51:03 | [train_policy] epoch #241 | Optimizing
2021-06-04 13:51:03 | [train_policy] epoch #241 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:03 | [train_policy] epoch #241 | computing loss before
2021-06-04 13:51:03 | [train_policy] epoch #241 | computing gradient
2021-06-04 13:51:03 | [train_policy] epoch #241 | gradient computed
2021-06-04 13:51:03 | [train_policy] epoch #241 | computing descent direction
2021-06-04 13:51:03 | [train_policy] epoch #241 | descent direction computed
2021-06-04 13:51:03 | [train_policy] epoch #241 | backtrack iters: 0
2021-06-04 13:51:03 | [train_policy] epoch #241 | optimization finished
2021-06-04 13:51:03 | [train_policy] epoch #241 | Computing KL after
2021-06-04 13:51:03 | [train_policy] epoch #241 | Computing loss after
2021-06-04 13:51:03 | [train_policy] epoch #241 | Fitting baseline...
2021-06-04 13:51:03 | [train_policy] epoch #241 | Saving snapshot...
2021-06-04 13:51:03 | [train_policy] epoch #241 | Saved
2021-06-04 13:51:03 | [train_policy] epoch #241 | Time 195.29 s
2021-06-04 13:51:03 | [train_policy] epoch #241 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284836
Evaluation/AverageDiscountedReturn          -45.5008
Evaluation/AverageReturn                    -45.5008
Evaluation/CompletionRate                     0
Evaluation/Iteration                        241
Evaluation/MaxReturn                        -33.0615
Evaluation/MinReturn                       -295.141
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         26.9001
Extras/EpisodeRewardMean                    -45.2215
LinearFeatureBaseline/ExplainedVariance       0.235546
PolicyExecTime                                0.234141
ProcessExecTime                               0.0312216
TotalEnvSteps                            244904
policy/Entropy                               -0.175555
policy/KL                                     0.0097253
policy/KLBefore                               0
policy/LossAfter                             -0.0241793
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.838991
policy/dLoss                                  0.0241793
---------------------------------------  ----------------
2021-06-04 13:51:03 | [train_policy] epoch #242 | Obtaining samples for iteration 242...
2021-06-04 13:51:04 | [train_policy] epoch #242 | Logging diagnostics...
2021-06-04 13:51:04 | [train_policy] epoch #242 | Optimizing policy...
2021-06-04 13:51:04 | [train_policy] epoch #242 | Computing loss before
2021-06-04 13:51:04 | [train_policy] epoch #242 | Computing KL before
2021-06-04 13:51:04 | [train_policy] epoch #242 | Optimizing
2021-06-04 13:51:04 | [train_policy] epoch #242 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:04 | [train_policy] epoch #242 | computing loss before
2021-06-04 13:51:04 | [train_policy] epoch #242 | computing gradient
2021-06-04 13:51:04 | [train_policy] epoch #242 | gradient computed
2021-06-04 13:51:04 | [train_policy] epoch #242 | computing descent direction
2021-06-04 13:51:04 | [train_policy] epoch #242 | descent direction computed
2021-06-04 13:51:04 | [train_policy] epoch #242 | backtrack iters: 1
2021-06-04 13:51:04 | [train_policy] epoch #242 | optimization finished
2021-06-04 13:51:04 | [train_policy] epoch #242 | Computing KL after
2021-06-04 13:51:04 | [train_policy] epoch #242 | Computing loss after
2021-06-04 13:51:04 | [train_policy] epoch #242 | Fitting baseline...
2021-06-04 13:51:04 | [train_policy] epoch #242 | Saving snapshot...
2021-06-04 13:51:04 | [train_policy] epoch #242 | Saved
2021-06-04 13:51:04 | [train_policy] epoch #242 | Time 196.09 s
2021-06-04 13:51:04 | [train_policy] epoch #242 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285168
Evaluation/AverageDiscountedReturn          -43.5063
Evaluation/AverageReturn                    -43.5063
Evaluation/CompletionRate                     0
Evaluation/Iteration                        242
Evaluation/MaxReturn                        -33.2553
Evaluation/MinReturn                        -60.7081
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.54713
Extras/EpisodeRewardMean                    -43.3196
LinearFeatureBaseline/ExplainedVariance       0.0693942
PolicyExecTime                                0.213844
ProcessExecTime                               0.0312221
TotalEnvSteps                            245916
policy/Entropy                               -0.183982
policy/KL                                     0.006659
policy/KLBefore                               0
policy/LossAfter                             -0.0191888
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.831951
policy/dLoss                                  0.0191888
---------------------------------------  ----------------
2021-06-04 13:51:04 | [train_policy] epoch #243 | Obtaining samples for iteration 243...
2021-06-04 13:51:05 | [train_policy] epoch #243 | Logging diagnostics...
2021-06-04 13:51:05 | [train_policy] epoch #243 | Optimizing policy...
2021-06-04 13:51:05 | [train_policy] epoch #243 | Computing loss before
2021-06-04 13:51:05 | [train_policy] epoch #243 | Computing KL before
2021-06-04 13:51:05 | [train_policy] epoch #243 | Optimizing
2021-06-04 13:51:05 | [train_policy] epoch #243 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:05 | [train_policy] epoch #243 | computing loss before
2021-06-04 13:51:05 | [train_policy] epoch #243 | computing gradient
2021-06-04 13:51:05 | [train_policy] epoch #243 | gradient computed
2021-06-04 13:51:05 | [train_policy] epoch #243 | computing descent direction
2021-06-04 13:51:05 | [train_policy] epoch #243 | descent direction computed
2021-06-04 13:51:05 | [train_policy] epoch #243 | backtrack iters: 0
2021-06-04 13:51:05 | [train_policy] epoch #243 | optimization finished
2021-06-04 13:51:05 | [train_policy] epoch #243 | Computing KL after
2021-06-04 13:51:05 | [train_policy] epoch #243 | Computing loss after
2021-06-04 13:51:05 | [train_policy] epoch #243 | Fitting baseline...
2021-06-04 13:51:05 | [train_policy] epoch #243 | Saving snapshot...
2021-06-04 13:51:05 | [train_policy] epoch #243 | Saved
2021-06-04 13:51:05 | [train_policy] epoch #243 | Time 196.89 s
2021-06-04 13:51:05 | [train_policy] epoch #243 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288569
Evaluation/AverageDiscountedReturn          -43.6992
Evaluation/AverageReturn                    -43.6992
Evaluation/CompletionRate                     0
Evaluation/Iteration                        243
Evaluation/MaxReturn                        -32.6699
Evaluation/MinReturn                        -63.7808
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.42939
Extras/EpisodeRewardMean                    -43.5963
LinearFeatureBaseline/ExplainedVariance       0.920388
PolicyExecTime                                0.232599
ProcessExecTime                               0.0316904
TotalEnvSteps                            246928
policy/Entropy                               -0.187978
policy/KL                                     0.00970445
policy/KLBefore                               0
policy/LossAfter                             -0.0113667
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.828633
policy/dLoss                                  0.0113667
---------------------------------------  ----------------
2021-06-04 13:51:05 | [train_policy] epoch #244 | Obtaining samples for iteration 244...
2021-06-04 13:51:05 | [train_policy] epoch #244 | Logging diagnostics...
2021-06-04 13:51:05 | [train_policy] epoch #244 | Optimizing policy...
2021-06-04 13:51:05 | [train_policy] epoch #244 | Computing loss before
2021-06-04 13:51:05 | [train_policy] epoch #244 | Computing KL before
2021-06-04 13:51:05 | [train_policy] epoch #244 | Optimizing
2021-06-04 13:51:05 | [train_policy] epoch #244 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:05 | [train_policy] epoch #244 | computing loss before
2021-06-04 13:51:05 | [train_policy] epoch #244 | computing gradient
2021-06-04 13:51:05 | [train_policy] epoch #244 | gradient computed
2021-06-04 13:51:05 | [train_policy] epoch #244 | computing descent direction
2021-06-04 13:51:05 | [train_policy] epoch #244 | descent direction computed
2021-06-04 13:51:05 | [train_policy] epoch #244 | backtrack iters: 1
2021-06-04 13:51:05 | [train_policy] epoch #244 | optimization finished
2021-06-04 13:51:05 | [train_policy] epoch #244 | Computing KL after
2021-06-04 13:51:05 | [train_policy] epoch #244 | Computing loss after
2021-06-04 13:51:05 | [train_policy] epoch #244 | Fitting baseline...
2021-06-04 13:51:05 | [train_policy] epoch #244 | Saving snapshot...
2021-06-04 13:51:05 | [train_policy] epoch #244 | Saved
2021-06-04 13:51:05 | [train_policy] epoch #244 | Time 197.69 s
2021-06-04 13:51:05 | [train_policy] epoch #244 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285641
Evaluation/AverageDiscountedReturn          -43.6343
Evaluation/AverageReturn                    -43.6343
Evaluation/CompletionRate                     0
Evaluation/Iteration                        244
Evaluation/MaxReturn                        -33.7386
Evaluation/MinReturn                        -67.1269
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.03279
Extras/EpisodeRewardMean                    -43.4852
LinearFeatureBaseline/ExplainedVariance       0.920086
PolicyExecTime                                0.22456
ProcessExecTime                               0.0312495
TotalEnvSteps                            247940
policy/Entropy                               -0.208574
policy/KL                                     0.00663429
policy/KLBefore                               0
policy/LossAfter                             -0.0217649
policy/LossBefore                            -1.46067e-08
policy/Perplexity                             0.811741
policy/dLoss                                  0.0217649
---------------------------------------  ----------------
2021-06-04 13:51:05 | [train_policy] epoch #245 | Obtaining samples for iteration 245...
2021-06-04 13:51:06 | [train_policy] epoch #245 | Logging diagnostics...
2021-06-04 13:51:06 | [train_policy] epoch #245 | Optimizing policy...
2021-06-04 13:51:06 | [train_policy] epoch #245 | Computing loss before
2021-06-04 13:51:06 | [train_policy] epoch #245 | Computing KL before
2021-06-04 13:51:06 | [train_policy] epoch #245 | Optimizing
2021-06-04 13:51:06 | [train_policy] epoch #245 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:06 | [train_policy] epoch #245 | computing loss before
2021-06-04 13:51:06 | [train_policy] epoch #245 | computing gradient
2021-06-04 13:51:06 | [train_policy] epoch #245 | gradient computed
2021-06-04 13:51:06 | [train_policy] epoch #245 | computing descent direction
2021-06-04 13:51:06 | [train_policy] epoch #245 | descent direction computed
2021-06-04 13:51:06 | [train_policy] epoch #245 | backtrack iters: 1
2021-06-04 13:51:06 | [train_policy] epoch #245 | optimization finished
2021-06-04 13:51:06 | [train_policy] epoch #245 | Computing KL after
2021-06-04 13:51:06 | [train_policy] epoch #245 | Computing loss after
2021-06-04 13:51:06 | [train_policy] epoch #245 | Fitting baseline...
2021-06-04 13:51:06 | [train_policy] epoch #245 | Saving snapshot...
2021-06-04 13:51:06 | [train_policy] epoch #245 | Saved
2021-06-04 13:51:06 | [train_policy] epoch #245 | Time 198.49 s
2021-06-04 13:51:06 | [train_policy] epoch #245 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285727
Evaluation/AverageDiscountedReturn          -65.1282
Evaluation/AverageReturn                    -65.1282
Evaluation/CompletionRate                     0
Evaluation/Iteration                        245
Evaluation/MaxReturn                        -32.4776
Evaluation/MinReturn                      -2055.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.762
Extras/EpisodeRewardMean                    -63.3666
LinearFeatureBaseline/ExplainedVariance       0.00986895
PolicyExecTime                                0.229147
ProcessExecTime                               0.0311821
TotalEnvSteps                            248952
policy/Entropy                               -0.221718
policy/KL                                     0.00656326
policy/KLBefore                               0
policy/LossAfter                             -0.0270225
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.801141
policy/dLoss                                  0.0270225
---------------------------------------  ----------------
2021-06-04 13:51:06 | [train_policy] epoch #246 | Obtaining samples for iteration 246...
2021-06-04 13:51:07 | [train_policy] epoch #246 | Logging diagnostics...
2021-06-04 13:51:07 | [train_policy] epoch #246 | Optimizing policy...
2021-06-04 13:51:07 | [train_policy] epoch #246 | Computing loss before
2021-06-04 13:51:07 | [train_policy] epoch #246 | Computing KL before
2021-06-04 13:51:07 | [train_policy] epoch #246 | Optimizing
2021-06-04 13:51:07 | [train_policy] epoch #246 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:07 | [train_policy] epoch #246 | computing loss before
2021-06-04 13:51:07 | [train_policy] epoch #246 | computing gradient
2021-06-04 13:51:07 | [train_policy] epoch #246 | gradient computed
2021-06-04 13:51:07 | [train_policy] epoch #246 | computing descent direction
2021-06-04 13:51:07 | [train_policy] epoch #246 | descent direction computed
2021-06-04 13:51:07 | [train_policy] epoch #246 | backtrack iters: 1
2021-06-04 13:51:07 | [train_policy] epoch #246 | optimization finished
2021-06-04 13:51:07 | [train_policy] epoch #246 | Computing KL after
2021-06-04 13:51:07 | [train_policy] epoch #246 | Computing loss after
2021-06-04 13:51:07 | [train_policy] epoch #246 | Fitting baseline...
2021-06-04 13:51:07 | [train_policy] epoch #246 | Saving snapshot...
2021-06-04 13:51:07 | [train_policy] epoch #246 | Saved
2021-06-04 13:51:07 | [train_policy] epoch #246 | Time 199.31 s
2021-06-04 13:51:07 | [train_policy] epoch #246 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.28625
Evaluation/AverageDiscountedReturn          -66.0845
Evaluation/AverageReturn                    -66.0845
Evaluation/CompletionRate                     0
Evaluation/Iteration                        246
Evaluation/MaxReturn                        -33.3634
Evaluation/MinReturn                      -2062.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.478
Extras/EpisodeRewardMean                    -63.9339
LinearFeatureBaseline/ExplainedVariance       0.0964003
PolicyExecTime                                0.23866
ProcessExecTime                               0.031472
TotalEnvSteps                            249964
policy/Entropy                               -0.285719
policy/KL                                     0.00684036
policy/KLBefore                               0
policy/LossAfter                             -0.0178691
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.751474
policy/dLoss                                  0.0178691
---------------------------------------  ----------------
2021-06-04 13:51:07 | [train_policy] epoch #247 | Obtaining samples for iteration 247...
2021-06-04 13:51:08 | [train_policy] epoch #247 | Logging diagnostics...
2021-06-04 13:51:08 | [train_policy] epoch #247 | Optimizing policy...
2021-06-04 13:51:08 | [train_policy] epoch #247 | Computing loss before
2021-06-04 13:51:08 | [train_policy] epoch #247 | Computing KL before
2021-06-04 13:51:08 | [train_policy] epoch #247 | Optimizing
2021-06-04 13:51:08 | [train_policy] epoch #247 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:08 | [train_policy] epoch #247 | computing loss before
2021-06-04 13:51:08 | [train_policy] epoch #247 | computing gradient
2021-06-04 13:51:08 | [train_policy] epoch #247 | gradient computed
2021-06-04 13:51:08 | [train_policy] epoch #247 | computing descent direction
2021-06-04 13:51:08 | [train_policy] epoch #247 | descent direction computed
2021-06-04 13:51:08 | [train_policy] epoch #247 | backtrack iters: 1
2021-06-04 13:51:08 | [train_policy] epoch #247 | optimization finished
2021-06-04 13:51:08 | [train_policy] epoch #247 | Computing KL after
2021-06-04 13:51:08 | [train_policy] epoch #247 | Computing loss after
2021-06-04 13:51:08 | [train_policy] epoch #247 | Fitting baseline...
2021-06-04 13:51:08 | [train_policy] epoch #247 | Saving snapshot...
2021-06-04 13:51:08 | [train_policy] epoch #247 | Saved
2021-06-04 13:51:08 | [train_policy] epoch #247 | Time 200.11 s
2021-06-04 13:51:08 | [train_policy] epoch #247 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284873
Evaluation/AverageDiscountedReturn          -45.0236
Evaluation/AverageReturn                    -45.0236
Evaluation/CompletionRate                     0
Evaluation/Iteration                        247
Evaluation/MaxReturn                        -34.3678
Evaluation/MinReturn                        -90.0803
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.95634
Extras/EpisodeRewardMean                    -45.2143
LinearFeatureBaseline/ExplainedVariance     -77.8873
PolicyExecTime                                0.224453
ProcessExecTime                               0.0312316
TotalEnvSteps                            250976
policy/Entropy                               -0.315936
policy/KL                                     0.00651408
policy/KLBefore                               0
policy/LossAfter                             -0.0197444
policy/LossBefore                            -1.27219e-08
policy/Perplexity                             0.729106
policy/dLoss                                  0.0197444
---------------------------------------  ----------------
2021-06-04 13:51:08 | [train_policy] epoch #248 | Obtaining samples for iteration 248...
2021-06-04 13:51:09 | [train_policy] epoch #248 | Logging diagnostics...
2021-06-04 13:51:09 | [train_policy] epoch #248 | Optimizing policy...
2021-06-04 13:51:09 | [train_policy] epoch #248 | Computing loss before
2021-06-04 13:51:09 | [train_policy] epoch #248 | Computing KL before
2021-06-04 13:51:09 | [train_policy] epoch #248 | Optimizing
2021-06-04 13:51:09 | [train_policy] epoch #248 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:09 | [train_policy] epoch #248 | computing loss before
2021-06-04 13:51:09 | [train_policy] epoch #248 | computing gradient
2021-06-04 13:51:09 | [train_policy] epoch #248 | gradient computed
2021-06-04 13:51:09 | [train_policy] epoch #248 | computing descent direction
2021-06-04 13:51:09 | [train_policy] epoch #248 | descent direction computed
2021-06-04 13:51:09 | [train_policy] epoch #248 | backtrack iters: 0
2021-06-04 13:51:09 | [train_policy] epoch #248 | optimization finished
2021-06-04 13:51:09 | [train_policy] epoch #248 | Computing KL after
2021-06-04 13:51:09 | [train_policy] epoch #248 | Computing loss after
2021-06-04 13:51:09 | [train_policy] epoch #248 | Fitting baseline...
2021-06-04 13:51:09 | [train_policy] epoch #248 | Saving snapshot...
2021-06-04 13:51:09 | [train_policy] epoch #248 | Saved
2021-06-04 13:51:09 | [train_policy] epoch #248 | Time 200.89 s
2021-06-04 13:51:09 | [train_policy] epoch #248 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285397
Evaluation/AverageDiscountedReturn          -43.3046
Evaluation/AverageReturn                    -43.3046
Evaluation/CompletionRate                     0
Evaluation/Iteration                        248
Evaluation/MaxReturn                        -30.841
Evaluation/MinReturn                        -66.3517
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.09173
Extras/EpisodeRewardMean                    -43.3352
LinearFeatureBaseline/ExplainedVariance       0.908828
PolicyExecTime                                0.222511
ProcessExecTime                               0.0312328
TotalEnvSteps                            251988
policy/Entropy                               -0.376742
policy/KL                                     0.0096854
policy/KLBefore                               0
policy/LossAfter                             -0.0225464
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.686093
policy/dLoss                                  0.0225464
---------------------------------------  ----------------
2021-06-04 13:51:09 | [train_policy] epoch #249 | Obtaining samples for iteration 249...
2021-06-04 13:51:09 | [train_policy] epoch #249 | Logging diagnostics...
2021-06-04 13:51:09 | [train_policy] epoch #249 | Optimizing policy...
2021-06-04 13:51:09 | [train_policy] epoch #249 | Computing loss before
2021-06-04 13:51:09 | [train_policy] epoch #249 | Computing KL before
2021-06-04 13:51:09 | [train_policy] epoch #249 | Optimizing
2021-06-04 13:51:09 | [train_policy] epoch #249 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:09 | [train_policy] epoch #249 | computing loss before
2021-06-04 13:51:09 | [train_policy] epoch #249 | computing gradient
2021-06-04 13:51:09 | [train_policy] epoch #249 | gradient computed
2021-06-04 13:51:09 | [train_policy] epoch #249 | computing descent direction
2021-06-04 13:51:09 | [train_policy] epoch #249 | descent direction computed
2021-06-04 13:51:09 | [train_policy] epoch #249 | backtrack iters: 1
2021-06-04 13:51:09 | [train_policy] epoch #249 | optimization finished
2021-06-04 13:51:09 | [train_policy] epoch #249 | Computing KL after
2021-06-04 13:51:09 | [train_policy] epoch #249 | Computing loss after
2021-06-04 13:51:09 | [train_policy] epoch #249 | Fitting baseline...
2021-06-04 13:51:09 | [train_policy] epoch #249 | Saving snapshot...
2021-06-04 13:51:09 | [train_policy] epoch #249 | Saved
2021-06-04 13:51:09 | [train_policy] epoch #249 | Time 201.70 s
2021-06-04 13:51:09 | [train_policy] epoch #249 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283968
Evaluation/AverageDiscountedReturn          -45.8643
Evaluation/AverageReturn                    -45.8643
Evaluation/CompletionRate                     0
Evaluation/Iteration                        249
Evaluation/MaxReturn                        -28.8173
Evaluation/MinReturn                       -152.594
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         16.4621
Extras/EpisodeRewardMean                    -45.4267
LinearFeatureBaseline/ExplainedVariance       0.603037
PolicyExecTime                                0.230028
ProcessExecTime                               0.031126
TotalEnvSteps                            253000
policy/Entropy                               -0.414132
policy/KL                                     0.00674912
policy/KLBefore                               0
policy/LossAfter                             -0.0183762
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.660914
policy/dLoss                                  0.0183762
---------------------------------------  ----------------
2021-06-04 13:51:10 | [train_policy] epoch #250 | Obtaining samples for iteration 250...
2021-06-04 13:51:10 | [train_policy] epoch #250 | Logging diagnostics...
2021-06-04 13:51:10 | [train_policy] epoch #250 | Optimizing policy...
2021-06-04 13:51:10 | [train_policy] epoch #250 | Computing loss before
2021-06-04 13:51:10 | [train_policy] epoch #250 | Computing KL before
2021-06-04 13:51:10 | [train_policy] epoch #250 | Optimizing
2021-06-04 13:51:10 | [train_policy] epoch #250 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:10 | [train_policy] epoch #250 | computing loss before
2021-06-04 13:51:10 | [train_policy] epoch #250 | computing gradient
2021-06-04 13:51:10 | [train_policy] epoch #250 | gradient computed
2021-06-04 13:51:10 | [train_policy] epoch #250 | computing descent direction
2021-06-04 13:51:10 | [train_policy] epoch #250 | descent direction computed
2021-06-04 13:51:10 | [train_policy] epoch #250 | backtrack iters: 0
2021-06-04 13:51:10 | [train_policy] epoch #250 | optimization finished
2021-06-04 13:51:10 | [train_policy] epoch #250 | Computing KL after
2021-06-04 13:51:10 | [train_policy] epoch #250 | Computing loss after
2021-06-04 13:51:10 | [train_policy] epoch #250 | Fitting baseline...
2021-06-04 13:51:10 | [train_policy] epoch #250 | Saving snapshot...
2021-06-04 13:51:10 | [train_policy] epoch #250 | Saved
2021-06-04 13:51:10 | [train_policy] epoch #250 | Time 202.49 s
2021-06-04 13:51:10 | [train_policy] epoch #250 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284775
Evaluation/AverageDiscountedReturn          -65.0311
Evaluation/AverageReturn                    -65.0311
Evaluation/CompletionRate                     0
Evaluation/Iteration                        250
Evaluation/MaxReturn                        -29.0683
Evaluation/MinReturn                      -2062.35
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.503
Extras/EpisodeRewardMean                    -62.9776
LinearFeatureBaseline/ExplainedVariance       0.0197618
PolicyExecTime                                0.227875
ProcessExecTime                               0.0312247
TotalEnvSteps                            254012
policy/Entropy                               -0.431271
policy/KL                                     0.00965178
policy/KLBefore                               0
policy/LossAfter                             -0.0249612
policy/LossBefore                            -2.02609e-08
policy/Perplexity                             0.649683
policy/dLoss                                  0.0249612
---------------------------------------  ----------------
2021-06-04 13:51:10 | [train_policy] epoch #251 | Obtaining samples for iteration 251...
2021-06-04 13:51:11 | [train_policy] epoch #251 | Logging diagnostics...
2021-06-04 13:51:11 | [train_policy] epoch #251 | Optimizing policy...
2021-06-04 13:51:11 | [train_policy] epoch #251 | Computing loss before
2021-06-04 13:51:11 | [train_policy] epoch #251 | Computing KL before
2021-06-04 13:51:11 | [train_policy] epoch #251 | Optimizing
2021-06-04 13:51:11 | [train_policy] epoch #251 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:11 | [train_policy] epoch #251 | computing loss before
2021-06-04 13:51:11 | [train_policy] epoch #251 | computing gradient
2021-06-04 13:51:11 | [train_policy] epoch #251 | gradient computed
2021-06-04 13:51:11 | [train_policy] epoch #251 | computing descent direction
2021-06-04 13:51:11 | [train_policy] epoch #251 | descent direction computed
2021-06-04 13:51:11 | [train_policy] epoch #251 | backtrack iters: 1
2021-06-04 13:51:11 | [train_policy] epoch #251 | optimization finished
2021-06-04 13:51:11 | [train_policy] epoch #251 | Computing KL after
2021-06-04 13:51:11 | [train_policy] epoch #251 | Computing loss after
2021-06-04 13:51:11 | [train_policy] epoch #251 | Fitting baseline...
2021-06-04 13:51:11 | [train_policy] epoch #251 | Saving snapshot...
2021-06-04 13:51:11 | [train_policy] epoch #251 | Saved
2021-06-04 13:51:11 | [train_policy] epoch #251 | Time 203.30 s
2021-06-04 13:51:11 | [train_policy] epoch #251 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.289722
Evaluation/AverageDiscountedReturn          -42.9933
Evaluation/AverageReturn                    -42.9933
Evaluation/CompletionRate                     0
Evaluation/Iteration                        251
Evaluation/MaxReturn                        -28.6332
Evaluation/MinReturn                        -63.8847
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.75372
Extras/EpisodeRewardMean                    -43.0667
LinearFeatureBaseline/ExplainedVariance     -22.8026
PolicyExecTime                                0.231566
ProcessExecTime                               0.03162
TotalEnvSteps                            255024
policy/Entropy                               -0.402335
policy/KL                                     0.00641446
policy/KLBefore                               0
policy/LossAfter                             -0.0179181
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.668756
policy/dLoss                                  0.0179181
---------------------------------------  ---------------
2021-06-04 13:51:11 | [train_policy] epoch #252 | Obtaining samples for iteration 252...
2021-06-04 13:51:12 | [train_policy] epoch #252 | Logging diagnostics...
2021-06-04 13:51:12 | [train_policy] epoch #252 | Optimizing policy...
2021-06-04 13:51:12 | [train_policy] epoch #252 | Computing loss before
2021-06-04 13:51:12 | [train_policy] epoch #252 | Computing KL before
2021-06-04 13:51:12 | [train_policy] epoch #252 | Optimizing
2021-06-04 13:51:12 | [train_policy] epoch #252 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:12 | [train_policy] epoch #252 | computing loss before
2021-06-04 13:51:12 | [train_policy] epoch #252 | computing gradient
2021-06-04 13:51:12 | [train_policy] epoch #252 | gradient computed
2021-06-04 13:51:12 | [train_policy] epoch #252 | computing descent direction
2021-06-04 13:51:12 | [train_policy] epoch #252 | descent direction computed
2021-06-04 13:51:12 | [train_policy] epoch #252 | backtrack iters: 1
2021-06-04 13:51:12 | [train_policy] epoch #252 | optimization finished
2021-06-04 13:51:12 | [train_policy] epoch #252 | Computing KL after
2021-06-04 13:51:12 | [train_policy] epoch #252 | Computing loss after
2021-06-04 13:51:12 | [train_policy] epoch #252 | Fitting baseline...
2021-06-04 13:51:12 | [train_policy] epoch #252 | Saving snapshot...
2021-06-04 13:51:12 | [train_policy] epoch #252 | Saved
2021-06-04 13:51:12 | [train_policy] epoch #252 | Time 204.07 s
2021-06-04 13:51:12 | [train_policy] epoch #252 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284569
Evaluation/AverageDiscountedReturn          -87.2121
Evaluation/AverageReturn                    -87.2121
Evaluation/CompletionRate                     0
Evaluation/Iteration                        252
Evaluation/MaxReturn                        -31.1646
Evaluation/MinReturn                      -2062.71
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.542
Extras/EpisodeRewardMean                    -83.6946
LinearFeatureBaseline/ExplainedVariance       0.00995772
PolicyExecTime                                0.209343
ProcessExecTime                               0.0312064
TotalEnvSteps                            256036
policy/Entropy                               -0.374524
policy/KL                                     0.00719268
policy/KLBefore                               0
policy/LossAfter                             -0.0176974
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.687617
policy/dLoss                                  0.0176974
---------------------------------------  ----------------
2021-06-04 13:51:12 | [train_policy] epoch #253 | Obtaining samples for iteration 253...
2021-06-04 13:51:12 | [train_policy] epoch #253 | Logging diagnostics...
2021-06-04 13:51:12 | [train_policy] epoch #253 | Optimizing policy...
2021-06-04 13:51:12 | [train_policy] epoch #253 | Computing loss before
2021-06-04 13:51:12 | [train_policy] epoch #253 | Computing KL before
2021-06-04 13:51:12 | [train_policy] epoch #253 | Optimizing
2021-06-04 13:51:12 | [train_policy] epoch #253 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:12 | [train_policy] epoch #253 | computing loss before
2021-06-04 13:51:12 | [train_policy] epoch #253 | computing gradient
2021-06-04 13:51:12 | [train_policy] epoch #253 | gradient computed
2021-06-04 13:51:12 | [train_policy] epoch #253 | computing descent direction
2021-06-04 13:51:13 | [train_policy] epoch #253 | descent direction computed
2021-06-04 13:51:13 | [train_policy] epoch #253 | backtrack iters: 1
2021-06-04 13:51:13 | [train_policy] epoch #253 | optimization finished
2021-06-04 13:51:13 | [train_policy] epoch #253 | Computing KL after
2021-06-04 13:51:13 | [train_policy] epoch #253 | Computing loss after
2021-06-04 13:51:13 | [train_policy] epoch #253 | Fitting baseline...
2021-06-04 13:51:13 | [train_policy] epoch #253 | Saving snapshot...
2021-06-04 13:51:13 | [train_policy] epoch #253 | Saved
2021-06-04 13:51:13 | [train_policy] epoch #253 | Time 204.84 s
2021-06-04 13:51:13 | [train_policy] epoch #253 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284041
Evaluation/AverageDiscountedReturn          -66.2617
Evaluation/AverageReturn                    -66.2617
Evaluation/CompletionRate                     0
Evaluation/Iteration                        253
Evaluation/MaxReturn                        -29.4485
Evaluation/MinReturn                      -2062.81
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.743
Extras/EpisodeRewardMean                    -64.6592
LinearFeatureBaseline/ExplainedVariance      -0.0379545
PolicyExecTime                                0.21648
ProcessExecTime                               0.0311308
TotalEnvSteps                            257048
policy/Entropy                               -0.354737
policy/KL                                     0.00665558
policy/KLBefore                               0
policy/LossAfter                             -0.0173502
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.701358
policy/dLoss                                  0.0173502
---------------------------------------  ----------------
2021-06-04 13:51:13 | [train_policy] epoch #254 | Obtaining samples for iteration 254...
2021-06-04 13:51:13 | [train_policy] epoch #254 | Logging diagnostics...
2021-06-04 13:51:13 | [train_policy] epoch #254 | Optimizing policy...
2021-06-04 13:51:13 | [train_policy] epoch #254 | Computing loss before
2021-06-04 13:51:13 | [train_policy] epoch #254 | Computing KL before
2021-06-04 13:51:13 | [train_policy] epoch #254 | Optimizing
2021-06-04 13:51:13 | [train_policy] epoch #254 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:13 | [train_policy] epoch #254 | computing loss before
2021-06-04 13:51:13 | [train_policy] epoch #254 | computing gradient
2021-06-04 13:51:13 | [train_policy] epoch #254 | gradient computed
2021-06-04 13:51:13 | [train_policy] epoch #254 | computing descent direction
2021-06-04 13:51:13 | [train_policy] epoch #254 | descent direction computed
2021-06-04 13:51:13 | [train_policy] epoch #254 | backtrack iters: 1
2021-06-04 13:51:13 | [train_policy] epoch #254 | optimization finished
2021-06-04 13:51:13 | [train_policy] epoch #254 | Computing KL after
2021-06-04 13:51:13 | [train_policy] epoch #254 | Computing loss after
2021-06-04 13:51:13 | [train_policy] epoch #254 | Fitting baseline...
2021-06-04 13:51:13 | [train_policy] epoch #254 | Saving snapshot...
2021-06-04 13:51:13 | [train_policy] epoch #254 | Saved
2021-06-04 13:51:13 | [train_policy] epoch #254 | Time 205.62 s
2021-06-04 13:51:13 | [train_policy] epoch #254 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28338
Evaluation/AverageDiscountedReturn          -65.6287
Evaluation/AverageReturn                    -65.6287
Evaluation/CompletionRate                     0
Evaluation/Iteration                        254
Evaluation/MaxReturn                        -29.5709
Evaluation/MinReturn                      -2062.17
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.421
Extras/EpisodeRewardMean                    -63.3982
LinearFeatureBaseline/ExplainedVariance       0.126185
PolicyExecTime                                0.220832
ProcessExecTime                               0.0310149
TotalEnvSteps                            258060
policy/Entropy                               -0.345097
policy/KL                                     0.00673236
policy/KLBefore                               0
policy/LossAfter                             -0.0203153
policy/LossBefore                             7.30334e-09
policy/Perplexity                             0.708152
policy/dLoss                                  0.0203153
---------------------------------------  ----------------
2021-06-04 13:51:13 | [train_policy] epoch #255 | Obtaining samples for iteration 255...
2021-06-04 13:51:14 | [train_policy] epoch #255 | Logging diagnostics...
2021-06-04 13:51:14 | [train_policy] epoch #255 | Optimizing policy...
2021-06-04 13:51:14 | [train_policy] epoch #255 | Computing loss before
2021-06-04 13:51:14 | [train_policy] epoch #255 | Computing KL before
2021-06-04 13:51:14 | [train_policy] epoch #255 | Optimizing
2021-06-04 13:51:14 | [train_policy] epoch #255 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:14 | [train_policy] epoch #255 | computing loss before
2021-06-04 13:51:14 | [train_policy] epoch #255 | computing gradient
2021-06-04 13:51:14 | [train_policy] epoch #255 | gradient computed
2021-06-04 13:51:14 | [train_policy] epoch #255 | computing descent direction
2021-06-04 13:51:14 | [train_policy] epoch #255 | descent direction computed
2021-06-04 13:51:14 | [train_policy] epoch #255 | backtrack iters: 0
2021-06-04 13:51:14 | [train_policy] epoch #255 | optimization finished
2021-06-04 13:51:14 | [train_policy] epoch #255 | Computing KL after
2021-06-04 13:51:14 | [train_policy] epoch #255 | Computing loss after
2021-06-04 13:51:14 | [train_policy] epoch #255 | Fitting baseline...
2021-06-04 13:51:14 | [train_policy] epoch #255 | Saving snapshot...
2021-06-04 13:51:14 | [train_policy] epoch #255 | Saved
2021-06-04 13:51:14 | [train_policy] epoch #255 | Time 206.41 s
2021-06-04 13:51:14 | [train_policy] epoch #255 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283455
Evaluation/AverageDiscountedReturn          -43.5899
Evaluation/AverageReturn                    -43.5899
Evaluation/CompletionRate                     0
Evaluation/Iteration                        255
Evaluation/MaxReturn                        -31.7379
Evaluation/MinReturn                        -70.7946
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.41962
Extras/EpisodeRewardMean                    -43.6183
LinearFeatureBaseline/ExplainedVariance     -30.4847
PolicyExecTime                                0.229856
ProcessExecTime                               0.0311408
TotalEnvSteps                            259072
policy/Entropy                               -0.360616
policy/KL                                     0.00988987
policy/KLBefore                               0
policy/LossAfter                             -0.0267065
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.697247
policy/dLoss                                  0.0267065
---------------------------------------  ----------------
2021-06-04 13:51:14 | [train_policy] epoch #256 | Obtaining samples for iteration 256...
2021-06-04 13:51:15 | [train_policy] epoch #256 | Logging diagnostics...
2021-06-04 13:51:15 | [train_policy] epoch #256 | Optimizing policy...
2021-06-04 13:51:15 | [train_policy] epoch #256 | Computing loss before
2021-06-04 13:51:15 | [train_policy] epoch #256 | Computing KL before
2021-06-04 13:51:15 | [train_policy] epoch #256 | Optimizing
2021-06-04 13:51:15 | [train_policy] epoch #256 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:15 | [train_policy] epoch #256 | computing loss before
2021-06-04 13:51:15 | [train_policy] epoch #256 | computing gradient
2021-06-04 13:51:15 | [train_policy] epoch #256 | gradient computed
2021-06-04 13:51:15 | [train_policy] epoch #256 | computing descent direction
2021-06-04 13:51:15 | [train_policy] epoch #256 | descent direction computed
2021-06-04 13:51:15 | [train_policy] epoch #256 | backtrack iters: 0
2021-06-04 13:51:15 | [train_policy] epoch #256 | optimization finished
2021-06-04 13:51:15 | [train_policy] epoch #256 | Computing KL after
2021-06-04 13:51:15 | [train_policy] epoch #256 | Computing loss after
2021-06-04 13:51:15 | [train_policy] epoch #256 | Fitting baseline...
2021-06-04 13:51:15 | [train_policy] epoch #256 | Saving snapshot...
2021-06-04 13:51:15 | [train_policy] epoch #256 | Saved
2021-06-04 13:51:15 | [train_policy] epoch #256 | Time 207.22 s
2021-06-04 13:51:15 | [train_policy] epoch #256 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284911
Evaluation/AverageDiscountedReturn          -65.4252
Evaluation/AverageReturn                    -65.4252
Evaluation/CompletionRate                     0
Evaluation/Iteration                        256
Evaluation/MaxReturn                        -32.3074
Evaluation/MinReturn                      -2062.03
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.408
Extras/EpisodeRewardMean                    -63.952
LinearFeatureBaseline/ExplainedVariance       0.0121476
PolicyExecTime                                0.231742
ProcessExecTime                               0.0313094
TotalEnvSteps                            260084
policy/Entropy                               -0.277004
policy/KL                                     0.00973924
policy/KLBefore                               0
policy/LossAfter                             -0.0265179
policy/LossBefore                            -1.41355e-08
policy/Perplexity                             0.758051
policy/dLoss                                  0.0265178
---------------------------------------  ----------------
2021-06-04 13:51:15 | [train_policy] epoch #257 | Obtaining samples for iteration 257...
2021-06-04 13:51:16 | [train_policy] epoch #257 | Logging diagnostics...
2021-06-04 13:51:16 | [train_policy] epoch #257 | Optimizing policy...
2021-06-04 13:51:16 | [train_policy] epoch #257 | Computing loss before
2021-06-04 13:51:16 | [train_policy] epoch #257 | Computing KL before
2021-06-04 13:51:16 | [train_policy] epoch #257 | Optimizing
2021-06-04 13:51:16 | [train_policy] epoch #257 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:16 | [train_policy] epoch #257 | computing loss before
2021-06-04 13:51:16 | [train_policy] epoch #257 | computing gradient
2021-06-04 13:51:16 | [train_policy] epoch #257 | gradient computed
2021-06-04 13:51:16 | [train_policy] epoch #257 | computing descent direction
2021-06-04 13:51:16 | [train_policy] epoch #257 | descent direction computed
2021-06-04 13:51:16 | [train_policy] epoch #257 | backtrack iters: 1
2021-06-04 13:51:16 | [train_policy] epoch #257 | optimization finished
2021-06-04 13:51:16 | [train_policy] epoch #257 | Computing KL after
2021-06-04 13:51:16 | [train_policy] epoch #257 | Computing loss after
2021-06-04 13:51:16 | [train_policy] epoch #257 | Fitting baseline...
2021-06-04 13:51:16 | [train_policy] epoch #257 | Saving snapshot...
2021-06-04 13:51:16 | [train_policy] epoch #257 | Saved
2021-06-04 13:51:16 | [train_policy] epoch #257 | Time 208.02 s
2021-06-04 13:51:16 | [train_policy] epoch #257 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287497
Evaluation/AverageDiscountedReturn          -47.7988
Evaluation/AverageReturn                    -47.7988
Evaluation/CompletionRate                     0
Evaluation/Iteration                        257
Evaluation/MaxReturn                        -29.0938
Evaluation/MinReturn                       -349.435
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         32.4567
Extras/EpisodeRewardMean                    -47.3714
LinearFeatureBaseline/ExplainedVariance      -4.68471
PolicyExecTime                                0.229228
ProcessExecTime                               0.0315115
TotalEnvSteps                            261096
policy/Entropy                               -0.306286
policy/KL                                     0.00687434
policy/KLBefore                               0
policy/LossAfter                             -0.0164798
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.736176
policy/dLoss                                  0.0164799
---------------------------------------  ----------------
2021-06-04 13:51:16 | [train_policy] epoch #258 | Obtaining samples for iteration 258...
2021-06-04 13:51:16 | [train_policy] epoch #258 | Logging diagnostics...
2021-06-04 13:51:16 | [train_policy] epoch #258 | Optimizing policy...
2021-06-04 13:51:16 | [train_policy] epoch #258 | Computing loss before
2021-06-04 13:51:16 | [train_policy] epoch #258 | Computing KL before
2021-06-04 13:51:16 | [train_policy] epoch #258 | Optimizing
2021-06-04 13:51:16 | [train_policy] epoch #258 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:16 | [train_policy] epoch #258 | computing loss before
2021-06-04 13:51:16 | [train_policy] epoch #258 | computing gradient
2021-06-04 13:51:16 | [train_policy] epoch #258 | gradient computed
2021-06-04 13:51:16 | [train_policy] epoch #258 | computing descent direction
2021-06-04 13:51:17 | [train_policy] epoch #258 | descent direction computed
2021-06-04 13:51:17 | [train_policy] epoch #258 | backtrack iters: 1
2021-06-04 13:51:17 | [train_policy] epoch #258 | optimization finished
2021-06-04 13:51:17 | [train_policy] epoch #258 | Computing KL after
2021-06-04 13:51:17 | [train_policy] epoch #258 | Computing loss after
2021-06-04 13:51:17 | [train_policy] epoch #258 | Fitting baseline...
2021-06-04 13:51:17 | [train_policy] epoch #258 | Saving snapshot...
2021-06-04 13:51:17 | [train_policy] epoch #258 | Saved
2021-06-04 13:51:17 | [train_policy] epoch #258 | Time 208.80 s
2021-06-04 13:51:17 | [train_policy] epoch #258 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285181
Evaluation/AverageDiscountedReturn          -43.0786
Evaluation/AverageReturn                    -43.0786
Evaluation/CompletionRate                     0
Evaluation/Iteration                        258
Evaluation/MaxReturn                        -30.3933
Evaluation/MinReturn                        -66.1715
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.81447
Extras/EpisodeRewardMean                    -46.2992
LinearFeatureBaseline/ExplainedVariance       0.17758
PolicyExecTime                                0.227619
ProcessExecTime                               0.0312479
TotalEnvSteps                            262108
policy/Entropy                               -0.351444
policy/KL                                     0.00646042
policy/KLBefore                               0
policy/LossAfter                             -0.0188318
policy/LossBefore                             1.17796e-09
policy/Perplexity                             0.703671
policy/dLoss                                  0.0188318
---------------------------------------  ----------------
2021-06-04 13:51:17 | [train_policy] epoch #259 | Obtaining samples for iteration 259...
2021-06-04 13:51:17 | [train_policy] epoch #259 | Logging diagnostics...
2021-06-04 13:51:17 | [train_policy] epoch #259 | Optimizing policy...
2021-06-04 13:51:17 | [train_policy] epoch #259 | Computing loss before
2021-06-04 13:51:17 | [train_policy] epoch #259 | Computing KL before
2021-06-04 13:51:17 | [train_policy] epoch #259 | Optimizing
2021-06-04 13:51:17 | [train_policy] epoch #259 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:17 | [train_policy] epoch #259 | computing loss before
2021-06-04 13:51:17 | [train_policy] epoch #259 | computing gradient
2021-06-04 13:51:17 | [train_policy] epoch #259 | gradient computed
2021-06-04 13:51:17 | [train_policy] epoch #259 | computing descent direction
2021-06-04 13:51:17 | [train_policy] epoch #259 | descent direction computed
2021-06-04 13:51:17 | [train_policy] epoch #259 | backtrack iters: 1
2021-06-04 13:51:17 | [train_policy] epoch #259 | optimization finished
2021-06-04 13:51:17 | [train_policy] epoch #259 | Computing KL after
2021-06-04 13:51:17 | [train_policy] epoch #259 | Computing loss after
2021-06-04 13:51:17 | [train_policy] epoch #259 | Fitting baseline...
2021-06-04 13:51:17 | [train_policy] epoch #259 | Saving snapshot...
2021-06-04 13:51:17 | [train_policy] epoch #259 | Saved
2021-06-04 13:51:17 | [train_policy] epoch #259 | Time 209.60 s
2021-06-04 13:51:17 | [train_policy] epoch #259 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284163
Evaluation/AverageDiscountedReturn          -43.1082
Evaluation/AverageReturn                    -43.1082
Evaluation/CompletionRate                     0
Evaluation/Iteration                        259
Evaluation/MaxReturn                        -29.5637
Evaluation/MinReturn                        -66.0728
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.11978
Extras/EpisodeRewardMean                    -42.7381
LinearFeatureBaseline/ExplainedVariance       0.918235
PolicyExecTime                                0.227394
ProcessExecTime                               0.0311205
TotalEnvSteps                            263120
policy/Entropy                               -0.395867
policy/KL                                     0.00666888
policy/KLBefore                               0
policy/LossAfter                             -0.0177338
policy/LossBefore                            -1.53134e-08
policy/Perplexity                             0.673096
policy/dLoss                                  0.0177338
---------------------------------------  ----------------
2021-06-04 13:51:17 | [train_policy] epoch #260 | Obtaining samples for iteration 260...
2021-06-04 13:51:18 | [train_policy] epoch #260 | Logging diagnostics...
2021-06-04 13:51:18 | [train_policy] epoch #260 | Optimizing policy...
2021-06-04 13:51:18 | [train_policy] epoch #260 | Computing loss before
2021-06-04 13:51:18 | [train_policy] epoch #260 | Computing KL before
2021-06-04 13:51:18 | [train_policy] epoch #260 | Optimizing
2021-06-04 13:51:18 | [train_policy] epoch #260 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:18 | [train_policy] epoch #260 | computing loss before
2021-06-04 13:51:18 | [train_policy] epoch #260 | computing gradient
2021-06-04 13:51:18 | [train_policy] epoch #260 | gradient computed
2021-06-04 13:51:18 | [train_policy] epoch #260 | computing descent direction
2021-06-04 13:51:18 | [train_policy] epoch #260 | descent direction computed
2021-06-04 13:51:18 | [train_policy] epoch #260 | backtrack iters: 0
2021-06-04 13:51:18 | [train_policy] epoch #260 | optimization finished
2021-06-04 13:51:18 | [train_policy] epoch #260 | Computing KL after
2021-06-04 13:51:18 | [train_policy] epoch #260 | Computing loss after
2021-06-04 13:51:18 | [train_policy] epoch #260 | Fitting baseline...
2021-06-04 13:51:18 | [train_policy] epoch #260 | Saving snapshot...
2021-06-04 13:51:18 | [train_policy] epoch #260 | Saved
2021-06-04 13:51:18 | [train_policy] epoch #260 | Time 210.39 s
2021-06-04 13:51:18 | [train_policy] epoch #260 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286946
Evaluation/AverageDiscountedReturn          -53.6909
Evaluation/AverageReturn                    -53.6909
Evaluation/CompletionRate                     0
Evaluation/Iteration                        260
Evaluation/MaxReturn                        -29.0311
Evaluation/MinReturn                       -971.197
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         96.5546
Extras/EpisodeRewardMean                    -53.0491
LinearFeatureBaseline/ExplainedVariance       0.0350068
PolicyExecTime                                0.232032
ProcessExecTime                               0.0313787
TotalEnvSteps                            264132
policy/Entropy                               -0.387161
policy/KL                                     0.00953049
policy/KLBefore                               0
policy/LossAfter                             -0.0278414
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.678981
policy/dLoss                                  0.0278414
---------------------------------------  ----------------
2021-06-04 13:51:18 | [train_policy] epoch #261 | Obtaining samples for iteration 261...
2021-06-04 13:51:19 | [train_policy] epoch #261 | Logging diagnostics...
2021-06-04 13:51:19 | [train_policy] epoch #261 | Optimizing policy...
2021-06-04 13:51:19 | [train_policy] epoch #261 | Computing loss before
2021-06-04 13:51:19 | [train_policy] epoch #261 | Computing KL before
2021-06-04 13:51:19 | [train_policy] epoch #261 | Optimizing
2021-06-04 13:51:19 | [train_policy] epoch #261 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:19 | [train_policy] epoch #261 | computing loss before
2021-06-04 13:51:19 | [train_policy] epoch #261 | computing gradient
2021-06-04 13:51:19 | [train_policy] epoch #261 | gradient computed
2021-06-04 13:51:19 | [train_policy] epoch #261 | computing descent direction
2021-06-04 13:51:19 | [train_policy] epoch #261 | descent direction computed
2021-06-04 13:51:19 | [train_policy] epoch #261 | backtrack iters: 0
2021-06-04 13:51:19 | [train_policy] epoch #261 | optimization finished
2021-06-04 13:51:19 | [train_policy] epoch #261 | Computing KL after
2021-06-04 13:51:19 | [train_policy] epoch #261 | Computing loss after
2021-06-04 13:51:19 | [train_policy] epoch #261 | Fitting baseline...
2021-06-04 13:51:19 | [train_policy] epoch #261 | Saving snapshot...
2021-06-04 13:51:19 | [train_policy] epoch #261 | Saved
2021-06-04 13:51:19 | [train_policy] epoch #261 | Time 211.16 s
2021-06-04 13:51:19 | [train_policy] epoch #261 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284985
Evaluation/AverageDiscountedReturn          -65.2537
Evaluation/AverageReturn                    -65.2537
Evaluation/CompletionRate                     0
Evaluation/Iteration                        261
Evaluation/MaxReturn                        -33.0872
Evaluation/MinReturn                      -2061.99
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.419
Extras/EpisodeRewardMean                    -63.4712
LinearFeatureBaseline/ExplainedVariance       0.0833183
PolicyExecTime                                0.211785
ProcessExecTime                               0.0312278
TotalEnvSteps                            265144
policy/Entropy                               -0.385943
policy/KL                                     0.0097631
policy/KLBefore                               0
policy/LossAfter                             -0.0250095
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.679809
policy/dLoss                                  0.0250095
---------------------------------------  ----------------
2021-06-04 13:51:19 | [train_policy] epoch #262 | Obtaining samples for iteration 262...
2021-06-04 13:51:20 | [train_policy] epoch #262 | Logging diagnostics...
2021-06-04 13:51:20 | [train_policy] epoch #262 | Optimizing policy...
2021-06-04 13:51:20 | [train_policy] epoch #262 | Computing loss before
2021-06-04 13:51:20 | [train_policy] epoch #262 | Computing KL before
2021-06-04 13:51:20 | [train_policy] epoch #262 | Optimizing
2021-06-04 13:51:20 | [train_policy] epoch #262 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:20 | [train_policy] epoch #262 | computing loss before
2021-06-04 13:51:20 | [train_policy] epoch #262 | computing gradient
2021-06-04 13:51:20 | [train_policy] epoch #262 | gradient computed
2021-06-04 13:51:20 | [train_policy] epoch #262 | computing descent direction
2021-06-04 13:51:20 | [train_policy] epoch #262 | descent direction computed
2021-06-04 13:51:20 | [train_policy] epoch #262 | backtrack iters: 1
2021-06-04 13:51:20 | [train_policy] epoch #262 | optimization finished
2021-06-04 13:51:20 | [train_policy] epoch #262 | Computing KL after
2021-06-04 13:51:20 | [train_policy] epoch #262 | Computing loss after
2021-06-04 13:51:20 | [train_policy] epoch #262 | Fitting baseline...
2021-06-04 13:51:20 | [train_policy] epoch #262 | Saving snapshot...
2021-06-04 13:51:20 | [train_policy] epoch #262 | Saved
2021-06-04 13:51:20 | [train_policy] epoch #262 | Time 211.96 s
2021-06-04 13:51:20 | [train_policy] epoch #262 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.287268
Evaluation/AverageDiscountedReturn          -67.7506
Evaluation/AverageReturn                    -67.7506
Evaluation/CompletionRate                     0
Evaluation/Iteration                        262
Evaluation/MaxReturn                        -32.2962
Evaluation/MinReturn                      -2062.06
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.107
Extras/EpisodeRewardMean                    -65.6357
LinearFeatureBaseline/ExplainedVariance       0.124449
PolicyExecTime                                0.226141
ProcessExecTime                               0.0315211
TotalEnvSteps                            266156
policy/Entropy                               -0.410975
policy/KL                                     0.00692145
policy/KLBefore                               0
policy/LossAfter                             -0.0342287
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.663004
policy/dLoss                                  0.0342287
---------------------------------------  ---------------
2021-06-04 13:51:20 | [train_policy] epoch #263 | Obtaining samples for iteration 263...
2021-06-04 13:51:20 | [train_policy] epoch #263 | Logging diagnostics...
2021-06-04 13:51:20 | [train_policy] epoch #263 | Optimizing policy...
2021-06-04 13:51:20 | [train_policy] epoch #263 | Computing loss before
2021-06-04 13:51:20 | [train_policy] epoch #263 | Computing KL before
2021-06-04 13:51:20 | [train_policy] epoch #263 | Optimizing
2021-06-04 13:51:20 | [train_policy] epoch #263 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:20 | [train_policy] epoch #263 | computing loss before
2021-06-04 13:51:20 | [train_policy] epoch #263 | computing gradient
2021-06-04 13:51:20 | [train_policy] epoch #263 | gradient computed
2021-06-04 13:51:20 | [train_policy] epoch #263 | computing descent direction
2021-06-04 13:51:20 | [train_policy] epoch #263 | descent direction computed
2021-06-04 13:51:20 | [train_policy] epoch #263 | backtrack iters: 1
2021-06-04 13:51:20 | [train_policy] epoch #263 | optimization finished
2021-06-04 13:51:20 | [train_policy] epoch #263 | Computing KL after
2021-06-04 13:51:20 | [train_policy] epoch #263 | Computing loss after
2021-06-04 13:51:20 | [train_policy] epoch #263 | Fitting baseline...
2021-06-04 13:51:20 | [train_policy] epoch #263 | Saving snapshot...
2021-06-04 13:51:21 | [train_policy] epoch #263 | Saved
2021-06-04 13:51:21 | [train_policy] epoch #263 | Time 212.72 s
2021-06-04 13:51:21 | [train_policy] epoch #263 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.283514
Evaluation/AverageDiscountedReturn          -45.0073
Evaluation/AverageReturn                    -45.0073
Evaluation/CompletionRate                     0
Evaluation/Iteration                        263
Evaluation/MaxReturn                        -34.056
Evaluation/MinReturn                        -63.8939
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.53852
Extras/EpisodeRewardMean                    -44.9941
LinearFeatureBaseline/ExplainedVariance     -33.4295
PolicyExecTime                                0.213395
ProcessExecTime                               0.0310922
TotalEnvSteps                            267168
policy/Entropy                               -0.415391
policy/KL                                     0.00694499
policy/KLBefore                               0
policy/LossAfter                             -0.0221116
policy/LossBefore                            -5.23013e-08
policy/Perplexity                             0.660082
policy/dLoss                                  0.0221116
---------------------------------------  ----------------
2021-06-04 13:51:21 | [train_policy] epoch #264 | Obtaining samples for iteration 264...
2021-06-04 13:51:21 | [train_policy] epoch #264 | Logging diagnostics...
2021-06-04 13:51:21 | [train_policy] epoch #264 | Optimizing policy...
2021-06-04 13:51:21 | [train_policy] epoch #264 | Computing loss before
2021-06-04 13:51:21 | [train_policy] epoch #264 | Computing KL before
2021-06-04 13:51:21 | [train_policy] epoch #264 | Optimizing
2021-06-04 13:51:21 | [train_policy] epoch #264 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:21 | [train_policy] epoch #264 | computing loss before
2021-06-04 13:51:21 | [train_policy] epoch #264 | computing gradient
2021-06-04 13:51:21 | [train_policy] epoch #264 | gradient computed
2021-06-04 13:51:21 | [train_policy] epoch #264 | computing descent direction
2021-06-04 13:51:21 | [train_policy] epoch #264 | descent direction computed
2021-06-04 13:51:21 | [train_policy] epoch #264 | backtrack iters: 0
2021-06-04 13:51:21 | [train_policy] epoch #264 | optimization finished
2021-06-04 13:51:21 | [train_policy] epoch #264 | Computing KL after
2021-06-04 13:51:21 | [train_policy] epoch #264 | Computing loss after
2021-06-04 13:51:21 | [train_policy] epoch #264 | Fitting baseline...
2021-06-04 13:51:21 | [train_policy] epoch #264 | Saving snapshot...
2021-06-04 13:51:21 | [train_policy] epoch #264 | Saved
2021-06-04 13:51:21 | [train_policy] epoch #264 | Time 213.51 s
2021-06-04 13:51:21 | [train_policy] epoch #264 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284689
Evaluation/AverageDiscountedReturn          -43.9763
Evaluation/AverageReturn                    -43.9763
Evaluation/CompletionRate                     0
Evaluation/Iteration                        264
Evaluation/MaxReturn                        -30.7928
Evaluation/MinReturn                       -102.23
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.81347
Extras/EpisodeRewardMean                    -44.0586
LinearFeatureBaseline/ExplainedVariance       0.825822
PolicyExecTime                                0.228582
ProcessExecTime                               0.0312288
TotalEnvSteps                            268180
policy/Entropy                               -0.408893
policy/KL                                     0.00985371
policy/KLBefore                               0
policy/LossAfter                             -0.0196585
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.664385
policy/dLoss                                  0.0196585
---------------------------------------  ----------------
2021-06-04 13:51:21 | [train_policy] epoch #265 | Obtaining samples for iteration 265...
2021-06-04 13:51:22 | [train_policy] epoch #265 | Logging diagnostics...
2021-06-04 13:51:22 | [train_policy] epoch #265 | Optimizing policy...
2021-06-04 13:51:22 | [train_policy] epoch #265 | Computing loss before
2021-06-04 13:51:22 | [train_policy] epoch #265 | Computing KL before
2021-06-04 13:51:22 | [train_policy] epoch #265 | Optimizing
2021-06-04 13:51:22 | [train_policy] epoch #265 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:22 | [train_policy] epoch #265 | computing loss before
2021-06-04 13:51:22 | [train_policy] epoch #265 | computing gradient
2021-06-04 13:51:22 | [train_policy] epoch #265 | gradient computed
2021-06-04 13:51:22 | [train_policy] epoch #265 | computing descent direction
2021-06-04 13:51:22 | [train_policy] epoch #265 | descent direction computed
2021-06-04 13:51:22 | [train_policy] epoch #265 | backtrack iters: 0
2021-06-04 13:51:22 | [train_policy] epoch #265 | optimization finished
2021-06-04 13:51:22 | [train_policy] epoch #265 | Computing KL after
2021-06-04 13:51:22 | [train_policy] epoch #265 | Computing loss after
2021-06-04 13:51:22 | [train_policy] epoch #265 | Fitting baseline...
2021-06-04 13:51:22 | [train_policy] epoch #265 | Saving snapshot...
2021-06-04 13:51:22 | [train_policy] epoch #265 | Saved
2021-06-04 13:51:22 | [train_policy] epoch #265 | Time 214.28 s
2021-06-04 13:51:22 | [train_policy] epoch #265 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                   0.283679
Evaluation/AverageDiscountedReturn          -66.8583
Evaluation/AverageReturn                    -66.8583
Evaluation/CompletionRate                     0
Evaluation/Iteration                        265
Evaluation/MaxReturn                        -33.6809
Evaluation/MinReturn                      -2062.12
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.422
Extras/EpisodeRewardMean                    -64.9966
LinearFeatureBaseline/ExplainedVariance       0.015372
PolicyExecTime                                0.212975
ProcessExecTime                               0.031333
TotalEnvSteps                            269192
policy/Entropy                               -0.375291
policy/KL                                     0.0098369
policy/KLBefore                               0
policy/LossAfter                             -0.0238494
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.68709
policy/dLoss                                  0.0238494
---------------------------------------  ---------------
2021-06-04 13:51:22 | [train_policy] epoch #266 | Obtaining samples for iteration 266...
2021-06-04 13:51:23 | [train_policy] epoch #266 | Logging diagnostics...
2021-06-04 13:51:23 | [train_policy] epoch #266 | Optimizing policy...
2021-06-04 13:51:23 | [train_policy] epoch #266 | Computing loss before
2021-06-04 13:51:23 | [train_policy] epoch #266 | Computing KL before
2021-06-04 13:51:23 | [train_policy] epoch #266 | Optimizing
2021-06-04 13:51:23 | [train_policy] epoch #266 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:23 | [train_policy] epoch #266 | computing loss before
2021-06-04 13:51:23 | [train_policy] epoch #266 | computing gradient
2021-06-04 13:51:23 | [train_policy] epoch #266 | gradient computed
2021-06-04 13:51:23 | [train_policy] epoch #266 | computing descent direction
2021-06-04 13:51:23 | [train_policy] epoch #266 | descent direction computed
2021-06-04 13:51:23 | [train_policy] epoch #266 | backtrack iters: 0
2021-06-04 13:51:23 | [train_policy] epoch #266 | optimization finished
2021-06-04 13:51:23 | [train_policy] epoch #266 | Computing KL after
2021-06-04 13:51:23 | [train_policy] epoch #266 | Computing loss after
2021-06-04 13:51:23 | [train_policy] epoch #266 | Fitting baseline...
2021-06-04 13:51:23 | [train_policy] epoch #266 | Saving snapshot...
2021-06-04 13:51:23 | [train_policy] epoch #266 | Saved
2021-06-04 13:51:23 | [train_policy] epoch #266 | Time 215.08 s
2021-06-04 13:51:23 | [train_policy] epoch #266 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.284154
Evaluation/AverageDiscountedReturn          -46.1966
Evaluation/AverageReturn                    -46.1966
Evaluation/CompletionRate                     0
Evaluation/Iteration                        266
Evaluation/MaxReturn                        -30.0851
Evaluation/MinReturn                       -192.469
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         17.9897
Extras/EpisodeRewardMean                    -46.2934
LinearFeatureBaseline/ExplainedVariance     -10.0259
PolicyExecTime                                0.239772
ProcessExecTime                               0.0315123
TotalEnvSteps                            270204
policy/Entropy                               -0.376535
policy/KL                                     0.00980041
policy/KLBefore                               0
policy/LossAfter                             -0.0601087
policy/LossBefore                             1.0366e-08
policy/Perplexity                             0.686235
policy/dLoss                                  0.0601087
---------------------------------------  ---------------
2021-06-04 13:51:23 | [train_policy] epoch #267 | Obtaining samples for iteration 267...
2021-06-04 13:51:24 | [train_policy] epoch #267 | Logging diagnostics...
2021-06-04 13:51:24 | [train_policy] epoch #267 | Optimizing policy...
2021-06-04 13:51:24 | [train_policy] epoch #267 | Computing loss before
2021-06-04 13:51:24 | [train_policy] epoch #267 | Computing KL before
2021-06-04 13:51:24 | [train_policy] epoch #267 | Optimizing
2021-06-04 13:51:24 | [train_policy] epoch #267 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:24 | [train_policy] epoch #267 | computing loss before
2021-06-04 13:51:24 | [train_policy] epoch #267 | computing gradient
2021-06-04 13:51:24 | [train_policy] epoch #267 | gradient computed
2021-06-04 13:51:24 | [train_policy] epoch #267 | computing descent direction
2021-06-04 13:51:24 | [train_policy] epoch #267 | descent direction computed
2021-06-04 13:51:24 | [train_policy] epoch #267 | backtrack iters: 1
2021-06-04 13:51:24 | [train_policy] epoch #267 | optimization finished
2021-06-04 13:51:24 | [train_policy] epoch #267 | Computing KL after
2021-06-04 13:51:24 | [train_policy] epoch #267 | Computing loss after
2021-06-04 13:51:24 | [train_policy] epoch #267 | Fitting baseline...
2021-06-04 13:51:24 | [train_policy] epoch #267 | Saving snapshot...
2021-06-04 13:51:24 | [train_policy] epoch #267 | Saved
2021-06-04 13:51:24 | [train_policy] epoch #267 | Time 215.89 s
2021-06-04 13:51:24 | [train_policy] epoch #267 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286872
Evaluation/AverageDiscountedReturn          -43.8845
Evaluation/AverageReturn                    -43.8845
Evaluation/CompletionRate                     0
Evaluation/Iteration                        267
Evaluation/MaxReturn                        -30.8835
Evaluation/MinReturn                        -85.9308
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.02798
Extras/EpisodeRewardMean                    -43.5471
LinearFeatureBaseline/ExplainedVariance       0.79816
PolicyExecTime                                0.229349
ProcessExecTime                               0.031512
TotalEnvSteps                            271216
policy/Entropy                               -0.414998
policy/KL                                     0.00647014
policy/KLBefore                               0
policy/LossAfter                             -0.0202289
policy/LossBefore                             1.60202e-08
policy/Perplexity                             0.660341
policy/dLoss                                  0.0202289
---------------------------------------  ----------------
2021-06-04 13:51:24 | [train_policy] epoch #268 | Obtaining samples for iteration 268...
2021-06-04 13:51:24 | [train_policy] epoch #268 | Logging diagnostics...
2021-06-04 13:51:24 | [train_policy] epoch #268 | Optimizing policy...
2021-06-04 13:51:24 | [train_policy] epoch #268 | Computing loss before
2021-06-04 13:51:24 | [train_policy] epoch #268 | Computing KL before
2021-06-04 13:51:24 | [train_policy] epoch #268 | Optimizing
2021-06-04 13:51:24 | [train_policy] epoch #268 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:24 | [train_policy] epoch #268 | computing loss before
2021-06-04 13:51:24 | [train_policy] epoch #268 | computing gradient
2021-06-04 13:51:24 | [train_policy] epoch #268 | gradient computed
2021-06-04 13:51:24 | [train_policy] epoch #268 | computing descent direction
2021-06-04 13:51:24 | [train_policy] epoch #268 | descent direction computed
2021-06-04 13:51:24 | [train_policy] epoch #268 | backtrack iters: 0
2021-06-04 13:51:24 | [train_policy] epoch #268 | optimization finished
2021-06-04 13:51:24 | [train_policy] epoch #268 | Computing KL after
2021-06-04 13:51:24 | [train_policy] epoch #268 | Computing loss after
2021-06-04 13:51:24 | [train_policy] epoch #268 | Fitting baseline...
2021-06-04 13:51:24 | [train_policy] epoch #268 | Saving snapshot...
2021-06-04 13:51:24 | [train_policy] epoch #268 | Saved
2021-06-04 13:51:24 | [train_policy] epoch #268 | Time 216.70 s
2021-06-04 13:51:24 | [train_policy] epoch #268 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284142
Evaluation/AverageDiscountedReturn          -46.072
Evaluation/AverageReturn                    -46.072
Evaluation/CompletionRate                     0
Evaluation/Iteration                        268
Evaluation/MaxReturn                        -29.8444
Evaluation/MinReturn                       -141.914
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         13.237
Extras/EpisodeRewardMean                    -46.0856
LinearFeatureBaseline/ExplainedVariance       0.685053
PolicyExecTime                                0.230116
ProcessExecTime                               0.0311811
TotalEnvSteps                            272228
policy/Entropy                               -0.404465
policy/KL                                     0.00980621
policy/KLBefore                               0
policy/LossAfter                             -0.0234107
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.667334
policy/dLoss                                  0.0234107
---------------------------------------  ----------------
2021-06-04 13:51:25 | [train_policy] epoch #269 | Obtaining samples for iteration 269...
2021-06-04 13:51:25 | [train_policy] epoch #269 | Logging diagnostics...
2021-06-04 13:51:25 | [train_policy] epoch #269 | Optimizing policy...
2021-06-04 13:51:25 | [train_policy] epoch #269 | Computing loss before
2021-06-04 13:51:25 | [train_policy] epoch #269 | Computing KL before
2021-06-04 13:51:25 | [train_policy] epoch #269 | Optimizing
2021-06-04 13:51:25 | [train_policy] epoch #269 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:25 | [train_policy] epoch #269 | computing loss before
2021-06-04 13:51:25 | [train_policy] epoch #269 | computing gradient
2021-06-04 13:51:25 | [train_policy] epoch #269 | gradient computed
2021-06-04 13:51:25 | [train_policy] epoch #269 | computing descent direction
2021-06-04 13:51:25 | [train_policy] epoch #269 | descent direction computed
2021-06-04 13:51:25 | [train_policy] epoch #269 | backtrack iters: 1
2021-06-04 13:51:25 | [train_policy] epoch #269 | optimization finished
2021-06-04 13:51:25 | [train_policy] epoch #269 | Computing KL after
2021-06-04 13:51:25 | [train_policy] epoch #269 | Computing loss after
2021-06-04 13:51:25 | [train_policy] epoch #269 | Fitting baseline...
2021-06-04 13:51:25 | [train_policy] epoch #269 | Saving snapshot...
2021-06-04 13:51:25 | [train_policy] epoch #269 | Saved
2021-06-04 13:51:25 | [train_policy] epoch #269 | Time 217.50 s
2021-06-04 13:51:25 | [train_policy] epoch #269 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286297
Evaluation/AverageDiscountedReturn          -43.956
Evaluation/AverageReturn                    -43.956
Evaluation/CompletionRate                     0
Evaluation/Iteration                        269
Evaluation/MaxReturn                        -30.3838
Evaluation/MinReturn                        -89.7098
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.22699
Extras/EpisodeRewardMean                    -43.9793
LinearFeatureBaseline/ExplainedVariance       0.640082
PolicyExecTime                                0.229236
ProcessExecTime                               0.0313237
TotalEnvSteps                            273240
policy/Entropy                               -0.421294
policy/KL                                     0.00648277
policy/KLBefore                               0
policy/LossAfter                             -0.0153238
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.656197
policy/dLoss                                  0.0153238
---------------------------------------  ----------------
2021-06-04 13:51:25 | [train_policy] epoch #270 | Obtaining samples for iteration 270...
2021-06-04 13:51:26 | [train_policy] epoch #270 | Logging diagnostics...
2021-06-04 13:51:26 | [train_policy] epoch #270 | Optimizing policy...
2021-06-04 13:51:26 | [train_policy] epoch #270 | Computing loss before
2021-06-04 13:51:26 | [train_policy] epoch #270 | Computing KL before
2021-06-04 13:51:26 | [train_policy] epoch #270 | Optimizing
2021-06-04 13:51:26 | [train_policy] epoch #270 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:26 | [train_policy] epoch #270 | computing loss before
2021-06-04 13:51:26 | [train_policy] epoch #270 | computing gradient
2021-06-04 13:51:26 | [train_policy] epoch #270 | gradient computed
2021-06-04 13:51:26 | [train_policy] epoch #270 | computing descent direction
2021-06-04 13:51:26 | [train_policy] epoch #270 | descent direction computed
2021-06-04 13:51:26 | [train_policy] epoch #270 | backtrack iters: 0
2021-06-04 13:51:26 | [train_policy] epoch #270 | optimization finished
2021-06-04 13:51:26 | [train_policy] epoch #270 | Computing KL after
2021-06-04 13:51:26 | [train_policy] epoch #270 | Computing loss after
2021-06-04 13:51:26 | [train_policy] epoch #270 | Fitting baseline...
2021-06-04 13:51:26 | [train_policy] epoch #270 | Saving snapshot...
2021-06-04 13:51:26 | [train_policy] epoch #270 | Saved
2021-06-04 13:51:26 | [train_policy] epoch #270 | Time 218.29 s
2021-06-04 13:51:26 | [train_policy] epoch #270 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284582
Evaluation/AverageDiscountedReturn          -45.3167
Evaluation/AverageReturn                    -45.3167
Evaluation/CompletionRate                     0
Evaluation/Iteration                        270
Evaluation/MaxReturn                        -29.9071
Evaluation/MinReturn                        -92.1077
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.06716
Extras/EpisodeRewardMean                    -45.8857
LinearFeatureBaseline/ExplainedVariance       0.881223
PolicyExecTime                                0.220109
ProcessExecTime                               0.0313618
TotalEnvSteps                            274252
policy/Entropy                               -0.418887
policy/KL                                     0.00994137
policy/KLBefore                               0
policy/LossAfter                             -0.0164799
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.657779
policy/dLoss                                  0.0164799
---------------------------------------  ----------------
2021-06-04 13:51:26 | [train_policy] epoch #271 | Obtaining samples for iteration 271...
2021-06-04 13:51:27 | [train_policy] epoch #271 | Logging diagnostics...
2021-06-04 13:51:27 | [train_policy] epoch #271 | Optimizing policy...
2021-06-04 13:51:27 | [train_policy] epoch #271 | Computing loss before
2021-06-04 13:51:27 | [train_policy] epoch #271 | Computing KL before
2021-06-04 13:51:27 | [train_policy] epoch #271 | Optimizing
2021-06-04 13:51:27 | [train_policy] epoch #271 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:27 | [train_policy] epoch #271 | computing loss before
2021-06-04 13:51:27 | [train_policy] epoch #271 | computing gradient
2021-06-04 13:51:27 | [train_policy] epoch #271 | gradient computed
2021-06-04 13:51:27 | [train_policy] epoch #271 | computing descent direction
2021-06-04 13:51:27 | [train_policy] epoch #271 | descent direction computed
2021-06-04 13:51:27 | [train_policy] epoch #271 | backtrack iters: 1
2021-06-04 13:51:27 | [train_policy] epoch #271 | optimization finished
2021-06-04 13:51:27 | [train_policy] epoch #271 | Computing KL after
2021-06-04 13:51:27 | [train_policy] epoch #271 | Computing loss after
2021-06-04 13:51:27 | [train_policy] epoch #271 | Fitting baseline...
2021-06-04 13:51:27 | [train_policy] epoch #271 | Saving snapshot...
2021-06-04 13:51:27 | [train_policy] epoch #271 | Saved
2021-06-04 13:51:27 | [train_policy] epoch #271 | Time 219.09 s
2021-06-04 13:51:27 | [train_policy] epoch #271 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.287745
Evaluation/AverageDiscountedReturn          -44.2448
Evaluation/AverageReturn                    -44.2448
Evaluation/CompletionRate                     0
Evaluation/Iteration                        271
Evaluation/MaxReturn                        -29.0189
Evaluation/MinReturn                        -86.8013
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.85568
Extras/EpisodeRewardMean                    -44.6829
LinearFeatureBaseline/ExplainedVariance       0.872856
PolicyExecTime                                0.224718
ProcessExecTime                               0.0314033
TotalEnvSteps                            275264
policy/Entropy                               -0.462586
policy/KL                                     0.00719168
policy/KLBefore                               0
policy/LossAfter                             -0.0157313
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.629653
policy/dLoss                                  0.0157313
---------------------------------------  ---------------
2021-06-04 13:51:27 | [train_policy] epoch #272 | Obtaining samples for iteration 272...
2021-06-04 13:51:28 | [train_policy] epoch #272 | Logging diagnostics...
2021-06-04 13:51:28 | [train_policy] epoch #272 | Optimizing policy...
2021-06-04 13:51:28 | [train_policy] epoch #272 | Computing loss before
2021-06-04 13:51:28 | [train_policy] epoch #272 | Computing KL before
2021-06-04 13:51:28 | [train_policy] epoch #272 | Optimizing
2021-06-04 13:51:28 | [train_policy] epoch #272 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:28 | [train_policy] epoch #272 | computing loss before
2021-06-04 13:51:28 | [train_policy] epoch #272 | computing gradient
2021-06-04 13:51:28 | [train_policy] epoch #272 | gradient computed
2021-06-04 13:51:28 | [train_policy] epoch #272 | computing descent direction
2021-06-04 13:51:28 | [train_policy] epoch #272 | descent direction computed
2021-06-04 13:51:28 | [train_policy] epoch #272 | backtrack iters: 1
2021-06-04 13:51:28 | [train_policy] epoch #272 | optimization finished
2021-06-04 13:51:28 | [train_policy] epoch #272 | Computing KL after
2021-06-04 13:51:28 | [train_policy] epoch #272 | Computing loss after
2021-06-04 13:51:28 | [train_policy] epoch #272 | Fitting baseline...
2021-06-04 13:51:28 | [train_policy] epoch #272 | Saving snapshot...
2021-06-04 13:51:28 | [train_policy] epoch #272 | Saved
2021-06-04 13:51:28 | [train_policy] epoch #272 | Time 219.88 s
2021-06-04 13:51:28 | [train_policy] epoch #272 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284386
Evaluation/AverageDiscountedReturn          -43.6836
Evaluation/AverageReturn                    -43.6836
Evaluation/CompletionRate                     0
Evaluation/Iteration                        272
Evaluation/MaxReturn                        -31.6992
Evaluation/MinReturn                        -89.8921
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.70582
Extras/EpisodeRewardMean                    -43.5807
LinearFeatureBaseline/ExplainedVariance       0.877844
PolicyExecTime                                0.226405
ProcessExecTime                               0.03107
TotalEnvSteps                            276276
policy/Entropy                               -0.440397
policy/KL                                     0.00653919
policy/KLBefore                               0
policy/LossAfter                             -0.0243902
policy/LossBefore                             7.30334e-09
policy/Perplexity                             0.64378
policy/dLoss                                  0.0243903
---------------------------------------  ----------------
2021-06-04 13:51:28 | [train_policy] epoch #273 | Obtaining samples for iteration 273...
2021-06-04 13:51:28 | [train_policy] epoch #273 | Logging diagnostics...
2021-06-04 13:51:28 | [train_policy] epoch #273 | Optimizing policy...
2021-06-04 13:51:28 | [train_policy] epoch #273 | Computing loss before
2021-06-04 13:51:28 | [train_policy] epoch #273 | Computing KL before
2021-06-04 13:51:28 | [train_policy] epoch #273 | Optimizing
2021-06-04 13:51:28 | [train_policy] epoch #273 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:28 | [train_policy] epoch #273 | computing loss before
2021-06-04 13:51:28 | [train_policy] epoch #273 | computing gradient
2021-06-04 13:51:28 | [train_policy] epoch #273 | gradient computed
2021-06-04 13:51:28 | [train_policy] epoch #273 | computing descent direction
2021-06-04 13:51:28 | [train_policy] epoch #273 | descent direction computed
2021-06-04 13:51:28 | [train_policy] epoch #273 | backtrack iters: 0
2021-06-04 13:51:28 | [train_policy] epoch #273 | optimization finished
2021-06-04 13:51:28 | [train_policy] epoch #273 | Computing KL after
2021-06-04 13:51:28 | [train_policy] epoch #273 | Computing loss after
2021-06-04 13:51:28 | [train_policy] epoch #273 | Fitting baseline...
2021-06-04 13:51:28 | [train_policy] epoch #273 | Saving snapshot...
2021-06-04 13:51:28 | [train_policy] epoch #273 | Saved
2021-06-04 13:51:28 | [train_policy] epoch #273 | Time 220.68 s
2021-06-04 13:51:28 | [train_policy] epoch #273 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285017
Evaluation/AverageDiscountedReturn          -45.5056
Evaluation/AverageReturn                    -45.5056
Evaluation/CompletionRate                     0
Evaluation/Iteration                        273
Evaluation/MaxReturn                        -34.3082
Evaluation/MinReturn                        -64.183
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.59718
Extras/EpisodeRewardMean                    -45.1796
LinearFeatureBaseline/ExplainedVariance       0.932432
PolicyExecTime                                0.236037
ProcessExecTime                               0.0312004
TotalEnvSteps                            277288
policy/Entropy                               -0.398385
policy/KL                                     0.00978436
policy/KLBefore                               0
policy/LossAfter                             -0.0159712
policy/LossBefore                             8.77578e-09
policy/Perplexity                             0.671403
policy/dLoss                                  0.0159712
---------------------------------------  ----------------
2021-06-04 13:51:28 | [train_policy] epoch #274 | Obtaining samples for iteration 274...
2021-06-04 13:51:29 | [train_policy] epoch #274 | Logging diagnostics...
2021-06-04 13:51:29 | [train_policy] epoch #274 | Optimizing policy...
2021-06-04 13:51:29 | [train_policy] epoch #274 | Computing loss before
2021-06-04 13:51:29 | [train_policy] epoch #274 | Computing KL before
2021-06-04 13:51:29 | [train_policy] epoch #274 | Optimizing
2021-06-04 13:51:29 | [train_policy] epoch #274 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:29 | [train_policy] epoch #274 | computing loss before
2021-06-04 13:51:29 | [train_policy] epoch #274 | computing gradient
2021-06-04 13:51:29 | [train_policy] epoch #274 | gradient computed
2021-06-04 13:51:29 | [train_policy] epoch #274 | computing descent direction
2021-06-04 13:51:29 | [train_policy] epoch #274 | descent direction computed
2021-06-04 13:51:29 | [train_policy] epoch #274 | backtrack iters: 1
2021-06-04 13:51:29 | [train_policy] epoch #274 | optimization finished
2021-06-04 13:51:29 | [train_policy] epoch #274 | Computing KL after
2021-06-04 13:51:29 | [train_policy] epoch #274 | Computing loss after
2021-06-04 13:51:29 | [train_policy] epoch #274 | Fitting baseline...
2021-06-04 13:51:29 | [train_policy] epoch #274 | Saving snapshot...
2021-06-04 13:51:29 | [train_policy] epoch #274 | Saved
2021-06-04 13:51:29 | [train_policy] epoch #274 | Time 221.49 s
2021-06-04 13:51:29 | [train_policy] epoch #274 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287189
Evaluation/AverageDiscountedReturn          -43.6325
Evaluation/AverageReturn                    -43.6325
Evaluation/CompletionRate                     0
Evaluation/Iteration                        274
Evaluation/MaxReturn                        -29.5875
Evaluation/MinReturn                        -97.9737
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.85467
Extras/EpisodeRewardMean                    -43.6361
LinearFeatureBaseline/ExplainedVariance       0.851907
PolicyExecTime                                0.229028
ProcessExecTime                               0.0312543
TotalEnvSteps                            278300
policy/Entropy                               -0.409097
policy/KL                                     0.00798566
policy/KLBefore                               0
policy/LossAfter                             -0.0346877
policy/LossBefore                             2.59151e-09
policy/Perplexity                             0.66425
policy/dLoss                                  0.0346877
---------------------------------------  ----------------
2021-06-04 13:51:29 | [train_policy] epoch #275 | Obtaining samples for iteration 275...
2021-06-04 13:51:30 | [train_policy] epoch #275 | Logging diagnostics...
2021-06-04 13:51:30 | [train_policy] epoch #275 | Optimizing policy...
2021-06-04 13:51:30 | [train_policy] epoch #275 | Computing loss before
2021-06-04 13:51:30 | [train_policy] epoch #275 | Computing KL before
2021-06-04 13:51:30 | [train_policy] epoch #275 | Optimizing
2021-06-04 13:51:30 | [train_policy] epoch #275 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:30 | [train_policy] epoch #275 | computing loss before
2021-06-04 13:51:30 | [train_policy] epoch #275 | computing gradient
2021-06-04 13:51:30 | [train_policy] epoch #275 | gradient computed
2021-06-04 13:51:30 | [train_policy] epoch #275 | computing descent direction
2021-06-04 13:51:30 | [train_policy] epoch #275 | descent direction computed
2021-06-04 13:51:30 | [train_policy] epoch #275 | backtrack iters: 0
2021-06-04 13:51:30 | [train_policy] epoch #275 | optimization finished
2021-06-04 13:51:30 | [train_policy] epoch #275 | Computing KL after
2021-06-04 13:51:30 | [train_policy] epoch #275 | Computing loss after
2021-06-04 13:51:30 | [train_policy] epoch #275 | Fitting baseline...
2021-06-04 13:51:30 | [train_policy] epoch #275 | Saving snapshot...
2021-06-04 13:51:30 | [train_policy] epoch #275 | Saved
2021-06-04 13:51:30 | [train_policy] epoch #275 | Time 222.28 s
2021-06-04 13:51:30 | [train_policy] epoch #275 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285747
Evaluation/AverageDiscountedReturn          -42.0605
Evaluation/AverageReturn                    -42.0605
Evaluation/CompletionRate                     0
Evaluation/Iteration                        275
Evaluation/MaxReturn                        -29.379
Evaluation/MinReturn                        -64.0297
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.85415
Extras/EpisodeRewardMean                    -41.9646
LinearFeatureBaseline/ExplainedVariance       0.919999
PolicyExecTime                                0.218271
ProcessExecTime                               0.0312119
TotalEnvSteps                            279312
policy/Entropy                               -0.37281
policy/KL                                     0.00996058
policy/KLBefore                               0
policy/LossAfter                             -0.0221501
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.688796
policy/dLoss                                  0.0221501
---------------------------------------  ----------------
2021-06-04 13:51:30 | [train_policy] epoch #276 | Obtaining samples for iteration 276...
2021-06-04 13:51:31 | [train_policy] epoch #276 | Logging diagnostics...
2021-06-04 13:51:31 | [train_policy] epoch #276 | Optimizing policy...
2021-06-04 13:51:31 | [train_policy] epoch #276 | Computing loss before
2021-06-04 13:51:31 | [train_policy] epoch #276 | Computing KL before
2021-06-04 13:51:31 | [train_policy] epoch #276 | Optimizing
2021-06-04 13:51:31 | [train_policy] epoch #276 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:31 | [train_policy] epoch #276 | computing loss before
2021-06-04 13:51:31 | [train_policy] epoch #276 | computing gradient
2021-06-04 13:51:31 | [train_policy] epoch #276 | gradient computed
2021-06-04 13:51:31 | [train_policy] epoch #276 | computing descent direction
2021-06-04 13:51:31 | [train_policy] epoch #276 | descent direction computed
2021-06-04 13:51:31 | [train_policy] epoch #276 | backtrack iters: 0
2021-06-04 13:51:31 | [train_policy] epoch #276 | optimization finished
2021-06-04 13:51:31 | [train_policy] epoch #276 | Computing KL after
2021-06-04 13:51:31 | [train_policy] epoch #276 | Computing loss after
2021-06-04 13:51:31 | [train_policy] epoch #276 | Fitting baseline...
2021-06-04 13:51:31 | [train_policy] epoch #276 | Saving snapshot...
2021-06-04 13:51:31 | [train_policy] epoch #276 | Saved
2021-06-04 13:51:31 | [train_policy] epoch #276 | Time 223.08 s
2021-06-04 13:51:31 | [train_policy] epoch #276 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284834
Evaluation/AverageDiscountedReturn          -43.5456
Evaluation/AverageReturn                    -43.5456
Evaluation/CompletionRate                     0
Evaluation/Iteration                        276
Evaluation/MaxReturn                        -28.9686
Evaluation/MinReturn                        -87.8834
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.58353
Extras/EpisodeRewardMean                    -43.3569
LinearFeatureBaseline/ExplainedVariance       0.886872
PolicyExecTime                                0.234292
ProcessExecTime                               0.0312161
TotalEnvSteps                            280324
policy/Entropy                               -0.345062
policy/KL                                     0.00975691
policy/KLBefore                               0
policy/LossAfter                             -0.0244844
policy/LossBefore                             5.88979e-09
policy/Perplexity                             0.708177
policy/dLoss                                  0.0244844
---------------------------------------  ----------------
2021-06-04 13:51:31 | [train_policy] epoch #277 | Obtaining samples for iteration 277...
2021-06-04 13:51:32 | [train_policy] epoch #277 | Logging diagnostics...
2021-06-04 13:51:32 | [train_policy] epoch #277 | Optimizing policy...
2021-06-04 13:51:32 | [train_policy] epoch #277 | Computing loss before
2021-06-04 13:51:32 | [train_policy] epoch #277 | Computing KL before
2021-06-04 13:51:32 | [train_policy] epoch #277 | Optimizing
2021-06-04 13:51:32 | [train_policy] epoch #277 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:32 | [train_policy] epoch #277 | computing loss before
2021-06-04 13:51:32 | [train_policy] epoch #277 | computing gradient
2021-06-04 13:51:32 | [train_policy] epoch #277 | gradient computed
2021-06-04 13:51:32 | [train_policy] epoch #277 | computing descent direction
2021-06-04 13:51:32 | [train_policy] epoch #277 | descent direction computed
2021-06-04 13:51:32 | [train_policy] epoch #277 | backtrack iters: 1
2021-06-04 13:51:32 | [train_policy] epoch #277 | optimization finished
2021-06-04 13:51:32 | [train_policy] epoch #277 | Computing KL after
2021-06-04 13:51:32 | [train_policy] epoch #277 | Computing loss after
2021-06-04 13:51:32 | [train_policy] epoch #277 | Fitting baseline...
2021-06-04 13:51:32 | [train_policy] epoch #277 | Saving snapshot...
2021-06-04 13:51:32 | [train_policy] epoch #277 | Saved
2021-06-04 13:51:32 | [train_policy] epoch #277 | Time 223.88 s
2021-06-04 13:51:32 | [train_policy] epoch #277 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284441
Evaluation/AverageDiscountedReturn          -43.4633
Evaluation/AverageReturn                    -43.4633
Evaluation/CompletionRate                     0
Evaluation/Iteration                        277
Evaluation/MaxReturn                        -30.505
Evaluation/MinReturn                        -88.5239
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.19235
Extras/EpisodeRewardMean                    -43.257
LinearFeatureBaseline/ExplainedVariance       0.881085
PolicyExecTime                                0.231894
ProcessExecTime                               0.0311818
TotalEnvSteps                            281336
policy/Entropy                               -0.356219
policy/KL                                     0.00646882
policy/KLBefore                               0
policy/LossAfter                             -0.0172239
policy/LossBefore                            -1.06016e-08
policy/Perplexity                             0.700319
policy/dLoss                                  0.0172239
---------------------------------------  ----------------
2021-06-04 13:51:32 | [train_policy] epoch #278 | Obtaining samples for iteration 278...
2021-06-04 13:51:32 | [train_policy] epoch #278 | Logging diagnostics...
2021-06-04 13:51:32 | [train_policy] epoch #278 | Optimizing policy...
2021-06-04 13:51:32 | [train_policy] epoch #278 | Computing loss before
2021-06-04 13:51:32 | [train_policy] epoch #278 | Computing KL before
2021-06-04 13:51:32 | [train_policy] epoch #278 | Optimizing
2021-06-04 13:51:32 | [train_policy] epoch #278 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:32 | [train_policy] epoch #278 | computing loss before
2021-06-04 13:51:32 | [train_policy] epoch #278 | computing gradient
2021-06-04 13:51:32 | [train_policy] epoch #278 | gradient computed
2021-06-04 13:51:32 | [train_policy] epoch #278 | computing descent direction
2021-06-04 13:51:32 | [train_policy] epoch #278 | descent direction computed
2021-06-04 13:51:32 | [train_policy] epoch #278 | backtrack iters: 1
2021-06-04 13:51:32 | [train_policy] epoch #278 | optimization finished
2021-06-04 13:51:32 | [train_policy] epoch #278 | Computing KL after
2021-06-04 13:51:32 | [train_policy] epoch #278 | Computing loss after
2021-06-04 13:51:32 | [train_policy] epoch #278 | Fitting baseline...
2021-06-04 13:51:32 | [train_policy] epoch #278 | Saving snapshot...
2021-06-04 13:51:32 | [train_policy] epoch #278 | Saved
2021-06-04 13:51:32 | [train_policy] epoch #278 | Time 224.67 s
2021-06-04 13:51:32 | [train_policy] epoch #278 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285792
Evaluation/AverageDiscountedReturn          -44.7047
Evaluation/AverageReturn                    -44.7047
Evaluation/CompletionRate                     0
Evaluation/Iteration                        278
Evaluation/MaxReturn                        -29.2548
Evaluation/MinReturn                       -102.777
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.37422
Extras/EpisodeRewardMean                    -44.925
LinearFeatureBaseline/ExplainedVariance       0.838766
PolicyExecTime                                0.214119
ProcessExecTime                               0.0311131
TotalEnvSteps                            282348
policy/Entropy                               -0.351283
policy/KL                                     0.00666594
policy/KLBefore                               0
policy/LossAfter                             -0.0153259
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.703785
policy/dLoss                                  0.0153259
---------------------------------------  ----------------
2021-06-04 13:51:32 | [train_policy] epoch #279 | Obtaining samples for iteration 279...
2021-06-04 13:51:33 | [train_policy] epoch #279 | Logging diagnostics...
2021-06-04 13:51:33 | [train_policy] epoch #279 | Optimizing policy...
2021-06-04 13:51:33 | [train_policy] epoch #279 | Computing loss before
2021-06-04 13:51:33 | [train_policy] epoch #279 | Computing KL before
2021-06-04 13:51:33 | [train_policy] epoch #279 | Optimizing
2021-06-04 13:51:33 | [train_policy] epoch #279 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:33 | [train_policy] epoch #279 | computing loss before
2021-06-04 13:51:33 | [train_policy] epoch #279 | computing gradient
2021-06-04 13:51:33 | [train_policy] epoch #279 | gradient computed
2021-06-04 13:51:33 | [train_policy] epoch #279 | computing descent direction
2021-06-04 13:51:33 | [train_policy] epoch #279 | descent direction computed
2021-06-04 13:51:33 | [train_policy] epoch #279 | backtrack iters: 1
2021-06-04 13:51:33 | [train_policy] epoch #279 | optimization finished
2021-06-04 13:51:33 | [train_policy] epoch #279 | Computing KL after
2021-06-04 13:51:33 | [train_policy] epoch #279 | Computing loss after
2021-06-04 13:51:33 | [train_policy] epoch #279 | Fitting baseline...
2021-06-04 13:51:33 | [train_policy] epoch #279 | Saving snapshot...
2021-06-04 13:51:33 | [train_policy] epoch #279 | Saved
2021-06-04 13:51:33 | [train_policy] epoch #279 | Time 225.49 s
2021-06-04 13:51:33 | [train_policy] epoch #279 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.288334
Evaluation/AverageDiscountedReturn          -43.1713
Evaluation/AverageReturn                    -43.1713
Evaluation/CompletionRate                     0
Evaluation/Iteration                        279
Evaluation/MaxReturn                        -29.705
Evaluation/MinReturn                        -63.9455
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.76095
Extras/EpisodeRewardMean                    -43.9772
LinearFeatureBaseline/ExplainedVariance       0.923901
PolicyExecTime                                0.24927
ProcessExecTime                               0.0315104
TotalEnvSteps                            283360
policy/Entropy                               -0.385659
policy/KL                                     0.00657051
policy/KLBefore                               0
policy/LossAfter                             -0.0165147
policy/LossBefore                            -1.18974e-08
policy/Perplexity                             0.680002
policy/dLoss                                  0.0165147
---------------------------------------  ----------------
2021-06-04 13:51:33 | [train_policy] epoch #280 | Obtaining samples for iteration 280...
2021-06-04 13:51:34 | [train_policy] epoch #280 | Logging diagnostics...
2021-06-04 13:51:34 | [train_policy] epoch #280 | Optimizing policy...
2021-06-04 13:51:34 | [train_policy] epoch #280 | Computing loss before
2021-06-04 13:51:34 | [train_policy] epoch #280 | Computing KL before
2021-06-04 13:51:34 | [train_policy] epoch #280 | Optimizing
2021-06-04 13:51:34 | [train_policy] epoch #280 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:34 | [train_policy] epoch #280 | computing loss before
2021-06-04 13:51:34 | [train_policy] epoch #280 | computing gradient
2021-06-04 13:51:34 | [train_policy] epoch #280 | gradient computed
2021-06-04 13:51:34 | [train_policy] epoch #280 | computing descent direction
2021-06-04 13:51:34 | [train_policy] epoch #280 | descent direction computed
2021-06-04 13:51:34 | [train_policy] epoch #280 | backtrack iters: 1
2021-06-04 13:51:34 | [train_policy] epoch #280 | optimization finished
2021-06-04 13:51:34 | [train_policy] epoch #280 | Computing KL after
2021-06-04 13:51:34 | [train_policy] epoch #280 | Computing loss after
2021-06-04 13:51:34 | [train_policy] epoch #280 | Fitting baseline...
2021-06-04 13:51:34 | [train_policy] epoch #280 | Saving snapshot...
2021-06-04 13:51:34 | [train_policy] epoch #280 | Saved
2021-06-04 13:51:34 | [train_policy] epoch #280 | Time 226.29 s
2021-06-04 13:51:34 | [train_policy] epoch #280 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286808
Evaluation/AverageDiscountedReturn          -43.6039
Evaluation/AverageReturn                    -43.6039
Evaluation/CompletionRate                     0
Evaluation/Iteration                        280
Evaluation/MaxReturn                        -30.0303
Evaluation/MinReturn                        -64.2339
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.60416
Extras/EpisodeRewardMean                    -43.4561
LinearFeatureBaseline/ExplainedVariance       0.933167
PolicyExecTime                                0.219926
ProcessExecTime                               0.0314333
TotalEnvSteps                            284372
policy/Entropy                               -0.438206
policy/KL                                     0.0068856
policy/KLBefore                               0
policy/LossAfter                             -0.0197817
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             0.645193
policy/dLoss                                  0.0197817
---------------------------------------  ----------------
2021-06-04 13:51:34 | [train_policy] epoch #281 | Obtaining samples for iteration 281...
2021-06-04 13:51:35 | [train_policy] epoch #281 | Logging diagnostics...
2021-06-04 13:51:35 | [train_policy] epoch #281 | Optimizing policy...
2021-06-04 13:51:35 | [train_policy] epoch #281 | Computing loss before
2021-06-04 13:51:35 | [train_policy] epoch #281 | Computing KL before
2021-06-04 13:51:35 | [train_policy] epoch #281 | Optimizing
2021-06-04 13:51:35 | [train_policy] epoch #281 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:35 | [train_policy] epoch #281 | computing loss before
2021-06-04 13:51:35 | [train_policy] epoch #281 | computing gradient
2021-06-04 13:51:35 | [train_policy] epoch #281 | gradient computed
2021-06-04 13:51:35 | [train_policy] epoch #281 | computing descent direction
2021-06-04 13:51:35 | [train_policy] epoch #281 | descent direction computed
2021-06-04 13:51:35 | [train_policy] epoch #281 | backtrack iters: 1
2021-06-04 13:51:35 | [train_policy] epoch #281 | optimization finished
2021-06-04 13:51:35 | [train_policy] epoch #281 | Computing KL after
2021-06-04 13:51:35 | [train_policy] epoch #281 | Computing loss after
2021-06-04 13:51:35 | [train_policy] epoch #281 | Fitting baseline...
2021-06-04 13:51:35 | [train_policy] epoch #281 | Saving snapshot...
2021-06-04 13:51:35 | [train_policy] epoch #281 | Saved
2021-06-04 13:51:35 | [train_policy] epoch #281 | Time 227.10 s
2021-06-04 13:51:35 | [train_policy] epoch #281 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285701
Evaluation/AverageDiscountedReturn          -44.0474
Evaluation/AverageReturn                    -44.0474
Evaluation/CompletionRate                     0
Evaluation/Iteration                        281
Evaluation/MaxReturn                        -32.3606
Evaluation/MinReturn                        -64.3076
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.15945
Extras/EpisodeRewardMean                    -44.2449
LinearFeatureBaseline/ExplainedVariance       0.904345
PolicyExecTime                                0.233973
ProcessExecTime                               0.0311089
TotalEnvSteps                            285384
policy/Entropy                               -0.455794
policy/KL                                     0.00670585
policy/KLBefore                               0
policy/LossAfter                             -0.0182371
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.633944
policy/dLoss                                  0.0182371
---------------------------------------  ----------------
2021-06-04 13:51:35 | [train_policy] epoch #282 | Obtaining samples for iteration 282...
2021-06-04 13:51:36 | [train_policy] epoch #282 | Logging diagnostics...
2021-06-04 13:51:36 | [train_policy] epoch #282 | Optimizing policy...
2021-06-04 13:51:36 | [train_policy] epoch #282 | Computing loss before
2021-06-04 13:51:36 | [train_policy] epoch #282 | Computing KL before
2021-06-04 13:51:36 | [train_policy] epoch #282 | Optimizing
2021-06-04 13:51:36 | [train_policy] epoch #282 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:36 | [train_policy] epoch #282 | computing loss before
2021-06-04 13:51:36 | [train_policy] epoch #282 | computing gradient
2021-06-04 13:51:36 | [train_policy] epoch #282 | gradient computed
2021-06-04 13:51:36 | [train_policy] epoch #282 | computing descent direction
2021-06-04 13:51:36 | [train_policy] epoch #282 | descent direction computed
2021-06-04 13:51:36 | [train_policy] epoch #282 | backtrack iters: 0
2021-06-04 13:51:36 | [train_policy] epoch #282 | optimization finished
2021-06-04 13:51:36 | [train_policy] epoch #282 | Computing KL after
2021-06-04 13:51:36 | [train_policy] epoch #282 | Computing loss after
2021-06-04 13:51:36 | [train_policy] epoch #282 | Fitting baseline...
2021-06-04 13:51:36 | [train_policy] epoch #282 | Saving snapshot...
2021-06-04 13:51:36 | [train_policy] epoch #282 | Saved
2021-06-04 13:51:36 | [train_policy] epoch #282 | Time 227.90 s
2021-06-04 13:51:36 | [train_policy] epoch #282 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285396
Evaluation/AverageDiscountedReturn          -41.8582
Evaluation/AverageReturn                    -41.8582
Evaluation/CompletionRate                     0
Evaluation/Iteration                        282
Evaluation/MaxReturn                        -30.1301
Evaluation/MinReturn                        -60.578
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.57947
Extras/EpisodeRewardMean                    -42.0237
LinearFeatureBaseline/ExplainedVariance       0.925327
PolicyExecTime                                0.229273
ProcessExecTime                               0.0312822
TotalEnvSteps                            286396
policy/Entropy                               -0.4135
policy/KL                                     0.00992307
policy/KLBefore                               0
policy/LossAfter                             -0.0217461
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.661332
policy/dLoss                                  0.0217461
---------------------------------------  ----------------
2021-06-04 13:51:36 | [train_policy] epoch #283 | Obtaining samples for iteration 283...
2021-06-04 13:51:36 | [train_policy] epoch #283 | Logging diagnostics...
2021-06-04 13:51:36 | [train_policy] epoch #283 | Optimizing policy...
2021-06-04 13:51:36 | [train_policy] epoch #283 | Computing loss before
2021-06-04 13:51:36 | [train_policy] epoch #283 | Computing KL before
2021-06-04 13:51:36 | [train_policy] epoch #283 | Optimizing
2021-06-04 13:51:36 | [train_policy] epoch #283 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:36 | [train_policy] epoch #283 | computing loss before
2021-06-04 13:51:36 | [train_policy] epoch #283 | computing gradient
2021-06-04 13:51:36 | [train_policy] epoch #283 | gradient computed
2021-06-04 13:51:36 | [train_policy] epoch #283 | computing descent direction
2021-06-04 13:51:36 | [train_policy] epoch #283 | descent direction computed
2021-06-04 13:51:36 | [train_policy] epoch #283 | backtrack iters: 1
2021-06-04 13:51:36 | [train_policy] epoch #283 | optimization finished
2021-06-04 13:51:36 | [train_policy] epoch #283 | Computing KL after
2021-06-04 13:51:36 | [train_policy] epoch #283 | Computing loss after
2021-06-04 13:51:36 | [train_policy] epoch #283 | Fitting baseline...
2021-06-04 13:51:36 | [train_policy] epoch #283 | Saving snapshot...
2021-06-04 13:51:36 | [train_policy] epoch #283 | Saved
2021-06-04 13:51:36 | [train_policy] epoch #283 | Time 228.69 s
2021-06-04 13:51:36 | [train_policy] epoch #283 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285624
Evaluation/AverageDiscountedReturn          -43.3321
Evaluation/AverageReturn                    -43.3321
Evaluation/CompletionRate                     0
Evaluation/Iteration                        283
Evaluation/MaxReturn                        -30.3734
Evaluation/MinReturn                        -65.1345
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.27777
Extras/EpisodeRewardMean                    -43.2315
LinearFeatureBaseline/ExplainedVariance       0.928109
PolicyExecTime                                0.212511
ProcessExecTime                               0.0311675
TotalEnvSteps                            287408
policy/Entropy                               -0.435971
policy/KL                                     0.00642597
policy/KLBefore                               0
policy/LossAfter                             -0.0137382
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.646636
policy/dLoss                                  0.0137382
---------------------------------------  ----------------
2021-06-04 13:51:36 | [train_policy] epoch #284 | Obtaining samples for iteration 284...
2021-06-04 13:51:37 | [train_policy] epoch #284 | Logging diagnostics...
2021-06-04 13:51:37 | [train_policy] epoch #284 | Optimizing policy...
2021-06-04 13:51:37 | [train_policy] epoch #284 | Computing loss before
2021-06-04 13:51:37 | [train_policy] epoch #284 | Computing KL before
2021-06-04 13:51:37 | [train_policy] epoch #284 | Optimizing
2021-06-04 13:51:37 | [train_policy] epoch #284 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:37 | [train_policy] epoch #284 | computing loss before
2021-06-04 13:51:37 | [train_policy] epoch #284 | computing gradient
2021-06-04 13:51:37 | [train_policy] epoch #284 | gradient computed
2021-06-04 13:51:37 | [train_policy] epoch #284 | computing descent direction
2021-06-04 13:51:37 | [train_policy] epoch #284 | descent direction computed
2021-06-04 13:51:37 | [train_policy] epoch #284 | backtrack iters: 1
2021-06-04 13:51:37 | [train_policy] epoch #284 | optimization finished
2021-06-04 13:51:37 | [train_policy] epoch #284 | Computing KL after
2021-06-04 13:51:37 | [train_policy] epoch #284 | Computing loss after
2021-06-04 13:51:37 | [train_policy] epoch #284 | Fitting baseline...
2021-06-04 13:51:37 | [train_policy] epoch #284 | Saving snapshot...
2021-06-04 13:51:37 | [train_policy] epoch #284 | Saved
2021-06-04 13:51:37 | [train_policy] epoch #284 | Time 229.48 s
2021-06-04 13:51:37 | [train_policy] epoch #284 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283431
Evaluation/AverageDiscountedReturn          -44.2737
Evaluation/AverageReturn                    -44.2737
Evaluation/CompletionRate                     0
Evaluation/Iteration                        284
Evaluation/MaxReturn                        -29.999
Evaluation/MinReturn                        -89.6476
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.41502
Extras/EpisodeRewardMean                    -44.4725
LinearFeatureBaseline/ExplainedVariance       0.847149
PolicyExecTime                                0.223045
ProcessExecTime                               0.0310397
TotalEnvSteps                            288420
policy/Entropy                               -0.436013
policy/KL                                     0.00652004
policy/KLBefore                               0
policy/LossAfter                             -0.0272811
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.646609
policy/dLoss                                  0.0272811
---------------------------------------  ----------------
2021-06-04 13:51:37 | [train_policy] epoch #285 | Obtaining samples for iteration 285...
2021-06-04 13:51:38 | [train_policy] epoch #285 | Logging diagnostics...
2021-06-04 13:51:38 | [train_policy] epoch #285 | Optimizing policy...
2021-06-04 13:51:38 | [train_policy] epoch #285 | Computing loss before
2021-06-04 13:51:38 | [train_policy] epoch #285 | Computing KL before
2021-06-04 13:51:38 | [train_policy] epoch #285 | Optimizing
2021-06-04 13:51:38 | [train_policy] epoch #285 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:38 | [train_policy] epoch #285 | computing loss before
2021-06-04 13:51:38 | [train_policy] epoch #285 | computing gradient
2021-06-04 13:51:38 | [train_policy] epoch #285 | gradient computed
2021-06-04 13:51:38 | [train_policy] epoch #285 | computing descent direction
2021-06-04 13:51:38 | [train_policy] epoch #285 | descent direction computed
2021-06-04 13:51:38 | [train_policy] epoch #285 | backtrack iters: 0
2021-06-04 13:51:38 | [train_policy] epoch #285 | optimization finished
2021-06-04 13:51:38 | [train_policy] epoch #285 | Computing KL after
2021-06-04 13:51:38 | [train_policy] epoch #285 | Computing loss after
2021-06-04 13:51:38 | [train_policy] epoch #285 | Fitting baseline...
2021-06-04 13:51:38 | [train_policy] epoch #285 | Saving snapshot...
2021-06-04 13:51:38 | [train_policy] epoch #285 | Saved
2021-06-04 13:51:38 | [train_policy] epoch #285 | Time 230.28 s
2021-06-04 13:51:38 | [train_policy] epoch #285 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286242
Evaluation/AverageDiscountedReturn          -43.1453
Evaluation/AverageReturn                    -43.1453
Evaluation/CompletionRate                     0
Evaluation/Iteration                        285
Evaluation/MaxReturn                        -29.4544
Evaluation/MinReturn                        -84.6574
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.44298
Extras/EpisodeRewardMean                    -43.5589
LinearFeatureBaseline/ExplainedVariance       0.876815
PolicyExecTime                                0.2339
ProcessExecTime                               0.0313232
TotalEnvSteps                            289432
policy/Entropy                               -0.439295
policy/KL                                     0.00973472
policy/KLBefore                               0
policy/LossAfter                             -0.0265995
policy/LossBefore                            -1.17796e-10
policy/Perplexity                             0.644491
policy/dLoss                                  0.0265995
---------------------------------------  ----------------
2021-06-04 13:51:38 | [train_policy] epoch #286 | Obtaining samples for iteration 286...
2021-06-04 13:51:39 | [train_policy] epoch #286 | Logging diagnostics...
2021-06-04 13:51:39 | [train_policy] epoch #286 | Optimizing policy...
2021-06-04 13:51:39 | [train_policy] epoch #286 | Computing loss before
2021-06-04 13:51:39 | [train_policy] epoch #286 | Computing KL before
2021-06-04 13:51:39 | [train_policy] epoch #286 | Optimizing
2021-06-04 13:51:39 | [train_policy] epoch #286 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:39 | [train_policy] epoch #286 | computing loss before
2021-06-04 13:51:39 | [train_policy] epoch #286 | computing gradient
2021-06-04 13:51:39 | [train_policy] epoch #286 | gradient computed
2021-06-04 13:51:39 | [train_policy] epoch #286 | computing descent direction
2021-06-04 13:51:39 | [train_policy] epoch #286 | descent direction computed
2021-06-04 13:51:39 | [train_policy] epoch #286 | backtrack iters: 0
2021-06-04 13:51:39 | [train_policy] epoch #286 | optimization finished
2021-06-04 13:51:39 | [train_policy] epoch #286 | Computing KL after
2021-06-04 13:51:39 | [train_policy] epoch #286 | Computing loss after
2021-06-04 13:51:39 | [train_policy] epoch #286 | Fitting baseline...
2021-06-04 13:51:39 | [train_policy] epoch #286 | Saving snapshot...
2021-06-04 13:51:39 | [train_policy] epoch #286 | Saved
2021-06-04 13:51:39 | [train_policy] epoch #286 | Time 231.07 s
2021-06-04 13:51:39 | [train_policy] epoch #286 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285487
Evaluation/AverageDiscountedReturn          -42.5625
Evaluation/AverageReturn                    -42.5625
Evaluation/CompletionRate                     0
Evaluation/Iteration                        286
Evaluation/MaxReturn                        -29.0951
Evaluation/MinReturn                        -64.0673
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.37803
Extras/EpisodeRewardMean                    -42.5414
LinearFeatureBaseline/ExplainedVariance       0.923372
PolicyExecTime                                0.222401
ProcessExecTime                               0.0312321
TotalEnvSteps                            290444
policy/Entropy                               -0.3866
policy/KL                                     0.00994981
policy/KLBefore                               0
policy/LossAfter                             -0.0218876
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.679363
policy/dLoss                                  0.0218876
---------------------------------------  ----------------
2021-06-04 13:51:39 | [train_policy] epoch #287 | Obtaining samples for iteration 287...
2021-06-04 13:51:39 | [train_policy] epoch #287 | Logging diagnostics...
2021-06-04 13:51:39 | [train_policy] epoch #287 | Optimizing policy...
2021-06-04 13:51:39 | [train_policy] epoch #287 | Computing loss before
2021-06-04 13:51:39 | [train_policy] epoch #287 | Computing KL before
2021-06-04 13:51:39 | [train_policy] epoch #287 | Optimizing
2021-06-04 13:51:39 | [train_policy] epoch #287 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:39 | [train_policy] epoch #287 | computing loss before
2021-06-04 13:51:39 | [train_policy] epoch #287 | computing gradient
2021-06-04 13:51:39 | [train_policy] epoch #287 | gradient computed
2021-06-04 13:51:39 | [train_policy] epoch #287 | computing descent direction
2021-06-04 13:51:40 | [train_policy] epoch #287 | descent direction computed
2021-06-04 13:51:40 | [train_policy] epoch #287 | backtrack iters: 0
2021-06-04 13:51:40 | [train_policy] epoch #287 | optimization finished
2021-06-04 13:51:40 | [train_policy] epoch #287 | Computing KL after
2021-06-04 13:51:40 | [train_policy] epoch #287 | Computing loss after
2021-06-04 13:51:40 | [train_policy] epoch #287 | Fitting baseline...
2021-06-04 13:51:40 | [train_policy] epoch #287 | Saving snapshot...
2021-06-04 13:51:40 | [train_policy] epoch #287 | Saved
2021-06-04 13:51:40 | [train_policy] epoch #287 | Time 231.84 s
2021-06-04 13:51:40 | [train_policy] epoch #287 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.285205
Evaluation/AverageDiscountedReturn          -44.7825
Evaluation/AverageReturn                    -44.7825
Evaluation/CompletionRate                     0
Evaluation/Iteration                        287
Evaluation/MaxReturn                        -35.1599
Evaluation/MinReturn                        -64.6537
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.89227
Extras/EpisodeRewardMean                    -45.0587
LinearFeatureBaseline/ExplainedVariance       0.926063
PolicyExecTime                                0.211908
ProcessExecTime                               0.0312266
TotalEnvSteps                            291456
policy/Entropy                               -0.325015
policy/KL                                     0.00951235
policy/KLBefore                               0
policy/LossAfter                             -0.0194173
policy/LossBefore                             2.40303e-08
policy/Perplexity                             0.722517
policy/dLoss                                  0.0194173
---------------------------------------  ----------------
2021-06-04 13:51:40 | [train_policy] epoch #288 | Obtaining samples for iteration 288...
2021-06-04 13:51:40 | [train_policy] epoch #288 | Logging diagnostics...
2021-06-04 13:51:40 | [train_policy] epoch #288 | Optimizing policy...
2021-06-04 13:51:40 | [train_policy] epoch #288 | Computing loss before
2021-06-04 13:51:40 | [train_policy] epoch #288 | Computing KL before
2021-06-04 13:51:40 | [train_policy] epoch #288 | Optimizing
2021-06-04 13:51:40 | [train_policy] epoch #288 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:40 | [train_policy] epoch #288 | computing loss before
2021-06-04 13:51:40 | [train_policy] epoch #288 | computing gradient
2021-06-04 13:51:40 | [train_policy] epoch #288 | gradient computed
2021-06-04 13:51:40 | [train_policy] epoch #288 | computing descent direction
2021-06-04 13:51:40 | [train_policy] epoch #288 | descent direction computed
2021-06-04 13:51:40 | [train_policy] epoch #288 | backtrack iters: 1
2021-06-04 13:51:40 | [train_policy] epoch #288 | optimization finished
2021-06-04 13:51:40 | [train_policy] epoch #288 | Computing KL after
2021-06-04 13:51:40 | [train_policy] epoch #288 | Computing loss after
2021-06-04 13:51:40 | [train_policy] epoch #288 | Fitting baseline...
2021-06-04 13:51:40 | [train_policy] epoch #288 | Saving snapshot...
2021-06-04 13:51:40 | [train_policy] epoch #288 | Saved
2021-06-04 13:51:40 | [train_policy] epoch #288 | Time 232.65 s
2021-06-04 13:51:40 | [train_policy] epoch #288 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286388
Evaluation/AverageDiscountedReturn          -43.1594
Evaluation/AverageReturn                    -43.1594
Evaluation/CompletionRate                     0
Evaluation/Iteration                        288
Evaluation/MaxReturn                        -28.8118
Evaluation/MinReturn                       -130.173
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.302
Extras/EpisodeRewardMean                    -43.1771
LinearFeatureBaseline/ExplainedVariance       0.703224
PolicyExecTime                                0.236391
ProcessExecTime                               0.0312204
TotalEnvSteps                            292468
policy/Entropy                               -0.36315
policy/KL                                     0.00758956
policy/KLBefore                               0
policy/LossAfter                             -0.0209741
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.695482
policy/dLoss                                  0.0209741
---------------------------------------  ----------------
2021-06-04 13:51:40 | [train_policy] epoch #289 | Obtaining samples for iteration 289...
2021-06-04 13:51:41 | [train_policy] epoch #289 | Logging diagnostics...
2021-06-04 13:51:41 | [train_policy] epoch #289 | Optimizing policy...
2021-06-04 13:51:41 | [train_policy] epoch #289 | Computing loss before
2021-06-04 13:51:41 | [train_policy] epoch #289 | Computing KL before
2021-06-04 13:51:41 | [train_policy] epoch #289 | Optimizing
2021-06-04 13:51:41 | [train_policy] epoch #289 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:41 | [train_policy] epoch #289 | computing loss before
2021-06-04 13:51:41 | [train_policy] epoch #289 | computing gradient
2021-06-04 13:51:41 | [train_policy] epoch #289 | gradient computed
2021-06-04 13:51:41 | [train_policy] epoch #289 | computing descent direction
2021-06-04 13:51:41 | [train_policy] epoch #289 | descent direction computed
2021-06-04 13:51:41 | [train_policy] epoch #289 | backtrack iters: 1
2021-06-04 13:51:41 | [train_policy] epoch #289 | optimization finished
2021-06-04 13:51:41 | [train_policy] epoch #289 | Computing KL after
2021-06-04 13:51:41 | [train_policy] epoch #289 | Computing loss after
2021-06-04 13:51:41 | [train_policy] epoch #289 | Fitting baseline...
2021-06-04 13:51:41 | [train_policy] epoch #289 | Saving snapshot...
2021-06-04 13:51:41 | [train_policy] epoch #289 | Saved
2021-06-04 13:51:41 | [train_policy] epoch #289 | Time 233.46 s
2021-06-04 13:51:41 | [train_policy] epoch #289 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286931
Evaluation/AverageDiscountedReturn          -43.4182
Evaluation/AverageReturn                    -43.4182
Evaluation/CompletionRate                     0
Evaluation/Iteration                        289
Evaluation/MaxReturn                        -31.0597
Evaluation/MinReturn                        -65.3466
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.49076
Extras/EpisodeRewardMean                    -43.3401
LinearFeatureBaseline/ExplainedVariance       0.903578
PolicyExecTime                                0.225803
ProcessExecTime                               0.031388
TotalEnvSteps                            293480
policy/Entropy                               -0.439009
policy/KL                                     0.00683799
policy/KLBefore                               0
policy/LossAfter                             -0.0131805
policy/LossBefore                             1.10728e-08
policy/Perplexity                             0.644675
policy/dLoss                                  0.0131805
---------------------------------------  ----------------
2021-06-04 13:51:41 | [train_policy] epoch #290 | Obtaining samples for iteration 290...
2021-06-04 13:51:42 | [train_policy] epoch #290 | Logging diagnostics...
2021-06-04 13:51:42 | [train_policy] epoch #290 | Optimizing policy...
2021-06-04 13:51:42 | [train_policy] epoch #290 | Computing loss before
2021-06-04 13:51:42 | [train_policy] epoch #290 | Computing KL before
2021-06-04 13:51:42 | [train_policy] epoch #290 | Optimizing
2021-06-04 13:51:42 | [train_policy] epoch #290 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:42 | [train_policy] epoch #290 | computing loss before
2021-06-04 13:51:42 | [train_policy] epoch #290 | computing gradient
2021-06-04 13:51:42 | [train_policy] epoch #290 | gradient computed
2021-06-04 13:51:42 | [train_policy] epoch #290 | computing descent direction
2021-06-04 13:51:42 | [train_policy] epoch #290 | descent direction computed
2021-06-04 13:51:42 | [train_policy] epoch #290 | backtrack iters: 1
2021-06-04 13:51:42 | [train_policy] epoch #290 | optimization finished
2021-06-04 13:51:42 | [train_policy] epoch #290 | Computing KL after
2021-06-04 13:51:42 | [train_policy] epoch #290 | Computing loss after
2021-06-04 13:51:42 | [train_policy] epoch #290 | Fitting baseline...
2021-06-04 13:51:42 | [train_policy] epoch #290 | Saving snapshot...
2021-06-04 13:51:42 | [train_policy] epoch #290 | Saved
2021-06-04 13:51:42 | [train_policy] epoch #290 | Time 234.27 s
2021-06-04 13:51:42 | [train_policy] epoch #290 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.28949
Evaluation/AverageDiscountedReturn          -43.5465
Evaluation/AverageReturn                    -43.5465
Evaluation/CompletionRate                     0
Evaluation/Iteration                        290
Evaluation/MaxReturn                        -29.8331
Evaluation/MinReturn                        -95.7268
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.62926
Extras/EpisodeRewardMean                    -43.38
LinearFeatureBaseline/ExplainedVariance       0.843493
PolicyExecTime                                0.220719
ProcessExecTime                               0.0315328
TotalEnvSteps                            294492
policy/Entropy                               -0.472326
policy/KL                                     0.00662944
policy/KLBefore                               0
policy/LossAfter                             -0.0234247
policy/LossBefore                            -1.93185e-08
policy/Perplexity                             0.62355
policy/dLoss                                  0.0234247
---------------------------------------  ----------------
2021-06-04 13:51:42 | [train_policy] epoch #291 | Obtaining samples for iteration 291...
2021-06-04 13:51:43 | [train_policy] epoch #291 | Logging diagnostics...
2021-06-04 13:51:43 | [train_policy] epoch #291 | Optimizing policy...
2021-06-04 13:51:43 | [train_policy] epoch #291 | Computing loss before
2021-06-04 13:51:43 | [train_policy] epoch #291 | Computing KL before
2021-06-04 13:51:43 | [train_policy] epoch #291 | Optimizing
2021-06-04 13:51:43 | [train_policy] epoch #291 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:43 | [train_policy] epoch #291 | computing loss before
2021-06-04 13:51:43 | [train_policy] epoch #291 | computing gradient
2021-06-04 13:51:43 | [train_policy] epoch #291 | gradient computed
2021-06-04 13:51:43 | [train_policy] epoch #291 | computing descent direction
2021-06-04 13:51:43 | [train_policy] epoch #291 | descent direction computed
2021-06-04 13:51:43 | [train_policy] epoch #291 | backtrack iters: 0
2021-06-04 13:51:43 | [train_policy] epoch #291 | optimization finished
2021-06-04 13:51:43 | [train_policy] epoch #291 | Computing KL after
2021-06-04 13:51:43 | [train_policy] epoch #291 | Computing loss after
2021-06-04 13:51:43 | [train_policy] epoch #291 | Fitting baseline...
2021-06-04 13:51:43 | [train_policy] epoch #291 | Saving snapshot...
2021-06-04 13:51:43 | [train_policy] epoch #291 | Saved
2021-06-04 13:51:43 | [train_policy] epoch #291 | Time 235.07 s
2021-06-04 13:51:43 | [train_policy] epoch #291 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287119
Evaluation/AverageDiscountedReturn          -44.2768
Evaluation/AverageReturn                    -44.2768
Evaluation/CompletionRate                     0
Evaluation/Iteration                        291
Evaluation/MaxReturn                        -29.9549
Evaluation/MinReturn                       -102.946
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.7776
Extras/EpisodeRewardMean                    -44.2459
LinearFeatureBaseline/ExplainedVariance       0.821776
PolicyExecTime                                0.232131
ProcessExecTime                               0.0314353
TotalEnvSteps                            295504
policy/Entropy                               -0.465038
policy/KL                                     0.00959308
policy/KLBefore                               0
policy/LossAfter                             -0.0288617
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.628111
policy/dLoss                                  0.0288616
---------------------------------------  ----------------
2021-06-04 13:51:43 | [train_policy] epoch #292 | Obtaining samples for iteration 292...
2021-06-04 13:51:43 | [train_policy] epoch #292 | Logging diagnostics...
2021-06-04 13:51:43 | [train_policy] epoch #292 | Optimizing policy...
2021-06-04 13:51:43 | [train_policy] epoch #292 | Computing loss before
2021-06-04 13:51:43 | [train_policy] epoch #292 | Computing KL before
2021-06-04 13:51:43 | [train_policy] epoch #292 | Optimizing
2021-06-04 13:51:44 | [train_policy] epoch #292 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:44 | [train_policy] epoch #292 | computing loss before
2021-06-04 13:51:44 | [train_policy] epoch #292 | computing gradient
2021-06-04 13:51:44 | [train_policy] epoch #292 | gradient computed
2021-06-04 13:51:44 | [train_policy] epoch #292 | computing descent direction
2021-06-04 13:51:44 | [train_policy] epoch #292 | descent direction computed
2021-06-04 13:51:44 | [train_policy] epoch #292 | backtrack iters: 0
2021-06-04 13:51:44 | [train_policy] epoch #292 | optimization finished
2021-06-04 13:51:44 | [train_policy] epoch #292 | Computing KL after
2021-06-04 13:51:44 | [train_policy] epoch #292 | Computing loss after
2021-06-04 13:51:44 | [train_policy] epoch #292 | Fitting baseline...
2021-06-04 13:51:44 | [train_policy] epoch #292 | Saving snapshot...
2021-06-04 13:51:44 | [train_policy] epoch #292 | Saved
2021-06-04 13:51:44 | [train_policy] epoch #292 | Time 235.87 s
2021-06-04 13:51:44 | [train_policy] epoch #292 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284991
Evaluation/AverageDiscountedReturn          -42.2786
Evaluation/AverageReturn                    -42.2786
Evaluation/CompletionRate                     0
Evaluation/Iteration                        292
Evaluation/MaxReturn                        -29.2762
Evaluation/MinReturn                        -64.2193
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.89787
Extras/EpisodeRewardMean                    -43.5658
LinearFeatureBaseline/ExplainedVariance       0.881498
PolicyExecTime                                0.223285
ProcessExecTime                               0.0311909
TotalEnvSteps                            296516
policy/Entropy                               -0.379494
policy/KL                                     0.0093924
policy/KLBefore                               0
policy/LossAfter                             -0.0222754
policy/LossBefore                             1.41355e-08
policy/Perplexity                             0.684208
policy/dLoss                                  0.0222754
---------------------------------------  ----------------
2021-06-04 13:51:44 | [train_policy] epoch #293 | Obtaining samples for iteration 293...
2021-06-04 13:51:44 | [train_policy] epoch #293 | Logging diagnostics...
2021-06-04 13:51:44 | [train_policy] epoch #293 | Optimizing policy...
2021-06-04 13:51:44 | [train_policy] epoch #293 | Computing loss before
2021-06-04 13:51:44 | [train_policy] epoch #293 | Computing KL before
2021-06-04 13:51:44 | [train_policy] epoch #293 | Optimizing
2021-06-04 13:51:44 | [train_policy] epoch #293 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:44 | [train_policy] epoch #293 | computing loss before
2021-06-04 13:51:44 | [train_policy] epoch #293 | computing gradient
2021-06-04 13:51:44 | [train_policy] epoch #293 | gradient computed
2021-06-04 13:51:44 | [train_policy] epoch #293 | computing descent direction
2021-06-04 13:51:44 | [train_policy] epoch #293 | descent direction computed
2021-06-04 13:51:44 | [train_policy] epoch #293 | backtrack iters: 1
2021-06-04 13:51:44 | [train_policy] epoch #293 | optimization finished
2021-06-04 13:51:44 | [train_policy] epoch #293 | Computing KL after
2021-06-04 13:51:44 | [train_policy] epoch #293 | Computing loss after
2021-06-04 13:51:44 | [train_policy] epoch #293 | Fitting baseline...
2021-06-04 13:51:44 | [train_policy] epoch #293 | Saving snapshot...
2021-06-04 13:51:44 | [train_policy] epoch #293 | Saved
2021-06-04 13:51:44 | [train_policy] epoch #293 | Time 236.67 s
2021-06-04 13:51:44 | [train_policy] epoch #293 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285366
Evaluation/AverageDiscountedReturn          -63.8012
Evaluation/AverageReturn                    -63.8012
Evaluation/CompletionRate                     0
Evaluation/Iteration                        293
Evaluation/MaxReturn                        -30.2858
Evaluation/MinReturn                      -2061.8
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.551
Extras/EpisodeRewardMean                    -61.9969
LinearFeatureBaseline/ExplainedVariance       0.0122952
PolicyExecTime                                0.228617
ProcessExecTime                               0.0311673
TotalEnvSteps                            297528
policy/Entropy                               -0.393256
policy/KL                                     0.00658252
policy/KLBefore                               0
policy/LossAfter                             -0.0205883
policy/LossBefore                            -4.00506e-08
policy/Perplexity                             0.674856
policy/dLoss                                  0.0205882
---------------------------------------  ----------------
2021-06-04 13:51:44 | [train_policy] epoch #294 | Obtaining samples for iteration 294...
2021-06-04 13:51:45 | [train_policy] epoch #294 | Logging diagnostics...
2021-06-04 13:51:45 | [train_policy] epoch #294 | Optimizing policy...
2021-06-04 13:51:45 | [train_policy] epoch #294 | Computing loss before
2021-06-04 13:51:45 | [train_policy] epoch #294 | Computing KL before
2021-06-04 13:51:45 | [train_policy] epoch #294 | Optimizing
2021-06-04 13:51:45 | [train_policy] epoch #294 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:45 | [train_policy] epoch #294 | computing loss before
2021-06-04 13:51:45 | [train_policy] epoch #294 | computing gradient
2021-06-04 13:51:45 | [train_policy] epoch #294 | gradient computed
2021-06-04 13:51:45 | [train_policy] epoch #294 | computing descent direction
2021-06-04 13:51:45 | [train_policy] epoch #294 | descent direction computed
2021-06-04 13:51:45 | [train_policy] epoch #294 | backtrack iters: 0
2021-06-04 13:51:45 | [train_policy] epoch #294 | optimization finished
2021-06-04 13:51:45 | [train_policy] epoch #294 | Computing KL after
2021-06-04 13:51:45 | [train_policy] epoch #294 | Computing loss after
2021-06-04 13:51:45 | [train_policy] epoch #294 | Fitting baseline...
2021-06-04 13:51:45 | [train_policy] epoch #294 | Saving snapshot...
2021-06-04 13:51:45 | [train_policy] epoch #294 | Saved
2021-06-04 13:51:45 | [train_policy] epoch #294 | Time 237.46 s
2021-06-04 13:51:45 | [train_policy] epoch #294 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284821
Evaluation/AverageDiscountedReturn          -64.5633
Evaluation/AverageReturn                    -64.5633
Evaluation/CompletionRate                     0
Evaluation/Iteration                        294
Evaluation/MaxReturn                        -28.6759
Evaluation/MinReturn                      -2061.68
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.488
Extras/EpisodeRewardMean                    -83.0699
LinearFeatureBaseline/ExplainedVariance       0.104381
PolicyExecTime                                0.227761
ProcessExecTime                               0.0312932
TotalEnvSteps                            298540
policy/Entropy                               -0.384302
policy/KL                                     0.00958434
policy/KLBefore                               0
policy/LossAfter                             -0.0352557
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.680926
policy/dLoss                                  0.0352557
---------------------------------------  ----------------
2021-06-04 13:51:45 | [train_policy] epoch #295 | Obtaining samples for iteration 295...
2021-06-04 13:51:46 | [train_policy] epoch #295 | Logging diagnostics...
2021-06-04 13:51:46 | [train_policy] epoch #295 | Optimizing policy...
2021-06-04 13:51:46 | [train_policy] epoch #295 | Computing loss before
2021-06-04 13:51:46 | [train_policy] epoch #295 | Computing KL before
2021-06-04 13:51:46 | [train_policy] epoch #295 | Optimizing
2021-06-04 13:51:46 | [train_policy] epoch #295 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:46 | [train_policy] epoch #295 | computing loss before
2021-06-04 13:51:46 | [train_policy] epoch #295 | computing gradient
2021-06-04 13:51:46 | [train_policy] epoch #295 | gradient computed
2021-06-04 13:51:46 | [train_policy] epoch #295 | computing descent direction
2021-06-04 13:51:46 | [train_policy] epoch #295 | descent direction computed
2021-06-04 13:51:46 | [train_policy] epoch #295 | backtrack iters: 1
2021-06-04 13:51:46 | [train_policy] epoch #295 | optimization finished
2021-06-04 13:51:46 | [train_policy] epoch #295 | Computing KL after
2021-06-04 13:51:46 | [train_policy] epoch #295 | Computing loss after
2021-06-04 13:51:46 | [train_policy] epoch #295 | Fitting baseline...
2021-06-04 13:51:46 | [train_policy] epoch #295 | Saving snapshot...
2021-06-04 13:51:46 | [train_policy] epoch #295 | Saved
2021-06-04 13:51:46 | [train_policy] epoch #295 | Time 238.29 s
2021-06-04 13:51:46 | [train_policy] epoch #295 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.285194
Evaluation/AverageDiscountedReturn          -64.5593
Evaluation/AverageReturn                    -64.5593
Evaluation/CompletionRate                     0
Evaluation/Iteration                        295
Evaluation/MaxReturn                        -29.1794
Evaluation/MinReturn                      -2062.13
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.596
Extras/EpisodeRewardMean                    -62.8738
LinearFeatureBaseline/ExplainedVariance       0.0890087
PolicyExecTime                                0.236336
ProcessExecTime                               0.0312836
TotalEnvSteps                            299552
policy/Entropy                               -0.42829
policy/KL                                     0.00676385
policy/KLBefore                               0
policy/LossAfter                             -0.0186399
policy/LossBefore                             6.12538e-09
policy/Perplexity                             0.651622
policy/dLoss                                  0.01864
---------------------------------------  ----------------
2021-06-04 13:51:46 | [train_policy] epoch #296 | Obtaining samples for iteration 296...
2021-06-04 13:51:47 | [train_policy] epoch #296 | Logging diagnostics...
2021-06-04 13:51:47 | [train_policy] epoch #296 | Optimizing policy...
2021-06-04 13:51:47 | [train_policy] epoch #296 | Computing loss before
2021-06-04 13:51:47 | [train_policy] epoch #296 | Computing KL before
2021-06-04 13:51:47 | [train_policy] epoch #296 | Optimizing
2021-06-04 13:51:47 | [train_policy] epoch #296 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:47 | [train_policy] epoch #296 | computing loss before
2021-06-04 13:51:47 | [train_policy] epoch #296 | computing gradient
2021-06-04 13:51:47 | [train_policy] epoch #296 | gradient computed
2021-06-04 13:51:47 | [train_policy] epoch #296 | computing descent direction
2021-06-04 13:51:47 | [train_policy] epoch #296 | descent direction computed
2021-06-04 13:51:47 | [train_policy] epoch #296 | backtrack iters: 1
2021-06-04 13:51:47 | [train_policy] epoch #296 | optimization finished
2021-06-04 13:51:47 | [train_policy] epoch #296 | Computing KL after
2021-06-04 13:51:47 | [train_policy] epoch #296 | Computing loss after
2021-06-04 13:51:47 | [train_policy] epoch #296 | Fitting baseline...
2021-06-04 13:51:47 | [train_policy] epoch #296 | Saving snapshot...
2021-06-04 13:51:47 | [train_policy] epoch #296 | Saved
2021-06-04 13:51:47 | [train_policy] epoch #296 | Time 239.08 s
2021-06-04 13:51:47 | [train_policy] epoch #296 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285115
Evaluation/AverageDiscountedReturn          -43.6929
Evaluation/AverageReturn                    -43.6929
Evaluation/CompletionRate                     0
Evaluation/Iteration                        296
Evaluation/MaxReturn                        -29.4736
Evaluation/MinReturn                       -107.059
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.84549
Extras/EpisodeRewardMean                    -44.003
LinearFeatureBaseline/ExplainedVariance     -16.5715
PolicyExecTime                                0.224255
ProcessExecTime                               0.0311944
TotalEnvSteps                            300564
policy/Entropy                               -0.409549
policy/KL                                     0.00661103
policy/KLBefore                               0
policy/LossAfter                             -0.0281391
policy/LossBefore                            -1.13084e-08
policy/Perplexity                             0.66395
policy/dLoss                                  0.0281391
---------------------------------------  ----------------
2021-06-04 13:51:47 | [train_policy] epoch #297 | Obtaining samples for iteration 297...
2021-06-04 13:51:48 | [train_policy] epoch #297 | Logging diagnostics...
2021-06-04 13:51:48 | [train_policy] epoch #297 | Optimizing policy...
2021-06-04 13:51:48 | [train_policy] epoch #297 | Computing loss before
2021-06-04 13:51:48 | [train_policy] epoch #297 | Computing KL before
2021-06-04 13:51:48 | [train_policy] epoch #297 | Optimizing
2021-06-04 13:51:48 | [train_policy] epoch #297 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:48 | [train_policy] epoch #297 | computing loss before
2021-06-04 13:51:48 | [train_policy] epoch #297 | computing gradient
2021-06-04 13:51:48 | [train_policy] epoch #297 | gradient computed
2021-06-04 13:51:48 | [train_policy] epoch #297 | computing descent direction
2021-06-04 13:51:48 | [train_policy] epoch #297 | descent direction computed
2021-06-04 13:51:48 | [train_policy] epoch #297 | backtrack iters: 1
2021-06-04 13:51:48 | [train_policy] epoch #297 | optimization finished
2021-06-04 13:51:48 | [train_policy] epoch #297 | Computing KL after
2021-06-04 13:51:48 | [train_policy] epoch #297 | Computing loss after
2021-06-04 13:51:48 | [train_policy] epoch #297 | Fitting baseline...
2021-06-04 13:51:48 | [train_policy] epoch #297 | Saving snapshot...
2021-06-04 13:51:48 | [train_policy] epoch #297 | Saved
2021-06-04 13:51:48 | [train_policy] epoch #297 | Time 239.90 s
2021-06-04 13:51:48 | [train_policy] epoch #297 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284921
Evaluation/AverageDiscountedReturn          -64.6771
Evaluation/AverageReturn                    -64.6771
Evaluation/CompletionRate                     0
Evaluation/Iteration                        297
Evaluation/MaxReturn                        -30.1562
Evaluation/MinReturn                      -2061.99
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.467
Extras/EpisodeRewardMean                    -63.2082
LinearFeatureBaseline/ExplainedVariance       0.0166032
PolicyExecTime                                0.234036
ProcessExecTime                               0.0312669
TotalEnvSteps                            301576
policy/Entropy                               -0.421049
policy/KL                                     0.0069296
policy/KLBefore                               0
policy/LossAfter                             -0.022587
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.656358
policy/dLoss                                  0.022587
---------------------------------------  ----------------
2021-06-04 13:51:48 | [train_policy] epoch #298 | Obtaining samples for iteration 298...
2021-06-04 13:51:48 | [train_policy] epoch #298 | Logging diagnostics...
2021-06-04 13:51:48 | [train_policy] epoch #298 | Optimizing policy...
2021-06-04 13:51:48 | [train_policy] epoch #298 | Computing loss before
2021-06-04 13:51:48 | [train_policy] epoch #298 | Computing KL before
2021-06-04 13:51:48 | [train_policy] epoch #298 | Optimizing
2021-06-04 13:51:48 | [train_policy] epoch #298 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:48 | [train_policy] epoch #298 | computing loss before
2021-06-04 13:51:48 | [train_policy] epoch #298 | computing gradient
2021-06-04 13:51:48 | [train_policy] epoch #298 | gradient computed
2021-06-04 13:51:48 | [train_policy] epoch #298 | computing descent direction
2021-06-04 13:51:48 | [train_policy] epoch #298 | descent direction computed
2021-06-04 13:51:48 | [train_policy] epoch #298 | backtrack iters: 0
2021-06-04 13:51:48 | [train_policy] epoch #298 | optimization finished
2021-06-04 13:51:48 | [train_policy] epoch #298 | Computing KL after
2021-06-04 13:51:48 | [train_policy] epoch #298 | Computing loss after
2021-06-04 13:51:48 | [train_policy] epoch #298 | Fitting baseline...
2021-06-04 13:51:48 | [train_policy] epoch #298 | Saving snapshot...
2021-06-04 13:51:48 | [train_policy] epoch #298 | Saved
2021-06-04 13:51:48 | [train_policy] epoch #298 | Time 240.69 s
2021-06-04 13:51:48 | [train_policy] epoch #298 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.284505
Evaluation/AverageDiscountedReturn          -44.2125
Evaluation/AverageReturn                    -44.2125
Evaluation/CompletionRate                     0
Evaluation/Iteration                        298
Evaluation/MaxReturn                        -33.6594
Evaluation/MinReturn                        -89.8947
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.51039
Extras/EpisodeRewardMean                    -43.9906
LinearFeatureBaseline/ExplainedVariance     -41.3838
PolicyExecTime                                0.227666
ProcessExecTime                               0.0311704
TotalEnvSteps                            302588
policy/Entropy                               -0.382611
policy/KL                                     0.00966376
policy/KLBefore                               0
policy/LossAfter                             -0.0386618
policy/LossBefore                            -1.2133e-08
policy/Perplexity                             0.682078
policy/dLoss                                  0.0386618
---------------------------------------  ---------------
2021-06-04 13:51:48 | [train_policy] epoch #299 | Obtaining samples for iteration 299...
2021-06-04 13:51:49 | [train_policy] epoch #299 | Logging diagnostics...
2021-06-04 13:51:49 | [train_policy] epoch #299 | Optimizing policy...
2021-06-04 13:51:49 | [train_policy] epoch #299 | Computing loss before
2021-06-04 13:51:49 | [train_policy] epoch #299 | Computing KL before
2021-06-04 13:51:49 | [train_policy] epoch #299 | Optimizing
2021-06-04 13:51:49 | [train_policy] epoch #299 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:49 | [train_policy] epoch #299 | computing loss before
2021-06-04 13:51:49 | [train_policy] epoch #299 | computing gradient
2021-06-04 13:51:49 | [train_policy] epoch #299 | gradient computed
2021-06-04 13:51:49 | [train_policy] epoch #299 | computing descent direction
2021-06-04 13:51:49 | [train_policy] epoch #299 | descent direction computed
2021-06-04 13:51:49 | [train_policy] epoch #299 | backtrack iters: 0
2021-06-04 13:51:49 | [train_policy] epoch #299 | optimization finished
2021-06-04 13:51:49 | [train_policy] epoch #299 | Computing KL after
2021-06-04 13:51:49 | [train_policy] epoch #299 | Computing loss after
2021-06-04 13:51:49 | [train_policy] epoch #299 | Fitting baseline...
2021-06-04 13:51:49 | [train_policy] epoch #299 | Saving snapshot...
2021-06-04 13:51:49 | [train_policy] epoch #299 | Saved
2021-06-04 13:51:49 | [train_policy] epoch #299 | Time 241.48 s
2021-06-04 13:51:49 | [train_policy] epoch #299 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285379
Evaluation/AverageDiscountedReturn          -43.6795
Evaluation/AverageReturn                    -43.6795
Evaluation/CompletionRate                     0
Evaluation/Iteration                        299
Evaluation/MaxReturn                        -30.0348
Evaluation/MinReturn                        -65.5453
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83611
Extras/EpisodeRewardMean                    -43.3497
LinearFeatureBaseline/ExplainedVariance       0.900029
PolicyExecTime                                0.223439
ProcessExecTime                               0.0312014
TotalEnvSteps                            303600
policy/Entropy                               -0.3622
policy/KL                                     0.00978309
policy/KLBefore                               0
policy/LossAfter                             -0.0193938
policy/LossBefore                            -1.01304e-08
policy/Perplexity                             0.696143
policy/dLoss                                  0.0193938
---------------------------------------  ----------------
2021-06-04 13:51:49 | [train_policy] epoch #300 | Obtaining samples for iteration 300...
2021-06-04 13:51:50 | [train_policy] epoch #300 | Logging diagnostics...
2021-06-04 13:51:50 | [train_policy] epoch #300 | Optimizing policy...
2021-06-04 13:51:50 | [train_policy] epoch #300 | Computing loss before
2021-06-04 13:51:50 | [train_policy] epoch #300 | Computing KL before
2021-06-04 13:51:50 | [train_policy] epoch #300 | Optimizing
2021-06-04 13:51:50 | [train_policy] epoch #300 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:50 | [train_policy] epoch #300 | computing loss before
2021-06-04 13:51:50 | [train_policy] epoch #300 | computing gradient
2021-06-04 13:51:50 | [train_policy] epoch #300 | gradient computed
2021-06-04 13:51:50 | [train_policy] epoch #300 | computing descent direction
2021-06-04 13:51:50 | [train_policy] epoch #300 | descent direction computed
2021-06-04 13:51:50 | [train_policy] epoch #300 | backtrack iters: 1
2021-06-04 13:51:50 | [train_policy] epoch #300 | optimization finished
2021-06-04 13:51:50 | [train_policy] epoch #300 | Computing KL after
2021-06-04 13:51:50 | [train_policy] epoch #300 | Computing loss after
2021-06-04 13:51:50 | [train_policy] epoch #300 | Fitting baseline...
2021-06-04 13:51:50 | [train_policy] epoch #300 | Saving snapshot...
2021-06-04 13:51:50 | [train_policy] epoch #300 | Saved
2021-06-04 13:51:50 | [train_policy] epoch #300 | Time 242.29 s
2021-06-04 13:51:50 | [train_policy] epoch #300 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285661
Evaluation/AverageDiscountedReturn          -44.7325
Evaluation/AverageReturn                    -44.7325
Evaluation/CompletionRate                     0
Evaluation/Iteration                        300
Evaluation/MaxReturn                        -29.5069
Evaluation/MinReturn                       -139.57
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         13.2041
Extras/EpisodeRewardMean                    -44.6005
LinearFeatureBaseline/ExplainedVariance       0.673928
PolicyExecTime                                0.219956
ProcessExecTime                               0.0311112
TotalEnvSteps                            304612
policy/Entropy                               -0.372187
policy/KL                                     0.00669059
policy/KLBefore                               0
policy/LossAfter                             -0.0220724
policy/LossBefore                             2.12032e-09
policy/Perplexity                             0.689226
policy/dLoss                                  0.0220724
---------------------------------------  ----------------
2021-06-04 13:51:50 | [train_policy] epoch #301 | Obtaining samples for iteration 301...
2021-06-04 13:51:51 | [train_policy] epoch #301 | Logging diagnostics...
2021-06-04 13:51:51 | [train_policy] epoch #301 | Optimizing policy...
2021-06-04 13:51:51 | [train_policy] epoch #301 | Computing loss before
2021-06-04 13:51:51 | [train_policy] epoch #301 | Computing KL before
2021-06-04 13:51:51 | [train_policy] epoch #301 | Optimizing
2021-06-04 13:51:51 | [train_policy] epoch #301 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:51 | [train_policy] epoch #301 | computing loss before
2021-06-04 13:51:51 | [train_policy] epoch #301 | computing gradient
2021-06-04 13:51:51 | [train_policy] epoch #301 | gradient computed
2021-06-04 13:51:51 | [train_policy] epoch #301 | computing descent direction
2021-06-04 13:51:51 | [train_policy] epoch #301 | descent direction computed
2021-06-04 13:51:51 | [train_policy] epoch #301 | backtrack iters: 1
2021-06-04 13:51:51 | [train_policy] epoch #301 | optimization finished
2021-06-04 13:51:51 | [train_policy] epoch #301 | Computing KL after
2021-06-04 13:51:51 | [train_policy] epoch #301 | Computing loss after
2021-06-04 13:51:51 | [train_policy] epoch #301 | Fitting baseline...
2021-06-04 13:51:51 | [train_policy] epoch #301 | Saving snapshot...
2021-06-04 13:51:51 | [train_policy] epoch #301 | Saved
2021-06-04 13:51:51 | [train_policy] epoch #301 | Time 243.09 s
2021-06-04 13:51:51 | [train_policy] epoch #301 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288781
Evaluation/AverageDiscountedReturn          -65.723
Evaluation/AverageReturn                    -65.723
Evaluation/CompletionRate                     0
Evaluation/Iteration                        301
Evaluation/MaxReturn                        -29.313
Evaluation/MinReturn                      -2062.31
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.508
Extras/EpisodeRewardMean                    -63.8574
LinearFeatureBaseline/ExplainedVariance       0.0205126
PolicyExecTime                                0.22337
ProcessExecTime                               0.0315723
TotalEnvSteps                            305624
policy/Entropy                               -0.416296
policy/KL                                     0.00669688
policy/KLBefore                               0
policy/LossAfter                             -0.0344184
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.659485
policy/dLoss                                  0.0344184
---------------------------------------  ----------------
2021-06-04 13:51:51 | [train_policy] epoch #302 | Obtaining samples for iteration 302...
2021-06-04 13:51:52 | [train_policy] epoch #302 | Logging diagnostics...
2021-06-04 13:51:52 | [train_policy] epoch #302 | Optimizing policy...
2021-06-04 13:51:52 | [train_policy] epoch #302 | Computing loss before
2021-06-04 13:51:52 | [train_policy] epoch #302 | Computing KL before
2021-06-04 13:51:52 | [train_policy] epoch #302 | Optimizing
2021-06-04 13:51:52 | [train_policy] epoch #302 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:52 | [train_policy] epoch #302 | computing loss before
2021-06-04 13:51:52 | [train_policy] epoch #302 | computing gradient
2021-06-04 13:51:52 | [train_policy] epoch #302 | gradient computed
2021-06-04 13:51:52 | [train_policy] epoch #302 | computing descent direction
2021-06-04 13:51:52 | [train_policy] epoch #302 | descent direction computed
2021-06-04 13:51:52 | [train_policy] epoch #302 | backtrack iters: 0
2021-06-04 13:51:52 | [train_policy] epoch #302 | optimization finished
2021-06-04 13:51:52 | [train_policy] epoch #302 | Computing KL after
2021-06-04 13:51:52 | [train_policy] epoch #302 | Computing loss after
2021-06-04 13:51:52 | [train_policy] epoch #302 | Fitting baseline...
2021-06-04 13:51:52 | [train_policy] epoch #302 | Saving snapshot...
2021-06-04 13:51:52 | [train_policy] epoch #302 | Saved
2021-06-04 13:51:52 | [train_policy] epoch #302 | Time 243.89 s
2021-06-04 13:51:52 | [train_policy] epoch #302 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285533
Evaluation/AverageDiscountedReturn          -42.4665
Evaluation/AverageReturn                    -42.4665
Evaluation/CompletionRate                     0
Evaluation/Iteration                        302
Evaluation/MaxReturn                        -29.4393
Evaluation/MinReturn                        -63.9215
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.97637
Extras/EpisodeRewardMean                    -42.6199
LinearFeatureBaseline/ExplainedVariance     -14.8402
PolicyExecTime                                0.233732
ProcessExecTime                               0.0311658
TotalEnvSteps                            306636
policy/Entropy                               -0.388231
policy/KL                                     0.00976785
policy/KLBefore                               0
policy/LossAfter                             -0.0267294
policy/LossBefore                            -5.60708e-08
policy/Perplexity                             0.678256
policy/dLoss                                  0.0267294
---------------------------------------  ----------------
2021-06-04 13:51:52 | [train_policy] epoch #303 | Obtaining samples for iteration 303...
2021-06-04 13:51:52 | [train_policy] epoch #303 | Logging diagnostics...
2021-06-04 13:51:52 | [train_policy] epoch #303 | Optimizing policy...
2021-06-04 13:51:52 | [train_policy] epoch #303 | Computing loss before
2021-06-04 13:51:52 | [train_policy] epoch #303 | Computing KL before
2021-06-04 13:51:52 | [train_policy] epoch #303 | Optimizing
2021-06-04 13:51:52 | [train_policy] epoch #303 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:52 | [train_policy] epoch #303 | computing loss before
2021-06-04 13:51:52 | [train_policy] epoch #303 | computing gradient
2021-06-04 13:51:52 | [train_policy] epoch #303 | gradient computed
2021-06-04 13:51:52 | [train_policy] epoch #303 | computing descent direction
2021-06-04 13:51:52 | [train_policy] epoch #303 | descent direction computed
2021-06-04 13:51:52 | [train_policy] epoch #303 | backtrack iters: 1
2021-06-04 13:51:52 | [train_policy] epoch #303 | optimization finished
2021-06-04 13:51:52 | [train_policy] epoch #303 | Computing KL after
2021-06-04 13:51:52 | [train_policy] epoch #303 | Computing loss after
2021-06-04 13:51:52 | [train_policy] epoch #303 | Fitting baseline...
2021-06-04 13:51:52 | [train_policy] epoch #303 | Saving snapshot...
2021-06-04 13:51:52 | [train_policy] epoch #303 | Saved
2021-06-04 13:51:52 | [train_policy] epoch #303 | Time 244.68 s
2021-06-04 13:51:52 | [train_policy] epoch #303 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285519
Evaluation/AverageDiscountedReturn          -43.132
Evaluation/AverageReturn                    -43.132
Evaluation/CompletionRate                     0
Evaluation/Iteration                        303
Evaluation/MaxReturn                        -30.8989
Evaluation/MinReturn                        -64.1091
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.90272
Extras/EpisodeRewardMean                    -43.2948
LinearFeatureBaseline/ExplainedVariance       0.929011
PolicyExecTime                                0.22217
ProcessExecTime                               0.0312805
TotalEnvSteps                            307648
policy/Entropy                               -0.385643
policy/KL                                     0.00643247
policy/KLBefore                               0
policy/LossAfter                             -0.0173076
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.680013
policy/dLoss                                  0.0173076
---------------------------------------  ----------------
2021-06-04 13:51:52 | [train_policy] epoch #304 | Obtaining samples for iteration 304...
2021-06-04 13:51:53 | [train_policy] epoch #304 | Logging diagnostics...
2021-06-04 13:51:53 | [train_policy] epoch #304 | Optimizing policy...
2021-06-04 13:51:53 | [train_policy] epoch #304 | Computing loss before
2021-06-04 13:51:53 | [train_policy] epoch #304 | Computing KL before
2021-06-04 13:51:53 | [train_policy] epoch #304 | Optimizing
2021-06-04 13:51:53 | [train_policy] epoch #304 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:53 | [train_policy] epoch #304 | computing loss before
2021-06-04 13:51:53 | [train_policy] epoch #304 | computing gradient
2021-06-04 13:51:53 | [train_policy] epoch #304 | gradient computed
2021-06-04 13:51:53 | [train_policy] epoch #304 | computing descent direction
2021-06-04 13:51:53 | [train_policy] epoch #304 | descent direction computed
2021-06-04 13:51:53 | [train_policy] epoch #304 | backtrack iters: 1
2021-06-04 13:51:53 | [train_policy] epoch #304 | optimization finished
2021-06-04 13:51:53 | [train_policy] epoch #304 | Computing KL after
2021-06-04 13:51:53 | [train_policy] epoch #304 | Computing loss after
2021-06-04 13:51:53 | [train_policy] epoch #304 | Fitting baseline...
2021-06-04 13:51:53 | [train_policy] epoch #304 | Saving snapshot...
2021-06-04 13:51:53 | [train_policy] epoch #304 | Saved
2021-06-04 13:51:53 | [train_policy] epoch #304 | Time 245.46 s
2021-06-04 13:51:53 | [train_policy] epoch #304 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.286637
Evaluation/AverageDiscountedReturn          -42.9429
Evaluation/AverageReturn                    -42.9429
Evaluation/CompletionRate                     0
Evaluation/Iteration                        304
Evaluation/MaxReturn                        -29.6511
Evaluation/MinReturn                        -64.2707
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.15049
Extras/EpisodeRewardMean                    -43.0107
LinearFeatureBaseline/ExplainedVariance       0.904497
PolicyExecTime                                0.215163
ProcessExecTime                               0.0313351
TotalEnvSteps                            308660
policy/Entropy                               -0.430619
policy/KL                                     0.00656195
policy/KLBefore                               0
policy/LossAfter                             -0.0161333
policy/LossBefore                             4.47624e-09
policy/Perplexity                             0.650106
policy/dLoss                                  0.0161333
---------------------------------------  ----------------
2021-06-04 13:51:53 | [train_policy] epoch #305 | Obtaining samples for iteration 305...
2021-06-04 13:51:54 | [train_policy] epoch #305 | Logging diagnostics...
2021-06-04 13:51:54 | [train_policy] epoch #305 | Optimizing policy...
2021-06-04 13:51:54 | [train_policy] epoch #305 | Computing loss before
2021-06-04 13:51:54 | [train_policy] epoch #305 | Computing KL before
2021-06-04 13:51:54 | [train_policy] epoch #305 | Optimizing
2021-06-04 13:51:54 | [train_policy] epoch #305 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:54 | [train_policy] epoch #305 | computing loss before
2021-06-04 13:51:54 | [train_policy] epoch #305 | computing gradient
2021-06-04 13:51:54 | [train_policy] epoch #305 | gradient computed
2021-06-04 13:51:54 | [train_policy] epoch #305 | computing descent direction
2021-06-04 13:51:54 | [train_policy] epoch #305 | descent direction computed
2021-06-04 13:51:54 | [train_policy] epoch #305 | backtrack iters: 0
2021-06-04 13:51:54 | [train_policy] epoch #305 | optimization finished
2021-06-04 13:51:54 | [train_policy] epoch #305 | Computing KL after
2021-06-04 13:51:54 | [train_policy] epoch #305 | Computing loss after
2021-06-04 13:51:54 | [train_policy] epoch #305 | Fitting baseline...
2021-06-04 13:51:54 | [train_policy] epoch #305 | Saving snapshot...
2021-06-04 13:51:54 | [train_policy] epoch #305 | Saved
2021-06-04 13:51:54 | [train_policy] epoch #305 | Time 246.27 s
2021-06-04 13:51:54 | [train_policy] epoch #305 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.285622
Evaluation/AverageDiscountedReturn          -44.1032
Evaluation/AverageReturn                    -44.1032
Evaluation/CompletionRate                     0
Evaluation/Iteration                        305
Evaluation/MaxReturn                        -30.045
Evaluation/MinReturn                        -73.614
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.24517
Extras/EpisodeRewardMean                    -44.154
LinearFeatureBaseline/ExplainedVariance       0.901285
PolicyExecTime                                0.230932
ProcessExecTime                               0.0311391
TotalEnvSteps                            309672
policy/Entropy                               -0.438046
policy/KL                                     0.00990441
policy/KLBefore                               0
policy/LossAfter                             -0.0251761
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.645296
policy/dLoss                                  0.0251761
---------------------------------------  ---------------
2021-06-04 13:51:54 | [train_policy] epoch #306 | Obtaining samples for iteration 306...
2021-06-04 13:51:55 | [train_policy] epoch #306 | Logging diagnostics...
2021-06-04 13:51:55 | [train_policy] epoch #306 | Optimizing policy...
2021-06-04 13:51:55 | [train_policy] epoch #306 | Computing loss before
2021-06-04 13:51:55 | [train_policy] epoch #306 | Computing KL before
2021-06-04 13:51:55 | [train_policy] epoch #306 | Optimizing
2021-06-04 13:51:55 | [train_policy] epoch #306 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:55 | [train_policy] epoch #306 | computing loss before
2021-06-04 13:51:55 | [train_policy] epoch #306 | computing gradient
2021-06-04 13:51:55 | [train_policy] epoch #306 | gradient computed
2021-06-04 13:51:55 | [train_policy] epoch #306 | computing descent direction
2021-06-04 13:51:55 | [train_policy] epoch #306 | descent direction computed
2021-06-04 13:51:55 | [train_policy] epoch #306 | backtrack iters: 0
2021-06-04 13:51:55 | [train_policy] epoch #306 | optimization finished
2021-06-04 13:51:55 | [train_policy] epoch #306 | Computing KL after
2021-06-04 13:51:55 | [train_policy] epoch #306 | Computing loss after
2021-06-04 13:51:55 | [train_policy] epoch #306 | Fitting baseline...
2021-06-04 13:51:55 | [train_policy] epoch #306 | Saving snapshot...
2021-06-04 13:51:55 | [train_policy] epoch #306 | Saved
2021-06-04 13:51:55 | [train_policy] epoch #306 | Time 247.07 s
2021-06-04 13:51:55 | [train_policy] epoch #306 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286522
Evaluation/AverageDiscountedReturn          -42.4432
Evaluation/AverageReturn                    -42.4432
Evaluation/CompletionRate                     0
Evaluation/Iteration                        306
Evaluation/MaxReturn                        -29.4095
Evaluation/MinReturn                        -83.6684
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.11122
Extras/EpisodeRewardMean                    -42.9844
LinearFeatureBaseline/ExplainedVariance       0.847428
PolicyExecTime                                0.23035
ProcessExecTime                               0.031379
TotalEnvSteps                            310684
policy/Entropy                               -0.448002
policy/KL                                     0.00964304
policy/KLBefore                               0
policy/LossAfter                             -0.0393541
policy/LossBefore                             1.41355e-08
policy/Perplexity                             0.638904
policy/dLoss                                  0.0393541
---------------------------------------  ----------------
2021-06-04 13:51:55 | [train_policy] epoch #307 | Obtaining samples for iteration 307...
2021-06-04 13:51:55 | [train_policy] epoch #307 | Logging diagnostics...
2021-06-04 13:51:55 | [train_policy] epoch #307 | Optimizing policy...
2021-06-04 13:51:55 | [train_policy] epoch #307 | Computing loss before
2021-06-04 13:51:55 | [train_policy] epoch #307 | Computing KL before
2021-06-04 13:51:56 | [train_policy] epoch #307 | Optimizing
2021-06-04 13:51:56 | [train_policy] epoch #307 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:56 | [train_policy] epoch #307 | computing loss before
2021-06-04 13:51:56 | [train_policy] epoch #307 | computing gradient
2021-06-04 13:51:56 | [train_policy] epoch #307 | gradient computed
2021-06-04 13:51:56 | [train_policy] epoch #307 | computing descent direction
2021-06-04 13:51:56 | [train_policy] epoch #307 | descent direction computed
2021-06-04 13:51:56 | [train_policy] epoch #307 | backtrack iters: 1
2021-06-04 13:51:56 | [train_policy] epoch #307 | optimization finished
2021-06-04 13:51:56 | [train_policy] epoch #307 | Computing KL after
2021-06-04 13:51:56 | [train_policy] epoch #307 | Computing loss after
2021-06-04 13:51:56 | [train_policy] epoch #307 | Fitting baseline...
2021-06-04 13:51:56 | [train_policy] epoch #307 | Saving snapshot...
2021-06-04 13:51:56 | [train_policy] epoch #307 | Saved
2021-06-04 13:51:56 | [train_policy] epoch #307 | Time 247.86 s
2021-06-04 13:51:56 | [train_policy] epoch #307 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286667
Evaluation/AverageDiscountedReturn          -42.8576
Evaluation/AverageReturn                    -42.8576
Evaluation/CompletionRate                     0
Evaluation/Iteration                        307
Evaluation/MaxReturn                        -30.239
Evaluation/MinReturn                        -64.2044
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.96948
Extras/EpisodeRewardMean                    -42.6239
LinearFeatureBaseline/ExplainedVariance       0.915728
PolicyExecTime                                0.230282
ProcessExecTime                               0.0313361
TotalEnvSteps                            311696
policy/Entropy                               -0.502443
policy/KL                                     0.00692801
policy/KLBefore                               0
policy/LossAfter                             -0.0155621
policy/LossBefore                            -1.95541e-08
policy/Perplexity                             0.605051
policy/dLoss                                  0.0155621
---------------------------------------  ----------------
2021-06-04 13:51:56 | [train_policy] epoch #308 | Obtaining samples for iteration 308...
2021-06-04 13:51:56 | [train_policy] epoch #308 | Logging diagnostics...
2021-06-04 13:51:56 | [train_policy] epoch #308 | Optimizing policy...
2021-06-04 13:51:56 | [train_policy] epoch #308 | Computing loss before
2021-06-04 13:51:56 | [train_policy] epoch #308 | Computing KL before
2021-06-04 13:51:56 | [train_policy] epoch #308 | Optimizing
2021-06-04 13:51:56 | [train_policy] epoch #308 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:56 | [train_policy] epoch #308 | computing loss before
2021-06-04 13:51:56 | [train_policy] epoch #308 | computing gradient
2021-06-04 13:51:56 | [train_policy] epoch #308 | gradient computed
2021-06-04 13:51:56 | [train_policy] epoch #308 | computing descent direction
2021-06-04 13:51:56 | [train_policy] epoch #308 | descent direction computed
2021-06-04 13:51:56 | [train_policy] epoch #308 | backtrack iters: 1
2021-06-04 13:51:56 | [train_policy] epoch #308 | optimization finished
2021-06-04 13:51:56 | [train_policy] epoch #308 | Computing KL after
2021-06-04 13:51:56 | [train_policy] epoch #308 | Computing loss after
2021-06-04 13:51:56 | [train_policy] epoch #308 | Fitting baseline...
2021-06-04 13:51:56 | [train_policy] epoch #308 | Saving snapshot...
2021-06-04 13:51:56 | [train_policy] epoch #308 | Saved
2021-06-04 13:51:56 | [train_policy] epoch #308 | Time 248.66 s
2021-06-04 13:51:56 | [train_policy] epoch #308 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285685
Evaluation/AverageDiscountedReturn          -43.767
Evaluation/AverageReturn                    -43.767
Evaluation/CompletionRate                     0
Evaluation/Iteration                        308
Evaluation/MaxReturn                        -29.7082
Evaluation/MinReturn                        -64.8504
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.70817
Extras/EpisodeRewardMean                    -43.921
LinearFeatureBaseline/ExplainedVariance       0.917028
PolicyExecTime                                0.22801
ProcessExecTime                               0.0312004
TotalEnvSteps                            312708
policy/Entropy                               -0.518855
policy/KL                                     0.0064499
policy/KLBefore                               0
policy/LossAfter                             -0.0166085
policy/LossBefore                             1.29575e-08
policy/Perplexity                             0.595202
policy/dLoss                                  0.0166086
---------------------------------------  ----------------
2021-06-04 13:51:56 | [train_policy] epoch #309 | Obtaining samples for iteration 309...
2021-06-04 13:51:57 | [train_policy] epoch #309 | Logging diagnostics...
2021-06-04 13:51:57 | [train_policy] epoch #309 | Optimizing policy...
2021-06-04 13:51:57 | [train_policy] epoch #309 | Computing loss before
2021-06-04 13:51:57 | [train_policy] epoch #309 | Computing KL before
2021-06-04 13:51:57 | [train_policy] epoch #309 | Optimizing
2021-06-04 13:51:57 | [train_policy] epoch #309 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:57 | [train_policy] epoch #309 | computing loss before
2021-06-04 13:51:57 | [train_policy] epoch #309 | computing gradient
2021-06-04 13:51:57 | [train_policy] epoch #309 | gradient computed
2021-06-04 13:51:57 | [train_policy] epoch #309 | computing descent direction
2021-06-04 13:51:57 | [train_policy] epoch #309 | descent direction computed
2021-06-04 13:51:57 | [train_policy] epoch #309 | backtrack iters: 1
2021-06-04 13:51:57 | [train_policy] epoch #309 | optimization finished
2021-06-04 13:51:57 | [train_policy] epoch #309 | Computing KL after
2021-06-04 13:51:57 | [train_policy] epoch #309 | Computing loss after
2021-06-04 13:51:57 | [train_policy] epoch #309 | Fitting baseline...
2021-06-04 13:51:57 | [train_policy] epoch #309 | Saving snapshot...
2021-06-04 13:51:57 | [train_policy] epoch #309 | Saved
2021-06-04 13:51:57 | [train_policy] epoch #309 | Time 249.48 s
2021-06-04 13:51:57 | [train_policy] epoch #309 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.286174
Evaluation/AverageDiscountedReturn          -44.9738
Evaluation/AverageReturn                    -44.9738
Evaluation/CompletionRate                     0
Evaluation/Iteration                        309
Evaluation/MaxReturn                        -30.0171
Evaluation/MinReturn                       -102.028
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.4237
Extras/EpisodeRewardMean                    -44.7801
LinearFeatureBaseline/ExplainedVariance       0.799784
PolicyExecTime                                0.232895
ProcessExecTime                               0.0313742
TotalEnvSteps                            313720
policy/Entropy                               -0.523567
policy/KL                                     0.00685909
policy/KLBefore                               0
policy/LossAfter                             -0.0204116
policy/LossBefore                            -0
policy/Perplexity                             0.592404
policy/dLoss                                  0.0204116
---------------------------------------  ---------------
2021-06-04 13:51:57 | [train_policy] epoch #310 | Obtaining samples for iteration 310...
2021-06-04 13:51:58 | [train_policy] epoch #310 | Logging diagnostics...
2021-06-04 13:51:58 | [train_policy] epoch #310 | Optimizing policy...
2021-06-04 13:51:58 | [train_policy] epoch #310 | Computing loss before
2021-06-04 13:51:58 | [train_policy] epoch #310 | Computing KL before
2021-06-04 13:51:58 | [train_policy] epoch #310 | Optimizing
2021-06-04 13:51:58 | [train_policy] epoch #310 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:58 | [train_policy] epoch #310 | computing loss before
2021-06-04 13:51:58 | [train_policy] epoch #310 | computing gradient
2021-06-04 13:51:58 | [train_policy] epoch #310 | gradient computed
2021-06-04 13:51:58 | [train_policy] epoch #310 | computing descent direction
2021-06-04 13:51:58 | [train_policy] epoch #310 | descent direction computed
2021-06-04 13:51:58 | [train_policy] epoch #310 | backtrack iters: 1
2021-06-04 13:51:58 | [train_policy] epoch #310 | optimization finished
2021-06-04 13:51:58 | [train_policy] epoch #310 | Computing KL after
2021-06-04 13:51:58 | [train_policy] epoch #310 | Computing loss after
2021-06-04 13:51:58 | [train_policy] epoch #310 | Fitting baseline...
2021-06-04 13:51:58 | [train_policy] epoch #310 | Saving snapshot...
2021-06-04 13:51:58 | [train_policy] epoch #310 | Saved
2021-06-04 13:51:58 | [train_policy] epoch #310 | Time 250.26 s
2021-06-04 13:51:58 | [train_policy] epoch #310 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285774
Evaluation/AverageDiscountedReturn          -42.8507
Evaluation/AverageReturn                    -42.8507
Evaluation/CompletionRate                     0
Evaluation/Iteration                        310
Evaluation/MaxReturn                        -29.9373
Evaluation/MinReturn                        -84.5758
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.37944
Extras/EpisodeRewardMean                    -43.1332
LinearFeatureBaseline/ExplainedVariance       0.879547
PolicyExecTime                                0.213431
ProcessExecTime                               0.0312963
TotalEnvSteps                            314732
policy/Entropy                               -0.55255
policy/KL                                     0.00664448
policy/KLBefore                               0
policy/LossAfter                             -0.0180462
policy/LossBefore                            -1.41944e-08
policy/Perplexity                             0.57548
policy/dLoss                                  0.0180462
---------------------------------------  ----------------
2021-06-04 13:51:58 | [train_policy] epoch #311 | Obtaining samples for iteration 311...
2021-06-04 13:51:59 | [train_policy] epoch #311 | Logging diagnostics...
2021-06-04 13:51:59 | [train_policy] epoch #311 | Optimizing policy...
2021-06-04 13:51:59 | [train_policy] epoch #311 | Computing loss before
2021-06-04 13:51:59 | [train_policy] epoch #311 | Computing KL before
2021-06-04 13:51:59 | [train_policy] epoch #311 | Optimizing
2021-06-04 13:51:59 | [train_policy] epoch #311 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:51:59 | [train_policy] epoch #311 | computing loss before
2021-06-04 13:51:59 | [train_policy] epoch #311 | computing gradient
2021-06-04 13:51:59 | [train_policy] epoch #311 | gradient computed
2021-06-04 13:51:59 | [train_policy] epoch #311 | computing descent direction
2021-06-04 13:51:59 | [train_policy] epoch #311 | descent direction computed
2021-06-04 13:51:59 | [train_policy] epoch #311 | backtrack iters: 0
2021-06-04 13:51:59 | [train_policy] epoch #311 | optimization finished
2021-06-04 13:51:59 | [train_policy] epoch #311 | Computing KL after
2021-06-04 13:51:59 | [train_policy] epoch #311 | Computing loss after
2021-06-04 13:51:59 | [train_policy] epoch #311 | Fitting baseline...
2021-06-04 13:51:59 | [train_policy] epoch #311 | Saving snapshot...
2021-06-04 13:51:59 | [train_policy] epoch #311 | Saved
2021-06-04 13:51:59 | [train_policy] epoch #311 | Time 251.06 s
2021-06-04 13:51:59 | [train_policy] epoch #311 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283785
Evaluation/AverageDiscountedReturn          -43.4073
Evaluation/AverageReturn                    -43.4073
Evaluation/CompletionRate                     0
Evaluation/Iteration                        311
Evaluation/MaxReturn                        -30.001
Evaluation/MinReturn                        -64.1159
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.02025
Extras/EpisodeRewardMean                    -43.3011
LinearFeatureBaseline/ExplainedVariance       0.919253
PolicyExecTime                                0.235253
ProcessExecTime                               0.0311198
TotalEnvSteps                            315744
policy/Entropy                               -0.550578
policy/KL                                     0.00946413
policy/KLBefore                               0
policy/LossAfter                             -0.0202237
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.576616
policy/dLoss                                  0.0202237
---------------------------------------  ----------------
2021-06-04 13:51:59 | [train_policy] epoch #312 | Obtaining samples for iteration 312...
2021-06-04 13:51:59 | [train_policy] epoch #312 | Logging diagnostics...
2021-06-04 13:51:59 | [train_policy] epoch #312 | Optimizing policy...
2021-06-04 13:51:59 | [train_policy] epoch #312 | Computing loss before
2021-06-04 13:51:59 | [train_policy] epoch #312 | Computing KL before
2021-06-04 13:52:00 | [train_policy] epoch #312 | Optimizing
2021-06-04 13:52:00 | [train_policy] epoch #312 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:00 | [train_policy] epoch #312 | computing loss before
2021-06-04 13:52:00 | [train_policy] epoch #312 | computing gradient
2021-06-04 13:52:00 | [train_policy] epoch #312 | gradient computed
2021-06-04 13:52:00 | [train_policy] epoch #312 | computing descent direction
2021-06-04 13:52:00 | [train_policy] epoch #312 | descent direction computed
2021-06-04 13:52:00 | [train_policy] epoch #312 | backtrack iters: 1
2021-06-04 13:52:00 | [train_policy] epoch #312 | optimization finished
2021-06-04 13:52:00 | [train_policy] epoch #312 | Computing KL after
2021-06-04 13:52:00 | [train_policy] epoch #312 | Computing loss after
2021-06-04 13:52:00 | [train_policy] epoch #312 | Fitting baseline...
2021-06-04 13:52:00 | [train_policy] epoch #312 | Saving snapshot...
2021-06-04 13:52:00 | [train_policy] epoch #312 | Saved
2021-06-04 13:52:00 | [train_policy] epoch #312 | Time 251.86 s
2021-06-04 13:52:00 | [train_policy] epoch #312 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287099
Evaluation/AverageDiscountedReturn          -43.5301
Evaluation/AverageReturn                    -43.5301
Evaluation/CompletionRate                     0
Evaluation/Iteration                        312
Evaluation/MaxReturn                        -29.8496
Evaluation/MinReturn                        -63.5529
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.71556
Extras/EpisodeRewardMean                    -43.3168
LinearFeatureBaseline/ExplainedVariance       0.92444
PolicyExecTime                                0.234252
ProcessExecTime                               0.0314109
TotalEnvSteps                            316756
policy/Entropy                               -0.556891
policy/KL                                     0.00685425
policy/KLBefore                               0
policy/LossAfter                             -0.0175965
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.572987
policy/dLoss                                  0.0175965
---------------------------------------  ----------------
2021-06-04 13:52:00 | [train_policy] epoch #313 | Obtaining samples for iteration 313...
2021-06-04 13:52:00 | [train_policy] epoch #313 | Logging diagnostics...
2021-06-04 13:52:00 | [train_policy] epoch #313 | Optimizing policy...
2021-06-04 13:52:00 | [train_policy] epoch #313 | Computing loss before
2021-06-04 13:52:00 | [train_policy] epoch #313 | Computing KL before
2021-06-04 13:52:00 | [train_policy] epoch #313 | Optimizing
2021-06-04 13:52:00 | [train_policy] epoch #313 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:00 | [train_policy] epoch #313 | computing loss before
2021-06-04 13:52:00 | [train_policy] epoch #313 | computing gradient
2021-06-04 13:52:00 | [train_policy] epoch #313 | gradient computed
2021-06-04 13:52:00 | [train_policy] epoch #313 | computing descent direction
2021-06-04 13:52:00 | [train_policy] epoch #313 | descent direction computed
2021-06-04 13:52:00 | [train_policy] epoch #313 | backtrack iters: 1
2021-06-04 13:52:00 | [train_policy] epoch #313 | optimization finished
2021-06-04 13:52:00 | [train_policy] epoch #313 | Computing KL after
2021-06-04 13:52:00 | [train_policy] epoch #313 | Computing loss after
2021-06-04 13:52:00 | [train_policy] epoch #313 | Fitting baseline...
2021-06-04 13:52:00 | [train_policy] epoch #313 | Saving snapshot...
2021-06-04 13:52:00 | [train_policy] epoch #313 | Saved
2021-06-04 13:52:00 | [train_policy] epoch #313 | Time 252.66 s
2021-06-04 13:52:00 | [train_policy] epoch #313 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286377
Evaluation/AverageDiscountedReturn          -41.9259
Evaluation/AverageReturn                    -41.9259
Evaluation/CompletionRate                     0
Evaluation/Iteration                        313
Evaluation/MaxReturn                        -30.7628
Evaluation/MinReturn                        -56.4562
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.11583
Extras/EpisodeRewardMean                    -42.1989
LinearFeatureBaseline/ExplainedVariance       0.900357
PolicyExecTime                                0.22729
ProcessExecTime                               0.0313473
TotalEnvSteps                            317768
policy/Entropy                               -0.596818
policy/KL                                     0.00662147
policy/KLBefore                               0
policy/LossAfter                             -0.0121514
policy/LossBefore                             9.89484e-09
policy/Perplexity                             0.550561
policy/dLoss                                  0.0121514
---------------------------------------  ----------------
2021-06-04 13:52:00 | [train_policy] epoch #314 | Obtaining samples for iteration 314...
2021-06-04 13:52:01 | [train_policy] epoch #314 | Logging diagnostics...
2021-06-04 13:52:01 | [train_policy] epoch #314 | Optimizing policy...
2021-06-04 13:52:01 | [train_policy] epoch #314 | Computing loss before
2021-06-04 13:52:01 | [train_policy] epoch #314 | Computing KL before
2021-06-04 13:52:01 | [train_policy] epoch #314 | Optimizing
2021-06-04 13:52:01 | [train_policy] epoch #314 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:01 | [train_policy] epoch #314 | computing loss before
2021-06-04 13:52:01 | [train_policy] epoch #314 | computing gradient
2021-06-04 13:52:01 | [train_policy] epoch #314 | gradient computed
2021-06-04 13:52:01 | [train_policy] epoch #314 | computing descent direction
2021-06-04 13:52:01 | [train_policy] epoch #314 | descent direction computed
2021-06-04 13:52:01 | [train_policy] epoch #314 | backtrack iters: 1
2021-06-04 13:52:01 | [train_policy] epoch #314 | optimization finished
2021-06-04 13:52:01 | [train_policy] epoch #314 | Computing KL after
2021-06-04 13:52:01 | [train_policy] epoch #314 | Computing loss after
2021-06-04 13:52:01 | [train_policy] epoch #314 | Fitting baseline...
2021-06-04 13:52:01 | [train_policy] epoch #314 | Saving snapshot...
2021-06-04 13:52:01 | [train_policy] epoch #314 | Saved
2021-06-04 13:52:01 | [train_policy] epoch #314 | Time 253.47 s
2021-06-04 13:52:01 | [train_policy] epoch #314 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28516
Evaluation/AverageDiscountedReturn          -43.2986
Evaluation/AverageReturn                    -43.2986
Evaluation/CompletionRate                     0
Evaluation/Iteration                        314
Evaluation/MaxReturn                        -30.1198
Evaluation/MinReturn                        -82.4312
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.96982
Extras/EpisodeRewardMean                    -43.0637
LinearFeatureBaseline/ExplainedVariance       0.870597
PolicyExecTime                                0.232394
ProcessExecTime                               0.0311513
TotalEnvSteps                            318780
policy/Entropy                               -0.631425
policy/KL                                     0.00661544
policy/KLBefore                               0
policy/LossAfter                             -0.0204947
policy/LossBefore                             8.71688e-09
policy/Perplexity                             0.531833
policy/dLoss                                  0.0204947
---------------------------------------  ----------------
2021-06-04 13:52:01 | [train_policy] epoch #315 | Obtaining samples for iteration 315...
2021-06-04 13:52:02 | [train_policy] epoch #315 | Logging diagnostics...
2021-06-04 13:52:02 | [train_policy] epoch #315 | Optimizing policy...
2021-06-04 13:52:02 | [train_policy] epoch #315 | Computing loss before
2021-06-04 13:52:02 | [train_policy] epoch #315 | Computing KL before
2021-06-04 13:52:02 | [train_policy] epoch #315 | Optimizing
2021-06-04 13:52:02 | [train_policy] epoch #315 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:02 | [train_policy] epoch #315 | computing loss before
2021-06-04 13:52:02 | [train_policy] epoch #315 | computing gradient
2021-06-04 13:52:02 | [train_policy] epoch #315 | gradient computed
2021-06-04 13:52:02 | [train_policy] epoch #315 | computing descent direction
2021-06-04 13:52:02 | [train_policy] epoch #315 | descent direction computed
2021-06-04 13:52:02 | [train_policy] epoch #315 | backtrack iters: 0
2021-06-04 13:52:02 | [train_policy] epoch #315 | optimization finished
2021-06-04 13:52:02 | [train_policy] epoch #315 | Computing KL after
2021-06-04 13:52:02 | [train_policy] epoch #315 | Computing loss after
2021-06-04 13:52:02 | [train_policy] epoch #315 | Fitting baseline...
2021-06-04 13:52:02 | [train_policy] epoch #315 | Saving snapshot...
2021-06-04 13:52:02 | [train_policy] epoch #315 | Saved
2021-06-04 13:52:02 | [train_policy] epoch #315 | Time 254.26 s
2021-06-04 13:52:02 | [train_policy] epoch #315 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.287204
Evaluation/AverageDiscountedReturn          -44.6855
Evaluation/AverageReturn                    -44.6855
Evaluation/CompletionRate                     0
Evaluation/Iteration                        315
Evaluation/MaxReturn                        -32.3867
Evaluation/MinReturn                       -100.467
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3097
Extras/EpisodeRewardMean                    -44.517
LinearFeatureBaseline/ExplainedVariance       0.846137
PolicyExecTime                                0.229565
ProcessExecTime                               0.0312872
TotalEnvSteps                            319792
policy/Entropy                               -0.627097
policy/KL                                     0.00993715
policy/KLBefore                               0
policy/LossAfter                             -0.0141387
policy/LossBefore                            -1.18974e-08
policy/Perplexity                             0.53414
policy/dLoss                                  0.0141387
---------------------------------------  ----------------
2021-06-04 13:52:02 | [train_policy] epoch #316 | Obtaining samples for iteration 316...
2021-06-04 13:52:03 | [train_policy] epoch #316 | Logging diagnostics...
2021-06-04 13:52:03 | [train_policy] epoch #316 | Optimizing policy...
2021-06-04 13:52:03 | [train_policy] epoch #316 | Computing loss before
2021-06-04 13:52:03 | [train_policy] epoch #316 | Computing KL before
2021-06-04 13:52:03 | [train_policy] epoch #316 | Optimizing
2021-06-04 13:52:03 | [train_policy] epoch #316 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:03 | [train_policy] epoch #316 | computing loss before
2021-06-04 13:52:03 | [train_policy] epoch #316 | computing gradient
2021-06-04 13:52:03 | [train_policy] epoch #316 | gradient computed
2021-06-04 13:52:03 | [train_policy] epoch #316 | computing descent direction
2021-06-04 13:52:03 | [train_policy] epoch #316 | descent direction computed
2021-06-04 13:52:03 | [train_policy] epoch #316 | backtrack iters: 1
2021-06-04 13:52:03 | [train_policy] epoch #316 | optimization finished
2021-06-04 13:52:03 | [train_policy] epoch #316 | Computing KL after
2021-06-04 13:52:03 | [train_policy] epoch #316 | Computing loss after
2021-06-04 13:52:03 | [train_policy] epoch #316 | Fitting baseline...
2021-06-04 13:52:03 | [train_policy] epoch #316 | Saving snapshot...
2021-06-04 13:52:03 | [train_policy] epoch #316 | Saved
2021-06-04 13:52:03 | [train_policy] epoch #316 | Time 255.07 s
2021-06-04 13:52:03 | [train_policy] epoch #316 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.289374
Evaluation/AverageDiscountedReturn          -43.8566
Evaluation/AverageReturn                    -43.8566
Evaluation/CompletionRate                     0
Evaluation/Iteration                        316
Evaluation/MaxReturn                        -29.6046
Evaluation/MinReturn                       -102.23
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.4088
Extras/EpisodeRewardMean                    -43.6831
LinearFeatureBaseline/ExplainedVariance       0.839711
PolicyExecTime                                0.225976
ProcessExecTime                               0.0315449
TotalEnvSteps                            320804
policy/Entropy                               -0.646924
policy/KL                                     0.00706184
policy/KLBefore                               0
policy/LossAfter                             -0.0229864
policy/LossBefore                            -2.49727e-08
policy/Perplexity                             0.523654
policy/dLoss                                  0.0229864
---------------------------------------  ----------------
2021-06-04 13:52:03 | [train_policy] epoch #317 | Obtaining samples for iteration 317...
2021-06-04 13:52:03 | [train_policy] epoch #317 | Logging diagnostics...
2021-06-04 13:52:03 | [train_policy] epoch #317 | Optimizing policy...
2021-06-04 13:52:03 | [train_policy] epoch #317 | Computing loss before
2021-06-04 13:52:03 | [train_policy] epoch #317 | Computing KL before
2021-06-04 13:52:03 | [train_policy] epoch #317 | Optimizing
2021-06-04 13:52:03 | [train_policy] epoch #317 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:03 | [train_policy] epoch #317 | computing loss before
2021-06-04 13:52:03 | [train_policy] epoch #317 | computing gradient
2021-06-04 13:52:04 | [train_policy] epoch #317 | gradient computed
2021-06-04 13:52:04 | [train_policy] epoch #317 | computing descent direction
2021-06-04 13:52:04 | [train_policy] epoch #317 | descent direction computed
2021-06-04 13:52:04 | [train_policy] epoch #317 | backtrack iters: 1
2021-06-04 13:52:04 | [train_policy] epoch #317 | optimization finished
2021-06-04 13:52:04 | [train_policy] epoch #317 | Computing KL after
2021-06-04 13:52:04 | [train_policy] epoch #317 | Computing loss after
2021-06-04 13:52:04 | [train_policy] epoch #317 | Fitting baseline...
2021-06-04 13:52:04 | [train_policy] epoch #317 | Saving snapshot...
2021-06-04 13:52:04 | [train_policy] epoch #317 | Saved
2021-06-04 13:52:04 | [train_policy] epoch #317 | Time 255.86 s
2021-06-04 13:52:04 | [train_policy] epoch #317 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287385
Evaluation/AverageDiscountedReturn          -43.1418
Evaluation/AverageReturn                    -43.1418
Evaluation/CompletionRate                     0
Evaluation/Iteration                        317
Evaluation/MaxReturn                        -32.369
Evaluation/MinReturn                        -63.0741
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.17619
Extras/EpisodeRewardMean                    -42.9865
LinearFeatureBaseline/ExplainedVariance       0.922678
PolicyExecTime                                0.21683
ProcessExecTime                               0.031234
TotalEnvSteps                            321816
policy/Entropy                               -0.643219
policy/KL                                     0.00669804
policy/KLBefore                               0
policy/LossAfter                             -0.0123813
policy/LossBefore                            -1.46067e-08
policy/Perplexity                             0.525598
policy/dLoss                                  0.0123812
---------------------------------------  ----------------
2021-06-04 13:52:04 | [train_policy] epoch #318 | Obtaining samples for iteration 318...
2021-06-04 13:52:04 | [train_policy] epoch #318 | Logging diagnostics...
2021-06-04 13:52:04 | [train_policy] epoch #318 | Optimizing policy...
2021-06-04 13:52:04 | [train_policy] epoch #318 | Computing loss before
2021-06-04 13:52:04 | [train_policy] epoch #318 | Computing KL before
2021-06-04 13:52:04 | [train_policy] epoch #318 | Optimizing
2021-06-04 13:52:04 | [train_policy] epoch #318 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:04 | [train_policy] epoch #318 | computing loss before
2021-06-04 13:52:04 | [train_policy] epoch #318 | computing gradient
2021-06-04 13:52:04 | [train_policy] epoch #318 | gradient computed
2021-06-04 13:52:04 | [train_policy] epoch #318 | computing descent direction
2021-06-04 13:52:04 | [train_policy] epoch #318 | descent direction computed
2021-06-04 13:52:04 | [train_policy] epoch #318 | backtrack iters: 0
2021-06-04 13:52:04 | [train_policy] epoch #318 | optimization finished
2021-06-04 13:52:04 | [train_policy] epoch #318 | Computing KL after
2021-06-04 13:52:04 | [train_policy] epoch #318 | Computing loss after
2021-06-04 13:52:04 | [train_policy] epoch #318 | Fitting baseline...
2021-06-04 13:52:04 | [train_policy] epoch #318 | Saving snapshot...
2021-06-04 13:52:04 | [train_policy] epoch #318 | Saved
2021-06-04 13:52:04 | [train_policy] epoch #318 | Time 256.65 s
2021-06-04 13:52:04 | [train_policy] epoch #318 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286849
Evaluation/AverageDiscountedReturn          -42.8632
Evaluation/AverageReturn                    -42.8632
Evaluation/CompletionRate                     0
Evaluation/Iteration                        318
Evaluation/MaxReturn                        -28.6401
Evaluation/MinReturn                        -83.6429
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.38033
Extras/EpisodeRewardMean                    -43.1738
LinearFeatureBaseline/ExplainedVariance       0.893599
PolicyExecTime                                0.228727
ProcessExecTime                               0.0312967
TotalEnvSteps                            322828
policy/Entropy                               -0.619331
policy/KL                                     0.00968853
policy/KLBefore                               0
policy/LossAfter                             -0.0175631
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.538304
policy/dLoss                                  0.0175631
---------------------------------------  ----------------
2021-06-04 13:52:04 | [train_policy] epoch #319 | Obtaining samples for iteration 319...
2021-06-04 13:52:05 | [train_policy] epoch #319 | Logging diagnostics...
2021-06-04 13:52:05 | [train_policy] epoch #319 | Optimizing policy...
2021-06-04 13:52:05 | [train_policy] epoch #319 | Computing loss before
2021-06-04 13:52:05 | [train_policy] epoch #319 | Computing KL before
2021-06-04 13:52:05 | [train_policy] epoch #319 | Optimizing
2021-06-04 13:52:05 | [train_policy] epoch #319 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:05 | [train_policy] epoch #319 | computing loss before
2021-06-04 13:52:05 | [train_policy] epoch #319 | computing gradient
2021-06-04 13:52:05 | [train_policy] epoch #319 | gradient computed
2021-06-04 13:52:05 | [train_policy] epoch #319 | computing descent direction
2021-06-04 13:52:05 | [train_policy] epoch #319 | descent direction computed
2021-06-04 13:52:05 | [train_policy] epoch #319 | backtrack iters: 0
2021-06-04 13:52:05 | [train_policy] epoch #319 | optimization finished
2021-06-04 13:52:05 | [train_policy] epoch #319 | Computing KL after
2021-06-04 13:52:05 | [train_policy] epoch #319 | Computing loss after
2021-06-04 13:52:05 | [train_policy] epoch #319 | Fitting baseline...
2021-06-04 13:52:05 | [train_policy] epoch #319 | Saving snapshot...
2021-06-04 13:52:05 | [train_policy] epoch #319 | Saved
2021-06-04 13:52:05 | [train_policy] epoch #319 | Time 257.43 s
2021-06-04 13:52:05 | [train_policy] epoch #319 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.290131
Evaluation/AverageDiscountedReturn          -44.3944
Evaluation/AverageReturn                    -44.3944
Evaluation/CompletionRate                     0
Evaluation/Iteration                        319
Evaluation/MaxReturn                        -30.3896
Evaluation/MinReturn                       -135.312
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         13.684
Extras/EpisodeRewardMean                    -44.1648
LinearFeatureBaseline/ExplainedVariance       0.674909
PolicyExecTime                                0.209407
ProcessExecTime                               0.0315456
TotalEnvSteps                            323840
policy/Entropy                               -0.608326
policy/KL                                     0.00964259
policy/KLBefore                               0
policy/LossAfter                             -0.0236715
policy/LossBefore                             1.86117e-08
policy/Perplexity                             0.544261
policy/dLoss                                  0.0236715
---------------------------------------  ----------------
2021-06-04 13:52:05 | [train_policy] epoch #320 | Obtaining samples for iteration 320...
2021-06-04 13:52:06 | [train_policy] epoch #320 | Logging diagnostics...
2021-06-04 13:52:06 | [train_policy] epoch #320 | Optimizing policy...
2021-06-04 13:52:06 | [train_policy] epoch #320 | Computing loss before
2021-06-04 13:52:06 | [train_policy] epoch #320 | Computing KL before
2021-06-04 13:52:06 | [train_policy] epoch #320 | Optimizing
2021-06-04 13:52:06 | [train_policy] epoch #320 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:06 | [train_policy] epoch #320 | computing loss before
2021-06-04 13:52:06 | [train_policy] epoch #320 | computing gradient
2021-06-04 13:52:06 | [train_policy] epoch #320 | gradient computed
2021-06-04 13:52:06 | [train_policy] epoch #320 | computing descent direction
2021-06-04 13:52:06 | [train_policy] epoch #320 | descent direction computed
2021-06-04 13:52:06 | [train_policy] epoch #320 | backtrack iters: 1
2021-06-04 13:52:06 | [train_policy] epoch #320 | optimization finished
2021-06-04 13:52:06 | [train_policy] epoch #320 | Computing KL after
2021-06-04 13:52:06 | [train_policy] epoch #320 | Computing loss after
2021-06-04 13:52:06 | [train_policy] epoch #320 | Fitting baseline...
2021-06-04 13:52:06 | [train_policy] epoch #320 | Saving snapshot...
2021-06-04 13:52:06 | [train_policy] epoch #320 | Saved
2021-06-04 13:52:06 | [train_policy] epoch #320 | Time 258.23 s
2021-06-04 13:52:06 | [train_policy] epoch #320 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286046
Evaluation/AverageDiscountedReturn          -43.1662
Evaluation/AverageReturn                    -43.1662
Evaluation/CompletionRate                     0
Evaluation/Iteration                        320
Evaluation/MaxReturn                        -32.9317
Evaluation/MinReturn                        -64.0651
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.56373
Extras/EpisodeRewardMean                    -43.6482
LinearFeatureBaseline/ExplainedVariance       0.878528
PolicyExecTime                                0.226508
ProcessExecTime                               0.0311742
TotalEnvSteps                            324852
policy/Entropy                               -0.62323
policy/KL                                     0.00664277
policy/KLBefore                               0
policy/LossAfter                             -0.0149978
policy/LossBefore                            -1.97897e-08
policy/Perplexity                             0.53621
policy/dLoss                                  0.0149978
---------------------------------------  ----------------
2021-06-04 13:52:06 | [train_policy] epoch #321 | Obtaining samples for iteration 321...
2021-06-04 13:52:07 | [train_policy] epoch #321 | Logging diagnostics...
2021-06-04 13:52:07 | [train_policy] epoch #321 | Optimizing policy...
2021-06-04 13:52:07 | [train_policy] epoch #321 | Computing loss before
2021-06-04 13:52:07 | [train_policy] epoch #321 | Computing KL before
2021-06-04 13:52:07 | [train_policy] epoch #321 | Optimizing
2021-06-04 13:52:07 | [train_policy] epoch #321 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:07 | [train_policy] epoch #321 | computing loss before
2021-06-04 13:52:07 | [train_policy] epoch #321 | computing gradient
2021-06-04 13:52:07 | [train_policy] epoch #321 | gradient computed
2021-06-04 13:52:07 | [train_policy] epoch #321 | computing descent direction
2021-06-04 13:52:07 | [train_policy] epoch #321 | descent direction computed
2021-06-04 13:52:07 | [train_policy] epoch #321 | backtrack iters: 0
2021-06-04 13:52:07 | [train_policy] epoch #321 | optimization finished
2021-06-04 13:52:07 | [train_policy] epoch #321 | Computing KL after
2021-06-04 13:52:07 | [train_policy] epoch #321 | Computing loss after
2021-06-04 13:52:07 | [train_policy] epoch #321 | Fitting baseline...
2021-06-04 13:52:07 | [train_policy] epoch #321 | Saving snapshot...
2021-06-04 13:52:07 | [train_policy] epoch #321 | Saved
2021-06-04 13:52:07 | [train_policy] epoch #321 | Time 259.05 s
2021-06-04 13:52:07 | [train_policy] epoch #321 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286139
Evaluation/AverageDiscountedReturn          -64.7929
Evaluation/AverageReturn                    -64.7929
Evaluation/CompletionRate                     0
Evaluation/Iteration                        321
Evaluation/MaxReturn                        -29.1626
Evaluation/MinReturn                      -2061.93
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.506
Extras/EpisodeRewardMean                    -62.9653
LinearFeatureBaseline/ExplainedVariance       0.0118817
PolicyExecTime                                0.239943
ProcessExecTime                               0.0311759
TotalEnvSteps                            325864
policy/Entropy                               -0.61995
policy/KL                                     0.00971794
policy/KLBefore                               0
policy/LossAfter                             -0.0200557
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.537971
policy/dLoss                                  0.0200557
---------------------------------------  ----------------
2021-06-04 13:52:07 | [train_policy] epoch #322 | Obtaining samples for iteration 322...
2021-06-04 13:52:07 | [train_policy] epoch #322 | Logging diagnostics...
2021-06-04 13:52:07 | [train_policy] epoch #322 | Optimizing policy...
2021-06-04 13:52:07 | [train_policy] epoch #322 | Computing loss before
2021-06-04 13:52:07 | [train_policy] epoch #322 | Computing KL before
2021-06-04 13:52:07 | [train_policy] epoch #322 | Optimizing
2021-06-04 13:52:07 | [train_policy] epoch #322 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:07 | [train_policy] epoch #322 | computing loss before
2021-06-04 13:52:07 | [train_policy] epoch #322 | computing gradient
2021-06-04 13:52:07 | [train_policy] epoch #322 | gradient computed
2021-06-04 13:52:07 | [train_policy] epoch #322 | computing descent direction
2021-06-04 13:52:08 | [train_policy] epoch #322 | descent direction computed
2021-06-04 13:52:08 | [train_policy] epoch #322 | backtrack iters: 1
2021-06-04 13:52:08 | [train_policy] epoch #322 | optimization finished
2021-06-04 13:52:08 | [train_policy] epoch #322 | Computing KL after
2021-06-04 13:52:08 | [train_policy] epoch #322 | Computing loss after
2021-06-04 13:52:08 | [train_policy] epoch #322 | Fitting baseline...
2021-06-04 13:52:08 | [train_policy] epoch #322 | Saving snapshot...
2021-06-04 13:52:08 | [train_policy] epoch #322 | Saved
2021-06-04 13:52:08 | [train_policy] epoch #322 | Time 259.83 s
2021-06-04 13:52:08 | [train_policy] epoch #322 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.288203
Evaluation/AverageDiscountedReturn          -42.2795
Evaluation/AverageReturn                    -42.2795
Evaluation/CompletionRate                     0
Evaluation/Iteration                        322
Evaluation/MaxReturn                        -29.5723
Evaluation/MinReturn                        -63.9165
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.91791
Extras/EpisodeRewardMean                    -42.2448
LinearFeatureBaseline/ExplainedVariance     -25.685
PolicyExecTime                                0.212864
ProcessExecTime                               0.0315361
TotalEnvSteps                            326876
policy/Entropy                               -0.607299
policy/KL                                     0.00656071
policy/KLBefore                               0
policy/LossAfter                             -0.0172147
policy/LossBefore                             6.12538e-08
policy/Perplexity                             0.54482
policy/dLoss                                  0.0172148
---------------------------------------  ----------------
2021-06-04 13:52:08 | [train_policy] epoch #323 | Obtaining samples for iteration 323...
2021-06-04 13:52:08 | [train_policy] epoch #323 | Logging diagnostics...
2021-06-04 13:52:08 | [train_policy] epoch #323 | Optimizing policy...
2021-06-04 13:52:08 | [train_policy] epoch #323 | Computing loss before
2021-06-04 13:52:08 | [train_policy] epoch #323 | Computing KL before
2021-06-04 13:52:08 | [train_policy] epoch #323 | Optimizing
2021-06-04 13:52:08 | [train_policy] epoch #323 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:08 | [train_policy] epoch #323 | computing loss before
2021-06-04 13:52:08 | [train_policy] epoch #323 | computing gradient
2021-06-04 13:52:08 | [train_policy] epoch #323 | gradient computed
2021-06-04 13:52:08 | [train_policy] epoch #323 | computing descent direction
2021-06-04 13:52:08 | [train_policy] epoch #323 | descent direction computed
2021-06-04 13:52:08 | [train_policy] epoch #323 | backtrack iters: 1
2021-06-04 13:52:08 | [train_policy] epoch #323 | optimization finished
2021-06-04 13:52:08 | [train_policy] epoch #323 | Computing KL after
2021-06-04 13:52:08 | [train_policy] epoch #323 | Computing loss after
2021-06-04 13:52:08 | [train_policy] epoch #323 | Fitting baseline...
2021-06-04 13:52:08 | [train_policy] epoch #323 | Saving snapshot...
2021-06-04 13:52:08 | [train_policy] epoch #323 | Saved
2021-06-04 13:52:08 | [train_policy] epoch #323 | Time 260.62 s
2021-06-04 13:52:08 | [train_policy] epoch #323 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286074
Evaluation/AverageDiscountedReturn          -42.2412
Evaluation/AverageReturn                    -42.2412
Evaluation/CompletionRate                     0
Evaluation/Iteration                        323
Evaluation/MaxReturn                        -29.8136
Evaluation/MinReturn                        -64.0686
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.03348
Extras/EpisodeRewardMean                    -42.49
LinearFeatureBaseline/ExplainedVariance       0.921691
PolicyExecTime                                0.221089
ProcessExecTime                               0.031296
TotalEnvSteps                            327888
policy/Entropy                               -0.664646
policy/KL                                     0.00675024
policy/KLBefore                               0
policy/LossAfter                             -0.0174252
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.514456
policy/dLoss                                  0.0174252
---------------------------------------  ----------------
2021-06-04 13:52:08 | [train_policy] epoch #324 | Obtaining samples for iteration 324...
2021-06-04 13:52:09 | [train_policy] epoch #324 | Logging diagnostics...
2021-06-04 13:52:09 | [train_policy] epoch #324 | Optimizing policy...
2021-06-04 13:52:09 | [train_policy] epoch #324 | Computing loss before
2021-06-04 13:52:09 | [train_policy] epoch #324 | Computing KL before
2021-06-04 13:52:09 | [train_policy] epoch #324 | Optimizing
2021-06-04 13:52:09 | [train_policy] epoch #324 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:09 | [train_policy] epoch #324 | computing loss before
2021-06-04 13:52:09 | [train_policy] epoch #324 | computing gradient
2021-06-04 13:52:09 | [train_policy] epoch #324 | gradient computed
2021-06-04 13:52:09 | [train_policy] epoch #324 | computing descent direction
2021-06-04 13:52:09 | [train_policy] epoch #324 | descent direction computed
2021-06-04 13:52:09 | [train_policy] epoch #324 | backtrack iters: 1
2021-06-04 13:52:09 | [train_policy] epoch #324 | optimization finished
2021-06-04 13:52:09 | [train_policy] epoch #324 | Computing KL after
2021-06-04 13:52:09 | [train_policy] epoch #324 | Computing loss after
2021-06-04 13:52:09 | [train_policy] epoch #324 | Fitting baseline...
2021-06-04 13:52:09 | [train_policy] epoch #324 | Saving snapshot...
2021-06-04 13:52:09 | [train_policy] epoch #324 | Saved
2021-06-04 13:52:09 | [train_policy] epoch #324 | Time 261.43 s
2021-06-04 13:52:09 | [train_policy] epoch #324 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.285624
Evaluation/AverageDiscountedReturn          -43.2365
Evaluation/AverageReturn                    -43.2365
Evaluation/CompletionRate                     0
Evaluation/Iteration                        324
Evaluation/MaxReturn                        -29.249
Evaluation/MinReturn                        -64.3366
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.29358
Extras/EpisodeRewardMean                    -43.1232
LinearFeatureBaseline/ExplainedVariance       0.922456
PolicyExecTime                                0.229144
ProcessExecTime                               0.0313039
TotalEnvSteps                            328900
policy/Entropy                               -0.677692
policy/KL                                     0.00662594
policy/KLBefore                               0
policy/LossAfter                             -0.0161419
policy/LossBefore                            -0
policy/Perplexity                             0.507788
policy/dLoss                                  0.0161419
---------------------------------------  ---------------
2021-06-04 13:52:09 | [train_policy] epoch #325 | Obtaining samples for iteration 325...
2021-06-04 13:52:10 | [train_policy] epoch #325 | Logging diagnostics...
2021-06-04 13:52:10 | [train_policy] epoch #325 | Optimizing policy...
2021-06-04 13:52:10 | [train_policy] epoch #325 | Computing loss before
2021-06-04 13:52:10 | [train_policy] epoch #325 | Computing KL before
2021-06-04 13:52:10 | [train_policy] epoch #325 | Optimizing
2021-06-04 13:52:10 | [train_policy] epoch #325 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:10 | [train_policy] epoch #325 | computing loss before
2021-06-04 13:52:10 | [train_policy] epoch #325 | computing gradient
2021-06-04 13:52:10 | [train_policy] epoch #325 | gradient computed
2021-06-04 13:52:10 | [train_policy] epoch #325 | computing descent direction
2021-06-04 13:52:10 | [train_policy] epoch #325 | descent direction computed
2021-06-04 13:52:10 | [train_policy] epoch #325 | backtrack iters: 1
2021-06-04 13:52:10 | [train_policy] epoch #325 | optimization finished
2021-06-04 13:52:10 | [train_policy] epoch #325 | Computing KL after
2021-06-04 13:52:10 | [train_policy] epoch #325 | Computing loss after
2021-06-04 13:52:10 | [train_policy] epoch #325 | Fitting baseline...
2021-06-04 13:52:10 | [train_policy] epoch #325 | Saving snapshot...
2021-06-04 13:52:10 | [train_policy] epoch #325 | Saved
2021-06-04 13:52:10 | [train_policy] epoch #325 | Time 262.24 s
2021-06-04 13:52:10 | [train_policy] epoch #325 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287572
Evaluation/AverageDiscountedReturn          -42.858
Evaluation/AverageReturn                    -42.858
Evaluation/CompletionRate                     0
Evaluation/Iteration                        325
Evaluation/MaxReturn                        -29.6258
Evaluation/MinReturn                        -84.1897
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12396
Extras/EpisodeRewardMean                    -42.9273
LinearFeatureBaseline/ExplainedVariance       0.887139
PolicyExecTime                                0.229282
ProcessExecTime                               0.0314853
TotalEnvSteps                            329912
policy/Entropy                               -0.654138
policy/KL                                     0.00644629
policy/KLBefore                               0
policy/LossAfter                             -0.0161497
policy/LossBefore                            -1.10728e-08
policy/Perplexity                             0.51989
policy/dLoss                                  0.0161496
---------------------------------------  ----------------
2021-06-04 13:52:10 | [train_policy] epoch #326 | Obtaining samples for iteration 326...
2021-06-04 13:52:11 | [train_policy] epoch #326 | Logging diagnostics...
2021-06-04 13:52:11 | [train_policy] epoch #326 | Optimizing policy...
2021-06-04 13:52:11 | [train_policy] epoch #326 | Computing loss before
2021-06-04 13:52:11 | [train_policy] epoch #326 | Computing KL before
2021-06-04 13:52:11 | [train_policy] epoch #326 | Optimizing
2021-06-04 13:52:11 | [train_policy] epoch #326 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:11 | [train_policy] epoch #326 | computing loss before
2021-06-04 13:52:11 | [train_policy] epoch #326 | computing gradient
2021-06-04 13:52:11 | [train_policy] epoch #326 | gradient computed
2021-06-04 13:52:11 | [train_policy] epoch #326 | computing descent direction
2021-06-04 13:52:11 | [train_policy] epoch #326 | descent direction computed
2021-06-04 13:52:11 | [train_policy] epoch #326 | backtrack iters: 1
2021-06-04 13:52:11 | [train_policy] epoch #326 | optimization finished
2021-06-04 13:52:11 | [train_policy] epoch #326 | Computing KL after
2021-06-04 13:52:11 | [train_policy] epoch #326 | Computing loss after
2021-06-04 13:52:11 | [train_policy] epoch #326 | Fitting baseline...
2021-06-04 13:52:11 | [train_policy] epoch #326 | Saving snapshot...
2021-06-04 13:52:11 | [train_policy] epoch #326 | Saved
2021-06-04 13:52:11 | [train_policy] epoch #326 | Time 263.03 s
2021-06-04 13:52:11 | [train_policy] epoch #326 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286508
Evaluation/AverageDiscountedReturn          -43.4777
Evaluation/AverageReturn                    -43.4777
Evaluation/CompletionRate                     0
Evaluation/Iteration                        326
Evaluation/MaxReturn                        -30.0051
Evaluation/MinReturn                        -63.023
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.13231
Extras/EpisodeRewardMean                    -43.5036
LinearFeatureBaseline/ExplainedVariance       0.915737
PolicyExecTime                                0.214193
ProcessExecTime                               0.0312815
TotalEnvSteps                            330924
policy/Entropy                               -0.672124
policy/KL                                     0.00663426
policy/KLBefore                               0
policy/LossAfter                             -0.0161542
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.510623
policy/dLoss                                  0.0161542
---------------------------------------  ----------------
2021-06-04 13:52:11 | [train_policy] epoch #327 | Obtaining samples for iteration 327...
2021-06-04 13:52:11 | [train_policy] epoch #327 | Logging diagnostics...
2021-06-04 13:52:11 | [train_policy] epoch #327 | Optimizing policy...
2021-06-04 13:52:11 | [train_policy] epoch #327 | Computing loss before
2021-06-04 13:52:11 | [train_policy] epoch #327 | Computing KL before
2021-06-04 13:52:11 | [train_policy] epoch #327 | Optimizing
2021-06-04 13:52:11 | [train_policy] epoch #327 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:11 | [train_policy] epoch #327 | computing loss before
2021-06-04 13:52:11 | [train_policy] epoch #327 | computing gradient
2021-06-04 13:52:11 | [train_policy] epoch #327 | gradient computed
2021-06-04 13:52:11 | [train_policy] epoch #327 | computing descent direction
2021-06-04 13:52:12 | [train_policy] epoch #327 | descent direction computed
2021-06-04 13:52:12 | [train_policy] epoch #327 | backtrack iters: 0
2021-06-04 13:52:12 | [train_policy] epoch #327 | optimization finished
2021-06-04 13:52:12 | [train_policy] epoch #327 | Computing KL after
2021-06-04 13:52:12 | [train_policy] epoch #327 | Computing loss after
2021-06-04 13:52:12 | [train_policy] epoch #327 | Fitting baseline...
2021-06-04 13:52:12 | [train_policy] epoch #327 | Saving snapshot...
2021-06-04 13:52:12 | [train_policy] epoch #327 | Saved
2021-06-04 13:52:12 | [train_policy] epoch #327 | Time 263.81 s
2021-06-04 13:52:12 | [train_policy] epoch #327 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285752
Evaluation/AverageDiscountedReturn          -42.8047
Evaluation/AverageReturn                    -42.8047
Evaluation/CompletionRate                     0
Evaluation/Iteration                        327
Evaluation/MaxReturn                        -28.9828
Evaluation/MinReturn                        -88.253
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.85503
Extras/EpisodeRewardMean                    -42.7804
LinearFeatureBaseline/ExplainedVariance       0.880562
PolicyExecTime                                0.2211
ProcessExecTime                               0.0312586
TotalEnvSteps                            331936
policy/Entropy                               -0.661979
policy/KL                                     0.00986256
policy/KLBefore                               0
policy/LossAfter                             -0.0230215
policy/LossBefore                            -1.38999e-08
policy/Perplexity                             0.51583
policy/dLoss                                  0.0230215
---------------------------------------  ----------------
2021-06-04 13:52:12 | [train_policy] epoch #328 | Obtaining samples for iteration 328...
2021-06-04 13:52:12 | [train_policy] epoch #328 | Logging diagnostics...
2021-06-04 13:52:12 | [train_policy] epoch #328 | Optimizing policy...
2021-06-04 13:52:12 | [train_policy] epoch #328 | Computing loss before
2021-06-04 13:52:12 | [train_policy] epoch #328 | Computing KL before
2021-06-04 13:52:12 | [train_policy] epoch #328 | Optimizing
2021-06-04 13:52:12 | [train_policy] epoch #328 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:12 | [train_policy] epoch #328 | computing loss before
2021-06-04 13:52:12 | [train_policy] epoch #328 | computing gradient
2021-06-04 13:52:12 | [train_policy] epoch #328 | gradient computed
2021-06-04 13:52:12 | [train_policy] epoch #328 | computing descent direction
2021-06-04 13:52:12 | [train_policy] epoch #328 | descent direction computed
2021-06-04 13:52:12 | [train_policy] epoch #328 | backtrack iters: 0
2021-06-04 13:52:12 | [train_policy] epoch #328 | optimization finished
2021-06-04 13:52:12 | [train_policy] epoch #328 | Computing KL after
2021-06-04 13:52:12 | [train_policy] epoch #328 | Computing loss after
2021-06-04 13:52:12 | [train_policy] epoch #328 | Fitting baseline...
2021-06-04 13:52:12 | [train_policy] epoch #328 | Saving snapshot...
2021-06-04 13:52:12 | [train_policy] epoch #328 | Saved
2021-06-04 13:52:12 | [train_policy] epoch #328 | Time 264.60 s
2021-06-04 13:52:12 | [train_policy] epoch #328 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285069
Evaluation/AverageDiscountedReturn          -44.0857
Evaluation/AverageReturn                    -44.0857
Evaluation/CompletionRate                     0
Evaluation/Iteration                        328
Evaluation/MaxReturn                        -30.0545
Evaluation/MinReturn                        -92.1123
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.9614
Extras/EpisodeRewardMean                    -44.1001
LinearFeatureBaseline/ExplainedVariance       0.880408
PolicyExecTime                                0.221803
ProcessExecTime                               0.031244
TotalEnvSteps                            332948
policy/Entropy                               -0.656572
policy/KL                                     0.00972561
policy/KLBefore                               0
policy/LossAfter                             -0.0343061
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.518626
policy/dLoss                                  0.0343061
---------------------------------------  ----------------
2021-06-04 13:52:12 | [train_policy] epoch #329 | Obtaining samples for iteration 329...
2021-06-04 13:52:13 | [train_policy] epoch #329 | Logging diagnostics...
2021-06-04 13:52:13 | [train_policy] epoch #329 | Optimizing policy...
2021-06-04 13:52:13 | [train_policy] epoch #329 | Computing loss before
2021-06-04 13:52:13 | [train_policy] epoch #329 | Computing KL before
2021-06-04 13:52:13 | [train_policy] epoch #329 | Optimizing
2021-06-04 13:52:13 | [train_policy] epoch #329 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:13 | [train_policy] epoch #329 | computing loss before
2021-06-04 13:52:13 | [train_policy] epoch #329 | computing gradient
2021-06-04 13:52:13 | [train_policy] epoch #329 | gradient computed
2021-06-04 13:52:13 | [train_policy] epoch #329 | computing descent direction
2021-06-04 13:52:13 | [train_policy] epoch #329 | descent direction computed
2021-06-04 13:52:13 | [train_policy] epoch #329 | backtrack iters: 0
2021-06-04 13:52:13 | [train_policy] epoch #329 | optimization finished
2021-06-04 13:52:13 | [train_policy] epoch #329 | Computing KL after
2021-06-04 13:52:13 | [train_policy] epoch #329 | Computing loss after
2021-06-04 13:52:13 | [train_policy] epoch #329 | Fitting baseline...
2021-06-04 13:52:13 | [train_policy] epoch #329 | Saving snapshot...
2021-06-04 13:52:13 | [train_policy] epoch #329 | Saved
2021-06-04 13:52:13 | [train_policy] epoch #329 | Time 265.39 s
2021-06-04 13:52:13 | [train_policy] epoch #329 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284845
Evaluation/AverageDiscountedReturn          -43.8911
Evaluation/AverageReturn                    -43.8911
Evaluation/CompletionRate                     0
Evaluation/Iteration                        329
Evaluation/MaxReturn                        -30.4138
Evaluation/MinReturn                        -84.0086
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.92058
Extras/EpisodeRewardMean                    -43.9795
LinearFeatureBaseline/ExplainedVariance       0.897408
PolicyExecTime                                0.227854
ProcessExecTime                               0.0311477
TotalEnvSteps                            333960
policy/Entropy                               -0.647208
policy/KL                                     0.00974403
policy/KLBefore                               0
policy/LossAfter                             -0.0229372
policy/LossBefore                            -4.12285e-09
policy/Perplexity                             0.523505
policy/dLoss                                  0.0229372
---------------------------------------  ----------------
2021-06-04 13:52:13 | [train_policy] epoch #330 | Obtaining samples for iteration 330...
2021-06-04 13:52:14 | [train_policy] epoch #330 | Logging diagnostics...
2021-06-04 13:52:14 | [train_policy] epoch #330 | Optimizing policy...
2021-06-04 13:52:14 | [train_policy] epoch #330 | Computing loss before
2021-06-04 13:52:14 | [train_policy] epoch #330 | Computing KL before
2021-06-04 13:52:14 | [train_policy] epoch #330 | Optimizing
2021-06-04 13:52:14 | [train_policy] epoch #330 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:14 | [train_policy] epoch #330 | computing loss before
2021-06-04 13:52:14 | [train_policy] epoch #330 | computing gradient
2021-06-04 13:52:14 | [train_policy] epoch #330 | gradient computed
2021-06-04 13:52:14 | [train_policy] epoch #330 | computing descent direction
2021-06-04 13:52:14 | [train_policy] epoch #330 | descent direction computed
2021-06-04 13:52:14 | [train_policy] epoch #330 | backtrack iters: 1
2021-06-04 13:52:14 | [train_policy] epoch #330 | optimization finished
2021-06-04 13:52:14 | [train_policy] epoch #330 | Computing KL after
2021-06-04 13:52:14 | [train_policy] epoch #330 | Computing loss after
2021-06-04 13:52:14 | [train_policy] epoch #330 | Fitting baseline...
2021-06-04 13:52:14 | [train_policy] epoch #330 | Saving snapshot...
2021-06-04 13:52:14 | [train_policy] epoch #330 | Saved
2021-06-04 13:52:14 | [train_policy] epoch #330 | Time 266.20 s
2021-06-04 13:52:14 | [train_policy] epoch #330 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.285645
Evaluation/AverageDiscountedReturn          -42.6692
Evaluation/AverageReturn                    -42.6692
Evaluation/CompletionRate                     0
Evaluation/Iteration                        330
Evaluation/MaxReturn                        -29.0852
Evaluation/MinReturn                        -64.0818
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.7778
Extras/EpisodeRewardMean                    -42.5719
LinearFeatureBaseline/ExplainedVariance       0.909649
PolicyExecTime                                0.222755
ProcessExecTime                               0.0313146
TotalEnvSteps                            334972
policy/Entropy                               -0.668537
policy/KL                                     0.00667703
policy/KLBefore                               0
policy/LossAfter                             -0.0143581
policy/LossBefore                             5.4775e-09
policy/Perplexity                             0.512458
policy/dLoss                                  0.0143581
---------------------------------------  ---------------
2021-06-04 13:52:14 | [train_policy] epoch #331 | Obtaining samples for iteration 331...
2021-06-04 13:52:15 | [train_policy] epoch #331 | Logging diagnostics...
2021-06-04 13:52:15 | [train_policy] epoch #331 | Optimizing policy...
2021-06-04 13:52:15 | [train_policy] epoch #331 | Computing loss before
2021-06-04 13:52:15 | [train_policy] epoch #331 | Computing KL before
2021-06-04 13:52:15 | [train_policy] epoch #331 | Optimizing
2021-06-04 13:52:15 | [train_policy] epoch #331 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:15 | [train_policy] epoch #331 | computing loss before
2021-06-04 13:52:15 | [train_policy] epoch #331 | computing gradient
2021-06-04 13:52:15 | [train_policy] epoch #331 | gradient computed
2021-06-04 13:52:15 | [train_policy] epoch #331 | computing descent direction
2021-06-04 13:52:15 | [train_policy] epoch #331 | descent direction computed
2021-06-04 13:52:15 | [train_policy] epoch #331 | backtrack iters: 1
2021-06-04 13:52:15 | [train_policy] epoch #331 | optimization finished
2021-06-04 13:52:15 | [train_policy] epoch #331 | Computing KL after
2021-06-04 13:52:15 | [train_policy] epoch #331 | Computing loss after
2021-06-04 13:52:15 | [train_policy] epoch #331 | Fitting baseline...
2021-06-04 13:52:15 | [train_policy] epoch #331 | Saving snapshot...
2021-06-04 13:52:15 | [train_policy] epoch #331 | Saved
2021-06-04 13:52:15 | [train_policy] epoch #331 | Time 267.00 s
2021-06-04 13:52:15 | [train_policy] epoch #331 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286631
Evaluation/AverageDiscountedReturn          -43.8489
Evaluation/AverageReturn                    -43.8489
Evaluation/CompletionRate                     0
Evaluation/Iteration                        331
Evaluation/MaxReturn                        -30.3047
Evaluation/MinReturn                        -62.9164
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43806
Extras/EpisodeRewardMean                    -43.6157
LinearFeatureBaseline/ExplainedVariance       0.919122
PolicyExecTime                                0.229704
ProcessExecTime                               0.0314295
TotalEnvSteps                            335984
policy/Entropy                               -0.702109
policy/KL                                     0.00664838
policy/KLBefore                               0
policy/LossAfter                             -0.0138395
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.495539
policy/dLoss                                  0.0138395
---------------------------------------  ----------------
2021-06-04 13:52:15 | [train_policy] epoch #332 | Obtaining samples for iteration 332...
2021-06-04 13:52:15 | [train_policy] epoch #332 | Logging diagnostics...
2021-06-04 13:52:15 | [train_policy] epoch #332 | Optimizing policy...
2021-06-04 13:52:15 | [train_policy] epoch #332 | Computing loss before
2021-06-04 13:52:15 | [train_policy] epoch #332 | Computing KL before
2021-06-04 13:52:15 | [train_policy] epoch #332 | Optimizing
2021-06-04 13:52:15 | [train_policy] epoch #332 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:15 | [train_policy] epoch #332 | computing loss before
2021-06-04 13:52:15 | [train_policy] epoch #332 | computing gradient
2021-06-04 13:52:15 | [train_policy] epoch #332 | gradient computed
2021-06-04 13:52:15 | [train_policy] epoch #332 | computing descent direction
2021-06-04 13:52:16 | [train_policy] epoch #332 | descent direction computed
2021-06-04 13:52:16 | [train_policy] epoch #332 | backtrack iters: 0
2021-06-04 13:52:16 | [train_policy] epoch #332 | optimization finished
2021-06-04 13:52:16 | [train_policy] epoch #332 | Computing KL after
2021-06-04 13:52:16 | [train_policy] epoch #332 | Computing loss after
2021-06-04 13:52:16 | [train_policy] epoch #332 | Fitting baseline...
2021-06-04 13:52:16 | [train_policy] epoch #332 | Saving snapshot...
2021-06-04 13:52:16 | [train_policy] epoch #332 | Saved
2021-06-04 13:52:16 | [train_policy] epoch #332 | Time 267.79 s
2021-06-04 13:52:16 | [train_policy] epoch #332 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285458
Evaluation/AverageDiscountedReturn          -44.3365
Evaluation/AverageReturn                    -44.3365
Evaluation/CompletionRate                     0
Evaluation/Iteration                        332
Evaluation/MaxReturn                        -30.0169
Evaluation/MinReturn                        -63.3277
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.74824
Extras/EpisodeRewardMean                    -44.2108
LinearFeatureBaseline/ExplainedVariance       0.916587
PolicyExecTime                                0.224928
ProcessExecTime                               0.0314538
TotalEnvSteps                            336996
policy/Entropy                               -0.650805
policy/KL                                     0.00927661
policy/KLBefore                               0
policy/LossAfter                             -0.0222683
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             0.521626
policy/dLoss                                  0.0222683
---------------------------------------  ----------------
2021-06-04 13:52:16 | [train_policy] epoch #333 | Obtaining samples for iteration 333...
2021-06-04 13:52:16 | [train_policy] epoch #333 | Logging diagnostics...
2021-06-04 13:52:16 | [train_policy] epoch #333 | Optimizing policy...
2021-06-04 13:52:16 | [train_policy] epoch #333 | Computing loss before
2021-06-04 13:52:16 | [train_policy] epoch #333 | Computing KL before
2021-06-04 13:52:16 | [train_policy] epoch #333 | Optimizing
2021-06-04 13:52:16 | [train_policy] epoch #333 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:16 | [train_policy] epoch #333 | computing loss before
2021-06-04 13:52:16 | [train_policy] epoch #333 | computing gradient
2021-06-04 13:52:16 | [train_policy] epoch #333 | gradient computed
2021-06-04 13:52:16 | [train_policy] epoch #333 | computing descent direction
2021-06-04 13:52:16 | [train_policy] epoch #333 | descent direction computed
2021-06-04 13:52:16 | [train_policy] epoch #333 | backtrack iters: 0
2021-06-04 13:52:16 | [train_policy] epoch #333 | optimization finished
2021-06-04 13:52:16 | [train_policy] epoch #333 | Computing KL after
2021-06-04 13:52:16 | [train_policy] epoch #333 | Computing loss after
2021-06-04 13:52:16 | [train_policy] epoch #333 | Fitting baseline...
2021-06-04 13:52:16 | [train_policy] epoch #333 | Saving snapshot...
2021-06-04 13:52:16 | [train_policy] epoch #333 | Saved
2021-06-04 13:52:16 | [train_policy] epoch #333 | Time 268.58 s
2021-06-04 13:52:16 | [train_policy] epoch #333 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285324
Evaluation/AverageDiscountedReturn          -42.5126
Evaluation/AverageReturn                    -42.5126
Evaluation/CompletionRate                     0
Evaluation/Iteration                        333
Evaluation/MaxReturn                        -30.8774
Evaluation/MinReturn                        -64.2076
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.06876
Extras/EpisodeRewardMean                    -42.7492
LinearFeatureBaseline/ExplainedVariance       0.928073
PolicyExecTime                                0.224502
ProcessExecTime                               0.0316646
TotalEnvSteps                            338008
policy/Entropy                               -0.642091
policy/KL                                     0.00983672
policy/KLBefore                               0
policy/LossAfter                             -0.0157027
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.526191
policy/dLoss                                  0.0157027
---------------------------------------  ----------------
2021-06-04 13:52:16 | [train_policy] epoch #334 | Obtaining samples for iteration 334...
2021-06-04 13:52:17 | [train_policy] epoch #334 | Logging diagnostics...
2021-06-04 13:52:17 | [train_policy] epoch #334 | Optimizing policy...
2021-06-04 13:52:17 | [train_policy] epoch #334 | Computing loss before
2021-06-04 13:52:17 | [train_policy] epoch #334 | Computing KL before
2021-06-04 13:52:17 | [train_policy] epoch #334 | Optimizing
2021-06-04 13:52:17 | [train_policy] epoch #334 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:17 | [train_policy] epoch #334 | computing loss before
2021-06-04 13:52:17 | [train_policy] epoch #334 | computing gradient
2021-06-04 13:52:17 | [train_policy] epoch #334 | gradient computed
2021-06-04 13:52:17 | [train_policy] epoch #334 | computing descent direction
2021-06-04 13:52:17 | [train_policy] epoch #334 | descent direction computed
2021-06-04 13:52:17 | [train_policy] epoch #334 | backtrack iters: 0
2021-06-04 13:52:17 | [train_policy] epoch #334 | optimization finished
2021-06-04 13:52:17 | [train_policy] epoch #334 | Computing KL after
2021-06-04 13:52:17 | [train_policy] epoch #334 | Computing loss after
2021-06-04 13:52:17 | [train_policy] epoch #334 | Fitting baseline...
2021-06-04 13:52:17 | [train_policy] epoch #334 | Saving snapshot...
2021-06-04 13:52:17 | [train_policy] epoch #334 | Saved
2021-06-04 13:52:17 | [train_policy] epoch #334 | Time 269.39 s
2021-06-04 13:52:17 | [train_policy] epoch #334 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.288819
Evaluation/AverageDiscountedReturn          -43.0661
Evaluation/AverageReturn                    -43.0661
Evaluation/CompletionRate                     0
Evaluation/Iteration                        334
Evaluation/MaxReturn                        -31.6196
Evaluation/MinReturn                        -62.8633
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.02793
Extras/EpisodeRewardMean                    -43.0501
LinearFeatureBaseline/ExplainedVariance       0.926062
PolicyExecTime                                0.227401
ProcessExecTime                               0.0316038
TotalEnvSteps                            339020
policy/Entropy                               -0.645508
policy/KL                                     0.00958848
policy/KLBefore                               0
policy/LossAfter                             -0.0206085
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.524396
policy/dLoss                                  0.0206085
---------------------------------------  ----------------
2021-06-04 13:52:17 | [train_policy] epoch #335 | Obtaining samples for iteration 335...
2021-06-04 13:52:18 | [train_policy] epoch #335 | Logging diagnostics...
2021-06-04 13:52:18 | [train_policy] epoch #335 | Optimizing policy...
2021-06-04 13:52:18 | [train_policy] epoch #335 | Computing loss before
2021-06-04 13:52:18 | [train_policy] epoch #335 | Computing KL before
2021-06-04 13:52:18 | [train_policy] epoch #335 | Optimizing
2021-06-04 13:52:18 | [train_policy] epoch #335 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:18 | [train_policy] epoch #335 | computing loss before
2021-06-04 13:52:18 | [train_policy] epoch #335 | computing gradient
2021-06-04 13:52:18 | [train_policy] epoch #335 | gradient computed
2021-06-04 13:52:18 | [train_policy] epoch #335 | computing descent direction
2021-06-04 13:52:18 | [train_policy] epoch #335 | descent direction computed
2021-06-04 13:52:18 | [train_policy] epoch #335 | backtrack iters: 1
2021-06-04 13:52:18 | [train_policy] epoch #335 | optimization finished
2021-06-04 13:52:18 | [train_policy] epoch #335 | Computing KL after
2021-06-04 13:52:18 | [train_policy] epoch #335 | Computing loss after
2021-06-04 13:52:18 | [train_policy] epoch #335 | Fitting baseline...
2021-06-04 13:52:18 | [train_policy] epoch #335 | Saving snapshot...
2021-06-04 13:52:18 | [train_policy] epoch #335 | Saved
2021-06-04 13:52:18 | [train_policy] epoch #335 | Time 270.19 s
2021-06-04 13:52:18 | [train_policy] epoch #335 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286736
Evaluation/AverageDiscountedReturn          -43.7689
Evaluation/AverageReturn                    -43.7689
Evaluation/CompletionRate                     0
Evaluation/Iteration                        335
Evaluation/MaxReturn                        -31.6435
Evaluation/MinReturn                        -94.8342
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.78832
Extras/EpisodeRewardMean                    -43.7614
LinearFeatureBaseline/ExplainedVariance       0.808743
PolicyExecTime                                0.223381
ProcessExecTime                               0.0313067
TotalEnvSteps                            340032
policy/Entropy                               -0.666121
policy/KL                                     0.00771815
policy/KLBefore                               0
policy/LossAfter                             -0.0233804
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.513697
policy/dLoss                                  0.0233804
---------------------------------------  ----------------
2021-06-04 13:52:18 | [train_policy] epoch #336 | Obtaining samples for iteration 336...
2021-06-04 13:52:19 | [train_policy] epoch #336 | Logging diagnostics...
2021-06-04 13:52:19 | [train_policy] epoch #336 | Optimizing policy...
2021-06-04 13:52:19 | [train_policy] epoch #336 | Computing loss before
2021-06-04 13:52:19 | [train_policy] epoch #336 | Computing KL before
2021-06-04 13:52:19 | [train_policy] epoch #336 | Optimizing
2021-06-04 13:52:19 | [train_policy] epoch #336 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:19 | [train_policy] epoch #336 | computing loss before
2021-06-04 13:52:19 | [train_policy] epoch #336 | computing gradient
2021-06-04 13:52:19 | [train_policy] epoch #336 | gradient computed
2021-06-04 13:52:19 | [train_policy] epoch #336 | computing descent direction
2021-06-04 13:52:19 | [train_policy] epoch #336 | descent direction computed
2021-06-04 13:52:19 | [train_policy] epoch #336 | backtrack iters: 0
2021-06-04 13:52:19 | [train_policy] epoch #336 | optimization finished
2021-06-04 13:52:19 | [train_policy] epoch #336 | Computing KL after
2021-06-04 13:52:19 | [train_policy] epoch #336 | Computing loss after
2021-06-04 13:52:19 | [train_policy] epoch #336 | Fitting baseline...
2021-06-04 13:52:19 | [train_policy] epoch #336 | Saving snapshot...
2021-06-04 13:52:19 | [train_policy] epoch #336 | Saved
2021-06-04 13:52:19 | [train_policy] epoch #336 | Time 270.98 s
2021-06-04 13:52:19 | [train_policy] epoch #336 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286203
Evaluation/AverageDiscountedReturn          -44.0659
Evaluation/AverageReturn                    -44.0659
Evaluation/CompletionRate                     0
Evaluation/Iteration                        336
Evaluation/MaxReturn                        -29.9316
Evaluation/MinReturn                        -87.1365
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.95212
Extras/EpisodeRewardMean                    -44.0473
LinearFeatureBaseline/ExplainedVariance       0.877911
PolicyExecTime                                0.227738
ProcessExecTime                               0.0314667
TotalEnvSteps                            341044
policy/Entropy                               -0.680247
policy/KL                                     0.00959576
policy/KLBefore                               0
policy/LossAfter                             -0.0217832
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             0.506492
policy/dLoss                                  0.0217832
---------------------------------------  ----------------
2021-06-04 13:52:19 | [train_policy] epoch #337 | Obtaining samples for iteration 337...
2021-06-04 13:52:19 | [train_policy] epoch #337 | Logging diagnostics...
2021-06-04 13:52:19 | [train_policy] epoch #337 | Optimizing policy...
2021-06-04 13:52:19 | [train_policy] epoch #337 | Computing loss before
2021-06-04 13:52:19 | [train_policy] epoch #337 | Computing KL before
2021-06-04 13:52:19 | [train_policy] epoch #337 | Optimizing
2021-06-04 13:52:19 | [train_policy] epoch #337 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:19 | [train_policy] epoch #337 | computing loss before
2021-06-04 13:52:19 | [train_policy] epoch #337 | computing gradient
2021-06-04 13:52:19 | [train_policy] epoch #337 | gradient computed
2021-06-04 13:52:19 | [train_policy] epoch #337 | computing descent direction
2021-06-04 13:52:19 | [train_policy] epoch #337 | descent direction computed
2021-06-04 13:52:19 | [train_policy] epoch #337 | backtrack iters: 0
2021-06-04 13:52:19 | [train_policy] epoch #337 | optimization finished
2021-06-04 13:52:19 | [train_policy] epoch #337 | Computing KL after
2021-06-04 13:52:19 | [train_policy] epoch #337 | Computing loss after
2021-06-04 13:52:19 | [train_policy] epoch #337 | Fitting baseline...
2021-06-04 13:52:20 | [train_policy] epoch #337 | Saving snapshot...
2021-06-04 13:52:20 | [train_policy] epoch #337 | Saved
2021-06-04 13:52:20 | [train_policy] epoch #337 | Time 271.76 s
2021-06-04 13:52:20 | [train_policy] epoch #337 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284956
Evaluation/AverageDiscountedReturn          -42.7694
Evaluation/AverageReturn                    -42.7694
Evaluation/CompletionRate                     0
Evaluation/Iteration                        337
Evaluation/MaxReturn                        -30.0804
Evaluation/MinReturn                        -80.6499
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.2044
Extras/EpisodeRewardMean                    -42.8677
LinearFeatureBaseline/ExplainedVariance       0.898877
PolicyExecTime                                0.214025
ProcessExecTime                               0.0312371
TotalEnvSteps                            342056
policy/Entropy                               -0.649265
policy/KL                                     0.00971989
policy/KLBefore                               0
policy/LossAfter                             -0.0201918
policy/LossBefore                            -1.36054e-08
policy/Perplexity                             0.522429
policy/dLoss                                  0.0201918
---------------------------------------  ----------------
2021-06-04 13:52:20 | [train_policy] epoch #338 | Obtaining samples for iteration 338...
2021-06-04 13:52:20 | [train_policy] epoch #338 | Logging diagnostics...
2021-06-04 13:52:20 | [train_policy] epoch #338 | Optimizing policy...
2021-06-04 13:52:20 | [train_policy] epoch #338 | Computing loss before
2021-06-04 13:52:20 | [train_policy] epoch #338 | Computing KL before
2021-06-04 13:52:20 | [train_policy] epoch #338 | Optimizing
2021-06-04 13:52:20 | [train_policy] epoch #338 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:20 | [train_policy] epoch #338 | computing loss before
2021-06-04 13:52:20 | [train_policy] epoch #338 | computing gradient
2021-06-04 13:52:20 | [train_policy] epoch #338 | gradient computed
2021-06-04 13:52:20 | [train_policy] epoch #338 | computing descent direction
2021-06-04 13:52:20 | [train_policy] epoch #338 | descent direction computed
2021-06-04 13:52:20 | [train_policy] epoch #338 | backtrack iters: 1
2021-06-04 13:52:20 | [train_policy] epoch #338 | optimization finished
2021-06-04 13:52:20 | [train_policy] epoch #338 | Computing KL after
2021-06-04 13:52:20 | [train_policy] epoch #338 | Computing loss after
2021-06-04 13:52:20 | [train_policy] epoch #338 | Fitting baseline...
2021-06-04 13:52:20 | [train_policy] epoch #338 | Saving snapshot...
2021-06-04 13:52:20 | [train_policy] epoch #338 | Saved
2021-06-04 13:52:20 | [train_policy] epoch #338 | Time 272.55 s
2021-06-04 13:52:20 | [train_policy] epoch #338 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28589
Evaluation/AverageDiscountedReturn          -43.5778
Evaluation/AverageReturn                    -43.5778
Evaluation/CompletionRate                     0
Evaluation/Iteration                        338
Evaluation/MaxReturn                        -33.5184
Evaluation/MinReturn                        -85.2965
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.22926
Extras/EpisodeRewardMean                    -43.6504
LinearFeatureBaseline/ExplainedVariance       0.899922
PolicyExecTime                                0.233132
ProcessExecTime                               0.0310249
TotalEnvSteps                            343068
policy/Entropy                               -0.688958
policy/KL                                     0.00643599
policy/KLBefore                               0
policy/LossAfter                             -0.0123764
policy/LossBefore                            -6.00758e-09
policy/Perplexity                             0.502099
policy/dLoss                                  0.0123764
---------------------------------------  ----------------
2021-06-04 13:52:20 | [train_policy] epoch #339 | Obtaining samples for iteration 339...
2021-06-04 13:52:21 | [train_policy] epoch #339 | Logging diagnostics...
2021-06-04 13:52:21 | [train_policy] epoch #339 | Optimizing policy...
2021-06-04 13:52:21 | [train_policy] epoch #339 | Computing loss before
2021-06-04 13:52:21 | [train_policy] epoch #339 | Computing KL before
2021-06-04 13:52:21 | [train_policy] epoch #339 | Optimizing
2021-06-04 13:52:21 | [train_policy] epoch #339 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:21 | [train_policy] epoch #339 | computing loss before
2021-06-04 13:52:21 | [train_policy] epoch #339 | computing gradient
2021-06-04 13:52:21 | [train_policy] epoch #339 | gradient computed
2021-06-04 13:52:21 | [train_policy] epoch #339 | computing descent direction
2021-06-04 13:52:21 | [train_policy] epoch #339 | descent direction computed
2021-06-04 13:52:21 | [train_policy] epoch #339 | backtrack iters: 0
2021-06-04 13:52:21 | [train_policy] epoch #339 | optimization finished
2021-06-04 13:52:21 | [train_policy] epoch #339 | Computing KL after
2021-06-04 13:52:21 | [train_policy] epoch #339 | Computing loss after
2021-06-04 13:52:21 | [train_policy] epoch #339 | Fitting baseline...
2021-06-04 13:52:21 | [train_policy] epoch #339 | Saving snapshot...
2021-06-04 13:52:21 | [train_policy] epoch #339 | Saved
2021-06-04 13:52:21 | [train_policy] epoch #339 | Time 273.35 s
2021-06-04 13:52:21 | [train_policy] epoch #339 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.283747
Evaluation/AverageDiscountedReturn          -44.8697
Evaluation/AverageReturn                    -44.8697
Evaluation/CompletionRate                     0
Evaluation/Iteration                        339
Evaluation/MaxReturn                        -30.1765
Evaluation/MinReturn                        -82.3278
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.6623
Extras/EpisodeRewardMean                    -44.6538
LinearFeatureBaseline/ExplainedVariance       0.882779
PolicyExecTime                                0.228857
ProcessExecTime                               0.0311346
TotalEnvSteps                            344080
policy/Entropy                               -0.665919
policy/KL                                     0.00953988
policy/KLBefore                               0
policy/LossAfter                             -0.0156459
policy/LossBefore                             5.5364e-09
policy/Perplexity                             0.513801
policy/dLoss                                  0.0156459
---------------------------------------  ---------------
2021-06-04 13:52:21 | [train_policy] epoch #340 | Obtaining samples for iteration 340...
2021-06-04 13:52:22 | [train_policy] epoch #340 | Logging diagnostics...
2021-06-04 13:52:22 | [train_policy] epoch #340 | Optimizing policy...
2021-06-04 13:52:22 | [train_policy] epoch #340 | Computing loss before
2021-06-04 13:52:22 | [train_policy] epoch #340 | Computing KL before
2021-06-04 13:52:22 | [train_policy] epoch #340 | Optimizing
2021-06-04 13:52:22 | [train_policy] epoch #340 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:22 | [train_policy] epoch #340 | computing loss before
2021-06-04 13:52:22 | [train_policy] epoch #340 | computing gradient
2021-06-04 13:52:22 | [train_policy] epoch #340 | gradient computed
2021-06-04 13:52:22 | [train_policy] epoch #340 | computing descent direction
2021-06-04 13:52:22 | [train_policy] epoch #340 | descent direction computed
2021-06-04 13:52:22 | [train_policy] epoch #340 | backtrack iters: 0
2021-06-04 13:52:22 | [train_policy] epoch #340 | optimization finished
2021-06-04 13:52:22 | [train_policy] epoch #340 | Computing KL after
2021-06-04 13:52:22 | [train_policy] epoch #340 | Computing loss after
2021-06-04 13:52:22 | [train_policy] epoch #340 | Fitting baseline...
2021-06-04 13:52:22 | [train_policy] epoch #340 | Saving snapshot...
2021-06-04 13:52:22 | [train_policy] epoch #340 | Saved
2021-06-04 13:52:22 | [train_policy] epoch #340 | Time 274.14 s
2021-06-04 13:52:22 | [train_policy] epoch #340 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286479
Evaluation/AverageDiscountedReturn          -43.6888
Evaluation/AverageReturn                    -43.6888
Evaluation/CompletionRate                     0
Evaluation/Iteration                        340
Evaluation/MaxReturn                        -29.1713
Evaluation/MinReturn                        -84.0947
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.5217
Extras/EpisodeRewardMean                    -43.4788
LinearFeatureBaseline/ExplainedVariance       0.833379
PolicyExecTime                                0.221194
ProcessExecTime                               0.031328
TotalEnvSteps                            345092
policy/Entropy                               -0.646917
policy/KL                                     0.00989616
policy/KLBefore                               0
policy/LossAfter                             -0.020368
policy/LossBefore                            -7.30334e-09
policy/Perplexity                             0.523658
policy/dLoss                                  0.020368
---------------------------------------  ----------------
2021-06-04 13:52:22 | [train_policy] epoch #341 | Obtaining samples for iteration 341...
2021-06-04 13:52:23 | [train_policy] epoch #341 | Logging diagnostics...
2021-06-04 13:52:23 | [train_policy] epoch #341 | Optimizing policy...
2021-06-04 13:52:23 | [train_policy] epoch #341 | Computing loss before
2021-06-04 13:52:23 | [train_policy] epoch #341 | Computing KL before
2021-06-04 13:52:23 | [train_policy] epoch #341 | Optimizing
2021-06-04 13:52:23 | [train_policy] epoch #341 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:23 | [train_policy] epoch #341 | computing loss before
2021-06-04 13:52:23 | [train_policy] epoch #341 | computing gradient
2021-06-04 13:52:23 | [train_policy] epoch #341 | gradient computed
2021-06-04 13:52:23 | [train_policy] epoch #341 | computing descent direction
2021-06-04 13:52:23 | [train_policy] epoch #341 | descent direction computed
2021-06-04 13:52:23 | [train_policy] epoch #341 | backtrack iters: 1
2021-06-04 13:52:23 | [train_policy] epoch #341 | optimization finished
2021-06-04 13:52:23 | [train_policy] epoch #341 | Computing KL after
2021-06-04 13:52:23 | [train_policy] epoch #341 | Computing loss after
2021-06-04 13:52:23 | [train_policy] epoch #341 | Fitting baseline...
2021-06-04 13:52:23 | [train_policy] epoch #341 | Saving snapshot...
2021-06-04 13:52:23 | [train_policy] epoch #341 | Saved
2021-06-04 13:52:23 | [train_policy] epoch #341 | Time 274.94 s
2021-06-04 13:52:23 | [train_policy] epoch #341 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.285361
Evaluation/AverageDiscountedReturn          -44.8485
Evaluation/AverageReturn                    -44.8485
Evaluation/CompletionRate                     0
Evaluation/Iteration                        341
Evaluation/MaxReturn                        -29.9067
Evaluation/MinReturn                        -82.5238
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.34241
Extras/EpisodeRewardMean                    -44.5708
LinearFeatureBaseline/ExplainedVariance       0.881327
PolicyExecTime                                0.228494
ProcessExecTime                               0.0311735
TotalEnvSteps                            346104
policy/Entropy                               -0.671677
policy/KL                                     0.00663534
policy/KLBefore                               0
policy/LossAfter                             -0.0169143
policy/LossBefore                             5.4186e-09
policy/Perplexity                             0.510851
policy/dLoss                                  0.0169143
---------------------------------------  ---------------
2021-06-04 13:52:23 | [train_policy] epoch #342 | Obtaining samples for iteration 342...
2021-06-04 13:52:23 | [train_policy] epoch #342 | Logging diagnostics...
2021-06-04 13:52:23 | [train_policy] epoch #342 | Optimizing policy...
2021-06-04 13:52:23 | [train_policy] epoch #342 | Computing loss before
2021-06-04 13:52:23 | [train_policy] epoch #342 | Computing KL before
2021-06-04 13:52:23 | [train_policy] epoch #342 | Optimizing
2021-06-04 13:52:23 | [train_policy] epoch #342 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:23 | [train_policy] epoch #342 | computing loss before
2021-06-04 13:52:23 | [train_policy] epoch #342 | computing gradient
2021-06-04 13:52:23 | [train_policy] epoch #342 | gradient computed
2021-06-04 13:52:23 | [train_policy] epoch #342 | computing descent direction
2021-06-04 13:52:23 | [train_policy] epoch #342 | descent direction computed
2021-06-04 13:52:23 | [train_policy] epoch #342 | backtrack iters: 1
2021-06-04 13:52:23 | [train_policy] epoch #342 | optimization finished
2021-06-04 13:52:23 | [train_policy] epoch #342 | Computing KL after
2021-06-04 13:52:23 | [train_policy] epoch #342 | Computing loss after
2021-06-04 13:52:23 | [train_policy] epoch #342 | Fitting baseline...
2021-06-04 13:52:24 | [train_policy] epoch #342 | Saving snapshot...
2021-06-04 13:52:24 | [train_policy] epoch #342 | Saved
2021-06-04 13:52:24 | [train_policy] epoch #342 | Time 275.75 s
2021-06-04 13:52:24 | [train_policy] epoch #342 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285981
Evaluation/AverageDiscountedReturn          -43.9839
Evaluation/AverageReturn                    -43.9839
Evaluation/CompletionRate                     0
Evaluation/Iteration                        342
Evaluation/MaxReturn                        -28.8672
Evaluation/MinReturn                        -64.3188
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85993
Extras/EpisodeRewardMean                    -44.1652
LinearFeatureBaseline/ExplainedVariance       0.904976
PolicyExecTime                                0.231187
ProcessExecTime                               0.0314102
TotalEnvSteps                            347116
policy/Entropy                               -0.677779
policy/KL                                     0.00654823
policy/KLBefore                               0
policy/LossAfter                             -0.0199313
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.507743
policy/dLoss                                  0.0199313
---------------------------------------  ----------------
2021-06-04 13:52:24 | [train_policy] epoch #343 | Obtaining samples for iteration 343...
2021-06-04 13:52:24 | [train_policy] epoch #343 | Logging diagnostics...
2021-06-04 13:52:24 | [train_policy] epoch #343 | Optimizing policy...
2021-06-04 13:52:24 | [train_policy] epoch #343 | Computing loss before
2021-06-04 13:52:24 | [train_policy] epoch #343 | Computing KL before
2021-06-04 13:52:24 | [train_policy] epoch #343 | Optimizing
2021-06-04 13:52:24 | [train_policy] epoch #343 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:24 | [train_policy] epoch #343 | computing loss before
2021-06-04 13:52:24 | [train_policy] epoch #343 | computing gradient
2021-06-04 13:52:24 | [train_policy] epoch #343 | gradient computed
2021-06-04 13:52:24 | [train_policy] epoch #343 | computing descent direction
2021-06-04 13:52:24 | [train_policy] epoch #343 | descent direction computed
2021-06-04 13:52:24 | [train_policy] epoch #343 | backtrack iters: 1
2021-06-04 13:52:24 | [train_policy] epoch #343 | optimization finished
2021-06-04 13:52:24 | [train_policy] epoch #343 | Computing KL after
2021-06-04 13:52:24 | [train_policy] epoch #343 | Computing loss after
2021-06-04 13:52:24 | [train_policy] epoch #343 | Fitting baseline...
2021-06-04 13:52:24 | [train_policy] epoch #343 | Saving snapshot...
2021-06-04 13:52:24 | [train_policy] epoch #343 | Saved
2021-06-04 13:52:24 | [train_policy] epoch #343 | Time 276.55 s
2021-06-04 13:52:24 | [train_policy] epoch #343 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285162
Evaluation/AverageDiscountedReturn          -42.5857
Evaluation/AverageReturn                    -42.5857
Evaluation/CompletionRate                     0
Evaluation/Iteration                        343
Evaluation/MaxReturn                        -29.452
Evaluation/MinReturn                        -63.2969
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.97623
Extras/EpisodeRewardMean                    -42.7537
LinearFeatureBaseline/ExplainedVariance       0.91708
PolicyExecTime                                0.216905
ProcessExecTime                               0.031239
TotalEnvSteps                            348128
policy/Entropy                               -0.666693
policy/KL                                     0.00667703
policy/KLBefore                               0
policy/LossAfter                             -0.0194736
policy/LossBefore                            -1.13084e-08
policy/Perplexity                             0.513404
policy/dLoss                                  0.0194736
---------------------------------------  ----------------
2021-06-04 13:52:24 | [train_policy] epoch #344 | Obtaining samples for iteration 344...
2021-06-04 13:52:25 | [train_policy] epoch #344 | Logging diagnostics...
2021-06-04 13:52:25 | [train_policy] epoch #344 | Optimizing policy...
2021-06-04 13:52:25 | [train_policy] epoch #344 | Computing loss before
2021-06-04 13:52:25 | [train_policy] epoch #344 | Computing KL before
2021-06-04 13:52:25 | [train_policy] epoch #344 | Optimizing
2021-06-04 13:52:25 | [train_policy] epoch #344 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:25 | [train_policy] epoch #344 | computing loss before
2021-06-04 13:52:25 | [train_policy] epoch #344 | computing gradient
2021-06-04 13:52:25 | [train_policy] epoch #344 | gradient computed
2021-06-04 13:52:25 | [train_policy] epoch #344 | computing descent direction
2021-06-04 13:52:25 | [train_policy] epoch #344 | descent direction computed
2021-06-04 13:52:25 | [train_policy] epoch #344 | backtrack iters: 1
2021-06-04 13:52:25 | [train_policy] epoch #344 | optimization finished
2021-06-04 13:52:25 | [train_policy] epoch #344 | Computing KL after
2021-06-04 13:52:25 | [train_policy] epoch #344 | Computing loss after
2021-06-04 13:52:25 | [train_policy] epoch #344 | Fitting baseline...
2021-06-04 13:52:25 | [train_policy] epoch #344 | Saving snapshot...
2021-06-04 13:52:25 | [train_policy] epoch #344 | Saved
2021-06-04 13:52:25 | [train_policy] epoch #344 | Time 277.36 s
2021-06-04 13:52:25 | [train_policy] epoch #344 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286321
Evaluation/AverageDiscountedReturn          -44.7319
Evaluation/AverageReturn                    -44.7319
Evaluation/CompletionRate                     0
Evaluation/Iteration                        344
Evaluation/MaxReturn                        -29.8447
Evaluation/MinReturn                        -87.6536
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77043
Extras/EpisodeRewardMean                    -44.6929
LinearFeatureBaseline/ExplainedVariance       0.881443
PolicyExecTime                                0.228462
ProcessExecTime                               0.0312753
TotalEnvSteps                            349140
policy/Entropy                               -0.723857
policy/KL                                     0.00648949
policy/KLBefore                               0
policy/LossAfter                             -0.0128276
policy/LossBefore                            -1.10728e-08
policy/Perplexity                             0.484879
policy/dLoss                                  0.0128276
---------------------------------------  ----------------
2021-06-04 13:52:25 | [train_policy] epoch #345 | Obtaining samples for iteration 345...
2021-06-04 13:52:26 | [train_policy] epoch #345 | Logging diagnostics...
2021-06-04 13:52:26 | [train_policy] epoch #345 | Optimizing policy...
2021-06-04 13:52:26 | [train_policy] epoch #345 | Computing loss before
2021-06-04 13:52:26 | [train_policy] epoch #345 | Computing KL before
2021-06-04 13:52:26 | [train_policy] epoch #345 | Optimizing
2021-06-04 13:52:26 | [train_policy] epoch #345 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:26 | [train_policy] epoch #345 | computing loss before
2021-06-04 13:52:26 | [train_policy] epoch #345 | computing gradient
2021-06-04 13:52:26 | [train_policy] epoch #345 | gradient computed
2021-06-04 13:52:26 | [train_policy] epoch #345 | computing descent direction
2021-06-04 13:52:26 | [train_policy] epoch #345 | descent direction computed
2021-06-04 13:52:26 | [train_policy] epoch #345 | backtrack iters: 1
2021-06-04 13:52:26 | [train_policy] epoch #345 | optimization finished
2021-06-04 13:52:26 | [train_policy] epoch #345 | Computing KL after
2021-06-04 13:52:26 | [train_policy] epoch #345 | Computing loss after
2021-06-04 13:52:26 | [train_policy] epoch #345 | Fitting baseline...
2021-06-04 13:52:26 | [train_policy] epoch #345 | Saving snapshot...
2021-06-04 13:52:26 | [train_policy] epoch #345 | Saved
2021-06-04 13:52:26 | [train_policy] epoch #345 | Time 278.17 s
2021-06-04 13:52:26 | [train_policy] epoch #345 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286743
Evaluation/AverageDiscountedReturn          -44.4789
Evaluation/AverageReturn                    -44.4789
Evaluation/CompletionRate                     0
Evaluation/Iteration                        345
Evaluation/MaxReturn                        -30.9657
Evaluation/MinReturn                        -87.9008
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.74911
Extras/EpisodeRewardMean                    -44.1678
LinearFeatureBaseline/ExplainedVariance       0.861736
PolicyExecTime                                0.239747
ProcessExecTime                               0.0312407
TotalEnvSteps                            350152
policy/Entropy                               -0.752258
policy/KL                                     0.00681797
policy/KLBefore                               0
policy/LossAfter                             -0.0279596
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.471301
policy/dLoss                                  0.0279596
---------------------------------------  ----------------
2021-06-04 13:52:26 | [train_policy] epoch #346 | Obtaining samples for iteration 346...
2021-06-04 13:52:27 | [train_policy] epoch #346 | Logging diagnostics...
2021-06-04 13:52:27 | [train_policy] epoch #346 | Optimizing policy...
2021-06-04 13:52:27 | [train_policy] epoch #346 | Computing loss before
2021-06-04 13:52:27 | [train_policy] epoch #346 | Computing KL before
2021-06-04 13:52:27 | [train_policy] epoch #346 | Optimizing
2021-06-04 13:52:27 | [train_policy] epoch #346 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:27 | [train_policy] epoch #346 | computing loss before
2021-06-04 13:52:27 | [train_policy] epoch #346 | computing gradient
2021-06-04 13:52:27 | [train_policy] epoch #346 | gradient computed
2021-06-04 13:52:27 | [train_policy] epoch #346 | computing descent direction
2021-06-04 13:52:27 | [train_policy] epoch #346 | descent direction computed
2021-06-04 13:52:27 | [train_policy] epoch #346 | backtrack iters: 0
2021-06-04 13:52:27 | [train_policy] epoch #346 | optimization finished
2021-06-04 13:52:27 | [train_policy] epoch #346 | Computing KL after
2021-06-04 13:52:27 | [train_policy] epoch #346 | Computing loss after
2021-06-04 13:52:27 | [train_policy] epoch #346 | Fitting baseline...
2021-06-04 13:52:27 | [train_policy] epoch #346 | Saving snapshot...
2021-06-04 13:52:27 | [train_policy] epoch #346 | Saved
2021-06-04 13:52:27 | [train_policy] epoch #346 | Time 278.98 s
2021-06-04 13:52:27 | [train_policy] epoch #346 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284891
Evaluation/AverageDiscountedReturn          -41.9867
Evaluation/AverageReturn                    -41.9867
Evaluation/CompletionRate                     0
Evaluation/Iteration                        346
Evaluation/MaxReturn                        -30.7779
Evaluation/MinReturn                        -63.999
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.92739
Extras/EpisodeRewardMean                    -42.3916
LinearFeatureBaseline/ExplainedVariance       0.872363
PolicyExecTime                                0.237917
ProcessExecTime                               0.0311201
TotalEnvSteps                            351164
policy/Entropy                               -0.717958
policy/KL                                     0.00932834
policy/KLBefore                               0
policy/LossAfter                             -0.0171404
policy/LossBefore                            -2.40303e-08
policy/Perplexity                             0.487747
policy/dLoss                                  0.0171404
---------------------------------------  ----------------
2021-06-04 13:52:27 | [train_policy] epoch #347 | Obtaining samples for iteration 347...
2021-06-04 13:52:27 | [train_policy] epoch #347 | Logging diagnostics...
2021-06-04 13:52:27 | [train_policy] epoch #347 | Optimizing policy...
2021-06-04 13:52:27 | [train_policy] epoch #347 | Computing loss before
2021-06-04 13:52:27 | [train_policy] epoch #347 | Computing KL before
2021-06-04 13:52:27 | [train_policy] epoch #347 | Optimizing
2021-06-04 13:52:27 | [train_policy] epoch #347 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:27 | [train_policy] epoch #347 | computing loss before
2021-06-04 13:52:27 | [train_policy] epoch #347 | computing gradient
2021-06-04 13:52:27 | [train_policy] epoch #347 | gradient computed
2021-06-04 13:52:27 | [train_policy] epoch #347 | computing descent direction
2021-06-04 13:52:27 | [train_policy] epoch #347 | descent direction computed
2021-06-04 13:52:27 | [train_policy] epoch #347 | backtrack iters: 1
2021-06-04 13:52:27 | [train_policy] epoch #347 | optimization finished
2021-06-04 13:52:27 | [train_policy] epoch #347 | Computing KL after
2021-06-04 13:52:27 | [train_policy] epoch #347 | Computing loss after
2021-06-04 13:52:28 | [train_policy] epoch #347 | Fitting baseline...
2021-06-04 13:52:28 | [train_policy] epoch #347 | Saving snapshot...
2021-06-04 13:52:28 | [train_policy] epoch #347 | Saved
2021-06-04 13:52:28 | [train_policy] epoch #347 | Time 279.76 s
2021-06-04 13:52:28 | [train_policy] epoch #347 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.288457
Evaluation/AverageDiscountedReturn          -43.5829
Evaluation/AverageReturn                    -43.5829
Evaluation/CompletionRate                     0
Evaluation/Iteration                        347
Evaluation/MaxReturn                        -29.4011
Evaluation/MinReturn                       -104.724
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2808
Extras/EpisodeRewardMean                    -43.4739
LinearFeatureBaseline/ExplainedVariance       0.818267
PolicyExecTime                                0.212611
ProcessExecTime                               0.031569
TotalEnvSteps                            352176
policy/Entropy                               -0.754366
policy/KL                                     0.0065557
policy/KLBefore                               0
policy/LossAfter                             -0.0210053
policy/LossBefore                             7.06774e-10
policy/Perplexity                             0.470309
policy/dLoss                                  0.0210053
---------------------------------------  ----------------
2021-06-04 13:52:28 | [train_policy] epoch #348 | Obtaining samples for iteration 348...
2021-06-04 13:52:28 | [train_policy] epoch #348 | Logging diagnostics...
2021-06-04 13:52:28 | [train_policy] epoch #348 | Optimizing policy...
2021-06-04 13:52:28 | [train_policy] epoch #348 | Computing loss before
2021-06-04 13:52:28 | [train_policy] epoch #348 | Computing KL before
2021-06-04 13:52:28 | [train_policy] epoch #348 | Optimizing
2021-06-04 13:52:28 | [train_policy] epoch #348 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:28 | [train_policy] epoch #348 | computing loss before
2021-06-04 13:52:28 | [train_policy] epoch #348 | computing gradient
2021-06-04 13:52:28 | [train_policy] epoch #348 | gradient computed
2021-06-04 13:52:28 | [train_policy] epoch #348 | computing descent direction
2021-06-04 13:52:28 | [train_policy] epoch #348 | descent direction computed
2021-06-04 13:52:28 | [train_policy] epoch #348 | backtrack iters: 0
2021-06-04 13:52:28 | [train_policy] epoch #348 | optimization finished
2021-06-04 13:52:28 | [train_policy] epoch #348 | Computing KL after
2021-06-04 13:52:28 | [train_policy] epoch #348 | Computing loss after
2021-06-04 13:52:28 | [train_policy] epoch #348 | Fitting baseline...
2021-06-04 13:52:28 | [train_policy] epoch #348 | Saving snapshot...
2021-06-04 13:52:28 | [train_policy] epoch #348 | Saved
2021-06-04 13:52:28 | [train_policy] epoch #348 | Time 280.57 s
2021-06-04 13:52:28 | [train_policy] epoch #348 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288097
Evaluation/AverageDiscountedReturn          -41.9857
Evaluation/AverageReturn                    -41.9857
Evaluation/CompletionRate                     0
Evaluation/Iteration                        348
Evaluation/MaxReturn                        -29.2449
Evaluation/MinReturn                        -64.0796
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.36207
Extras/EpisodeRewardMean                    -42.3986
LinearFeatureBaseline/ExplainedVariance       0.889091
PolicyExecTime                                0.236858
ProcessExecTime                               0.0315664
TotalEnvSteps                            353188
policy/Entropy                               -0.75031
policy/KL                                     0.00992901
policy/KLBefore                               0
policy/LossAfter                             -0.0423814
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.47222
policy/dLoss                                  0.0423814
---------------------------------------  ----------------
2021-06-04 13:52:28 | [train_policy] epoch #349 | Obtaining samples for iteration 349...
2021-06-04 13:52:29 | [train_policy] epoch #349 | Logging diagnostics...
2021-06-04 13:52:29 | [train_policy] epoch #349 | Optimizing policy...
2021-06-04 13:52:29 | [train_policy] epoch #349 | Computing loss before
2021-06-04 13:52:29 | [train_policy] epoch #349 | Computing KL before
2021-06-04 13:52:29 | [train_policy] epoch #349 | Optimizing
2021-06-04 13:52:29 | [train_policy] epoch #349 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:29 | [train_policy] epoch #349 | computing loss before
2021-06-04 13:52:29 | [train_policy] epoch #349 | computing gradient
2021-06-04 13:52:29 | [train_policy] epoch #349 | gradient computed
2021-06-04 13:52:29 | [train_policy] epoch #349 | computing descent direction
2021-06-04 13:52:29 | [train_policy] epoch #349 | descent direction computed
2021-06-04 13:52:29 | [train_policy] epoch #349 | backtrack iters: 1
2021-06-04 13:52:29 | [train_policy] epoch #349 | optimization finished
2021-06-04 13:52:29 | [train_policy] epoch #349 | Computing KL after
2021-06-04 13:52:29 | [train_policy] epoch #349 | Computing loss after
2021-06-04 13:52:29 | [train_policy] epoch #349 | Fitting baseline...
2021-06-04 13:52:29 | [train_policy] epoch #349 | Saving snapshot...
2021-06-04 13:52:29 | [train_policy] epoch #349 | Saved
2021-06-04 13:52:29 | [train_policy] epoch #349 | Time 281.38 s
2021-06-04 13:52:29 | [train_policy] epoch #349 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286771
Evaluation/AverageDiscountedReturn          -43.0093
Evaluation/AverageReturn                    -43.0093
Evaluation/CompletionRate                     0
Evaluation/Iteration                        349
Evaluation/MaxReturn                        -30.4012
Evaluation/MinReturn                        -91.7333
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25254
Extras/EpisodeRewardMean                    -42.7613
LinearFeatureBaseline/ExplainedVariance       0.861831
PolicyExecTime                                0.226643
ProcessExecTime                               0.0312808
TotalEnvSteps                            354200
policy/Entropy                               -0.765765
policy/KL                                     0.00658809
policy/KLBefore                               0
policy/LossAfter                             -0.0238271
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.464978
policy/dLoss                                  0.0238271
---------------------------------------  ----------------
2021-06-04 13:52:29 | [train_policy] epoch #350 | Obtaining samples for iteration 350...
2021-06-04 13:52:30 | [train_policy] epoch #350 | Logging diagnostics...
2021-06-04 13:52:30 | [train_policy] epoch #350 | Optimizing policy...
2021-06-04 13:52:30 | [train_policy] epoch #350 | Computing loss before
2021-06-04 13:52:30 | [train_policy] epoch #350 | Computing KL before
2021-06-04 13:52:30 | [train_policy] epoch #350 | Optimizing
2021-06-04 13:52:30 | [train_policy] epoch #350 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:30 | [train_policy] epoch #350 | computing loss before
2021-06-04 13:52:30 | [train_policy] epoch #350 | computing gradient
2021-06-04 13:52:30 | [train_policy] epoch #350 | gradient computed
2021-06-04 13:52:30 | [train_policy] epoch #350 | computing descent direction
2021-06-04 13:52:30 | [train_policy] epoch #350 | descent direction computed
2021-06-04 13:52:30 | [train_policy] epoch #350 | backtrack iters: 1
2021-06-04 13:52:30 | [train_policy] epoch #350 | optimization finished
2021-06-04 13:52:30 | [train_policy] epoch #350 | Computing KL after
2021-06-04 13:52:30 | [train_policy] epoch #350 | Computing loss after
2021-06-04 13:52:30 | [train_policy] epoch #350 | Fitting baseline...
2021-06-04 13:52:30 | [train_policy] epoch #350 | Saving snapshot...
2021-06-04 13:52:30 | [train_policy] epoch #350 | Saved
2021-06-04 13:52:30 | [train_policy] epoch #350 | Time 282.19 s
2021-06-04 13:52:30 | [train_policy] epoch #350 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286821
Evaluation/AverageDiscountedReturn          -43.492
Evaluation/AverageReturn                    -43.492
Evaluation/CompletionRate                     0
Evaluation/Iteration                        350
Evaluation/MaxReturn                        -29.8609
Evaluation/MinReturn                        -84.9211
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.0965
Extras/EpisodeRewardMean                    -43.3792
LinearFeatureBaseline/ExplainedVariance       0.844873
PolicyExecTime                                0.22567
ProcessExecTime                               0.0313981
TotalEnvSteps                            355212
policy/Entropy                               -0.74238
policy/KL                                     0.00647943
policy/KLBefore                               0
policy/LossAfter                             -0.0157932
policy/LossBefore                            -3.06269e-09
policy/Perplexity                             0.47598
policy/dLoss                                  0.0157931
---------------------------------------  ----------------
2021-06-04 13:52:30 | [train_policy] epoch #351 | Obtaining samples for iteration 351...
2021-06-04 13:52:31 | [train_policy] epoch #351 | Logging diagnostics...
2021-06-04 13:52:31 | [train_policy] epoch #351 | Optimizing policy...
2021-06-04 13:52:31 | [train_policy] epoch #351 | Computing loss before
2021-06-04 13:52:31 | [train_policy] epoch #351 | Computing KL before
2021-06-04 13:52:31 | [train_policy] epoch #351 | Optimizing
2021-06-04 13:52:31 | [train_policy] epoch #351 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:31 | [train_policy] epoch #351 | computing loss before
2021-06-04 13:52:31 | [train_policy] epoch #351 | computing gradient
2021-06-04 13:52:31 | [train_policy] epoch #351 | gradient computed
2021-06-04 13:52:31 | [train_policy] epoch #351 | computing descent direction
2021-06-04 13:52:31 | [train_policy] epoch #351 | descent direction computed
2021-06-04 13:52:31 | [train_policy] epoch #351 | backtrack iters: 0
2021-06-04 13:52:31 | [train_policy] epoch #351 | optimization finished
2021-06-04 13:52:31 | [train_policy] epoch #351 | Computing KL after
2021-06-04 13:52:31 | [train_policy] epoch #351 | Computing loss after
2021-06-04 13:52:31 | [train_policy] epoch #351 | Fitting baseline...
2021-06-04 13:52:31 | [train_policy] epoch #351 | Saving snapshot...
2021-06-04 13:52:31 | [train_policy] epoch #351 | Saved
2021-06-04 13:52:31 | [train_policy] epoch #351 | Time 282.97 s
2021-06-04 13:52:31 | [train_policy] epoch #351 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284801
Evaluation/AverageDiscountedReturn          -43.5956
Evaluation/AverageReturn                    -43.5956
Evaluation/CompletionRate                     0
Evaluation/Iteration                        351
Evaluation/MaxReturn                        -29.9811
Evaluation/MinReturn                        -92.7164
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.95128
Extras/EpisodeRewardMean                    -43.3774
LinearFeatureBaseline/ExplainedVariance       0.869788
PolicyExecTime                                0.213912
ProcessExecTime                               0.0312681
TotalEnvSteps                            356224
policy/Entropy                               -0.792807
policy/KL                                     0.00991472
policy/KLBefore                               0
policy/LossAfter                             -0.0195755
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.452573
policy/dLoss                                  0.0195755
---------------------------------------  ----------------
2021-06-04 13:52:31 | [train_policy] epoch #352 | Obtaining samples for iteration 352...
2021-06-04 13:52:31 | [train_policy] epoch #352 | Logging diagnostics...
2021-06-04 13:52:31 | [train_policy] epoch #352 | Optimizing policy...
2021-06-04 13:52:31 | [train_policy] epoch #352 | Computing loss before
2021-06-04 13:52:31 | [train_policy] epoch #352 | Computing KL before
2021-06-04 13:52:31 | [train_policy] epoch #352 | Optimizing
2021-06-04 13:52:31 | [train_policy] epoch #352 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:31 | [train_policy] epoch #352 | computing loss before
2021-06-04 13:52:31 | [train_policy] epoch #352 | computing gradient
2021-06-04 13:52:31 | [train_policy] epoch #352 | gradient computed
2021-06-04 13:52:31 | [train_policy] epoch #352 | computing descent direction
2021-06-04 13:52:31 | [train_policy] epoch #352 | descent direction computed
2021-06-04 13:52:32 | [train_policy] epoch #352 | backtrack iters: 1
2021-06-04 13:52:32 | [train_policy] epoch #352 | optimization finished
2021-06-04 13:52:32 | [train_policy] epoch #352 | Computing KL after
2021-06-04 13:52:32 | [train_policy] epoch #352 | Computing loss after
2021-06-04 13:52:32 | [train_policy] epoch #352 | Fitting baseline...
2021-06-04 13:52:32 | [train_policy] epoch #352 | Saving snapshot...
2021-06-04 13:52:32 | [train_policy] epoch #352 | Saved
2021-06-04 13:52:32 | [train_policy] epoch #352 | Time 283.78 s
2021-06-04 13:52:32 | [train_policy] epoch #352 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284083
Evaluation/AverageDiscountedReturn          -42.4187
Evaluation/AverageReturn                    -42.4187
Evaluation/CompletionRate                     0
Evaluation/Iteration                        352
Evaluation/MaxReturn                        -33.1295
Evaluation/MinReturn                        -85.0006
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.21059
Extras/EpisodeRewardMean                    -42.4979
LinearFeatureBaseline/ExplainedVariance       0.899655
PolicyExecTime                                0.232432
ProcessExecTime                               0.0311635
TotalEnvSteps                            357236
policy/Entropy                               -0.805407
policy/KL                                     0.0066668
policy/KLBefore                               0
policy/LossAfter                             -0.0163107
policy/LossBefore                            -9.65925e-09
policy/Perplexity                             0.446906
policy/dLoss                                  0.0163107
---------------------------------------  ----------------
2021-06-04 13:52:32 | [train_policy] epoch #353 | Obtaining samples for iteration 353...
2021-06-04 13:52:32 | [train_policy] epoch #353 | Logging diagnostics...
2021-06-04 13:52:32 | [train_policy] epoch #353 | Optimizing policy...
2021-06-04 13:52:32 | [train_policy] epoch #353 | Computing loss before
2021-06-04 13:52:32 | [train_policy] epoch #353 | Computing KL before
2021-06-04 13:52:32 | [train_policy] epoch #353 | Optimizing
2021-06-04 13:52:32 | [train_policy] epoch #353 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:32 | [train_policy] epoch #353 | computing loss before
2021-06-04 13:52:32 | [train_policy] epoch #353 | computing gradient
2021-06-04 13:52:32 | [train_policy] epoch #353 | gradient computed
2021-06-04 13:52:32 | [train_policy] epoch #353 | computing descent direction
2021-06-04 13:52:32 | [train_policy] epoch #353 | descent direction computed
2021-06-04 13:52:32 | [train_policy] epoch #353 | backtrack iters: 1
2021-06-04 13:52:32 | [train_policy] epoch #353 | optimization finished
2021-06-04 13:52:32 | [train_policy] epoch #353 | Computing KL after
2021-06-04 13:52:32 | [train_policy] epoch #353 | Computing loss after
2021-06-04 13:52:32 | [train_policy] epoch #353 | Fitting baseline...
2021-06-04 13:52:32 | [train_policy] epoch #353 | Saving snapshot...
2021-06-04 13:52:32 | [train_policy] epoch #353 | Saved
2021-06-04 13:52:32 | [train_policy] epoch #353 | Time 284.58 s
2021-06-04 13:52:32 | [train_policy] epoch #353 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286134
Evaluation/AverageDiscountedReturn          -44.5578
Evaluation/AverageReturn                    -44.5578
Evaluation/CompletionRate                     0
Evaluation/Iteration                        353
Evaluation/MaxReturn                        -29.5803
Evaluation/MinReturn                        -83.0089
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.46622
Extras/EpisodeRewardMean                    -44.4386
LinearFeatureBaseline/ExplainedVariance       0.895073
PolicyExecTime                                0.233882
ProcessExecTime                               0.0313902
TotalEnvSteps                            358248
policy/Entropy                               -0.865126
policy/KL                                     0.00672851
policy/KLBefore                               0
policy/LossAfter                             -0.0150566
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.420999
policy/dLoss                                  0.0150566
---------------------------------------  ----------------
2021-06-04 13:52:32 | [train_policy] epoch #354 | Obtaining samples for iteration 354...
2021-06-04 13:52:33 | [train_policy] epoch #354 | Logging diagnostics...
2021-06-04 13:52:33 | [train_policy] epoch #354 | Optimizing policy...
2021-06-04 13:52:33 | [train_policy] epoch #354 | Computing loss before
2021-06-04 13:52:33 | [train_policy] epoch #354 | Computing KL before
2021-06-04 13:52:33 | [train_policy] epoch #354 | Optimizing
2021-06-04 13:52:33 | [train_policy] epoch #354 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:33 | [train_policy] epoch #354 | computing loss before
2021-06-04 13:52:33 | [train_policy] epoch #354 | computing gradient
2021-06-04 13:52:33 | [train_policy] epoch #354 | gradient computed
2021-06-04 13:52:33 | [train_policy] epoch #354 | computing descent direction
2021-06-04 13:52:33 | [train_policy] epoch #354 | descent direction computed
2021-06-04 13:52:33 | [train_policy] epoch #354 | backtrack iters: 1
2021-06-04 13:52:33 | [train_policy] epoch #354 | optimization finished
2021-06-04 13:52:33 | [train_policy] epoch #354 | Computing KL after
2021-06-04 13:52:33 | [train_policy] epoch #354 | Computing loss after
2021-06-04 13:52:33 | [train_policy] epoch #354 | Fitting baseline...
2021-06-04 13:52:33 | [train_policy] epoch #354 | Saving snapshot...
2021-06-04 13:52:33 | [train_policy] epoch #354 | Saved
2021-06-04 13:52:33 | [train_policy] epoch #354 | Time 285.39 s
2021-06-04 13:52:33 | [train_policy] epoch #354 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.289222
Evaluation/AverageDiscountedReturn          -43.5137
Evaluation/AverageReturn                    -43.5137
Evaluation/CompletionRate                     0
Evaluation/Iteration                        354
Evaluation/MaxReturn                        -31.1266
Evaluation/MinReturn                        -82.3784
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.5372
Extras/EpisodeRewardMean                    -43.737
LinearFeatureBaseline/ExplainedVariance       0.561126
PolicyExecTime                                0.232131
ProcessExecTime                               0.0314686
TotalEnvSteps                            359260
policy/Entropy                               -0.871403
policy/KL                                     0.00651978
policy/KLBefore                               0
policy/LossAfter                             -0.00915494
policy/LossBefore                             9.89484e-09
policy/Perplexity                             0.418364
policy/dLoss                                  0.00915495
---------------------------------------  ----------------
2021-06-04 13:52:33 | [train_policy] epoch #355 | Obtaining samples for iteration 355...
2021-06-04 13:52:34 | [train_policy] epoch #355 | Logging diagnostics...
2021-06-04 13:52:34 | [train_policy] epoch #355 | Optimizing policy...
2021-06-04 13:52:34 | [train_policy] epoch #355 | Computing loss before
2021-06-04 13:52:34 | [train_policy] epoch #355 | Computing KL before
2021-06-04 13:52:34 | [train_policy] epoch #355 | Optimizing
2021-06-04 13:52:34 | [train_policy] epoch #355 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:34 | [train_policy] epoch #355 | computing loss before
2021-06-04 13:52:34 | [train_policy] epoch #355 | computing gradient
2021-06-04 13:52:34 | [train_policy] epoch #355 | gradient computed
2021-06-04 13:52:34 | [train_policy] epoch #355 | computing descent direction
2021-06-04 13:52:34 | [train_policy] epoch #355 | descent direction computed
2021-06-04 13:52:34 | [train_policy] epoch #355 | backtrack iters: 0
2021-06-04 13:52:34 | [train_policy] epoch #355 | optimization finished
2021-06-04 13:52:34 | [train_policy] epoch #355 | Computing KL after
2021-06-04 13:52:34 | [train_policy] epoch #355 | Computing loss after
2021-06-04 13:52:34 | [train_policy] epoch #355 | Fitting baseline...
2021-06-04 13:52:34 | [train_policy] epoch #355 | Saving snapshot...
2021-06-04 13:52:34 | [train_policy] epoch #355 | Saved
2021-06-04 13:52:34 | [train_policy] epoch #355 | Time 286.18 s
2021-06-04 13:52:34 | [train_policy] epoch #355 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284351
Evaluation/AverageDiscountedReturn          -42.1651
Evaluation/AverageReturn                    -42.1651
Evaluation/CompletionRate                     0
Evaluation/Iteration                        355
Evaluation/MaxReturn                        -31.6632
Evaluation/MinReturn                        -64.0316
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.58835
Extras/EpisodeRewardMean                    -42.0896
LinearFeatureBaseline/ExplainedVariance       0.921337
PolicyExecTime                                0.216651
ProcessExecTime                               0.0312102
TotalEnvSteps                            360272
policy/Entropy                               -0.866328
policy/KL                                     0.00941326
policy/KLBefore                               0
policy/LossAfter                             -0.0174009
policy/LossBefore                            -6.83215e-09
policy/Perplexity                             0.420493
policy/dLoss                                  0.0174009
---------------------------------------  ----------------
2021-06-04 13:52:34 | [train_policy] epoch #356 | Obtaining samples for iteration 356...
2021-06-04 13:52:35 | [train_policy] epoch #356 | Logging diagnostics...
2021-06-04 13:52:35 | [train_policy] epoch #356 | Optimizing policy...
2021-06-04 13:52:35 | [train_policy] epoch #356 | Computing loss before
2021-06-04 13:52:35 | [train_policy] epoch #356 | Computing KL before
2021-06-04 13:52:35 | [train_policy] epoch #356 | Optimizing
2021-06-04 13:52:35 | [train_policy] epoch #356 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:35 | [train_policy] epoch #356 | computing loss before
2021-06-04 13:52:35 | [train_policy] epoch #356 | computing gradient
2021-06-04 13:52:35 | [train_policy] epoch #356 | gradient computed
2021-06-04 13:52:35 | [train_policy] epoch #356 | computing descent direction
2021-06-04 13:52:35 | [train_policy] epoch #356 | descent direction computed
2021-06-04 13:52:35 | [train_policy] epoch #356 | backtrack iters: 0
2021-06-04 13:52:35 | [train_policy] epoch #356 | optimization finished
2021-06-04 13:52:35 | [train_policy] epoch #356 | Computing KL after
2021-06-04 13:52:35 | [train_policy] epoch #356 | Computing loss after
2021-06-04 13:52:35 | [train_policy] epoch #356 | Fitting baseline...
2021-06-04 13:52:35 | [train_policy] epoch #356 | Saving snapshot...
2021-06-04 13:52:35 | [train_policy] epoch #356 | Saved
2021-06-04 13:52:35 | [train_policy] epoch #356 | Time 286.98 s
2021-06-04 13:52:35 | [train_policy] epoch #356 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285008
Evaluation/AverageDiscountedReturn          -43.4452
Evaluation/AverageReturn                    -43.4452
Evaluation/CompletionRate                     0
Evaluation/Iteration                        356
Evaluation/MaxReturn                        -32.7555
Evaluation/MinReturn                        -64.1078
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.09966
Extras/EpisodeRewardMean                    -43.1963
LinearFeatureBaseline/ExplainedVariance       0.919506
PolicyExecTime                                0.232783
ProcessExecTime                               0.0312028
TotalEnvSteps                            361284
policy/Entropy                               -0.844728
policy/KL                                     0.00965923
policy/KLBefore                               0
policy/LossAfter                             -0.019245
policy/LossBefore                            -7.56838e-09
policy/Perplexity                             0.429674
policy/dLoss                                  0.0192449
---------------------------------------  ----------------
2021-06-04 13:52:35 | [train_policy] epoch #357 | Obtaining samples for iteration 357...
2021-06-04 13:52:35 | [train_policy] epoch #357 | Logging diagnostics...
2021-06-04 13:52:35 | [train_policy] epoch #357 | Optimizing policy...
2021-06-04 13:52:35 | [train_policy] epoch #357 | Computing loss before
2021-06-04 13:52:35 | [train_policy] epoch #357 | Computing KL before
2021-06-04 13:52:35 | [train_policy] epoch #357 | Optimizing
2021-06-04 13:52:35 | [train_policy] epoch #357 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:35 | [train_policy] epoch #357 | computing loss before
2021-06-04 13:52:35 | [train_policy] epoch #357 | computing gradient
2021-06-04 13:52:35 | [train_policy] epoch #357 | gradient computed
2021-06-04 13:52:35 | [train_policy] epoch #357 | computing descent direction
2021-06-04 13:52:36 | [train_policy] epoch #357 | descent direction computed
2021-06-04 13:52:36 | [train_policy] epoch #357 | backtrack iters: 0
2021-06-04 13:52:36 | [train_policy] epoch #357 | optimization finished
2021-06-04 13:52:36 | [train_policy] epoch #357 | Computing KL after
2021-06-04 13:52:36 | [train_policy] epoch #357 | Computing loss after
2021-06-04 13:52:36 | [train_policy] epoch #357 | Fitting baseline...
2021-06-04 13:52:36 | [train_policy] epoch #357 | Saving snapshot...
2021-06-04 13:52:36 | [train_policy] epoch #357 | Saved
2021-06-04 13:52:36 | [train_policy] epoch #357 | Time 287.79 s
2021-06-04 13:52:36 | [train_policy] epoch #357 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284605
Evaluation/AverageDiscountedReturn          -44.5158
Evaluation/AverageReturn                    -44.5158
Evaluation/CompletionRate                     0
Evaluation/Iteration                        357
Evaluation/MaxReturn                        -30.041
Evaluation/MinReturn                        -81.9595
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.83629
Extras/EpisodeRewardMean                    -44.7601
LinearFeatureBaseline/ExplainedVariance       0.888489
PolicyExecTime                                0.23897
ProcessExecTime                               0.0311003
TotalEnvSteps                            362296
policy/Entropy                               -0.8401
policy/KL                                     0.00988915
policy/KLBefore                               0
policy/LossAfter                             -0.0268195
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.431667
policy/dLoss                                  0.0268195
---------------------------------------  ----------------
2021-06-04 13:52:36 | [train_policy] epoch #358 | Obtaining samples for iteration 358...
2021-06-04 13:52:36 | [train_policy] epoch #358 | Logging diagnostics...
2021-06-04 13:52:36 | [train_policy] epoch #358 | Optimizing policy...
2021-06-04 13:52:36 | [train_policy] epoch #358 | Computing loss before
2021-06-04 13:52:36 | [train_policy] epoch #358 | Computing KL before
2021-06-04 13:52:36 | [train_policy] epoch #358 | Optimizing
2021-06-04 13:52:36 | [train_policy] epoch #358 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:36 | [train_policy] epoch #358 | computing loss before
2021-06-04 13:52:36 | [train_policy] epoch #358 | computing gradient
2021-06-04 13:52:36 | [train_policy] epoch #358 | gradient computed
2021-06-04 13:52:36 | [train_policy] epoch #358 | computing descent direction
2021-06-04 13:52:36 | [train_policy] epoch #358 | descent direction computed
2021-06-04 13:52:36 | [train_policy] epoch #358 | backtrack iters: 0
2021-06-04 13:52:36 | [train_policy] epoch #358 | optimization finished
2021-06-04 13:52:36 | [train_policy] epoch #358 | Computing KL after
2021-06-04 13:52:36 | [train_policy] epoch #358 | Computing loss after
2021-06-04 13:52:36 | [train_policy] epoch #358 | Fitting baseline...
2021-06-04 13:52:36 | [train_policy] epoch #358 | Saving snapshot...
2021-06-04 13:52:36 | [train_policy] epoch #358 | Saved
2021-06-04 13:52:36 | [train_policy] epoch #358 | Time 288.58 s
2021-06-04 13:52:36 | [train_policy] epoch #358 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285792
Evaluation/AverageDiscountedReturn          -42.4348
Evaluation/AverageReturn                    -42.4348
Evaluation/CompletionRate                     0
Evaluation/Iteration                        358
Evaluation/MaxReturn                        -30.8936
Evaluation/MinReturn                        -83.6626
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25752
Extras/EpisodeRewardMean                    -42.3493
LinearFeatureBaseline/ExplainedVariance       0.883764
PolicyExecTime                                0.224061
ProcessExecTime                               0.0313904
TotalEnvSteps                            363308
policy/Entropy                               -0.822065
policy/KL                                     0.00960256
policy/KLBefore                               0
policy/LossAfter                             -0.0191693
policy/LossBefore                             6.00758e-08
policy/Perplexity                             0.439523
policy/dLoss                                  0.0191694
---------------------------------------  ----------------
2021-06-04 13:52:36 | [train_policy] epoch #359 | Obtaining samples for iteration 359...
2021-06-04 13:52:37 | [train_policy] epoch #359 | Logging diagnostics...
2021-06-04 13:52:37 | [train_policy] epoch #359 | Optimizing policy...
2021-06-04 13:52:37 | [train_policy] epoch #359 | Computing loss before
2021-06-04 13:52:37 | [train_policy] epoch #359 | Computing KL before
2021-06-04 13:52:37 | [train_policy] epoch #359 | Optimizing
2021-06-04 13:52:37 | [train_policy] epoch #359 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:37 | [train_policy] epoch #359 | computing loss before
2021-06-04 13:52:37 | [train_policy] epoch #359 | computing gradient
2021-06-04 13:52:37 | [train_policy] epoch #359 | gradient computed
2021-06-04 13:52:37 | [train_policy] epoch #359 | computing descent direction
2021-06-04 13:52:37 | [train_policy] epoch #359 | descent direction computed
2021-06-04 13:52:37 | [train_policy] epoch #359 | backtrack iters: 0
2021-06-04 13:52:37 | [train_policy] epoch #359 | optimization finished
2021-06-04 13:52:37 | [train_policy] epoch #359 | Computing KL after
2021-06-04 13:52:37 | [train_policy] epoch #359 | Computing loss after
2021-06-04 13:52:37 | [train_policy] epoch #359 | Fitting baseline...
2021-06-04 13:52:37 | [train_policy] epoch #359 | Saving snapshot...
2021-06-04 13:52:37 | [train_policy] epoch #359 | Saved
2021-06-04 13:52:37 | [train_policy] epoch #359 | Time 289.39 s
2021-06-04 13:52:37 | [train_policy] epoch #359 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.289143
Evaluation/AverageDiscountedReturn          -43.8187
Evaluation/AverageReturn                    -43.8187
Evaluation/CompletionRate                     0
Evaluation/Iteration                        359
Evaluation/MaxReturn                        -31.9831
Evaluation/MinReturn                        -82.8445
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.26249
Extras/EpisodeRewardMean                    -44.2709
LinearFeatureBaseline/ExplainedVariance       0.878119
PolicyExecTime                                0.235432
ProcessExecTime                               0.0315845
TotalEnvSteps                            364320
policy/Entropy                               -0.836779
policy/KL                                     0.00986519
policy/KLBefore                               0
policy/LossAfter                             -0.0306708
policy/LossBefore                            -1.17796e-09
policy/Perplexity                             0.433104
policy/dLoss                                  0.0306708
---------------------------------------  ----------------
2021-06-04 13:52:37 | [train_policy] epoch #360 | Obtaining samples for iteration 360...
2021-06-04 13:52:38 | [train_policy] epoch #360 | Logging diagnostics...
2021-06-04 13:52:38 | [train_policy] epoch #360 | Optimizing policy...
2021-06-04 13:52:38 | [train_policy] epoch #360 | Computing loss before
2021-06-04 13:52:38 | [train_policy] epoch #360 | Computing KL before
2021-06-04 13:52:38 | [train_policy] epoch #360 | Optimizing
2021-06-04 13:52:38 | [train_policy] epoch #360 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:38 | [train_policy] epoch #360 | computing loss before
2021-06-04 13:52:38 | [train_policy] epoch #360 | computing gradient
2021-06-04 13:52:38 | [train_policy] epoch #360 | gradient computed
2021-06-04 13:52:38 | [train_policy] epoch #360 | computing descent direction
2021-06-04 13:52:38 | [train_policy] epoch #360 | descent direction computed
2021-06-04 13:52:38 | [train_policy] epoch #360 | backtrack iters: 1
2021-06-04 13:52:38 | [train_policy] epoch #360 | optimization finished
2021-06-04 13:52:38 | [train_policy] epoch #360 | Computing KL after
2021-06-04 13:52:38 | [train_policy] epoch #360 | Computing loss after
2021-06-04 13:52:38 | [train_policy] epoch #360 | Fitting baseline...
2021-06-04 13:52:38 | [train_policy] epoch #360 | Saving snapshot...
2021-06-04 13:52:38 | [train_policy] epoch #360 | Saved
2021-06-04 13:52:38 | [train_policy] epoch #360 | Time 290.20 s
2021-06-04 13:52:38 | [train_policy] epoch #360 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284811
Evaluation/AverageDiscountedReturn          -43.859
Evaluation/AverageReturn                    -43.859
Evaluation/CompletionRate                     0
Evaluation/Iteration                        360
Evaluation/MaxReturn                        -31.9667
Evaluation/MinReturn                        -98.2042
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.2849
Extras/EpisodeRewardMean                    -43.638
LinearFeatureBaseline/ExplainedVariance       0.806977
PolicyExecTime                                0.230817
ProcessExecTime                               0.0312114
TotalEnvSteps                            365332
policy/Entropy                               -0.861015
policy/KL                                     0.00677468
policy/KLBefore                               0
policy/LossAfter                             -0.0206035
policy/LossBefore                             1.69626e-08
policy/Perplexity                             0.422733
policy/dLoss                                  0.0206035
---------------------------------------  ----------------
2021-06-04 13:52:38 | [train_policy] epoch #361 | Obtaining samples for iteration 361...
2021-06-04 13:52:39 | [train_policy] epoch #361 | Logging diagnostics...
2021-06-04 13:52:39 | [train_policy] epoch #361 | Optimizing policy...
2021-06-04 13:52:39 | [train_policy] epoch #361 | Computing loss before
2021-06-04 13:52:39 | [train_policy] epoch #361 | Computing KL before
2021-06-04 13:52:39 | [train_policy] epoch #361 | Optimizing
2021-06-04 13:52:39 | [train_policy] epoch #361 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:39 | [train_policy] epoch #361 | computing loss before
2021-06-04 13:52:39 | [train_policy] epoch #361 | computing gradient
2021-06-04 13:52:39 | [train_policy] epoch #361 | gradient computed
2021-06-04 13:52:39 | [train_policy] epoch #361 | computing descent direction
2021-06-04 13:52:39 | [train_policy] epoch #361 | descent direction computed
2021-06-04 13:52:39 | [train_policy] epoch #361 | backtrack iters: 1
2021-06-04 13:52:39 | [train_policy] epoch #361 | optimization finished
2021-06-04 13:52:39 | [train_policy] epoch #361 | Computing KL after
2021-06-04 13:52:39 | [train_policy] epoch #361 | Computing loss after
2021-06-04 13:52:39 | [train_policy] epoch #361 | Fitting baseline...
2021-06-04 13:52:39 | [train_policy] epoch #361 | Saving snapshot...
2021-06-04 13:52:39 | [train_policy] epoch #361 | Saved
2021-06-04 13:52:39 | [train_policy] epoch #361 | Time 290.99 s
2021-06-04 13:52:39 | [train_policy] epoch #361 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286011
Evaluation/AverageDiscountedReturn          -41.7544
Evaluation/AverageReturn                    -41.7544
Evaluation/CompletionRate                     0
Evaluation/Iteration                        361
Evaluation/MaxReturn                        -30.7259
Evaluation/MinReturn                        -58.2328
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.4967
Extras/EpisodeRewardMean                    -41.9582
LinearFeatureBaseline/ExplainedVariance       0.801267
PolicyExecTime                                0.222108
ProcessExecTime                               0.03128
TotalEnvSteps                            366344
policy/Entropy                               -0.864325
policy/KL                                     0.00654806
policy/KLBefore                               0
policy/LossAfter                             -0.0243113
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.421336
policy/dLoss                                  0.0243113
---------------------------------------  ----------------
2021-06-04 13:52:39 | [train_policy] epoch #362 | Obtaining samples for iteration 362...
2021-06-04 13:52:39 | [train_policy] epoch #362 | Logging diagnostics...
2021-06-04 13:52:39 | [train_policy] epoch #362 | Optimizing policy...
2021-06-04 13:52:39 | [train_policy] epoch #362 | Computing loss before
2021-06-04 13:52:39 | [train_policy] epoch #362 | Computing KL before
2021-06-04 13:52:39 | [train_policy] epoch #362 | Optimizing
2021-06-04 13:52:39 | [train_policy] epoch #362 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:39 | [train_policy] epoch #362 | computing loss before
2021-06-04 13:52:39 | [train_policy] epoch #362 | computing gradient
2021-06-04 13:52:39 | [train_policy] epoch #362 | gradient computed
2021-06-04 13:52:39 | [train_policy] epoch #362 | computing descent direction
2021-06-04 13:52:40 | [train_policy] epoch #362 | descent direction computed
2021-06-04 13:52:40 | [train_policy] epoch #362 | backtrack iters: 0
2021-06-04 13:52:40 | [train_policy] epoch #362 | optimization finished
2021-06-04 13:52:40 | [train_policy] epoch #362 | Computing KL after
2021-06-04 13:52:40 | [train_policy] epoch #362 | Computing loss after
2021-06-04 13:52:40 | [train_policy] epoch #362 | Fitting baseline...
2021-06-04 13:52:40 | [train_policy] epoch #362 | Saving snapshot...
2021-06-04 13:52:40 | [train_policy] epoch #362 | Saved
2021-06-04 13:52:40 | [train_policy] epoch #362 | Time 291.78 s
2021-06-04 13:52:40 | [train_policy] epoch #362 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284117
Evaluation/AverageDiscountedReturn          -42.5812
Evaluation/AverageReturn                    -42.5812
Evaluation/CompletionRate                     0
Evaluation/Iteration                        362
Evaluation/MaxReturn                        -32.1951
Evaluation/MinReturn                        -64.0026
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.32842
Extras/EpisodeRewardMean                    -42.3646
LinearFeatureBaseline/ExplainedVariance       0.913515
PolicyExecTime                                0.228792
ProcessExecTime                               0.0311263
TotalEnvSteps                            367356
policy/Entropy                               -0.889986
policy/KL                                     0.00967759
policy/KLBefore                               0
policy/LossAfter                             -0.0177675
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.410662
policy/dLoss                                  0.0177675
---------------------------------------  ----------------
2021-06-04 13:52:40 | [train_policy] epoch #363 | Obtaining samples for iteration 363...
2021-06-04 13:52:40 | [train_policy] epoch #363 | Logging diagnostics...
2021-06-04 13:52:40 | [train_policy] epoch #363 | Optimizing policy...
2021-06-04 13:52:40 | [train_policy] epoch #363 | Computing loss before
2021-06-04 13:52:40 | [train_policy] epoch #363 | Computing KL before
2021-06-04 13:52:40 | [train_policy] epoch #363 | Optimizing
2021-06-04 13:52:40 | [train_policy] epoch #363 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:40 | [train_policy] epoch #363 | computing loss before
2021-06-04 13:52:40 | [train_policy] epoch #363 | computing gradient
2021-06-04 13:52:40 | [train_policy] epoch #363 | gradient computed
2021-06-04 13:52:40 | [train_policy] epoch #363 | computing descent direction
2021-06-04 13:52:40 | [train_policy] epoch #363 | descent direction computed
2021-06-04 13:52:40 | [train_policy] epoch #363 | backtrack iters: 0
2021-06-04 13:52:40 | [train_policy] epoch #363 | optimization finished
2021-06-04 13:52:40 | [train_policy] epoch #363 | Computing KL after
2021-06-04 13:52:40 | [train_policy] epoch #363 | Computing loss after
2021-06-04 13:52:40 | [train_policy] epoch #363 | Fitting baseline...
2021-06-04 13:52:40 | [train_policy] epoch #363 | Saving snapshot...
2021-06-04 13:52:40 | [train_policy] epoch #363 | Saved
2021-06-04 13:52:40 | [train_policy] epoch #363 | Time 292.58 s
2021-06-04 13:52:40 | [train_policy] epoch #363 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28731
Evaluation/AverageDiscountedReturn          -42.1124
Evaluation/AverageReturn                    -42.1124
Evaluation/CompletionRate                     0
Evaluation/Iteration                        363
Evaluation/MaxReturn                        -30.3752
Evaluation/MinReturn                        -92.7069
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.22777
Extras/EpisodeRewardMean                    -41.9773
LinearFeatureBaseline/ExplainedVariance       0.846188
PolicyExecTime                                0.231842
ProcessExecTime                               0.0315204
TotalEnvSteps                            368368
policy/Entropy                               -0.893254
policy/KL                                     0.00998628
policy/KLBefore                               0
policy/LossAfter                             -0.0195236
policy/LossBefore                            -7.06774e-10
policy/Perplexity                             0.409322
policy/dLoss                                  0.0195236
---------------------------------------  ----------------
2021-06-04 13:52:40 | [train_policy] epoch #364 | Obtaining samples for iteration 364...
2021-06-04 13:52:41 | [train_policy] epoch #364 | Logging diagnostics...
2021-06-04 13:52:41 | [train_policy] epoch #364 | Optimizing policy...
2021-06-04 13:52:41 | [train_policy] epoch #364 | Computing loss before
2021-06-04 13:52:41 | [train_policy] epoch #364 | Computing KL before
2021-06-04 13:52:41 | [train_policy] epoch #364 | Optimizing
2021-06-04 13:52:41 | [train_policy] epoch #364 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:41 | [train_policy] epoch #364 | computing loss before
2021-06-04 13:52:41 | [train_policy] epoch #364 | computing gradient
2021-06-04 13:52:41 | [train_policy] epoch #364 | gradient computed
2021-06-04 13:52:41 | [train_policy] epoch #364 | computing descent direction
2021-06-04 13:52:41 | [train_policy] epoch #364 | descent direction computed
2021-06-04 13:52:41 | [train_policy] epoch #364 | backtrack iters: 1
2021-06-04 13:52:41 | [train_policy] epoch #364 | optimization finished
2021-06-04 13:52:41 | [train_policy] epoch #364 | Computing KL after
2021-06-04 13:52:41 | [train_policy] epoch #364 | Computing loss after
2021-06-04 13:52:41 | [train_policy] epoch #364 | Fitting baseline...
2021-06-04 13:52:41 | [train_policy] epoch #364 | Saving snapshot...
2021-06-04 13:52:41 | [train_policy] epoch #364 | Saved
2021-06-04 13:52:41 | [train_policy] epoch #364 | Time 293.40 s
2021-06-04 13:52:41 | [train_policy] epoch #364 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284095
Evaluation/AverageDiscountedReturn          -64.2383
Evaluation/AverageReturn                    -64.2383
Evaluation/CompletionRate                     0
Evaluation/Iteration                        364
Evaluation/MaxReturn                        -29.5977
Evaluation/MinReturn                      -2065.29
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.893
Extras/EpisodeRewardMean                    -62.5574
LinearFeatureBaseline/ExplainedVariance       0.0156304
PolicyExecTime                                0.236521
ProcessExecTime                               0.0311389
TotalEnvSteps                            369380
policy/Entropy                               -0.900427
policy/KL                                     0.00676689
policy/KLBefore                               0
policy/LossAfter                             -0.0328291
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.406396
policy/dLoss                                  0.0328291
---------------------------------------  ----------------
2021-06-04 13:52:41 | [train_policy] epoch #365 | Obtaining samples for iteration 365...
2021-06-04 13:52:42 | [train_policy] epoch #365 | Logging diagnostics...
2021-06-04 13:52:42 | [train_policy] epoch #365 | Optimizing policy...
2021-06-04 13:52:42 | [train_policy] epoch #365 | Computing loss before
2021-06-04 13:52:42 | [train_policy] epoch #365 | Computing KL before
2021-06-04 13:52:42 | [train_policy] epoch #365 | Optimizing
2021-06-04 13:52:42 | [train_policy] epoch #365 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:42 | [train_policy] epoch #365 | computing loss before
2021-06-04 13:52:42 | [train_policy] epoch #365 | computing gradient
2021-06-04 13:52:42 | [train_policy] epoch #365 | gradient computed
2021-06-04 13:52:42 | [train_policy] epoch #365 | computing descent direction
2021-06-04 13:52:42 | [train_policy] epoch #365 | descent direction computed
2021-06-04 13:52:42 | [train_policy] epoch #365 | backtrack iters: 1
2021-06-04 13:52:42 | [train_policy] epoch #365 | optimization finished
2021-06-04 13:52:42 | [train_policy] epoch #365 | Computing KL after
2021-06-04 13:52:42 | [train_policy] epoch #365 | Computing loss after
2021-06-04 13:52:42 | [train_policy] epoch #365 | Fitting baseline...
2021-06-04 13:52:42 | [train_policy] epoch #365 | Saving snapshot...
2021-06-04 13:52:42 | [train_policy] epoch #365 | Saved
2021-06-04 13:52:42 | [train_policy] epoch #365 | Time 294.20 s
2021-06-04 13:52:42 | [train_policy] epoch #365 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284755
Evaluation/AverageDiscountedReturn          -65.8275
Evaluation/AverageReturn                    -65.8275
Evaluation/CompletionRate                     0
Evaluation/Iteration                        365
Evaluation/MaxReturn                        -31.2418
Evaluation/MinReturn                      -2062.11
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.578
Extras/EpisodeRewardMean                    -63.8611
LinearFeatureBaseline/ExplainedVariance      -0.00434401
PolicyExecTime                                0.229828
ProcessExecTime                               0.0312889
TotalEnvSteps                            370392
policy/Entropy                               -0.899165
policy/KL                                     0.00657797
policy/KLBefore                               0
policy/LossAfter                             -0.0187674
policy/LossBefore                            -1.64914e-09
policy/Perplexity                             0.406909
policy/dLoss                                  0.0187674
---------------------------------------  ----------------
2021-06-04 13:52:42 | [train_policy] epoch #366 | Obtaining samples for iteration 366...
2021-06-04 13:52:43 | [train_policy] epoch #366 | Logging diagnostics...
2021-06-04 13:52:43 | [train_policy] epoch #366 | Optimizing policy...
2021-06-04 13:52:43 | [train_policy] epoch #366 | Computing loss before
2021-06-04 13:52:43 | [train_policy] epoch #366 | Computing KL before
2021-06-04 13:52:43 | [train_policy] epoch #366 | Optimizing
2021-06-04 13:52:43 | [train_policy] epoch #366 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:43 | [train_policy] epoch #366 | computing loss before
2021-06-04 13:52:43 | [train_policy] epoch #366 | computing gradient
2021-06-04 13:52:43 | [train_policy] epoch #366 | gradient computed
2021-06-04 13:52:43 | [train_policy] epoch #366 | computing descent direction
2021-06-04 13:52:43 | [train_policy] epoch #366 | descent direction computed
2021-06-04 13:52:43 | [train_policy] epoch #366 | backtrack iters: 0
2021-06-04 13:52:43 | [train_policy] epoch #366 | optimization finished
2021-06-04 13:52:43 | [train_policy] epoch #366 | Computing KL after
2021-06-04 13:52:43 | [train_policy] epoch #366 | Computing loss after
2021-06-04 13:52:43 | [train_policy] epoch #366 | Fitting baseline...
2021-06-04 13:52:43 | [train_policy] epoch #366 | Saving snapshot...
2021-06-04 13:52:43 | [train_policy] epoch #366 | Saved
2021-06-04 13:52:43 | [train_policy] epoch #366 | Time 294.99 s
2021-06-04 13:52:43 | [train_policy] epoch #366 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284411
Evaluation/AverageDiscountedReturn          -41.1997
Evaluation/AverageReturn                    -41.1997
Evaluation/CompletionRate                     0
Evaluation/Iteration                        366
Evaluation/MaxReturn                        -30.9589
Evaluation/MinReturn                        -63.4705
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.12096
Extras/EpisodeRewardMean                    -41.9077
LinearFeatureBaseline/ExplainedVariance      -9.72851
PolicyExecTime                                0.226212
ProcessExecTime                               0.031148
TotalEnvSteps                            371404
policy/Entropy                               -0.889894
policy/KL                                     0.00985556
policy/KLBefore                               0
policy/LossAfter                             -0.0203789
policy/LossBefore                            -6.83215e-09
policy/Perplexity                             0.410699
policy/dLoss                                  0.0203789
---------------------------------------  ----------------
2021-06-04 13:52:43 | [train_policy] epoch #367 | Obtaining samples for iteration 367...
2021-06-04 13:52:43 | [train_policy] epoch #367 | Logging diagnostics...
2021-06-04 13:52:43 | [train_policy] epoch #367 | Optimizing policy...
2021-06-04 13:52:43 | [train_policy] epoch #367 | Computing loss before
2021-06-04 13:52:43 | [train_policy] epoch #367 | Computing KL before
2021-06-04 13:52:43 | [train_policy] epoch #367 | Optimizing
2021-06-04 13:52:43 | [train_policy] epoch #367 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:43 | [train_policy] epoch #367 | computing loss before
2021-06-04 13:52:43 | [train_policy] epoch #367 | computing gradient
2021-06-04 13:52:43 | [train_policy] epoch #367 | gradient computed
2021-06-04 13:52:43 | [train_policy] epoch #367 | computing descent direction
2021-06-04 13:52:44 | [train_policy] epoch #367 | descent direction computed
2021-06-04 13:52:44 | [train_policy] epoch #367 | backtrack iters: 1
2021-06-04 13:52:44 | [train_policy] epoch #367 | optimization finished
2021-06-04 13:52:44 | [train_policy] epoch #367 | Computing KL after
2021-06-04 13:52:44 | [train_policy] epoch #367 | Computing loss after
2021-06-04 13:52:44 | [train_policy] epoch #367 | Fitting baseline...
2021-06-04 13:52:44 | [train_policy] epoch #367 | Saving snapshot...
2021-06-04 13:52:44 | [train_policy] epoch #367 | Saved
2021-06-04 13:52:44 | [train_policy] epoch #367 | Time 295.79 s
2021-06-04 13:52:44 | [train_policy] epoch #367 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284077
Evaluation/AverageDiscountedReturn          -41.1345
Evaluation/AverageReturn                    -41.1345
Evaluation/CompletionRate                     0
Evaluation/Iteration                        367
Evaluation/MaxReturn                        -30.5826
Evaluation/MinReturn                        -57.0672
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.17383
Extras/EpisodeRewardMean                    -40.874
LinearFeatureBaseline/ExplainedVariance       0.925998
PolicyExecTime                                0.234019
ProcessExecTime                               0.0311723
TotalEnvSteps                            372416
policy/Entropy                               -0.895738
policy/KL                                     0.00649782
policy/KLBefore                               0
policy/LossAfter                             -0.0190755
policy/LossBefore                             6.36097e-09
policy/Perplexity                             0.408306
policy/dLoss                                  0.0190755
---------------------------------------  ----------------
2021-06-04 13:52:44 | [train_policy] epoch #368 | Obtaining samples for iteration 368...
2021-06-04 13:52:44 | [train_policy] epoch #368 | Logging diagnostics...
2021-06-04 13:52:44 | [train_policy] epoch #368 | Optimizing policy...
2021-06-04 13:52:44 | [train_policy] epoch #368 | Computing loss before
2021-06-04 13:52:44 | [train_policy] epoch #368 | Computing KL before
2021-06-04 13:52:44 | [train_policy] epoch #368 | Optimizing
2021-06-04 13:52:44 | [train_policy] epoch #368 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:44 | [train_policy] epoch #368 | computing loss before
2021-06-04 13:52:44 | [train_policy] epoch #368 | computing gradient
2021-06-04 13:52:44 | [train_policy] epoch #368 | gradient computed
2021-06-04 13:52:44 | [train_policy] epoch #368 | computing descent direction
2021-06-04 13:52:44 | [train_policy] epoch #368 | descent direction computed
2021-06-04 13:52:44 | [train_policy] epoch #368 | backtrack iters: 1
2021-06-04 13:52:44 | [train_policy] epoch #368 | optimization finished
2021-06-04 13:52:44 | [train_policy] epoch #368 | Computing KL after
2021-06-04 13:52:44 | [train_policy] epoch #368 | Computing loss after
2021-06-04 13:52:44 | [train_policy] epoch #368 | Fitting baseline...
2021-06-04 13:52:44 | [train_policy] epoch #368 | Saving snapshot...
2021-06-04 13:52:44 | [train_policy] epoch #368 | Saved
2021-06-04 13:52:44 | [train_policy] epoch #368 | Time 296.59 s
2021-06-04 13:52:44 | [train_policy] epoch #368 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288461
Evaluation/AverageDiscountedReturn          -86.6789
Evaluation/AverageReturn                    -86.6789
Evaluation/CompletionRate                     0
Evaluation/Iteration                        368
Evaluation/MaxReturn                        -30.6702
Evaluation/MinReturn                      -2062.43
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.628
Extras/EpisodeRewardMean                    -82.7924
LinearFeatureBaseline/ExplainedVariance       0.00867162
PolicyExecTime                                0.227889
ProcessExecTime                               0.0316
TotalEnvSteps                            373428
policy/Entropy                               -0.92004
policy/KL                                     0.00653646
policy/KLBefore                               0
policy/LossAfter                             -0.0250911
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             0.398503
policy/dLoss                                  0.0250911
---------------------------------------  ----------------
2021-06-04 13:52:44 | [train_policy] epoch #369 | Obtaining samples for iteration 369...
2021-06-04 13:52:45 | [train_policy] epoch #369 | Logging diagnostics...
2021-06-04 13:52:45 | [train_policy] epoch #369 | Optimizing policy...
2021-06-04 13:52:45 | [train_policy] epoch #369 | Computing loss before
2021-06-04 13:52:45 | [train_policy] epoch #369 | Computing KL before
2021-06-04 13:52:45 | [train_policy] epoch #369 | Optimizing
2021-06-04 13:52:45 | [train_policy] epoch #369 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:45 | [train_policy] epoch #369 | computing loss before
2021-06-04 13:52:45 | [train_policy] epoch #369 | computing gradient
2021-06-04 13:52:45 | [train_policy] epoch #369 | gradient computed
2021-06-04 13:52:45 | [train_policy] epoch #369 | computing descent direction
2021-06-04 13:52:45 | [train_policy] epoch #369 | descent direction computed
2021-06-04 13:52:45 | [train_policy] epoch #369 | backtrack iters: 0
2021-06-04 13:52:45 | [train_policy] epoch #369 | optimization finished
2021-06-04 13:52:45 | [train_policy] epoch #369 | Computing KL after
2021-06-04 13:52:45 | [train_policy] epoch #369 | Computing loss after
2021-06-04 13:52:45 | [train_policy] epoch #369 | Fitting baseline...
2021-06-04 13:52:45 | [train_policy] epoch #369 | Saving snapshot...
2021-06-04 13:52:45 | [train_policy] epoch #369 | Saved
2021-06-04 13:52:45 | [train_policy] epoch #369 | Time 297.41 s
2021-06-04 13:52:45 | [train_policy] epoch #369 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287107
Evaluation/AverageDiscountedReturn          -41.402
Evaluation/AverageReturn                    -41.402
Evaluation/CompletionRate                     0
Evaluation/Iteration                        369
Evaluation/MaxReturn                        -30.5991
Evaluation/MinReturn                        -64.0551
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.27056
Extras/EpisodeRewardMean                    -61.6746
LinearFeatureBaseline/ExplainedVariance    -103.364
PolicyExecTime                                0.237375
ProcessExecTime                               0.0313246
TotalEnvSteps                            374440
policy/Entropy                               -0.893116
policy/KL                                     0.00954669
policy/KLBefore                               0
policy/LossAfter                             -0.0398777
policy/LossBefore                            -3.06269e-08
policy/Perplexity                             0.409378
policy/dLoss                                  0.0398777
---------------------------------------  ----------------
2021-06-04 13:52:45 | [train_policy] epoch #370 | Obtaining samples for iteration 370...
2021-06-04 13:52:46 | [train_policy] epoch #370 | Logging diagnostics...
2021-06-04 13:52:46 | [train_policy] epoch #370 | Optimizing policy...
2021-06-04 13:52:46 | [train_policy] epoch #370 | Computing loss before
2021-06-04 13:52:46 | [train_policy] epoch #370 | Computing KL before
2021-06-04 13:52:46 | [train_policy] epoch #370 | Optimizing
2021-06-04 13:52:46 | [train_policy] epoch #370 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:46 | [train_policy] epoch #370 | computing loss before
2021-06-04 13:52:46 | [train_policy] epoch #370 | computing gradient
2021-06-04 13:52:46 | [train_policy] epoch #370 | gradient computed
2021-06-04 13:52:46 | [train_policy] epoch #370 | computing descent direction
2021-06-04 13:52:46 | [train_policy] epoch #370 | descent direction computed
2021-06-04 13:52:46 | [train_policy] epoch #370 | backtrack iters: 0
2021-06-04 13:52:46 | [train_policy] epoch #370 | optimization finished
2021-06-04 13:52:46 | [train_policy] epoch #370 | Computing KL after
2021-06-04 13:52:46 | [train_policy] epoch #370 | Computing loss after
2021-06-04 13:52:46 | [train_policy] epoch #370 | Fitting baseline...
2021-06-04 13:52:46 | [train_policy] epoch #370 | Saving snapshot...
2021-06-04 13:52:46 | [train_policy] epoch #370 | Saved
2021-06-04 13:52:46 | [train_policy] epoch #370 | Time 298.20 s
2021-06-04 13:52:46 | [train_policy] epoch #370 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28711
Evaluation/AverageDiscountedReturn          -68.2803
Evaluation/AverageReturn                    -68.2803
Evaluation/CompletionRate                     0
Evaluation/Iteration                        370
Evaluation/MaxReturn                        -30.1918
Evaluation/MinReturn                      -2062.57
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        214.627
Extras/EpisodeRewardMean                    -65.8175
LinearFeatureBaseline/ExplainedVariance       0.0142268
PolicyExecTime                                0.223426
ProcessExecTime                               0.0314002
TotalEnvSteps                            375452
policy/Entropy                               -0.87912
policy/KL                                     0.00978677
policy/KLBefore                               0
policy/LossAfter                             -0.0238865
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.415148
policy/dLoss                                  0.0238865
---------------------------------------  ----------------
2021-06-04 13:52:46 | [train_policy] epoch #371 | Obtaining samples for iteration 371...
2021-06-04 13:52:47 | [train_policy] epoch #371 | Logging diagnostics...
2021-06-04 13:52:47 | [train_policy] epoch #371 | Optimizing policy...
2021-06-04 13:52:47 | [train_policy] epoch #371 | Computing loss before
2021-06-04 13:52:47 | [train_policy] epoch #371 | Computing KL before
2021-06-04 13:52:47 | [train_policy] epoch #371 | Optimizing
2021-06-04 13:52:47 | [train_policy] epoch #371 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:47 | [train_policy] epoch #371 | computing loss before
2021-06-04 13:52:47 | [train_policy] epoch #371 | computing gradient
2021-06-04 13:52:47 | [train_policy] epoch #371 | gradient computed
2021-06-04 13:52:47 | [train_policy] epoch #371 | computing descent direction
2021-06-04 13:52:47 | [train_policy] epoch #371 | descent direction computed
2021-06-04 13:52:47 | [train_policy] epoch #371 | backtrack iters: 1
2021-06-04 13:52:47 | [train_policy] epoch #371 | optimization finished
2021-06-04 13:52:47 | [train_policy] epoch #371 | Computing KL after
2021-06-04 13:52:47 | [train_policy] epoch #371 | Computing loss after
2021-06-04 13:52:47 | [train_policy] epoch #371 | Fitting baseline...
2021-06-04 13:52:47 | [train_policy] epoch #371 | Saving snapshot...
2021-06-04 13:52:47 | [train_policy] epoch #371 | Saved
2021-06-04 13:52:47 | [train_policy] epoch #371 | Time 299.00 s
2021-06-04 13:52:47 | [train_policy] epoch #371 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285607
Evaluation/AverageDiscountedReturn          -65.5627
Evaluation/AverageReturn                    -65.5627
Evaluation/CompletionRate                     0
Evaluation/Iteration                        371
Evaluation/MaxReturn                        -31.3031
Evaluation/MinReturn                      -2062.86
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.657
Extras/EpisodeRewardMean                    -63.681
LinearFeatureBaseline/ExplainedVariance       0.110142
PolicyExecTime                                0.213551
ProcessExecTime                               0.03128
TotalEnvSteps                            376464
policy/Entropy                               -0.869629
policy/KL                                     0.00702191
policy/KLBefore                               0
policy/LossAfter                             -0.0210658
policy/LossBefore                            -1.69626e-08
policy/Perplexity                             0.419107
policy/dLoss                                  0.0210658
---------------------------------------  ----------------
2021-06-04 13:52:47 | [train_policy] epoch #372 | Obtaining samples for iteration 372...
2021-06-04 13:52:47 | [train_policy] epoch #372 | Logging diagnostics...
2021-06-04 13:52:47 | [train_policy] epoch #372 | Optimizing policy...
2021-06-04 13:52:47 | [train_policy] epoch #372 | Computing loss before
2021-06-04 13:52:47 | [train_policy] epoch #372 | Computing KL before
2021-06-04 13:52:47 | [train_policy] epoch #372 | Optimizing
2021-06-04 13:52:47 | [train_policy] epoch #372 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:47 | [train_policy] epoch #372 | computing loss before
2021-06-04 13:52:47 | [train_policy] epoch #372 | computing gradient
2021-06-04 13:52:47 | [train_policy] epoch #372 | gradient computed
2021-06-04 13:52:47 | [train_policy] epoch #372 | computing descent direction
2021-06-04 13:52:47 | [train_policy] epoch #372 | descent direction computed
2021-06-04 13:52:48 | [train_policy] epoch #372 | backtrack iters: 0
2021-06-04 13:52:48 | [train_policy] epoch #372 | optimization finished
2021-06-04 13:52:48 | [train_policy] epoch #372 | Computing KL after
2021-06-04 13:52:48 | [train_policy] epoch #372 | Computing loss after
2021-06-04 13:52:48 | [train_policy] epoch #372 | Fitting baseline...
2021-06-04 13:52:48 | [train_policy] epoch #372 | Saving snapshot...
2021-06-04 13:52:48 | [train_policy] epoch #372 | Saved
2021-06-04 13:52:48 | [train_policy] epoch #372 | Time 299.78 s
2021-06-04 13:52:48 | [train_policy] epoch #372 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.285155
Evaluation/AverageDiscountedReturn          -42.8242
Evaluation/AverageReturn                    -42.8242
Evaluation/CompletionRate                     0
Evaluation/Iteration                        372
Evaluation/MaxReturn                        -30.4172
Evaluation/MinReturn                        -85.1154
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.96313
Extras/EpisodeRewardMean                    -62.8941
LinearFeatureBaseline/ExplainedVariance     -24.12
PolicyExecTime                                0.21676
ProcessExecTime                               0.0311234
TotalEnvSteps                            377476
policy/Entropy                               -0.882864
policy/KL                                     0.00961784
policy/KLBefore                               0
policy/LossAfter                             -0.0452563
policy/LossBefore                             1.60202e-08
policy/Perplexity                             0.413597
policy/dLoss                                  0.0452564
---------------------------------------  ----------------
2021-06-04 13:52:48 | [train_policy] epoch #373 | Obtaining samples for iteration 373...
2021-06-04 13:52:48 | [train_policy] epoch #373 | Logging diagnostics...
2021-06-04 13:52:48 | [train_policy] epoch #373 | Optimizing policy...
2021-06-04 13:52:48 | [train_policy] epoch #373 | Computing loss before
2021-06-04 13:52:48 | [train_policy] epoch #373 | Computing KL before
2021-06-04 13:52:48 | [train_policy] epoch #373 | Optimizing
2021-06-04 13:52:48 | [train_policy] epoch #373 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:48 | [train_policy] epoch #373 | computing loss before
2021-06-04 13:52:48 | [train_policy] epoch #373 | computing gradient
2021-06-04 13:52:48 | [train_policy] epoch #373 | gradient computed
2021-06-04 13:52:48 | [train_policy] epoch #373 | computing descent direction
2021-06-04 13:52:48 | [train_policy] epoch #373 | descent direction computed
2021-06-04 13:52:48 | [train_policy] epoch #373 | backtrack iters: 1
2021-06-04 13:52:48 | [train_policy] epoch #373 | optimization finished
2021-06-04 13:52:48 | [train_policy] epoch #373 | Computing KL after
2021-06-04 13:52:48 | [train_policy] epoch #373 | Computing loss after
2021-06-04 13:52:48 | [train_policy] epoch #373 | Fitting baseline...
2021-06-04 13:52:48 | [train_policy] epoch #373 | Saving snapshot...
2021-06-04 13:52:48 | [train_policy] epoch #373 | Saved
2021-06-04 13:52:48 | [train_policy] epoch #373 | Time 300.57 s
2021-06-04 13:52:48 | [train_policy] epoch #373 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.289228
Evaluation/AverageDiscountedReturn          -41.4446
Evaluation/AverageReturn                    -41.4446
Evaluation/CompletionRate                     0
Evaluation/Iteration                        373
Evaluation/MaxReturn                        -30.762
Evaluation/MinReturn                        -64.2947
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.64365
Extras/EpisodeRewardMean                    -41.4895
LinearFeatureBaseline/ExplainedVariance       0.914503
PolicyExecTime                                0.214452
ProcessExecTime                               0.0315671
TotalEnvSteps                            378488
policy/Entropy                               -0.863042
policy/KL                                     0.00657731
policy/KLBefore                               0
policy/LossAfter                             -0.025939
policy/LossBefore                             3.25116e-08
policy/Perplexity                             0.421877
policy/dLoss                                  0.025939
---------------------------------------  ----------------
2021-06-04 13:52:48 | [train_policy] epoch #374 | Obtaining samples for iteration 374...
2021-06-04 13:52:49 | [train_policy] epoch #374 | Logging diagnostics...
2021-06-04 13:52:49 | [train_policy] epoch #374 | Optimizing policy...
2021-06-04 13:52:49 | [train_policy] epoch #374 | Computing loss before
2021-06-04 13:52:49 | [train_policy] epoch #374 | Computing KL before
2021-06-04 13:52:49 | [train_policy] epoch #374 | Optimizing
2021-06-04 13:52:49 | [train_policy] epoch #374 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:49 | [train_policy] epoch #374 | computing loss before
2021-06-04 13:52:49 | [train_policy] epoch #374 | computing gradient
2021-06-04 13:52:49 | [train_policy] epoch #374 | gradient computed
2021-06-04 13:52:49 | [train_policy] epoch #374 | computing descent direction
2021-06-04 13:52:49 | [train_policy] epoch #374 | descent direction computed
2021-06-04 13:52:49 | [train_policy] epoch #374 | backtrack iters: 1
2021-06-04 13:52:49 | [train_policy] epoch #374 | optimization finished
2021-06-04 13:52:49 | [train_policy] epoch #374 | Computing KL after
2021-06-04 13:52:49 | [train_policy] epoch #374 | Computing loss after
2021-06-04 13:52:49 | [train_policy] epoch #374 | Fitting baseline...
2021-06-04 13:52:49 | [train_policy] epoch #374 | Saving snapshot...
2021-06-04 13:52:49 | [train_policy] epoch #374 | Saved
2021-06-04 13:52:49 | [train_policy] epoch #374 | Time 301.40 s
2021-06-04 13:52:49 | [train_policy] epoch #374 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.29753
Evaluation/AverageDiscountedReturn          -44.2154
Evaluation/AverageReturn                    -44.2154
Evaluation/CompletionRate                     0
Evaluation/Iteration                        374
Evaluation/MaxReturn                        -30.8325
Evaluation/MinReturn                        -89.8915
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.4025
Extras/EpisodeRewardMean                    -44.1229
LinearFeatureBaseline/ExplainedVariance       0.828547
PolicyExecTime                                0.24053
ProcessExecTime                               0.0324938
TotalEnvSteps                            379500
policy/Entropy                               -0.903177
policy/KL                                     0.00664088
policy/KLBefore                               0
policy/LossAfter                             -0.0177636
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             0.40528
policy/dLoss                                  0.0177636
---------------------------------------  ----------------
2021-06-04 13:52:49 | [train_policy] epoch #375 | Obtaining samples for iteration 375...
2021-06-04 13:52:50 | [train_policy] epoch #375 | Logging diagnostics...
2021-06-04 13:52:50 | [train_policy] epoch #375 | Optimizing policy...
2021-06-04 13:52:50 | [train_policy] epoch #375 | Computing loss before
2021-06-04 13:52:50 | [train_policy] epoch #375 | Computing KL before
2021-06-04 13:52:50 | [train_policy] epoch #375 | Optimizing
2021-06-04 13:52:50 | [train_policy] epoch #375 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:50 | [train_policy] epoch #375 | computing loss before
2021-06-04 13:52:50 | [train_policy] epoch #375 | computing gradient
2021-06-04 13:52:50 | [train_policy] epoch #375 | gradient computed
2021-06-04 13:52:50 | [train_policy] epoch #375 | computing descent direction
2021-06-04 13:52:50 | [train_policy] epoch #375 | descent direction computed
2021-06-04 13:52:50 | [train_policy] epoch #375 | backtrack iters: 1
2021-06-04 13:52:50 | [train_policy] epoch #375 | optimization finished
2021-06-04 13:52:50 | [train_policy] epoch #375 | Computing KL after
2021-06-04 13:52:50 | [train_policy] epoch #375 | Computing loss after
2021-06-04 13:52:50 | [train_policy] epoch #375 | Fitting baseline...
2021-06-04 13:52:50 | [train_policy] epoch #375 | Saving snapshot...
2021-06-04 13:52:50 | [train_policy] epoch #375 | Saved
2021-06-04 13:52:50 | [train_policy] epoch #375 | Time 302.20 s
2021-06-04 13:52:50 | [train_policy] epoch #375 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.293351
Evaluation/AverageDiscountedReturn          -43.0692
Evaluation/AverageReturn                    -43.0692
Evaluation/CompletionRate                     0
Evaluation/Iteration                        375
Evaluation/MaxReturn                        -30.4881
Evaluation/MinReturn                        -87.0767
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.63785
Extras/EpisodeRewardMean                    -43.1455
LinearFeatureBaseline/ExplainedVariance       0.860026
PolicyExecTime                                0.219927
ProcessExecTime                               0.0318971
TotalEnvSteps                            380512
policy/Entropy                               -0.917611
policy/KL                                     0.0066574
policy/KLBefore                               0
policy/LossAfter                             -0.0162781
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.399472
policy/dLoss                                  0.0162781
---------------------------------------  ----------------
2021-06-04 13:52:50 | [train_policy] epoch #376 | Obtaining samples for iteration 376...
2021-06-04 13:52:51 | [train_policy] epoch #376 | Logging diagnostics...
2021-06-04 13:52:51 | [train_policy] epoch #376 | Optimizing policy...
2021-06-04 13:52:51 | [train_policy] epoch #376 | Computing loss before
2021-06-04 13:52:51 | [train_policy] epoch #376 | Computing KL before
2021-06-04 13:52:51 | [train_policy] epoch #376 | Optimizing
2021-06-04 13:52:51 | [train_policy] epoch #376 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:51 | [train_policy] epoch #376 | computing loss before
2021-06-04 13:52:51 | [train_policy] epoch #376 | computing gradient
2021-06-04 13:52:51 | [train_policy] epoch #376 | gradient computed
2021-06-04 13:52:51 | [train_policy] epoch #376 | computing descent direction
2021-06-04 13:52:51 | [train_policy] epoch #376 | descent direction computed
2021-06-04 13:52:51 | [train_policy] epoch #376 | backtrack iters: 1
2021-06-04 13:52:51 | [train_policy] epoch #376 | optimization finished
2021-06-04 13:52:51 | [train_policy] epoch #376 | Computing KL after
2021-06-04 13:52:51 | [train_policy] epoch #376 | Computing loss after
2021-06-04 13:52:51 | [train_policy] epoch #376 | Fitting baseline...
2021-06-04 13:52:51 | [train_policy] epoch #376 | Saving snapshot...
2021-06-04 13:52:51 | [train_policy] epoch #376 | Saved
2021-06-04 13:52:51 | [train_policy] epoch #376 | Time 303.00 s
2021-06-04 13:52:51 | [train_policy] epoch #376 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284321
Evaluation/AverageDiscountedReturn          -41.1938
Evaluation/AverageReturn                    -41.1938
Evaluation/CompletionRate                     0
Evaluation/Iteration                        376
Evaluation/MaxReturn                        -30.2685
Evaluation/MinReturn                        -89.7389
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.79471
Extras/EpisodeRewardMean                    -41.256
LinearFeatureBaseline/ExplainedVariance       0.849134
PolicyExecTime                                0.228452
ProcessExecTime                               0.0311847
TotalEnvSteps                            381524
policy/Entropy                               -0.916949
policy/KL                                     0.00647432
policy/KLBefore                               0
policy/LossAfter                             -0.0233649
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.399737
policy/dLoss                                  0.0233649
---------------------------------------  ----------------
2021-06-04 13:52:51 | [train_policy] epoch #377 | Obtaining samples for iteration 377...
2021-06-04 13:52:51 | [train_policy] epoch #377 | Logging diagnostics...
2021-06-04 13:52:51 | [train_policy] epoch #377 | Optimizing policy...
2021-06-04 13:52:51 | [train_policy] epoch #377 | Computing loss before
2021-06-04 13:52:51 | [train_policy] epoch #377 | Computing KL before
2021-06-04 13:52:51 | [train_policy] epoch #377 | Optimizing
2021-06-04 13:52:51 | [train_policy] epoch #377 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:51 | [train_policy] epoch #377 | computing loss before
2021-06-04 13:52:51 | [train_policy] epoch #377 | computing gradient
2021-06-04 13:52:51 | [train_policy] epoch #377 | gradient computed
2021-06-04 13:52:51 | [train_policy] epoch #377 | computing descent direction
2021-06-04 13:52:52 | [train_policy] epoch #377 | descent direction computed
2021-06-04 13:52:52 | [train_policy] epoch #377 | backtrack iters: 1
2021-06-04 13:52:52 | [train_policy] epoch #377 | optimization finished
2021-06-04 13:52:52 | [train_policy] epoch #377 | Computing KL after
2021-06-04 13:52:52 | [train_policy] epoch #377 | Computing loss after
2021-06-04 13:52:52 | [train_policy] epoch #377 | Fitting baseline...
2021-06-04 13:52:52 | [train_policy] epoch #377 | Saving snapshot...
2021-06-04 13:52:52 | [train_policy] epoch #377 | Saved
2021-06-04 13:52:52 | [train_policy] epoch #377 | Time 303.79 s
2021-06-04 13:52:52 | [train_policy] epoch #377 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284801
Evaluation/AverageDiscountedReturn          -44.1897
Evaluation/AverageReturn                    -44.1897
Evaluation/CompletionRate                     0
Evaluation/Iteration                        377
Evaluation/MaxReturn                        -30.2551
Evaluation/MinReturn                        -98.9471
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.55946
Extras/EpisodeRewardMean                    -43.8949
LinearFeatureBaseline/ExplainedVariance       0.84403
PolicyExecTime                                0.220297
ProcessExecTime                               0.0313258
TotalEnvSteps                            382536
policy/Entropy                               -0.92608
policy/KL                                     0.00662082
policy/KLBefore                               0
policy/LossAfter                             -0.0247853
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.396104
policy/dLoss                                  0.0247853
---------------------------------------  ----------------
2021-06-04 13:52:52 | [train_policy] epoch #378 | Obtaining samples for iteration 378...
2021-06-04 13:52:52 | [train_policy] epoch #378 | Logging diagnostics...
2021-06-04 13:52:52 | [train_policy] epoch #378 | Optimizing policy...
2021-06-04 13:52:52 | [train_policy] epoch #378 | Computing loss before
2021-06-04 13:52:52 | [train_policy] epoch #378 | Computing KL before
2021-06-04 13:52:52 | [train_policy] epoch #378 | Optimizing
2021-06-04 13:52:52 | [train_policy] epoch #378 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:52 | [train_policy] epoch #378 | computing loss before
2021-06-04 13:52:52 | [train_policy] epoch #378 | computing gradient
2021-06-04 13:52:52 | [train_policy] epoch #378 | gradient computed
2021-06-04 13:52:52 | [train_policy] epoch #378 | computing descent direction
2021-06-04 13:52:52 | [train_policy] epoch #378 | descent direction computed
2021-06-04 13:52:52 | [train_policy] epoch #378 | backtrack iters: 0
2021-06-04 13:52:52 | [train_policy] epoch #378 | optimization finished
2021-06-04 13:52:52 | [train_policy] epoch #378 | Computing KL after
2021-06-04 13:52:52 | [train_policy] epoch #378 | Computing loss after
2021-06-04 13:52:52 | [train_policy] epoch #378 | Fitting baseline...
2021-06-04 13:52:52 | [train_policy] epoch #378 | Saving snapshot...
2021-06-04 13:52:52 | [train_policy] epoch #378 | Saved
2021-06-04 13:52:52 | [train_policy] epoch #378 | Time 304.59 s
2021-06-04 13:52:52 | [train_policy] epoch #378 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.28649
Evaluation/AverageDiscountedReturn          -41.7561
Evaluation/AverageReturn                    -41.7561
Evaluation/CompletionRate                     0
Evaluation/Iteration                        378
Evaluation/MaxReturn                        -30.6087
Evaluation/MinReturn                        -64.0286
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85852
Extras/EpisodeRewardMean                    -41.7866
LinearFeatureBaseline/ExplainedVariance       0.911911
PolicyExecTime                                0.230864
ProcessExecTime                               0.0313921
TotalEnvSteps                            383548
policy/Entropy                               -0.936853
policy/KL                                     0.00984844
policy/KLBefore                               0
policy/LossAfter                             -0.0159321
policy/LossBefore                             1.6727e-08
policy/Perplexity                             0.391859
policy/dLoss                                  0.0159321
---------------------------------------  ---------------
2021-06-04 13:52:52 | [train_policy] epoch #379 | Obtaining samples for iteration 379...
2021-06-04 13:52:53 | [train_policy] epoch #379 | Logging diagnostics...
2021-06-04 13:52:53 | [train_policy] epoch #379 | Optimizing policy...
2021-06-04 13:52:53 | [train_policy] epoch #379 | Computing loss before
2021-06-04 13:52:53 | [train_policy] epoch #379 | Computing KL before
2021-06-04 13:52:53 | [train_policy] epoch #379 | Optimizing
2021-06-04 13:52:53 | [train_policy] epoch #379 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:53 | [train_policy] epoch #379 | computing loss before
2021-06-04 13:52:53 | [train_policy] epoch #379 | computing gradient
2021-06-04 13:52:53 | [train_policy] epoch #379 | gradient computed
2021-06-04 13:52:53 | [train_policy] epoch #379 | computing descent direction
2021-06-04 13:52:53 | [train_policy] epoch #379 | descent direction computed
2021-06-04 13:52:53 | [train_policy] epoch #379 | backtrack iters: 0
2021-06-04 13:52:53 | [train_policy] epoch #379 | optimization finished
2021-06-04 13:52:53 | [train_policy] epoch #379 | Computing KL after
2021-06-04 13:52:53 | [train_policy] epoch #379 | Computing loss after
2021-06-04 13:52:53 | [train_policy] epoch #379 | Fitting baseline...
2021-06-04 13:52:53 | [train_policy] epoch #379 | Saving snapshot...
2021-06-04 13:52:53 | [train_policy] epoch #379 | Saved
2021-06-04 13:52:53 | [train_policy] epoch #379 | Time 305.39 s
2021-06-04 13:52:53 | [train_policy] epoch #379 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288176
Evaluation/AverageDiscountedReturn          -45.7461
Evaluation/AverageReturn                    -45.7461
Evaluation/CompletionRate                     0
Evaluation/Iteration                        379
Evaluation/MaxReturn                        -33.2879
Evaluation/MinReturn                       -101.008
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.6044
Extras/EpisodeRewardMean                    -45.5877
LinearFeatureBaseline/ExplainedVariance       0.7986
PolicyExecTime                                0.227133
ProcessExecTime                               0.0315773
TotalEnvSteps                            384560
policy/Entropy                               -0.879721
policy/KL                                     0.00976987
policy/KLBefore                               0
policy/LossAfter                             -0.0183606
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.414899
policy/dLoss                                  0.0183606
---------------------------------------  ----------------
2021-06-04 13:52:53 | [train_policy] epoch #380 | Obtaining samples for iteration 380...
2021-06-04 13:52:54 | [train_policy] epoch #380 | Logging diagnostics...
2021-06-04 13:52:54 | [train_policy] epoch #380 | Optimizing policy...
2021-06-04 13:52:54 | [train_policy] epoch #380 | Computing loss before
2021-06-04 13:52:54 | [train_policy] epoch #380 | Computing KL before
2021-06-04 13:52:54 | [train_policy] epoch #380 | Optimizing
2021-06-04 13:52:54 | [train_policy] epoch #380 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:54 | [train_policy] epoch #380 | computing loss before
2021-06-04 13:52:54 | [train_policy] epoch #380 | computing gradient
2021-06-04 13:52:54 | [train_policy] epoch #380 | gradient computed
2021-06-04 13:52:54 | [train_policy] epoch #380 | computing descent direction
2021-06-04 13:52:54 | [train_policy] epoch #380 | descent direction computed
2021-06-04 13:52:54 | [train_policy] epoch #380 | backtrack iters: 0
2021-06-04 13:52:54 | [train_policy] epoch #380 | optimization finished
2021-06-04 13:52:54 | [train_policy] epoch #380 | Computing KL after
2021-06-04 13:52:54 | [train_policy] epoch #380 | Computing loss after
2021-06-04 13:52:54 | [train_policy] epoch #380 | Fitting baseline...
2021-06-04 13:52:54 | [train_policy] epoch #380 | Saving snapshot...
2021-06-04 13:52:54 | [train_policy] epoch #380 | Saved
2021-06-04 13:52:54 | [train_policy] epoch #380 | Time 306.17 s
2021-06-04 13:52:54 | [train_policy] epoch #380 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285767
Evaluation/AverageDiscountedReturn          -42.375
Evaluation/AverageReturn                    -42.375
Evaluation/CompletionRate                     0
Evaluation/Iteration                        380
Evaluation/MaxReturn                        -29.8428
Evaluation/MinReturn                        -63.3449
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.20421
Extras/EpisodeRewardMean                    -42.3244
LinearFeatureBaseline/ExplainedVariance       0.827009
PolicyExecTime                                0.218635
ProcessExecTime                               0.0312972
TotalEnvSteps                            385572
policy/Entropy                               -0.897204
policy/KL                                     0.00995693
policy/KLBefore                               0
policy/LossAfter                             -0.0281863
policy/LossBefore                            -1.46067e-08
policy/Perplexity                             0.407708
policy/dLoss                                  0.0281863
---------------------------------------  ----------------
2021-06-04 13:52:54 | [train_policy] epoch #381 | Obtaining samples for iteration 381...
2021-06-04 13:52:55 | [train_policy] epoch #381 | Logging diagnostics...
2021-06-04 13:52:55 | [train_policy] epoch #381 | Optimizing policy...
2021-06-04 13:52:55 | [train_policy] epoch #381 | Computing loss before
2021-06-04 13:52:55 | [train_policy] epoch #381 | Computing KL before
2021-06-04 13:52:55 | [train_policy] epoch #381 | Optimizing
2021-06-04 13:52:55 | [train_policy] epoch #381 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:55 | [train_policy] epoch #381 | computing loss before
2021-06-04 13:52:55 | [train_policy] epoch #381 | computing gradient
2021-06-04 13:52:55 | [train_policy] epoch #381 | gradient computed
2021-06-04 13:52:55 | [train_policy] epoch #381 | computing descent direction
2021-06-04 13:52:55 | [train_policy] epoch #381 | descent direction computed
2021-06-04 13:52:55 | [train_policy] epoch #381 | backtrack iters: 0
2021-06-04 13:52:55 | [train_policy] epoch #381 | optimization finished
2021-06-04 13:52:55 | [train_policy] epoch #381 | Computing KL after
2021-06-04 13:52:55 | [train_policy] epoch #381 | Computing loss after
2021-06-04 13:52:55 | [train_policy] epoch #381 | Fitting baseline...
2021-06-04 13:52:55 | [train_policy] epoch #381 | Saving snapshot...
2021-06-04 13:52:55 | [train_policy] epoch #381 | Saved
2021-06-04 13:52:55 | [train_policy] epoch #381 | Time 306.97 s
2021-06-04 13:52:55 | [train_policy] epoch #381 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284996
Evaluation/AverageDiscountedReturn          -43.0634
Evaluation/AverageReturn                    -43.0634
Evaluation/CompletionRate                     0
Evaluation/Iteration                        381
Evaluation/MaxReturn                        -30.8331
Evaluation/MinReturn                       -100.871
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.295
Extras/EpisodeRewardMean                    -42.8794
LinearFeatureBaseline/ExplainedVariance       0.776038
PolicyExecTime                                0.23504
ProcessExecTime                               0.0311518
TotalEnvSteps                            386584
policy/Entropy                               -0.905458
policy/KL                                     0.00995414
policy/KLBefore                               0
policy/LossAfter                             -0.0340577
policy/LossBefore                            -3.47497e-09
policy/Perplexity                             0.404357
policy/dLoss                                  0.0340577
---------------------------------------  ----------------
2021-06-04 13:52:55 | [train_policy] epoch #382 | Obtaining samples for iteration 382...
2021-06-04 13:52:55 | [train_policy] epoch #382 | Logging diagnostics...
2021-06-04 13:52:55 | [train_policy] epoch #382 | Optimizing policy...
2021-06-04 13:52:55 | [train_policy] epoch #382 | Computing loss before
2021-06-04 13:52:55 | [train_policy] epoch #382 | Computing KL before
2021-06-04 13:52:55 | [train_policy] epoch #382 | Optimizing
2021-06-04 13:52:55 | [train_policy] epoch #382 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:55 | [train_policy] epoch #382 | computing loss before
2021-06-04 13:52:55 | [train_policy] epoch #382 | computing gradient
2021-06-04 13:52:55 | [train_policy] epoch #382 | gradient computed
2021-06-04 13:52:55 | [train_policy] epoch #382 | computing descent direction
2021-06-04 13:52:55 | [train_policy] epoch #382 | descent direction computed
2021-06-04 13:52:56 | [train_policy] epoch #382 | backtrack iters: 0
2021-06-04 13:52:56 | [train_policy] epoch #382 | optimization finished
2021-06-04 13:52:56 | [train_policy] epoch #382 | Computing KL after
2021-06-04 13:52:56 | [train_policy] epoch #382 | Computing loss after
2021-06-04 13:52:56 | [train_policy] epoch #382 | Fitting baseline...
2021-06-04 13:52:56 | [train_policy] epoch #382 | Saving snapshot...
2021-06-04 13:52:56 | [train_policy] epoch #382 | Saved
2021-06-04 13:52:56 | [train_policy] epoch #382 | Time 307.78 s
2021-06-04 13:52:56 | [train_policy] epoch #382 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286198
Evaluation/AverageDiscountedReturn          -43.4672
Evaluation/AverageReturn                    -43.4672
Evaluation/CompletionRate                     0
Evaluation/Iteration                        382
Evaluation/MaxReturn                        -28.8673
Evaluation/MinReturn                       -100.015
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.22374
Extras/EpisodeRewardMean                    -43.205
LinearFeatureBaseline/ExplainedVariance       0.84059
PolicyExecTime                                0.230618
ProcessExecTime                               0.0312855
TotalEnvSteps                            387596
policy/Entropy                               -0.901127
policy/KL                                     0.0092689
policy/KLBefore                               0
policy/LossAfter                             -0.0228326
policy/LossBefore                            -1.02482e-08
policy/Perplexity                             0.406112
policy/dLoss                                  0.0228326
---------------------------------------  ----------------
2021-06-04 13:52:56 | [train_policy] epoch #383 | Obtaining samples for iteration 383...
2021-06-04 13:52:56 | [train_policy] epoch #383 | Logging diagnostics...
2021-06-04 13:52:56 | [train_policy] epoch #383 | Optimizing policy...
2021-06-04 13:52:56 | [train_policy] epoch #383 | Computing loss before
2021-06-04 13:52:56 | [train_policy] epoch #383 | Computing KL before
2021-06-04 13:52:56 | [train_policy] epoch #383 | Optimizing
2021-06-04 13:52:56 | [train_policy] epoch #383 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:56 | [train_policy] epoch #383 | computing loss before
2021-06-04 13:52:56 | [train_policy] epoch #383 | computing gradient
2021-06-04 13:52:56 | [train_policy] epoch #383 | gradient computed
2021-06-04 13:52:56 | [train_policy] epoch #383 | computing descent direction
2021-06-04 13:52:56 | [train_policy] epoch #383 | descent direction computed
2021-06-04 13:52:56 | [train_policy] epoch #383 | backtrack iters: 1
2021-06-04 13:52:56 | [train_policy] epoch #383 | optimization finished
2021-06-04 13:52:56 | [train_policy] epoch #383 | Computing KL after
2021-06-04 13:52:56 | [train_policy] epoch #383 | Computing loss after
2021-06-04 13:52:56 | [train_policy] epoch #383 | Fitting baseline...
2021-06-04 13:52:56 | [train_policy] epoch #383 | Saving snapshot...
2021-06-04 13:52:56 | [train_policy] epoch #383 | Saved
2021-06-04 13:52:56 | [train_policy] epoch #383 | Time 308.58 s
2021-06-04 13:52:56 | [train_policy] epoch #383 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286418
Evaluation/AverageDiscountedReturn          -41.285
Evaluation/AverageReturn                    -41.285
Evaluation/CompletionRate                     0
Evaluation/Iteration                        383
Evaluation/MaxReturn                        -29.4916
Evaluation/MinReturn                        -64.1141
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.93562
Extras/EpisodeRewardMean                    -41.2228
LinearFeatureBaseline/ExplainedVariance       0.911712
PolicyExecTime                                0.226629
ProcessExecTime                               0.031441
TotalEnvSteps                            388608
policy/Entropy                               -0.881635
policy/KL                                     0.00647164
policy/KLBefore                               0
policy/LossAfter                             -0.0198912
policy/LossBefore                            -1.27219e-08
policy/Perplexity                             0.414105
policy/dLoss                                  0.0198912
---------------------------------------  ----------------
2021-06-04 13:52:56 | [train_policy] epoch #384 | Obtaining samples for iteration 384...
2021-06-04 13:52:57 | [train_policy] epoch #384 | Logging diagnostics...
2021-06-04 13:52:57 | [train_policy] epoch #384 | Optimizing policy...
2021-06-04 13:52:57 | [train_policy] epoch #384 | Computing loss before
2021-06-04 13:52:57 | [train_policy] epoch #384 | Computing KL before
2021-06-04 13:52:57 | [train_policy] epoch #384 | Optimizing
2021-06-04 13:52:57 | [train_policy] epoch #384 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:57 | [train_policy] epoch #384 | computing loss before
2021-06-04 13:52:57 | [train_policy] epoch #384 | computing gradient
2021-06-04 13:52:57 | [train_policy] epoch #384 | gradient computed
2021-06-04 13:52:57 | [train_policy] epoch #384 | computing descent direction
2021-06-04 13:52:57 | [train_policy] epoch #384 | descent direction computed
2021-06-04 13:52:57 | [train_policy] epoch #384 | backtrack iters: 1
2021-06-04 13:52:57 | [train_policy] epoch #384 | optimization finished
2021-06-04 13:52:57 | [train_policy] epoch #384 | Computing KL after
2021-06-04 13:52:57 | [train_policy] epoch #384 | Computing loss after
2021-06-04 13:52:57 | [train_policy] epoch #384 | Fitting baseline...
2021-06-04 13:52:57 | [train_policy] epoch #384 | Saving snapshot...
2021-06-04 13:52:57 | [train_policy] epoch #384 | Saved
2021-06-04 13:52:57 | [train_policy] epoch #384 | Time 309.40 s
2021-06-04 13:52:57 | [train_policy] epoch #384 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285263
Evaluation/AverageDiscountedReturn          -43.1416
Evaluation/AverageReturn                    -43.1416
Evaluation/CompletionRate                     0
Evaluation/Iteration                        384
Evaluation/MaxReturn                        -31.2149
Evaluation/MinReturn                        -89.2459
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2648
Extras/EpisodeRewardMean                    -42.9753
LinearFeatureBaseline/ExplainedVariance       0.826405
PolicyExecTime                                0.230603
ProcessExecTime                               0.0311933
TotalEnvSteps                            389620
policy/Entropy                               -0.884282
policy/KL                                     0.00645546
policy/KLBefore                               0
policy/LossAfter                             -0.0217535
policy/LossBefore                             4.47624e-09
policy/Perplexity                             0.413011
policy/dLoss                                  0.0217535
---------------------------------------  ----------------
2021-06-04 13:52:57 | [train_policy] epoch #385 | Obtaining samples for iteration 385...
2021-06-04 13:52:58 | [train_policy] epoch #385 | Logging diagnostics...
2021-06-04 13:52:58 | [train_policy] epoch #385 | Optimizing policy...
2021-06-04 13:52:58 | [train_policy] epoch #385 | Computing loss before
2021-06-04 13:52:58 | [train_policy] epoch #385 | Computing KL before
2021-06-04 13:52:58 | [train_policy] epoch #385 | Optimizing
2021-06-04 13:52:58 | [train_policy] epoch #385 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:58 | [train_policy] epoch #385 | computing loss before
2021-06-04 13:52:58 | [train_policy] epoch #385 | computing gradient
2021-06-04 13:52:58 | [train_policy] epoch #385 | gradient computed
2021-06-04 13:52:58 | [train_policy] epoch #385 | computing descent direction
2021-06-04 13:52:58 | [train_policy] epoch #385 | descent direction computed
2021-06-04 13:52:58 | [train_policy] epoch #385 | backtrack iters: 1
2021-06-04 13:52:58 | [train_policy] epoch #385 | optimization finished
2021-06-04 13:52:58 | [train_policy] epoch #385 | Computing KL after
2021-06-04 13:52:58 | [train_policy] epoch #385 | Computing loss after
2021-06-04 13:52:58 | [train_policy] epoch #385 | Fitting baseline...
2021-06-04 13:52:58 | [train_policy] epoch #385 | Saving snapshot...
2021-06-04 13:52:58 | [train_policy] epoch #385 | Saved
2021-06-04 13:52:58 | [train_policy] epoch #385 | Time 310.20 s
2021-06-04 13:52:58 | [train_policy] epoch #385 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284302
Evaluation/AverageDiscountedReturn          -41.0655
Evaluation/AverageReturn                    -41.0655
Evaluation/CompletionRate                     0
Evaluation/Iteration                        385
Evaluation/MaxReturn                        -29.6902
Evaluation/MinReturn                        -63.93
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.79571
Extras/EpisodeRewardMean                    -41.2427
LinearFeatureBaseline/ExplainedVariance       0.900442
PolicyExecTime                                0.226674
ProcessExecTime                               0.0311728
TotalEnvSteps                            390632
policy/Entropy                               -0.886784
policy/KL                                     0.006477
policy/KLBefore                               0
policy/LossAfter                             -0.0173323
policy/LossBefore                             1.23686e-08
policy/Perplexity                             0.411979
policy/dLoss                                  0.0173323
---------------------------------------  ----------------
2021-06-04 13:52:58 | [train_policy] epoch #386 | Obtaining samples for iteration 386...
2021-06-04 13:52:59 | [train_policy] epoch #386 | Logging diagnostics...
2021-06-04 13:52:59 | [train_policy] epoch #386 | Optimizing policy...
2021-06-04 13:52:59 | [train_policy] epoch #386 | Computing loss before
2021-06-04 13:52:59 | [train_policy] epoch #386 | Computing KL before
2021-06-04 13:52:59 | [train_policy] epoch #386 | Optimizing
2021-06-04 13:52:59 | [train_policy] epoch #386 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:59 | [train_policy] epoch #386 | computing loss before
2021-06-04 13:52:59 | [train_policy] epoch #386 | computing gradient
2021-06-04 13:52:59 | [train_policy] epoch #386 | gradient computed
2021-06-04 13:52:59 | [train_policy] epoch #386 | computing descent direction
2021-06-04 13:52:59 | [train_policy] epoch #386 | descent direction computed
2021-06-04 13:52:59 | [train_policy] epoch #386 | backtrack iters: 1
2021-06-04 13:52:59 | [train_policy] epoch #386 | optimization finished
2021-06-04 13:52:59 | [train_policy] epoch #386 | Computing KL after
2021-06-04 13:52:59 | [train_policy] epoch #386 | Computing loss after
2021-06-04 13:52:59 | [train_policy] epoch #386 | Fitting baseline...
2021-06-04 13:52:59 | [train_policy] epoch #386 | Saving snapshot...
2021-06-04 13:52:59 | [train_policy] epoch #386 | Saved
2021-06-04 13:52:59 | [train_policy] epoch #386 | Time 310.99 s
2021-06-04 13:52:59 | [train_policy] epoch #386 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.284924
Evaluation/AverageDiscountedReturn          -45.0112
Evaluation/AverageReturn                    -45.0112
Evaluation/CompletionRate                     0
Evaluation/Iteration                        386
Evaluation/MaxReturn                        -30.7451
Evaluation/MinReturn                       -126.618
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.1313
Extras/EpisodeRewardMean                    -44.7957
LinearFeatureBaseline/ExplainedVariance       0.731076
PolicyExecTime                                0.217998
ProcessExecTime                               0.0312037
TotalEnvSteps                            391644
policy/Entropy                               -0.886298
policy/KL                                     0.00639672
policy/KLBefore                               0
policy/LossAfter                             -0.0201784
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             0.412179
policy/dLoss                                  0.0201784
---------------------------------------  ---------------
2021-06-04 13:52:59 | [train_policy] epoch #387 | Obtaining samples for iteration 387...
2021-06-04 13:52:59 | [train_policy] epoch #387 | Logging diagnostics...
2021-06-04 13:52:59 | [train_policy] epoch #387 | Optimizing policy...
2021-06-04 13:52:59 | [train_policy] epoch #387 | Computing loss before
2021-06-04 13:52:59 | [train_policy] epoch #387 | Computing KL before
2021-06-04 13:52:59 | [train_policy] epoch #387 | Optimizing
2021-06-04 13:52:59 | [train_policy] epoch #387 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:52:59 | [train_policy] epoch #387 | computing loss before
2021-06-04 13:52:59 | [train_policy] epoch #387 | computing gradient
2021-06-04 13:52:59 | [train_policy] epoch #387 | gradient computed
2021-06-04 13:52:59 | [train_policy] epoch #387 | computing descent direction
2021-06-04 13:52:59 | [train_policy] epoch #387 | descent direction computed
2021-06-04 13:53:00 | [train_policy] epoch #387 | backtrack iters: 1
2021-06-04 13:53:00 | [train_policy] epoch #387 | optimization finished
2021-06-04 13:53:00 | [train_policy] epoch #387 | Computing KL after
2021-06-04 13:53:00 | [train_policy] epoch #387 | Computing loss after
2021-06-04 13:53:00 | [train_policy] epoch #387 | Fitting baseline...
2021-06-04 13:53:00 | [train_policy] epoch #387 | Saving snapshot...
2021-06-04 13:53:00 | [train_policy] epoch #387 | Saved
2021-06-04 13:53:00 | [train_policy] epoch #387 | Time 311.79 s
2021-06-04 13:53:00 | [train_policy] epoch #387 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28484
Evaluation/AverageDiscountedReturn          -41.4713
Evaluation/AverageReturn                    -41.4713
Evaluation/CompletionRate                     0
Evaluation/Iteration                        387
Evaluation/MaxReturn                        -30.5705
Evaluation/MinReturn                        -64.0338
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.12949
Extras/EpisodeRewardMean                    -41.6327
LinearFeatureBaseline/ExplainedVariance       0.902556
PolicyExecTime                                0.224012
ProcessExecTime                               0.0312073
TotalEnvSteps                            392656
policy/Entropy                               -0.922544
policy/KL                                     0.00653703
policy/KLBefore                               0
policy/LossAfter                             -0.0140121
policy/LossBefore                             1.17796e-10
policy/Perplexity                             0.397506
policy/dLoss                                  0.0140121
---------------------------------------  ----------------
2021-06-04 13:53:00 | [train_policy] epoch #388 | Obtaining samples for iteration 388...
2021-06-04 13:53:00 | [train_policy] epoch #388 | Logging diagnostics...
2021-06-04 13:53:00 | [train_policy] epoch #388 | Optimizing policy...
2021-06-04 13:53:00 | [train_policy] epoch #388 | Computing loss before
2021-06-04 13:53:00 | [train_policy] epoch #388 | Computing KL before
2021-06-04 13:53:00 | [train_policy] epoch #388 | Optimizing
2021-06-04 13:53:00 | [train_policy] epoch #388 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:00 | [train_policy] epoch #388 | computing loss before
2021-06-04 13:53:00 | [train_policy] epoch #388 | computing gradient
2021-06-04 13:53:00 | [train_policy] epoch #388 | gradient computed
2021-06-04 13:53:00 | [train_policy] epoch #388 | computing descent direction
2021-06-04 13:53:00 | [train_policy] epoch #388 | descent direction computed
2021-06-04 13:53:00 | [train_policy] epoch #388 | backtrack iters: 1
2021-06-04 13:53:00 | [train_policy] epoch #388 | optimization finished
2021-06-04 13:53:00 | [train_policy] epoch #388 | Computing KL after
2021-06-04 13:53:00 | [train_policy] epoch #388 | Computing loss after
2021-06-04 13:53:00 | [train_policy] epoch #388 | Fitting baseline...
2021-06-04 13:53:00 | [train_policy] epoch #388 | Saving snapshot...
2021-06-04 13:53:00 | [train_policy] epoch #388 | Saved
2021-06-04 13:53:00 | [train_policy] epoch #388 | Time 312.59 s
2021-06-04 13:53:00 | [train_policy] epoch #388 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.29307
Evaluation/AverageDiscountedReturn          -43.3693
Evaluation/AverageReturn                    -43.3693
Evaluation/CompletionRate                     0
Evaluation/Iteration                        388
Evaluation/MaxReturn                        -30.3601
Evaluation/MinReturn                        -64.1295
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.4543
Extras/EpisodeRewardMean                    -43.2672
LinearFeatureBaseline/ExplainedVariance       0.901222
PolicyExecTime                                0.222968
ProcessExecTime                               0.0320184
TotalEnvSteps                            393668
policy/Entropy                               -0.953654
policy/KL                                     0.00664724
policy/KLBefore                               0
policy/LossAfter                             -0.0151982
policy/LossBefore                            -1.20152e-08
policy/Perplexity                             0.38533
policy/dLoss                                  0.0151982
---------------------------------------  ----------------
2021-06-04 13:53:00 | [train_policy] epoch #389 | Obtaining samples for iteration 389...
2021-06-04 13:53:01 | [train_policy] epoch #389 | Logging diagnostics...
2021-06-04 13:53:01 | [train_policy] epoch #389 | Optimizing policy...
2021-06-04 13:53:01 | [train_policy] epoch #389 | Computing loss before
2021-06-04 13:53:01 | [train_policy] epoch #389 | Computing KL before
2021-06-04 13:53:01 | [train_policy] epoch #389 | Optimizing
2021-06-04 13:53:01 | [train_policy] epoch #389 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:01 | [train_policy] epoch #389 | computing loss before
2021-06-04 13:53:01 | [train_policy] epoch #389 | computing gradient
2021-06-04 13:53:01 | [train_policy] epoch #389 | gradient computed
2021-06-04 13:53:01 | [train_policy] epoch #389 | computing descent direction
2021-06-04 13:53:01 | [train_policy] epoch #389 | descent direction computed
2021-06-04 13:53:01 | [train_policy] epoch #389 | backtrack iters: 1
2021-06-04 13:53:01 | [train_policy] epoch #389 | optimization finished
2021-06-04 13:53:01 | [train_policy] epoch #389 | Computing KL after
2021-06-04 13:53:01 | [train_policy] epoch #389 | Computing loss after
2021-06-04 13:53:01 | [train_policy] epoch #389 | Fitting baseline...
2021-06-04 13:53:01 | [train_policy] epoch #389 | Saving snapshot...
2021-06-04 13:53:01 | [train_policy] epoch #389 | Saved
2021-06-04 13:53:01 | [train_policy] epoch #389 | Time 313.40 s
2021-06-04 13:53:01 | [train_policy] epoch #389 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.29088
Evaluation/AverageDiscountedReturn          -42.9861
Evaluation/AverageReturn                    -42.9861
Evaluation/CompletionRate                     0
Evaluation/Iteration                        389
Evaluation/MaxReturn                        -30.0654
Evaluation/MinReturn                        -63.957
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.52026
Extras/EpisodeRewardMean                    -42.9782
LinearFeatureBaseline/ExplainedVariance       0.915826
PolicyExecTime                                0.22942
ProcessExecTime                               0.0316057
TotalEnvSteps                            394680
policy/Entropy                               -0.992254
policy/KL                                     0.00670026
policy/KLBefore                               0
policy/LossAfter                             -0.0160071
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.37074
policy/dLoss                                  0.0160071
---------------------------------------  ----------------
2021-06-04 13:53:01 | [train_policy] epoch #390 | Obtaining samples for iteration 390...
2021-06-04 13:53:02 | [train_policy] epoch #390 | Logging diagnostics...
2021-06-04 13:53:02 | [train_policy] epoch #390 | Optimizing policy...
2021-06-04 13:53:02 | [train_policy] epoch #390 | Computing loss before
2021-06-04 13:53:02 | [train_policy] epoch #390 | Computing KL before
2021-06-04 13:53:02 | [train_policy] epoch #390 | Optimizing
2021-06-04 13:53:02 | [train_policy] epoch #390 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:02 | [train_policy] epoch #390 | computing loss before
2021-06-04 13:53:02 | [train_policy] epoch #390 | computing gradient
2021-06-04 13:53:02 | [train_policy] epoch #390 | gradient computed
2021-06-04 13:53:02 | [train_policy] epoch #390 | computing descent direction
2021-06-04 13:53:02 | [train_policy] epoch #390 | descent direction computed
2021-06-04 13:53:02 | [train_policy] epoch #390 | backtrack iters: 1
2021-06-04 13:53:02 | [train_policy] epoch #390 | optimization finished
2021-06-04 13:53:02 | [train_policy] epoch #390 | Computing KL after
2021-06-04 13:53:02 | [train_policy] epoch #390 | Computing loss after
2021-06-04 13:53:02 | [train_policy] epoch #390 | Fitting baseline...
2021-06-04 13:53:02 | [train_policy] epoch #390 | Saving snapshot...
2021-06-04 13:53:02 | [train_policy] epoch #390 | Saved
2021-06-04 13:53:02 | [train_policy] epoch #390 | Time 314.20 s
2021-06-04 13:53:02 | [train_policy] epoch #390 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285154
Evaluation/AverageDiscountedReturn          -41.9889
Evaluation/AverageReturn                    -41.9889
Evaluation/CompletionRate                     0
Evaluation/Iteration                        390
Evaluation/MaxReturn                        -29.5003
Evaluation/MinReturn                        -58.3436
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.70111
Extras/EpisodeRewardMean                    -42.1289
LinearFeatureBaseline/ExplainedVariance       0.91556
PolicyExecTime                                0.23209
ProcessExecTime                               0.0313144
TotalEnvSteps                            395692
policy/Entropy                               -1.03267
policy/KL                                     0.00668758
policy/KLBefore                               0
policy/LossAfter                             -0.0178765
policy/LossBefore                             1.13084e-08
policy/Perplexity                             0.356056
policy/dLoss                                  0.0178765
---------------------------------------  ----------------
2021-06-04 13:53:02 | [train_policy] epoch #391 | Obtaining samples for iteration 391...
2021-06-04 13:53:03 | [train_policy] epoch #391 | Logging diagnostics...
2021-06-04 13:53:03 | [train_policy] epoch #391 | Optimizing policy...
2021-06-04 13:53:03 | [train_policy] epoch #391 | Computing loss before
2021-06-04 13:53:03 | [train_policy] epoch #391 | Computing KL before
2021-06-04 13:53:03 | [train_policy] epoch #391 | Optimizing
2021-06-04 13:53:03 | [train_policy] epoch #391 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:03 | [train_policy] epoch #391 | computing loss before
2021-06-04 13:53:03 | [train_policy] epoch #391 | computing gradient
2021-06-04 13:53:03 | [train_policy] epoch #391 | gradient computed
2021-06-04 13:53:03 | [train_policy] epoch #391 | computing descent direction
2021-06-04 13:53:03 | [train_policy] epoch #391 | descent direction computed
2021-06-04 13:53:03 | [train_policy] epoch #391 | backtrack iters: 1
2021-06-04 13:53:03 | [train_policy] epoch #391 | optimization finished
2021-06-04 13:53:03 | [train_policy] epoch #391 | Computing KL after
2021-06-04 13:53:03 | [train_policy] epoch #391 | Computing loss after
2021-06-04 13:53:03 | [train_policy] epoch #391 | Fitting baseline...
2021-06-04 13:53:03 | [train_policy] epoch #391 | Saving snapshot...
2021-06-04 13:53:03 | [train_policy] epoch #391 | Saved
2021-06-04 13:53:03 | [train_policy] epoch #391 | Time 315.01 s
2021-06-04 13:53:03 | [train_policy] epoch #391 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287166
Evaluation/AverageDiscountedReturn          -43.4751
Evaluation/AverageReturn                    -43.4751
Evaluation/CompletionRate                     0
Evaluation/Iteration                        391
Evaluation/MaxReturn                        -30.6785
Evaluation/MinReturn                       -103.581
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.435
Extras/EpisodeRewardMean                    -43.4585
LinearFeatureBaseline/ExplainedVariance       0.788836
PolicyExecTime                                0.231635
ProcessExecTime                               0.0312517
TotalEnvSteps                            396704
policy/Entropy                               -1.02846
policy/KL                                     0.00673436
policy/KLBefore                               0
policy/LossAfter                             -0.0165091
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.357556
policy/dLoss                                  0.0165091
---------------------------------------  ----------------
2021-06-04 13:53:03 | [train_policy] epoch #392 | Obtaining samples for iteration 392...
2021-06-04 13:53:03 | [train_policy] epoch #392 | Logging diagnostics...
2021-06-04 13:53:03 | [train_policy] epoch #392 | Optimizing policy...
2021-06-04 13:53:03 | [train_policy] epoch #392 | Computing loss before
2021-06-04 13:53:03 | [train_policy] epoch #392 | Computing KL before
2021-06-04 13:53:03 | [train_policy] epoch #392 | Optimizing
2021-06-04 13:53:03 | [train_policy] epoch #392 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:03 | [train_policy] epoch #392 | computing loss before
2021-06-04 13:53:03 | [train_policy] epoch #392 | computing gradient
2021-06-04 13:53:03 | [train_policy] epoch #392 | gradient computed
2021-06-04 13:53:03 | [train_policy] epoch #392 | computing descent direction
2021-06-04 13:53:04 | [train_policy] epoch #392 | descent direction computed
2021-06-04 13:53:04 | [train_policy] epoch #392 | backtrack iters: 0
2021-06-04 13:53:04 | [train_policy] epoch #392 | optimization finished
2021-06-04 13:53:04 | [train_policy] epoch #392 | Computing KL after
2021-06-04 13:53:04 | [train_policy] epoch #392 | Computing loss after
2021-06-04 13:53:04 | [train_policy] epoch #392 | Fitting baseline...
2021-06-04 13:53:04 | [train_policy] epoch #392 | Saving snapshot...
2021-06-04 13:53:04 | [train_policy] epoch #392 | Saved
2021-06-04 13:53:04 | [train_policy] epoch #392 | Time 315.81 s
2021-06-04 13:53:04 | [train_policy] epoch #392 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286987
Evaluation/AverageDiscountedReturn          -41.8162
Evaluation/AverageReturn                    -41.8162
Evaluation/CompletionRate                     0
Evaluation/Iteration                        392
Evaluation/MaxReturn                        -29.9838
Evaluation/MinReturn                        -63.9062
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.29135
Extras/EpisodeRewardMean                    -42.0913
LinearFeatureBaseline/ExplainedVariance       0.903898
PolicyExecTime                                0.232193
ProcessExecTime                               0.0313637
TotalEnvSteps                            397716
policy/Entropy                               -1.00055
policy/KL                                     0.00960497
policy/KLBefore                               0
policy/LossAfter                             -0.0180124
policy/LossBefore                             3.81658e-08
policy/Perplexity                             0.367678
policy/dLoss                                  0.0180124
---------------------------------------  ----------------
2021-06-04 13:53:04 | [train_policy] epoch #393 | Obtaining samples for iteration 393...
2021-06-04 13:53:04 | [train_policy] epoch #393 | Logging diagnostics...
2021-06-04 13:53:04 | [train_policy] epoch #393 | Optimizing policy...
2021-06-04 13:53:04 | [train_policy] epoch #393 | Computing loss before
2021-06-04 13:53:04 | [train_policy] epoch #393 | Computing KL before
2021-06-04 13:53:04 | [train_policy] epoch #393 | Optimizing
2021-06-04 13:53:04 | [train_policy] epoch #393 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:04 | [train_policy] epoch #393 | computing loss before
2021-06-04 13:53:04 | [train_policy] epoch #393 | computing gradient
2021-06-04 13:53:04 | [train_policy] epoch #393 | gradient computed
2021-06-04 13:53:04 | [train_policy] epoch #393 | computing descent direction
2021-06-04 13:53:04 | [train_policy] epoch #393 | descent direction computed
2021-06-04 13:53:04 | [train_policy] epoch #393 | backtrack iters: 0
2021-06-04 13:53:04 | [train_policy] epoch #393 | optimization finished
2021-06-04 13:53:04 | [train_policy] epoch #393 | Computing KL after
2021-06-04 13:53:04 | [train_policy] epoch #393 | Computing loss after
2021-06-04 13:53:04 | [train_policy] epoch #393 | Fitting baseline...
2021-06-04 13:53:04 | [train_policy] epoch #393 | Saving snapshot...
2021-06-04 13:53:04 | [train_policy] epoch #393 | Saved
2021-06-04 13:53:04 | [train_policy] epoch #393 | Time 316.61 s
2021-06-04 13:53:04 | [train_policy] epoch #393 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287183
Evaluation/AverageDiscountedReturn          -45.3057
Evaluation/AverageReturn                    -45.3057
Evaluation/CompletionRate                     0
Evaluation/Iteration                        393
Evaluation/MaxReturn                        -30.6998
Evaluation/MinReturn                       -301.357
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         27.823
Extras/EpisodeRewardMean                    -45.2944
LinearFeatureBaseline/ExplainedVariance       0.233851
PolicyExecTime                                0.235607
ProcessExecTime                               0.0313766
TotalEnvSteps                            398728
policy/Entropy                               -0.981687
policy/KL                                     0.00977547
policy/KLBefore                               0
policy/LossAfter                             -0.0275944
policy/LossBefore                             1.31931e-08
policy/Perplexity                             0.374679
policy/dLoss                                  0.0275944
---------------------------------------  ----------------
2021-06-04 13:53:04 | [train_policy] epoch #394 | Obtaining samples for iteration 394...
2021-06-04 13:53:05 | [train_policy] epoch #394 | Logging diagnostics...
2021-06-04 13:53:05 | [train_policy] epoch #394 | Optimizing policy...
2021-06-04 13:53:05 | [train_policy] epoch #394 | Computing loss before
2021-06-04 13:53:05 | [train_policy] epoch #394 | Computing KL before
2021-06-04 13:53:05 | [train_policy] epoch #394 | Optimizing
2021-06-04 13:53:05 | [train_policy] epoch #394 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:05 | [train_policy] epoch #394 | computing loss before
2021-06-04 13:53:05 | [train_policy] epoch #394 | computing gradient
2021-06-04 13:53:05 | [train_policy] epoch #394 | gradient computed
2021-06-04 13:53:05 | [train_policy] epoch #394 | computing descent direction
2021-06-04 13:53:05 | [train_policy] epoch #394 | descent direction computed
2021-06-04 13:53:05 | [train_policy] epoch #394 | backtrack iters: 0
2021-06-04 13:53:05 | [train_policy] epoch #394 | optimization finished
2021-06-04 13:53:05 | [train_policy] epoch #394 | Computing KL after
2021-06-04 13:53:05 | [train_policy] epoch #394 | Computing loss after
2021-06-04 13:53:05 | [train_policy] epoch #394 | Fitting baseline...
2021-06-04 13:53:05 | [train_policy] epoch #394 | Saving snapshot...
2021-06-04 13:53:05 | [train_policy] epoch #394 | Saved
2021-06-04 13:53:05 | [train_policy] epoch #394 | Time 317.42 s
2021-06-04 13:53:05 | [train_policy] epoch #394 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285769
Evaluation/AverageDiscountedReturn          -43.7486
Evaluation/AverageReturn                    -43.7486
Evaluation/CompletionRate                     0
Evaluation/Iteration                        394
Evaluation/MaxReturn                        -32.0562
Evaluation/MinReturn                        -94.3129
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.90077
Extras/EpisodeRewardMean                    -43.5618
LinearFeatureBaseline/ExplainedVariance       0.197171
PolicyExecTime                                0.238286
ProcessExecTime                               0.0313439
TotalEnvSteps                            399740
policy/Entropy                               -0.97859
policy/KL                                     0.00975046
policy/KLBefore                               0
policy/LossAfter                             -0.0936092
policy/LossBefore                             1.97897e-08
policy/Perplexity                             0.375841
policy/dLoss                                  0.0936092
---------------------------------------  ----------------
2021-06-04 13:53:05 | [train_policy] epoch #395 | Obtaining samples for iteration 395...
2021-06-04 13:53:06 | [train_policy] epoch #395 | Logging diagnostics...
2021-06-04 13:53:06 | [train_policy] epoch #395 | Optimizing policy...
2021-06-04 13:53:06 | [train_policy] epoch #395 | Computing loss before
2021-06-04 13:53:06 | [train_policy] epoch #395 | Computing KL before
2021-06-04 13:53:06 | [train_policy] epoch #395 | Optimizing
2021-06-04 13:53:06 | [train_policy] epoch #395 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:06 | [train_policy] epoch #395 | computing loss before
2021-06-04 13:53:06 | [train_policy] epoch #395 | computing gradient
2021-06-04 13:53:06 | [train_policy] epoch #395 | gradient computed
2021-06-04 13:53:06 | [train_policy] epoch #395 | computing descent direction
2021-06-04 13:53:06 | [train_policy] epoch #395 | descent direction computed
2021-06-04 13:53:06 | [train_policy] epoch #395 | backtrack iters: 0
2021-06-04 13:53:06 | [train_policy] epoch #395 | optimization finished
2021-06-04 13:53:06 | [train_policy] epoch #395 | Computing KL after
2021-06-04 13:53:06 | [train_policy] epoch #395 | Computing loss after
2021-06-04 13:53:06 | [train_policy] epoch #395 | Fitting baseline...
2021-06-04 13:53:06 | [train_policy] epoch #395 | Saving snapshot...
2021-06-04 13:53:06 | [train_policy] epoch #395 | Saved
2021-06-04 13:53:06 | [train_policy] epoch #395 | Time 318.20 s
2021-06-04 13:53:06 | [train_policy] epoch #395 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284402
Evaluation/AverageDiscountedReturn          -42.5718
Evaluation/AverageReturn                    -42.5718
Evaluation/CompletionRate                     0
Evaluation/Iteration                        395
Evaluation/MaxReturn                        -30.2032
Evaluation/MinReturn                        -63.8192
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.80742
Extras/EpisodeRewardMean                    -42.7534
LinearFeatureBaseline/ExplainedVariance       0.90589
PolicyExecTime                                0.212237
ProcessExecTime                               0.0314276
TotalEnvSteps                            400752
policy/Entropy                               -0.992102
policy/KL                                     0.00970683
policy/KLBefore                               0
policy/LossAfter                             -0.0168597
policy/LossBefore                             1.64914e-08
policy/Perplexity                             0.370797
policy/dLoss                                  0.0168598
---------------------------------------  ----------------
2021-06-04 13:53:06 | [train_policy] epoch #396 | Obtaining samples for iteration 396...
2021-06-04 13:53:07 | [train_policy] epoch #396 | Logging diagnostics...
2021-06-04 13:53:07 | [train_policy] epoch #396 | Optimizing policy...
2021-06-04 13:53:07 | [train_policy] epoch #396 | Computing loss before
2021-06-04 13:53:07 | [train_policy] epoch #396 | Computing KL before
2021-06-04 13:53:07 | [train_policy] epoch #396 | Optimizing
2021-06-04 13:53:07 | [train_policy] epoch #396 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:07 | [train_policy] epoch #396 | computing loss before
2021-06-04 13:53:07 | [train_policy] epoch #396 | computing gradient
2021-06-04 13:53:07 | [train_policy] epoch #396 | gradient computed
2021-06-04 13:53:07 | [train_policy] epoch #396 | computing descent direction
2021-06-04 13:53:07 | [train_policy] epoch #396 | descent direction computed
2021-06-04 13:53:07 | [train_policy] epoch #396 | backtrack iters: 1
2021-06-04 13:53:07 | [train_policy] epoch #396 | optimization finished
2021-06-04 13:53:07 | [train_policy] epoch #396 | Computing KL after
2021-06-04 13:53:07 | [train_policy] epoch #396 | Computing loss after
2021-06-04 13:53:07 | [train_policy] epoch #396 | Fitting baseline...
2021-06-04 13:53:07 | [train_policy] epoch #396 | Saving snapshot...
2021-06-04 13:53:07 | [train_policy] epoch #396 | Saved
2021-06-04 13:53:07 | [train_policy] epoch #396 | Time 319.00 s
2021-06-04 13:53:07 | [train_policy] epoch #396 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284223
Evaluation/AverageDiscountedReturn          -42.0913
Evaluation/AverageReturn                    -42.0913
Evaluation/CompletionRate                     0
Evaluation/Iteration                        396
Evaluation/MaxReturn                        -31.1216
Evaluation/MinReturn                        -63.9905
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.3226
Extras/EpisodeRewardMean                    -42.4885
LinearFeatureBaseline/ExplainedVariance       0.916904
PolicyExecTime                                0.232635
ProcessExecTime                               0.0311267
TotalEnvSteps                            401764
policy/Entropy                               -0.978918
policy/KL                                     0.00652682
policy/KLBefore                               0
policy/LossAfter                             -0.021294
policy/LossBefore                            -1.17796e-08
policy/Perplexity                             0.375717
policy/dLoss                                  0.021294
---------------------------------------  ----------------
2021-06-04 13:53:07 | [train_policy] epoch #397 | Obtaining samples for iteration 397...
2021-06-04 13:53:07 | [train_policy] epoch #397 | Logging diagnostics...
2021-06-04 13:53:07 | [train_policy] epoch #397 | Optimizing policy...
2021-06-04 13:53:07 | [train_policy] epoch #397 | Computing loss before
2021-06-04 13:53:07 | [train_policy] epoch #397 | Computing KL before
2021-06-04 13:53:07 | [train_policy] epoch #397 | Optimizing
2021-06-04 13:53:07 | [train_policy] epoch #397 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:07 | [train_policy] epoch #397 | computing loss before
2021-06-04 13:53:07 | [train_policy] epoch #397 | computing gradient
2021-06-04 13:53:07 | [train_policy] epoch #397 | gradient computed
2021-06-04 13:53:07 | [train_policy] epoch #397 | computing descent direction
2021-06-04 13:53:08 | [train_policy] epoch #397 | descent direction computed
2021-06-04 13:53:08 | [train_policy] epoch #397 | backtrack iters: 0
2021-06-04 13:53:08 | [train_policy] epoch #397 | optimization finished
2021-06-04 13:53:08 | [train_policy] epoch #397 | Computing KL after
2021-06-04 13:53:08 | [train_policy] epoch #397 | Computing loss after
2021-06-04 13:53:08 | [train_policy] epoch #397 | Fitting baseline...
2021-06-04 13:53:08 | [train_policy] epoch #397 | Saving snapshot...
2021-06-04 13:53:08 | [train_policy] epoch #397 | Saved
2021-06-04 13:53:08 | [train_policy] epoch #397 | Time 319.79 s
2021-06-04 13:53:08 | [train_policy] epoch #397 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286908
Evaluation/AverageDiscountedReturn          -41.8915
Evaluation/AverageReturn                    -41.8915
Evaluation/CompletionRate                     0
Evaluation/Iteration                        397
Evaluation/MaxReturn                        -29.9325
Evaluation/MinReturn                        -91.7606
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.96287
Extras/EpisodeRewardMean                    -41.5993
LinearFeatureBaseline/ExplainedVariance       0.866531
PolicyExecTime                                0.21885
ProcessExecTime                               0.0314631
TotalEnvSteps                            402776
policy/Entropy                               -0.949351
policy/KL                                     0.00967799
policy/KLBefore                               0
policy/LossAfter                             -0.0309573
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.386992
policy/dLoss                                  0.0309573
---------------------------------------  ----------------
2021-06-04 13:53:08 | [train_policy] epoch #398 | Obtaining samples for iteration 398...
2021-06-04 13:53:08 | [train_policy] epoch #398 | Logging diagnostics...
2021-06-04 13:53:08 | [train_policy] epoch #398 | Optimizing policy...
2021-06-04 13:53:08 | [train_policy] epoch #398 | Computing loss before
2021-06-04 13:53:08 | [train_policy] epoch #398 | Computing KL before
2021-06-04 13:53:08 | [train_policy] epoch #398 | Optimizing
2021-06-04 13:53:08 | [train_policy] epoch #398 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:08 | [train_policy] epoch #398 | computing loss before
2021-06-04 13:53:08 | [train_policy] epoch #398 | computing gradient
2021-06-04 13:53:08 | [train_policy] epoch #398 | gradient computed
2021-06-04 13:53:08 | [train_policy] epoch #398 | computing descent direction
2021-06-04 13:53:08 | [train_policy] epoch #398 | descent direction computed
2021-06-04 13:53:08 | [train_policy] epoch #398 | backtrack iters: 1
2021-06-04 13:53:08 | [train_policy] epoch #398 | optimization finished
2021-06-04 13:53:08 | [train_policy] epoch #398 | Computing KL after
2021-06-04 13:53:08 | [train_policy] epoch #398 | Computing loss after
2021-06-04 13:53:08 | [train_policy] epoch #398 | Fitting baseline...
2021-06-04 13:53:08 | [train_policy] epoch #398 | Saving snapshot...
2021-06-04 13:53:08 | [train_policy] epoch #398 | Saved
2021-06-04 13:53:08 | [train_policy] epoch #398 | Time 320.61 s
2021-06-04 13:53:08 | [train_policy] epoch #398 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287492
Evaluation/AverageDiscountedReturn          -65.2321
Evaluation/AverageReturn                    -65.2321
Evaluation/CompletionRate                     0
Evaluation/Iteration                        398
Evaluation/MaxReturn                        -29.7744
Evaluation/MinReturn                      -2062.37
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.615
Extras/EpisodeRewardMean                    -63.2649
LinearFeatureBaseline/ExplainedVariance       0.0173842
PolicyExecTime                                0.241328
ProcessExecTime                               0.0315073
TotalEnvSteps                            403788
policy/Entropy                               -0.960346
policy/KL                                     0.00702416
policy/KLBefore                               0
policy/LossAfter                             -0.0181458
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.38276
policy/dLoss                                  0.0181458
---------------------------------------  ----------------
2021-06-04 13:53:08 | [train_policy] epoch #399 | Obtaining samples for iteration 399...
2021-06-04 13:53:09 | [train_policy] epoch #399 | Logging diagnostics...
2021-06-04 13:53:09 | [train_policy] epoch #399 | Optimizing policy...
2021-06-04 13:53:09 | [train_policy] epoch #399 | Computing loss before
2021-06-04 13:53:09 | [train_policy] epoch #399 | Computing KL before
2021-06-04 13:53:09 | [train_policy] epoch #399 | Optimizing
2021-06-04 13:53:09 | [train_policy] epoch #399 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:09 | [train_policy] epoch #399 | computing loss before
2021-06-04 13:53:09 | [train_policy] epoch #399 | computing gradient
2021-06-04 13:53:09 | [train_policy] epoch #399 | gradient computed
2021-06-04 13:53:09 | [train_policy] epoch #399 | computing descent direction
2021-06-04 13:53:09 | [train_policy] epoch #399 | descent direction computed
2021-06-04 13:53:09 | [train_policy] epoch #399 | backtrack iters: 0
2021-06-04 13:53:09 | [train_policy] epoch #399 | optimization finished
2021-06-04 13:53:09 | [train_policy] epoch #399 | Computing KL after
2021-06-04 13:53:09 | [train_policy] epoch #399 | Computing loss after
2021-06-04 13:53:09 | [train_policy] epoch #399 | Fitting baseline...
2021-06-04 13:53:09 | [train_policy] epoch #399 | Saving snapshot...
2021-06-04 13:53:09 | [train_policy] epoch #399 | Saved
2021-06-04 13:53:09 | [train_policy] epoch #399 | Time 321.41 s
2021-06-04 13:53:09 | [train_policy] epoch #399 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.285522
Evaluation/AverageDiscountedReturn          -64.494
Evaluation/AverageReturn                    -64.494
Evaluation/CompletionRate                     0
Evaluation/Iteration                        399
Evaluation/MaxReturn                        -30.8644
Evaluation/MinReturn                      -2062.08
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.529
Extras/EpisodeRewardMean                    -62.5525
LinearFeatureBaseline/ExplainedVariance       0.132087
PolicyExecTime                                0.230191
ProcessExecTime                               0.0314093
TotalEnvSteps                            404800
policy/Entropy                               -0.97401
policy/KL                                     0.00964702
policy/KLBefore                               0
policy/LossAfter                             -0.0222502
policy/LossBefore                            -1.1544e-08
policy/Perplexity                             0.377566
policy/dLoss                                  0.0222502
---------------------------------------  ---------------
2021-06-04 13:53:09 | [train_policy] epoch #400 | Obtaining samples for iteration 400...
2021-06-04 13:53:10 | [train_policy] epoch #400 | Logging diagnostics...
2021-06-04 13:53:10 | [train_policy] epoch #400 | Optimizing policy...
2021-06-04 13:53:10 | [train_policy] epoch #400 | Computing loss before
2021-06-04 13:53:10 | [train_policy] epoch #400 | Computing KL before
2021-06-04 13:53:10 | [train_policy] epoch #400 | Optimizing
2021-06-04 13:53:10 | [train_policy] epoch #400 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:10 | [train_policy] epoch #400 | computing loss before
2021-06-04 13:53:10 | [train_policy] epoch #400 | computing gradient
2021-06-04 13:53:10 | [train_policy] epoch #400 | gradient computed
2021-06-04 13:53:10 | [train_policy] epoch #400 | computing descent direction
2021-06-04 13:53:10 | [train_policy] epoch #400 | descent direction computed
2021-06-04 13:53:10 | [train_policy] epoch #400 | backtrack iters: 1
2021-06-04 13:53:10 | [train_policy] epoch #400 | optimization finished
2021-06-04 13:53:10 | [train_policy] epoch #400 | Computing KL after
2021-06-04 13:53:10 | [train_policy] epoch #400 | Computing loss after
2021-06-04 13:53:10 | [train_policy] epoch #400 | Fitting baseline...
2021-06-04 13:53:10 | [train_policy] epoch #400 | Saving snapshot...
2021-06-04 13:53:10 | [train_policy] epoch #400 | Saved
2021-06-04 13:53:10 | [train_policy] epoch #400 | Time 322.21 s
2021-06-04 13:53:10 | [train_policy] epoch #400 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285888
Evaluation/AverageDiscountedReturn          -46.6495
Evaluation/AverageReturn                    -46.6495
Evaluation/CompletionRate                     0
Evaluation/Iteration                        400
Evaluation/MaxReturn                        -30.665
Evaluation/MinReturn                       -501.828
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         48.3085
Extras/EpisodeRewardMean                    -46.4018
LinearFeatureBaseline/ExplainedVariance      -1.90248
PolicyExecTime                                0.225799
ProcessExecTime                               0.0312684
TotalEnvSteps                            405812
policy/Entropy                               -0.954177
policy/KL                                     0.0064876
policy/KLBefore                               0
policy/LossAfter                             -0.0240594
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.385129
policy/dLoss                                  0.0240595
---------------------------------------  ----------------
2021-06-04 13:53:10 | [train_policy] epoch #401 | Obtaining samples for iteration 401...
2021-06-04 13:53:11 | [train_policy] epoch #401 | Logging diagnostics...
2021-06-04 13:53:11 | [train_policy] epoch #401 | Optimizing policy...
2021-06-04 13:53:11 | [train_policy] epoch #401 | Computing loss before
2021-06-04 13:53:11 | [train_policy] epoch #401 | Computing KL before
2021-06-04 13:53:11 | [train_policy] epoch #401 | Optimizing
2021-06-04 13:53:11 | [train_policy] epoch #401 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:11 | [train_policy] epoch #401 | computing loss before
2021-06-04 13:53:11 | [train_policy] epoch #401 | computing gradient
2021-06-04 13:53:11 | [train_policy] epoch #401 | gradient computed
2021-06-04 13:53:11 | [train_policy] epoch #401 | computing descent direction
2021-06-04 13:53:11 | [train_policy] epoch #401 | descent direction computed
2021-06-04 13:53:11 | [train_policy] epoch #401 | backtrack iters: 0
2021-06-04 13:53:11 | [train_policy] epoch #401 | optimization finished
2021-06-04 13:53:11 | [train_policy] epoch #401 | Computing KL after
2021-06-04 13:53:11 | [train_policy] epoch #401 | Computing loss after
2021-06-04 13:53:11 | [train_policy] epoch #401 | Fitting baseline...
2021-06-04 13:53:11 | [train_policy] epoch #401 | Saving snapshot...
2021-06-04 13:53:11 | [train_policy] epoch #401 | Saved
2021-06-04 13:53:11 | [train_policy] epoch #401 | Time 322.99 s
2021-06-04 13:53:11 | [train_policy] epoch #401 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.28422
Evaluation/AverageDiscountedReturn          -64.5123
Evaluation/AverageReturn                    -64.5123
Evaluation/CompletionRate                     0
Evaluation/Iteration                        401
Evaluation/MaxReturn                        -30.6744
Evaluation/MinReturn                      -2062.1
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.575
Extras/EpisodeRewardMean                    -62.7236
LinearFeatureBaseline/ExplainedVariance       0.0617999
PolicyExecTime                                0.229147
ProcessExecTime                               0.0311112
TotalEnvSteps                            406824
policy/Entropy                               -0.986029
policy/KL                                     0.00986098
policy/KLBefore                               0
policy/LossAfter                             -0.0217279
policy/LossBefore                             1.0366e-08
policy/Perplexity                             0.373055
policy/dLoss                                  0.0217279
---------------------------------------  ---------------
2021-06-04 13:53:11 | [train_policy] epoch #402 | Obtaining samples for iteration 402...
2021-06-04 13:53:11 | [train_policy] epoch #402 | Logging diagnostics...
2021-06-04 13:53:11 | [train_policy] epoch #402 | Optimizing policy...
2021-06-04 13:53:11 | [train_policy] epoch #402 | Computing loss before
2021-06-04 13:53:11 | [train_policy] epoch #402 | Computing KL before
2021-06-04 13:53:11 | [train_policy] epoch #402 | Optimizing
2021-06-04 13:53:11 | [train_policy] epoch #402 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:11 | [train_policy] epoch #402 | computing loss before
2021-06-04 13:53:11 | [train_policy] epoch #402 | computing gradient
2021-06-04 13:53:11 | [train_policy] epoch #402 | gradient computed
2021-06-04 13:53:11 | [train_policy] epoch #402 | computing descent direction
2021-06-04 13:53:12 | [train_policy] epoch #402 | descent direction computed
2021-06-04 13:53:12 | [train_policy] epoch #402 | backtrack iters: 1
2021-06-04 13:53:12 | [train_policy] epoch #402 | optimization finished
2021-06-04 13:53:12 | [train_policy] epoch #402 | Computing KL after
2021-06-04 13:53:12 | [train_policy] epoch #402 | Computing loss after
2021-06-04 13:53:12 | [train_policy] epoch #402 | Fitting baseline...
2021-06-04 13:53:12 | [train_policy] epoch #402 | Saving snapshot...
2021-06-04 13:53:12 | [train_policy] epoch #402 | Saved
2021-06-04 13:53:12 | [train_policy] epoch #402 | Time 323.80 s
2021-06-04 13:53:12 | [train_policy] epoch #402 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28579
Evaluation/AverageDiscountedReturn          -47.8859
Evaluation/AverageReturn                    -47.8859
Evaluation/CompletionRate                     0
Evaluation/Iteration                        402
Evaluation/MaxReturn                        -29.2485
Evaluation/MinReturn                       -505.996
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         48.5908
Extras/EpisodeRewardMean                    -47.1204
LinearFeatureBaseline/ExplainedVariance      -0.875461
PolicyExecTime                                0.238095
ProcessExecTime                               0.0312231
TotalEnvSteps                            407836
policy/Entropy                               -1.01073
policy/KL                                     0.00668939
policy/KLBefore                               0
policy/LossAfter                             -0.0230661
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.363954
policy/dLoss                                  0.0230661
---------------------------------------  ----------------
2021-06-04 13:53:12 | [train_policy] epoch #403 | Obtaining samples for iteration 403...
2021-06-04 13:53:12 | [train_policy] epoch #403 | Logging diagnostics...
2021-06-04 13:53:12 | [train_policy] epoch #403 | Optimizing policy...
2021-06-04 13:53:12 | [train_policy] epoch #403 | Computing loss before
2021-06-04 13:53:12 | [train_policy] epoch #403 | Computing KL before
2021-06-04 13:53:12 | [train_policy] epoch #403 | Optimizing
2021-06-04 13:53:12 | [train_policy] epoch #403 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:12 | [train_policy] epoch #403 | computing loss before
2021-06-04 13:53:12 | [train_policy] epoch #403 | computing gradient
2021-06-04 13:53:12 | [train_policy] epoch #403 | gradient computed
2021-06-04 13:53:12 | [train_policy] epoch #403 | computing descent direction
2021-06-04 13:53:12 | [train_policy] epoch #403 | descent direction computed
2021-06-04 13:53:12 | [train_policy] epoch #403 | backtrack iters: 0
2021-06-04 13:53:12 | [train_policy] epoch #403 | optimization finished
2021-06-04 13:53:12 | [train_policy] epoch #403 | Computing KL after
2021-06-04 13:53:12 | [train_policy] epoch #403 | Computing loss after
2021-06-04 13:53:12 | [train_policy] epoch #403 | Fitting baseline...
2021-06-04 13:53:12 | [train_policy] epoch #403 | Saving snapshot...
2021-06-04 13:53:12 | [train_policy] epoch #403 | Saved
2021-06-04 13:53:12 | [train_policy] epoch #403 | Time 324.60 s
2021-06-04 13:53:12 | [train_policy] epoch #403 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288224
Evaluation/AverageDiscountedReturn          -64.5761
Evaluation/AverageReturn                    -64.5761
Evaluation/CompletionRate                     0
Evaluation/Iteration                        403
Evaluation/MaxReturn                        -30.9185
Evaluation/MinReturn                      -2062.01
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.528
Extras/EpisodeRewardMean                    -62.9305
LinearFeatureBaseline/ExplainedVariance       0.0648232
PolicyExecTime                                0.224329
ProcessExecTime                               0.031512
TotalEnvSteps                            408848
policy/Entropy                               -1.0169
policy/KL                                     0.0094994
policy/KLBefore                               0
policy/LossAfter                             -0.0204595
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.361715
policy/dLoss                                  0.0204595
---------------------------------------  ----------------
2021-06-04 13:53:12 | [train_policy] epoch #404 | Obtaining samples for iteration 404...
2021-06-04 13:53:13 | [train_policy] epoch #404 | Logging diagnostics...
2021-06-04 13:53:13 | [train_policy] epoch #404 | Optimizing policy...
2021-06-04 13:53:13 | [train_policy] epoch #404 | Computing loss before
2021-06-04 13:53:13 | [train_policy] epoch #404 | Computing KL before
2021-06-04 13:53:13 | [train_policy] epoch #404 | Optimizing
2021-06-04 13:53:13 | [train_policy] epoch #404 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:13 | [train_policy] epoch #404 | computing loss before
2021-06-04 13:53:13 | [train_policy] epoch #404 | computing gradient
2021-06-04 13:53:13 | [train_policy] epoch #404 | gradient computed
2021-06-04 13:53:13 | [train_policy] epoch #404 | computing descent direction
2021-06-04 13:53:13 | [train_policy] epoch #404 | descent direction computed
2021-06-04 13:53:13 | [train_policy] epoch #404 | backtrack iters: 1
2021-06-04 13:53:13 | [train_policy] epoch #404 | optimization finished
2021-06-04 13:53:13 | [train_policy] epoch #404 | Computing KL after
2021-06-04 13:53:13 | [train_policy] epoch #404 | Computing loss after
2021-06-04 13:53:13 | [train_policy] epoch #404 | Fitting baseline...
2021-06-04 13:53:13 | [train_policy] epoch #404 | Saving snapshot...
2021-06-04 13:53:13 | [train_policy] epoch #404 | Saved
2021-06-04 13:53:13 | [train_policy] epoch #404 | Time 325.38 s
2021-06-04 13:53:13 | [train_policy] epoch #404 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283573
Evaluation/AverageDiscountedReturn          -62.4205
Evaluation/AverageReturn                    -62.4205
Evaluation/CompletionRate                     0
Evaluation/Iteration                        404
Evaluation/MaxReturn                        -32.8287
Evaluation/MinReturn                      -2000.05
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        203.219
Extras/EpisodeRewardMean                    -60.9049
LinearFeatureBaseline/ExplainedVariance       0.127551
PolicyExecTime                                0.216247
ProcessExecTime                               0.0311143
TotalEnvSteps                            409860
policy/Entropy                               -1.01441
policy/KL                                     0.0064023
policy/KLBefore                               0
policy/LossAfter                             -0.0161149
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.362615
policy/dLoss                                  0.0161148
---------------------------------------  ----------------
2021-06-04 13:53:13 | [train_policy] epoch #405 | Obtaining samples for iteration 405...
2021-06-04 13:53:14 | [train_policy] epoch #405 | Logging diagnostics...
2021-06-04 13:53:14 | [train_policy] epoch #405 | Optimizing policy...
2021-06-04 13:53:14 | [train_policy] epoch #405 | Computing loss before
2021-06-04 13:53:14 | [train_policy] epoch #405 | Computing KL before
2021-06-04 13:53:14 | [train_policy] epoch #405 | Optimizing
2021-06-04 13:53:14 | [train_policy] epoch #405 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:14 | [train_policy] epoch #405 | computing loss before
2021-06-04 13:53:14 | [train_policy] epoch #405 | computing gradient
2021-06-04 13:53:14 | [train_policy] epoch #405 | gradient computed
2021-06-04 13:53:14 | [train_policy] epoch #405 | computing descent direction
2021-06-04 13:53:14 | [train_policy] epoch #405 | descent direction computed
2021-06-04 13:53:14 | [train_policy] epoch #405 | backtrack iters: 1
2021-06-04 13:53:14 | [train_policy] epoch #405 | optimization finished
2021-06-04 13:53:14 | [train_policy] epoch #405 | Computing KL after
2021-06-04 13:53:14 | [train_policy] epoch #405 | Computing loss after
2021-06-04 13:53:14 | [train_policy] epoch #405 | Fitting baseline...
2021-06-04 13:53:14 | [train_policy] epoch #405 | Saving snapshot...
2021-06-04 13:53:14 | [train_policy] epoch #405 | Saved
2021-06-04 13:53:14 | [train_policy] epoch #405 | Time 326.18 s
2021-06-04 13:53:14 | [train_policy] epoch #405 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.287029
Evaluation/AverageDiscountedReturn          -42.6015
Evaluation/AverageReturn                    -42.6015
Evaluation/CompletionRate                     0
Evaluation/Iteration                        405
Evaluation/MaxReturn                        -30.8015
Evaluation/MinReturn                        -63.8101
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.42101
Extras/EpisodeRewardMean                    -42.4034
LinearFeatureBaseline/ExplainedVariance     -35.22
PolicyExecTime                                0.223321
ProcessExecTime                               0.0314755
TotalEnvSteps                            410872
policy/Entropy                               -1.01716
policy/KL                                     0.00672573
policy/KLBefore                               0
policy/LossAfter                             -0.0270605
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.361619
policy/dLoss                                  0.0270605
---------------------------------------  ---------------
2021-06-04 13:53:14 | [train_policy] epoch #406 | Obtaining samples for iteration 406...
2021-06-04 13:53:15 | [train_policy] epoch #406 | Logging diagnostics...
2021-06-04 13:53:15 | [train_policy] epoch #406 | Optimizing policy...
2021-06-04 13:53:15 | [train_policy] epoch #406 | Computing loss before
2021-06-04 13:53:15 | [train_policy] epoch #406 | Computing KL before
2021-06-04 13:53:15 | [train_policy] epoch #406 | Optimizing
2021-06-04 13:53:15 | [train_policy] epoch #406 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:15 | [train_policy] epoch #406 | computing loss before
2021-06-04 13:53:15 | [train_policy] epoch #406 | computing gradient
2021-06-04 13:53:15 | [train_policy] epoch #406 | gradient computed
2021-06-04 13:53:15 | [train_policy] epoch #406 | computing descent direction
2021-06-04 13:53:15 | [train_policy] epoch #406 | descent direction computed
2021-06-04 13:53:15 | [train_policy] epoch #406 | backtrack iters: 1
2021-06-04 13:53:15 | [train_policy] epoch #406 | optimization finished
2021-06-04 13:53:15 | [train_policy] epoch #406 | Computing KL after
2021-06-04 13:53:15 | [train_policy] epoch #406 | Computing loss after
2021-06-04 13:53:15 | [train_policy] epoch #406 | Fitting baseline...
2021-06-04 13:53:15 | [train_policy] epoch #406 | Saving snapshot...
2021-06-04 13:53:15 | [train_policy] epoch #406 | Saved
2021-06-04 13:53:15 | [train_policy] epoch #406 | Time 326.98 s
2021-06-04 13:53:15 | [train_policy] epoch #406 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287133
Evaluation/AverageDiscountedReturn          -41.8799
Evaluation/AverageReturn                    -41.8799
Evaluation/CompletionRate                     0
Evaluation/Iteration                        406
Evaluation/MaxReturn                        -31.5158
Evaluation/MinReturn                        -63.8522
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.99562
Extras/EpisodeRewardMean                    -41.944
LinearFeatureBaseline/ExplainedVariance       0.908106
PolicyExecTime                                0.236961
ProcessExecTime                               0.0313809
TotalEnvSteps                            411884
policy/Entropy                               -1.02655
policy/KL                                     0.00665919
policy/KLBefore                               0
policy/LossAfter                             -0.015284
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.358239
policy/dLoss                                  0.015284
---------------------------------------  ----------------
2021-06-04 13:53:15 | [train_policy] epoch #407 | Obtaining samples for iteration 407...
2021-06-04 13:53:15 | [train_policy] epoch #407 | Logging diagnostics...
2021-06-04 13:53:15 | [train_policy] epoch #407 | Optimizing policy...
2021-06-04 13:53:15 | [train_policy] epoch #407 | Computing loss before
2021-06-04 13:53:15 | [train_policy] epoch #407 | Computing KL before
2021-06-04 13:53:15 | [train_policy] epoch #407 | Optimizing
2021-06-04 13:53:15 | [train_policy] epoch #407 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:15 | [train_policy] epoch #407 | computing loss before
2021-06-04 13:53:15 | [train_policy] epoch #407 | computing gradient
2021-06-04 13:53:15 | [train_policy] epoch #407 | gradient computed
2021-06-04 13:53:15 | [train_policy] epoch #407 | computing descent direction
2021-06-04 13:53:15 | [train_policy] epoch #407 | descent direction computed
2021-06-04 13:53:16 | [train_policy] epoch #407 | backtrack iters: 1
2021-06-04 13:53:16 | [train_policy] epoch #407 | optimization finished
2021-06-04 13:53:16 | [train_policy] epoch #407 | Computing KL after
2021-06-04 13:53:16 | [train_policy] epoch #407 | Computing loss after
2021-06-04 13:53:16 | [train_policy] epoch #407 | Fitting baseline...
2021-06-04 13:53:16 | [train_policy] epoch #407 | Saving snapshot...
2021-06-04 13:53:16 | [train_policy] epoch #407 | Saved
2021-06-04 13:53:16 | [train_policy] epoch #407 | Time 327.78 s
2021-06-04 13:53:16 | [train_policy] epoch #407 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285439
Evaluation/AverageDiscountedReturn          -86.4709
Evaluation/AverageReturn                    -86.4709
Evaluation/CompletionRate                     0
Evaluation/Iteration                        407
Evaluation/MaxReturn                        -29.6159
Evaluation/MinReturn                      -2062.86
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.706
Extras/EpisodeRewardMean                    -82.9358
LinearFeatureBaseline/ExplainedVariance       0.00935132
PolicyExecTime                                0.223799
ProcessExecTime                               0.0312822
TotalEnvSteps                            412896
policy/Entropy                               -1.03608
policy/KL                                     0.00715945
policy/KLBefore                               0
policy/LossAfter                             -0.0243665
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.354842
policy/dLoss                                  0.0243665
---------------------------------------  ----------------
2021-06-04 13:53:16 | [train_policy] epoch #408 | Obtaining samples for iteration 408...
2021-06-04 13:53:16 | [train_policy] epoch #408 | Logging diagnostics...
2021-06-04 13:53:16 | [train_policy] epoch #408 | Optimizing policy...
2021-06-04 13:53:16 | [train_policy] epoch #408 | Computing loss before
2021-06-04 13:53:16 | [train_policy] epoch #408 | Computing KL before
2021-06-04 13:53:16 | [train_policy] epoch #408 | Optimizing
2021-06-04 13:53:16 | [train_policy] epoch #408 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:16 | [train_policy] epoch #408 | computing loss before
2021-06-04 13:53:16 | [train_policy] epoch #408 | computing gradient
2021-06-04 13:53:16 | [train_policy] epoch #408 | gradient computed
2021-06-04 13:53:16 | [train_policy] epoch #408 | computing descent direction
2021-06-04 13:53:16 | [train_policy] epoch #408 | descent direction computed
2021-06-04 13:53:16 | [train_policy] epoch #408 | backtrack iters: 0
2021-06-04 13:53:16 | [train_policy] epoch #408 | optimization finished
2021-06-04 13:53:16 | [train_policy] epoch #408 | Computing KL after
2021-06-04 13:53:16 | [train_policy] epoch #408 | Computing loss after
2021-06-04 13:53:16 | [train_policy] epoch #408 | Fitting baseline...
2021-06-04 13:53:16 | [train_policy] epoch #408 | Saving snapshot...
2021-06-04 13:53:16 | [train_policy] epoch #408 | Saved
2021-06-04 13:53:16 | [train_policy] epoch #408 | Time 328.57 s
2021-06-04 13:53:16 | [train_policy] epoch #408 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284179
Evaluation/AverageDiscountedReturn          -42.5612
Evaluation/AverageReturn                    -42.5612
Evaluation/CompletionRate                     0
Evaluation/Iteration                        408
Evaluation/MaxReturn                        -29.601
Evaluation/MinReturn                        -63.864
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85916
Extras/EpisodeRewardMean                    -42.8085
LinearFeatureBaseline/ExplainedVariance   -1845.05
PolicyExecTime                                0.224775
ProcessExecTime                               0.0311642
TotalEnvSteps                            413908
policy/Entropy                               -1.0258
policy/KL                                     0.00991811
policy/KLBefore                               0
policy/LossAfter                             -0.042523
policy/LossBefore                             7.06774e-10
policy/Perplexity                             0.358509
policy/dLoss                                  0.042523
---------------------------------------  ----------------
2021-06-04 13:53:16 | [train_policy] epoch #409 | Obtaining samples for iteration 409...
2021-06-04 13:53:17 | [train_policy] epoch #409 | Logging diagnostics...
2021-06-04 13:53:17 | [train_policy] epoch #409 | Optimizing policy...
2021-06-04 13:53:17 | [train_policy] epoch #409 | Computing loss before
2021-06-04 13:53:17 | [train_policy] epoch #409 | Computing KL before
2021-06-04 13:53:17 | [train_policy] epoch #409 | Optimizing
2021-06-04 13:53:17 | [train_policy] epoch #409 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:17 | [train_policy] epoch #409 | computing loss before
2021-06-04 13:53:17 | [train_policy] epoch #409 | computing gradient
2021-06-04 13:53:17 | [train_policy] epoch #409 | gradient computed
2021-06-04 13:53:17 | [train_policy] epoch #409 | computing descent direction
2021-06-04 13:53:17 | [train_policy] epoch #409 | descent direction computed
2021-06-04 13:53:17 | [train_policy] epoch #409 | backtrack iters: 1
2021-06-04 13:53:17 | [train_policy] epoch #409 | optimization finished
2021-06-04 13:53:17 | [train_policy] epoch #409 | Computing KL after
2021-06-04 13:53:17 | [train_policy] epoch #409 | Computing loss after
2021-06-04 13:53:17 | [train_policy] epoch #409 | Fitting baseline...
2021-06-04 13:53:17 | [train_policy] epoch #409 | Saving snapshot...
2021-06-04 13:53:17 | [train_policy] epoch #409 | Saved
2021-06-04 13:53:17 | [train_policy] epoch #409 | Time 329.38 s
2021-06-04 13:53:17 | [train_policy] epoch #409 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287789
Evaluation/AverageDiscountedReturn          -42.2529
Evaluation/AverageReturn                    -42.2529
Evaluation/CompletionRate                     0
Evaluation/Iteration                        409
Evaluation/MaxReturn                        -28.4553
Evaluation/MinReturn                       -169.868
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         15.0645
Extras/EpisodeRewardMean                    -42.1349
LinearFeatureBaseline/ExplainedVariance       0.526198
PolicyExecTime                                0.230169
ProcessExecTime                               0.0315411
TotalEnvSteps                            414920
policy/Entropy                               -0.9838
policy/KL                                     0.00663725
policy/KLBefore                               0
policy/LossAfter                             -0.0198959
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.373888
policy/dLoss                                  0.0198959
---------------------------------------  ----------------
2021-06-04 13:53:17 | [train_policy] epoch #410 | Obtaining samples for iteration 410...
2021-06-04 13:53:18 | [train_policy] epoch #410 | Logging diagnostics...
2021-06-04 13:53:18 | [train_policy] epoch #410 | Optimizing policy...
2021-06-04 13:53:18 | [train_policy] epoch #410 | Computing loss before
2021-06-04 13:53:18 | [train_policy] epoch #410 | Computing KL before
2021-06-04 13:53:18 | [train_policy] epoch #410 | Optimizing
2021-06-04 13:53:18 | [train_policy] epoch #410 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:18 | [train_policy] epoch #410 | computing loss before
2021-06-04 13:53:18 | [train_policy] epoch #410 | computing gradient
2021-06-04 13:53:18 | [train_policy] epoch #410 | gradient computed
2021-06-04 13:53:18 | [train_policy] epoch #410 | computing descent direction
2021-06-04 13:53:18 | [train_policy] epoch #410 | descent direction computed
2021-06-04 13:53:18 | [train_policy] epoch #410 | backtrack iters: 0
2021-06-04 13:53:18 | [train_policy] epoch #410 | optimization finished
2021-06-04 13:53:18 | [train_policy] epoch #410 | Computing KL after
2021-06-04 13:53:18 | [train_policy] epoch #410 | Computing loss after
2021-06-04 13:53:18 | [train_policy] epoch #410 | Fitting baseline...
2021-06-04 13:53:18 | [train_policy] epoch #410 | Saving snapshot...
2021-06-04 13:53:18 | [train_policy] epoch #410 | Saved
2021-06-04 13:53:18 | [train_policy] epoch #410 | Time 330.19 s
2021-06-04 13:53:18 | [train_policy] epoch #410 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.284083
Evaluation/AverageDiscountedReturn          -67.1881
Evaluation/AverageReturn                    -67.1881
Evaluation/CompletionRate                     0
Evaluation/Iteration                        410
Evaluation/MaxReturn                        -31.5934
Evaluation/MinReturn                      -2062.36
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.517
Extras/EpisodeRewardMean                    -65.2249
LinearFeatureBaseline/ExplainedVariance       0.0274283
PolicyExecTime                                0.228881
ProcessExecTime                               0.03105
TotalEnvSteps                            415932
policy/Entropy                               -0.976502
policy/KL                                     0.00959285
policy/KLBefore                               0
policy/LossAfter                             -0.0204774
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.376626
policy/dLoss                                  0.0204774
---------------------------------------  ---------------
2021-06-04 13:53:18 | [train_policy] epoch #411 | Obtaining samples for iteration 411...
2021-06-04 13:53:19 | [train_policy] epoch #411 | Logging diagnostics...
2021-06-04 13:53:19 | [train_policy] epoch #411 | Optimizing policy...
2021-06-04 13:53:19 | [train_policy] epoch #411 | Computing loss before
2021-06-04 13:53:19 | [train_policy] epoch #411 | Computing KL before
2021-06-04 13:53:19 | [train_policy] epoch #411 | Optimizing
2021-06-04 13:53:19 | [train_policy] epoch #411 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:19 | [train_policy] epoch #411 | computing loss before
2021-06-04 13:53:19 | [train_policy] epoch #411 | computing gradient
2021-06-04 13:53:19 | [train_policy] epoch #411 | gradient computed
2021-06-04 13:53:19 | [train_policy] epoch #411 | computing descent direction
2021-06-04 13:53:19 | [train_policy] epoch #411 | descent direction computed
2021-06-04 13:53:19 | [train_policy] epoch #411 | backtrack iters: 0
2021-06-04 13:53:19 | [train_policy] epoch #411 | optimization finished
2021-06-04 13:53:19 | [train_policy] epoch #411 | Computing KL after
2021-06-04 13:53:19 | [train_policy] epoch #411 | Computing loss after
2021-06-04 13:53:19 | [train_policy] epoch #411 | Fitting baseline...
2021-06-04 13:53:19 | [train_policy] epoch #411 | Saving snapshot...
2021-06-04 13:53:19 | [train_policy] epoch #411 | Saved
2021-06-04 13:53:19 | [train_policy] epoch #411 | Time 330.97 s
2021-06-04 13:53:19 | [train_policy] epoch #411 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285283
Evaluation/AverageDiscountedReturn          -41.706
Evaluation/AverageReturn                    -41.706
Evaluation/CompletionRate                     0
Evaluation/Iteration                        411
Evaluation/MaxReturn                        -29.4425
Evaluation/MinReturn                        -58.3836
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.96992
Extras/EpisodeRewardMean                    -41.9617
LinearFeatureBaseline/ExplainedVariance     -13.6585
PolicyExecTime                                0.222996
ProcessExecTime                               0.0312438
TotalEnvSteps                            416944
policy/Entropy                               -0.977555
policy/KL                                     0.00967976
policy/KLBefore                               0
policy/LossAfter                             -0.0262403
policy/LossBefore                            -1.41355e-08
policy/Perplexity                             0.37623
policy/dLoss                                  0.0262403
---------------------------------------  ----------------
2021-06-04 13:53:19 | [train_policy] epoch #412 | Obtaining samples for iteration 412...
2021-06-04 13:53:19 | [train_policy] epoch #412 | Logging diagnostics...
2021-06-04 13:53:19 | [train_policy] epoch #412 | Optimizing policy...
2021-06-04 13:53:19 | [train_policy] epoch #412 | Computing loss before
2021-06-04 13:53:19 | [train_policy] epoch #412 | Computing KL before
2021-06-04 13:53:19 | [train_policy] epoch #412 | Optimizing
2021-06-04 13:53:19 | [train_policy] epoch #412 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:19 | [train_policy] epoch #412 | computing loss before
2021-06-04 13:53:19 | [train_policy] epoch #412 | computing gradient
2021-06-04 13:53:19 | [train_policy] epoch #412 | gradient computed
2021-06-04 13:53:19 | [train_policy] epoch #412 | computing descent direction
2021-06-04 13:53:19 | [train_policy] epoch #412 | descent direction computed
2021-06-04 13:53:19 | [train_policy] epoch #412 | backtrack iters: 1
2021-06-04 13:53:19 | [train_policy] epoch #412 | optimization finished
2021-06-04 13:53:19 | [train_policy] epoch #412 | Computing KL after
2021-06-04 13:53:19 | [train_policy] epoch #412 | Computing loss after
2021-06-04 13:53:20 | [train_policy] epoch #412 | Fitting baseline...
2021-06-04 13:53:20 | [train_policy] epoch #412 | Saving snapshot...
2021-06-04 13:53:20 | [train_policy] epoch #412 | Saved
2021-06-04 13:53:20 | [train_policy] epoch #412 | Time 331.76 s
2021-06-04 13:53:20 | [train_policy] epoch #412 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285366
Evaluation/AverageDiscountedReturn          -64.9243
Evaluation/AverageReturn                    -64.9243
Evaluation/CompletionRate                     0
Evaluation/Iteration                        412
Evaluation/MaxReturn                        -29.3611
Evaluation/MinReturn                      -2062.63
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.561
Extras/EpisodeRewardMean                    -63.3719
LinearFeatureBaseline/ExplainedVariance       0.0106256
PolicyExecTime                                0.214751
ProcessExecTime                               0.0312366
TotalEnvSteps                            417956
policy/Entropy                               -1.01097
policy/KL                                     0.00662963
policy/KLBefore                               0
policy/LossAfter                             -0.0175671
policy/LossBefore                             1.22508e-08
policy/Perplexity                             0.363867
policy/dLoss                                  0.0175671
---------------------------------------  ----------------
2021-06-04 13:53:20 | [train_policy] epoch #413 | Obtaining samples for iteration 413...
2021-06-04 13:53:20 | [train_policy] epoch #413 | Logging diagnostics...
2021-06-04 13:53:20 | [train_policy] epoch #413 | Optimizing policy...
2021-06-04 13:53:20 | [train_policy] epoch #413 | Computing loss before
2021-06-04 13:53:20 | [train_policy] epoch #413 | Computing KL before
2021-06-04 13:53:20 | [train_policy] epoch #413 | Optimizing
2021-06-04 13:53:20 | [train_policy] epoch #413 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:20 | [train_policy] epoch #413 | computing loss before
2021-06-04 13:53:20 | [train_policy] epoch #413 | computing gradient
2021-06-04 13:53:20 | [train_policy] epoch #413 | gradient computed
2021-06-04 13:53:20 | [train_policy] epoch #413 | computing descent direction
2021-06-04 13:53:20 | [train_policy] epoch #413 | descent direction computed
2021-06-04 13:53:20 | [train_policy] epoch #413 | backtrack iters: 1
2021-06-04 13:53:20 | [train_policy] epoch #413 | optimization finished
2021-06-04 13:53:20 | [train_policy] epoch #413 | Computing KL after
2021-06-04 13:53:20 | [train_policy] epoch #413 | Computing loss after
2021-06-04 13:53:20 | [train_policy] epoch #413 | Fitting baseline...
2021-06-04 13:53:20 | [train_policy] epoch #413 | Saving snapshot...
2021-06-04 13:53:20 | [train_policy] epoch #413 | Saved
2021-06-04 13:53:20 | [train_policy] epoch #413 | Time 332.56 s
2021-06-04 13:53:20 | [train_policy] epoch #413 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285233
Evaluation/AverageDiscountedReturn          -43.283
Evaluation/AverageReturn                    -43.283
Evaluation/CompletionRate                     0
Evaluation/Iteration                        413
Evaluation/MaxReturn                        -29.8318
Evaluation/MinReturn                       -152.421
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         13.7787
Extras/EpisodeRewardMean                    -42.9839
LinearFeatureBaseline/ExplainedVariance     -17.2032
PolicyExecTime                                0.225982
ProcessExecTime                               0.031235
TotalEnvSteps                            418968
policy/Entropy                               -1.00689
policy/KL                                     0.00645686
policy/KLBefore                               0
policy/LossAfter                             -0.0481702
policy/LossBefore                             6.12538e-09
policy/Perplexity                             0.365354
policy/dLoss                                  0.0481702
---------------------------------------  ----------------
2021-06-04 13:53:20 | [train_policy] epoch #414 | Obtaining samples for iteration 414...
2021-06-04 13:53:21 | [train_policy] epoch #414 | Logging diagnostics...
2021-06-04 13:53:21 | [train_policy] epoch #414 | Optimizing policy...
2021-06-04 13:53:21 | [train_policy] epoch #414 | Computing loss before
2021-06-04 13:53:21 | [train_policy] epoch #414 | Computing KL before
2021-06-04 13:53:21 | [train_policy] epoch #414 | Optimizing
2021-06-04 13:53:21 | [train_policy] epoch #414 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:21 | [train_policy] epoch #414 | computing loss before
2021-06-04 13:53:21 | [train_policy] epoch #414 | computing gradient
2021-06-04 13:53:21 | [train_policy] epoch #414 | gradient computed
2021-06-04 13:53:21 | [train_policy] epoch #414 | computing descent direction
2021-06-04 13:53:21 | [train_policy] epoch #414 | descent direction computed
2021-06-04 13:53:21 | [train_policy] epoch #414 | backtrack iters: 1
2021-06-04 13:53:21 | [train_policy] epoch #414 | optimization finished
2021-06-04 13:53:21 | [train_policy] epoch #414 | Computing KL after
2021-06-04 13:53:21 | [train_policy] epoch #414 | Computing loss after
2021-06-04 13:53:21 | [train_policy] epoch #414 | Fitting baseline...
2021-06-04 13:53:21 | [train_policy] epoch #414 | Saving snapshot...
2021-06-04 13:53:21 | [train_policy] epoch #414 | Saved
2021-06-04 13:53:21 | [train_policy] epoch #414 | Time 333.37 s
2021-06-04 13:53:21 | [train_policy] epoch #414 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285206
Evaluation/AverageDiscountedReturn          -64.8473
Evaluation/AverageReturn                    -64.8473
Evaluation/CompletionRate                     0
Evaluation/Iteration                        414
Evaluation/MaxReturn                        -29.9593
Evaluation/MinReturn                      -2062.08
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.192
Extras/EpisodeRewardMean                    -63.1284
LinearFeatureBaseline/ExplainedVariance       0.0251371
PolicyExecTime                                0.229813
ProcessExecTime                               0.0311821
TotalEnvSteps                            419980
policy/Entropy                               -1.02116
policy/KL                                     0.00651238
policy/KLBefore                               0
policy/LossAfter                             -0.0206422
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.360176
policy/dLoss                                  0.0206422
---------------------------------------  ----------------
2021-06-04 13:53:21 | [train_policy] epoch #415 | Obtaining samples for iteration 415...
2021-06-04 13:53:22 | [train_policy] epoch #415 | Logging diagnostics...
2021-06-04 13:53:22 | [train_policy] epoch #415 | Optimizing policy...
2021-06-04 13:53:22 | [train_policy] epoch #415 | Computing loss before
2021-06-04 13:53:22 | [train_policy] epoch #415 | Computing KL before
2021-06-04 13:53:22 | [train_policy] epoch #415 | Optimizing
2021-06-04 13:53:22 | [train_policy] epoch #415 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:22 | [train_policy] epoch #415 | computing loss before
2021-06-04 13:53:22 | [train_policy] epoch #415 | computing gradient
2021-06-04 13:53:22 | [train_policy] epoch #415 | gradient computed
2021-06-04 13:53:22 | [train_policy] epoch #415 | computing descent direction
2021-06-04 13:53:22 | [train_policy] epoch #415 | descent direction computed
2021-06-04 13:53:22 | [train_policy] epoch #415 | backtrack iters: 0
2021-06-04 13:53:22 | [train_policy] epoch #415 | optimization finished
2021-06-04 13:53:22 | [train_policy] epoch #415 | Computing KL after
2021-06-04 13:53:22 | [train_policy] epoch #415 | Computing loss after
2021-06-04 13:53:22 | [train_policy] epoch #415 | Fitting baseline...
2021-06-04 13:53:22 | [train_policy] epoch #415 | Saving snapshot...
2021-06-04 13:53:22 | [train_policy] epoch #415 | Saved
2021-06-04 13:53:22 | [train_policy] epoch #415 | Time 334.16 s
2021-06-04 13:53:22 | [train_policy] epoch #415 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284343
Evaluation/AverageDiscountedReturn          -43.857
Evaluation/AverageReturn                    -43.857
Evaluation/CompletionRate                     0
Evaluation/Iteration                        415
Evaluation/MaxReturn                        -32.2701
Evaluation/MinReturn                        -63.9369
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.99279
Extras/EpisodeRewardMean                    -43.5001
LinearFeatureBaseline/ExplainedVariance     -31.0184
PolicyExecTime                                0.227486
ProcessExecTime                               0.0311463
TotalEnvSteps                            420992
policy/Entropy                               -1.01389
policy/KL                                     0.00995039
policy/KLBefore                               0
policy/LossAfter                             -0.0381964
policy/LossBefore                            -9.23519e-08
policy/Perplexity                             0.362804
policy/dLoss                                  0.0381963
---------------------------------------  ----------------
2021-06-04 13:53:22 | [train_policy] epoch #416 | Obtaining samples for iteration 416...
2021-06-04 13:53:23 | [train_policy] epoch #416 | Logging diagnostics...
2021-06-04 13:53:23 | [train_policy] epoch #416 | Optimizing policy...
2021-06-04 13:53:23 | [train_policy] epoch #416 | Computing loss before
2021-06-04 13:53:23 | [train_policy] epoch #416 | Computing KL before
2021-06-04 13:53:23 | [train_policy] epoch #416 | Optimizing
2021-06-04 13:53:23 | [train_policy] epoch #416 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:23 | [train_policy] epoch #416 | computing loss before
2021-06-04 13:53:23 | [train_policy] epoch #416 | computing gradient
2021-06-04 13:53:23 | [train_policy] epoch #416 | gradient computed
2021-06-04 13:53:23 | [train_policy] epoch #416 | computing descent direction
2021-06-04 13:53:23 | [train_policy] epoch #416 | descent direction computed
2021-06-04 13:53:23 | [train_policy] epoch #416 | backtrack iters: 1
2021-06-04 13:53:23 | [train_policy] epoch #416 | optimization finished
2021-06-04 13:53:23 | [train_policy] epoch #416 | Computing KL after
2021-06-04 13:53:23 | [train_policy] epoch #416 | Computing loss after
2021-06-04 13:53:23 | [train_policy] epoch #416 | Fitting baseline...
2021-06-04 13:53:23 | [train_policy] epoch #416 | Saving snapshot...
2021-06-04 13:53:23 | [train_policy] epoch #416 | Saved
2021-06-04 13:53:23 | [train_policy] epoch #416 | Time 334.96 s
2021-06-04 13:53:23 | [train_policy] epoch #416 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286228
Evaluation/AverageDiscountedReturn          -41.7801
Evaluation/AverageReturn                    -41.7801
Evaluation/CompletionRate                     0
Evaluation/Iteration                        416
Evaluation/MaxReturn                        -31.3279
Evaluation/MinReturn                        -63.0269
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.20243
Extras/EpisodeRewardMean                    -41.9698
LinearFeatureBaseline/ExplainedVariance       0.913333
PolicyExecTime                                0.219859
ProcessExecTime                               0.0313618
TotalEnvSteps                            422004
policy/Entropy                               -1.04291
policy/KL                                     0.00689658
policy/KLBefore                               0
policy/LossAfter                             -0.0199038
policy/LossBefore                            -1.13084e-08
policy/Perplexity                             0.352428
policy/dLoss                                  0.0199038
---------------------------------------  ----------------
2021-06-04 13:53:23 | [train_policy] epoch #417 | Obtaining samples for iteration 417...
2021-06-04 13:53:23 | [train_policy] epoch #417 | Logging diagnostics...
2021-06-04 13:53:23 | [train_policy] epoch #417 | Optimizing policy...
2021-06-04 13:53:23 | [train_policy] epoch #417 | Computing loss before
2021-06-04 13:53:23 | [train_policy] epoch #417 | Computing KL before
2021-06-04 13:53:23 | [train_policy] epoch #417 | Optimizing
2021-06-04 13:53:23 | [train_policy] epoch #417 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:23 | [train_policy] epoch #417 | computing loss before
2021-06-04 13:53:23 | [train_policy] epoch #417 | computing gradient
2021-06-04 13:53:23 | [train_policy] epoch #417 | gradient computed
2021-06-04 13:53:23 | [train_policy] epoch #417 | computing descent direction
2021-06-04 13:53:23 | [train_policy] epoch #417 | descent direction computed
2021-06-04 13:53:23 | [train_policy] epoch #417 | backtrack iters: 1
2021-06-04 13:53:23 | [train_policy] epoch #417 | optimization finished
2021-06-04 13:53:23 | [train_policy] epoch #417 | Computing KL after
2021-06-04 13:53:23 | [train_policy] epoch #417 | Computing loss after
2021-06-04 13:53:23 | [train_policy] epoch #417 | Fitting baseline...
2021-06-04 13:53:24 | [train_policy] epoch #417 | Saving snapshot...
2021-06-04 13:53:24 | [train_policy] epoch #417 | Saved
2021-06-04 13:53:24 | [train_policy] epoch #417 | Time 335.76 s
2021-06-04 13:53:24 | [train_policy] epoch #417 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28486
Evaluation/AverageDiscountedReturn          -41.1636
Evaluation/AverageReturn                    -41.1636
Evaluation/CompletionRate                     0
Evaluation/Iteration                        417
Evaluation/MaxReturn                        -29.3418
Evaluation/MinReturn                        -63.8615
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.99627
Extras/EpisodeRewardMean                    -41.0301
LinearFeatureBaseline/ExplainedVariance       0.911427
PolicyExecTime                                0.231843
ProcessExecTime                               0.0311828
TotalEnvSteps                            423016
policy/Entropy                               -1.05427
policy/KL                                     0.0065486
policy/KLBefore                               0
policy/LossAfter                             -0.022525
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.348448
policy/dLoss                                  0.022525
---------------------------------------  ----------------
2021-06-04 13:53:24 | [train_policy] epoch #418 | Obtaining samples for iteration 418...
2021-06-04 13:53:24 | [train_policy] epoch #418 | Logging diagnostics...
2021-06-04 13:53:24 | [train_policy] epoch #418 | Optimizing policy...
2021-06-04 13:53:24 | [train_policy] epoch #418 | Computing loss before
2021-06-04 13:53:24 | [train_policy] epoch #418 | Computing KL before
2021-06-04 13:53:24 | [train_policy] epoch #418 | Optimizing
2021-06-04 13:53:24 | [train_policy] epoch #418 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:24 | [train_policy] epoch #418 | computing loss before
2021-06-04 13:53:24 | [train_policy] epoch #418 | computing gradient
2021-06-04 13:53:24 | [train_policy] epoch #418 | gradient computed
2021-06-04 13:53:24 | [train_policy] epoch #418 | computing descent direction
2021-06-04 13:53:24 | [train_policy] epoch #418 | descent direction computed
2021-06-04 13:53:24 | [train_policy] epoch #418 | backtrack iters: 1
2021-06-04 13:53:24 | [train_policy] epoch #418 | optimization finished
2021-06-04 13:53:24 | [train_policy] epoch #418 | Computing KL after
2021-06-04 13:53:24 | [train_policy] epoch #418 | Computing loss after
2021-06-04 13:53:24 | [train_policy] epoch #418 | Fitting baseline...
2021-06-04 13:53:24 | [train_policy] epoch #418 | Saving snapshot...
2021-06-04 13:53:24 | [train_policy] epoch #418 | Saved
2021-06-04 13:53:24 | [train_policy] epoch #418 | Time 336.55 s
2021-06-04 13:53:24 | [train_policy] epoch #418 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284776
Evaluation/AverageDiscountedReturn          -43.61
Evaluation/AverageReturn                    -43.61
Evaluation/CompletionRate                     0
Evaluation/Iteration                        418
Evaluation/MaxReturn                        -32.7222
Evaluation/MinReturn                        -94.7504
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.89784
Extras/EpisodeRewardMean                    -43.0867
LinearFeatureBaseline/ExplainedVariance       0.851938
PolicyExecTime                                0.225921
ProcessExecTime                               0.0311818
TotalEnvSteps                            424028
policy/Entropy                               -1.0291
policy/KL                                     0.00680207
policy/KLBefore                               0
policy/LossAfter                             -0.0153764
policy/LossBefore                             1.48423e-08
policy/Perplexity                             0.357329
policy/dLoss                                  0.0153764
---------------------------------------  ----------------
2021-06-04 13:53:24 | [train_policy] epoch #419 | Obtaining samples for iteration 419...
2021-06-04 13:53:25 | [train_policy] epoch #419 | Logging diagnostics...
2021-06-04 13:53:25 | [train_policy] epoch #419 | Optimizing policy...
2021-06-04 13:53:25 | [train_policy] epoch #419 | Computing loss before
2021-06-04 13:53:25 | [train_policy] epoch #419 | Computing KL before
2021-06-04 13:53:25 | [train_policy] epoch #419 | Optimizing
2021-06-04 13:53:25 | [train_policy] epoch #419 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:25 | [train_policy] epoch #419 | computing loss before
2021-06-04 13:53:25 | [train_policy] epoch #419 | computing gradient
2021-06-04 13:53:25 | [train_policy] epoch #419 | gradient computed
2021-06-04 13:53:25 | [train_policy] epoch #419 | computing descent direction
2021-06-04 13:53:25 | [train_policy] epoch #419 | descent direction computed
2021-06-04 13:53:25 | [train_policy] epoch #419 | backtrack iters: 1
2021-06-04 13:53:25 | [train_policy] epoch #419 | optimization finished
2021-06-04 13:53:25 | [train_policy] epoch #419 | Computing KL after
2021-06-04 13:53:25 | [train_policy] epoch #419 | Computing loss after
2021-06-04 13:53:25 | [train_policy] epoch #419 | Fitting baseline...
2021-06-04 13:53:25 | [train_policy] epoch #419 | Saving snapshot...
2021-06-04 13:53:25 | [train_policy] epoch #419 | Saved
2021-06-04 13:53:25 | [train_policy] epoch #419 | Time 337.36 s
2021-06-04 13:53:25 | [train_policy] epoch #419 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285149
Evaluation/AverageDiscountedReturn          -45.3715
Evaluation/AverageReturn                    -45.3715
Evaluation/CompletionRate                     0
Evaluation/Iteration                        419
Evaluation/MaxReturn                        -28.1599
Evaluation/MinReturn                       -292.534
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         27.6422
Extras/EpisodeRewardMean                    -45.1656
LinearFeatureBaseline/ExplainedVariance       0.284612
PolicyExecTime                                0.236352
ProcessExecTime                               0.0311999
TotalEnvSteps                            425040
policy/Entropy                               -1.02211
policy/KL                                     0.00672245
policy/KLBefore                               0
policy/LossAfter                             -0.0145207
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.359836
policy/dLoss                                  0.0145207
---------------------------------------  ----------------
2021-06-04 13:53:25 | [train_policy] epoch #420 | Obtaining samples for iteration 420...
2021-06-04 13:53:26 | [train_policy] epoch #420 | Logging diagnostics...
2021-06-04 13:53:26 | [train_policy] epoch #420 | Optimizing policy...
2021-06-04 13:53:26 | [train_policy] epoch #420 | Computing loss before
2021-06-04 13:53:26 | [train_policy] epoch #420 | Computing KL before
2021-06-04 13:53:26 | [train_policy] epoch #420 | Optimizing
2021-06-04 13:53:26 | [train_policy] epoch #420 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:26 | [train_policy] epoch #420 | computing loss before
2021-06-04 13:53:26 | [train_policy] epoch #420 | computing gradient
2021-06-04 13:53:26 | [train_policy] epoch #420 | gradient computed
2021-06-04 13:53:26 | [train_policy] epoch #420 | computing descent direction
2021-06-04 13:53:26 | [train_policy] epoch #420 | descent direction computed
2021-06-04 13:53:26 | [train_policy] epoch #420 | backtrack iters: 0
2021-06-04 13:53:26 | [train_policy] epoch #420 | optimization finished
2021-06-04 13:53:26 | [train_policy] epoch #420 | Computing KL after
2021-06-04 13:53:26 | [train_policy] epoch #420 | Computing loss after
2021-06-04 13:53:26 | [train_policy] epoch #420 | Fitting baseline...
2021-06-04 13:53:26 | [train_policy] epoch #420 | Saving snapshot...
2021-06-04 13:53:26 | [train_policy] epoch #420 | Saved
2021-06-04 13:53:26 | [train_policy] epoch #420 | Time 338.15 s
2021-06-04 13:53:26 | [train_policy] epoch #420 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285707
Evaluation/AverageDiscountedReturn          -41.7223
Evaluation/AverageReturn                    -41.7223
Evaluation/CompletionRate                     0
Evaluation/Iteration                        420
Evaluation/MaxReturn                        -29.97
Evaluation/MinReturn                        -63.944
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.78491
Extras/EpisodeRewardMean                    -41.6051
LinearFeatureBaseline/ExplainedVariance       0.593809
PolicyExecTime                                0.229573
ProcessExecTime                               0.0313461
TotalEnvSteps                            426052
policy/Entropy                               -0.980052
policy/KL                                     0.00936722
policy/KLBefore                               0
policy/LossAfter                             -0.060183
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.375292
policy/dLoss                                  0.060183
---------------------------------------  ----------------
2021-06-04 13:53:26 | [train_policy] epoch #421 | Obtaining samples for iteration 421...
2021-06-04 13:53:27 | [train_policy] epoch #421 | Logging diagnostics...
2021-06-04 13:53:27 | [train_policy] epoch #421 | Optimizing policy...
2021-06-04 13:53:27 | [train_policy] epoch #421 | Computing loss before
2021-06-04 13:53:27 | [train_policy] epoch #421 | Computing KL before
2021-06-04 13:53:27 | [train_policy] epoch #421 | Optimizing
2021-06-04 13:53:27 | [train_policy] epoch #421 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:27 | [train_policy] epoch #421 | computing loss before
2021-06-04 13:53:27 | [train_policy] epoch #421 | computing gradient
2021-06-04 13:53:27 | [train_policy] epoch #421 | gradient computed
2021-06-04 13:53:27 | [train_policy] epoch #421 | computing descent direction
2021-06-04 13:53:27 | [train_policy] epoch #421 | descent direction computed
2021-06-04 13:53:27 | [train_policy] epoch #421 | backtrack iters: 1
2021-06-04 13:53:27 | [train_policy] epoch #421 | optimization finished
2021-06-04 13:53:27 | [train_policy] epoch #421 | Computing KL after
2021-06-04 13:53:27 | [train_policy] epoch #421 | Computing loss after
2021-06-04 13:53:27 | [train_policy] epoch #421 | Fitting baseline...
2021-06-04 13:53:27 | [train_policy] epoch #421 | Saving snapshot...
2021-06-04 13:53:27 | [train_policy] epoch #421 | Saved
2021-06-04 13:53:27 | [train_policy] epoch #421 | Time 338.97 s
2021-06-04 13:53:27 | [train_policy] epoch #421 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.289072
Evaluation/AverageDiscountedReturn          -43.723
Evaluation/AverageReturn                    -43.723
Evaluation/CompletionRate                     0
Evaluation/Iteration                        421
Evaluation/MaxReturn                        -29.5245
Evaluation/MinReturn                       -122.52
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.9068
Extras/EpisodeRewardMean                    -43.2722
LinearFeatureBaseline/ExplainedVariance       0.729842
PolicyExecTime                                0.224546
ProcessExecTime                               0.0316858
TotalEnvSteps                            427064
policy/Entropy                               -0.964695
policy/KL                                     0.00654705
policy/KLBefore                               0
policy/LossAfter                             -0.0151382
policy/LossBefore                            -0
policy/Perplexity                             0.381099
policy/dLoss                                  0.0151382
---------------------------------------  ---------------
2021-06-04 13:53:27 | [train_policy] epoch #422 | Obtaining samples for iteration 422...
2021-06-04 13:53:27 | [train_policy] epoch #422 | Logging diagnostics...
2021-06-04 13:53:27 | [train_policy] epoch #422 | Optimizing policy...
2021-06-04 13:53:27 | [train_policy] epoch #422 | Computing loss before
2021-06-04 13:53:27 | [train_policy] epoch #422 | Computing KL before
2021-06-04 13:53:27 | [train_policy] epoch #422 | Optimizing
2021-06-04 13:53:27 | [train_policy] epoch #422 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:27 | [train_policy] epoch #422 | computing loss before
2021-06-04 13:53:27 | [train_policy] epoch #422 | computing gradient
2021-06-04 13:53:27 | [train_policy] epoch #422 | gradient computed
2021-06-04 13:53:27 | [train_policy] epoch #422 | computing descent direction
2021-06-04 13:53:27 | [train_policy] epoch #422 | descent direction computed
2021-06-04 13:53:27 | [train_policy] epoch #422 | backtrack iters: 1
2021-06-04 13:53:27 | [train_policy] epoch #422 | optimization finished
2021-06-04 13:53:27 | [train_policy] epoch #422 | Computing KL after
2021-06-04 13:53:27 | [train_policy] epoch #422 | Computing loss after
2021-06-04 13:53:27 | [train_policy] epoch #422 | Fitting baseline...
2021-06-04 13:53:28 | [train_policy] epoch #422 | Saving snapshot...
2021-06-04 13:53:28 | [train_policy] epoch #422 | Saved
2021-06-04 13:53:28 | [train_policy] epoch #422 | Time 339.76 s
2021-06-04 13:53:28 | [train_policy] epoch #422 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285614
Evaluation/AverageDiscountedReturn          -44.3552
Evaluation/AverageReturn                    -44.3552
Evaluation/CompletionRate                     0
Evaluation/Iteration                        422
Evaluation/MaxReturn                        -31.0715
Evaluation/MinReturn                        -88.308
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1496
Extras/EpisodeRewardMean                    -44.5498
LinearFeatureBaseline/ExplainedVariance       0.856961
PolicyExecTime                                0.212602
ProcessExecTime                               0.0311985
TotalEnvSteps                            428076
policy/Entropy                               -0.982312
policy/KL                                     0.00642654
policy/KLBefore                               0
policy/LossAfter                             -0.0166715
policy/LossBefore                            -5.56585e-09
policy/Perplexity                             0.374444
policy/dLoss                                  0.0166714
---------------------------------------  ----------------
2021-06-04 13:53:28 | [train_policy] epoch #423 | Obtaining samples for iteration 423...
2021-06-04 13:53:28 | [train_policy] epoch #423 | Logging diagnostics...
2021-06-04 13:53:28 | [train_policy] epoch #423 | Optimizing policy...
2021-06-04 13:53:28 | [train_policy] epoch #423 | Computing loss before
2021-06-04 13:53:28 | [train_policy] epoch #423 | Computing KL before
2021-06-04 13:53:28 | [train_policy] epoch #423 | Optimizing
2021-06-04 13:53:28 | [train_policy] epoch #423 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:28 | [train_policy] epoch #423 | computing loss before
2021-06-04 13:53:28 | [train_policy] epoch #423 | computing gradient
2021-06-04 13:53:28 | [train_policy] epoch #423 | gradient computed
2021-06-04 13:53:28 | [train_policy] epoch #423 | computing descent direction
2021-06-04 13:53:28 | [train_policy] epoch #423 | descent direction computed
2021-06-04 13:53:28 | [train_policy] epoch #423 | backtrack iters: 1
2021-06-04 13:53:28 | [train_policy] epoch #423 | optimization finished
2021-06-04 13:53:28 | [train_policy] epoch #423 | Computing KL after
2021-06-04 13:53:28 | [train_policy] epoch #423 | Computing loss after
2021-06-04 13:53:28 | [train_policy] epoch #423 | Fitting baseline...
2021-06-04 13:53:28 | [train_policy] epoch #423 | Saving snapshot...
2021-06-04 13:53:28 | [train_policy] epoch #423 | Saved
2021-06-04 13:53:28 | [train_policy] epoch #423 | Time 340.56 s
2021-06-04 13:53:28 | [train_policy] epoch #423 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286751
Evaluation/AverageDiscountedReturn          -43.1291
Evaluation/AverageReturn                    -43.1291
Evaluation/CompletionRate                     0
Evaluation/Iteration                        423
Evaluation/MaxReturn                        -29.3144
Evaluation/MinReturn                        -95.291
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.4874
Extras/EpisodeRewardMean                    -42.7853
LinearFeatureBaseline/ExplainedVariance       0.814535
PolicyExecTime                                0.227811
ProcessExecTime                               0.0312724
TotalEnvSteps                            429088
policy/Entropy                               -0.999031
policy/KL                                     0.00645779
policy/KLBefore                               0
policy/LossAfter                             -0.0183423
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.368236
policy/dLoss                                  0.0183423
---------------------------------------  ----------------
2021-06-04 13:53:28 | [train_policy] epoch #424 | Obtaining samples for iteration 424...
2021-06-04 13:53:29 | [train_policy] epoch #424 | Logging diagnostics...
2021-06-04 13:53:29 | [train_policy] epoch #424 | Optimizing policy...
2021-06-04 13:53:29 | [train_policy] epoch #424 | Computing loss before
2021-06-04 13:53:29 | [train_policy] epoch #424 | Computing KL before
2021-06-04 13:53:29 | [train_policy] epoch #424 | Optimizing
2021-06-04 13:53:29 | [train_policy] epoch #424 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:29 | [train_policy] epoch #424 | computing loss before
2021-06-04 13:53:29 | [train_policy] epoch #424 | computing gradient
2021-06-04 13:53:29 | [train_policy] epoch #424 | gradient computed
2021-06-04 13:53:29 | [train_policy] epoch #424 | computing descent direction
2021-06-04 13:53:29 | [train_policy] epoch #424 | descent direction computed
2021-06-04 13:53:29 | [train_policy] epoch #424 | backtrack iters: 1
2021-06-04 13:53:29 | [train_policy] epoch #424 | optimization finished
2021-06-04 13:53:29 | [train_policy] epoch #424 | Computing KL after
2021-06-04 13:53:29 | [train_policy] epoch #424 | Computing loss after
2021-06-04 13:53:29 | [train_policy] epoch #424 | Fitting baseline...
2021-06-04 13:53:29 | [train_policy] epoch #424 | Saving snapshot...
2021-06-04 13:53:29 | [train_policy] epoch #424 | Saved
2021-06-04 13:53:29 | [train_policy] epoch #424 | Time 341.37 s
2021-06-04 13:53:29 | [train_policy] epoch #424 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285186
Evaluation/AverageDiscountedReturn          -42.3257
Evaluation/AverageReturn                    -42.3257
Evaluation/CompletionRate                     0
Evaluation/Iteration                        424
Evaluation/MaxReturn                        -29.709
Evaluation/MinReturn                        -63.0795
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.24238
Extras/EpisodeRewardMean                    -42.2467
LinearFeatureBaseline/ExplainedVariance       0.915167
PolicyExecTime                                0.218876
ProcessExecTime                               0.0312352
TotalEnvSteps                            430100
policy/Entropy                               -1.03593
policy/KL                                     0.0064236
policy/KLBefore                               0
policy/LossAfter                             -0.0145959
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.354897
policy/dLoss                                  0.0145959
---------------------------------------  ----------------
2021-06-04 13:53:29 | [train_policy] epoch #425 | Obtaining samples for iteration 425...
2021-06-04 13:53:30 | [train_policy] epoch #425 | Logging diagnostics...
2021-06-04 13:53:30 | [train_policy] epoch #425 | Optimizing policy...
2021-06-04 13:53:30 | [train_policy] epoch #425 | Computing loss before
2021-06-04 13:53:30 | [train_policy] epoch #425 | Computing KL before
2021-06-04 13:53:30 | [train_policy] epoch #425 | Optimizing
2021-06-04 13:53:30 | [train_policy] epoch #425 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:30 | [train_policy] epoch #425 | computing loss before
2021-06-04 13:53:30 | [train_policy] epoch #425 | computing gradient
2021-06-04 13:53:30 | [train_policy] epoch #425 | gradient computed
2021-06-04 13:53:30 | [train_policy] epoch #425 | computing descent direction
2021-06-04 13:53:30 | [train_policy] epoch #425 | descent direction computed
2021-06-04 13:53:30 | [train_policy] epoch #425 | backtrack iters: 1
2021-06-04 13:53:30 | [train_policy] epoch #425 | optimization finished
2021-06-04 13:53:30 | [train_policy] epoch #425 | Computing KL after
2021-06-04 13:53:30 | [train_policy] epoch #425 | Computing loss after
2021-06-04 13:53:30 | [train_policy] epoch #425 | Fitting baseline...
2021-06-04 13:53:30 | [train_policy] epoch #425 | Saving snapshot...
2021-06-04 13:53:30 | [train_policy] epoch #425 | Saved
2021-06-04 13:53:30 | [train_policy] epoch #425 | Time 342.19 s
2021-06-04 13:53:30 | [train_policy] epoch #425 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.295458
Evaluation/AverageDiscountedReturn          -42.129
Evaluation/AverageReturn                    -42.129
Evaluation/CompletionRate                     0
Evaluation/Iteration                        425
Evaluation/MaxReturn                        -28.1064
Evaluation/MinReturn                       -102.096
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.93554
Extras/EpisodeRewardMean                    -42.5355
LinearFeatureBaseline/ExplainedVariance       0.81742
PolicyExecTime                                0.237069
ProcessExecTime                               0.0329273
TotalEnvSteps                            431112
policy/Entropy                               -1.05114
policy/KL                                     0.00649692
policy/KLBefore                               0
policy/LossAfter                             -0.0151235
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.349539
policy/dLoss                                  0.0151235
---------------------------------------  ----------------
2021-06-04 13:53:30 | [train_policy] epoch #426 | Obtaining samples for iteration 426...
2021-06-04 13:53:31 | [train_policy] epoch #426 | Logging diagnostics...
2021-06-04 13:53:31 | [train_policy] epoch #426 | Optimizing policy...
2021-06-04 13:53:31 | [train_policy] epoch #426 | Computing loss before
2021-06-04 13:53:31 | [train_policy] epoch #426 | Computing KL before
2021-06-04 13:53:31 | [train_policy] epoch #426 | Optimizing
2021-06-04 13:53:31 | [train_policy] epoch #426 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:31 | [train_policy] epoch #426 | computing loss before
2021-06-04 13:53:31 | [train_policy] epoch #426 | computing gradient
2021-06-04 13:53:31 | [train_policy] epoch #426 | gradient computed
2021-06-04 13:53:31 | [train_policy] epoch #426 | computing descent direction
2021-06-04 13:53:31 | [train_policy] epoch #426 | descent direction computed
2021-06-04 13:53:31 | [train_policy] epoch #426 | backtrack iters: 0
2021-06-04 13:53:31 | [train_policy] epoch #426 | optimization finished
2021-06-04 13:53:31 | [train_policy] epoch #426 | Computing KL after
2021-06-04 13:53:31 | [train_policy] epoch #426 | Computing loss after
2021-06-04 13:53:31 | [train_policy] epoch #426 | Fitting baseline...
2021-06-04 13:53:31 | [train_policy] epoch #426 | Saving snapshot...
2021-06-04 13:53:31 | [train_policy] epoch #426 | Saved
2021-06-04 13:53:31 | [train_policy] epoch #426 | Time 342.98 s
2021-06-04 13:53:31 | [train_policy] epoch #426 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.287501
Evaluation/AverageDiscountedReturn          -42.8354
Evaluation/AverageReturn                    -42.8354
Evaluation/CompletionRate                     0
Evaluation/Iteration                        426
Evaluation/MaxReturn                        -30.012
Evaluation/MinReturn                        -92.0585
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1613
Extras/EpisodeRewardMean                    -43.3264
LinearFeatureBaseline/ExplainedVariance       0.839593
PolicyExecTime                                0.214051
ProcessExecTime                               0.0314584
TotalEnvSteps                            432124
policy/Entropy                               -1.03789
policy/KL                                     0.00971779
policy/KLBefore                               0
policy/LossAfter                             -0.0210329
policy/LossBefore                            -2.23812e-09
policy/Perplexity                             0.354201
policy/dLoss                                  0.0210329
---------------------------------------  ----------------
2021-06-04 13:53:31 | [train_policy] epoch #427 | Obtaining samples for iteration 427...
2021-06-04 13:53:31 | [train_policy] epoch #427 | Logging diagnostics...
2021-06-04 13:53:31 | [train_policy] epoch #427 | Optimizing policy...
2021-06-04 13:53:31 | [train_policy] epoch #427 | Computing loss before
2021-06-04 13:53:31 | [train_policy] epoch #427 | Computing KL before
2021-06-04 13:53:31 | [train_policy] epoch #427 | Optimizing
2021-06-04 13:53:31 | [train_policy] epoch #427 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:31 | [train_policy] epoch #427 | computing loss before
2021-06-04 13:53:31 | [train_policy] epoch #427 | computing gradient
2021-06-04 13:53:31 | [train_policy] epoch #427 | gradient computed
2021-06-04 13:53:31 | [train_policy] epoch #427 | computing descent direction
2021-06-04 13:53:31 | [train_policy] epoch #427 | descent direction computed
2021-06-04 13:53:32 | [train_policy] epoch #427 | backtrack iters: 1
2021-06-04 13:53:32 | [train_policy] epoch #427 | optimization finished
2021-06-04 13:53:32 | [train_policy] epoch #427 | Computing KL after
2021-06-04 13:53:32 | [train_policy] epoch #427 | Computing loss after
2021-06-04 13:53:32 | [train_policy] epoch #427 | Fitting baseline...
2021-06-04 13:53:32 | [train_policy] epoch #427 | Saving snapshot...
2021-06-04 13:53:32 | [train_policy] epoch #427 | Saved
2021-06-04 13:53:32 | [train_policy] epoch #427 | Time 343.78 s
2021-06-04 13:53:32 | [train_policy] epoch #427 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28483
Evaluation/AverageDiscountedReturn          -43.5292
Evaluation/AverageReturn                    -43.5292
Evaluation/CompletionRate                     0
Evaluation/Iteration                        427
Evaluation/MaxReturn                        -29.2665
Evaluation/MinReturn                        -85.2703
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3134
Extras/EpisodeRewardMean                    -43.8315
LinearFeatureBaseline/ExplainedVariance       0.874912
PolicyExecTime                                0.227898
ProcessExecTime                               0.0312285
TotalEnvSteps                            433136
policy/Entropy                               -1.07422
policy/KL                                     0.006545
policy/KLBefore                               0
policy/LossAfter                             -0.0157077
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.341564
policy/dLoss                                  0.0157077
---------------------------------------  ----------------
2021-06-04 13:53:32 | [train_policy] epoch #428 | Obtaining samples for iteration 428...
2021-06-04 13:53:32 | [train_policy] epoch #428 | Logging diagnostics...
2021-06-04 13:53:32 | [train_policy] epoch #428 | Optimizing policy...
2021-06-04 13:53:32 | [train_policy] epoch #428 | Computing loss before
2021-06-04 13:53:32 | [train_policy] epoch #428 | Computing KL before
2021-06-04 13:53:32 | [train_policy] epoch #428 | Optimizing
2021-06-04 13:53:32 | [train_policy] epoch #428 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:32 | [train_policy] epoch #428 | computing loss before
2021-06-04 13:53:32 | [train_policy] epoch #428 | computing gradient
2021-06-04 13:53:32 | [train_policy] epoch #428 | gradient computed
2021-06-04 13:53:32 | [train_policy] epoch #428 | computing descent direction
2021-06-04 13:53:32 | [train_policy] epoch #428 | descent direction computed
2021-06-04 13:53:32 | [train_policy] epoch #428 | backtrack iters: 1
2021-06-04 13:53:32 | [train_policy] epoch #428 | optimization finished
2021-06-04 13:53:32 | [train_policy] epoch #428 | Computing KL after
2021-06-04 13:53:32 | [train_policy] epoch #428 | Computing loss after
2021-06-04 13:53:32 | [train_policy] epoch #428 | Fitting baseline...
2021-06-04 13:53:32 | [train_policy] epoch #428 | Saving snapshot...
2021-06-04 13:53:32 | [train_policy] epoch #428 | Saved
2021-06-04 13:53:32 | [train_policy] epoch #428 | Time 344.58 s
2021-06-04 13:53:32 | [train_policy] epoch #428 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.286122
Evaluation/AverageDiscountedReturn          -42.9931
Evaluation/AverageReturn                    -42.9931
Evaluation/CompletionRate                     0
Evaluation/Iteration                        428
Evaluation/MaxReturn                        -28.8887
Evaluation/MinReturn                        -98.7501
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.11997
Extras/EpisodeRewardMean                    -42.9317
LinearFeatureBaseline/ExplainedVariance       0.839528
PolicyExecTime                                0.227922
ProcessExecTime                               0.0313423
TotalEnvSteps                            434148
policy/Entropy                               -1.08663
policy/KL                                     0.0064636
policy/KLBefore                               0
policy/LossAfter                             -0.0152657
policy/LossBefore                             1.0366e-08
policy/Perplexity                             0.337352
policy/dLoss                                  0.0152657
---------------------------------------  ---------------
2021-06-04 13:53:32 | [train_policy] epoch #429 | Obtaining samples for iteration 429...
2021-06-04 13:53:33 | [train_policy] epoch #429 | Logging diagnostics...
2021-06-04 13:53:33 | [train_policy] epoch #429 | Optimizing policy...
2021-06-04 13:53:33 | [train_policy] epoch #429 | Computing loss before
2021-06-04 13:53:33 | [train_policy] epoch #429 | Computing KL before
2021-06-04 13:53:33 | [train_policy] epoch #429 | Optimizing
2021-06-04 13:53:33 | [train_policy] epoch #429 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:33 | [train_policy] epoch #429 | computing loss before
2021-06-04 13:53:33 | [train_policy] epoch #429 | computing gradient
2021-06-04 13:53:33 | [train_policy] epoch #429 | gradient computed
2021-06-04 13:53:33 | [train_policy] epoch #429 | computing descent direction
2021-06-04 13:53:33 | [train_policy] epoch #429 | descent direction computed
2021-06-04 13:53:33 | [train_policy] epoch #429 | backtrack iters: 1
2021-06-04 13:53:33 | [train_policy] epoch #429 | optimization finished
2021-06-04 13:53:33 | [train_policy] epoch #429 | Computing KL after
2021-06-04 13:53:33 | [train_policy] epoch #429 | Computing loss after
2021-06-04 13:53:33 | [train_policy] epoch #429 | Fitting baseline...
2021-06-04 13:53:33 | [train_policy] epoch #429 | Saving snapshot...
2021-06-04 13:53:33 | [train_policy] epoch #429 | Saved
2021-06-04 13:53:33 | [train_policy] epoch #429 | Time 345.37 s
2021-06-04 13:53:33 | [train_policy] epoch #429 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284091
Evaluation/AverageDiscountedReturn          -44.1469
Evaluation/AverageReturn                    -44.1469
Evaluation/CompletionRate                     0
Evaluation/Iteration                        429
Evaluation/MaxReturn                        -29.9775
Evaluation/MinReturn                        -92.9816
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1977
Extras/EpisodeRewardMean                    -44.0131
LinearFeatureBaseline/ExplainedVariance       0.866855
PolicyExecTime                                0.212981
ProcessExecTime                               0.0310473
TotalEnvSteps                            435160
policy/Entropy                               -1.11341
policy/KL                                     0.00657017
policy/KLBefore                               0
policy/LossAfter                             -0.0158699
policy/LossBefore                             1.76694e-09
policy/Perplexity                             0.328436
policy/dLoss                                  0.0158699
---------------------------------------  ----------------
2021-06-04 13:53:33 | [train_policy] epoch #430 | Obtaining samples for iteration 430...
2021-06-04 13:53:34 | [train_policy] epoch #430 | Logging diagnostics...
2021-06-04 13:53:34 | [train_policy] epoch #430 | Optimizing policy...
2021-06-04 13:53:34 | [train_policy] epoch #430 | Computing loss before
2021-06-04 13:53:34 | [train_policy] epoch #430 | Computing KL before
2021-06-04 13:53:34 | [train_policy] epoch #430 | Optimizing
2021-06-04 13:53:34 | [train_policy] epoch #430 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:34 | [train_policy] epoch #430 | computing loss before
2021-06-04 13:53:34 | [train_policy] epoch #430 | computing gradient
2021-06-04 13:53:34 | [train_policy] epoch #430 | gradient computed
2021-06-04 13:53:34 | [train_policy] epoch #430 | computing descent direction
2021-06-04 13:53:34 | [train_policy] epoch #430 | descent direction computed
2021-06-04 13:53:34 | [train_policy] epoch #430 | backtrack iters: 1
2021-06-04 13:53:34 | [train_policy] epoch #430 | optimization finished
2021-06-04 13:53:34 | [train_policy] epoch #430 | Computing KL after
2021-06-04 13:53:34 | [train_policy] epoch #430 | Computing loss after
2021-06-04 13:53:34 | [train_policy] epoch #430 | Fitting baseline...
2021-06-04 13:53:34 | [train_policy] epoch #430 | Saving snapshot...
2021-06-04 13:53:34 | [train_policy] epoch #430 | Saved
2021-06-04 13:53:34 | [train_policy] epoch #430 | Time 346.17 s
2021-06-04 13:53:34 | [train_policy] epoch #430 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287995
Evaluation/AverageDiscountedReturn          -43.5656
Evaluation/AverageReturn                    -43.5656
Evaluation/CompletionRate                     0
Evaluation/Iteration                        430
Evaluation/MaxReturn                        -29.1273
Evaluation/MinReturn                        -62.9121
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.93314
Extras/EpisodeRewardMean                    -43.2232
LinearFeatureBaseline/ExplainedVariance       0.904573
PolicyExecTime                                0.228531
ProcessExecTime                               0.0315118
TotalEnvSteps                            436172
policy/Entropy                               -1.14582
policy/KL                                     0.00657632
policy/KLBefore                               0
policy/LossAfter                             -0.0167214
policy/LossBefore                             2.63862e-08
policy/Perplexity                             0.317962
policy/dLoss                                  0.0167214
---------------------------------------  ----------------
2021-06-04 13:53:34 | [train_policy] epoch #431 | Obtaining samples for iteration 431...
2021-06-04 13:53:35 | [train_policy] epoch #431 | Logging diagnostics...
2021-06-04 13:53:35 | [train_policy] epoch #431 | Optimizing policy...
2021-06-04 13:53:35 | [train_policy] epoch #431 | Computing loss before
2021-06-04 13:53:35 | [train_policy] epoch #431 | Computing KL before
2021-06-04 13:53:35 | [train_policy] epoch #431 | Optimizing
2021-06-04 13:53:35 | [train_policy] epoch #431 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:35 | [train_policy] epoch #431 | computing loss before
2021-06-04 13:53:35 | [train_policy] epoch #431 | computing gradient
2021-06-04 13:53:35 | [train_policy] epoch #431 | gradient computed
2021-06-04 13:53:35 | [train_policy] epoch #431 | computing descent direction
2021-06-04 13:53:35 | [train_policy] epoch #431 | descent direction computed
2021-06-04 13:53:35 | [train_policy] epoch #431 | backtrack iters: 0
2021-06-04 13:53:35 | [train_policy] epoch #431 | optimization finished
2021-06-04 13:53:35 | [train_policy] epoch #431 | Computing KL after
2021-06-04 13:53:35 | [train_policy] epoch #431 | Computing loss after
2021-06-04 13:53:35 | [train_policy] epoch #431 | Fitting baseline...
2021-06-04 13:53:35 | [train_policy] epoch #431 | Saving snapshot...
2021-06-04 13:53:35 | [train_policy] epoch #431 | Saved
2021-06-04 13:53:35 | [train_policy] epoch #431 | Time 346.96 s
2021-06-04 13:53:35 | [train_policy] epoch #431 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284626
Evaluation/AverageDiscountedReturn          -41.5603
Evaluation/AverageReturn                    -41.5603
Evaluation/CompletionRate                     0
Evaluation/Iteration                        431
Evaluation/MaxReturn                        -31.7479
Evaluation/MinReturn                        -63.1538
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.7724
Extras/EpisodeRewardMean                    -41.8603
LinearFeatureBaseline/ExplainedVariance       0.863285
PolicyExecTime                                0.212502
ProcessExecTime                               0.031086
TotalEnvSteps                            437184
policy/Entropy                               -1.14635
policy/KL                                     0.0098052
policy/KLBefore                               0
policy/LossAfter                             -0.0163675
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.317796
policy/dLoss                                  0.0163675
---------------------------------------  ----------------
2021-06-04 13:53:35 | [train_policy] epoch #432 | Obtaining samples for iteration 432...
2021-06-04 13:53:35 | [train_policy] epoch #432 | Logging diagnostics...
2021-06-04 13:53:35 | [train_policy] epoch #432 | Optimizing policy...
2021-06-04 13:53:35 | [train_policy] epoch #432 | Computing loss before
2021-06-04 13:53:35 | [train_policy] epoch #432 | Computing KL before
2021-06-04 13:53:35 | [train_policy] epoch #432 | Optimizing
2021-06-04 13:53:35 | [train_policy] epoch #432 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:35 | [train_policy] epoch #432 | computing loss before
2021-06-04 13:53:35 | [train_policy] epoch #432 | computing gradient
2021-06-04 13:53:35 | [train_policy] epoch #432 | gradient computed
2021-06-04 13:53:35 | [train_policy] epoch #432 | computing descent direction
2021-06-04 13:53:35 | [train_policy] epoch #432 | descent direction computed
2021-06-04 13:53:35 | [train_policy] epoch #432 | backtrack iters: 1
2021-06-04 13:53:35 | [train_policy] epoch #432 | optimization finished
2021-06-04 13:53:35 | [train_policy] epoch #432 | Computing KL after
2021-06-04 13:53:35 | [train_policy] epoch #432 | Computing loss after
2021-06-04 13:53:36 | [train_policy] epoch #432 | Fitting baseline...
2021-06-04 13:53:36 | [train_policy] epoch #432 | Saving snapshot...
2021-06-04 13:53:36 | [train_policy] epoch #432 | Saved
2021-06-04 13:53:36 | [train_policy] epoch #432 | Time 347.77 s
2021-06-04 13:53:36 | [train_policy] epoch #432 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287103
Evaluation/AverageDiscountedReturn          -43.4239
Evaluation/AverageReturn                    -43.4239
Evaluation/CompletionRate                     0
Evaluation/Iteration                        432
Evaluation/MaxReturn                        -31.2541
Evaluation/MinReturn                        -64.1794
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.38391
Extras/EpisodeRewardMean                    -43.3932
LinearFeatureBaseline/ExplainedVariance       0.918234
PolicyExecTime                                0.23596
ProcessExecTime                               0.0313828
TotalEnvSteps                            438196
policy/Entropy                               -1.18138
policy/KL                                     0.00651629
policy/KLBefore                               0
policy/LossAfter                             -0.0147421
policy/LossBefore                            -9.42366e-09
policy/Perplexity                             0.306854
policy/dLoss                                  0.0147421
---------------------------------------  ----------------
2021-06-04 13:53:36 | [train_policy] epoch #433 | Obtaining samples for iteration 433...
2021-06-04 13:53:36 | [train_policy] epoch #433 | Logging diagnostics...
2021-06-04 13:53:36 | [train_policy] epoch #433 | Optimizing policy...
2021-06-04 13:53:36 | [train_policy] epoch #433 | Computing loss before
2021-06-04 13:53:36 | [train_policy] epoch #433 | Computing KL before
2021-06-04 13:53:36 | [train_policy] epoch #433 | Optimizing
2021-06-04 13:53:36 | [train_policy] epoch #433 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:36 | [train_policy] epoch #433 | computing loss before
2021-06-04 13:53:36 | [train_policy] epoch #433 | computing gradient
2021-06-04 13:53:36 | [train_policy] epoch #433 | gradient computed
2021-06-04 13:53:36 | [train_policy] epoch #433 | computing descent direction
2021-06-04 13:53:36 | [train_policy] epoch #433 | descent direction computed
2021-06-04 13:53:36 | [train_policy] epoch #433 | backtrack iters: 0
2021-06-04 13:53:36 | [train_policy] epoch #433 | optimization finished
2021-06-04 13:53:36 | [train_policy] epoch #433 | Computing KL after
2021-06-04 13:53:36 | [train_policy] epoch #433 | Computing loss after
2021-06-04 13:53:36 | [train_policy] epoch #433 | Fitting baseline...
2021-06-04 13:53:36 | [train_policy] epoch #433 | Saving snapshot...
2021-06-04 13:53:36 | [train_policy] epoch #433 | Saved
2021-06-04 13:53:36 | [train_policy] epoch #433 | Time 348.55 s
2021-06-04 13:53:36 | [train_policy] epoch #433 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.283485
Evaluation/AverageDiscountedReturn          -42.1202
Evaluation/AverageReturn                    -42.1202
Evaluation/CompletionRate                     0
Evaluation/Iteration                        433
Evaluation/MaxReturn                        -31.2727
Evaluation/MinReturn                        -87.2919
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.58596
Extras/EpisodeRewardMean                    -42.2375
LinearFeatureBaseline/ExplainedVariance       0.880819
PolicyExecTime                                0.212564
ProcessExecTime                               0.0310667
TotalEnvSteps                            439208
policy/Entropy                               -1.1491
policy/KL                                     0.0094137
policy/KLBefore                               0
policy/LossAfter                             -0.0203767
policy/LossBefore                             1.27219e-08
policy/Perplexity                             0.316923
policy/dLoss                                  0.0203768
---------------------------------------  ----------------
2021-06-04 13:53:36 | [train_policy] epoch #434 | Obtaining samples for iteration 434...
2021-06-04 13:53:37 | [train_policy] epoch #434 | Logging diagnostics...
2021-06-04 13:53:37 | [train_policy] epoch #434 | Optimizing policy...
2021-06-04 13:53:37 | [train_policy] epoch #434 | Computing loss before
2021-06-04 13:53:37 | [train_policy] epoch #434 | Computing KL before
2021-06-04 13:53:37 | [train_policy] epoch #434 | Optimizing
2021-06-04 13:53:37 | [train_policy] epoch #434 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:37 | [train_policy] epoch #434 | computing loss before
2021-06-04 13:53:37 | [train_policy] epoch #434 | computing gradient
2021-06-04 13:53:37 | [train_policy] epoch #434 | gradient computed
2021-06-04 13:53:37 | [train_policy] epoch #434 | computing descent direction
2021-06-04 13:53:37 | [train_policy] epoch #434 | descent direction computed
2021-06-04 13:53:37 | [train_policy] epoch #434 | backtrack iters: 1
2021-06-04 13:53:37 | [train_policy] epoch #434 | optimization finished
2021-06-04 13:53:37 | [train_policy] epoch #434 | Computing KL after
2021-06-04 13:53:37 | [train_policy] epoch #434 | Computing loss after
2021-06-04 13:53:37 | [train_policy] epoch #434 | Fitting baseline...
2021-06-04 13:53:37 | [train_policy] epoch #434 | Saving snapshot...
2021-06-04 13:53:37 | [train_policy] epoch #434 | Saved
2021-06-04 13:53:37 | [train_policy] epoch #434 | Time 349.35 s
2021-06-04 13:53:37 | [train_policy] epoch #434 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283834
Evaluation/AverageDiscountedReturn          -42.0215
Evaluation/AverageReturn                    -42.0215
Evaluation/CompletionRate                     0
Evaluation/Iteration                        434
Evaluation/MaxReturn                        -30.1003
Evaluation/MinReturn                        -60.0842
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.88215
Extras/EpisodeRewardMean                    -42.0073
LinearFeatureBaseline/ExplainedVariance       0.912128
PolicyExecTime                                0.229377
ProcessExecTime                               0.0310936
TotalEnvSteps                            440220
policy/Entropy                               -1.19493
policy/KL                                     0.00669754
policy/KLBefore                               0
policy/LossAfter                             -0.0196976
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.302725
policy/dLoss                                  0.0196976
---------------------------------------  ----------------
2021-06-04 13:53:37 | [train_policy] epoch #435 | Obtaining samples for iteration 435...
2021-06-04 13:53:38 | [train_policy] epoch #435 | Logging diagnostics...
2021-06-04 13:53:38 | [train_policy] epoch #435 | Optimizing policy...
2021-06-04 13:53:38 | [train_policy] epoch #435 | Computing loss before
2021-06-04 13:53:38 | [train_policy] epoch #435 | Computing KL before
2021-06-04 13:53:38 | [train_policy] epoch #435 | Optimizing
2021-06-04 13:53:38 | [train_policy] epoch #435 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:38 | [train_policy] epoch #435 | computing loss before
2021-06-04 13:53:38 | [train_policy] epoch #435 | computing gradient
2021-06-04 13:53:38 | [train_policy] epoch #435 | gradient computed
2021-06-04 13:53:38 | [train_policy] epoch #435 | computing descent direction
2021-06-04 13:53:38 | [train_policy] epoch #435 | descent direction computed
2021-06-04 13:53:38 | [train_policy] epoch #435 | backtrack iters: 1
2021-06-04 13:53:38 | [train_policy] epoch #435 | optimization finished
2021-06-04 13:53:38 | [train_policy] epoch #435 | Computing KL after
2021-06-04 13:53:38 | [train_policy] epoch #435 | Computing loss after
2021-06-04 13:53:38 | [train_policy] epoch #435 | Fitting baseline...
2021-06-04 13:53:38 | [train_policy] epoch #435 | Saving snapshot...
2021-06-04 13:53:38 | [train_policy] epoch #435 | Saved
2021-06-04 13:53:38 | [train_policy] epoch #435 | Time 350.16 s
2021-06-04 13:53:38 | [train_policy] epoch #435 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28951
Evaluation/AverageDiscountedReturn          -41.7358
Evaluation/AverageReturn                    -41.7358
Evaluation/CompletionRate                     0
Evaluation/Iteration                        435
Evaluation/MaxReturn                        -29.2091
Evaluation/MinReturn                        -92.7436
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2286
Extras/EpisodeRewardMean                    -42.1855
LinearFeatureBaseline/ExplainedVariance       0.801658
PolicyExecTime                                0.220345
ProcessExecTime                               0.031451
TotalEnvSteps                            441232
policy/Entropy                               -1.19584
policy/KL                                     0.00644293
policy/KLBefore                               0
policy/LossAfter                             -0.0274068
policy/LossBefore                             2.09676e-08
policy/Perplexity                             0.30245
policy/dLoss                                  0.0274068
---------------------------------------  ----------------
2021-06-04 13:53:38 | [train_policy] epoch #436 | Obtaining samples for iteration 436...
2021-06-04 13:53:39 | [train_policy] epoch #436 | Logging diagnostics...
2021-06-04 13:53:39 | [train_policy] epoch #436 | Optimizing policy...
2021-06-04 13:53:39 | [train_policy] epoch #436 | Computing loss before
2021-06-04 13:53:39 | [train_policy] epoch #436 | Computing KL before
2021-06-04 13:53:39 | [train_policy] epoch #436 | Optimizing
2021-06-04 13:53:39 | [train_policy] epoch #436 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:39 | [train_policy] epoch #436 | computing loss before
2021-06-04 13:53:39 | [train_policy] epoch #436 | computing gradient
2021-06-04 13:53:39 | [train_policy] epoch #436 | gradient computed
2021-06-04 13:53:39 | [train_policy] epoch #436 | computing descent direction
2021-06-04 13:53:39 | [train_policy] epoch #436 | descent direction computed
2021-06-04 13:53:39 | [train_policy] epoch #436 | backtrack iters: 1
2021-06-04 13:53:39 | [train_policy] epoch #436 | optimization finished
2021-06-04 13:53:39 | [train_policy] epoch #436 | Computing KL after
2021-06-04 13:53:39 | [train_policy] epoch #436 | Computing loss after
2021-06-04 13:53:39 | [train_policy] epoch #436 | Fitting baseline...
2021-06-04 13:53:39 | [train_policy] epoch #436 | Saving snapshot...
2021-06-04 13:53:39 | [train_policy] epoch #436 | Saved
2021-06-04 13:53:39 | [train_policy] epoch #436 | Time 350.97 s
2021-06-04 13:53:39 | [train_policy] epoch #436 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.289088
Evaluation/AverageDiscountedReturn          -42.4242
Evaluation/AverageReturn                    -42.4242
Evaluation/CompletionRate                     0
Evaluation/Iteration                        436
Evaluation/MaxReturn                        -30.6574
Evaluation/MinReturn                        -88.055
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.87748
Extras/EpisodeRewardMean                    -43.02
LinearFeatureBaseline/ExplainedVariance       0.872372
PolicyExecTime                                0.231233
ProcessExecTime                               0.0316341
TotalEnvSteps                            442244
policy/Entropy                               -1.21004
policy/KL                                     0.00650238
policy/KLBefore                               0
policy/LossAfter                             -0.0150353
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.298184
policy/dLoss                                  0.0150353
---------------------------------------  ----------------
2021-06-04 13:53:39 | [train_policy] epoch #437 | Obtaining samples for iteration 437...
2021-06-04 13:53:39 | [train_policy] epoch #437 | Logging diagnostics...
2021-06-04 13:53:39 | [train_policy] epoch #437 | Optimizing policy...
2021-06-04 13:53:39 | [train_policy] epoch #437 | Computing loss before
2021-06-04 13:53:39 | [train_policy] epoch #437 | Computing KL before
2021-06-04 13:53:39 | [train_policy] epoch #437 | Optimizing
2021-06-04 13:53:39 | [train_policy] epoch #437 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:39 | [train_policy] epoch #437 | computing loss before
2021-06-04 13:53:39 | [train_policy] epoch #437 | computing gradient
2021-06-04 13:53:39 | [train_policy] epoch #437 | gradient computed
2021-06-04 13:53:39 | [train_policy] epoch #437 | computing descent direction
2021-06-04 13:53:39 | [train_policy] epoch #437 | descent direction computed
2021-06-04 13:53:39 | [train_policy] epoch #437 | backtrack iters: 0
2021-06-04 13:53:39 | [train_policy] epoch #437 | optimization finished
2021-06-04 13:53:39 | [train_policy] epoch #437 | Computing KL after
2021-06-04 13:53:39 | [train_policy] epoch #437 | Computing loss after
2021-06-04 13:53:40 | [train_policy] epoch #437 | Fitting baseline...
2021-06-04 13:53:40 | [train_policy] epoch #437 | Saving snapshot...
2021-06-04 13:53:40 | [train_policy] epoch #437 | Saved
2021-06-04 13:53:40 | [train_policy] epoch #437 | Time 351.76 s
2021-06-04 13:53:40 | [train_policy] epoch #437 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.284532
Evaluation/AverageDiscountedReturn          -42.3143
Evaluation/AverageReturn                    -42.3143
Evaluation/CompletionRate                     0
Evaluation/Iteration                        437
Evaluation/MaxReturn                        -29.5927
Evaluation/MinReturn                        -63.2773
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.25792
Extras/EpisodeRewardMean                    -42.5934
LinearFeatureBaseline/ExplainedVariance       0.892637
PolicyExecTime                                0.225263
ProcessExecTime                               0.0311842
TotalEnvSteps                            443256
policy/Entropy                               -1.20003
policy/KL                                     0.00982318
policy/KLBefore                               0
policy/LossAfter                             -0.0252223
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             0.301187
policy/dLoss                                  0.0252223
---------------------------------------  ---------------
2021-06-04 13:53:40 | [train_policy] epoch #438 | Obtaining samples for iteration 438...
2021-06-04 13:53:40 | [train_policy] epoch #438 | Logging diagnostics...
2021-06-04 13:53:40 | [train_policy] epoch #438 | Optimizing policy...
2021-06-04 13:53:40 | [train_policy] epoch #438 | Computing loss before
2021-06-04 13:53:40 | [train_policy] epoch #438 | Computing KL before
2021-06-04 13:53:40 | [train_policy] epoch #438 | Optimizing
2021-06-04 13:53:40 | [train_policy] epoch #438 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:40 | [train_policy] epoch #438 | computing loss before
2021-06-04 13:53:40 | [train_policy] epoch #438 | computing gradient
2021-06-04 13:53:40 | [train_policy] epoch #438 | gradient computed
2021-06-04 13:53:40 | [train_policy] epoch #438 | computing descent direction
2021-06-04 13:53:40 | [train_policy] epoch #438 | descent direction computed
2021-06-04 13:53:40 | [train_policy] epoch #438 | backtrack iters: 1
2021-06-04 13:53:40 | [train_policy] epoch #438 | optimization finished
2021-06-04 13:53:40 | [train_policy] epoch #438 | Computing KL after
2021-06-04 13:53:40 | [train_policy] epoch #438 | Computing loss after
2021-06-04 13:53:40 | [train_policy] epoch #438 | Fitting baseline...
2021-06-04 13:53:40 | [train_policy] epoch #438 | Saving snapshot...
2021-06-04 13:53:40 | [train_policy] epoch #438 | Saved
2021-06-04 13:53:40 | [train_policy] epoch #438 | Time 352.57 s
2021-06-04 13:53:40 | [train_policy] epoch #438 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285527
Evaluation/AverageDiscountedReturn          -45.369
Evaluation/AverageReturn                    -45.369
Evaluation/CompletionRate                     0
Evaluation/Iteration                        438
Evaluation/MaxReturn                        -29.9968
Evaluation/MinReturn                       -166.73
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         16.2194
Extras/EpisodeRewardMean                    -44.8712
LinearFeatureBaseline/ExplainedVariance       0.547011
PolicyExecTime                                0.231394
ProcessExecTime                               0.0312343
TotalEnvSteps                            444268
policy/Entropy                               -1.22532
policy/KL                                     0.00648041
policy/KLBefore                               0
policy/LossAfter                             -0.01888
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.293664
policy/dLoss                                  0.0188799
---------------------------------------  ----------------
2021-06-04 13:53:40 | [train_policy] epoch #439 | Obtaining samples for iteration 439...
2021-06-04 13:53:41 | [train_policy] epoch #439 | Logging diagnostics...
2021-06-04 13:53:41 | [train_policy] epoch #439 | Optimizing policy...
2021-06-04 13:53:41 | [train_policy] epoch #439 | Computing loss before
2021-06-04 13:53:41 | [train_policy] epoch #439 | Computing KL before
2021-06-04 13:53:41 | [train_policy] epoch #439 | Optimizing
2021-06-04 13:53:41 | [train_policy] epoch #439 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:41 | [train_policy] epoch #439 | computing loss before
2021-06-04 13:53:41 | [train_policy] epoch #439 | computing gradient
2021-06-04 13:53:41 | [train_policy] epoch #439 | gradient computed
2021-06-04 13:53:41 | [train_policy] epoch #439 | computing descent direction
2021-06-04 13:53:41 | [train_policy] epoch #439 | descent direction computed
2021-06-04 13:53:41 | [train_policy] epoch #439 | backtrack iters: 1
2021-06-04 13:53:41 | [train_policy] epoch #439 | optimization finished
2021-06-04 13:53:41 | [train_policy] epoch #439 | Computing KL after
2021-06-04 13:53:41 | [train_policy] epoch #439 | Computing loss after
2021-06-04 13:53:41 | [train_policy] epoch #439 | Fitting baseline...
2021-06-04 13:53:41 | [train_policy] epoch #439 | Saving snapshot...
2021-06-04 13:53:41 | [train_policy] epoch #439 | Saved
2021-06-04 13:53:41 | [train_policy] epoch #439 | Time 353.37 s
2021-06-04 13:53:41 | [train_policy] epoch #439 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285378
Evaluation/AverageDiscountedReturn          -41.928
Evaluation/AverageReturn                    -41.928
Evaluation/CompletionRate                     0
Evaluation/Iteration                        439
Evaluation/MaxReturn                        -30.2784
Evaluation/MinReturn                        -64.0607
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.23409
Extras/EpisodeRewardMean                    -42.1009
LinearFeatureBaseline/ExplainedVariance       0.814255
PolicyExecTime                                0.235976
ProcessExecTime                               0.0310738
TotalEnvSteps                            445280
policy/Entropy                               -1.26202
policy/KL                                     0.00665196
policy/KLBefore                               0
policy/LossAfter                             -0.0167981
policy/LossBefore                             1.97897e-08
policy/Perplexity                             0.283081
policy/dLoss                                  0.0167981
---------------------------------------  ----------------
2021-06-04 13:53:41 | [train_policy] epoch #440 | Obtaining samples for iteration 440...
2021-06-04 13:53:42 | [train_policy] epoch #440 | Logging diagnostics...
2021-06-04 13:53:42 | [train_policy] epoch #440 | Optimizing policy...
2021-06-04 13:53:42 | [train_policy] epoch #440 | Computing loss before
2021-06-04 13:53:42 | [train_policy] epoch #440 | Computing KL before
2021-06-04 13:53:42 | [train_policy] epoch #440 | Optimizing
2021-06-04 13:53:42 | [train_policy] epoch #440 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:42 | [train_policy] epoch #440 | computing loss before
2021-06-04 13:53:42 | [train_policy] epoch #440 | computing gradient
2021-06-04 13:53:42 | [train_policy] epoch #440 | gradient computed
2021-06-04 13:53:42 | [train_policy] epoch #440 | computing descent direction
2021-06-04 13:53:42 | [train_policy] epoch #440 | descent direction computed
2021-06-04 13:53:42 | [train_policy] epoch #440 | backtrack iters: 1
2021-06-04 13:53:42 | [train_policy] epoch #440 | optimization finished
2021-06-04 13:53:42 | [train_policy] epoch #440 | Computing KL after
2021-06-04 13:53:42 | [train_policy] epoch #440 | Computing loss after
2021-06-04 13:53:42 | [train_policy] epoch #440 | Fitting baseline...
2021-06-04 13:53:42 | [train_policy] epoch #440 | Saving snapshot...
2021-06-04 13:53:42 | [train_policy] epoch #440 | Saved
2021-06-04 13:53:42 | [train_policy] epoch #440 | Time 354.17 s
2021-06-04 13:53:42 | [train_policy] epoch #440 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.288864
Evaluation/AverageDiscountedReturn          -42.8779
Evaluation/AverageReturn                    -42.8779
Evaluation/CompletionRate                     0
Evaluation/Iteration                        440
Evaluation/MaxReturn                        -29.3702
Evaluation/MinReturn                        -62.719
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.93234
Extras/EpisodeRewardMean                    -43.0589
LinearFeatureBaseline/ExplainedVariance       0.913114
PolicyExecTime                                0.211149
ProcessExecTime                               0.031642
TotalEnvSteps                            446292
policy/Entropy                               -1.29044
policy/KL                                     0.00659173
policy/KLBefore                               0
policy/LossAfter                             -0.0151608
policy/LossBefore                             1.1544e-08
policy/Perplexity                             0.27515
policy/dLoss                                  0.0151608
---------------------------------------  ---------------
2021-06-04 13:53:42 | [train_policy] epoch #441 | Obtaining samples for iteration 441...
2021-06-04 13:53:43 | [train_policy] epoch #441 | Logging diagnostics...
2021-06-04 13:53:43 | [train_policy] epoch #441 | Optimizing policy...
2021-06-04 13:53:43 | [train_policy] epoch #441 | Computing loss before
2021-06-04 13:53:43 | [train_policy] epoch #441 | Computing KL before
2021-06-04 13:53:43 | [train_policy] epoch #441 | Optimizing
2021-06-04 13:53:43 | [train_policy] epoch #441 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:43 | [train_policy] epoch #441 | computing loss before
2021-06-04 13:53:43 | [train_policy] epoch #441 | computing gradient
2021-06-04 13:53:43 | [train_policy] epoch #441 | gradient computed
2021-06-04 13:53:43 | [train_policy] epoch #441 | computing descent direction
2021-06-04 13:53:43 | [train_policy] epoch #441 | descent direction computed
2021-06-04 13:53:43 | [train_policy] epoch #441 | backtrack iters: 1
2021-06-04 13:53:43 | [train_policy] epoch #441 | optimization finished
2021-06-04 13:53:43 | [train_policy] epoch #441 | Computing KL after
2021-06-04 13:53:43 | [train_policy] epoch #441 | Computing loss after
2021-06-04 13:53:43 | [train_policy] epoch #441 | Fitting baseline...
2021-06-04 13:53:43 | [train_policy] epoch #441 | Saving snapshot...
2021-06-04 13:53:43 | [train_policy] epoch #441 | Saved
2021-06-04 13:53:43 | [train_policy] epoch #441 | Time 354.97 s
2021-06-04 13:53:43 | [train_policy] epoch #441 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285311
Evaluation/AverageDiscountedReturn          -65.1348
Evaluation/AverageReturn                    -65.1348
Evaluation/CompletionRate                     0
Evaluation/Iteration                        441
Evaluation/MaxReturn                        -32.4721
Evaluation/MinReturn                      -2061.85
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.889
Extras/EpisodeRewardMean                    -63.3288
LinearFeatureBaseline/ExplainedVariance       0.0142568
PolicyExecTime                                0.224235
ProcessExecTime                               0.0312517
TotalEnvSteps                            447304
policy/Entropy                               -1.29015
policy/KL                                     0.00651378
policy/KLBefore                               0
policy/LossAfter                             -0.0260771
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.275229
policy/dLoss                                  0.0260771
---------------------------------------  ----------------
2021-06-04 13:53:43 | [train_policy] epoch #442 | Obtaining samples for iteration 442...
2021-06-04 13:53:43 | [train_policy] epoch #442 | Logging diagnostics...
2021-06-04 13:53:43 | [train_policy] epoch #442 | Optimizing policy...
2021-06-04 13:53:43 | [train_policy] epoch #442 | Computing loss before
2021-06-04 13:53:43 | [train_policy] epoch #442 | Computing KL before
2021-06-04 13:53:43 | [train_policy] epoch #442 | Optimizing
2021-06-04 13:53:43 | [train_policy] epoch #442 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:43 | [train_policy] epoch #442 | computing loss before
2021-06-04 13:53:43 | [train_policy] epoch #442 | computing gradient
2021-06-04 13:53:43 | [train_policy] epoch #442 | gradient computed
2021-06-04 13:53:43 | [train_policy] epoch #442 | computing descent direction
2021-06-04 13:53:43 | [train_policy] epoch #442 | descent direction computed
2021-06-04 13:53:43 | [train_policy] epoch #442 | backtrack iters: 0
2021-06-04 13:53:43 | [train_policy] epoch #442 | optimization finished
2021-06-04 13:53:43 | [train_policy] epoch #442 | Computing KL after
2021-06-04 13:53:43 | [train_policy] epoch #442 | Computing loss after
2021-06-04 13:53:44 | [train_policy] epoch #442 | Fitting baseline...
2021-06-04 13:53:44 | [train_policy] epoch #442 | Saving snapshot...
2021-06-04 13:53:44 | [train_policy] epoch #442 | Saved
2021-06-04 13:53:44 | [train_policy] epoch #442 | Time 355.76 s
2021-06-04 13:53:44 | [train_policy] epoch #442 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285177
Evaluation/AverageDiscountedReturn          -42.8667
Evaluation/AverageReturn                    -42.8667
Evaluation/CompletionRate                     0
Evaluation/Iteration                        442
Evaluation/MaxReturn                        -30.2288
Evaluation/MinReturn                        -64.106
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.8068
Extras/EpisodeRewardMean                    -42.9044
LinearFeatureBaseline/ExplainedVariance     -21.7551
PolicyExecTime                                0.228574
ProcessExecTime                               0.0313342
TotalEnvSteps                            448316
policy/Entropy                               -1.2809
policy/KL                                     0.00977729
policy/KLBefore                               0
policy/LossAfter                             -0.0332498
policy/LossBefore                            -2.07321e-08
policy/Perplexity                             0.277786
policy/dLoss                                  0.0332497
---------------------------------------  ----------------
2021-06-04 13:53:44 | [train_policy] epoch #443 | Obtaining samples for iteration 443...
2021-06-04 13:53:44 | [train_policy] epoch #443 | Logging diagnostics...
2021-06-04 13:53:44 | [train_policy] epoch #443 | Optimizing policy...
2021-06-04 13:53:44 | [train_policy] epoch #443 | Computing loss before
2021-06-04 13:53:44 | [train_policy] epoch #443 | Computing KL before
2021-06-04 13:53:44 | [train_policy] epoch #443 | Optimizing
2021-06-04 13:53:44 | [train_policy] epoch #443 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:44 | [train_policy] epoch #443 | computing loss before
2021-06-04 13:53:44 | [train_policy] epoch #443 | computing gradient
2021-06-04 13:53:44 | [train_policy] epoch #443 | gradient computed
2021-06-04 13:53:44 | [train_policy] epoch #443 | computing descent direction
2021-06-04 13:53:44 | [train_policy] epoch #443 | descent direction computed
2021-06-04 13:53:44 | [train_policy] epoch #443 | backtrack iters: 0
2021-06-04 13:53:44 | [train_policy] epoch #443 | optimization finished
2021-06-04 13:53:44 | [train_policy] epoch #443 | Computing KL after
2021-06-04 13:53:44 | [train_policy] epoch #443 | Computing loss after
2021-06-04 13:53:44 | [train_policy] epoch #443 | Fitting baseline...
2021-06-04 13:53:44 | [train_policy] epoch #443 | Saving snapshot...
2021-06-04 13:53:44 | [train_policy] epoch #443 | Saved
2021-06-04 13:53:44 | [train_policy] epoch #443 | Time 356.57 s
2021-06-04 13:53:44 | [train_policy] epoch #443 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.285807
Evaluation/AverageDiscountedReturn          -42.8686
Evaluation/AverageReturn                    -42.8686
Evaluation/CompletionRate                     0
Evaluation/Iteration                        443
Evaluation/MaxReturn                        -29.3012
Evaluation/MinReturn                       -102.993
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.37396
Extras/EpisodeRewardMean                    -43.0675
LinearFeatureBaseline/ExplainedVariance       0.827717
PolicyExecTime                                0.22965
ProcessExecTime                               0.0312769
TotalEnvSteps                            449328
policy/Entropy                               -1.27735
policy/KL                                     0.00977315
policy/KLBefore                               0
policy/LossAfter                             -0.0294992
policy/LossBefore                             5.4186e-09
policy/Perplexity                             0.278774
policy/dLoss                                  0.0294992
---------------------------------------  ---------------
2021-06-04 13:53:44 | [train_policy] epoch #444 | Obtaining samples for iteration 444...
2021-06-04 13:53:45 | [train_policy] epoch #444 | Logging diagnostics...
2021-06-04 13:53:45 | [train_policy] epoch #444 | Optimizing policy...
2021-06-04 13:53:45 | [train_policy] epoch #444 | Computing loss before
2021-06-04 13:53:45 | [train_policy] epoch #444 | Computing KL before
2021-06-04 13:53:45 | [train_policy] epoch #444 | Optimizing
2021-06-04 13:53:45 | [train_policy] epoch #444 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:45 | [train_policy] epoch #444 | computing loss before
2021-06-04 13:53:45 | [train_policy] epoch #444 | computing gradient
2021-06-04 13:53:45 | [train_policy] epoch #444 | gradient computed
2021-06-04 13:53:45 | [train_policy] epoch #444 | computing descent direction
2021-06-04 13:53:45 | [train_policy] epoch #444 | descent direction computed
2021-06-04 13:53:45 | [train_policy] epoch #444 | backtrack iters: 1
2021-06-04 13:53:45 | [train_policy] epoch #444 | optimization finished
2021-06-04 13:53:45 | [train_policy] epoch #444 | Computing KL after
2021-06-04 13:53:45 | [train_policy] epoch #444 | Computing loss after
2021-06-04 13:53:45 | [train_policy] epoch #444 | Fitting baseline...
2021-06-04 13:53:45 | [train_policy] epoch #444 | Saving snapshot...
2021-06-04 13:53:45 | [train_policy] epoch #444 | Saved
2021-06-04 13:53:45 | [train_policy] epoch #444 | Time 357.39 s
2021-06-04 13:53:45 | [train_policy] epoch #444 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286373
Evaluation/AverageDiscountedReturn          -42.9616
Evaluation/AverageReturn                    -42.9616
Evaluation/CompletionRate                     0
Evaluation/Iteration                        444
Evaluation/MaxReturn                        -28.2769
Evaluation/MinReturn                        -97.8644
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.58478
Extras/EpisodeRewardMean                    -42.8684
LinearFeatureBaseline/ExplainedVariance       0.838702
PolicyExecTime                                0.223315
ProcessExecTime                               0.0314596
TotalEnvSteps                            450340
policy/Entropy                               -1.28402
policy/KL                                     0.00648915
policy/KLBefore                               0
policy/LossAfter                             -0.022266
policy/LossBefore                             4.82963e-09
policy/Perplexity                             0.276921
policy/dLoss                                  0.022266
---------------------------------------  ----------------
2021-06-04 13:53:45 | [train_policy] epoch #445 | Obtaining samples for iteration 445...
2021-06-04 13:53:46 | [train_policy] epoch #445 | Logging diagnostics...
2021-06-04 13:53:46 | [train_policy] epoch #445 | Optimizing policy...
2021-06-04 13:53:46 | [train_policy] epoch #445 | Computing loss before
2021-06-04 13:53:46 | [train_policy] epoch #445 | Computing KL before
2021-06-04 13:53:46 | [train_policy] epoch #445 | Optimizing
2021-06-04 13:53:46 | [train_policy] epoch #445 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:46 | [train_policy] epoch #445 | computing loss before
2021-06-04 13:53:46 | [train_policy] epoch #445 | computing gradient
2021-06-04 13:53:46 | [train_policy] epoch #445 | gradient computed
2021-06-04 13:53:46 | [train_policy] epoch #445 | computing descent direction
2021-06-04 13:53:46 | [train_policy] epoch #445 | descent direction computed
2021-06-04 13:53:46 | [train_policy] epoch #445 | backtrack iters: 1
2021-06-04 13:53:46 | [train_policy] epoch #445 | optimization finished
2021-06-04 13:53:46 | [train_policy] epoch #445 | Computing KL after
2021-06-04 13:53:46 | [train_policy] epoch #445 | Computing loss after
2021-06-04 13:53:46 | [train_policy] epoch #445 | Fitting baseline...
2021-06-04 13:53:46 | [train_policy] epoch #445 | Saving snapshot...
2021-06-04 13:53:46 | [train_policy] epoch #445 | Saved
2021-06-04 13:53:46 | [train_policy] epoch #445 | Time 358.19 s
2021-06-04 13:53:46 | [train_policy] epoch #445 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288635
Evaluation/AverageDiscountedReturn          -62.6848
Evaluation/AverageReturn                    -62.6848
Evaluation/CompletionRate                     0
Evaluation/Iteration                        445
Evaluation/MaxReturn                        -29.0448
Evaluation/MinReturn                      -2061.88
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.704
Extras/EpisodeRewardMean                    -61.1221
LinearFeatureBaseline/ExplainedVariance       0.0173432
PolicyExecTime                                0.210982
ProcessExecTime                               0.0315852
TotalEnvSteps                            451352
policy/Entropy                               -1.30052
policy/KL                                     0.00671228
policy/KLBefore                               0
policy/LossAfter                             -0.00503039
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.27239
policy/dLoss                                  0.00503038
---------------------------------------  ----------------
2021-06-04 13:53:46 | [train_policy] epoch #446 | Obtaining samples for iteration 446...
2021-06-04 13:53:47 | [train_policy] epoch #446 | Logging diagnostics...
2021-06-04 13:53:47 | [train_policy] epoch #446 | Optimizing policy...
2021-06-04 13:53:47 | [train_policy] epoch #446 | Computing loss before
2021-06-04 13:53:47 | [train_policy] epoch #446 | Computing KL before
2021-06-04 13:53:47 | [train_policy] epoch #446 | Optimizing
2021-06-04 13:53:47 | [train_policy] epoch #446 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:47 | [train_policy] epoch #446 | computing loss before
2021-06-04 13:53:47 | [train_policy] epoch #446 | computing gradient
2021-06-04 13:53:47 | [train_policy] epoch #446 | gradient computed
2021-06-04 13:53:47 | [train_policy] epoch #446 | computing descent direction
2021-06-04 13:53:47 | [train_policy] epoch #446 | descent direction computed
2021-06-04 13:53:47 | [train_policy] epoch #446 | backtrack iters: 1
2021-06-04 13:53:47 | [train_policy] epoch #446 | optimization finished
2021-06-04 13:53:47 | [train_policy] epoch #446 | Computing KL after
2021-06-04 13:53:47 | [train_policy] epoch #446 | Computing loss after
2021-06-04 13:53:47 | [train_policy] epoch #446 | Fitting baseline...
2021-06-04 13:53:47 | [train_policy] epoch #446 | Saving snapshot...
2021-06-04 13:53:47 | [train_policy] epoch #446 | Saved
2021-06-04 13:53:47 | [train_policy] epoch #446 | Time 358.99 s
2021-06-04 13:53:47 | [train_policy] epoch #446 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286362
Evaluation/AverageDiscountedReturn          -44.5352
Evaluation/AverageReturn                    -44.5352
Evaluation/CompletionRate                     0
Evaluation/Iteration                        446
Evaluation/MaxReturn                        -28.3885
Evaluation/MinReturn                        -90.9733
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.6175
Extras/EpisodeRewardMean                    -44.2397
LinearFeatureBaseline/ExplainedVariance     -47.6508
PolicyExecTime                                0.229933
ProcessExecTime                               0.0314188
TotalEnvSteps                            452364
policy/Entropy                               -1.322
policy/KL                                     0.00668827
policy/KLBefore                               0
policy/LossAfter                             -0.0145428
policy/LossBefore                            -2.23812e-08
policy/Perplexity                             0.266601
policy/dLoss                                  0.0145428
---------------------------------------  ----------------
2021-06-04 13:53:47 | [train_policy] epoch #447 | Obtaining samples for iteration 447...
2021-06-04 13:53:47 | [train_policy] epoch #447 | Logging diagnostics...
2021-06-04 13:53:47 | [train_policy] epoch #447 | Optimizing policy...
2021-06-04 13:53:47 | [train_policy] epoch #447 | Computing loss before
2021-06-04 13:53:47 | [train_policy] epoch #447 | Computing KL before
2021-06-04 13:53:47 | [train_policy] epoch #447 | Optimizing
2021-06-04 13:53:47 | [train_policy] epoch #447 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:47 | [train_policy] epoch #447 | computing loss before
2021-06-04 13:53:47 | [train_policy] epoch #447 | computing gradient
2021-06-04 13:53:47 | [train_policy] epoch #447 | gradient computed
2021-06-04 13:53:47 | [train_policy] epoch #447 | computing descent direction
2021-06-04 13:53:48 | [train_policy] epoch #447 | descent direction computed
2021-06-04 13:53:48 | [train_policy] epoch #447 | backtrack iters: 0
2021-06-04 13:53:48 | [train_policy] epoch #447 | optimization finished
2021-06-04 13:53:48 | [train_policy] epoch #447 | Computing KL after
2021-06-04 13:53:48 | [train_policy] epoch #447 | Computing loss after
2021-06-04 13:53:48 | [train_policy] epoch #447 | Fitting baseline...
2021-06-04 13:53:48 | [train_policy] epoch #447 | Saving snapshot...
2021-06-04 13:53:48 | [train_policy] epoch #447 | Saved
2021-06-04 13:53:48 | [train_policy] epoch #447 | Time 359.79 s
2021-06-04 13:53:48 | [train_policy] epoch #447 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284698
Evaluation/AverageDiscountedReturn          -41.0219
Evaluation/AverageReturn                    -41.0219
Evaluation/CompletionRate                     0
Evaluation/Iteration                        447
Evaluation/MaxReturn                        -29.3695
Evaluation/MinReturn                        -62.9631
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.23213
Extras/EpisodeRewardMean                    -40.9961
LinearFeatureBaseline/ExplainedVariance       0.895259
PolicyExecTime                                0.231355
ProcessExecTime                               0.0311399
TotalEnvSteps                            453376
policy/Entropy                               -1.30795
policy/KL                                     0.00980516
policy/KLBefore                               0
policy/LossAfter                             -0.0221394
policy/LossBefore                            -1.59024e-08
policy/Perplexity                             0.270373
policy/dLoss                                  0.0221393
---------------------------------------  ----------------
2021-06-04 13:53:48 | [train_policy] epoch #448 | Obtaining samples for iteration 448...
2021-06-04 13:53:48 | [train_policy] epoch #448 | Logging diagnostics...
2021-06-04 13:53:48 | [train_policy] epoch #448 | Optimizing policy...
2021-06-04 13:53:48 | [train_policy] epoch #448 | Computing loss before
2021-06-04 13:53:48 | [train_policy] epoch #448 | Computing KL before
2021-06-04 13:53:48 | [train_policy] epoch #448 | Optimizing
2021-06-04 13:53:48 | [train_policy] epoch #448 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:48 | [train_policy] epoch #448 | computing loss before
2021-06-04 13:53:48 | [train_policy] epoch #448 | computing gradient
2021-06-04 13:53:48 | [train_policy] epoch #448 | gradient computed
2021-06-04 13:53:48 | [train_policy] epoch #448 | computing descent direction
2021-06-04 13:53:48 | [train_policy] epoch #448 | descent direction computed
2021-06-04 13:53:48 | [train_policy] epoch #448 | backtrack iters: 0
2021-06-04 13:53:48 | [train_policy] epoch #448 | optimization finished
2021-06-04 13:53:48 | [train_policy] epoch #448 | Computing KL after
2021-06-04 13:53:48 | [train_policy] epoch #448 | Computing loss after
2021-06-04 13:53:48 | [train_policy] epoch #448 | Fitting baseline...
2021-06-04 13:53:48 | [train_policy] epoch #448 | Saving snapshot...
2021-06-04 13:53:48 | [train_policy] epoch #448 | Saved
2021-06-04 13:53:48 | [train_policy] epoch #448 | Time 360.58 s
2021-06-04 13:53:48 | [train_policy] epoch #448 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285146
Evaluation/AverageDiscountedReturn          -63.2046
Evaluation/AverageReturn                    -63.2046
Evaluation/CompletionRate                     0
Evaluation/Iteration                        448
Evaluation/MaxReturn                        -29.0374
Evaluation/MinReturn                      -2061.8
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.668
Extras/EpisodeRewardMean                    -61.3976
LinearFeatureBaseline/ExplainedVariance       0.0133758
PolicyExecTime                                0.217468
ProcessExecTime                               0.0312266
TotalEnvSteps                            454388
policy/Entropy                               -1.33083
policy/KL                                     0.00978624
policy/KLBefore                               0
policy/LossAfter                             -0.0232702
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.264258
policy/dLoss                                  0.0232702
---------------------------------------  ----------------
2021-06-04 13:53:48 | [train_policy] epoch #449 | Obtaining samples for iteration 449...
2021-06-04 13:53:49 | [train_policy] epoch #449 | Logging diagnostics...
2021-06-04 13:53:49 | [train_policy] epoch #449 | Optimizing policy...
2021-06-04 13:53:49 | [train_policy] epoch #449 | Computing loss before
2021-06-04 13:53:49 | [train_policy] epoch #449 | Computing KL before
2021-06-04 13:53:49 | [train_policy] epoch #449 | Optimizing
2021-06-04 13:53:49 | [train_policy] epoch #449 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:49 | [train_policy] epoch #449 | computing loss before
2021-06-04 13:53:49 | [train_policy] epoch #449 | computing gradient
2021-06-04 13:53:49 | [train_policy] epoch #449 | gradient computed
2021-06-04 13:53:49 | [train_policy] epoch #449 | computing descent direction
2021-06-04 13:53:49 | [train_policy] epoch #449 | descent direction computed
2021-06-04 13:53:49 | [train_policy] epoch #449 | backtrack iters: 0
2021-06-04 13:53:49 | [train_policy] epoch #449 | optimization finished
2021-06-04 13:53:49 | [train_policy] epoch #449 | Computing KL after
2021-06-04 13:53:49 | [train_policy] epoch #449 | Computing loss after
2021-06-04 13:53:49 | [train_policy] epoch #449 | Fitting baseline...
2021-06-04 13:53:49 | [train_policy] epoch #449 | Saving snapshot...
2021-06-04 13:53:49 | [train_policy] epoch #449 | Saved
2021-06-04 13:53:49 | [train_policy] epoch #449 | Time 361.39 s
2021-06-04 13:53:49 | [train_policy] epoch #449 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.292929
Evaluation/AverageDiscountedReturn          -43.3105
Evaluation/AverageReturn                    -43.3105
Evaluation/CompletionRate                     0
Evaluation/Iteration                        449
Evaluation/MaxReturn                        -29.5932
Evaluation/MinReturn                        -95.9612
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.1354
Extras/EpisodeRewardMean                    -43.2316
LinearFeatureBaseline/ExplainedVariance     -41.2577
PolicyExecTime                                0.229718
ProcessExecTime                               0.0321958
TotalEnvSteps                            455400
policy/Entropy                               -1.36358
policy/KL                                     0.00995133
policy/KLBefore                               0
policy/LossAfter                             -0.0193754
policy/LossBefore                             1.04249e-08
policy/Perplexity                             0.255744
policy/dLoss                                  0.0193755
---------------------------------------  ----------------
2021-06-04 13:53:49 | [train_policy] epoch #450 | Obtaining samples for iteration 450...
2021-06-04 13:53:50 | [train_policy] epoch #450 | Logging diagnostics...
2021-06-04 13:53:50 | [train_policy] epoch #450 | Optimizing policy...
2021-06-04 13:53:50 | [train_policy] epoch #450 | Computing loss before
2021-06-04 13:53:50 | [train_policy] epoch #450 | Computing KL before
2021-06-04 13:53:50 | [train_policy] epoch #450 | Optimizing
2021-06-04 13:53:50 | [train_policy] epoch #450 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:50 | [train_policy] epoch #450 | computing loss before
2021-06-04 13:53:50 | [train_policy] epoch #450 | computing gradient
2021-06-04 13:53:50 | [train_policy] epoch #450 | gradient computed
2021-06-04 13:53:50 | [train_policy] epoch #450 | computing descent direction
2021-06-04 13:53:50 | [train_policy] epoch #450 | descent direction computed
2021-06-04 13:53:50 | [train_policy] epoch #450 | backtrack iters: 0
2021-06-04 13:53:50 | [train_policy] epoch #450 | optimization finished
2021-06-04 13:53:50 | [train_policy] epoch #450 | Computing KL after
2021-06-04 13:53:50 | [train_policy] epoch #450 | Computing loss after
2021-06-04 13:53:50 | [train_policy] epoch #450 | Fitting baseline...
2021-06-04 13:53:50 | [train_policy] epoch #450 | Saving snapshot...
2021-06-04 13:53:50 | [train_policy] epoch #450 | Saved
2021-06-04 13:53:50 | [train_policy] epoch #450 | Time 362.20 s
2021-06-04 13:53:50 | [train_policy] epoch #450 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.288385
Evaluation/AverageDiscountedReturn          -42.9003
Evaluation/AverageReturn                    -42.9003
Evaluation/CompletionRate                     0
Evaluation/Iteration                        450
Evaluation/MaxReturn                        -29.7435
Evaluation/MinReturn                        -96.5109
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.95509
Extras/EpisodeRewardMean                    -43.1226
LinearFeatureBaseline/ExplainedVariance       0.815633
PolicyExecTime                                0.232112
ProcessExecTime                               0.0315373
TotalEnvSteps                            456412
policy/Entropy                               -1.31668
policy/KL                                     0.00922702
policy/KLBefore                               0
policy/LossAfter                             -0.0182332
policy/LossBefore                             2.7093e-08
policy/Perplexity                             0.268023
policy/dLoss                                  0.0182332
---------------------------------------  ---------------
2021-06-04 13:53:50 | [train_policy] epoch #451 | Obtaining samples for iteration 451...
2021-06-04 13:53:51 | [train_policy] epoch #451 | Logging diagnostics...
2021-06-04 13:53:51 | [train_policy] epoch #451 | Optimizing policy...
2021-06-04 13:53:51 | [train_policy] epoch #451 | Computing loss before
2021-06-04 13:53:51 | [train_policy] epoch #451 | Computing KL before
2021-06-04 13:53:51 | [train_policy] epoch #451 | Optimizing
2021-06-04 13:53:51 | [train_policy] epoch #451 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:51 | [train_policy] epoch #451 | computing loss before
2021-06-04 13:53:51 | [train_policy] epoch #451 | computing gradient
2021-06-04 13:53:51 | [train_policy] epoch #451 | gradient computed
2021-06-04 13:53:51 | [train_policy] epoch #451 | computing descent direction
2021-06-04 13:53:51 | [train_policy] epoch #451 | descent direction computed
2021-06-04 13:53:51 | [train_policy] epoch #451 | backtrack iters: 0
2021-06-04 13:53:51 | [train_policy] epoch #451 | optimization finished
2021-06-04 13:53:51 | [train_policy] epoch #451 | Computing KL after
2021-06-04 13:53:51 | [train_policy] epoch #451 | Computing loss after
2021-06-04 13:53:51 | [train_policy] epoch #451 | Fitting baseline...
2021-06-04 13:53:51 | [train_policy] epoch #451 | Saving snapshot...
2021-06-04 13:53:51 | [train_policy] epoch #451 | Saved
2021-06-04 13:53:51 | [train_policy] epoch #451 | Time 363.00 s
2021-06-04 13:53:51 | [train_policy] epoch #451 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284936
Evaluation/AverageDiscountedReturn          -41.1973
Evaluation/AverageReturn                    -41.1973
Evaluation/CompletionRate                     0
Evaluation/Iteration                        451
Evaluation/MaxReturn                        -28.7695
Evaluation/MinReturn                        -64.0076
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.96943
Extras/EpisodeRewardMean                    -41.3407
LinearFeatureBaseline/ExplainedVariance       0.905262
PolicyExecTime                                0.226443
ProcessExecTime                               0.0312347
TotalEnvSteps                            457424
policy/Entropy                               -1.31333
policy/KL                                     0.009846
policy/KLBefore                               0
policy/LossAfter                             -0.0210841
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.268923
policy/dLoss                                  0.0210841
---------------------------------------  ----------------
2021-06-04 13:53:51 | [train_policy] epoch #452 | Obtaining samples for iteration 452...
2021-06-04 13:53:51 | [train_policy] epoch #452 | Logging diagnostics...
2021-06-04 13:53:51 | [train_policy] epoch #452 | Optimizing policy...
2021-06-04 13:53:51 | [train_policy] epoch #452 | Computing loss before
2021-06-04 13:53:51 | [train_policy] epoch #452 | Computing KL before
2021-06-04 13:53:51 | [train_policy] epoch #452 | Optimizing
2021-06-04 13:53:51 | [train_policy] epoch #452 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:51 | [train_policy] epoch #452 | computing loss before
2021-06-04 13:53:51 | [train_policy] epoch #452 | computing gradient
2021-06-04 13:53:51 | [train_policy] epoch #452 | gradient computed
2021-06-04 13:53:51 | [train_policy] epoch #452 | computing descent direction
2021-06-04 13:53:52 | [train_policy] epoch #452 | descent direction computed
2021-06-04 13:53:52 | [train_policy] epoch #452 | backtrack iters: 1
2021-06-04 13:53:52 | [train_policy] epoch #452 | optimization finished
2021-06-04 13:53:52 | [train_policy] epoch #452 | Computing KL after
2021-06-04 13:53:52 | [train_policy] epoch #452 | Computing loss after
2021-06-04 13:53:52 | [train_policy] epoch #452 | Fitting baseline...
2021-06-04 13:53:52 | [train_policy] epoch #452 | Saving snapshot...
2021-06-04 13:53:52 | [train_policy] epoch #452 | Saved
2021-06-04 13:53:52 | [train_policy] epoch #452 | Time 363.81 s
2021-06-04 13:53:52 | [train_policy] epoch #452 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285926
Evaluation/AverageDiscountedReturn          -41.8631
Evaluation/AverageReturn                    -41.8631
Evaluation/CompletionRate                     0
Evaluation/Iteration                        452
Evaluation/MaxReturn                        -29.6557
Evaluation/MinReturn                        -93.3713
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.64235
Extras/EpisodeRewardMean                    -41.5472
LinearFeatureBaseline/ExplainedVariance       0.836934
PolicyExecTime                                0.233717
ProcessExecTime                               0.0313292
TotalEnvSteps                            458436
policy/Entropy                               -1.32933
policy/KL                                     0.00639839
policy/KLBefore                               0
policy/LossAfter                             -0.0226725
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.264654
policy/dLoss                                  0.0226725
---------------------------------------  ----------------
2021-06-04 13:53:52 | [train_policy] epoch #453 | Obtaining samples for iteration 453...
2021-06-04 13:53:52 | [train_policy] epoch #453 | Logging diagnostics...
2021-06-04 13:53:52 | [train_policy] epoch #453 | Optimizing policy...
2021-06-04 13:53:52 | [train_policy] epoch #453 | Computing loss before
2021-06-04 13:53:52 | [train_policy] epoch #453 | Computing KL before
2021-06-04 13:53:52 | [train_policy] epoch #453 | Optimizing
2021-06-04 13:53:52 | [train_policy] epoch #453 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:52 | [train_policy] epoch #453 | computing loss before
2021-06-04 13:53:52 | [train_policy] epoch #453 | computing gradient
2021-06-04 13:53:52 | [train_policy] epoch #453 | gradient computed
2021-06-04 13:53:52 | [train_policy] epoch #453 | computing descent direction
2021-06-04 13:53:52 | [train_policy] epoch #453 | descent direction computed
2021-06-04 13:53:52 | [train_policy] epoch #453 | backtrack iters: 1
2021-06-04 13:53:52 | [train_policy] epoch #453 | optimization finished
2021-06-04 13:53:52 | [train_policy] epoch #453 | Computing KL after
2021-06-04 13:53:52 | [train_policy] epoch #453 | Computing loss after
2021-06-04 13:53:52 | [train_policy] epoch #453 | Fitting baseline...
2021-06-04 13:53:52 | [train_policy] epoch #453 | Saving snapshot...
2021-06-04 13:53:52 | [train_policy] epoch #453 | Saved
2021-06-04 13:53:52 | [train_policy] epoch #453 | Time 364.59 s
2021-06-04 13:53:52 | [train_policy] epoch #453 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284268
Evaluation/AverageDiscountedReturn          -41.5505
Evaluation/AverageReturn                    -41.5505
Evaluation/CompletionRate                     0
Evaluation/Iteration                        453
Evaluation/MaxReturn                        -30.3509
Evaluation/MinReturn                        -63.9268
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.86062
Extras/EpisodeRewardMean                    -41.2701
LinearFeatureBaseline/ExplainedVariance       0.920298
PolicyExecTime                                0.222118
ProcessExecTime                               0.0311682
TotalEnvSteps                            459448
policy/Entropy                               -1.33755
policy/KL                                     0.00650697
policy/KLBefore                               0
policy/LossAfter                             -0.015991
policy/LossBefore                            -1.17796e-08
policy/Perplexity                             0.262488
policy/dLoss                                  0.015991
---------------------------------------  ----------------
2021-06-04 13:53:52 | [train_policy] epoch #454 | Obtaining samples for iteration 454...
2021-06-04 13:53:53 | [train_policy] epoch #454 | Logging diagnostics...
2021-06-04 13:53:53 | [train_policy] epoch #454 | Optimizing policy...
2021-06-04 13:53:53 | [train_policy] epoch #454 | Computing loss before
2021-06-04 13:53:53 | [train_policy] epoch #454 | Computing KL before
2021-06-04 13:53:53 | [train_policy] epoch #454 | Optimizing
2021-06-04 13:53:53 | [train_policy] epoch #454 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:53 | [train_policy] epoch #454 | computing loss before
2021-06-04 13:53:53 | [train_policy] epoch #454 | computing gradient
2021-06-04 13:53:53 | [train_policy] epoch #454 | gradient computed
2021-06-04 13:53:53 | [train_policy] epoch #454 | computing descent direction
2021-06-04 13:53:53 | [train_policy] epoch #454 | descent direction computed
2021-06-04 13:53:53 | [train_policy] epoch #454 | backtrack iters: 1
2021-06-04 13:53:53 | [train_policy] epoch #454 | optimization finished
2021-06-04 13:53:53 | [train_policy] epoch #454 | Computing KL after
2021-06-04 13:53:53 | [train_policy] epoch #454 | Computing loss after
2021-06-04 13:53:53 | [train_policy] epoch #454 | Fitting baseline...
2021-06-04 13:53:53 | [train_policy] epoch #454 | Saving snapshot...
2021-06-04 13:53:53 | [train_policy] epoch #454 | Saved
2021-06-04 13:53:53 | [train_policy] epoch #454 | Time 365.40 s
2021-06-04 13:53:53 | [train_policy] epoch #454 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285655
Evaluation/AverageDiscountedReturn          -44.5176
Evaluation/AverageReturn                    -44.5176
Evaluation/CompletionRate                     0
Evaluation/Iteration                        454
Evaluation/MaxReturn                        -30.7139
Evaluation/MinReturn                       -255.523
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         23.9253
Extras/EpisodeRewardMean                    -44.2368
LinearFeatureBaseline/ExplainedVariance       0.316065
PolicyExecTime                                0.227939
ProcessExecTime                               0.0311835
TotalEnvSteps                            460460
policy/Entropy                               -1.35842
policy/KL                                     0.00654035
policy/KLBefore                               0
policy/LossAfter                             -0.0195808
policy/LossBefore                             7.77452e-09
policy/Perplexity                             0.257066
policy/dLoss                                  0.0195808
---------------------------------------  ----------------
2021-06-04 13:53:53 | [train_policy] epoch #455 | Obtaining samples for iteration 455...
2021-06-04 13:53:54 | [train_policy] epoch #455 | Logging diagnostics...
2021-06-04 13:53:54 | [train_policy] epoch #455 | Optimizing policy...
2021-06-04 13:53:54 | [train_policy] epoch #455 | Computing loss before
2021-06-04 13:53:54 | [train_policy] epoch #455 | Computing KL before
2021-06-04 13:53:54 | [train_policy] epoch #455 | Optimizing
2021-06-04 13:53:54 | [train_policy] epoch #455 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:54 | [train_policy] epoch #455 | computing loss before
2021-06-04 13:53:54 | [train_policy] epoch #455 | computing gradient
2021-06-04 13:53:54 | [train_policy] epoch #455 | gradient computed
2021-06-04 13:53:54 | [train_policy] epoch #455 | computing descent direction
2021-06-04 13:53:54 | [train_policy] epoch #455 | descent direction computed
2021-06-04 13:53:54 | [train_policy] epoch #455 | backtrack iters: 0
2021-06-04 13:53:54 | [train_policy] epoch #455 | optimization finished
2021-06-04 13:53:54 | [train_policy] epoch #455 | Computing KL after
2021-06-04 13:53:54 | [train_policy] epoch #455 | Computing loss after
2021-06-04 13:53:54 | [train_policy] epoch #455 | Fitting baseline...
2021-06-04 13:53:54 | [train_policy] epoch #455 | Saving snapshot...
2021-06-04 13:53:54 | [train_policy] epoch #455 | Saved
2021-06-04 13:53:54 | [train_policy] epoch #455 | Time 366.20 s
2021-06-04 13:53:54 | [train_policy] epoch #455 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.28686
Evaluation/AverageDiscountedReturn          -43.8929
Evaluation/AverageReturn                    -43.8929
Evaluation/CompletionRate                     0
Evaluation/Iteration                        455
Evaluation/MaxReturn                        -30.1896
Evaluation/MinReturn                       -100.065
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.2431
Extras/EpisodeRewardMean                    -43.8162
LinearFeatureBaseline/ExplainedVariance       0.681652
PolicyExecTime                                0.223149
ProcessExecTime                               0.0313911
TotalEnvSteps                            461472
policy/Entropy                               -1.35882
policy/KL                                     0.0096092
policy/KLBefore                               0
policy/LossAfter                             -0.0169671
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             0.256964
policy/dLoss                                  0.0169671
---------------------------------------  ---------------
2021-06-04 13:53:54 | [train_policy] epoch #456 | Obtaining samples for iteration 456...
2021-06-04 13:53:55 | [train_policy] epoch #456 | Logging diagnostics...
2021-06-04 13:53:55 | [train_policy] epoch #456 | Optimizing policy...
2021-06-04 13:53:55 | [train_policy] epoch #456 | Computing loss before
2021-06-04 13:53:55 | [train_policy] epoch #456 | Computing KL before
2021-06-04 13:53:55 | [train_policy] epoch #456 | Optimizing
2021-06-04 13:53:55 | [train_policy] epoch #456 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:55 | [train_policy] epoch #456 | computing loss before
2021-06-04 13:53:55 | [train_policy] epoch #456 | computing gradient
2021-06-04 13:53:55 | [train_policy] epoch #456 | gradient computed
2021-06-04 13:53:55 | [train_policy] epoch #456 | computing descent direction
2021-06-04 13:53:55 | [train_policy] epoch #456 | descent direction computed
2021-06-04 13:53:55 | [train_policy] epoch #456 | backtrack iters: 1
2021-06-04 13:53:55 | [train_policy] epoch #456 | optimization finished
2021-06-04 13:53:55 | [train_policy] epoch #456 | Computing KL after
2021-06-04 13:53:55 | [train_policy] epoch #456 | Computing loss after
2021-06-04 13:53:55 | [train_policy] epoch #456 | Fitting baseline...
2021-06-04 13:53:55 | [train_policy] epoch #456 | Saving snapshot...
2021-06-04 13:53:55 | [train_policy] epoch #456 | Saved
2021-06-04 13:53:55 | [train_policy] epoch #456 | Time 366.99 s
2021-06-04 13:53:55 | [train_policy] epoch #456 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284986
Evaluation/AverageDiscountedReturn          -42.8425
Evaluation/AverageReturn                    -42.8425
Evaluation/CompletionRate                     0
Evaluation/Iteration                        456
Evaluation/MaxReturn                        -31.9568
Evaluation/MinReturn                       -106.835
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.9389
Extras/EpisodeRewardMean                    -43.0919
LinearFeatureBaseline/ExplainedVariance       0.812869
PolicyExecTime                                0.219795
ProcessExecTime                               0.0312736
TotalEnvSteps                            462484
policy/Entropy                               -1.38249
policy/KL                                     0.0064796
policy/KLBefore                               0
policy/LossAfter                             -0.0236916
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.250953
policy/dLoss                                  0.0236916
---------------------------------------  ----------------
2021-06-04 13:53:55 | [train_policy] epoch #457 | Obtaining samples for iteration 457...
2021-06-04 13:53:55 | [train_policy] epoch #457 | Logging diagnostics...
2021-06-04 13:53:55 | [train_policy] epoch #457 | Optimizing policy...
2021-06-04 13:53:55 | [train_policy] epoch #457 | Computing loss before
2021-06-04 13:53:55 | [train_policy] epoch #457 | Computing KL before
2021-06-04 13:53:55 | [train_policy] epoch #457 | Optimizing
2021-06-04 13:53:55 | [train_policy] epoch #457 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:55 | [train_policy] epoch #457 | computing loss before
2021-06-04 13:53:55 | [train_policy] epoch #457 | computing gradient
2021-06-04 13:53:55 | [train_policy] epoch #457 | gradient computed
2021-06-04 13:53:55 | [train_policy] epoch #457 | computing descent direction
2021-06-04 13:53:56 | [train_policy] epoch #457 | descent direction computed
2021-06-04 13:53:56 | [train_policy] epoch #457 | backtrack iters: 0
2021-06-04 13:53:56 | [train_policy] epoch #457 | optimization finished
2021-06-04 13:53:56 | [train_policy] epoch #457 | Computing KL after
2021-06-04 13:53:56 | [train_policy] epoch #457 | Computing loss after
2021-06-04 13:53:56 | [train_policy] epoch #457 | Fitting baseline...
2021-06-04 13:53:56 | [train_policy] epoch #457 | Saving snapshot...
2021-06-04 13:53:56 | [train_policy] epoch #457 | Saved
2021-06-04 13:53:56 | [train_policy] epoch #457 | Time 367.79 s
2021-06-04 13:53:56 | [train_policy] epoch #457 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286705
Evaluation/AverageDiscountedReturn          -43.3892
Evaluation/AverageReturn                    -43.3892
Evaluation/CompletionRate                     0
Evaluation/Iteration                        457
Evaluation/MaxReturn                        -29.5666
Evaluation/MinReturn                        -89.7806
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.518
Extras/EpisodeRewardMean                    -43.2923
LinearFeatureBaseline/ExplainedVariance       0.830719
PolicyExecTime                                0.234206
ProcessExecTime                               0.0313635
TotalEnvSteps                            463496
policy/Entropy                               -1.36002
policy/KL                                     0.00971306
policy/KLBefore                               0
policy/LossAfter                             -0.0233073
policy/LossBefore                            -7.06774e-10
policy/Perplexity                             0.256656
policy/dLoss                                  0.0233073
---------------------------------------  ----------------
2021-06-04 13:53:56 | [train_policy] epoch #458 | Obtaining samples for iteration 458...
2021-06-04 13:53:56 | [train_policy] epoch #458 | Logging diagnostics...
2021-06-04 13:53:56 | [train_policy] epoch #458 | Optimizing policy...
2021-06-04 13:53:56 | [train_policy] epoch #458 | Computing loss before
2021-06-04 13:53:56 | [train_policy] epoch #458 | Computing KL before
2021-06-04 13:53:56 | [train_policy] epoch #458 | Optimizing
2021-06-04 13:53:56 | [train_policy] epoch #458 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:56 | [train_policy] epoch #458 | computing loss before
2021-06-04 13:53:56 | [train_policy] epoch #458 | computing gradient
2021-06-04 13:53:56 | [train_policy] epoch #458 | gradient computed
2021-06-04 13:53:56 | [train_policy] epoch #458 | computing descent direction
2021-06-04 13:53:56 | [train_policy] epoch #458 | descent direction computed
2021-06-04 13:53:56 | [train_policy] epoch #458 | backtrack iters: 0
2021-06-04 13:53:56 | [train_policy] epoch #458 | optimization finished
2021-06-04 13:53:56 | [train_policy] epoch #458 | Computing KL after
2021-06-04 13:53:56 | [train_policy] epoch #458 | Computing loss after
2021-06-04 13:53:56 | [train_policy] epoch #458 | Fitting baseline...
2021-06-04 13:53:56 | [train_policy] epoch #458 | Saving snapshot...
2021-06-04 13:53:56 | [train_policy] epoch #458 | Saved
2021-06-04 13:53:56 | [train_policy] epoch #458 | Time 368.59 s
2021-06-04 13:53:56 | [train_policy] epoch #458 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286752
Evaluation/AverageDiscountedReturn          -42.9621
Evaluation/AverageReturn                    -42.9621
Evaluation/CompletionRate                     0
Evaluation/Iteration                        458
Evaluation/MaxReturn                        -30.261
Evaluation/MinReturn                        -86.1855
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.4252
Extras/EpisodeRewardMean                    -42.7841
LinearFeatureBaseline/ExplainedVariance       0.876645
PolicyExecTime                                0.23238
ProcessExecTime                               0.031353
TotalEnvSteps                            464508
policy/Entropy                               -1.34614
policy/KL                                     0.00973929
policy/KLBefore                               0
policy/LossAfter                             -0.0178915
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.260244
policy/dLoss                                  0.0178915
---------------------------------------  ----------------
2021-06-04 13:53:56 | [train_policy] epoch #459 | Obtaining samples for iteration 459...
2021-06-04 13:53:57 | [train_policy] epoch #459 | Logging diagnostics...
2021-06-04 13:53:57 | [train_policy] epoch #459 | Optimizing policy...
2021-06-04 13:53:57 | [train_policy] epoch #459 | Computing loss before
2021-06-04 13:53:57 | [train_policy] epoch #459 | Computing KL before
2021-06-04 13:53:57 | [train_policy] epoch #459 | Optimizing
2021-06-04 13:53:57 | [train_policy] epoch #459 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:57 | [train_policy] epoch #459 | computing loss before
2021-06-04 13:53:57 | [train_policy] epoch #459 | computing gradient
2021-06-04 13:53:57 | [train_policy] epoch #459 | gradient computed
2021-06-04 13:53:57 | [train_policy] epoch #459 | computing descent direction
2021-06-04 13:53:57 | [train_policy] epoch #459 | descent direction computed
2021-06-04 13:53:57 | [train_policy] epoch #459 | backtrack iters: 1
2021-06-04 13:53:57 | [train_policy] epoch #459 | optimization finished
2021-06-04 13:53:57 | [train_policy] epoch #459 | Computing KL after
2021-06-04 13:53:57 | [train_policy] epoch #459 | Computing loss after
2021-06-04 13:53:57 | [train_policy] epoch #459 | Fitting baseline...
2021-06-04 13:53:57 | [train_policy] epoch #459 | Saving snapshot...
2021-06-04 13:53:57 | [train_policy] epoch #459 | Saved
2021-06-04 13:53:57 | [train_policy] epoch #459 | Time 369.42 s
2021-06-04 13:53:57 | [train_policy] epoch #459 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.286203
Evaluation/AverageDiscountedReturn          -43.4379
Evaluation/AverageReturn                    -43.4379
Evaluation/CompletionRate                     0
Evaluation/Iteration                        459
Evaluation/MaxReturn                        -30.8391
Evaluation/MinReturn                       -106.477
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.9141
Extras/EpisodeRewardMean                    -43.3808
LinearFeatureBaseline/ExplainedVariance       0.804936
PolicyExecTime                                0.245183
ProcessExecTime                               0.0314882
TotalEnvSteps                            465520
policy/Entropy                               -1.35299
policy/KL                                     0.00671056
policy/KLBefore                               0
policy/LossAfter                             -0.0180962
policy/LossBefore                             1.37232e-08
policy/Perplexity                             0.258467
policy/dLoss                                  0.0180962
---------------------------------------  ----------------
2021-06-04 13:53:57 | [train_policy] epoch #460 | Obtaining samples for iteration 460...
2021-06-04 13:53:58 | [train_policy] epoch #460 | Logging diagnostics...
2021-06-04 13:53:58 | [train_policy] epoch #460 | Optimizing policy...
2021-06-04 13:53:58 | [train_policy] epoch #460 | Computing loss before
2021-06-04 13:53:58 | [train_policy] epoch #460 | Computing KL before
2021-06-04 13:53:58 | [train_policy] epoch #460 | Optimizing
2021-06-04 13:53:58 | [train_policy] epoch #460 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:58 | [train_policy] epoch #460 | computing loss before
2021-06-04 13:53:58 | [train_policy] epoch #460 | computing gradient
2021-06-04 13:53:58 | [train_policy] epoch #460 | gradient computed
2021-06-04 13:53:58 | [train_policy] epoch #460 | computing descent direction
2021-06-04 13:53:58 | [train_policy] epoch #460 | descent direction computed
2021-06-04 13:53:58 | [train_policy] epoch #460 | backtrack iters: 0
2021-06-04 13:53:58 | [train_policy] epoch #460 | optimization finished
2021-06-04 13:53:58 | [train_policy] epoch #460 | Computing KL after
2021-06-04 13:53:58 | [train_policy] epoch #460 | Computing loss after
2021-06-04 13:53:58 | [train_policy] epoch #460 | Fitting baseline...
2021-06-04 13:53:58 | [train_policy] epoch #460 | Saving snapshot...
2021-06-04 13:53:58 | [train_policy] epoch #460 | Saved
2021-06-04 13:53:58 | [train_policy] epoch #460 | Time 370.21 s
2021-06-04 13:53:58 | [train_policy] epoch #460 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28417
Evaluation/AverageDiscountedReturn          -42.5138
Evaluation/AverageReturn                    -42.5138
Evaluation/CompletionRate                     0
Evaluation/Iteration                        460
Evaluation/MaxReturn                        -30.683
Evaluation/MinReturn                        -63.9193
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5643
Extras/EpisodeRewardMean                    -42.7467
LinearFeatureBaseline/ExplainedVariance       0.879615
PolicyExecTime                                0.226998
ProcessExecTime                               0.0311787
TotalEnvSteps                            466532
policy/Entropy                               -1.34146
policy/KL                                     0.00989388
policy/KLBefore                               0
policy/LossAfter                             -0.0182824
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.261464
policy/dLoss                                  0.0182823
---------------------------------------  ----------------
2021-06-04 13:53:58 | [train_policy] epoch #461 | Obtaining samples for iteration 461...
2021-06-04 13:53:59 | [train_policy] epoch #461 | Logging diagnostics...
2021-06-04 13:53:59 | [train_policy] epoch #461 | Optimizing policy...
2021-06-04 13:53:59 | [train_policy] epoch #461 | Computing loss before
2021-06-04 13:53:59 | [train_policy] epoch #461 | Computing KL before
2021-06-04 13:53:59 | [train_policy] epoch #461 | Optimizing
2021-06-04 13:53:59 | [train_policy] epoch #461 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:59 | [train_policy] epoch #461 | computing loss before
2021-06-04 13:53:59 | [train_policy] epoch #461 | computing gradient
2021-06-04 13:53:59 | [train_policy] epoch #461 | gradient computed
2021-06-04 13:53:59 | [train_policy] epoch #461 | computing descent direction
2021-06-04 13:53:59 | [train_policy] epoch #461 | descent direction computed
2021-06-04 13:53:59 | [train_policy] epoch #461 | backtrack iters: 1
2021-06-04 13:53:59 | [train_policy] epoch #461 | optimization finished
2021-06-04 13:53:59 | [train_policy] epoch #461 | Computing KL after
2021-06-04 13:53:59 | [train_policy] epoch #461 | Computing loss after
2021-06-04 13:53:59 | [train_policy] epoch #461 | Fitting baseline...
2021-06-04 13:53:59 | [train_policy] epoch #461 | Saving snapshot...
2021-06-04 13:53:59 | [train_policy] epoch #461 | Saved
2021-06-04 13:53:59 | [train_policy] epoch #461 | Time 371.02 s
2021-06-04 13:53:59 | [train_policy] epoch #461 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28476
Evaluation/AverageDiscountedReturn          -43.1812
Evaluation/AverageReturn                    -43.1812
Evaluation/CompletionRate                     0
Evaluation/Iteration                        461
Evaluation/MaxReturn                        -30.3032
Evaluation/MinReturn                        -62.9445
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.44449
Extras/EpisodeRewardMean                    -43.1815
LinearFeatureBaseline/ExplainedVariance       0.896919
PolicyExecTime                                0.229771
ProcessExecTime                               0.0310407
TotalEnvSteps                            467544
policy/Entropy                               -1.32846
policy/KL                                     0.00643034
policy/KLBefore                               0
policy/LossAfter                             -0.0183632
policy/LossBefore                             7.30334e-09
policy/Perplexity                             0.264884
policy/dLoss                                  0.0183632
---------------------------------------  ----------------
2021-06-04 13:53:59 | [train_policy] epoch #462 | Obtaining samples for iteration 462...
2021-06-04 13:53:59 | [train_policy] epoch #462 | Logging diagnostics...
2021-06-04 13:53:59 | [train_policy] epoch #462 | Optimizing policy...
2021-06-04 13:53:59 | [train_policy] epoch #462 | Computing loss before
2021-06-04 13:53:59 | [train_policy] epoch #462 | Computing KL before
2021-06-04 13:53:59 | [train_policy] epoch #462 | Optimizing
2021-06-04 13:53:59 | [train_policy] epoch #462 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:53:59 | [train_policy] epoch #462 | computing loss before
2021-06-04 13:53:59 | [train_policy] epoch #462 | computing gradient
2021-06-04 13:53:59 | [train_policy] epoch #462 | gradient computed
2021-06-04 13:53:59 | [train_policy] epoch #462 | computing descent direction
2021-06-04 13:54:00 | [train_policy] epoch #462 | descent direction computed
2021-06-04 13:54:00 | [train_policy] epoch #462 | backtrack iters: 1
2021-06-04 13:54:00 | [train_policy] epoch #462 | optimization finished
2021-06-04 13:54:00 | [train_policy] epoch #462 | Computing KL after
2021-06-04 13:54:00 | [train_policy] epoch #462 | Computing loss after
2021-06-04 13:54:00 | [train_policy] epoch #462 | Fitting baseline...
2021-06-04 13:54:00 | [train_policy] epoch #462 | Saving snapshot...
2021-06-04 13:54:00 | [train_policy] epoch #462 | Saved
2021-06-04 13:54:00 | [train_policy] epoch #462 | Time 371.80 s
2021-06-04 13:54:00 | [train_policy] epoch #462 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284142
Evaluation/AverageDiscountedReturn          -44.0622
Evaluation/AverageReturn                    -44.0622
Evaluation/CompletionRate                     0
Evaluation/Iteration                        462
Evaluation/MaxReturn                        -29.4249
Evaluation/MinReturn                        -85.2865
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3487
Extras/EpisodeRewardMean                    -44.1329
LinearFeatureBaseline/ExplainedVariance       0.83949
PolicyExecTime                                0.2167
ProcessExecTime                               0.0311697
TotalEnvSteps                            468556
policy/Entropy                               -1.36553
policy/KL                                     0.00671943
policy/KLBefore                               0
policy/LossAfter                             -0.0206219
policy/LossBefore                            -1.17796e-08
policy/Perplexity                             0.255245
policy/dLoss                                  0.0206218
---------------------------------------  ----------------
2021-06-04 13:54:00 | [train_policy] epoch #463 | Obtaining samples for iteration 463...
2021-06-04 13:54:00 | [train_policy] epoch #463 | Logging diagnostics...
2021-06-04 13:54:00 | [train_policy] epoch #463 | Optimizing policy...
2021-06-04 13:54:00 | [train_policy] epoch #463 | Computing loss before
2021-06-04 13:54:00 | [train_policy] epoch #463 | Computing KL before
2021-06-04 13:54:00 | [train_policy] epoch #463 | Optimizing
2021-06-04 13:54:00 | [train_policy] epoch #463 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:00 | [train_policy] epoch #463 | computing loss before
2021-06-04 13:54:00 | [train_policy] epoch #463 | computing gradient
2021-06-04 13:54:00 | [train_policy] epoch #463 | gradient computed
2021-06-04 13:54:00 | [train_policy] epoch #463 | computing descent direction
2021-06-04 13:54:00 | [train_policy] epoch #463 | descent direction computed
2021-06-04 13:54:00 | [train_policy] epoch #463 | backtrack iters: 1
2021-06-04 13:54:00 | [train_policy] epoch #463 | optimization finished
2021-06-04 13:54:00 | [train_policy] epoch #463 | Computing KL after
2021-06-04 13:54:00 | [train_policy] epoch #463 | Computing loss after
2021-06-04 13:54:00 | [train_policy] epoch #463 | Fitting baseline...
2021-06-04 13:54:00 | [train_policy] epoch #463 | Saving snapshot...
2021-06-04 13:54:00 | [train_policy] epoch #463 | Saved
2021-06-04 13:54:00 | [train_policy] epoch #463 | Time 372.59 s
2021-06-04 13:54:00 | [train_policy] epoch #463 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288252
Evaluation/AverageDiscountedReturn          -42.5699
Evaluation/AverageReturn                    -42.5699
Evaluation/CompletionRate                     0
Evaluation/Iteration                        463
Evaluation/MaxReturn                        -32.4821
Evaluation/MinReturn                        -64.0829
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85365
Extras/EpisodeRewardMean                    -42.7485
LinearFeatureBaseline/ExplainedVariance       0.928706
PolicyExecTime                                0.213186
ProcessExecTime                               0.0316775
TotalEnvSteps                            469568
policy/Entropy                               -1.40516
policy/KL                                     0.00671463
policy/KLBefore                               0
policy/LossAfter                             -0.0128668
policy/LossBefore                            -2.56795e-08
policy/Perplexity                             0.245328
policy/dLoss                                  0.0128668
---------------------------------------  ----------------
2021-06-04 13:54:00 | [train_policy] epoch #464 | Obtaining samples for iteration 464...
2021-06-04 13:54:01 | [train_policy] epoch #464 | Logging diagnostics...
2021-06-04 13:54:01 | [train_policy] epoch #464 | Optimizing policy...
2021-06-04 13:54:01 | [train_policy] epoch #464 | Computing loss before
2021-06-04 13:54:01 | [train_policy] epoch #464 | Computing KL before
2021-06-04 13:54:01 | [train_policy] epoch #464 | Optimizing
2021-06-04 13:54:01 | [train_policy] epoch #464 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:01 | [train_policy] epoch #464 | computing loss before
2021-06-04 13:54:01 | [train_policy] epoch #464 | computing gradient
2021-06-04 13:54:01 | [train_policy] epoch #464 | gradient computed
2021-06-04 13:54:01 | [train_policy] epoch #464 | computing descent direction
2021-06-04 13:54:01 | [train_policy] epoch #464 | descent direction computed
2021-06-04 13:54:01 | [train_policy] epoch #464 | backtrack iters: 1
2021-06-04 13:54:01 | [train_policy] epoch #464 | optimization finished
2021-06-04 13:54:01 | [train_policy] epoch #464 | Computing KL after
2021-06-04 13:54:01 | [train_policy] epoch #464 | Computing loss after
2021-06-04 13:54:01 | [train_policy] epoch #464 | Fitting baseline...
2021-06-04 13:54:01 | [train_policy] epoch #464 | Saving snapshot...
2021-06-04 13:54:01 | [train_policy] epoch #464 | Saved
2021-06-04 13:54:01 | [train_policy] epoch #464 | Time 373.39 s
2021-06-04 13:54:01 | [train_policy] epoch #464 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286819
Evaluation/AverageDiscountedReturn          -42.5896
Evaluation/AverageReturn                    -42.5896
Evaluation/CompletionRate                     0
Evaluation/Iteration                        464
Evaluation/MaxReturn                        -29.5164
Evaluation/MinReturn                       -100.184
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.315
Extras/EpisodeRewardMean                    -42.4615
LinearFeatureBaseline/ExplainedVariance       0.810288
PolicyExecTime                                0.222846
ProcessExecTime                               0.0313332
TotalEnvSteps                            470580
policy/Entropy                               -1.41943
policy/KL                                     0.00661589
policy/KLBefore                               0
policy/LossAfter                             -0.0166607
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.241853
policy/dLoss                                  0.0166607
---------------------------------------  ----------------
2021-06-04 13:54:01 | [train_policy] epoch #465 | Obtaining samples for iteration 465...
2021-06-04 13:54:02 | [train_policy] epoch #465 | Logging diagnostics...
2021-06-04 13:54:02 | [train_policy] epoch #465 | Optimizing policy...
2021-06-04 13:54:02 | [train_policy] epoch #465 | Computing loss before
2021-06-04 13:54:02 | [train_policy] epoch #465 | Computing KL before
2021-06-04 13:54:02 | [train_policy] epoch #465 | Optimizing
2021-06-04 13:54:02 | [train_policy] epoch #465 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:02 | [train_policy] epoch #465 | computing loss before
2021-06-04 13:54:02 | [train_policy] epoch #465 | computing gradient
2021-06-04 13:54:02 | [train_policy] epoch #465 | gradient computed
2021-06-04 13:54:02 | [train_policy] epoch #465 | computing descent direction
2021-06-04 13:54:02 | [train_policy] epoch #465 | descent direction computed
2021-06-04 13:54:02 | [train_policy] epoch #465 | backtrack iters: 1
2021-06-04 13:54:02 | [train_policy] epoch #465 | optimization finished
2021-06-04 13:54:02 | [train_policy] epoch #465 | Computing KL after
2021-06-04 13:54:02 | [train_policy] epoch #465 | Computing loss after
2021-06-04 13:54:02 | [train_policy] epoch #465 | Fitting baseline...
2021-06-04 13:54:02 | [train_policy] epoch #465 | Saving snapshot...
2021-06-04 13:54:02 | [train_policy] epoch #465 | Saved
2021-06-04 13:54:02 | [train_policy] epoch #465 | Time 374.18 s
2021-06-04 13:54:02 | [train_policy] epoch #465 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.290413
Evaluation/AverageDiscountedReturn          -42.5292
Evaluation/AverageReturn                    -42.5292
Evaluation/CompletionRate                     0
Evaluation/Iteration                        465
Evaluation/MaxReturn                        -30.2914
Evaluation/MinReturn                        -89.0778
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.8267
Extras/EpisodeRewardMean                    -42.2585
LinearFeatureBaseline/ExplainedVariance       0.87636
PolicyExecTime                                0.217937
ProcessExecTime                               0.0317867
TotalEnvSteps                            471592
policy/Entropy                               -1.42378
policy/KL                                     0.0064955
policy/KLBefore                               0
policy/LossAfter                             -0.0191005
policy/LossBefore                             1.62558e-08
policy/Perplexity                             0.240802
policy/dLoss                                  0.0191005
---------------------------------------  ----------------
2021-06-04 13:54:02 | [train_policy] epoch #466 | Obtaining samples for iteration 466...
2021-06-04 13:54:03 | [train_policy] epoch #466 | Logging diagnostics...
2021-06-04 13:54:03 | [train_policy] epoch #466 | Optimizing policy...
2021-06-04 13:54:03 | [train_policy] epoch #466 | Computing loss before
2021-06-04 13:54:03 | [train_policy] epoch #466 | Computing KL before
2021-06-04 13:54:03 | [train_policy] epoch #466 | Optimizing
2021-06-04 13:54:03 | [train_policy] epoch #466 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:03 | [train_policy] epoch #466 | computing loss before
2021-06-04 13:54:03 | [train_policy] epoch #466 | computing gradient
2021-06-04 13:54:03 | [train_policy] epoch #466 | gradient computed
2021-06-04 13:54:03 | [train_policy] epoch #466 | computing descent direction
2021-06-04 13:54:03 | [train_policy] epoch #466 | descent direction computed
2021-06-04 13:54:03 | [train_policy] epoch #466 | backtrack iters: 0
2021-06-04 13:54:03 | [train_policy] epoch #466 | optimization finished
2021-06-04 13:54:03 | [train_policy] epoch #466 | Computing KL after
2021-06-04 13:54:03 | [train_policy] epoch #466 | Computing loss after
2021-06-04 13:54:03 | [train_policy] epoch #466 | Fitting baseline...
2021-06-04 13:54:03 | [train_policy] epoch #466 | Saving snapshot...
2021-06-04 13:54:03 | [train_policy] epoch #466 | Saved
2021-06-04 13:54:03 | [train_policy] epoch #466 | Time 374.99 s
2021-06-04 13:54:03 | [train_policy] epoch #466 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286862
Evaluation/AverageDiscountedReturn          -41.6405
Evaluation/AverageReturn                    -41.6405
Evaluation/CompletionRate                     0
Evaluation/Iteration                        466
Evaluation/MaxReturn                        -30.3408
Evaluation/MinReturn                        -64.0583
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.04236
Extras/EpisodeRewardMean                    -41.807
LinearFeatureBaseline/ExplainedVariance       0.871851
PolicyExecTime                                0.234409
ProcessExecTime                               0.0313256
TotalEnvSteps                            472604
policy/Entropy                               -1.42176
policy/KL                                     0.00997657
policy/KLBefore                               0
policy/LossAfter                             -0.0250783
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.241289
policy/dLoss                                  0.0250783
---------------------------------------  ----------------
2021-06-04 13:54:03 | [train_policy] epoch #467 | Obtaining samples for iteration 467...
2021-06-04 13:54:03 | [train_policy] epoch #467 | Logging diagnostics...
2021-06-04 13:54:03 | [train_policy] epoch #467 | Optimizing policy...
2021-06-04 13:54:03 | [train_policy] epoch #467 | Computing loss before
2021-06-04 13:54:03 | [train_policy] epoch #467 | Computing KL before
2021-06-04 13:54:03 | [train_policy] epoch #467 | Optimizing
2021-06-04 13:54:03 | [train_policy] epoch #467 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:03 | [train_policy] epoch #467 | computing loss before
2021-06-04 13:54:03 | [train_policy] epoch #467 | computing gradient
2021-06-04 13:54:03 | [train_policy] epoch #467 | gradient computed
2021-06-04 13:54:03 | [train_policy] epoch #467 | computing descent direction
2021-06-04 13:54:03 | [train_policy] epoch #467 | descent direction computed
2021-06-04 13:54:04 | [train_policy] epoch #467 | backtrack iters: 1
2021-06-04 13:54:04 | [train_policy] epoch #467 | optimization finished
2021-06-04 13:54:04 | [train_policy] epoch #467 | Computing KL after
2021-06-04 13:54:04 | [train_policy] epoch #467 | Computing loss after
2021-06-04 13:54:04 | [train_policy] epoch #467 | Fitting baseline...
2021-06-04 13:54:04 | [train_policy] epoch #467 | Saving snapshot...
2021-06-04 13:54:04 | [train_policy] epoch #467 | Saved
2021-06-04 13:54:04 | [train_policy] epoch #467 | Time 375.78 s
2021-06-04 13:54:04 | [train_policy] epoch #467 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.28639
Evaluation/AverageDiscountedReturn          -42.245
Evaluation/AverageReturn                    -42.245
Evaluation/CompletionRate                     0
Evaluation/Iteration                        467
Evaluation/MaxReturn                        -29.903
Evaluation/MinReturn                        -84.3465
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.75883
Extras/EpisodeRewardMean                    -41.7005
LinearFeatureBaseline/ExplainedVariance       0.848054
PolicyExecTime                                0.221488
ProcessExecTime                               0.0312703
TotalEnvSteps                            473616
policy/Entropy                               -1.41685
policy/KL                                     0.00646676
policy/KLBefore                               0
policy/LossAfter                             -0.0178156
policy/LossBefore                            -1.7905e-08
policy/Perplexity                             0.242477
policy/dLoss                                  0.0178156
---------------------------------------  ---------------
2021-06-04 13:54:04 | [train_policy] epoch #468 | Obtaining samples for iteration 468...
2021-06-04 13:54:04 | [train_policy] epoch #468 | Logging diagnostics...
2021-06-04 13:54:04 | [train_policy] epoch #468 | Optimizing policy...
2021-06-04 13:54:04 | [train_policy] epoch #468 | Computing loss before
2021-06-04 13:54:04 | [train_policy] epoch #468 | Computing KL before
2021-06-04 13:54:04 | [train_policy] epoch #468 | Optimizing
2021-06-04 13:54:04 | [train_policy] epoch #468 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:04 | [train_policy] epoch #468 | computing loss before
2021-06-04 13:54:04 | [train_policy] epoch #468 | computing gradient
2021-06-04 13:54:04 | [train_policy] epoch #468 | gradient computed
2021-06-04 13:54:04 | [train_policy] epoch #468 | computing descent direction
2021-06-04 13:54:04 | [train_policy] epoch #468 | descent direction computed
2021-06-04 13:54:04 | [train_policy] epoch #468 | backtrack iters: 1
2021-06-04 13:54:04 | [train_policy] epoch #468 | optimization finished
2021-06-04 13:54:04 | [train_policy] epoch #468 | Computing KL after
2021-06-04 13:54:04 | [train_policy] epoch #468 | Computing loss after
2021-06-04 13:54:04 | [train_policy] epoch #468 | Fitting baseline...
2021-06-04 13:54:04 | [train_policy] epoch #468 | Saving snapshot...
2021-06-04 13:54:04 | [train_policy] epoch #468 | Saved
2021-06-04 13:54:04 | [train_policy] epoch #468 | Time 376.58 s
2021-06-04 13:54:04 | [train_policy] epoch #468 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285508
Evaluation/AverageDiscountedReturn          -42.8943
Evaluation/AverageReturn                    -42.8943
Evaluation/CompletionRate                     0
Evaluation/Iteration                        468
Evaluation/MaxReturn                        -31.5021
Evaluation/MinReturn                        -62.9459
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.81303
Extras/EpisodeRewardMean                    -42.7372
LinearFeatureBaseline/ExplainedVariance       0.905017
PolicyExecTime                                0.22967
ProcessExecTime                               0.0312507
TotalEnvSteps                            474628
policy/Entropy                               -1.40881
policy/KL                                     0.00642646
policy/KLBefore                               0
policy/LossAfter                             -0.0107695
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.244435
policy/dLoss                                  0.0107695
---------------------------------------  ----------------
2021-06-04 13:54:04 | [train_policy] epoch #469 | Obtaining samples for iteration 469...
2021-06-04 13:54:05 | [train_policy] epoch #469 | Logging diagnostics...
2021-06-04 13:54:05 | [train_policy] epoch #469 | Optimizing policy...
2021-06-04 13:54:05 | [train_policy] epoch #469 | Computing loss before
2021-06-04 13:54:05 | [train_policy] epoch #469 | Computing KL before
2021-06-04 13:54:05 | [train_policy] epoch #469 | Optimizing
2021-06-04 13:54:05 | [train_policy] epoch #469 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:05 | [train_policy] epoch #469 | computing loss before
2021-06-04 13:54:05 | [train_policy] epoch #469 | computing gradient
2021-06-04 13:54:05 | [train_policy] epoch #469 | gradient computed
2021-06-04 13:54:05 | [train_policy] epoch #469 | computing descent direction
2021-06-04 13:54:05 | [train_policy] epoch #469 | descent direction computed
2021-06-04 13:54:05 | [train_policy] epoch #469 | backtrack iters: 1
2021-06-04 13:54:05 | [train_policy] epoch #469 | optimization finished
2021-06-04 13:54:05 | [train_policy] epoch #469 | Computing KL after
2021-06-04 13:54:05 | [train_policy] epoch #469 | Computing loss after
2021-06-04 13:54:05 | [train_policy] epoch #469 | Fitting baseline...
2021-06-04 13:54:05 | [train_policy] epoch #469 | Saving snapshot...
2021-06-04 13:54:05 | [train_policy] epoch #469 | Saved
2021-06-04 13:54:05 | [train_policy] epoch #469 | Time 377.40 s
2021-06-04 13:54:05 | [train_policy] epoch #469 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287869
Evaluation/AverageDiscountedReturn          -42.8656
Evaluation/AverageReturn                    -42.8656
Evaluation/CompletionRate                     0
Evaluation/Iteration                        469
Evaluation/MaxReturn                        -29.9321
Evaluation/MinReturn                       -107.1
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.99277
Extras/EpisodeRewardMean                    -42.9731
LinearFeatureBaseline/ExplainedVariance       0.810688
PolicyExecTime                                0.233901
ProcessExecTime                               0.0314317
TotalEnvSteps                            475640
policy/Entropy                               -1.40466
policy/KL                                     0.00657227
policy/KLBefore                               0
policy/LossAfter                             -0.0233744
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.24545
policy/dLoss                                  0.0233744
---------------------------------------  ----------------
2021-06-04 13:54:05 | [train_policy] epoch #470 | Obtaining samples for iteration 470...
2021-06-04 13:54:06 | [train_policy] epoch #470 | Logging diagnostics...
2021-06-04 13:54:06 | [train_policy] epoch #470 | Optimizing policy...
2021-06-04 13:54:06 | [train_policy] epoch #470 | Computing loss before
2021-06-04 13:54:06 | [train_policy] epoch #470 | Computing KL before
2021-06-04 13:54:06 | [train_policy] epoch #470 | Optimizing
2021-06-04 13:54:06 | [train_policy] epoch #470 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:06 | [train_policy] epoch #470 | computing loss before
2021-06-04 13:54:06 | [train_policy] epoch #470 | computing gradient
2021-06-04 13:54:06 | [train_policy] epoch #470 | gradient computed
2021-06-04 13:54:06 | [train_policy] epoch #470 | computing descent direction
2021-06-04 13:54:06 | [train_policy] epoch #470 | descent direction computed
2021-06-04 13:54:06 | [train_policy] epoch #470 | backtrack iters: 0
2021-06-04 13:54:06 | [train_policy] epoch #470 | optimization finished
2021-06-04 13:54:06 | [train_policy] epoch #470 | Computing KL after
2021-06-04 13:54:06 | [train_policy] epoch #470 | Computing loss after
2021-06-04 13:54:06 | [train_policy] epoch #470 | Fitting baseline...
2021-06-04 13:54:06 | [train_policy] epoch #470 | Saving snapshot...
2021-06-04 13:54:06 | [train_policy] epoch #470 | Saved
2021-06-04 13:54:06 | [train_policy] epoch #470 | Time 378.19 s
2021-06-04 13:54:06 | [train_policy] epoch #470 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287652
Evaluation/AverageDiscountedReturn          -41.8529
Evaluation/AverageReturn                    -41.8529
Evaluation/CompletionRate                     0
Evaluation/Iteration                        470
Evaluation/MaxReturn                        -30.9888
Evaluation/MinReturn                        -86.3924
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.392
Extras/EpisodeRewardMean                    -41.7621
LinearFeatureBaseline/ExplainedVariance       0.86014
PolicyExecTime                                0.227844
ProcessExecTime                               0.0314858
TotalEnvSteps                            476652
policy/Entropy                               -1.41026
policy/KL                                     0.0098838
policy/KLBefore                               0
policy/LossAfter                             -0.0163096
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.24408
policy/dLoss                                  0.0163096
---------------------------------------  ----------------
2021-06-04 13:54:06 | [train_policy] epoch #471 | Obtaining samples for iteration 471...
2021-06-04 13:54:07 | [train_policy] epoch #471 | Logging diagnostics...
2021-06-04 13:54:07 | [train_policy] epoch #471 | Optimizing policy...
2021-06-04 13:54:07 | [train_policy] epoch #471 | Computing loss before
2021-06-04 13:54:07 | [train_policy] epoch #471 | Computing KL before
2021-06-04 13:54:07 | [train_policy] epoch #471 | Optimizing
2021-06-04 13:54:07 | [train_policy] epoch #471 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:07 | [train_policy] epoch #471 | computing loss before
2021-06-04 13:54:07 | [train_policy] epoch #471 | computing gradient
2021-06-04 13:54:07 | [train_policy] epoch #471 | gradient computed
2021-06-04 13:54:07 | [train_policy] epoch #471 | computing descent direction
2021-06-04 13:54:07 | [train_policy] epoch #471 | descent direction computed
2021-06-04 13:54:07 | [train_policy] epoch #471 | backtrack iters: 0
2021-06-04 13:54:07 | [train_policy] epoch #471 | optimization finished
2021-06-04 13:54:07 | [train_policy] epoch #471 | Computing KL after
2021-06-04 13:54:07 | [train_policy] epoch #471 | Computing loss after
2021-06-04 13:54:07 | [train_policy] epoch #471 | Fitting baseline...
2021-06-04 13:54:07 | [train_policy] epoch #471 | Saving snapshot...
2021-06-04 13:54:07 | [train_policy] epoch #471 | Saved
2021-06-04 13:54:07 | [train_policy] epoch #471 | Time 378.96 s
2021-06-04 13:54:07 | [train_policy] epoch #471 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284522
Evaluation/AverageDiscountedReturn          -44.0323
Evaluation/AverageReturn                    -44.0323
Evaluation/CompletionRate                     0
Evaluation/Iteration                        471
Evaluation/MaxReturn                        -30.8652
Evaluation/MinReturn                        -94.3737
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.225
Extras/EpisodeRewardMean                    -44.1383
LinearFeatureBaseline/ExplainedVariance       0.834334
PolicyExecTime                                0.216787
ProcessExecTime                               0.031112
TotalEnvSteps                            477664
policy/Entropy                               -1.3733
policy/KL                                     0.00967959
policy/KLBefore                               0
policy/LossAfter                             -0.0188955
policy/LossBefore                            -1.27219e-08
policy/Perplexity                             0.25327
policy/dLoss                                  0.0188955
---------------------------------------  ----------------
2021-06-04 13:54:07 | [train_policy] epoch #472 | Obtaining samples for iteration 472...
2021-06-04 13:54:07 | [train_policy] epoch #472 | Logging diagnostics...
2021-06-04 13:54:07 | [train_policy] epoch #472 | Optimizing policy...
2021-06-04 13:54:07 | [train_policy] epoch #472 | Computing loss before
2021-06-04 13:54:07 | [train_policy] epoch #472 | Computing KL before
2021-06-04 13:54:07 | [train_policy] epoch #472 | Optimizing
2021-06-04 13:54:07 | [train_policy] epoch #472 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:07 | [train_policy] epoch #472 | computing loss before
2021-06-04 13:54:07 | [train_policy] epoch #472 | computing gradient
2021-06-04 13:54:07 | [train_policy] epoch #472 | gradient computed
2021-06-04 13:54:07 | [train_policy] epoch #472 | computing descent direction
2021-06-04 13:54:07 | [train_policy] epoch #472 | descent direction computed
2021-06-04 13:54:07 | [train_policy] epoch #472 | backtrack iters: 1
2021-06-04 13:54:07 | [train_policy] epoch #472 | optimization finished
2021-06-04 13:54:07 | [train_policy] epoch #472 | Computing KL after
2021-06-04 13:54:07 | [train_policy] epoch #472 | Computing loss after
2021-06-04 13:54:08 | [train_policy] epoch #472 | Fitting baseline...
2021-06-04 13:54:08 | [train_policy] epoch #472 | Saving snapshot...
2021-06-04 13:54:08 | [train_policy] epoch #472 | Saved
2021-06-04 13:54:08 | [train_policy] epoch #472 | Time 379.78 s
2021-06-04 13:54:08 | [train_policy] epoch #472 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284974
Evaluation/AverageDiscountedReturn          -43.7934
Evaluation/AverageReturn                    -43.7934
Evaluation/CompletionRate                     0
Evaluation/Iteration                        472
Evaluation/MaxReturn                        -30.1286
Evaluation/MinReturn                        -91.6398
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1056
Extras/EpisodeRewardMean                    -44.048
LinearFeatureBaseline/ExplainedVariance       0.870445
PolicyExecTime                                0.230222
ProcessExecTime                               0.0310543
TotalEnvSteps                            478676
policy/Entropy                               -1.35398
policy/KL                                     0.00651272
policy/KLBefore                               0
policy/LossAfter                             -0.0186128
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.258209
policy/dLoss                                  0.0186128
---------------------------------------  ----------------
2021-06-04 13:54:08 | [train_policy] epoch #473 | Obtaining samples for iteration 473...
2021-06-04 13:54:08 | [train_policy] epoch #473 | Logging diagnostics...
2021-06-04 13:54:08 | [train_policy] epoch #473 | Optimizing policy...
2021-06-04 13:54:08 | [train_policy] epoch #473 | Computing loss before
2021-06-04 13:54:08 | [train_policy] epoch #473 | Computing KL before
2021-06-04 13:54:08 | [train_policy] epoch #473 | Optimizing
2021-06-04 13:54:08 | [train_policy] epoch #473 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:08 | [train_policy] epoch #473 | computing loss before
2021-06-04 13:54:08 | [train_policy] epoch #473 | computing gradient
2021-06-04 13:54:08 | [train_policy] epoch #473 | gradient computed
2021-06-04 13:54:08 | [train_policy] epoch #473 | computing descent direction
2021-06-04 13:54:08 | [train_policy] epoch #473 | descent direction computed
2021-06-04 13:54:08 | [train_policy] epoch #473 | backtrack iters: 0
2021-06-04 13:54:08 | [train_policy] epoch #473 | optimization finished
2021-06-04 13:54:08 | [train_policy] epoch #473 | Computing KL after
2021-06-04 13:54:08 | [train_policy] epoch #473 | Computing loss after
2021-06-04 13:54:08 | [train_policy] epoch #473 | Fitting baseline...
2021-06-04 13:54:08 | [train_policy] epoch #473 | Saving snapshot...
2021-06-04 13:54:08 | [train_policy] epoch #473 | Saved
2021-06-04 13:54:08 | [train_policy] epoch #473 | Time 380.57 s
2021-06-04 13:54:08 | [train_policy] epoch #473 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285559
Evaluation/AverageDiscountedReturn          -42.595
Evaluation/AverageReturn                    -42.595
Evaluation/CompletionRate                     0
Evaluation/Iteration                        473
Evaluation/MaxReturn                        -30.4454
Evaluation/MinReturn                        -88.0017
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.52149
Extras/EpisodeRewardMean                    -42.7223
LinearFeatureBaseline/ExplainedVariance       0.888541
PolicyExecTime                                0.217356
ProcessExecTime                               0.0312951
TotalEnvSteps                            479688
policy/Entropy                               -1.29804
policy/KL                                     0.00941539
policy/KLBefore                               0
policy/LossAfter                             -0.0136396
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             0.273065
policy/dLoss                                  0.0136396
---------------------------------------  ----------------
2021-06-04 13:54:08 | [train_policy] epoch #474 | Obtaining samples for iteration 474...
2021-06-04 13:54:09 | [train_policy] epoch #474 | Logging diagnostics...
2021-06-04 13:54:09 | [train_policy] epoch #474 | Optimizing policy...
2021-06-04 13:54:09 | [train_policy] epoch #474 | Computing loss before
2021-06-04 13:54:09 | [train_policy] epoch #474 | Computing KL before
2021-06-04 13:54:09 | [train_policy] epoch #474 | Optimizing
2021-06-04 13:54:09 | [train_policy] epoch #474 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:09 | [train_policy] epoch #474 | computing loss before
2021-06-04 13:54:09 | [train_policy] epoch #474 | computing gradient
2021-06-04 13:54:09 | [train_policy] epoch #474 | gradient computed
2021-06-04 13:54:09 | [train_policy] epoch #474 | computing descent direction
2021-06-04 13:54:09 | [train_policy] epoch #474 | descent direction computed
2021-06-04 13:54:09 | [train_policy] epoch #474 | backtrack iters: 1
2021-06-04 13:54:09 | [train_policy] epoch #474 | optimization finished
2021-06-04 13:54:09 | [train_policy] epoch #474 | Computing KL after
2021-06-04 13:54:09 | [train_policy] epoch #474 | Computing loss after
2021-06-04 13:54:09 | [train_policy] epoch #474 | Fitting baseline...
2021-06-04 13:54:09 | [train_policy] epoch #474 | Saving snapshot...
2021-06-04 13:54:09 | [train_policy] epoch #474 | Saved
2021-06-04 13:54:09 | [train_policy] epoch #474 | Time 381.38 s
2021-06-04 13:54:09 | [train_policy] epoch #474 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.28429
Evaluation/AverageDiscountedReturn          -43.3239
Evaluation/AverageReturn                    -43.3239
Evaluation/CompletionRate                     0
Evaluation/Iteration                        474
Evaluation/MaxReturn                        -29.4681
Evaluation/MinReturn                       -109.957
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.376
Extras/EpisodeRewardMean                    -42.9811
LinearFeatureBaseline/ExplainedVariance       0.802139
PolicyExecTime                                0.23473
ProcessExecTime                               0.0311329
TotalEnvSteps                            480700
policy/Entropy                               -1.31067
policy/KL                                     0.00660604
policy/KLBefore                               0
policy/LossAfter                             -0.0228501
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.269638
policy/dLoss                                  0.0228501
---------------------------------------  ----------------
2021-06-04 13:54:09 | [train_policy] epoch #475 | Obtaining samples for iteration 475...
2021-06-04 13:54:10 | [train_policy] epoch #475 | Logging diagnostics...
2021-06-04 13:54:10 | [train_policy] epoch #475 | Optimizing policy...
2021-06-04 13:54:10 | [train_policy] epoch #475 | Computing loss before
2021-06-04 13:54:10 | [train_policy] epoch #475 | Computing KL before
2021-06-04 13:54:10 | [train_policy] epoch #475 | Optimizing
2021-06-04 13:54:10 | [train_policy] epoch #475 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:10 | [train_policy] epoch #475 | computing loss before
2021-06-04 13:54:10 | [train_policy] epoch #475 | computing gradient
2021-06-04 13:54:10 | [train_policy] epoch #475 | gradient computed
2021-06-04 13:54:10 | [train_policy] epoch #475 | computing descent direction
2021-06-04 13:54:10 | [train_policy] epoch #475 | descent direction computed
2021-06-04 13:54:10 | [train_policy] epoch #475 | backtrack iters: 0
2021-06-04 13:54:10 | [train_policy] epoch #475 | optimization finished
2021-06-04 13:54:10 | [train_policy] epoch #475 | Computing KL after
2021-06-04 13:54:10 | [train_policy] epoch #475 | Computing loss after
2021-06-04 13:54:10 | [train_policy] epoch #475 | Fitting baseline...
2021-06-04 13:54:10 | [train_policy] epoch #475 | Saving snapshot...
2021-06-04 13:54:10 | [train_policy] epoch #475 | Saved
2021-06-04 13:54:10 | [train_policy] epoch #475 | Time 382.17 s
2021-06-04 13:54:10 | [train_policy] epoch #475 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28402
Evaluation/AverageDiscountedReturn          -42.213
Evaluation/AverageReturn                    -42.213
Evaluation/CompletionRate                     0
Evaluation/Iteration                        475
Evaluation/MaxReturn                        -29.9608
Evaluation/MinReturn                        -82.4056
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.97308
Extras/EpisodeRewardMean                    -41.8388
LinearFeatureBaseline/ExplainedVariance       0.878928
PolicyExecTime                                0.220774
ProcessExecTime                               0.0310035
TotalEnvSteps                            481712
policy/Entropy                               -1.29449
policy/KL                                     0.00987949
policy/KLBefore                               0
policy/LossAfter                             -0.0189718
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.274037
policy/dLoss                                  0.0189718
---------------------------------------  ----------------
2021-06-04 13:54:10 | [train_policy] epoch #476 | Obtaining samples for iteration 476...
2021-06-04 13:54:11 | [train_policy] epoch #476 | Logging diagnostics...
2021-06-04 13:54:11 | [train_policy] epoch #476 | Optimizing policy...
2021-06-04 13:54:11 | [train_policy] epoch #476 | Computing loss before
2021-06-04 13:54:11 | [train_policy] epoch #476 | Computing KL before
2021-06-04 13:54:11 | [train_policy] epoch #476 | Optimizing
2021-06-04 13:54:11 | [train_policy] epoch #476 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:11 | [train_policy] epoch #476 | computing loss before
2021-06-04 13:54:11 | [train_policy] epoch #476 | computing gradient
2021-06-04 13:54:11 | [train_policy] epoch #476 | gradient computed
2021-06-04 13:54:11 | [train_policy] epoch #476 | computing descent direction
2021-06-04 13:54:11 | [train_policy] epoch #476 | descent direction computed
2021-06-04 13:54:11 | [train_policy] epoch #476 | backtrack iters: 1
2021-06-04 13:54:11 | [train_policy] epoch #476 | optimization finished
2021-06-04 13:54:11 | [train_policy] epoch #476 | Computing KL after
2021-06-04 13:54:11 | [train_policy] epoch #476 | Computing loss after
2021-06-04 13:54:11 | [train_policy] epoch #476 | Fitting baseline...
2021-06-04 13:54:11 | [train_policy] epoch #476 | Saving snapshot...
2021-06-04 13:54:11 | [train_policy] epoch #476 | Saved
2021-06-04 13:54:11 | [train_policy] epoch #476 | Time 382.96 s
2021-06-04 13:54:11 | [train_policy] epoch #476 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285432
Evaluation/AverageDiscountedReturn          -42.5883
Evaluation/AverageReturn                    -42.5883
Evaluation/CompletionRate                     0
Evaluation/Iteration                        476
Evaluation/MaxReturn                        -28.5435
Evaluation/MinReturn                        -83.3316
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77206
Extras/EpisodeRewardMean                    -42.1422
LinearFeatureBaseline/ExplainedVariance       0.877922
PolicyExecTime                                0.210537
ProcessExecTime                               0.0312593
TotalEnvSteps                            482724
policy/Entropy                               -1.33345
policy/KL                                     0.00656913
policy/KLBefore                               0
policy/LossAfter                             -0.0204621
policy/LossBefore                            -1.06016e-08
policy/Perplexity                             0.263565
policy/dLoss                                  0.0204621
---------------------------------------  ----------------
2021-06-04 13:54:11 | [train_policy] epoch #477 | Obtaining samples for iteration 477...
2021-06-04 13:54:11 | [train_policy] epoch #477 | Logging diagnostics...
2021-06-04 13:54:11 | [train_policy] epoch #477 | Optimizing policy...
2021-06-04 13:54:11 | [train_policy] epoch #477 | Computing loss before
2021-06-04 13:54:11 | [train_policy] epoch #477 | Computing KL before
2021-06-04 13:54:11 | [train_policy] epoch #477 | Optimizing
2021-06-04 13:54:11 | [train_policy] epoch #477 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:11 | [train_policy] epoch #477 | computing loss before
2021-06-04 13:54:11 | [train_policy] epoch #477 | computing gradient
2021-06-04 13:54:11 | [train_policy] epoch #477 | gradient computed
2021-06-04 13:54:11 | [train_policy] epoch #477 | computing descent direction
2021-06-04 13:54:11 | [train_policy] epoch #477 | descent direction computed
2021-06-04 13:54:11 | [train_policy] epoch #477 | backtrack iters: 0
2021-06-04 13:54:11 | [train_policy] epoch #477 | optimization finished
2021-06-04 13:54:11 | [train_policy] epoch #477 | Computing KL after
2021-06-04 13:54:11 | [train_policy] epoch #477 | Computing loss after
2021-06-04 13:54:11 | [train_policy] epoch #477 | Fitting baseline...
2021-06-04 13:54:11 | [train_policy] epoch #477 | Saving snapshot...
2021-06-04 13:54:12 | [train_policy] epoch #477 | Saved
2021-06-04 13:54:12 | [train_policy] epoch #477 | Time 383.75 s
2021-06-04 13:54:12 | [train_policy] epoch #477 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.284028
Evaluation/AverageDiscountedReturn          -63.8879
Evaluation/AverageReturn                    -63.8879
Evaluation/CompletionRate                     0
Evaluation/Iteration                        477
Evaluation/MaxReturn                        -30.2289
Evaluation/MinReturn                      -1497.2
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        158.492
Extras/EpisodeRewardMean                    -62.0695
LinearFeatureBaseline/ExplainedVariance       0.0246567
PolicyExecTime                                0.225763
ProcessExecTime                               0.0311272
TotalEnvSteps                            483736
policy/Entropy                               -1.30906
policy/KL                                     0.00960059
policy/KLBefore                               0
policy/LossAfter                             -0.0252435
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.270074
policy/dLoss                                  0.0252435
---------------------------------------  ---------------
2021-06-04 13:54:12 | [train_policy] epoch #478 | Obtaining samples for iteration 478...
2021-06-04 13:54:12 | [train_policy] epoch #478 | Logging diagnostics...
2021-06-04 13:54:12 | [train_policy] epoch #478 | Optimizing policy...
2021-06-04 13:54:12 | [train_policy] epoch #478 | Computing loss before
2021-06-04 13:54:12 | [train_policy] epoch #478 | Computing KL before
2021-06-04 13:54:12 | [train_policy] epoch #478 | Optimizing
2021-06-04 13:54:12 | [train_policy] epoch #478 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:12 | [train_policy] epoch #478 | computing loss before
2021-06-04 13:54:12 | [train_policy] epoch #478 | computing gradient
2021-06-04 13:54:12 | [train_policy] epoch #478 | gradient computed
2021-06-04 13:54:12 | [train_policy] epoch #478 | computing descent direction
2021-06-04 13:54:12 | [train_policy] epoch #478 | descent direction computed
2021-06-04 13:54:12 | [train_policy] epoch #478 | backtrack iters: 1
2021-06-04 13:54:12 | [train_policy] epoch #478 | optimization finished
2021-06-04 13:54:12 | [train_policy] epoch #478 | Computing KL after
2021-06-04 13:54:12 | [train_policy] epoch #478 | Computing loss after
2021-06-04 13:54:12 | [train_policy] epoch #478 | Fitting baseline...
2021-06-04 13:54:12 | [train_policy] epoch #478 | Saving snapshot...
2021-06-04 13:54:12 | [train_policy] epoch #478 | Saved
2021-06-04 13:54:12 | [train_policy] epoch #478 | Time 384.55 s
2021-06-04 13:54:12 | [train_policy] epoch #478 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285648
Evaluation/AverageDiscountedReturn          -41.6338
Evaluation/AverageReturn                    -41.6338
Evaluation/CompletionRate                     0
Evaluation/Iteration                        478
Evaluation/MaxReturn                        -29.3744
Evaluation/MinReturn                        -63.9098
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.80111
Extras/EpisodeRewardMean                    -41.5768
LinearFeatureBaseline/ExplainedVariance     -11.2232
PolicyExecTime                                0.225099
ProcessExecTime                               0.0314748
TotalEnvSteps                            484748
policy/Entropy                               -1.32194
policy/KL                                     0.00681827
policy/KLBefore                               0
policy/LossAfter                             -0.023734
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.266617
policy/dLoss                                  0.023734
---------------------------------------  ----------------
2021-06-04 13:54:12 | [train_policy] epoch #479 | Obtaining samples for iteration 479...
2021-06-04 13:54:13 | [train_policy] epoch #479 | Logging diagnostics...
2021-06-04 13:54:13 | [train_policy] epoch #479 | Optimizing policy...
2021-06-04 13:54:13 | [train_policy] epoch #479 | Computing loss before
2021-06-04 13:54:13 | [train_policy] epoch #479 | Computing KL before
2021-06-04 13:54:13 | [train_policy] epoch #479 | Optimizing
2021-06-04 13:54:13 | [train_policy] epoch #479 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:13 | [train_policy] epoch #479 | computing loss before
2021-06-04 13:54:13 | [train_policy] epoch #479 | computing gradient
2021-06-04 13:54:13 | [train_policy] epoch #479 | gradient computed
2021-06-04 13:54:13 | [train_policy] epoch #479 | computing descent direction
2021-06-04 13:54:13 | [train_policy] epoch #479 | descent direction computed
2021-06-04 13:54:13 | [train_policy] epoch #479 | backtrack iters: 1
2021-06-04 13:54:13 | [train_policy] epoch #479 | optimization finished
2021-06-04 13:54:13 | [train_policy] epoch #479 | Computing KL after
2021-06-04 13:54:13 | [train_policy] epoch #479 | Computing loss after
2021-06-04 13:54:13 | [train_policy] epoch #479 | Fitting baseline...
2021-06-04 13:54:13 | [train_policy] epoch #479 | Saving snapshot...
2021-06-04 13:54:13 | [train_policy] epoch #479 | Saved
2021-06-04 13:54:13 | [train_policy] epoch #479 | Time 385.35 s
2021-06-04 13:54:13 | [train_policy] epoch #479 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284785
Evaluation/AverageDiscountedReturn          -43.2861
Evaluation/AverageReturn                    -43.2861
Evaluation/CompletionRate                     0
Evaluation/Iteration                        479
Evaluation/MaxReturn                        -29.7926
Evaluation/MinReturn                        -90.0306
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.4228
Extras/EpisodeRewardMean                    -43.1408
LinearFeatureBaseline/ExplainedVariance       0.838036
PolicyExecTime                                0.226589
ProcessExecTime                               0.0311735
TotalEnvSteps                            485760
policy/Entropy                               -1.32012
policy/KL                                     0.00645548
policy/KLBefore                               0
policy/LossAfter                             -0.0213827
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.267103
policy/dLoss                                  0.0213827
---------------------------------------  ----------------
2021-06-04 13:54:13 | [train_policy] epoch #480 | Obtaining samples for iteration 480...
2021-06-04 13:54:14 | [train_policy] epoch #480 | Logging diagnostics...
2021-06-04 13:54:14 | [train_policy] epoch #480 | Optimizing policy...
2021-06-04 13:54:14 | [train_policy] epoch #480 | Computing loss before
2021-06-04 13:54:14 | [train_policy] epoch #480 | Computing KL before
2021-06-04 13:54:14 | [train_policy] epoch #480 | Optimizing
2021-06-04 13:54:14 | [train_policy] epoch #480 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:14 | [train_policy] epoch #480 | computing loss before
2021-06-04 13:54:14 | [train_policy] epoch #480 | computing gradient
2021-06-04 13:54:14 | [train_policy] epoch #480 | gradient computed
2021-06-04 13:54:14 | [train_policy] epoch #480 | computing descent direction
2021-06-04 13:54:14 | [train_policy] epoch #480 | descent direction computed
2021-06-04 13:54:14 | [train_policy] epoch #480 | backtrack iters: 0
2021-06-04 13:54:14 | [train_policy] epoch #480 | optimization finished
2021-06-04 13:54:14 | [train_policy] epoch #480 | Computing KL after
2021-06-04 13:54:14 | [train_policy] epoch #480 | Computing loss after
2021-06-04 13:54:14 | [train_policy] epoch #480 | Fitting baseline...
2021-06-04 13:54:14 | [train_policy] epoch #480 | Saving snapshot...
2021-06-04 13:54:14 | [train_policy] epoch #480 | Saved
2021-06-04 13:54:14 | [train_policy] epoch #480 | Time 386.13 s
2021-06-04 13:54:14 | [train_policy] epoch #480 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284827
Evaluation/AverageDiscountedReturn          -42.9552
Evaluation/AverageReturn                    -42.9552
Evaluation/CompletionRate                     0
Evaluation/Iteration                        480
Evaluation/MaxReturn                        -27.76
Evaluation/MinReturn                        -63.9825
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.35002
Extras/EpisodeRewardMean                    -43.1107
LinearFeatureBaseline/ExplainedVariance       0.87161
PolicyExecTime                                0.21454
ProcessExecTime                               0.0319118
TotalEnvSteps                            486772
policy/Entropy                               -1.32405
policy/KL                                     0.00997334
policy/KLBefore                               0
policy/LossAfter                             -0.0246178
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.266055
policy/dLoss                                  0.0246178
---------------------------------------  ----------------
2021-06-04 13:54:14 | [train_policy] epoch #481 | Obtaining samples for iteration 481...
2021-06-04 13:54:15 | [train_policy] epoch #481 | Logging diagnostics...
2021-06-04 13:54:15 | [train_policy] epoch #481 | Optimizing policy...
2021-06-04 13:54:15 | [train_policy] epoch #481 | Computing loss before
2021-06-04 13:54:15 | [train_policy] epoch #481 | Computing KL before
2021-06-04 13:54:15 | [train_policy] epoch #481 | Optimizing
2021-06-04 13:54:15 | [train_policy] epoch #481 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:15 | [train_policy] epoch #481 | computing loss before
2021-06-04 13:54:15 | [train_policy] epoch #481 | computing gradient
2021-06-04 13:54:15 | [train_policy] epoch #481 | gradient computed
2021-06-04 13:54:15 | [train_policy] epoch #481 | computing descent direction
2021-06-04 13:54:15 | [train_policy] epoch #481 | descent direction computed
2021-06-04 13:54:15 | [train_policy] epoch #481 | backtrack iters: 0
2021-06-04 13:54:15 | [train_policy] epoch #481 | optimization finished
2021-06-04 13:54:15 | [train_policy] epoch #481 | Computing KL after
2021-06-04 13:54:15 | [train_policy] epoch #481 | Computing loss after
2021-06-04 13:54:15 | [train_policy] epoch #481 | Fitting baseline...
2021-06-04 13:54:15 | [train_policy] epoch #481 | Saving snapshot...
2021-06-04 13:54:15 | [train_policy] epoch #481 | Saved
2021-06-04 13:54:15 | [train_policy] epoch #481 | Time 386.92 s
2021-06-04 13:54:15 | [train_policy] epoch #481 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284501
Evaluation/AverageDiscountedReturn          -41.1644
Evaluation/AverageReturn                    -41.1644
Evaluation/CompletionRate                     0
Evaluation/Iteration                        481
Evaluation/MaxReturn                        -28.527
Evaluation/MinReturn                        -64.0316
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.19674
Extras/EpisodeRewardMean                    -41.4244
LinearFeatureBaseline/ExplainedVariance       0.906721
PolicyExecTime                                0.232356
ProcessExecTime                               0.0311482
TotalEnvSteps                            487784
policy/Entropy                               -1.33571
policy/KL                                     0.00986024
policy/KLBefore                               0
policy/LossAfter                             -0.0188096
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.262972
policy/dLoss                                  0.0188096
---------------------------------------  ----------------
2021-06-04 13:54:15 | [train_policy] epoch #482 | Obtaining samples for iteration 482...
2021-06-04 13:54:15 | [train_policy] epoch #482 | Logging diagnostics...
2021-06-04 13:54:15 | [train_policy] epoch #482 | Optimizing policy...
2021-06-04 13:54:15 | [train_policy] epoch #482 | Computing loss before
2021-06-04 13:54:15 | [train_policy] epoch #482 | Computing KL before
2021-06-04 13:54:15 | [train_policy] epoch #482 | Optimizing
2021-06-04 13:54:15 | [train_policy] epoch #482 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:15 | [train_policy] epoch #482 | computing loss before
2021-06-04 13:54:15 | [train_policy] epoch #482 | computing gradient
2021-06-04 13:54:15 | [train_policy] epoch #482 | gradient computed
2021-06-04 13:54:15 | [train_policy] epoch #482 | computing descent direction
2021-06-04 13:54:15 | [train_policy] epoch #482 | descent direction computed
2021-06-04 13:54:15 | [train_policy] epoch #482 | backtrack iters: 0
2021-06-04 13:54:15 | [train_policy] epoch #482 | optimization finished
2021-06-04 13:54:15 | [train_policy] epoch #482 | Computing KL after
2021-06-04 13:54:15 | [train_policy] epoch #482 | Computing loss after
2021-06-04 13:54:15 | [train_policy] epoch #482 | Fitting baseline...
2021-06-04 13:54:15 | [train_policy] epoch #482 | Saving snapshot...
2021-06-04 13:54:15 | [train_policy] epoch #482 | Saved
2021-06-04 13:54:15 | [train_policy] epoch #482 | Time 387.71 s
2021-06-04 13:54:15 | [train_policy] epoch #482 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285547
Evaluation/AverageDiscountedReturn          -42.0689
Evaluation/AverageReturn                    -42.0689
Evaluation/CompletionRate                     0
Evaluation/Iteration                        482
Evaluation/MaxReturn                        -28.7738
Evaluation/MinReturn                        -87.8092
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.73826
Extras/EpisodeRewardMean                    -41.6984
LinearFeatureBaseline/ExplainedVariance       0.871484
PolicyExecTime                                0.219612
ProcessExecTime                               0.0312769
TotalEnvSteps                            488796
policy/Entropy                               -1.33002
policy/KL                                     0.00985131
policy/KLBefore                               0
policy/LossAfter                             -0.0281548
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.264473
policy/dLoss                                  0.0281548
---------------------------------------  ----------------
2021-06-04 13:54:16 | [train_policy] epoch #483 | Obtaining samples for iteration 483...
2021-06-04 13:54:16 | [train_policy] epoch #483 | Logging diagnostics...
2021-06-04 13:54:16 | [train_policy] epoch #483 | Optimizing policy...
2021-06-04 13:54:16 | [train_policy] epoch #483 | Computing loss before
2021-06-04 13:54:16 | [train_policy] epoch #483 | Computing KL before
2021-06-04 13:54:16 | [train_policy] epoch #483 | Optimizing
2021-06-04 13:54:16 | [train_policy] epoch #483 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:16 | [train_policy] epoch #483 | computing loss before
2021-06-04 13:54:16 | [train_policy] epoch #483 | computing gradient
2021-06-04 13:54:16 | [train_policy] epoch #483 | gradient computed
2021-06-04 13:54:16 | [train_policy] epoch #483 | computing descent direction
2021-06-04 13:54:16 | [train_policy] epoch #483 | descent direction computed
2021-06-04 13:54:16 | [train_policy] epoch #483 | backtrack iters: 1
2021-06-04 13:54:16 | [train_policy] epoch #483 | optimization finished
2021-06-04 13:54:16 | [train_policy] epoch #483 | Computing KL after
2021-06-04 13:54:16 | [train_policy] epoch #483 | Computing loss after
2021-06-04 13:54:16 | [train_policy] epoch #483 | Fitting baseline...
2021-06-04 13:54:16 | [train_policy] epoch #483 | Saving snapshot...
2021-06-04 13:54:16 | [train_policy] epoch #483 | Saved
2021-06-04 13:54:16 | [train_policy] epoch #483 | Time 388.50 s
2021-06-04 13:54:16 | [train_policy] epoch #483 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284925
Evaluation/AverageDiscountedReturn          -43.2491
Evaluation/AverageReturn                    -43.2491
Evaluation/CompletionRate                     0
Evaluation/Iteration                        483
Evaluation/MaxReturn                        -29.7332
Evaluation/MinReturn                        -63.8226
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.62389
Extras/EpisodeRewardMean                    -42.7899
LinearFeatureBaseline/ExplainedVariance       0.923455
PolicyExecTime                                0.211807
ProcessExecTime                               0.0312579
TotalEnvSteps                            489808
policy/Entropy                               -1.3382
policy/KL                                     0.00665811
policy/KLBefore                               0
policy/LossAfter                             -0.012102
policy/LossBefore                            -8.01011e-09
policy/Perplexity                             0.262316
policy/dLoss                                  0.012102
---------------------------------------  ----------------
2021-06-04 13:54:16 | [train_policy] epoch #484 | Obtaining samples for iteration 484...
2021-06-04 13:54:17 | [train_policy] epoch #484 | Logging diagnostics...
2021-06-04 13:54:17 | [train_policy] epoch #484 | Optimizing policy...
2021-06-04 13:54:17 | [train_policy] epoch #484 | Computing loss before
2021-06-04 13:54:17 | [train_policy] epoch #484 | Computing KL before
2021-06-04 13:54:17 | [train_policy] epoch #484 | Optimizing
2021-06-04 13:54:17 | [train_policy] epoch #484 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:17 | [train_policy] epoch #484 | computing loss before
2021-06-04 13:54:17 | [train_policy] epoch #484 | computing gradient
2021-06-04 13:54:17 | [train_policy] epoch #484 | gradient computed
2021-06-04 13:54:17 | [train_policy] epoch #484 | computing descent direction
2021-06-04 13:54:17 | [train_policy] epoch #484 | descent direction computed
2021-06-04 13:54:17 | [train_policy] epoch #484 | backtrack iters: 1
2021-06-04 13:54:17 | [train_policy] epoch #484 | optimization finished
2021-06-04 13:54:17 | [train_policy] epoch #484 | Computing KL after
2021-06-04 13:54:17 | [train_policy] epoch #484 | Computing loss after
2021-06-04 13:54:17 | [train_policy] epoch #484 | Fitting baseline...
2021-06-04 13:54:17 | [train_policy] epoch #484 | Saving snapshot...
2021-06-04 13:54:17 | [train_policy] epoch #484 | Saved
2021-06-04 13:54:17 | [train_policy] epoch #484 | Time 389.30 s
2021-06-04 13:54:17 | [train_policy] epoch #484 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.286866
Evaluation/AverageDiscountedReturn          -42.0125
Evaluation/AverageReturn                    -42.0125
Evaluation/CompletionRate                     0
Evaluation/Iteration                        484
Evaluation/MaxReturn                        -29.1391
Evaluation/MinReturn                        -63.8878
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.51327
Extras/EpisodeRewardMean                    -42.2755
LinearFeatureBaseline/ExplainedVariance       0.9155
PolicyExecTime                                0.222939
ProcessExecTime                               0.0313649
TotalEnvSteps                            490820
policy/Entropy                               -1.38874
policy/KL                                     0.00663829
policy/KLBefore                               0
policy/LossAfter                             -0.00974548
policy/LossBefore                             2.0202e-08
policy/Perplexity                             0.24939
policy/dLoss                                  0.0097455
---------------------------------------  ---------------
2021-06-04 13:54:17 | [train_policy] epoch #485 | Obtaining samples for iteration 485...
2021-06-04 13:54:18 | [train_policy] epoch #485 | Logging diagnostics...
2021-06-04 13:54:18 | [train_policy] epoch #485 | Optimizing policy...
2021-06-04 13:54:18 | [train_policy] epoch #485 | Computing loss before
2021-06-04 13:54:18 | [train_policy] epoch #485 | Computing KL before
2021-06-04 13:54:18 | [train_policy] epoch #485 | Optimizing
2021-06-04 13:54:18 | [train_policy] epoch #485 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:18 | [train_policy] epoch #485 | computing loss before
2021-06-04 13:54:18 | [train_policy] epoch #485 | computing gradient
2021-06-04 13:54:18 | [train_policy] epoch #485 | gradient computed
2021-06-04 13:54:18 | [train_policy] epoch #485 | computing descent direction
2021-06-04 13:54:18 | [train_policy] epoch #485 | descent direction computed
2021-06-04 13:54:18 | [train_policy] epoch #485 | backtrack iters: 0
2021-06-04 13:54:18 | [train_policy] epoch #485 | optimization finished
2021-06-04 13:54:18 | [train_policy] epoch #485 | Computing KL after
2021-06-04 13:54:18 | [train_policy] epoch #485 | Computing loss after
2021-06-04 13:54:18 | [train_policy] epoch #485 | Fitting baseline...
2021-06-04 13:54:18 | [train_policy] epoch #485 | Saving snapshot...
2021-06-04 13:54:18 | [train_policy] epoch #485 | Saved
2021-06-04 13:54:18 | [train_policy] epoch #485 | Time 390.09 s
2021-06-04 13:54:18 | [train_policy] epoch #485 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286984
Evaluation/AverageDiscountedReturn          -64.53
Evaluation/AverageReturn                    -64.53
Evaluation/CompletionRate                     0
Evaluation/Iteration                        485
Evaluation/MaxReturn                        -29.2949
Evaluation/MinReturn                      -2061.89
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.604
Extras/EpisodeRewardMean                    -62.8709
LinearFeatureBaseline/ExplainedVariance       0.0122821
PolicyExecTime                                0.229766
ProcessExecTime                               0.0313184
TotalEnvSteps                            491832
policy/Entropy                               -1.39859
policy/KL                                     0.00955561
policy/KLBefore                               0
policy/LossAfter                             -0.0251422
policy/LossBefore                             1.36643e-08
policy/Perplexity                             0.246945
policy/dLoss                                  0.0251422
---------------------------------------  ----------------
2021-06-04 13:54:18 | [train_policy] epoch #486 | Obtaining samples for iteration 486...
2021-06-04 13:54:19 | [train_policy] epoch #486 | Logging diagnostics...
2021-06-04 13:54:19 | [train_policy] epoch #486 | Optimizing policy...
2021-06-04 13:54:19 | [train_policy] epoch #486 | Computing loss before
2021-06-04 13:54:19 | [train_policy] epoch #486 | Computing KL before
2021-06-04 13:54:19 | [train_policy] epoch #486 | Optimizing
2021-06-04 13:54:19 | [train_policy] epoch #486 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:19 | [train_policy] epoch #486 | computing loss before
2021-06-04 13:54:19 | [train_policy] epoch #486 | computing gradient
2021-06-04 13:54:19 | [train_policy] epoch #486 | gradient computed
2021-06-04 13:54:19 | [train_policy] epoch #486 | computing descent direction
2021-06-04 13:54:19 | [train_policy] epoch #486 | descent direction computed
2021-06-04 13:54:19 | [train_policy] epoch #486 | backtrack iters: 1
2021-06-04 13:54:19 | [train_policy] epoch #486 | optimization finished
2021-06-04 13:54:19 | [train_policy] epoch #486 | Computing KL after
2021-06-04 13:54:19 | [train_policy] epoch #486 | Computing loss after
2021-06-04 13:54:19 | [train_policy] epoch #486 | Fitting baseline...
2021-06-04 13:54:19 | [train_policy] epoch #486 | Saving snapshot...
2021-06-04 13:54:19 | [train_policy] epoch #486 | Saved
2021-06-04 13:54:19 | [train_policy] epoch #486 | Time 390.90 s
2021-06-04 13:54:19 | [train_policy] epoch #486 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285296
Evaluation/AverageDiscountedReturn          -42.8574
Evaluation/AverageReturn                    -42.8574
Evaluation/CompletionRate                     0
Evaluation/Iteration                        486
Evaluation/MaxReturn                        -28.7475
Evaluation/MinReturn                        -95.4217
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.0881
Extras/EpisodeRewardMean                    -42.7074
LinearFeatureBaseline/ExplainedVariance     -23.3852
PolicyExecTime                                0.230499
ProcessExecTime                               0.031163
TotalEnvSteps                            492844
policy/Entropy                               -1.40799
policy/KL                                     0.00647662
policy/KLBefore                               0
policy/LossAfter                             -0.0214142
policy/LossBefore                             3.72235e-08
policy/Perplexity                             0.244634
policy/dLoss                                  0.0214143
---------------------------------------  ----------------
2021-06-04 13:54:19 | [train_policy] epoch #487 | Obtaining samples for iteration 487...
2021-06-04 13:54:19 | [train_policy] epoch #487 | Logging diagnostics...
2021-06-04 13:54:19 | [train_policy] epoch #487 | Optimizing policy...
2021-06-04 13:54:19 | [train_policy] epoch #487 | Computing loss before
2021-06-04 13:54:19 | [train_policy] epoch #487 | Computing KL before
2021-06-04 13:54:19 | [train_policy] epoch #487 | Optimizing
2021-06-04 13:54:19 | [train_policy] epoch #487 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:19 | [train_policy] epoch #487 | computing loss before
2021-06-04 13:54:19 | [train_policy] epoch #487 | computing gradient
2021-06-04 13:54:19 | [train_policy] epoch #487 | gradient computed
2021-06-04 13:54:19 | [train_policy] epoch #487 | computing descent direction
2021-06-04 13:54:19 | [train_policy] epoch #487 | descent direction computed
2021-06-04 13:54:19 | [train_policy] epoch #487 | backtrack iters: 1
2021-06-04 13:54:19 | [train_policy] epoch #487 | optimization finished
2021-06-04 13:54:19 | [train_policy] epoch #487 | Computing KL after
2021-06-04 13:54:19 | [train_policy] epoch #487 | Computing loss after
2021-06-04 13:54:19 | [train_policy] epoch #487 | Fitting baseline...
2021-06-04 13:54:20 | [train_policy] epoch #487 | Saving snapshot...
2021-06-04 13:54:20 | [train_policy] epoch #487 | Saved
2021-06-04 13:54:20 | [train_policy] epoch #487 | Time 391.75 s
2021-06-04 13:54:20 | [train_policy] epoch #487 | EpochTime 0.82 s
---------------------------------------  ----------------
EnvExecTime                                   0.311638
Evaluation/AverageDiscountedReturn          -43.2735
Evaluation/AverageReturn                    -43.2735
Evaluation/CompletionRate                     0
Evaluation/Iteration                        487
Evaluation/MaxReturn                        -29.5218
Evaluation/MinReturn                       -102.049
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1036
Extras/EpisodeRewardMean                    -42.8154
LinearFeatureBaseline/ExplainedVariance       0.828509
PolicyExecTime                                0.239634
ProcessExecTime                               0.0340047
TotalEnvSteps                            493856
policy/Entropy                               -1.43751
policy/KL                                     0.00654734
policy/KLBefore                               0
policy/LossAfter                             -0.0205226
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.237519
policy/dLoss                                  0.0205226
---------------------------------------  ----------------
2021-06-04 13:54:20 | [train_policy] epoch #488 | Obtaining samples for iteration 488...
2021-06-04 13:54:20 | [train_policy] epoch #488 | Logging diagnostics...
2021-06-04 13:54:20 | [train_policy] epoch #488 | Optimizing policy...
2021-06-04 13:54:20 | [train_policy] epoch #488 | Computing loss before
2021-06-04 13:54:20 | [train_policy] epoch #488 | Computing KL before
2021-06-04 13:54:20 | [train_policy] epoch #488 | Optimizing
2021-06-04 13:54:20 | [train_policy] epoch #488 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:20 | [train_policy] epoch #488 | computing loss before
2021-06-04 13:54:20 | [train_policy] epoch #488 | computing gradient
2021-06-04 13:54:20 | [train_policy] epoch #488 | gradient computed
2021-06-04 13:54:20 | [train_policy] epoch #488 | computing descent direction
2021-06-04 13:54:20 | [train_policy] epoch #488 | descent direction computed
2021-06-04 13:54:20 | [train_policy] epoch #488 | backtrack iters: 1
2021-06-04 13:54:20 | [train_policy] epoch #488 | optimization finished
2021-06-04 13:54:20 | [train_policy] epoch #488 | Computing KL after
2021-06-04 13:54:20 | [train_policy] epoch #488 | Computing loss after
2021-06-04 13:54:20 | [train_policy] epoch #488 | Fitting baseline...
2021-06-04 13:54:20 | [train_policy] epoch #488 | Saving snapshot...
2021-06-04 13:54:20 | [train_policy] epoch #488 | Saved
2021-06-04 13:54:20 | [train_policy] epoch #488 | Time 392.58 s
2021-06-04 13:54:20 | [train_policy] epoch #488 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.295351
Evaluation/AverageDiscountedReturn          -40.1368
Evaluation/AverageReturn                    -40.1368
Evaluation/CompletionRate                     0
Evaluation/Iteration                        488
Evaluation/MaxReturn                        -31.2764
Evaluation/MinReturn                        -56.5483
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.93877
Extras/EpisodeRewardMean                    -40.0784
LinearFeatureBaseline/ExplainedVariance       0.789287
PolicyExecTime                                0.229782
ProcessExecTime                               0.0322726
TotalEnvSteps                            494868
policy/Entropy                               -1.44843
policy/KL                                     0.00652418
policy/KLBefore                               0
policy/LossAfter                             -0.0132829
policy/LossBefore                            -2.32352e-08
policy/Perplexity                             0.23494
policy/dLoss                                  0.0132828
---------------------------------------  ----------------
2021-06-04 13:54:20 | [train_policy] epoch #489 | Obtaining samples for iteration 489...
2021-06-04 13:54:21 | [train_policy] epoch #489 | Logging diagnostics...
2021-06-04 13:54:21 | [train_policy] epoch #489 | Optimizing policy...
2021-06-04 13:54:21 | [train_policy] epoch #489 | Computing loss before
2021-06-04 13:54:21 | [train_policy] epoch #489 | Computing KL before
2021-06-04 13:54:21 | [train_policy] epoch #489 | Optimizing
2021-06-04 13:54:21 | [train_policy] epoch #489 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:21 | [train_policy] epoch #489 | computing loss before
2021-06-04 13:54:21 | [train_policy] epoch #489 | computing gradient
2021-06-04 13:54:21 | [train_policy] epoch #489 | gradient computed
2021-06-04 13:54:21 | [train_policy] epoch #489 | computing descent direction
2021-06-04 13:54:21 | [train_policy] epoch #489 | descent direction computed
2021-06-04 13:54:21 | [train_policy] epoch #489 | backtrack iters: 1
2021-06-04 13:54:21 | [train_policy] epoch #489 | optimization finished
2021-06-04 13:54:21 | [train_policy] epoch #489 | Computing KL after
2021-06-04 13:54:21 | [train_policy] epoch #489 | Computing loss after
2021-06-04 13:54:21 | [train_policy] epoch #489 | Fitting baseline...
2021-06-04 13:54:21 | [train_policy] epoch #489 | Saving snapshot...
2021-06-04 13:54:21 | [train_policy] epoch #489 | Saved
2021-06-04 13:54:21 | [train_policy] epoch #489 | Time 393.39 s
2021-06-04 13:54:21 | [train_policy] epoch #489 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.282682
Evaluation/AverageDiscountedReturn          -42.1637
Evaluation/AverageReturn                    -42.1637
Evaluation/CompletionRate                     0
Evaluation/Iteration                        489
Evaluation/MaxReturn                        -29.6973
Evaluation/MinReturn                        -58.1506
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.11242
Extras/EpisodeRewardMean                    -42.2705
LinearFeatureBaseline/ExplainedVariance       0.922652
PolicyExecTime                                0.22821
ProcessExecTime                               0.0312219
TotalEnvSteps                            495880
policy/Entropy                               -1.46669
policy/KL                                     0.00640604
policy/KLBefore                               0
policy/LossAfter                             -0.0155505
policy/LossBefore                             7.30334e-09
policy/Perplexity                             0.230688
policy/dLoss                                  0.0155505
---------------------------------------  ----------------
2021-06-04 13:54:21 | [train_policy] epoch #490 | Obtaining samples for iteration 490...
2021-06-04 13:54:22 | [train_policy] epoch #490 | Logging diagnostics...
2021-06-04 13:54:22 | [train_policy] epoch #490 | Optimizing policy...
2021-06-04 13:54:22 | [train_policy] epoch #490 | Computing loss before
2021-06-04 13:54:22 | [train_policy] epoch #490 | Computing KL before
2021-06-04 13:54:22 | [train_policy] epoch #490 | Optimizing
2021-06-04 13:54:22 | [train_policy] epoch #490 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:22 | [train_policy] epoch #490 | computing loss before
2021-06-04 13:54:22 | [train_policy] epoch #490 | computing gradient
2021-06-04 13:54:22 | [train_policy] epoch #490 | gradient computed
2021-06-04 13:54:22 | [train_policy] epoch #490 | computing descent direction
2021-06-04 13:54:22 | [train_policy] epoch #490 | descent direction computed
2021-06-04 13:54:22 | [train_policy] epoch #490 | backtrack iters: 0
2021-06-04 13:54:22 | [train_policy] epoch #490 | optimization finished
2021-06-04 13:54:22 | [train_policy] epoch #490 | Computing KL after
2021-06-04 13:54:22 | [train_policy] epoch #490 | Computing loss after
2021-06-04 13:54:22 | [train_policy] epoch #490 | Fitting baseline...
2021-06-04 13:54:22 | [train_policy] epoch #490 | Saving snapshot...
2021-06-04 13:54:22 | [train_policy] epoch #490 | Saved
2021-06-04 13:54:22 | [train_policy] epoch #490 | Time 394.20 s
2021-06-04 13:54:22 | [train_policy] epoch #490 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284125
Evaluation/AverageDiscountedReturn          -41.5895
Evaluation/AverageReturn                    -41.5895
Evaluation/CompletionRate                     0
Evaluation/Iteration                        490
Evaluation/MaxReturn                        -29.5121
Evaluation/MinReturn                        -63.1352
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.1452
Extras/EpisodeRewardMean                    -41.8493
LinearFeatureBaseline/ExplainedVariance       0.906859
PolicyExecTime                                0.235197
ProcessExecTime                               0.031306
TotalEnvSteps                            496892
policy/Entropy                               -1.47865
policy/KL                                     0.00945495
policy/KLBefore                               0
policy/LossAfter                             -0.0262946
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.227945
policy/dLoss                                  0.0262946
---------------------------------------  ----------------
2021-06-04 13:54:22 | [train_policy] epoch #491 | Obtaining samples for iteration 491...
2021-06-04 13:54:23 | [train_policy] epoch #491 | Logging diagnostics...
2021-06-04 13:54:23 | [train_policy] epoch #491 | Optimizing policy...
2021-06-04 13:54:23 | [train_policy] epoch #491 | Computing loss before
2021-06-04 13:54:23 | [train_policy] epoch #491 | Computing KL before
2021-06-04 13:54:23 | [train_policy] epoch #491 | Optimizing
2021-06-04 13:54:23 | [train_policy] epoch #491 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:23 | [train_policy] epoch #491 | computing loss before
2021-06-04 13:54:23 | [train_policy] epoch #491 | computing gradient
2021-06-04 13:54:23 | [train_policy] epoch #491 | gradient computed
2021-06-04 13:54:23 | [train_policy] epoch #491 | computing descent direction
2021-06-04 13:54:23 | [train_policy] epoch #491 | descent direction computed
2021-06-04 13:54:23 | [train_policy] epoch #491 | backtrack iters: 1
2021-06-04 13:54:23 | [train_policy] epoch #491 | optimization finished
2021-06-04 13:54:23 | [train_policy] epoch #491 | Computing KL after
2021-06-04 13:54:23 | [train_policy] epoch #491 | Computing loss after
2021-06-04 13:54:23 | [train_policy] epoch #491 | Fitting baseline...
2021-06-04 13:54:23 | [train_policy] epoch #491 | Saving snapshot...
2021-06-04 13:54:23 | [train_policy] epoch #491 | Saved
2021-06-04 13:54:23 | [train_policy] epoch #491 | Time 395.00 s
2021-06-04 13:54:23 | [train_policy] epoch #491 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284445
Evaluation/AverageDiscountedReturn          -43.5046
Evaluation/AverageReturn                    -43.5046
Evaluation/CompletionRate                     0
Evaluation/Iteration                        491
Evaluation/MaxReturn                        -28.9525
Evaluation/MinReturn                       -131.529
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.922
Extras/EpisodeRewardMean                    -43.0447
LinearFeatureBaseline/ExplainedVariance       0.706544
PolicyExecTime                                0.226644
ProcessExecTime                               0.0311198
TotalEnvSteps                            497904
policy/Entropy                               -1.44277
policy/KL                                     0.00645535
policy/KLBefore                               0
policy/LossAfter                             -0.0194919
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             0.236273
policy/dLoss                                  0.0194919
---------------------------------------  ----------------
2021-06-04 13:54:23 | [train_policy] epoch #492 | Obtaining samples for iteration 492...
2021-06-04 13:54:23 | [train_policy] epoch #492 | Logging diagnostics...
2021-06-04 13:54:23 | [train_policy] epoch #492 | Optimizing policy...
2021-06-04 13:54:23 | [train_policy] epoch #492 | Computing loss before
2021-06-04 13:54:23 | [train_policy] epoch #492 | Computing KL before
2021-06-04 13:54:23 | [train_policy] epoch #492 | Optimizing
2021-06-04 13:54:23 | [train_policy] epoch #492 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:23 | [train_policy] epoch #492 | computing loss before
2021-06-04 13:54:23 | [train_policy] epoch #492 | computing gradient
2021-06-04 13:54:23 | [train_policy] epoch #492 | gradient computed
2021-06-04 13:54:23 | [train_policy] epoch #492 | computing descent direction
2021-06-04 13:54:23 | [train_policy] epoch #492 | descent direction computed
2021-06-04 13:54:24 | [train_policy] epoch #492 | backtrack iters: 1
2021-06-04 13:54:24 | [train_policy] epoch #492 | optimization finished
2021-06-04 13:54:24 | [train_policy] epoch #492 | Computing KL after
2021-06-04 13:54:24 | [train_policy] epoch #492 | Computing loss after
2021-06-04 13:54:24 | [train_policy] epoch #492 | Fitting baseline...
2021-06-04 13:54:24 | [train_policy] epoch #492 | Saving snapshot...
2021-06-04 13:54:24 | [train_policy] epoch #492 | Saved
2021-06-04 13:54:24 | [train_policy] epoch #492 | Time 395.79 s
2021-06-04 13:54:24 | [train_policy] epoch #492 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285666
Evaluation/AverageDiscountedReturn          -43.1894
Evaluation/AverageReturn                    -43.1894
Evaluation/CompletionRate                     0
Evaluation/Iteration                        492
Evaluation/MaxReturn                        -29.7155
Evaluation/MinReturn                        -64.0409
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.07813
Extras/EpisodeRewardMean                    -43.2521
LinearFeatureBaseline/ExplainedVariance       0.888474
PolicyExecTime                                0.210776
ProcessExecTime                               0.0311551
TotalEnvSteps                            498916
policy/Entropy                               -1.45764
policy/KL                                     0.00647155
policy/KLBefore                               0
policy/LossAfter                             -0.0101734
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.232785
policy/dLoss                                  0.0101734
---------------------------------------  ----------------
2021-06-04 13:54:24 | [train_policy] epoch #493 | Obtaining samples for iteration 493...
2021-06-04 13:54:24 | [train_policy] epoch #493 | Logging diagnostics...
2021-06-04 13:54:24 | [train_policy] epoch #493 | Optimizing policy...
2021-06-04 13:54:24 | [train_policy] epoch #493 | Computing loss before
2021-06-04 13:54:24 | [train_policy] epoch #493 | Computing KL before
2021-06-04 13:54:24 | [train_policy] epoch #493 | Optimizing
2021-06-04 13:54:24 | [train_policy] epoch #493 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:24 | [train_policy] epoch #493 | computing loss before
2021-06-04 13:54:24 | [train_policy] epoch #493 | computing gradient
2021-06-04 13:54:24 | [train_policy] epoch #493 | gradient computed
2021-06-04 13:54:24 | [train_policy] epoch #493 | computing descent direction
2021-06-04 13:54:24 | [train_policy] epoch #493 | descent direction computed
2021-06-04 13:54:24 | [train_policy] epoch #493 | backtrack iters: 0
2021-06-04 13:54:24 | [train_policy] epoch #493 | optimization finished
2021-06-04 13:54:24 | [train_policy] epoch #493 | Computing KL after
2021-06-04 13:54:24 | [train_policy] epoch #493 | Computing loss after
2021-06-04 13:54:24 | [train_policy] epoch #493 | Fitting baseline...
2021-06-04 13:54:24 | [train_policy] epoch #493 | Saving snapshot...
2021-06-04 13:54:24 | [train_policy] epoch #493 | Saved
2021-06-04 13:54:24 | [train_policy] epoch #493 | Time 396.59 s
2021-06-04 13:54:24 | [train_policy] epoch #493 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28487
Evaluation/AverageDiscountedReturn          -43.393
Evaluation/AverageReturn                    -43.393
Evaluation/CompletionRate                     0
Evaluation/Iteration                        493
Evaluation/MaxReturn                        -31.7821
Evaluation/MinReturn                        -96.7087
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1869
Extras/EpisodeRewardMean                    -43.2713
LinearFeatureBaseline/ExplainedVariance       0.852023
PolicyExecTime                                0.22366
ProcessExecTime                               0.0310121
TotalEnvSteps                            499928
policy/Entropy                               -1.46058
policy/KL                                     0.00972159
policy/KLBefore                               0
policy/LossAfter                             -0.0201615
policy/LossBefore                            -2.59151e-09
policy/Perplexity                             0.232102
policy/dLoss                                  0.0201615
---------------------------------------  ----------------
2021-06-04 13:54:24 | [train_policy] epoch #494 | Obtaining samples for iteration 494...
2021-06-04 13:54:25 | [train_policy] epoch #494 | Logging diagnostics...
2021-06-04 13:54:25 | [train_policy] epoch #494 | Optimizing policy...
2021-06-04 13:54:25 | [train_policy] epoch #494 | Computing loss before
2021-06-04 13:54:25 | [train_policy] epoch #494 | Computing KL before
2021-06-04 13:54:25 | [train_policy] epoch #494 | Optimizing
2021-06-04 13:54:25 | [train_policy] epoch #494 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:25 | [train_policy] epoch #494 | computing loss before
2021-06-04 13:54:25 | [train_policy] epoch #494 | computing gradient
2021-06-04 13:54:25 | [train_policy] epoch #494 | gradient computed
2021-06-04 13:54:25 | [train_policy] epoch #494 | computing descent direction
2021-06-04 13:54:25 | [train_policy] epoch #494 | descent direction computed
2021-06-04 13:54:25 | [train_policy] epoch #494 | backtrack iters: 0
2021-06-04 13:54:25 | [train_policy] epoch #494 | optimization finished
2021-06-04 13:54:25 | [train_policy] epoch #494 | Computing KL after
2021-06-04 13:54:25 | [train_policy] epoch #494 | Computing loss after
2021-06-04 13:54:25 | [train_policy] epoch #494 | Fitting baseline...
2021-06-04 13:54:25 | [train_policy] epoch #494 | Saving snapshot...
2021-06-04 13:54:25 | [train_policy] epoch #494 | Saved
2021-06-04 13:54:25 | [train_policy] epoch #494 | Time 397.39 s
2021-06-04 13:54:25 | [train_policy] epoch #494 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284806
Evaluation/AverageDiscountedReturn          -43.9132
Evaluation/AverageReturn                    -43.9132
Evaluation/CompletionRate                     0
Evaluation/Iteration                        494
Evaluation/MaxReturn                        -29.9543
Evaluation/MinReturn                        -83.983
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.31443
Extras/EpisodeRewardMean                    -44.6693
LinearFeatureBaseline/ExplainedVariance       0.873504
PolicyExecTime                                0.236073
ProcessExecTime                               0.0311663
TotalEnvSteps                            500940
policy/Entropy                               -1.48348
policy/KL                                     0.00988617
policy/KLBefore                               0
policy/LossAfter                             -0.0228985
policy/LossBefore                            -1.70804e-08
policy/Perplexity                             0.226847
policy/dLoss                                  0.0228985
---------------------------------------  ----------------
2021-06-04 13:54:25 | [train_policy] epoch #495 | Obtaining samples for iteration 495...
2021-06-04 13:54:26 | [train_policy] epoch #495 | Logging diagnostics...
2021-06-04 13:54:26 | [train_policy] epoch #495 | Optimizing policy...
2021-06-04 13:54:26 | [train_policy] epoch #495 | Computing loss before
2021-06-04 13:54:26 | [train_policy] epoch #495 | Computing KL before
2021-06-04 13:54:26 | [train_policy] epoch #495 | Optimizing
2021-06-04 13:54:26 | [train_policy] epoch #495 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:26 | [train_policy] epoch #495 | computing loss before
2021-06-04 13:54:26 | [train_policy] epoch #495 | computing gradient
2021-06-04 13:54:26 | [train_policy] epoch #495 | gradient computed
2021-06-04 13:54:26 | [train_policy] epoch #495 | computing descent direction
2021-06-04 13:54:26 | [train_policy] epoch #495 | descent direction computed
2021-06-04 13:54:26 | [train_policy] epoch #495 | backtrack iters: 1
2021-06-04 13:54:26 | [train_policy] epoch #495 | optimization finished
2021-06-04 13:54:26 | [train_policy] epoch #495 | Computing KL after
2021-06-04 13:54:26 | [train_policy] epoch #495 | Computing loss after
2021-06-04 13:54:26 | [train_policy] epoch #495 | Fitting baseline...
2021-06-04 13:54:26 | [train_policy] epoch #495 | Saving snapshot...
2021-06-04 13:54:26 | [train_policy] epoch #495 | Saved
2021-06-04 13:54:26 | [train_policy] epoch #495 | Time 398.17 s
2021-06-04 13:54:26 | [train_policy] epoch #495 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284276
Evaluation/AverageDiscountedReturn          -41.5302
Evaluation/AverageReturn                    -41.5302
Evaluation/CompletionRate                     0
Evaluation/Iteration                        495
Evaluation/MaxReturn                        -28.5241
Evaluation/MinReturn                        -91.0951
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.07948
Extras/EpisodeRewardMean                    -41.7149
LinearFeatureBaseline/ExplainedVariance       0.857374
PolicyExecTime                                0.215329
ProcessExecTime                               0.0311677
TotalEnvSteps                            501952
policy/Entropy                               -1.49225
policy/KL                                     0.00657459
policy/KLBefore                               0
policy/LossAfter                             -0.0142254
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.224866
policy/dLoss                                  0.0142254
---------------------------------------  ----------------
2021-06-04 13:54:26 | [train_policy] epoch #496 | Obtaining samples for iteration 496...
2021-06-04 13:54:27 | [train_policy] epoch #496 | Logging diagnostics...
2021-06-04 13:54:27 | [train_policy] epoch #496 | Optimizing policy...
2021-06-04 13:54:27 | [train_policy] epoch #496 | Computing loss before
2021-06-04 13:54:27 | [train_policy] epoch #496 | Computing KL before
2021-06-04 13:54:27 | [train_policy] epoch #496 | Optimizing
2021-06-04 13:54:27 | [train_policy] epoch #496 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:27 | [train_policy] epoch #496 | computing loss before
2021-06-04 13:54:27 | [train_policy] epoch #496 | computing gradient
2021-06-04 13:54:27 | [train_policy] epoch #496 | gradient computed
2021-06-04 13:54:27 | [train_policy] epoch #496 | computing descent direction
2021-06-04 13:54:27 | [train_policy] epoch #496 | descent direction computed
2021-06-04 13:54:27 | [train_policy] epoch #496 | backtrack iters: 1
2021-06-04 13:54:27 | [train_policy] epoch #496 | optimization finished
2021-06-04 13:54:27 | [train_policy] epoch #496 | Computing KL after
2021-06-04 13:54:27 | [train_policy] epoch #496 | Computing loss after
2021-06-04 13:54:27 | [train_policy] epoch #496 | Fitting baseline...
2021-06-04 13:54:27 | [train_policy] epoch #496 | Saving snapshot...
2021-06-04 13:54:27 | [train_policy] epoch #496 | Saved
2021-06-04 13:54:27 | [train_policy] epoch #496 | Time 398.99 s
2021-06-04 13:54:27 | [train_policy] epoch #496 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.285472
Evaluation/AverageDiscountedReturn          -63.1078
Evaluation/AverageReturn                    -63.1078
Evaluation/CompletionRate                     0
Evaluation/Iteration                        496
Evaluation/MaxReturn                        -29.3038
Evaluation/MinReturn                      -2062.04
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.758
Extras/EpisodeRewardMean                    -61.1688
LinearFeatureBaseline/ExplainedVariance       0.0160112
PolicyExecTime                                0.234467
ProcessExecTime                               0.0311983
TotalEnvSteps                            502964
policy/Entropy                               -1.50303
policy/KL                                     0.00680011
policy/KLBefore                               0
policy/LossAfter                             -0.0315733
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.222455
policy/dLoss                                  0.0315732
---------------------------------------  ----------------
2021-06-04 13:54:27 | [train_policy] epoch #497 | Obtaining samples for iteration 497...
2021-06-04 13:54:27 | [train_policy] epoch #497 | Logging diagnostics...
2021-06-04 13:54:27 | [train_policy] epoch #497 | Optimizing policy...
2021-06-04 13:54:27 | [train_policy] epoch #497 | Computing loss before
2021-06-04 13:54:27 | [train_policy] epoch #497 | Computing KL before
2021-06-04 13:54:27 | [train_policy] epoch #497 | Optimizing
2021-06-04 13:54:27 | [train_policy] epoch #497 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:27 | [train_policy] epoch #497 | computing loss before
2021-06-04 13:54:27 | [train_policy] epoch #497 | computing gradient
2021-06-04 13:54:27 | [train_policy] epoch #497 | gradient computed
2021-06-04 13:54:27 | [train_policy] epoch #497 | computing descent direction
2021-06-04 13:54:28 | [train_policy] epoch #497 | descent direction computed
2021-06-04 13:54:28 | [train_policy] epoch #497 | backtrack iters: 0
2021-06-04 13:54:28 | [train_policy] epoch #497 | optimization finished
2021-06-04 13:54:28 | [train_policy] epoch #497 | Computing KL after
2021-06-04 13:54:28 | [train_policy] epoch #497 | Computing loss after
2021-06-04 13:54:28 | [train_policy] epoch #497 | Fitting baseline...
2021-06-04 13:54:28 | [train_policy] epoch #497 | Saving snapshot...
2021-06-04 13:54:28 | [train_policy] epoch #497 | Saved
2021-06-04 13:54:28 | [train_policy] epoch #497 | Time 399.78 s
2021-06-04 13:54:28 | [train_policy] epoch #497 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286077
Evaluation/AverageDiscountedReturn          -42.2547
Evaluation/AverageReturn                    -42.2547
Evaluation/CompletionRate                     0
Evaluation/Iteration                        497
Evaluation/MaxReturn                        -29.2032
Evaluation/MinReturn                        -64.036
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.73663
Extras/EpisodeRewardMean                    -42.6291
LinearFeatureBaseline/ExplainedVariance     -35.9097
PolicyExecTime                                0.226036
ProcessExecTime                               0.0313511
TotalEnvSteps                            503976
policy/Entropy                               -1.49482
policy/KL                                     0.00974957
policy/KLBefore                               0
policy/LossAfter                             -0.0300234
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.224289
policy/dLoss                                  0.0300234
---------------------------------------  ----------------
2021-06-04 13:54:28 | [train_policy] epoch #498 | Obtaining samples for iteration 498...
2021-06-04 13:54:28 | [train_policy] epoch #498 | Logging diagnostics...
2021-06-04 13:54:28 | [train_policy] epoch #498 | Optimizing policy...
2021-06-04 13:54:28 | [train_policy] epoch #498 | Computing loss before
2021-06-04 13:54:28 | [train_policy] epoch #498 | Computing KL before
2021-06-04 13:54:28 | [train_policy] epoch #498 | Optimizing
2021-06-04 13:54:28 | [train_policy] epoch #498 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:28 | [train_policy] epoch #498 | computing loss before
2021-06-04 13:54:28 | [train_policy] epoch #498 | computing gradient
2021-06-04 13:54:28 | [train_policy] epoch #498 | gradient computed
2021-06-04 13:54:28 | [train_policy] epoch #498 | computing descent direction
2021-06-04 13:54:28 | [train_policy] epoch #498 | descent direction computed
2021-06-04 13:54:28 | [train_policy] epoch #498 | backtrack iters: 0
2021-06-04 13:54:28 | [train_policy] epoch #498 | optimization finished
2021-06-04 13:54:28 | [train_policy] epoch #498 | Computing KL after
2021-06-04 13:54:28 | [train_policy] epoch #498 | Computing loss after
2021-06-04 13:54:28 | [train_policy] epoch #498 | Fitting baseline...
2021-06-04 13:54:28 | [train_policy] epoch #498 | Saving snapshot...
2021-06-04 13:54:28 | [train_policy] epoch #498 | Saved
2021-06-04 13:54:28 | [train_policy] epoch #498 | Time 400.56 s
2021-06-04 13:54:28 | [train_policy] epoch #498 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285721
Evaluation/AverageDiscountedReturn          -41.4494
Evaluation/AverageReturn                    -41.4494
Evaluation/CompletionRate                     0
Evaluation/Iteration                        498
Evaluation/MaxReturn                        -29.7342
Evaluation/MinReturn                        -64.0065
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.51618
Extras/EpisodeRewardMean                    -41.4332
LinearFeatureBaseline/ExplainedVariance       0.91466
PolicyExecTime                                0.211971
ProcessExecTime                               0.0313239
TotalEnvSteps                            504988
policy/Entropy                               -1.46185
policy/KL                                     0.00969383
policy/KLBefore                               0
policy/LossAfter                             -0.0198272
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.231806
policy/dLoss                                  0.0198272
---------------------------------------  ----------------
2021-06-04 13:54:28 | [train_policy] epoch #499 | Obtaining samples for iteration 499...
2021-06-04 13:54:29 | [train_policy] epoch #499 | Logging diagnostics...
2021-06-04 13:54:29 | [train_policy] epoch #499 | Optimizing policy...
2021-06-04 13:54:29 | [train_policy] epoch #499 | Computing loss before
2021-06-04 13:54:29 | [train_policy] epoch #499 | Computing KL before
2021-06-04 13:54:29 | [train_policy] epoch #499 | Optimizing
2021-06-04 13:54:29 | [train_policy] epoch #499 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:29 | [train_policy] epoch #499 | computing loss before
2021-06-04 13:54:29 | [train_policy] epoch #499 | computing gradient
2021-06-04 13:54:29 | [train_policy] epoch #499 | gradient computed
2021-06-04 13:54:29 | [train_policy] epoch #499 | computing descent direction
2021-06-04 13:54:29 | [train_policy] epoch #499 | descent direction computed
2021-06-04 13:54:29 | [train_policy] epoch #499 | backtrack iters: 1
2021-06-04 13:54:29 | [train_policy] epoch #499 | optimization finished
2021-06-04 13:54:29 | [train_policy] epoch #499 | Computing KL after
2021-06-04 13:54:29 | [train_policy] epoch #499 | Computing loss after
2021-06-04 13:54:29 | [train_policy] epoch #499 | Fitting baseline...
2021-06-04 13:54:29 | [train_policy] epoch #499 | Saving snapshot...
2021-06-04 13:54:29 | [train_policy] epoch #499 | Saved
2021-06-04 13:54:29 | [train_policy] epoch #499 | Time 401.37 s
2021-06-04 13:54:29 | [train_policy] epoch #499 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286918
Evaluation/AverageDiscountedReturn          -41.5311
Evaluation/AverageReturn                    -41.5311
Evaluation/CompletionRate                     0
Evaluation/Iteration                        499
Evaluation/MaxReturn                        -29.079
Evaluation/MinReturn                        -62.9603
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83402
Extras/EpisodeRewardMean                    -41.4909
LinearFeatureBaseline/ExplainedVariance       0.914701
PolicyExecTime                                0.223772
ProcessExecTime                               0.0313866
TotalEnvSteps                            506000
policy/Entropy                               -1.45952
policy/KL                                     0.00642516
policy/KLBefore                               0
policy/LossAfter                             -0.0138116
policy/LossBefore                            -2.23812e-08
policy/Perplexity                             0.232348
policy/dLoss                                  0.0138116
---------------------------------------  ----------------
2021-06-04 13:54:29 | [train_policy] epoch #500 | Obtaining samples for iteration 500...
2021-06-04 13:54:30 | [train_policy] epoch #500 | Logging diagnostics...
2021-06-04 13:54:30 | [train_policy] epoch #500 | Optimizing policy...
2021-06-04 13:54:30 | [train_policy] epoch #500 | Computing loss before
2021-06-04 13:54:30 | [train_policy] epoch #500 | Computing KL before
2021-06-04 13:54:30 | [train_policy] epoch #500 | Optimizing
2021-06-04 13:54:30 | [train_policy] epoch #500 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:30 | [train_policy] epoch #500 | computing loss before
2021-06-04 13:54:30 | [train_policy] epoch #500 | computing gradient
2021-06-04 13:54:30 | [train_policy] epoch #500 | gradient computed
2021-06-04 13:54:30 | [train_policy] epoch #500 | computing descent direction
2021-06-04 13:54:30 | [train_policy] epoch #500 | descent direction computed
2021-06-04 13:54:30 | [train_policy] epoch #500 | backtrack iters: 1
2021-06-04 13:54:30 | [train_policy] epoch #500 | optimization finished
2021-06-04 13:54:30 | [train_policy] epoch #500 | Computing KL after
2021-06-04 13:54:30 | [train_policy] epoch #500 | Computing loss after
2021-06-04 13:54:30 | [train_policy] epoch #500 | Fitting baseline...
2021-06-04 13:54:30 | [train_policy] epoch #500 | Saving snapshot...
2021-06-04 13:54:30 | [train_policy] epoch #500 | Saved
2021-06-04 13:54:30 | [train_policy] epoch #500 | Time 402.18 s
2021-06-04 13:54:30 | [train_policy] epoch #500 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284472
Evaluation/AverageDiscountedReturn          -62.4961
Evaluation/AverageReturn                    -62.4961
Evaluation/CompletionRate                     0
Evaluation/Iteration                        500
Evaluation/MaxReturn                        -29.6128
Evaluation/MinReturn                      -2061.99
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.769
Extras/EpisodeRewardMean                    -60.9023
LinearFeatureBaseline/ExplainedVariance       0.0140771
PolicyExecTime                                0.229737
ProcessExecTime                               0.0311961
TotalEnvSteps                            507012
policy/Entropy                               -1.44539
policy/KL                                     0.00681608
policy/KLBefore                               0
policy/LossAfter                             -0.0209512
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.235654
policy/dLoss                                  0.0209512
---------------------------------------  ----------------
2021-06-04 13:54:30 | [train_policy] epoch #501 | Obtaining samples for iteration 501...
2021-06-04 13:54:31 | [train_policy] epoch #501 | Logging diagnostics...
2021-06-04 13:54:31 | [train_policy] epoch #501 | Optimizing policy...
2021-06-04 13:54:31 | [train_policy] epoch #501 | Computing loss before
2021-06-04 13:54:31 | [train_policy] epoch #501 | Computing KL before
2021-06-04 13:54:31 | [train_policy] epoch #501 | Optimizing
2021-06-04 13:54:31 | [train_policy] epoch #501 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:31 | [train_policy] epoch #501 | computing loss before
2021-06-04 13:54:31 | [train_policy] epoch #501 | computing gradient
2021-06-04 13:54:31 | [train_policy] epoch #501 | gradient computed
2021-06-04 13:54:31 | [train_policy] epoch #501 | computing descent direction
2021-06-04 13:54:31 | [train_policy] epoch #501 | descent direction computed
2021-06-04 13:54:31 | [train_policy] epoch #501 | backtrack iters: 1
2021-06-04 13:54:31 | [train_policy] epoch #501 | optimization finished
2021-06-04 13:54:31 | [train_policy] epoch #501 | Computing KL after
2021-06-04 13:54:31 | [train_policy] epoch #501 | Computing loss after
2021-06-04 13:54:31 | [train_policy] epoch #501 | Fitting baseline...
2021-06-04 13:54:31 | [train_policy] epoch #501 | Saving snapshot...
2021-06-04 13:54:31 | [train_policy] epoch #501 | Saved
2021-06-04 13:54:31 | [train_policy] epoch #501 | Time 402.99 s
2021-06-04 13:54:31 | [train_policy] epoch #501 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285072
Evaluation/AverageDiscountedReturn          -41.5363
Evaluation/AverageReturn                    -41.5363
Evaluation/CompletionRate                     0
Evaluation/Iteration                        501
Evaluation/MaxReturn                        -29.6546
Evaluation/MinReturn                        -63.9404
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43216
Extras/EpisodeRewardMean                    -61.6751
LinearFeatureBaseline/ExplainedVariance     -26.5937
PolicyExecTime                                0.223943
ProcessExecTime                               0.031184
TotalEnvSteps                            508024
policy/Entropy                               -1.4516
policy/KL                                     0.00645518
policy/KLBefore                               0
policy/LossAfter                             -0.0132922
policy/LossBefore                            -1.44889e-08
policy/Perplexity                             0.234196
policy/dLoss                                  0.0132922
---------------------------------------  ----------------
2021-06-04 13:54:31 | [train_policy] epoch #502 | Obtaining samples for iteration 502...
2021-06-04 13:54:31 | [train_policy] epoch #502 | Logging diagnostics...
2021-06-04 13:54:31 | [train_policy] epoch #502 | Optimizing policy...
2021-06-04 13:54:31 | [train_policy] epoch #502 | Computing loss before
2021-06-04 13:54:31 | [train_policy] epoch #502 | Computing KL before
2021-06-04 13:54:31 | [train_policy] epoch #502 | Optimizing
2021-06-04 13:54:31 | [train_policy] epoch #502 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:31 | [train_policy] epoch #502 | computing loss before
2021-06-04 13:54:31 | [train_policy] epoch #502 | computing gradient
2021-06-04 13:54:31 | [train_policy] epoch #502 | gradient computed
2021-06-04 13:54:31 | [train_policy] epoch #502 | computing descent direction
2021-06-04 13:54:32 | [train_policy] epoch #502 | descent direction computed
2021-06-04 13:54:32 | [train_policy] epoch #502 | backtrack iters: 1
2021-06-04 13:54:32 | [train_policy] epoch #502 | optimization finished
2021-06-04 13:54:32 | [train_policy] epoch #502 | Computing KL after
2021-06-04 13:54:32 | [train_policy] epoch #502 | Computing loss after
2021-06-04 13:54:32 | [train_policy] epoch #502 | Fitting baseline...
2021-06-04 13:54:32 | [train_policy] epoch #502 | Saving snapshot...
2021-06-04 13:54:32 | [train_policy] epoch #502 | Saved
2021-06-04 13:54:32 | [train_policy] epoch #502 | Time 403.80 s
2021-06-04 13:54:32 | [train_policy] epoch #502 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284403
Evaluation/AverageDiscountedReturn          -41.5511
Evaluation/AverageReturn                    -41.5511
Evaluation/CompletionRate                     0
Evaluation/Iteration                        502
Evaluation/MaxReturn                        -32.0465
Evaluation/MinReturn                       -129.925
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.3159
Extras/EpisodeRewardMean                    -41.4537
LinearFeatureBaseline/ExplainedVariance       0.702712
PolicyExecTime                                0.229816
ProcessExecTime                               0.0311866
TotalEnvSteps                            509036
policy/Entropy                               -1.41725
policy/KL                                     0.00644588
policy/KLBefore                               0
policy/LossAfter                             -0.00646187
policy/LossBefore                            -3.53387e-09
policy/Perplexity                             0.24238
policy/dLoss                                  0.00646187
---------------------------------------  ----------------
2021-06-04 13:54:32 | [train_policy] epoch #503 | Obtaining samples for iteration 503...
2021-06-04 13:54:32 | [train_policy] epoch #503 | Logging diagnostics...
2021-06-04 13:54:32 | [train_policy] epoch #503 | Optimizing policy...
2021-06-04 13:54:32 | [train_policy] epoch #503 | Computing loss before
2021-06-04 13:54:32 | [train_policy] epoch #503 | Computing KL before
2021-06-04 13:54:32 | [train_policy] epoch #503 | Optimizing
2021-06-04 13:54:32 | [train_policy] epoch #503 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:32 | [train_policy] epoch #503 | computing loss before
2021-06-04 13:54:32 | [train_policy] epoch #503 | computing gradient
2021-06-04 13:54:32 | [train_policy] epoch #503 | gradient computed
2021-06-04 13:54:32 | [train_policy] epoch #503 | computing descent direction
2021-06-04 13:54:32 | [train_policy] epoch #503 | descent direction computed
2021-06-04 13:54:32 | [train_policy] epoch #503 | backtrack iters: 1
2021-06-04 13:54:32 | [train_policy] epoch #503 | optimization finished
2021-06-04 13:54:32 | [train_policy] epoch #503 | Computing KL after
2021-06-04 13:54:32 | [train_policy] epoch #503 | Computing loss after
2021-06-04 13:54:32 | [train_policy] epoch #503 | Fitting baseline...
2021-06-04 13:54:32 | [train_policy] epoch #503 | Saving snapshot...
2021-06-04 13:54:32 | [train_policy] epoch #503 | Saved
2021-06-04 13:54:32 | [train_policy] epoch #503 | Time 404.60 s
2021-06-04 13:54:32 | [train_policy] epoch #503 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284458
Evaluation/AverageDiscountedReturn          -47.1471
Evaluation/AverageReturn                    -47.1471
Evaluation/CompletionRate                     0
Evaluation/Iteration                        503
Evaluation/MaxReturn                        -30.6248
Evaluation/MinReturn                       -436.446
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         42.103
Extras/EpisodeRewardMean                    -46.6625
LinearFeatureBaseline/ExplainedVariance       0.183028
PolicyExecTime                                0.212466
ProcessExecTime                               0.0311885
TotalEnvSteps                            510048
policy/Entropy                               -1.42934
policy/KL                                     0.006608
policy/KLBefore                               0
policy/LossAfter                             -0.0171151
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.239468
policy/dLoss                                  0.0171151
---------------------------------------  ----------------
2021-06-04 13:54:32 | [train_policy] epoch #504 | Obtaining samples for iteration 504...
2021-06-04 13:54:33 | [train_policy] epoch #504 | Logging diagnostics...
2021-06-04 13:54:33 | [train_policy] epoch #504 | Optimizing policy...
2021-06-04 13:54:33 | [train_policy] epoch #504 | Computing loss before
2021-06-04 13:54:33 | [train_policy] epoch #504 | Computing KL before
2021-06-04 13:54:33 | [train_policy] epoch #504 | Optimizing
2021-06-04 13:54:33 | [train_policy] epoch #504 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:33 | [train_policy] epoch #504 | computing loss before
2021-06-04 13:54:33 | [train_policy] epoch #504 | computing gradient
2021-06-04 13:54:33 | [train_policy] epoch #504 | gradient computed
2021-06-04 13:54:33 | [train_policy] epoch #504 | computing descent direction
2021-06-04 13:54:33 | [train_policy] epoch #504 | descent direction computed
2021-06-04 13:54:33 | [train_policy] epoch #504 | backtrack iters: 1
2021-06-04 13:54:33 | [train_policy] epoch #504 | optimization finished
2021-06-04 13:54:33 | [train_policy] epoch #504 | Computing KL after
2021-06-04 13:54:33 | [train_policy] epoch #504 | Computing loss after
2021-06-04 13:54:33 | [train_policy] epoch #504 | Fitting baseline...
2021-06-04 13:54:33 | [train_policy] epoch #504 | Saving snapshot...
2021-06-04 13:54:33 | [train_policy] epoch #504 | Saved
2021-06-04 13:54:33 | [train_policy] epoch #504 | Time 405.41 s
2021-06-04 13:54:33 | [train_policy] epoch #504 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285185
Evaluation/AverageDiscountedReturn          -43.1807
Evaluation/AverageReturn                    -43.1807
Evaluation/CompletionRate                     0
Evaluation/Iteration                        504
Evaluation/MaxReturn                        -29.517
Evaluation/MinReturn                       -106.585
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.3313
Extras/EpisodeRewardMean                    -42.7962
LinearFeatureBaseline/ExplainedVariance       0.27209
PolicyExecTime                                0.224362
ProcessExecTime                               0.0312095
TotalEnvSteps                            511060
policy/Entropy                               -1.45093
policy/KL                                     0.00671307
policy/KLBefore                               0
policy/LossAfter                             -0.0109387
policy/LossBefore                             5.06522e-08
policy/Perplexity                             0.234352
policy/dLoss                                  0.0109388
---------------------------------------  ----------------
2021-06-04 13:54:33 | [train_policy] epoch #505 | Obtaining samples for iteration 505...
2021-06-04 13:54:34 | [train_policy] epoch #505 | Logging diagnostics...
2021-06-04 13:54:34 | [train_policy] epoch #505 | Optimizing policy...
2021-06-04 13:54:34 | [train_policy] epoch #505 | Computing loss before
2021-06-04 13:54:34 | [train_policy] epoch #505 | Computing KL before
2021-06-04 13:54:34 | [train_policy] epoch #505 | Optimizing
2021-06-04 13:54:34 | [train_policy] epoch #505 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:34 | [train_policy] epoch #505 | computing loss before
2021-06-04 13:54:34 | [train_policy] epoch #505 | computing gradient
2021-06-04 13:54:34 | [train_policy] epoch #505 | gradient computed
2021-06-04 13:54:34 | [train_policy] epoch #505 | computing descent direction
2021-06-04 13:54:34 | [train_policy] epoch #505 | descent direction computed
2021-06-04 13:54:34 | [train_policy] epoch #505 | backtrack iters: 1
2021-06-04 13:54:34 | [train_policy] epoch #505 | optimization finished
2021-06-04 13:54:34 | [train_policy] epoch #505 | Computing KL after
2021-06-04 13:54:34 | [train_policy] epoch #505 | Computing loss after
2021-06-04 13:54:34 | [train_policy] epoch #505 | Fitting baseline...
2021-06-04 13:54:34 | [train_policy] epoch #505 | Saving snapshot...
2021-06-04 13:54:34 | [train_policy] epoch #505 | Saved
2021-06-04 13:54:34 | [train_policy] epoch #505 | Time 406.22 s
2021-06-04 13:54:34 | [train_policy] epoch #505 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283661
Evaluation/AverageDiscountedReturn          -41.675
Evaluation/AverageReturn                    -41.675
Evaluation/CompletionRate                     0
Evaluation/Iteration                        505
Evaluation/MaxReturn                        -30.0004
Evaluation/MinReturn                        -63.191
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.80398
Extras/EpisodeRewardMean                    -41.5229
LinearFeatureBaseline/ExplainedVariance       0.902837
PolicyExecTime                                0.234896
ProcessExecTime                               0.0311568
TotalEnvSteps                            512072
policy/Entropy                               -1.47443
policy/KL                                     0.00667455
policy/KLBefore                               0
policy/LossAfter                             -0.0116101
policy/LossBefore                             2.35591e-10
policy/Perplexity                             0.22891
policy/dLoss                                  0.0116101
---------------------------------------  ----------------
2021-06-04 13:54:34 | [train_policy] epoch #506 | Obtaining samples for iteration 506...
2021-06-04 13:54:35 | [train_policy] epoch #506 | Logging diagnostics...
2021-06-04 13:54:35 | [train_policy] epoch #506 | Optimizing policy...
2021-06-04 13:54:35 | [train_policy] epoch #506 | Computing loss before
2021-06-04 13:54:35 | [train_policy] epoch #506 | Computing KL before
2021-06-04 13:54:35 | [train_policy] epoch #506 | Optimizing
2021-06-04 13:54:35 | [train_policy] epoch #506 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:35 | [train_policy] epoch #506 | computing loss before
2021-06-04 13:54:35 | [train_policy] epoch #506 | computing gradient
2021-06-04 13:54:35 | [train_policy] epoch #506 | gradient computed
2021-06-04 13:54:35 | [train_policy] epoch #506 | computing descent direction
2021-06-04 13:54:35 | [train_policy] epoch #506 | descent direction computed
2021-06-04 13:54:35 | [train_policy] epoch #506 | backtrack iters: 1
2021-06-04 13:54:35 | [train_policy] epoch #506 | optimization finished
2021-06-04 13:54:35 | [train_policy] epoch #506 | Computing KL after
2021-06-04 13:54:35 | [train_policy] epoch #506 | Computing loss after
2021-06-04 13:54:35 | [train_policy] epoch #506 | Fitting baseline...
2021-06-04 13:54:35 | [train_policy] epoch #506 | Saving snapshot...
2021-06-04 13:54:35 | [train_policy] epoch #506 | Saved
2021-06-04 13:54:35 | [train_policy] epoch #506 | Time 407.03 s
2021-06-04 13:54:35 | [train_policy] epoch #506 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285183
Evaluation/AverageDiscountedReturn          -67.5084
Evaluation/AverageReturn                    -67.5084
Evaluation/CompletionRate                     0
Evaluation/Iteration                        506
Evaluation/MaxReturn                        -29.0975
Evaluation/MinReturn                      -2062.08
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.958
Extras/EpisodeRewardMean                    -65.3835
LinearFeatureBaseline/ExplainedVariance       0.0140543
PolicyExecTime                                0.225872
ProcessExecTime                               0.0314479
TotalEnvSteps                            513084
policy/Entropy                               -1.46757
policy/KL                                     0.00646659
policy/KLBefore                               0
policy/LossAfter                             -0.0168185
policy/LossBefore                            -9.42366e-09
policy/Perplexity                             0.230484
policy/dLoss                                  0.0168185
---------------------------------------  ----------------
2021-06-04 13:54:35 | [train_policy] epoch #507 | Obtaining samples for iteration 507...
2021-06-04 13:54:35 | [train_policy] epoch #507 | Logging diagnostics...
2021-06-04 13:54:35 | [train_policy] epoch #507 | Optimizing policy...
2021-06-04 13:54:35 | [train_policy] epoch #507 | Computing loss before
2021-06-04 13:54:35 | [train_policy] epoch #507 | Computing KL before
2021-06-04 13:54:35 | [train_policy] epoch #507 | Optimizing
2021-06-04 13:54:35 | [train_policy] epoch #507 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:35 | [train_policy] epoch #507 | computing loss before
2021-06-04 13:54:35 | [train_policy] epoch #507 | computing gradient
2021-06-04 13:54:35 | [train_policy] epoch #507 | gradient computed
2021-06-04 13:54:35 | [train_policy] epoch #507 | computing descent direction
2021-06-04 13:54:36 | [train_policy] epoch #507 | descent direction computed
2021-06-04 13:54:36 | [train_policy] epoch #507 | backtrack iters: 0
2021-06-04 13:54:36 | [train_policy] epoch #507 | optimization finished
2021-06-04 13:54:36 | [train_policy] epoch #507 | Computing KL after
2021-06-04 13:54:36 | [train_policy] epoch #507 | Computing loss after
2021-06-04 13:54:36 | [train_policy] epoch #507 | Fitting baseline...
2021-06-04 13:54:36 | [train_policy] epoch #507 | Saving snapshot...
2021-06-04 13:54:36 | [train_policy] epoch #507 | Saved
2021-06-04 13:54:36 | [train_policy] epoch #507 | Time 407.85 s
2021-06-04 13:54:36 | [train_policy] epoch #507 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287997
Evaluation/AverageDiscountedReturn          -42.6437
Evaluation/AverageReturn                    -42.6437
Evaluation/CompletionRate                     0
Evaluation/Iteration                        507
Evaluation/MaxReturn                        -29.1993
Evaluation/MinReturn                        -63.1314
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.62205
Extras/EpisodeRewardMean                    -42.6762
LinearFeatureBaseline/ExplainedVariance     -15.2791
PolicyExecTime                                0.23258
ProcessExecTime                               0.0314145
TotalEnvSteps                            514096
policy/Entropy                               -1.46829
policy/KL                                     0.00980132
policy/KLBefore                               0
policy/LossAfter                             -0.0131104
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.230318
policy/dLoss                                  0.0131104
---------------------------------------  ----------------
2021-06-04 13:54:36 | [train_policy] epoch #508 | Obtaining samples for iteration 508...
2021-06-04 13:54:36 | [train_policy] epoch #508 | Logging diagnostics...
2021-06-04 13:54:36 | [train_policy] epoch #508 | Optimizing policy...
2021-06-04 13:54:36 | [train_policy] epoch #508 | Computing loss before
2021-06-04 13:54:36 | [train_policy] epoch #508 | Computing KL before
2021-06-04 13:54:36 | [train_policy] epoch #508 | Optimizing
2021-06-04 13:54:36 | [train_policy] epoch #508 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:36 | [train_policy] epoch #508 | computing loss before
2021-06-04 13:54:36 | [train_policy] epoch #508 | computing gradient
2021-06-04 13:54:36 | [train_policy] epoch #508 | gradient computed
2021-06-04 13:54:36 | [train_policy] epoch #508 | computing descent direction
2021-06-04 13:54:36 | [train_policy] epoch #508 | descent direction computed
2021-06-04 13:54:36 | [train_policy] epoch #508 | backtrack iters: 1
2021-06-04 13:54:36 | [train_policy] epoch #508 | optimization finished
2021-06-04 13:54:36 | [train_policy] epoch #508 | Computing KL after
2021-06-04 13:54:36 | [train_policy] epoch #508 | Computing loss after
2021-06-04 13:54:36 | [train_policy] epoch #508 | Fitting baseline...
2021-06-04 13:54:36 | [train_policy] epoch #508 | Saving snapshot...
2021-06-04 13:54:36 | [train_policy] epoch #508 | Saved
2021-06-04 13:54:36 | [train_policy] epoch #508 | Time 408.64 s
2021-06-04 13:54:36 | [train_policy] epoch #508 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.284266
Evaluation/AverageDiscountedReturn          -65.2202
Evaluation/AverageReturn                    -65.2202
Evaluation/CompletionRate                     0
Evaluation/Iteration                        508
Evaluation/MaxReturn                        -30.7285
Evaluation/MinReturn                      -2062.1
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.584
Extras/EpisodeRewardMean                    -63.2372
LinearFeatureBaseline/ExplainedVariance       0.012389
PolicyExecTime                                0.211774
ProcessExecTime                               0.0311844
TotalEnvSteps                            515108
policy/Entropy                               -1.42744
policy/KL                                     0.00678578
policy/KLBefore                               0
policy/LossAfter                             -0.0171572
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.239922
policy/dLoss                                  0.0171572
---------------------------------------  ---------------
2021-06-04 13:54:36 | [train_policy] epoch #509 | Obtaining samples for iteration 509...
2021-06-04 13:54:37 | [train_policy] epoch #509 | Logging diagnostics...
2021-06-04 13:54:37 | [train_policy] epoch #509 | Optimizing policy...
2021-06-04 13:54:37 | [train_policy] epoch #509 | Computing loss before
2021-06-04 13:54:37 | [train_policy] epoch #509 | Computing KL before
2021-06-04 13:54:37 | [train_policy] epoch #509 | Optimizing
2021-06-04 13:54:37 | [train_policy] epoch #509 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:37 | [train_policy] epoch #509 | computing loss before
2021-06-04 13:54:37 | [train_policy] epoch #509 | computing gradient
2021-06-04 13:54:37 | [train_policy] epoch #509 | gradient computed
2021-06-04 13:54:37 | [train_policy] epoch #509 | computing descent direction
2021-06-04 13:54:37 | [train_policy] epoch #509 | descent direction computed
2021-06-04 13:54:37 | [train_policy] epoch #509 | backtrack iters: 0
2021-06-04 13:54:37 | [train_policy] epoch #509 | optimization finished
2021-06-04 13:54:37 | [train_policy] epoch #509 | Computing KL after
2021-06-04 13:54:37 | [train_policy] epoch #509 | Computing loss after
2021-06-04 13:54:37 | [train_policy] epoch #509 | Fitting baseline...
2021-06-04 13:54:37 | [train_policy] epoch #509 | Saving snapshot...
2021-06-04 13:54:37 | [train_policy] epoch #509 | Saved
2021-06-04 13:54:37 | [train_policy] epoch #509 | Time 409.45 s
2021-06-04 13:54:37 | [train_policy] epoch #509 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283346
Evaluation/AverageDiscountedReturn          -42.6297
Evaluation/AverageReturn                    -42.6297
Evaluation/CompletionRate                     0
Evaluation/Iteration                        509
Evaluation/MaxReturn                        -30.5655
Evaluation/MinReturn                        -64.1032
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85996
Extras/EpisodeRewardMean                    -42.6859
LinearFeatureBaseline/ExplainedVariance     -16.7067
PolicyExecTime                                0.236538
ProcessExecTime                               0.031203
TotalEnvSteps                            516120
policy/Entropy                               -1.39325
policy/KL                                     0.00993567
policy/KLBefore                               0
policy/LossAfter                             -0.0286099
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.248267
policy/dLoss                                  0.0286099
---------------------------------------  ----------------
2021-06-04 13:54:37 | [train_policy] epoch #510 | Obtaining samples for iteration 510...
2021-06-04 13:54:38 | [train_policy] epoch #510 | Logging diagnostics...
2021-06-04 13:54:38 | [train_policy] epoch #510 | Optimizing policy...
2021-06-04 13:54:38 | [train_policy] epoch #510 | Computing loss before
2021-06-04 13:54:38 | [train_policy] epoch #510 | Computing KL before
2021-06-04 13:54:38 | [train_policy] epoch #510 | Optimizing
2021-06-04 13:54:38 | [train_policy] epoch #510 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:38 | [train_policy] epoch #510 | computing loss before
2021-06-04 13:54:38 | [train_policy] epoch #510 | computing gradient
2021-06-04 13:54:38 | [train_policy] epoch #510 | gradient computed
2021-06-04 13:54:38 | [train_policy] epoch #510 | computing descent direction
2021-06-04 13:54:38 | [train_policy] epoch #510 | descent direction computed
2021-06-04 13:54:38 | [train_policy] epoch #510 | backtrack iters: 0
2021-06-04 13:54:38 | [train_policy] epoch #510 | optimization finished
2021-06-04 13:54:38 | [train_policy] epoch #510 | Computing KL after
2021-06-04 13:54:38 | [train_policy] epoch #510 | Computing loss after
2021-06-04 13:54:38 | [train_policy] epoch #510 | Fitting baseline...
2021-06-04 13:54:38 | [train_policy] epoch #510 | Saving snapshot...
2021-06-04 13:54:38 | [train_policy] epoch #510 | Saved
2021-06-04 13:54:38 | [train_policy] epoch #510 | Time 410.26 s
2021-06-04 13:54:38 | [train_policy] epoch #510 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284268
Evaluation/AverageDiscountedReturn          -42.845
Evaluation/AverageReturn                    -42.845
Evaluation/CompletionRate                     0
Evaluation/Iteration                        510
Evaluation/MaxReturn                        -31.7181
Evaluation/MinReturn                        -92.2806
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.6167
Extras/EpisodeRewardMean                    -42.8527
LinearFeatureBaseline/ExplainedVariance       0.869181
PolicyExecTime                                0.232145
ProcessExecTime                               0.0311284
TotalEnvSteps                            517132
policy/Entropy                               -1.37879
policy/KL                                     0.00939544
policy/KLBefore                               0
policy/LossAfter                             -0.0162563
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.251883
policy/dLoss                                  0.0162563
---------------------------------------  ----------------
2021-06-04 13:54:38 | [train_policy] epoch #511 | Obtaining samples for iteration 511...
2021-06-04 13:54:39 | [train_policy] epoch #511 | Logging diagnostics...
2021-06-04 13:54:39 | [train_policy] epoch #511 | Optimizing policy...
2021-06-04 13:54:39 | [train_policy] epoch #511 | Computing loss before
2021-06-04 13:54:39 | [train_policy] epoch #511 | Computing KL before
2021-06-04 13:54:39 | [train_policy] epoch #511 | Optimizing
2021-06-04 13:54:39 | [train_policy] epoch #511 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:39 | [train_policy] epoch #511 | computing loss before
2021-06-04 13:54:39 | [train_policy] epoch #511 | computing gradient
2021-06-04 13:54:39 | [train_policy] epoch #511 | gradient computed
2021-06-04 13:54:39 | [train_policy] epoch #511 | computing descent direction
2021-06-04 13:54:39 | [train_policy] epoch #511 | descent direction computed
2021-06-04 13:54:39 | [train_policy] epoch #511 | backtrack iters: 1
2021-06-04 13:54:39 | [train_policy] epoch #511 | optimization finished
2021-06-04 13:54:39 | [train_policy] epoch #511 | Computing KL after
2021-06-04 13:54:39 | [train_policy] epoch #511 | Computing loss after
2021-06-04 13:54:39 | [train_policy] epoch #511 | Fitting baseline...
2021-06-04 13:54:39 | [train_policy] epoch #511 | Saving snapshot...
2021-06-04 13:54:39 | [train_policy] epoch #511 | Saved
2021-06-04 13:54:39 | [train_policy] epoch #511 | Time 411.07 s
2021-06-04 13:54:39 | [train_policy] epoch #511 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285728
Evaluation/AverageDiscountedReturn          -42.702
Evaluation/AverageReturn                    -42.702
Evaluation/CompletionRate                     0
Evaluation/Iteration                        511
Evaluation/MaxReturn                        -32.0121
Evaluation/MinReturn                        -63.2943
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.26865
Extras/EpisodeRewardMean                    -42.9013
LinearFeatureBaseline/ExplainedVariance       0.909124
PolicyExecTime                                0.224332
ProcessExecTime                               0.0313601
TotalEnvSteps                            518144
policy/Entropy                               -1.41053
policy/KL                                     0.00657551
policy/KLBefore                               0
policy/LossAfter                             -0.0178153
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.244013
policy/dLoss                                  0.0178153
---------------------------------------  ----------------
2021-06-04 13:54:39 | [train_policy] epoch #512 | Obtaining samples for iteration 512...
2021-06-04 13:54:39 | [train_policy] epoch #512 | Logging diagnostics...
2021-06-04 13:54:39 | [train_policy] epoch #512 | Optimizing policy...
2021-06-04 13:54:39 | [train_policy] epoch #512 | Computing loss before
2021-06-04 13:54:40 | [train_policy] epoch #512 | Computing KL before
2021-06-04 13:54:40 | [train_policy] epoch #512 | Optimizing
2021-06-04 13:54:40 | [train_policy] epoch #512 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:40 | [train_policy] epoch #512 | computing loss before
2021-06-04 13:54:40 | [train_policy] epoch #512 | computing gradient
2021-06-04 13:54:40 | [train_policy] epoch #512 | gradient computed
2021-06-04 13:54:40 | [train_policy] epoch #512 | computing descent direction
2021-06-04 13:54:40 | [train_policy] epoch #512 | descent direction computed
2021-06-04 13:54:40 | [train_policy] epoch #512 | backtrack iters: 0
2021-06-04 13:54:40 | [train_policy] epoch #512 | optimization finished
2021-06-04 13:54:40 | [train_policy] epoch #512 | Computing KL after
2021-06-04 13:54:40 | [train_policy] epoch #512 | Computing loss after
2021-06-04 13:54:40 | [train_policy] epoch #512 | Fitting baseline...
2021-06-04 13:54:40 | [train_policy] epoch #512 | Saving snapshot...
2021-06-04 13:54:40 | [train_policy] epoch #512 | Saved
2021-06-04 13:54:40 | [train_policy] epoch #512 | Time 411.87 s
2021-06-04 13:54:40 | [train_policy] epoch #512 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283638
Evaluation/AverageDiscountedReturn          -41.2092
Evaluation/AverageReturn                    -41.2092
Evaluation/CompletionRate                     0
Evaluation/Iteration                        512
Evaluation/MaxReturn                        -30.0453
Evaluation/MinReturn                        -63.3601
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.93196
Extras/EpisodeRewardMean                    -41.1476
LinearFeatureBaseline/ExplainedVariance       0.917001
PolicyExecTime                                0.223875
ProcessExecTime                               0.031054
TotalEnvSteps                            519156
policy/Entropy                               -1.42376
policy/KL                                     0.00986959
policy/KLBefore                               0
policy/LossAfter                             -0.0214696
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.240807
policy/dLoss                                  0.0214696
---------------------------------------  ----------------
2021-06-04 13:54:40 | [train_policy] epoch #513 | Obtaining samples for iteration 513...
2021-06-04 13:54:40 | [train_policy] epoch #513 | Logging diagnostics...
2021-06-04 13:54:40 | [train_policy] epoch #513 | Optimizing policy...
2021-06-04 13:54:40 | [train_policy] epoch #513 | Computing loss before
2021-06-04 13:54:40 | [train_policy] epoch #513 | Computing KL before
2021-06-04 13:54:40 | [train_policy] epoch #513 | Optimizing
2021-06-04 13:54:40 | [train_policy] epoch #513 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:40 | [train_policy] epoch #513 | computing loss before
2021-06-04 13:54:40 | [train_policy] epoch #513 | computing gradient
2021-06-04 13:54:40 | [train_policy] epoch #513 | gradient computed
2021-06-04 13:54:40 | [train_policy] epoch #513 | computing descent direction
2021-06-04 13:54:40 | [train_policy] epoch #513 | descent direction computed
2021-06-04 13:54:40 | [train_policy] epoch #513 | backtrack iters: 0
2021-06-04 13:54:40 | [train_policy] epoch #513 | optimization finished
2021-06-04 13:54:40 | [train_policy] epoch #513 | Computing KL after
2021-06-04 13:54:40 | [train_policy] epoch #513 | Computing loss after
2021-06-04 13:54:40 | [train_policy] epoch #513 | Fitting baseline...
2021-06-04 13:54:40 | [train_policy] epoch #513 | Saving snapshot...
2021-06-04 13:54:40 | [train_policy] epoch #513 | Saved
2021-06-04 13:54:40 | [train_policy] epoch #513 | Time 412.65 s
2021-06-04 13:54:40 | [train_policy] epoch #513 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.285069
Evaluation/AverageDiscountedReturn          -43.0748
Evaluation/AverageReturn                    -43.0748
Evaluation/CompletionRate                     0
Evaluation/Iteration                        513
Evaluation/MaxReturn                        -29.464
Evaluation/MinReturn                        -93.4325
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.36626
Extras/EpisodeRewardMean                    -42.8209
LinearFeatureBaseline/ExplainedVariance       0.856188
PolicyExecTime                                0.209933
ProcessExecTime                               0.0311556
TotalEnvSteps                            520168
policy/Entropy                               -1.41359
policy/KL                                     0.00952953
policy/KLBefore                               0
policy/LossAfter                             -0.0291804
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.243268
policy/dLoss                                  0.0291804
---------------------------------------  ----------------
2021-06-04 13:54:40 | [train_policy] epoch #514 | Obtaining samples for iteration 514...
2021-06-04 13:54:41 | [train_policy] epoch #514 | Logging diagnostics...
2021-06-04 13:54:41 | [train_policy] epoch #514 | Optimizing policy...
2021-06-04 13:54:41 | [train_policy] epoch #514 | Computing loss before
2021-06-04 13:54:41 | [train_policy] epoch #514 | Computing KL before
2021-06-04 13:54:41 | [train_policy] epoch #514 | Optimizing
2021-06-04 13:54:41 | [train_policy] epoch #514 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:41 | [train_policy] epoch #514 | computing loss before
2021-06-04 13:54:41 | [train_policy] epoch #514 | computing gradient
2021-06-04 13:54:41 | [train_policy] epoch #514 | gradient computed
2021-06-04 13:54:41 | [train_policy] epoch #514 | computing descent direction
2021-06-04 13:54:41 | [train_policy] epoch #514 | descent direction computed
2021-06-04 13:54:41 | [train_policy] epoch #514 | backtrack iters: 1
2021-06-04 13:54:41 | [train_policy] epoch #514 | optimization finished
2021-06-04 13:54:41 | [train_policy] epoch #514 | Computing KL after
2021-06-04 13:54:41 | [train_policy] epoch #514 | Computing loss after
2021-06-04 13:54:41 | [train_policy] epoch #514 | Fitting baseline...
2021-06-04 13:54:41 | [train_policy] epoch #514 | Saving snapshot...
2021-06-04 13:54:41 | [train_policy] epoch #514 | Saved
2021-06-04 13:54:41 | [train_policy] epoch #514 | Time 413.46 s
2021-06-04 13:54:41 | [train_policy] epoch #514 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284205
Evaluation/AverageDiscountedReturn          -41.724
Evaluation/AverageReturn                    -41.724
Evaluation/CompletionRate                     0
Evaluation/Iteration                        514
Evaluation/MaxReturn                        -29.4807
Evaluation/MinReturn                        -58.4951
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.30053
Extras/EpisodeRewardMean                    -41.8632
LinearFeatureBaseline/ExplainedVariance       0.85446
PolicyExecTime                                0.228692
ProcessExecTime                               0.0311317
TotalEnvSteps                            521180
policy/Entropy                               -1.42082
policy/KL                                     0.00641117
policy/KLBefore                               0
policy/LossAfter                             -0.019966
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.241517
policy/dLoss                                  0.019966
---------------------------------------  ----------------
2021-06-04 13:54:41 | [train_policy] epoch #515 | Obtaining samples for iteration 515...
2021-06-04 13:54:42 | [train_policy] epoch #515 | Logging diagnostics...
2021-06-04 13:54:42 | [train_policy] epoch #515 | Optimizing policy...
2021-06-04 13:54:42 | [train_policy] epoch #515 | Computing loss before
2021-06-04 13:54:42 | [train_policy] epoch #515 | Computing KL before
2021-06-04 13:54:42 | [train_policy] epoch #515 | Optimizing
2021-06-04 13:54:42 | [train_policy] epoch #515 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:42 | [train_policy] epoch #515 | computing loss before
2021-06-04 13:54:42 | [train_policy] epoch #515 | computing gradient
2021-06-04 13:54:42 | [train_policy] epoch #515 | gradient computed
2021-06-04 13:54:42 | [train_policy] epoch #515 | computing descent direction
2021-06-04 13:54:42 | [train_policy] epoch #515 | descent direction computed
2021-06-04 13:54:42 | [train_policy] epoch #515 | backtrack iters: 0
2021-06-04 13:54:42 | [train_policy] epoch #515 | optimization finished
2021-06-04 13:54:42 | [train_policy] epoch #515 | Computing KL after
2021-06-04 13:54:42 | [train_policy] epoch #515 | Computing loss after
2021-06-04 13:54:42 | [train_policy] epoch #515 | Fitting baseline...
2021-06-04 13:54:42 | [train_policy] epoch #515 | Saving snapshot...
2021-06-04 13:54:42 | [train_policy] epoch #515 | Saved
2021-06-04 13:54:42 | [train_policy] epoch #515 | Time 414.26 s
2021-06-04 13:54:42 | [train_policy] epoch #515 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284221
Evaluation/AverageDiscountedReturn          -42.1991
Evaluation/AverageReturn                    -42.1991
Evaluation/CompletionRate                     0
Evaluation/Iteration                        515
Evaluation/MaxReturn                        -29.665
Evaluation/MinReturn                       -119.322
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.932
Extras/EpisodeRewardMean                    -42.1454
LinearFeatureBaseline/ExplainedVariance       0.737287
PolicyExecTime                                0.229937
ProcessExecTime                               0.0312405
TotalEnvSteps                            522192
policy/Entropy                               -1.40358
policy/KL                                     0.00967283
policy/KLBefore                               0
policy/LossAfter                             -0.0190709
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.245717
policy/dLoss                                  0.0190709
---------------------------------------  ----------------
2021-06-04 13:54:42 | [train_policy] epoch #516 | Obtaining samples for iteration 516...
2021-06-04 13:54:43 | [train_policy] epoch #516 | Logging diagnostics...
2021-06-04 13:54:43 | [train_policy] epoch #516 | Optimizing policy...
2021-06-04 13:54:43 | [train_policy] epoch #516 | Computing loss before
2021-06-04 13:54:43 | [train_policy] epoch #516 | Computing KL before
2021-06-04 13:54:43 | [train_policy] epoch #516 | Optimizing
2021-06-04 13:54:43 | [train_policy] epoch #516 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:43 | [train_policy] epoch #516 | computing loss before
2021-06-04 13:54:43 | [train_policy] epoch #516 | computing gradient
2021-06-04 13:54:43 | [train_policy] epoch #516 | gradient computed
2021-06-04 13:54:43 | [train_policy] epoch #516 | computing descent direction
2021-06-04 13:54:43 | [train_policy] epoch #516 | descent direction computed
2021-06-04 13:54:43 | [train_policy] epoch #516 | backtrack iters: 1
2021-06-04 13:54:43 | [train_policy] epoch #516 | optimization finished
2021-06-04 13:54:43 | [train_policy] epoch #516 | Computing KL after
2021-06-04 13:54:43 | [train_policy] epoch #516 | Computing loss after
2021-06-04 13:54:43 | [train_policy] epoch #516 | Fitting baseline...
2021-06-04 13:54:43 | [train_policy] epoch #516 | Saving snapshot...
2021-06-04 13:54:43 | [train_policy] epoch #516 | Saved
2021-06-04 13:54:43 | [train_policy] epoch #516 | Time 415.07 s
2021-06-04 13:54:43 | [train_policy] epoch #516 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.285612
Evaluation/AverageDiscountedReturn          -43.3845
Evaluation/AverageReturn                    -43.3845
Evaluation/CompletionRate                     0
Evaluation/Iteration                        516
Evaluation/MaxReturn                        -31.2013
Evaluation/MinReturn                       -101.797
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.77205
Extras/EpisodeRewardMean                    -43.7507
LinearFeatureBaseline/ExplainedVariance       0.8326
PolicyExecTime                                0.225499
ProcessExecTime                               0.0315492
TotalEnvSteps                            523204
policy/Entropy                               -1.40668
policy/KL                                     0.00682348
policy/KLBefore                               0
policy/LossAfter                             -0.0203548
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.244955
policy/dLoss                                  0.0203548
---------------------------------------  ---------------
2021-06-04 13:54:43 | [train_policy] epoch #517 | Obtaining samples for iteration 517...
2021-06-04 13:54:43 | [train_policy] epoch #517 | Logging diagnostics...
2021-06-04 13:54:43 | [train_policy] epoch #517 | Optimizing policy...
2021-06-04 13:54:43 | [train_policy] epoch #517 | Computing loss before
2021-06-04 13:54:43 | [train_policy] epoch #517 | Computing KL before
2021-06-04 13:54:43 | [train_policy] epoch #517 | Optimizing
2021-06-04 13:54:43 | [train_policy] epoch #517 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:43 | [train_policy] epoch #517 | computing loss before
2021-06-04 13:54:43 | [train_policy] epoch #517 | computing gradient
2021-06-04 13:54:43 | [train_policy] epoch #517 | gradient computed
2021-06-04 13:54:43 | [train_policy] epoch #517 | computing descent direction
2021-06-04 13:54:44 | [train_policy] epoch #517 | descent direction computed
2021-06-04 13:54:44 | [train_policy] epoch #517 | backtrack iters: 1
2021-06-04 13:54:44 | [train_policy] epoch #517 | optimization finished
2021-06-04 13:54:44 | [train_policy] epoch #517 | Computing KL after
2021-06-04 13:54:44 | [train_policy] epoch #517 | Computing loss after
2021-06-04 13:54:44 | [train_policy] epoch #517 | Fitting baseline...
2021-06-04 13:54:44 | [train_policy] epoch #517 | Saving snapshot...
2021-06-04 13:54:44 | [train_policy] epoch #517 | Saved
2021-06-04 13:54:44 | [train_policy] epoch #517 | Time 415.86 s
2021-06-04 13:54:44 | [train_policy] epoch #517 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284791
Evaluation/AverageDiscountedReturn          -42.7912
Evaluation/AverageReturn                    -42.7912
Evaluation/CompletionRate                     0
Evaluation/Iteration                        517
Evaluation/MaxReturn                        -28.3151
Evaluation/MinReturn                        -97.0012
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.85389
Extras/EpisodeRewardMean                    -43.0424
LinearFeatureBaseline/ExplainedVariance       0.833147
PolicyExecTime                                0.212123
ProcessExecTime                               0.031188
TotalEnvSteps                            524216
policy/Entropy                               -1.42007
policy/KL                                     0.00669141
policy/KLBefore                               0
policy/LossAfter                             -0.0146609
policy/LossBefore                            -1.57846e-08
policy/Perplexity                             0.241696
policy/dLoss                                  0.0146609
---------------------------------------  ----------------
2021-06-04 13:54:44 | [train_policy] epoch #518 | Obtaining samples for iteration 518...
2021-06-04 13:54:44 | [train_policy] epoch #518 | Logging diagnostics...
2021-06-04 13:54:44 | [train_policy] epoch #518 | Optimizing policy...
2021-06-04 13:54:44 | [train_policy] epoch #518 | Computing loss before
2021-06-04 13:54:44 | [train_policy] epoch #518 | Computing KL before
2021-06-04 13:54:44 | [train_policy] epoch #518 | Optimizing
2021-06-04 13:54:44 | [train_policy] epoch #518 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:44 | [train_policy] epoch #518 | computing loss before
2021-06-04 13:54:44 | [train_policy] epoch #518 | computing gradient
2021-06-04 13:54:44 | [train_policy] epoch #518 | gradient computed
2021-06-04 13:54:44 | [train_policy] epoch #518 | computing descent direction
2021-06-04 13:54:44 | [train_policy] epoch #518 | descent direction computed
2021-06-04 13:54:44 | [train_policy] epoch #518 | backtrack iters: 0
2021-06-04 13:54:44 | [train_policy] epoch #518 | optimization finished
2021-06-04 13:54:44 | [train_policy] epoch #518 | Computing KL after
2021-06-04 13:54:44 | [train_policy] epoch #518 | Computing loss after
2021-06-04 13:54:44 | [train_policy] epoch #518 | Fitting baseline...
2021-06-04 13:54:44 | [train_policy] epoch #518 | Saving snapshot...
2021-06-04 13:54:44 | [train_policy] epoch #518 | Saved
2021-06-04 13:54:44 | [train_policy] epoch #518 | Time 416.66 s
2021-06-04 13:54:44 | [train_policy] epoch #518 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284193
Evaluation/AverageDiscountedReturn          -41.1531
Evaluation/AverageReturn                    -41.1531
Evaluation/CompletionRate                     0
Evaluation/Iteration                        518
Evaluation/MaxReturn                        -29.1549
Evaluation/MinReturn                        -64.089
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.19207
Extras/EpisodeRewardMean                    -40.8252
LinearFeatureBaseline/ExplainedVariance       0.908608
PolicyExecTime                                0.232414
ProcessExecTime                               0.0310187
TotalEnvSteps                            525228
policy/Entropy                               -1.39239
policy/KL                                     0.009674
policy/KLBefore                               0
policy/LossAfter                             -0.0245889
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.24848
policy/dLoss                                  0.0245889
---------------------------------------  ----------------
2021-06-04 13:54:44 | [train_policy] epoch #519 | Obtaining samples for iteration 519...
2021-06-04 13:54:45 | [train_policy] epoch #519 | Logging diagnostics...
2021-06-04 13:54:45 | [train_policy] epoch #519 | Optimizing policy...
2021-06-04 13:54:45 | [train_policy] epoch #519 | Computing loss before
2021-06-04 13:54:45 | [train_policy] epoch #519 | Computing KL before
2021-06-04 13:54:45 | [train_policy] epoch #519 | Optimizing
2021-06-04 13:54:45 | [train_policy] epoch #519 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:45 | [train_policy] epoch #519 | computing loss before
2021-06-04 13:54:45 | [train_policy] epoch #519 | computing gradient
2021-06-04 13:54:45 | [train_policy] epoch #519 | gradient computed
2021-06-04 13:54:45 | [train_policy] epoch #519 | computing descent direction
2021-06-04 13:54:45 | [train_policy] epoch #519 | descent direction computed
2021-06-04 13:54:45 | [train_policy] epoch #519 | backtrack iters: 0
2021-06-04 13:54:45 | [train_policy] epoch #519 | optimization finished
2021-06-04 13:54:45 | [train_policy] epoch #519 | Computing KL after
2021-06-04 13:54:45 | [train_policy] epoch #519 | Computing loss after
2021-06-04 13:54:45 | [train_policy] epoch #519 | Fitting baseline...
2021-06-04 13:54:45 | [train_policy] epoch #519 | Saving snapshot...
2021-06-04 13:54:45 | [train_policy] epoch #519 | Saved
2021-06-04 13:54:45 | [train_policy] epoch #519 | Time 417.46 s
2021-06-04 13:54:45 | [train_policy] epoch #519 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28449
Evaluation/AverageDiscountedReturn          -41.764
Evaluation/AverageReturn                    -41.764
Evaluation/CompletionRate                     0
Evaluation/Iteration                        519
Evaluation/MaxReturn                        -29.9622
Evaluation/MinReturn                        -64.1664
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.49646
Extras/EpisodeRewardMean                    -41.96
LinearFeatureBaseline/ExplainedVariance       0.910269
PolicyExecTime                                0.220675
ProcessExecTime                               0.0311797
TotalEnvSteps                            526240
policy/Entropy                               -1.39594
policy/KL                                     0.00991271
policy/KLBefore                               0
policy/LossAfter                             -0.0241172
policy/LossBefore                             1.41355e-08
policy/Perplexity                             0.2476
policy/dLoss                                  0.0241172
---------------------------------------  ----------------
2021-06-04 13:54:45 | [train_policy] epoch #520 | Obtaining samples for iteration 520...
2021-06-04 13:54:46 | [train_policy] epoch #520 | Logging diagnostics...
2021-06-04 13:54:46 | [train_policy] epoch #520 | Optimizing policy...
2021-06-04 13:54:46 | [train_policy] epoch #520 | Computing loss before
2021-06-04 13:54:46 | [train_policy] epoch #520 | Computing KL before
2021-06-04 13:54:46 | [train_policy] epoch #520 | Optimizing
2021-06-04 13:54:46 | [train_policy] epoch #520 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:46 | [train_policy] epoch #520 | computing loss before
2021-06-04 13:54:46 | [train_policy] epoch #520 | computing gradient
2021-06-04 13:54:46 | [train_policy] epoch #520 | gradient computed
2021-06-04 13:54:46 | [train_policy] epoch #520 | computing descent direction
2021-06-04 13:54:46 | [train_policy] epoch #520 | descent direction computed
2021-06-04 13:54:46 | [train_policy] epoch #520 | backtrack iters: 1
2021-06-04 13:54:46 | [train_policy] epoch #520 | optimization finished
2021-06-04 13:54:46 | [train_policy] epoch #520 | Computing KL after
2021-06-04 13:54:46 | [train_policy] epoch #520 | Computing loss after
2021-06-04 13:54:46 | [train_policy] epoch #520 | Fitting baseline...
2021-06-04 13:54:46 | [train_policy] epoch #520 | Saving snapshot...
2021-06-04 13:54:46 | [train_policy] epoch #520 | Saved
2021-06-04 13:54:46 | [train_policy] epoch #520 | Time 418.26 s
2021-06-04 13:54:46 | [train_policy] epoch #520 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284012
Evaluation/AverageDiscountedReturn          -44.5678
Evaluation/AverageReturn                    -44.5678
Evaluation/CompletionRate                     0
Evaluation/Iteration                        520
Evaluation/MaxReturn                        -28.3613
Evaluation/MinReturn                        -94.394
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.1801
Extras/EpisodeRewardMean                    -44.4588
LinearFeatureBaseline/ExplainedVariance       0.815367
PolicyExecTime                                0.22243
ProcessExecTime                               0.0311468
TotalEnvSteps                            527252
policy/Entropy                               -1.38474
policy/KL                                     0.00672177
policy/KLBefore                               0
policy/LossAfter                             -0.0213805
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.250388
policy/dLoss                                  0.0213805
---------------------------------------  ----------------
2021-06-04 13:54:46 | [train_policy] epoch #521 | Obtaining samples for iteration 521...
2021-06-04 13:54:47 | [train_policy] epoch #521 | Logging diagnostics...
2021-06-04 13:54:47 | [train_policy] epoch #521 | Optimizing policy...
2021-06-04 13:54:47 | [train_policy] epoch #521 | Computing loss before
2021-06-04 13:54:47 | [train_policy] epoch #521 | Computing KL before
2021-06-04 13:54:47 | [train_policy] epoch #521 | Optimizing
2021-06-04 13:54:47 | [train_policy] epoch #521 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:47 | [train_policy] epoch #521 | computing loss before
2021-06-04 13:54:47 | [train_policy] epoch #521 | computing gradient
2021-06-04 13:54:47 | [train_policy] epoch #521 | gradient computed
2021-06-04 13:54:47 | [train_policy] epoch #521 | computing descent direction
2021-06-04 13:54:47 | [train_policy] epoch #521 | descent direction computed
2021-06-04 13:54:47 | [train_policy] epoch #521 | backtrack iters: 1
2021-06-04 13:54:47 | [train_policy] epoch #521 | optimization finished
2021-06-04 13:54:47 | [train_policy] epoch #521 | Computing KL after
2021-06-04 13:54:47 | [train_policy] epoch #521 | Computing loss after
2021-06-04 13:54:47 | [train_policy] epoch #521 | Fitting baseline...
2021-06-04 13:54:47 | [train_policy] epoch #521 | Saving snapshot...
2021-06-04 13:54:47 | [train_policy] epoch #521 | Saved
2021-06-04 13:54:47 | [train_policy] epoch #521 | Time 419.08 s
2021-06-04 13:54:47 | [train_policy] epoch #521 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285182
Evaluation/AverageDiscountedReturn          -42.3012
Evaluation/AverageReturn                    -42.3012
Evaluation/CompletionRate                     0
Evaluation/Iteration                        521
Evaluation/MaxReturn                        -30.573
Evaluation/MinReturn                        -60.1942
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.06223
Extras/EpisodeRewardMean                    -42.7618
LinearFeatureBaseline/ExplainedVariance       0.898354
PolicyExecTime                                0.238802
ProcessExecTime                               0.0311556
TotalEnvSteps                            528264
policy/Entropy                               -1.40743
policy/KL                                     0.00642988
policy/KLBefore                               0
policy/LossAfter                             -0.0207885
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.244771
policy/dLoss                                  0.0207885
---------------------------------------  ----------------
2021-06-04 13:54:47 | [train_policy] epoch #522 | Obtaining samples for iteration 522...
2021-06-04 13:54:47 | [train_policy] epoch #522 | Logging diagnostics...
2021-06-04 13:54:47 | [train_policy] epoch #522 | Optimizing policy...
2021-06-04 13:54:47 | [train_policy] epoch #522 | Computing loss before
2021-06-04 13:54:47 | [train_policy] epoch #522 | Computing KL before
2021-06-04 13:54:48 | [train_policy] epoch #522 | Optimizing
2021-06-04 13:54:48 | [train_policy] epoch #522 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:48 | [train_policy] epoch #522 | computing loss before
2021-06-04 13:54:48 | [train_policy] epoch #522 | computing gradient
2021-06-04 13:54:48 | [train_policy] epoch #522 | gradient computed
2021-06-04 13:54:48 | [train_policy] epoch #522 | computing descent direction
2021-06-04 13:54:48 | [train_policy] epoch #522 | descent direction computed
2021-06-04 13:54:48 | [train_policy] epoch #522 | backtrack iters: 0
2021-06-04 13:54:48 | [train_policy] epoch #522 | optimization finished
2021-06-04 13:54:48 | [train_policy] epoch #522 | Computing KL after
2021-06-04 13:54:48 | [train_policy] epoch #522 | Computing loss after
2021-06-04 13:54:48 | [train_policy] epoch #522 | Fitting baseline...
2021-06-04 13:54:48 | [train_policy] epoch #522 | Saving snapshot...
2021-06-04 13:54:48 | [train_policy] epoch #522 | Saved
2021-06-04 13:54:48 | [train_policy] epoch #522 | Time 419.87 s
2021-06-04 13:54:48 | [train_policy] epoch #522 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283131
Evaluation/AverageDiscountedReturn          -42.0273
Evaluation/AverageReturn                    -42.0273
Evaluation/CompletionRate                     0
Evaluation/Iteration                        522
Evaluation/MaxReturn                        -28.3276
Evaluation/MinReturn                        -84.8292
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.60049
Extras/EpisodeRewardMean                    -42.049
LinearFeatureBaseline/ExplainedVariance       0.887182
PolicyExecTime                                0.2105
ProcessExecTime                               0.0311003
TotalEnvSteps                            529276
policy/Entropy                               -1.44136
policy/KL                                     0.00966226
policy/KLBefore                               0
policy/LossAfter                             -0.0209024
policy/LossBefore                            -3.20993e-09
policy/Perplexity                             0.236606
policy/dLoss                                  0.0209024
---------------------------------------  ----------------
2021-06-04 13:54:48 | [train_policy] epoch #523 | Obtaining samples for iteration 523...
2021-06-04 13:54:48 | [train_policy] epoch #523 | Logging diagnostics...
2021-06-04 13:54:48 | [train_policy] epoch #523 | Optimizing policy...
2021-06-04 13:54:48 | [train_policy] epoch #523 | Computing loss before
2021-06-04 13:54:48 | [train_policy] epoch #523 | Computing KL before
2021-06-04 13:54:48 | [train_policy] epoch #523 | Optimizing
2021-06-04 13:54:48 | [train_policy] epoch #523 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:48 | [train_policy] epoch #523 | computing loss before
2021-06-04 13:54:48 | [train_policy] epoch #523 | computing gradient
2021-06-04 13:54:48 | [train_policy] epoch #523 | gradient computed
2021-06-04 13:54:48 | [train_policy] epoch #523 | computing descent direction
2021-06-04 13:54:48 | [train_policy] epoch #523 | descent direction computed
2021-06-04 13:54:48 | [train_policy] epoch #523 | backtrack iters: 0
2021-06-04 13:54:48 | [train_policy] epoch #523 | optimization finished
2021-06-04 13:54:48 | [train_policy] epoch #523 | Computing KL after
2021-06-04 13:54:48 | [train_policy] epoch #523 | Computing loss after
2021-06-04 13:54:48 | [train_policy] epoch #523 | Fitting baseline...
2021-06-04 13:54:48 | [train_policy] epoch #523 | Saving snapshot...
2021-06-04 13:54:48 | [train_policy] epoch #523 | Saved
2021-06-04 13:54:48 | [train_policy] epoch #523 | Time 420.67 s
2021-06-04 13:54:48 | [train_policy] epoch #523 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283222
Evaluation/AverageDiscountedReturn          -42.3787
Evaluation/AverageReturn                    -42.3787
Evaluation/CompletionRate                     0
Evaluation/Iteration                        523
Evaluation/MaxReturn                        -28.0148
Evaluation/MinReturn                        -83.4089
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.9979
Extras/EpisodeRewardMean                    -42.0641
LinearFeatureBaseline/ExplainedVariance       0.891447
PolicyExecTime                                0.225562
ProcessExecTime                               0.0311177
TotalEnvSteps                            530288
policy/Entropy                               -1.41605
policy/KL                                     0.00974129
policy/KLBefore                               0
policy/LossAfter                             -0.020284
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             0.24267
policy/dLoss                                  0.020284
---------------------------------------  ----------------
2021-06-04 13:54:48 | [train_policy] epoch #524 | Obtaining samples for iteration 524...
2021-06-04 13:54:49 | [train_policy] epoch #524 | Logging diagnostics...
2021-06-04 13:54:49 | [train_policy] epoch #524 | Optimizing policy...
2021-06-04 13:54:49 | [train_policy] epoch #524 | Computing loss before
2021-06-04 13:54:49 | [train_policy] epoch #524 | Computing KL before
2021-06-04 13:54:49 | [train_policy] epoch #524 | Optimizing
2021-06-04 13:54:49 | [train_policy] epoch #524 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:49 | [train_policy] epoch #524 | computing loss before
2021-06-04 13:54:49 | [train_policy] epoch #524 | computing gradient
2021-06-04 13:54:49 | [train_policy] epoch #524 | gradient computed
2021-06-04 13:54:49 | [train_policy] epoch #524 | computing descent direction
2021-06-04 13:54:49 | [train_policy] epoch #524 | descent direction computed
2021-06-04 13:54:49 | [train_policy] epoch #524 | backtrack iters: 1
2021-06-04 13:54:49 | [train_policy] epoch #524 | optimization finished
2021-06-04 13:54:49 | [train_policy] epoch #524 | Computing KL after
2021-06-04 13:54:49 | [train_policy] epoch #524 | Computing loss after
2021-06-04 13:54:49 | [train_policy] epoch #524 | Fitting baseline...
2021-06-04 13:54:49 | [train_policy] epoch #524 | Saving snapshot...
2021-06-04 13:54:49 | [train_policy] epoch #524 | Saved
2021-06-04 13:54:49 | [train_policy] epoch #524 | Time 421.49 s
2021-06-04 13:54:49 | [train_policy] epoch #524 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.293972
Evaluation/AverageDiscountedReturn          -42.5085
Evaluation/AverageReturn                    -42.5085
Evaluation/CompletionRate                     0
Evaluation/Iteration                        524
Evaluation/MaxReturn                        -29.8659
Evaluation/MinReturn                       -112.943
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.0073
Extras/EpisodeRewardMean                    -42.3812
LinearFeatureBaseline/ExplainedVariance       0.780786
PolicyExecTime                                0.21777
ProcessExecTime                               0.0319293
TotalEnvSteps                            531300
policy/Entropy                               -1.4676
policy/KL                                     0.00666486
policy/KLBefore                               0
policy/LossAfter                             -0.0164319
policy/LossBefore                            -7.06774e-10
policy/Perplexity                             0.230478
policy/dLoss                                  0.0164319
---------------------------------------  ----------------
2021-06-04 13:54:49 | [train_policy] epoch #525 | Obtaining samples for iteration 525...
2021-06-04 13:54:50 | [train_policy] epoch #525 | Logging diagnostics...
2021-06-04 13:54:50 | [train_policy] epoch #525 | Optimizing policy...
2021-06-04 13:54:50 | [train_policy] epoch #525 | Computing loss before
2021-06-04 13:54:50 | [train_policy] epoch #525 | Computing KL before
2021-06-04 13:54:50 | [train_policy] epoch #525 | Optimizing
2021-06-04 13:54:50 | [train_policy] epoch #525 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:50 | [train_policy] epoch #525 | computing loss before
2021-06-04 13:54:50 | [train_policy] epoch #525 | computing gradient
2021-06-04 13:54:50 | [train_policy] epoch #525 | gradient computed
2021-06-04 13:54:50 | [train_policy] epoch #525 | computing descent direction
2021-06-04 13:54:50 | [train_policy] epoch #525 | descent direction computed
2021-06-04 13:54:50 | [train_policy] epoch #525 | backtrack iters: 1
2021-06-04 13:54:50 | [train_policy] epoch #525 | optimization finished
2021-06-04 13:54:50 | [train_policy] epoch #525 | Computing KL after
2021-06-04 13:54:50 | [train_policy] epoch #525 | Computing loss after
2021-06-04 13:54:50 | [train_policy] epoch #525 | Fitting baseline...
2021-06-04 13:54:50 | [train_policy] epoch #525 | Saving snapshot...
2021-06-04 13:54:50 | [train_policy] epoch #525 | Saved
2021-06-04 13:54:50 | [train_policy] epoch #525 | Time 422.31 s
2021-06-04 13:54:50 | [train_policy] epoch #525 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287917
Evaluation/AverageDiscountedReturn          -44.2454
Evaluation/AverageReturn                    -44.2454
Evaluation/CompletionRate                     0
Evaluation/Iteration                        525
Evaluation/MaxReturn                        -30.4618
Evaluation/MinReturn                        -83.3309
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.0866
Extras/EpisodeRewardMean                    -44.2213
LinearFeatureBaseline/ExplainedVariance       0.819644
PolicyExecTime                                0.23194
ProcessExecTime                               0.0316052
TotalEnvSteps                            532312
policy/Entropy                               -1.47548
policy/KL                                     0.00647908
policy/KLBefore                               0
policy/LossAfter                             -0.0196906
policy/LossBefore                             1.60202e-08
policy/Perplexity                             0.228668
policy/dLoss                                  0.0196906
---------------------------------------  ----------------
2021-06-04 13:54:50 | [train_policy] epoch #526 | Obtaining samples for iteration 526...
2021-06-04 13:54:51 | [train_policy] epoch #526 | Logging diagnostics...
2021-06-04 13:54:51 | [train_policy] epoch #526 | Optimizing policy...
2021-06-04 13:54:51 | [train_policy] epoch #526 | Computing loss before
2021-06-04 13:54:51 | [train_policy] epoch #526 | Computing KL before
2021-06-04 13:54:51 | [train_policy] epoch #526 | Optimizing
2021-06-04 13:54:51 | [train_policy] epoch #526 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:51 | [train_policy] epoch #526 | computing loss before
2021-06-04 13:54:51 | [train_policy] epoch #526 | computing gradient
2021-06-04 13:54:51 | [train_policy] epoch #526 | gradient computed
2021-06-04 13:54:51 | [train_policy] epoch #526 | computing descent direction
2021-06-04 13:54:51 | [train_policy] epoch #526 | descent direction computed
2021-06-04 13:54:51 | [train_policy] epoch #526 | backtrack iters: 1
2021-06-04 13:54:51 | [train_policy] epoch #526 | optimization finished
2021-06-04 13:54:51 | [train_policy] epoch #526 | Computing KL after
2021-06-04 13:54:51 | [train_policy] epoch #526 | Computing loss after
2021-06-04 13:54:51 | [train_policy] epoch #526 | Fitting baseline...
2021-06-04 13:54:51 | [train_policy] epoch #526 | Saving snapshot...
2021-06-04 13:54:51 | [train_policy] epoch #526 | Saved
2021-06-04 13:54:51 | [train_policy] epoch #526 | Time 423.10 s
2021-06-04 13:54:51 | [train_policy] epoch #526 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285069
Evaluation/AverageDiscountedReturn          -42.3204
Evaluation/AverageReturn                    -42.3204
Evaluation/CompletionRate                     0
Evaluation/Iteration                        526
Evaluation/MaxReturn                        -30.5081
Evaluation/MinReturn                        -64.0932
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.2149
Extras/EpisodeRewardMean                    -41.9369
LinearFeatureBaseline/ExplainedVariance       0.905994
PolicyExecTime                                0.205617
ProcessExecTime                               0.0312083
TotalEnvSteps                            533324
policy/Entropy                               -1.49723
policy/KL                                     0.00649809
policy/KLBefore                               0
policy/LossAfter                             -0.0176956
policy/LossBefore                             7.65672e-09
policy/Perplexity                             0.22375
policy/dLoss                                  0.0176957
---------------------------------------  ----------------
2021-06-04 13:54:51 | [train_policy] epoch #527 | Obtaining samples for iteration 527...
2021-06-04 13:54:52 | [train_policy] epoch #527 | Logging diagnostics...
2021-06-04 13:54:52 | [train_policy] epoch #527 | Optimizing policy...
2021-06-04 13:54:52 | [train_policy] epoch #527 | Computing loss before
2021-06-04 13:54:52 | [train_policy] epoch #527 | Computing KL before
2021-06-04 13:54:52 | [train_policy] epoch #527 | Optimizing
2021-06-04 13:54:52 | [train_policy] epoch #527 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:52 | [train_policy] epoch #527 | computing loss before
2021-06-04 13:54:52 | [train_policy] epoch #527 | computing gradient
2021-06-04 13:54:52 | [train_policy] epoch #527 | gradient computed
2021-06-04 13:54:52 | [train_policy] epoch #527 | computing descent direction
2021-06-04 13:54:52 | [train_policy] epoch #527 | descent direction computed
2021-06-04 13:54:52 | [train_policy] epoch #527 | backtrack iters: 0
2021-06-04 13:54:52 | [train_policy] epoch #527 | optimization finished
2021-06-04 13:54:52 | [train_policy] epoch #527 | Computing KL after
2021-06-04 13:54:52 | [train_policy] epoch #527 | Computing loss after
2021-06-04 13:54:52 | [train_policy] epoch #527 | Fitting baseline...
2021-06-04 13:54:52 | [train_policy] epoch #527 | Saving snapshot...
2021-06-04 13:54:52 | [train_policy] epoch #527 | Saved
2021-06-04 13:54:52 | [train_policy] epoch #527 | Time 423.90 s
2021-06-04 13:54:52 | [train_policy] epoch #527 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284192
Evaluation/AverageDiscountedReturn          -41.9294
Evaluation/AverageReturn                    -41.9294
Evaluation/CompletionRate                     0
Evaluation/Iteration                        527
Evaluation/MaxReturn                        -29.3749
Evaluation/MinReturn                        -64.0818
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.71572
Extras/EpisodeRewardMean                    -41.8091
LinearFeatureBaseline/ExplainedVariance       0.909329
PolicyExecTime                                0.232325
ProcessExecTime                               0.0310826
TotalEnvSteps                            534336
policy/Entropy                               -1.44955
policy/KL                                     0.00971087
policy/KLBefore                               0
policy/LossAfter                             -0.0173529
policy/LossBefore                             1.69626e-08
policy/Perplexity                             0.234675
policy/dLoss                                  0.0173529
---------------------------------------  ----------------
2021-06-04 13:54:52 | [train_policy] epoch #528 | Obtaining samples for iteration 528...
2021-06-04 13:54:52 | [train_policy] epoch #528 | Logging diagnostics...
2021-06-04 13:54:52 | [train_policy] epoch #528 | Optimizing policy...
2021-06-04 13:54:52 | [train_policy] epoch #528 | Computing loss before
2021-06-04 13:54:52 | [train_policy] epoch #528 | Computing KL before
2021-06-04 13:54:52 | [train_policy] epoch #528 | Optimizing
2021-06-04 13:54:52 | [train_policy] epoch #528 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:52 | [train_policy] epoch #528 | computing loss before
2021-06-04 13:54:52 | [train_policy] epoch #528 | computing gradient
2021-06-04 13:54:52 | [train_policy] epoch #528 | gradient computed
2021-06-04 13:54:52 | [train_policy] epoch #528 | computing descent direction
2021-06-04 13:54:52 | [train_policy] epoch #528 | descent direction computed
2021-06-04 13:54:52 | [train_policy] epoch #528 | backtrack iters: 0
2021-06-04 13:54:52 | [train_policy] epoch #528 | optimization finished
2021-06-04 13:54:52 | [train_policy] epoch #528 | Computing KL after
2021-06-04 13:54:52 | [train_policy] epoch #528 | Computing loss after
2021-06-04 13:54:52 | [train_policy] epoch #528 | Fitting baseline...
2021-06-04 13:54:52 | [train_policy] epoch #528 | Saving snapshot...
2021-06-04 13:54:52 | [train_policy] epoch #528 | Saved
2021-06-04 13:54:52 | [train_policy] epoch #528 | Time 424.69 s
2021-06-04 13:54:52 | [train_policy] epoch #528 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28545
Evaluation/AverageDiscountedReturn          -42.4518
Evaluation/AverageReturn                    -42.4518
Evaluation/CompletionRate                     0
Evaluation/Iteration                        528
Evaluation/MaxReturn                        -30.6414
Evaluation/MinReturn                        -63.083
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.37084
Extras/EpisodeRewardMean                    -42.3479
LinearFeatureBaseline/ExplainedVariance       0.903957
PolicyExecTime                                0.224515
ProcessExecTime                               0.0313139
TotalEnvSteps                            535348
policy/Entropy                               -1.4035
policy/KL                                     0.00957751
policy/KLBefore                               0
policy/LossAfter                             -0.0162115
policy/LossBefore                             7.18554e-09
policy/Perplexity                             0.245735
policy/dLoss                                  0.0162115
---------------------------------------  ----------------
2021-06-04 13:54:52 | [train_policy] epoch #529 | Obtaining samples for iteration 529...
2021-06-04 13:54:53 | [train_policy] epoch #529 | Logging diagnostics...
2021-06-04 13:54:53 | [train_policy] epoch #529 | Optimizing policy...
2021-06-04 13:54:53 | [train_policy] epoch #529 | Computing loss before
2021-06-04 13:54:53 | [train_policy] epoch #529 | Computing KL before
2021-06-04 13:54:53 | [train_policy] epoch #529 | Optimizing
2021-06-04 13:54:53 | [train_policy] epoch #529 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:53 | [train_policy] epoch #529 | computing loss before
2021-06-04 13:54:53 | [train_policy] epoch #529 | computing gradient
2021-06-04 13:54:53 | [train_policy] epoch #529 | gradient computed
2021-06-04 13:54:53 | [train_policy] epoch #529 | computing descent direction
2021-06-04 13:54:53 | [train_policy] epoch #529 | descent direction computed
2021-06-04 13:54:53 | [train_policy] epoch #529 | backtrack iters: 1
2021-06-04 13:54:53 | [train_policy] epoch #529 | optimization finished
2021-06-04 13:54:53 | [train_policy] epoch #529 | Computing KL after
2021-06-04 13:54:53 | [train_policy] epoch #529 | Computing loss after
2021-06-04 13:54:53 | [train_policy] epoch #529 | Fitting baseline...
2021-06-04 13:54:53 | [train_policy] epoch #529 | Saving snapshot...
2021-06-04 13:54:53 | [train_policy] epoch #529 | Saved
2021-06-04 13:54:53 | [train_policy] epoch #529 | Time 425.49 s
2021-06-04 13:54:53 | [train_policy] epoch #529 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284734
Evaluation/AverageDiscountedReturn          -42.7742
Evaluation/AverageReturn                    -42.7742
Evaluation/CompletionRate                     0
Evaluation/Iteration                        529
Evaluation/MaxReturn                        -28.3926
Evaluation/MinReturn                        -86.5847
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.99597
Extras/EpisodeRewardMean                    -42.7613
LinearFeatureBaseline/ExplainedVariance       0.862134
PolicyExecTime                                0.227672
ProcessExecTime                               0.0312188
TotalEnvSteps                            536360
policy/Entropy                               -1.41927
policy/KL                                     0.00641645
policy/KLBefore                               0
policy/LossAfter                             -0.0218469
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.24189
policy/dLoss                                  0.0218469
---------------------------------------  ----------------
2021-06-04 13:54:53 | [train_policy] epoch #530 | Obtaining samples for iteration 530...
2021-06-04 13:54:54 | [train_policy] epoch #530 | Logging diagnostics...
2021-06-04 13:54:54 | [train_policy] epoch #530 | Optimizing policy...
2021-06-04 13:54:54 | [train_policy] epoch #530 | Computing loss before
2021-06-04 13:54:54 | [train_policy] epoch #530 | Computing KL before
2021-06-04 13:54:54 | [train_policy] epoch #530 | Optimizing
2021-06-04 13:54:54 | [train_policy] epoch #530 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:54 | [train_policy] epoch #530 | computing loss before
2021-06-04 13:54:54 | [train_policy] epoch #530 | computing gradient
2021-06-04 13:54:54 | [train_policy] epoch #530 | gradient computed
2021-06-04 13:54:54 | [train_policy] epoch #530 | computing descent direction
2021-06-04 13:54:54 | [train_policy] epoch #530 | descent direction computed
2021-06-04 13:54:54 | [train_policy] epoch #530 | backtrack iters: 0
2021-06-04 13:54:54 | [train_policy] epoch #530 | optimization finished
2021-06-04 13:54:54 | [train_policy] epoch #530 | Computing KL after
2021-06-04 13:54:54 | [train_policy] epoch #530 | Computing loss after
2021-06-04 13:54:54 | [train_policy] epoch #530 | Fitting baseline...
2021-06-04 13:54:54 | [train_policy] epoch #530 | Saving snapshot...
2021-06-04 13:54:54 | [train_policy] epoch #530 | Saved
2021-06-04 13:54:54 | [train_policy] epoch #530 | Time 426.30 s
2021-06-04 13:54:54 | [train_policy] epoch #530 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286565
Evaluation/AverageDiscountedReturn          -43.2988
Evaluation/AverageReturn                    -43.2988
Evaluation/CompletionRate                     0
Evaluation/Iteration                        530
Evaluation/MaxReturn                        -28.604
Evaluation/MinReturn                        -83.9149
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.48721
Extras/EpisodeRewardMean                    -43.5584
LinearFeatureBaseline/ExplainedVariance       0.863528
PolicyExecTime                                0.228119
ProcessExecTime                               0.0313509
TotalEnvSteps                            537372
policy/Entropy                               -1.40025
policy/KL                                     0.0099262
policy/KLBefore                               0
policy/LossAfter                             -0.0291487
policy/LossBefore                            -1.35465e-09
policy/Perplexity                             0.246534
policy/dLoss                                  0.0291487
---------------------------------------  ----------------
2021-06-04 13:54:54 | [train_policy] epoch #531 | Obtaining samples for iteration 531...
2021-06-04 13:54:55 | [train_policy] epoch #531 | Logging diagnostics...
2021-06-04 13:54:55 | [train_policy] epoch #531 | Optimizing policy...
2021-06-04 13:54:55 | [train_policy] epoch #531 | Computing loss before
2021-06-04 13:54:55 | [train_policy] epoch #531 | Computing KL before
2021-06-04 13:54:55 | [train_policy] epoch #531 | Optimizing
2021-06-04 13:54:55 | [train_policy] epoch #531 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:55 | [train_policy] epoch #531 | computing loss before
2021-06-04 13:54:55 | [train_policy] epoch #531 | computing gradient
2021-06-04 13:54:55 | [train_policy] epoch #531 | gradient computed
2021-06-04 13:54:55 | [train_policy] epoch #531 | computing descent direction
2021-06-04 13:54:55 | [train_policy] epoch #531 | descent direction computed
2021-06-04 13:54:55 | [train_policy] epoch #531 | backtrack iters: 1
2021-06-04 13:54:55 | [train_policy] epoch #531 | optimization finished
2021-06-04 13:54:55 | [train_policy] epoch #531 | Computing KL after
2021-06-04 13:54:55 | [train_policy] epoch #531 | Computing loss after
2021-06-04 13:54:55 | [train_policy] epoch #531 | Fitting baseline...
2021-06-04 13:54:55 | [train_policy] epoch #531 | Saving snapshot...
2021-06-04 13:54:55 | [train_policy] epoch #531 | Saved
2021-06-04 13:54:55 | [train_policy] epoch #531 | Time 427.08 s
2021-06-04 13:54:55 | [train_policy] epoch #531 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284778
Evaluation/AverageDiscountedReturn          -42.7216
Evaluation/AverageReturn                    -42.7216
Evaluation/CompletionRate                     0
Evaluation/Iteration                        531
Evaluation/MaxReturn                        -30.181
Evaluation/MinReturn                        -84.2767
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.4566
Extras/EpisodeRewardMean                    -42.916
LinearFeatureBaseline/ExplainedVariance       0.865552
PolicyExecTime                                0.212849
ProcessExecTime                               0.0312052
TotalEnvSteps                            538384
policy/Entropy                               -1.43672
policy/KL                                     0.00660428
policy/KLBefore                               0
policy/LossAfter                             -0.015602
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.237705
policy/dLoss                                  0.015602
---------------------------------------  ----------------
2021-06-04 13:54:55 | [train_policy] epoch #532 | Obtaining samples for iteration 532...
2021-06-04 13:54:56 | [train_policy] epoch #532 | Logging diagnostics...
2021-06-04 13:54:56 | [train_policy] epoch #532 | Optimizing policy...
2021-06-04 13:54:56 | [train_policy] epoch #532 | Computing loss before
2021-06-04 13:54:56 | [train_policy] epoch #532 | Computing KL before
2021-06-04 13:54:56 | [train_policy] epoch #532 | Optimizing
2021-06-04 13:54:56 | [train_policy] epoch #532 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:56 | [train_policy] epoch #532 | computing loss before
2021-06-04 13:54:56 | [train_policy] epoch #532 | computing gradient
2021-06-04 13:54:56 | [train_policy] epoch #532 | gradient computed
2021-06-04 13:54:56 | [train_policy] epoch #532 | computing descent direction
2021-06-04 13:54:56 | [train_policy] epoch #532 | descent direction computed
2021-06-04 13:54:56 | [train_policy] epoch #532 | backtrack iters: 0
2021-06-04 13:54:56 | [train_policy] epoch #532 | optimization finished
2021-06-04 13:54:56 | [train_policy] epoch #532 | Computing KL after
2021-06-04 13:54:56 | [train_policy] epoch #532 | Computing loss after
2021-06-04 13:54:56 | [train_policy] epoch #532 | Fitting baseline...
2021-06-04 13:54:56 | [train_policy] epoch #532 | Saving snapshot...
2021-06-04 13:54:56 | [train_policy] epoch #532 | Saved
2021-06-04 13:54:56 | [train_policy] epoch #532 | Time 427.87 s
2021-06-04 13:54:56 | [train_policy] epoch #532 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285571
Evaluation/AverageDiscountedReturn          -43.2694
Evaluation/AverageReturn                    -43.2694
Evaluation/CompletionRate                     0
Evaluation/Iteration                        532
Evaluation/MaxReturn                        -29.5098
Evaluation/MinReturn                        -88.7204
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.80946
Extras/EpisodeRewardMean                    -43.4067
LinearFeatureBaseline/ExplainedVariance       0.853494
PolicyExecTime                                0.223213
ProcessExecTime                               0.0313125
TotalEnvSteps                            539396
policy/Entropy                               -1.43519
policy/KL                                     0.0098829
policy/KLBefore                               0
policy/LossAfter                             -0.0189938
policy/LossBefore                            -7.77452e-09
policy/Perplexity                             0.23807
policy/dLoss                                  0.0189938
---------------------------------------  ----------------
2021-06-04 13:54:56 | [train_policy] epoch #533 | Obtaining samples for iteration 533...
2021-06-04 13:54:56 | [train_policy] epoch #533 | Logging diagnostics...
2021-06-04 13:54:56 | [train_policy] epoch #533 | Optimizing policy...
2021-06-04 13:54:56 | [train_policy] epoch #533 | Computing loss before
2021-06-04 13:54:56 | [train_policy] epoch #533 | Computing KL before
2021-06-04 13:54:56 | [train_policy] epoch #533 | Optimizing
2021-06-04 13:54:56 | [train_policy] epoch #533 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:56 | [train_policy] epoch #533 | computing loss before
2021-06-04 13:54:56 | [train_policy] epoch #533 | computing gradient
2021-06-04 13:54:56 | [train_policy] epoch #533 | gradient computed
2021-06-04 13:54:56 | [train_policy] epoch #533 | computing descent direction
2021-06-04 13:54:56 | [train_policy] epoch #533 | descent direction computed
2021-06-04 13:54:56 | [train_policy] epoch #533 | backtrack iters: 0
2021-06-04 13:54:56 | [train_policy] epoch #533 | optimization finished
2021-06-04 13:54:56 | [train_policy] epoch #533 | Computing KL after
2021-06-04 13:54:56 | [train_policy] epoch #533 | Computing loss after
2021-06-04 13:54:56 | [train_policy] epoch #533 | Fitting baseline...
2021-06-04 13:54:56 | [train_policy] epoch #533 | Saving snapshot...
2021-06-04 13:54:56 | [train_policy] epoch #533 | Saved
2021-06-04 13:54:56 | [train_policy] epoch #533 | Time 428.66 s
2021-06-04 13:54:56 | [train_policy] epoch #533 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284689
Evaluation/AverageDiscountedReturn          -42.372
Evaluation/AverageReturn                    -42.372
Evaluation/CompletionRate                     0
Evaluation/Iteration                        533
Evaluation/MaxReturn                        -28.1054
Evaluation/MinReturn                        -64.143
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.02892
Extras/EpisodeRewardMean                    -42.9879
LinearFeatureBaseline/ExplainedVariance       0.902991
PolicyExecTime                                0.226799
ProcessExecTime                               0.0311587
TotalEnvSteps                            540408
policy/Entropy                               -1.42169
policy/KL                                     0.00966271
policy/KLBefore                               0
policy/LossAfter                             -0.0139321
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.241307
policy/dLoss                                  0.0139321
---------------------------------------  ----------------
2021-06-04 13:54:56 | [train_policy] epoch #534 | Obtaining samples for iteration 534...
2021-06-04 13:54:57 | [train_policy] epoch #534 | Logging diagnostics...
2021-06-04 13:54:57 | [train_policy] epoch #534 | Optimizing policy...
2021-06-04 13:54:57 | [train_policy] epoch #534 | Computing loss before
2021-06-04 13:54:57 | [train_policy] epoch #534 | Computing KL before
2021-06-04 13:54:57 | [train_policy] epoch #534 | Optimizing
2021-06-04 13:54:57 | [train_policy] epoch #534 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:57 | [train_policy] epoch #534 | computing loss before
2021-06-04 13:54:57 | [train_policy] epoch #534 | computing gradient
2021-06-04 13:54:57 | [train_policy] epoch #534 | gradient computed
2021-06-04 13:54:57 | [train_policy] epoch #534 | computing descent direction
2021-06-04 13:54:57 | [train_policy] epoch #534 | descent direction computed
2021-06-04 13:54:57 | [train_policy] epoch #534 | backtrack iters: 1
2021-06-04 13:54:57 | [train_policy] epoch #534 | optimization finished
2021-06-04 13:54:57 | [train_policy] epoch #534 | Computing KL after
2021-06-04 13:54:57 | [train_policy] epoch #534 | Computing loss after
2021-06-04 13:54:57 | [train_policy] epoch #534 | Fitting baseline...
2021-06-04 13:54:57 | [train_policy] epoch #534 | Saving snapshot...
2021-06-04 13:54:57 | [train_policy] epoch #534 | Saved
2021-06-04 13:54:57 | [train_policy] epoch #534 | Time 429.48 s
2021-06-04 13:54:57 | [train_policy] epoch #534 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.285922
Evaluation/AverageDiscountedReturn          -41.0359
Evaluation/AverageReturn                    -41.0359
Evaluation/CompletionRate                     0
Evaluation/Iteration                        534
Evaluation/MaxReturn                        -27.5213
Evaluation/MinReturn                        -98.2762
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.09111
Extras/EpisodeRewardMean                    -41.3301
LinearFeatureBaseline/ExplainedVariance       0.835292
PolicyExecTime                                0.234299
ProcessExecTime                               0.0311675
TotalEnvSteps                            541420
policy/Entropy                               -1.42644
policy/KL                                     0.00642899
policy/KLBefore                               0
policy/LossAfter                             -0.0185228
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.240163
policy/dLoss                                  0.0185228
---------------------------------------  ----------------
2021-06-04 13:54:57 | [train_policy] epoch #535 | Obtaining samples for iteration 535...
2021-06-04 13:54:58 | [train_policy] epoch #535 | Logging diagnostics...
2021-06-04 13:54:58 | [train_policy] epoch #535 | Optimizing policy...
2021-06-04 13:54:58 | [train_policy] epoch #535 | Computing loss before
2021-06-04 13:54:58 | [train_policy] epoch #535 | Computing KL before
2021-06-04 13:54:58 | [train_policy] epoch #535 | Optimizing
2021-06-04 13:54:58 | [train_policy] epoch #535 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:58 | [train_policy] epoch #535 | computing loss before
2021-06-04 13:54:58 | [train_policy] epoch #535 | computing gradient
2021-06-04 13:54:58 | [train_policy] epoch #535 | gradient computed
2021-06-04 13:54:58 | [train_policy] epoch #535 | computing descent direction
2021-06-04 13:54:58 | [train_policy] epoch #535 | descent direction computed
2021-06-04 13:54:58 | [train_policy] epoch #535 | backtrack iters: 1
2021-06-04 13:54:58 | [train_policy] epoch #535 | optimization finished
2021-06-04 13:54:58 | [train_policy] epoch #535 | Computing KL after
2021-06-04 13:54:58 | [train_policy] epoch #535 | Computing loss after
2021-06-04 13:54:58 | [train_policy] epoch #535 | Fitting baseline...
2021-06-04 13:54:58 | [train_policy] epoch #535 | Saving snapshot...
2021-06-04 13:54:58 | [train_policy] epoch #535 | Saved
2021-06-04 13:54:58 | [train_policy] epoch #535 | Time 430.29 s
2021-06-04 13:54:58 | [train_policy] epoch #535 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287346
Evaluation/AverageDiscountedReturn          -42.4318
Evaluation/AverageReturn                    -42.4318
Evaluation/CompletionRate                     0
Evaluation/Iteration                        535
Evaluation/MaxReturn                        -28.1902
Evaluation/MinReturn                        -64.2031
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.18885
Extras/EpisodeRewardMean                    -42.2378
LinearFeatureBaseline/ExplainedVariance       0.854955
PolicyExecTime                                0.227891
ProcessExecTime                               0.0315051
TotalEnvSteps                            542432
policy/Entropy                               -1.43132
policy/KL                                     0.00652198
policy/KLBefore                               0
policy/LossAfter                             -0.0181594
policy/LossBefore                            -1.53134e-08
policy/Perplexity                             0.238994
policy/dLoss                                  0.0181594
---------------------------------------  ----------------
2021-06-04 13:54:58 | [train_policy] epoch #536 | Obtaining samples for iteration 536...
2021-06-04 13:54:59 | [train_policy] epoch #536 | Logging diagnostics...
2021-06-04 13:54:59 | [train_policy] epoch #536 | Optimizing policy...
2021-06-04 13:54:59 | [train_policy] epoch #536 | Computing loss before
2021-06-04 13:54:59 | [train_policy] epoch #536 | Computing KL before
2021-06-04 13:54:59 | [train_policy] epoch #536 | Optimizing
2021-06-04 13:54:59 | [train_policy] epoch #536 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:54:59 | [train_policy] epoch #536 | computing loss before
2021-06-04 13:54:59 | [train_policy] epoch #536 | computing gradient
2021-06-04 13:54:59 | [train_policy] epoch #536 | gradient computed
2021-06-04 13:54:59 | [train_policy] epoch #536 | computing descent direction
2021-06-04 13:54:59 | [train_policy] epoch #536 | descent direction computed
2021-06-04 13:54:59 | [train_policy] epoch #536 | backtrack iters: 0
2021-06-04 13:54:59 | [train_policy] epoch #536 | optimization finished
2021-06-04 13:54:59 | [train_policy] epoch #536 | Computing KL after
2021-06-04 13:54:59 | [train_policy] epoch #536 | Computing loss after
2021-06-04 13:54:59 | [train_policy] epoch #536 | Fitting baseline...
2021-06-04 13:54:59 | [train_policy] epoch #536 | Saving snapshot...
2021-06-04 13:54:59 | [train_policy] epoch #536 | Saved
2021-06-04 13:54:59 | [train_policy] epoch #536 | Time 431.08 s
2021-06-04 13:54:59 | [train_policy] epoch #536 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.285959
Evaluation/AverageDiscountedReturn          -43.3354
Evaluation/AverageReturn                    -43.3354
Evaluation/CompletionRate                     0
Evaluation/Iteration                        536
Evaluation/MaxReturn                        -30.0156
Evaluation/MinReturn                        -64.1998
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.99118
Extras/EpisodeRewardMean                    -43.4663
LinearFeatureBaseline/ExplainedVariance       0.914987
PolicyExecTime                                0.225665
ProcessExecTime                               0.0313742
TotalEnvSteps                            543444
policy/Entropy                               -1.42558
policy/KL                                     0.0093647
policy/KLBefore                               0
policy/LossAfter                             -0.0170526
policy/LossBefore                            -2.7093e-09
policy/Perplexity                             0.24037
policy/dLoss                                  0.0170526
---------------------------------------  ---------------
2021-06-04 13:54:59 | [train_policy] epoch #537 | Obtaining samples for iteration 537...
2021-06-04 13:55:00 | [train_policy] epoch #537 | Logging diagnostics...
2021-06-04 13:55:00 | [train_policy] epoch #537 | Optimizing policy...
2021-06-04 13:55:00 | [train_policy] epoch #537 | Computing loss before
2021-06-04 13:55:00 | [train_policy] epoch #537 | Computing KL before
2021-06-04 13:55:00 | [train_policy] epoch #537 | Optimizing
2021-06-04 13:55:00 | [train_policy] epoch #537 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:00 | [train_policy] epoch #537 | computing loss before
2021-06-04 13:55:00 | [train_policy] epoch #537 | computing gradient
2021-06-04 13:55:00 | [train_policy] epoch #537 | gradient computed
2021-06-04 13:55:00 | [train_policy] epoch #537 | computing descent direction
2021-06-04 13:55:00 | [train_policy] epoch #537 | descent direction computed
2021-06-04 13:55:00 | [train_policy] epoch #537 | backtrack iters: 1
2021-06-04 13:55:00 | [train_policy] epoch #537 | optimization finished
2021-06-04 13:55:00 | [train_policy] epoch #537 | Computing KL after
2021-06-04 13:55:00 | [train_policy] epoch #537 | Computing loss after
2021-06-04 13:55:00 | [train_policy] epoch #537 | Fitting baseline...
2021-06-04 13:55:00 | [train_policy] epoch #537 | Saving snapshot...
2021-06-04 13:55:00 | [train_policy] epoch #537 | Saved
2021-06-04 13:55:00 | [train_policy] epoch #537 | Time 431.88 s
2021-06-04 13:55:00 | [train_policy] epoch #537 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284366
Evaluation/AverageDiscountedReturn          -42.3457
Evaluation/AverageReturn                    -42.3457
Evaluation/CompletionRate                     0
Evaluation/Iteration                        537
Evaluation/MaxReturn                        -28.1203
Evaluation/MinReturn                        -82.7657
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77295
Extras/EpisodeRewardMean                    -42.2516
LinearFeatureBaseline/ExplainedVariance       0.882245
PolicyExecTime                                0.237561
ProcessExecTime                               0.0310714
TotalEnvSteps                            544456
policy/Entropy                               -1.42631
policy/KL                                     0.0065563
policy/KLBefore                               0
policy/LossAfter                             -0.0137633
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.240193
policy/dLoss                                  0.0137633
---------------------------------------  ----------------
2021-06-04 13:55:00 | [train_policy] epoch #538 | Obtaining samples for iteration 538...
2021-06-04 13:55:00 | [train_policy] epoch #538 | Logging diagnostics...
2021-06-04 13:55:00 | [train_policy] epoch #538 | Optimizing policy...
2021-06-04 13:55:00 | [train_policy] epoch #538 | Computing loss before
2021-06-04 13:55:00 | [train_policy] epoch #538 | Computing KL before
2021-06-04 13:55:00 | [train_policy] epoch #538 | Optimizing
2021-06-04 13:55:00 | [train_policy] epoch #538 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:00 | [train_policy] epoch #538 | computing loss before
2021-06-04 13:55:00 | [train_policy] epoch #538 | computing gradient
2021-06-04 13:55:00 | [train_policy] epoch #538 | gradient computed
2021-06-04 13:55:00 | [train_policy] epoch #538 | computing descent direction
2021-06-04 13:55:00 | [train_policy] epoch #538 | descent direction computed
2021-06-04 13:55:00 | [train_policy] epoch #538 | backtrack iters: 1
2021-06-04 13:55:00 | [train_policy] epoch #538 | optimization finished
2021-06-04 13:55:00 | [train_policy] epoch #538 | Computing KL after
2021-06-04 13:55:00 | [train_policy] epoch #538 | Computing loss after
2021-06-04 13:55:00 | [train_policy] epoch #538 | Fitting baseline...
2021-06-04 13:55:00 | [train_policy] epoch #538 | Saving snapshot...
2021-06-04 13:55:00 | [train_policy] epoch #538 | Saved
2021-06-04 13:55:00 | [train_policy] epoch #538 | Time 432.67 s
2021-06-04 13:55:00 | [train_policy] epoch #538 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.296076
Evaluation/AverageDiscountedReturn          -42.7301
Evaluation/AverageReturn                    -42.7301
Evaluation/CompletionRate                     0
Evaluation/Iteration                        538
Evaluation/MaxReturn                        -29.5808
Evaluation/MinReturn                        -85.4597
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.91636
Extras/EpisodeRewardMean                    -42.5229
LinearFeatureBaseline/ExplainedVariance       0.858674
PolicyExecTime                                0.219703
ProcessExecTime                               0.0322747
TotalEnvSteps                            545468
policy/Entropy                               -1.43938
policy/KL                                     0.0065409
policy/KLBefore                               0
policy/LossAfter                             -0.0136872
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.237074
policy/dLoss                                  0.0136872
---------------------------------------  ----------------
2021-06-04 13:55:00 | [train_policy] epoch #539 | Obtaining samples for iteration 539...
2021-06-04 13:55:01 | [train_policy] epoch #539 | Logging diagnostics...
2021-06-04 13:55:01 | [train_policy] epoch #539 | Optimizing policy...
2021-06-04 13:55:01 | [train_policy] epoch #539 | Computing loss before
2021-06-04 13:55:01 | [train_policy] epoch #539 | Computing KL before
2021-06-04 13:55:01 | [train_policy] epoch #539 | Optimizing
2021-06-04 13:55:01 | [train_policy] epoch #539 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:01 | [train_policy] epoch #539 | computing loss before
2021-06-04 13:55:01 | [train_policy] epoch #539 | computing gradient
2021-06-04 13:55:01 | [train_policy] epoch #539 | gradient computed
2021-06-04 13:55:01 | [train_policy] epoch #539 | computing descent direction
2021-06-04 13:55:01 | [train_policy] epoch #539 | descent direction computed
2021-06-04 13:55:01 | [train_policy] epoch #539 | backtrack iters: 1
2021-06-04 13:55:01 | [train_policy] epoch #539 | optimization finished
2021-06-04 13:55:01 | [train_policy] epoch #539 | Computing KL after
2021-06-04 13:55:01 | [train_policy] epoch #539 | Computing loss after
2021-06-04 13:55:01 | [train_policy] epoch #539 | Fitting baseline...
2021-06-04 13:55:01 | [train_policy] epoch #539 | Saving snapshot...
2021-06-04 13:55:01 | [train_policy] epoch #539 | Saved
2021-06-04 13:55:01 | [train_policy] epoch #539 | Time 433.46 s
2021-06-04 13:55:01 | [train_policy] epoch #539 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285442
Evaluation/AverageDiscountedReturn          -42.9172
Evaluation/AverageReturn                    -42.9172
Evaluation/CompletionRate                     0
Evaluation/Iteration                        539
Evaluation/MaxReturn                        -29.6677
Evaluation/MinReturn                        -83.9331
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.8471
Extras/EpisodeRewardMean                    -43.2764
LinearFeatureBaseline/ExplainedVariance       0.887201
PolicyExecTime                                0.213463
ProcessExecTime                               0.0311635
TotalEnvSteps                            546480
policy/Entropy                               -1.42501
policy/KL                                     0.00651322
policy/KLBefore                               0
policy/LossAfter                             -0.0171536
policy/LossBefore                             1.22508e-08
policy/Perplexity                             0.240506
policy/dLoss                                  0.0171536
---------------------------------------  ----------------
2021-06-04 13:55:01 | [train_policy] epoch #540 | Obtaining samples for iteration 540...
2021-06-04 13:55:02 | [train_policy] epoch #540 | Logging diagnostics...
2021-06-04 13:55:02 | [train_policy] epoch #540 | Optimizing policy...
2021-06-04 13:55:02 | [train_policy] epoch #540 | Computing loss before
2021-06-04 13:55:02 | [train_policy] epoch #540 | Computing KL before
2021-06-04 13:55:02 | [train_policy] epoch #540 | Optimizing
2021-06-04 13:55:02 | [train_policy] epoch #540 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:02 | [train_policy] epoch #540 | computing loss before
2021-06-04 13:55:02 | [train_policy] epoch #540 | computing gradient
2021-06-04 13:55:02 | [train_policy] epoch #540 | gradient computed
2021-06-04 13:55:02 | [train_policy] epoch #540 | computing descent direction
2021-06-04 13:55:02 | [train_policy] epoch #540 | descent direction computed
2021-06-04 13:55:02 | [train_policy] epoch #540 | backtrack iters: 1
2021-06-04 13:55:02 | [train_policy] epoch #540 | optimization finished
2021-06-04 13:55:02 | [train_policy] epoch #540 | Computing KL after
2021-06-04 13:55:02 | [train_policy] epoch #540 | Computing loss after
2021-06-04 13:55:02 | [train_policy] epoch #540 | Fitting baseline...
2021-06-04 13:55:02 | [train_policy] epoch #540 | Saving snapshot...
2021-06-04 13:55:02 | [train_policy] epoch #540 | Saved
2021-06-04 13:55:02 | [train_policy] epoch #540 | Time 434.28 s
2021-06-04 13:55:02 | [train_policy] epoch #540 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.291605
Evaluation/AverageDiscountedReturn          -43.6196
Evaluation/AverageReturn                    -43.6196
Evaluation/CompletionRate                     0
Evaluation/Iteration                        540
Evaluation/MaxReturn                        -32.2549
Evaluation/MinReturn                        -82.8164
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.35736
Extras/EpisodeRewardMean                    -43.6998
LinearFeatureBaseline/ExplainedVariance       0.893046
PolicyExecTime                                0.234166
ProcessExecTime                               0.0318277
TotalEnvSteps                            547492
policy/Entropy                               -1.48407
policy/KL                                     0.00667459
policy/KLBefore                               0
policy/LossAfter                             -0.0199156
policy/LossBefore                             7.30334e-09
policy/Perplexity                             0.226714
policy/dLoss                                  0.0199156
---------------------------------------  ----------------
2021-06-04 13:55:02 | [train_policy] epoch #541 | Obtaining samples for iteration 541...
2021-06-04 13:55:03 | [train_policy] epoch #541 | Logging diagnostics...
2021-06-04 13:55:03 | [train_policy] epoch #541 | Optimizing policy...
2021-06-04 13:55:03 | [train_policy] epoch #541 | Computing loss before
2021-06-04 13:55:03 | [train_policy] epoch #541 | Computing KL before
2021-06-04 13:55:03 | [train_policy] epoch #541 | Optimizing
2021-06-04 13:55:03 | [train_policy] epoch #541 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:03 | [train_policy] epoch #541 | computing loss before
2021-06-04 13:55:03 | [train_policy] epoch #541 | computing gradient
2021-06-04 13:55:03 | [train_policy] epoch #541 | gradient computed
2021-06-04 13:55:03 | [train_policy] epoch #541 | computing descent direction
2021-06-04 13:55:03 | [train_policy] epoch #541 | descent direction computed
2021-06-04 13:55:03 | [train_policy] epoch #541 | backtrack iters: 1
2021-06-04 13:55:03 | [train_policy] epoch #541 | optimization finished
2021-06-04 13:55:03 | [train_policy] epoch #541 | Computing KL after
2021-06-04 13:55:03 | [train_policy] epoch #541 | Computing loss after
2021-06-04 13:55:03 | [train_policy] epoch #541 | Fitting baseline...
2021-06-04 13:55:03 | [train_policy] epoch #541 | Saving snapshot...
2021-06-04 13:55:03 | [train_policy] epoch #541 | Saved
2021-06-04 13:55:03 | [train_policy] epoch #541 | Time 435.09 s
2021-06-04 13:55:03 | [train_policy] epoch #541 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.28638
Evaluation/AverageDiscountedReturn          -43.3269
Evaluation/AverageReturn                    -43.3269
Evaluation/CompletionRate                     0
Evaluation/Iteration                        541
Evaluation/MaxReturn                        -29.9134
Evaluation/MinReturn                        -84.0962
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.4529
Extras/EpisodeRewardMean                    -43.2
LinearFeatureBaseline/ExplainedVariance       0.862653
PolicyExecTime                                0.223317
ProcessExecTime                               0.0315289
TotalEnvSteps                            548504
policy/Entropy                               -1.50132
policy/KL                                     0.00640238
policy/KLBefore                               0
policy/LossAfter                             -0.0226666
policy/LossBefore                            -1.13084e-08
policy/Perplexity                             0.222836
policy/dLoss                                  0.0226666
---------------------------------------  ----------------
2021-06-04 13:55:03 | [train_policy] epoch #542 | Obtaining samples for iteration 542...
2021-06-04 13:55:04 | [train_policy] epoch #542 | Logging diagnostics...
2021-06-04 13:55:04 | [train_policy] epoch #542 | Optimizing policy...
2021-06-04 13:55:04 | [train_policy] epoch #542 | Computing loss before
2021-06-04 13:55:04 | [train_policy] epoch #542 | Computing KL before
2021-06-04 13:55:04 | [train_policy] epoch #542 | Optimizing
2021-06-04 13:55:04 | [train_policy] epoch #542 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:04 | [train_policy] epoch #542 | computing loss before
2021-06-04 13:55:04 | [train_policy] epoch #542 | computing gradient
2021-06-04 13:55:04 | [train_policy] epoch #542 | gradient computed
2021-06-04 13:55:04 | [train_policy] epoch #542 | computing descent direction
2021-06-04 13:55:04 | [train_policy] epoch #542 | descent direction computed
2021-06-04 13:55:04 | [train_policy] epoch #542 | backtrack iters: 1
2021-06-04 13:55:04 | [train_policy] epoch #542 | optimization finished
2021-06-04 13:55:04 | [train_policy] epoch #542 | Computing KL after
2021-06-04 13:55:04 | [train_policy] epoch #542 | Computing loss after
2021-06-04 13:55:04 | [train_policy] epoch #542 | Fitting baseline...
2021-06-04 13:55:04 | [train_policy] epoch #542 | Saving snapshot...
2021-06-04 13:55:04 | [train_policy] epoch #542 | Saved
2021-06-04 13:55:04 | [train_policy] epoch #542 | Time 435.88 s
2021-06-04 13:55:04 | [train_policy] epoch #542 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284531
Evaluation/AverageDiscountedReturn          -43.3402
Evaluation/AverageReturn                    -43.3402
Evaluation/CompletionRate                     0
Evaluation/Iteration                        542
Evaluation/MaxReturn                        -30.5524
Evaluation/MinReturn                        -81.0873
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.09526
Extras/EpisodeRewardMean                    -43.7291
LinearFeatureBaseline/ExplainedVariance       0.878905
PolicyExecTime                                0.222652
ProcessExecTime                               0.0311644
TotalEnvSteps                            549516
policy/Entropy                               -1.507
policy/KL                                     0.00644782
policy/KLBefore                               0
policy/LossAfter                             -0.0148159
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.221573
policy/dLoss                                  0.0148159
---------------------------------------  ----------------
2021-06-04 13:55:04 | [train_policy] epoch #543 | Obtaining samples for iteration 543...
2021-06-04 13:55:04 | [train_policy] epoch #543 | Logging diagnostics...
2021-06-04 13:55:04 | [train_policy] epoch #543 | Optimizing policy...
2021-06-04 13:55:04 | [train_policy] epoch #543 | Computing loss before
2021-06-04 13:55:04 | [train_policy] epoch #543 | Computing KL before
2021-06-04 13:55:04 | [train_policy] epoch #543 | Optimizing
2021-06-04 13:55:04 | [train_policy] epoch #543 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:04 | [train_policy] epoch #543 | computing loss before
2021-06-04 13:55:04 | [train_policy] epoch #543 | computing gradient
2021-06-04 13:55:04 | [train_policy] epoch #543 | gradient computed
2021-06-04 13:55:04 | [train_policy] epoch #543 | computing descent direction
2021-06-04 13:55:04 | [train_policy] epoch #543 | descent direction computed
2021-06-04 13:55:04 | [train_policy] epoch #543 | backtrack iters: 0
2021-06-04 13:55:04 | [train_policy] epoch #543 | optimization finished
2021-06-04 13:55:04 | [train_policy] epoch #543 | Computing KL after
2021-06-04 13:55:04 | [train_policy] epoch #543 | Computing loss after
2021-06-04 13:55:04 | [train_policy] epoch #543 | Fitting baseline...
2021-06-04 13:55:04 | [train_policy] epoch #543 | Saving snapshot...
2021-06-04 13:55:04 | [train_policy] epoch #543 | Saved
2021-06-04 13:55:04 | [train_policy] epoch #543 | Time 436.68 s
2021-06-04 13:55:04 | [train_policy] epoch #543 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285568
Evaluation/AverageDiscountedReturn          -41.5078
Evaluation/AverageReturn                    -41.5078
Evaluation/CompletionRate                     0
Evaluation/Iteration                        543
Evaluation/MaxReturn                        -30.6827
Evaluation/MinReturn                        -80.392
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.02301
Extras/EpisodeRewardMean                    -42.1817
LinearFeatureBaseline/ExplainedVariance       0.892479
PolicyExecTime                                0.23991
ProcessExecTime                               0.0311565
TotalEnvSteps                            550528
policy/Entropy                               -1.5162
policy/KL                                     0.00992926
policy/KLBefore                               0
policy/LossAfter                             -0.0188659
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.219545
policy/dLoss                                  0.0188659
---------------------------------------  ----------------
2021-06-04 13:55:04 | [train_policy] epoch #544 | Obtaining samples for iteration 544...
2021-06-04 13:55:05 | [train_policy] epoch #544 | Logging diagnostics...
2021-06-04 13:55:05 | [train_policy] epoch #544 | Optimizing policy...
2021-06-04 13:55:05 | [train_policy] epoch #544 | Computing loss before
2021-06-04 13:55:05 | [train_policy] epoch #544 | Computing KL before
2021-06-04 13:55:05 | [train_policy] epoch #544 | Optimizing
2021-06-04 13:55:05 | [train_policy] epoch #544 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:05 | [train_policy] epoch #544 | computing loss before
2021-06-04 13:55:05 | [train_policy] epoch #544 | computing gradient
2021-06-04 13:55:05 | [train_policy] epoch #544 | gradient computed
2021-06-04 13:55:05 | [train_policy] epoch #544 | computing descent direction
2021-06-04 13:55:05 | [train_policy] epoch #544 | descent direction computed
2021-06-04 13:55:05 | [train_policy] epoch #544 | backtrack iters: 1
2021-06-04 13:55:05 | [train_policy] epoch #544 | optimization finished
2021-06-04 13:55:05 | [train_policy] epoch #544 | Computing KL after
2021-06-04 13:55:05 | [train_policy] epoch #544 | Computing loss after
2021-06-04 13:55:05 | [train_policy] epoch #544 | Fitting baseline...
2021-06-04 13:55:05 | [train_policy] epoch #544 | Saving snapshot...
2021-06-04 13:55:05 | [train_policy] epoch #544 | Saved
2021-06-04 13:55:05 | [train_policy] epoch #544 | Time 437.47 s
2021-06-04 13:55:05 | [train_policy] epoch #544 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.285157
Evaluation/AverageDiscountedReturn          -41.6565
Evaluation/AverageReturn                    -41.6565
Evaluation/CompletionRate                     0
Evaluation/Iteration                        544
Evaluation/MaxReturn                        -27.9279
Evaluation/MinReturn                        -81.8978
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.6114
Extras/EpisodeRewardMean                    -41.797
LinearFeatureBaseline/ExplainedVariance       0.882876
PolicyExecTime                                0.214914
ProcessExecTime                               0.0311737
TotalEnvSteps                            551540
policy/Entropy                               -1.50689
policy/KL                                     0.00645432
policy/KLBefore                               0
policy/LossAfter                             -0.00833169
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.221599
policy/dLoss                                  0.00833169
---------------------------------------  ---------------
2021-06-04 13:55:05 | [train_policy] epoch #545 | Obtaining samples for iteration 545...
2021-06-04 13:55:06 | [train_policy] epoch #545 | Logging diagnostics...
2021-06-04 13:55:06 | [train_policy] epoch #545 | Optimizing policy...
2021-06-04 13:55:06 | [train_policy] epoch #545 | Computing loss before
2021-06-04 13:55:06 | [train_policy] epoch #545 | Computing KL before
2021-06-04 13:55:06 | [train_policy] epoch #545 | Optimizing
2021-06-04 13:55:06 | [train_policy] epoch #545 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:06 | [train_policy] epoch #545 | computing loss before
2021-06-04 13:55:06 | [train_policy] epoch #545 | computing gradient
2021-06-04 13:55:06 | [train_policy] epoch #545 | gradient computed
2021-06-04 13:55:06 | [train_policy] epoch #545 | computing descent direction
2021-06-04 13:55:06 | [train_policy] epoch #545 | descent direction computed
2021-06-04 13:55:06 | [train_policy] epoch #545 | backtrack iters: 1
2021-06-04 13:55:06 | [train_policy] epoch #545 | optimization finished
2021-06-04 13:55:06 | [train_policy] epoch #545 | Computing KL after
2021-06-04 13:55:06 | [train_policy] epoch #545 | Computing loss after
2021-06-04 13:55:06 | [train_policy] epoch #545 | Fitting baseline...
2021-06-04 13:55:06 | [train_policy] epoch #545 | Saving snapshot...
2021-06-04 13:55:06 | [train_policy] epoch #545 | Saved
2021-06-04 13:55:06 | [train_policy] epoch #545 | Time 438.27 s
2021-06-04 13:55:06 | [train_policy] epoch #545 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285146
Evaluation/AverageDiscountedReturn          -42.1977
Evaluation/AverageReturn                    -42.1977
Evaluation/CompletionRate                     0
Evaluation/Iteration                        545
Evaluation/MaxReturn                        -29.9092
Evaluation/MinReturn                        -56.9506
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.61932
Extras/EpisodeRewardMean                    -42.0345
LinearFeatureBaseline/ExplainedVariance       0.904534
PolicyExecTime                                0.217563
ProcessExecTime                               0.0311449
TotalEnvSteps                            552552
policy/Entropy                               -1.53012
policy/KL                                     0.00661003
policy/KLBefore                               0
policy/LossAfter                             -0.0157305
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.21651
policy/dLoss                                  0.0157305
---------------------------------------  ----------------
2021-06-04 13:55:06 | [train_policy] epoch #546 | Obtaining samples for iteration 546...
2021-06-04 13:55:07 | [train_policy] epoch #546 | Logging diagnostics...
2021-06-04 13:55:07 | [train_policy] epoch #546 | Optimizing policy...
2021-06-04 13:55:07 | [train_policy] epoch #546 | Computing loss before
2021-06-04 13:55:07 | [train_policy] epoch #546 | Computing KL before
2021-06-04 13:55:07 | [train_policy] epoch #546 | Optimizing
2021-06-04 13:55:07 | [train_policy] epoch #546 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:07 | [train_policy] epoch #546 | computing loss before
2021-06-04 13:55:07 | [train_policy] epoch #546 | computing gradient
2021-06-04 13:55:07 | [train_policy] epoch #546 | gradient computed
2021-06-04 13:55:07 | [train_policy] epoch #546 | computing descent direction
2021-06-04 13:55:07 | [train_policy] epoch #546 | descent direction computed
2021-06-04 13:55:07 | [train_policy] epoch #546 | backtrack iters: 1
2021-06-04 13:55:07 | [train_policy] epoch #546 | optimization finished
2021-06-04 13:55:07 | [train_policy] epoch #546 | Computing KL after
2021-06-04 13:55:07 | [train_policy] epoch #546 | Computing loss after
2021-06-04 13:55:07 | [train_policy] epoch #546 | Fitting baseline...
2021-06-04 13:55:07 | [train_policy] epoch #546 | Saving snapshot...
2021-06-04 13:55:07 | [train_policy] epoch #546 | Saved
2021-06-04 13:55:07 | [train_policy] epoch #546 | Time 439.07 s
2021-06-04 13:55:07 | [train_policy] epoch #546 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28572
Evaluation/AverageDiscountedReturn          -42.5729
Evaluation/AverageReturn                    -42.5729
Evaluation/CompletionRate                     0
Evaluation/Iteration                        546
Evaluation/MaxReturn                        -28.1567
Evaluation/MinReturn                        -64.2885
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.018
Extras/EpisodeRewardMean                    -42.7108
LinearFeatureBaseline/ExplainedVariance       0.915222
PolicyExecTime                                0.230144
ProcessExecTime                               0.0312552
TotalEnvSteps                            553564
policy/Entropy                               -1.50306
policy/KL                                     0.00645077
policy/KLBefore                               0
policy/LossAfter                             -0.0147605
policy/LossBefore                             2.07321e-08
policy/Perplexity                             0.222449
policy/dLoss                                  0.0147606
---------------------------------------  ----------------
2021-06-04 13:55:07 | [train_policy] epoch #547 | Obtaining samples for iteration 547...
2021-06-04 13:55:08 | [train_policy] epoch #547 | Logging diagnostics...
2021-06-04 13:55:08 | [train_policy] epoch #547 | Optimizing policy...
2021-06-04 13:55:08 | [train_policy] epoch #547 | Computing loss before
2021-06-04 13:55:08 | [train_policy] epoch #547 | Computing KL before
2021-06-04 13:55:08 | [train_policy] epoch #547 | Optimizing
2021-06-04 13:55:08 | [train_policy] epoch #547 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:08 | [train_policy] epoch #547 | computing loss before
2021-06-04 13:55:08 | [train_policy] epoch #547 | computing gradient
2021-06-04 13:55:08 | [train_policy] epoch #547 | gradient computed
2021-06-04 13:55:08 | [train_policy] epoch #547 | computing descent direction
2021-06-04 13:55:08 | [train_policy] epoch #547 | descent direction computed
2021-06-04 13:55:08 | [train_policy] epoch #547 | backtrack iters: 0
2021-06-04 13:55:08 | [train_policy] epoch #547 | optimization finished
2021-06-04 13:55:08 | [train_policy] epoch #547 | Computing KL after
2021-06-04 13:55:08 | [train_policy] epoch #547 | Computing loss after
2021-06-04 13:55:08 | [train_policy] epoch #547 | Fitting baseline...
2021-06-04 13:55:08 | [train_policy] epoch #547 | Saving snapshot...
2021-06-04 13:55:08 | [train_policy] epoch #547 | Saved
2021-06-04 13:55:08 | [train_policy] epoch #547 | Time 439.88 s
2021-06-04 13:55:08 | [train_policy] epoch #547 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285138
Evaluation/AverageDiscountedReturn          -43.1607
Evaluation/AverageReturn                    -43.1607
Evaluation/CompletionRate                     0
Evaluation/Iteration                        547
Evaluation/MaxReturn                        -30.172
Evaluation/MinReturn                        -63.2706
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.60462
Extras/EpisodeRewardMean                    -42.8384
LinearFeatureBaseline/ExplainedVariance       0.921171
PolicyExecTime                                0.236044
ProcessExecTime                               0.0312324
TotalEnvSteps                            554576
policy/Entropy                               -1.49343
policy/KL                                     0.00993555
policy/KLBefore                               0
policy/LossAfter                             -0.0197583
policy/LossBefore                             1.30753e-08
policy/Perplexity                             0.2246
policy/dLoss                                  0.0197583
---------------------------------------  ----------------
2021-06-04 13:55:08 | [train_policy] epoch #548 | Obtaining samples for iteration 548...
2021-06-04 13:55:08 | [train_policy] epoch #548 | Logging diagnostics...
2021-06-04 13:55:08 | [train_policy] epoch #548 | Optimizing policy...
2021-06-04 13:55:08 | [train_policy] epoch #548 | Computing loss before
2021-06-04 13:55:08 | [train_policy] epoch #548 | Computing KL before
2021-06-04 13:55:08 | [train_policy] epoch #548 | Optimizing
2021-06-04 13:55:08 | [train_policy] epoch #548 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:08 | [train_policy] epoch #548 | computing loss before
2021-06-04 13:55:08 | [train_policy] epoch #548 | computing gradient
2021-06-04 13:55:08 | [train_policy] epoch #548 | gradient computed
2021-06-04 13:55:08 | [train_policy] epoch #548 | computing descent direction
2021-06-04 13:55:08 | [train_policy] epoch #548 | descent direction computed
2021-06-04 13:55:08 | [train_policy] epoch #548 | backtrack iters: 1
2021-06-04 13:55:08 | [train_policy] epoch #548 | optimization finished
2021-06-04 13:55:08 | [train_policy] epoch #548 | Computing KL after
2021-06-04 13:55:08 | [train_policy] epoch #548 | Computing loss after
2021-06-04 13:55:08 | [train_policy] epoch #548 | Fitting baseline...
2021-06-04 13:55:08 | [train_policy] epoch #548 | Saving snapshot...
2021-06-04 13:55:08 | [train_policy] epoch #548 | Saved
2021-06-04 13:55:08 | [train_policy] epoch #548 | Time 440.67 s
2021-06-04 13:55:08 | [train_policy] epoch #548 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284972
Evaluation/AverageDiscountedReturn          -42.6426
Evaluation/AverageReturn                    -42.6426
Evaluation/CompletionRate                     0
Evaluation/Iteration                        548
Evaluation/MaxReturn                        -28.2616
Evaluation/MinReturn                        -64.2872
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.82717
Extras/EpisodeRewardMean                    -42.8307
LinearFeatureBaseline/ExplainedVariance       0.896289
PolicyExecTime                                0.220642
ProcessExecTime                               0.0312016
TotalEnvSteps                            555588
policy/Entropy                               -1.50098
policy/KL                                     0.00641021
policy/KLBefore                               0
policy/LossAfter                             -0.0173115
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             0.222911
policy/dLoss                                  0.0173115
---------------------------------------  ----------------
2021-06-04 13:55:08 | [train_policy] epoch #549 | Obtaining samples for iteration 549...
2021-06-04 13:55:09 | [train_policy] epoch #549 | Logging diagnostics...
2021-06-04 13:55:09 | [train_policy] epoch #549 | Optimizing policy...
2021-06-04 13:55:09 | [train_policy] epoch #549 | Computing loss before
2021-06-04 13:55:09 | [train_policy] epoch #549 | Computing KL before
2021-06-04 13:55:09 | [train_policy] epoch #549 | Optimizing
2021-06-04 13:55:09 | [train_policy] epoch #549 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:09 | [train_policy] epoch #549 | computing loss before
2021-06-04 13:55:09 | [train_policy] epoch #549 | computing gradient
2021-06-04 13:55:09 | [train_policy] epoch #549 | gradient computed
2021-06-04 13:55:09 | [train_policy] epoch #549 | computing descent direction
2021-06-04 13:55:09 | [train_policy] epoch #549 | descent direction computed
2021-06-04 13:55:09 | [train_policy] epoch #549 | backtrack iters: 1
2021-06-04 13:55:09 | [train_policy] epoch #549 | optimization finished
2021-06-04 13:55:09 | [train_policy] epoch #549 | Computing KL after
2021-06-04 13:55:09 | [train_policy] epoch #549 | Computing loss after
2021-06-04 13:55:09 | [train_policy] epoch #549 | Fitting baseline...
2021-06-04 13:55:09 | [train_policy] epoch #549 | Saving snapshot...
2021-06-04 13:55:09 | [train_policy] epoch #549 | Saved
2021-06-04 13:55:09 | [train_policy] epoch #549 | Time 441.49 s
2021-06-04 13:55:09 | [train_policy] epoch #549 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.286749
Evaluation/AverageDiscountedReturn          -42.7934
Evaluation/AverageReturn                    -42.7934
Evaluation/CompletionRate                     0
Evaluation/Iteration                        549
Evaluation/MaxReturn                        -28.25
Evaluation/MinReturn                        -77.6986
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.55864
Extras/EpisodeRewardMean                    -42.5046
LinearFeatureBaseline/ExplainedVariance       0.885819
PolicyExecTime                                0.238577
ProcessExecTime                               0.0312526
TotalEnvSteps                            556600
policy/Entropy                               -1.51354
policy/KL                                     0.00656427
policy/KLBefore                               0
policy/LossAfter                             -0.0172289
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.220128
policy/dLoss                                  0.0172289
---------------------------------------  ----------------
2021-06-04 13:55:09 | [train_policy] epoch #550 | Obtaining samples for iteration 550...
2021-06-04 13:55:10 | [train_policy] epoch #550 | Logging diagnostics...
2021-06-04 13:55:10 | [train_policy] epoch #550 | Optimizing policy...
2021-06-04 13:55:10 | [train_policy] epoch #550 | Computing loss before
2021-06-04 13:55:10 | [train_policy] epoch #550 | Computing KL before
2021-06-04 13:55:10 | [train_policy] epoch #550 | Optimizing
2021-06-04 13:55:10 | [train_policy] epoch #550 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:10 | [train_policy] epoch #550 | computing loss before
2021-06-04 13:55:10 | [train_policy] epoch #550 | computing gradient
2021-06-04 13:55:10 | [train_policy] epoch #550 | gradient computed
2021-06-04 13:55:10 | [train_policy] epoch #550 | computing descent direction
2021-06-04 13:55:10 | [train_policy] epoch #550 | descent direction computed
2021-06-04 13:55:10 | [train_policy] epoch #550 | backtrack iters: 1
2021-06-04 13:55:10 | [train_policy] epoch #550 | optimization finished
2021-06-04 13:55:10 | [train_policy] epoch #550 | Computing KL after
2021-06-04 13:55:10 | [train_policy] epoch #550 | Computing loss after
2021-06-04 13:55:10 | [train_policy] epoch #550 | Fitting baseline...
2021-06-04 13:55:10 | [train_policy] epoch #550 | Saving snapshot...
2021-06-04 13:55:10 | [train_policy] epoch #550 | Saved
2021-06-04 13:55:10 | [train_policy] epoch #550 | Time 442.28 s
2021-06-04 13:55:10 | [train_policy] epoch #550 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284858
Evaluation/AverageDiscountedReturn          -41.2151
Evaluation/AverageReturn                    -41.2151
Evaluation/CompletionRate                     0
Evaluation/Iteration                        550
Evaluation/MaxReturn                        -30.0999
Evaluation/MinReturn                        -64.2317
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5061
Extras/EpisodeRewardMean                    -41.2611
LinearFeatureBaseline/ExplainedVariance       0.921592
PolicyExecTime                                0.220956
ProcessExecTime                               0.0312264
TotalEnvSteps                            557612
policy/Entropy                               -1.52055
policy/KL                                     0.00647235
policy/KLBefore                               0
policy/LossAfter                             -0.0143508
policy/LossBefore                             1.54312e-08
policy/Perplexity                             0.218592
policy/dLoss                                  0.0143508
---------------------------------------  ----------------
2021-06-04 13:55:10 | [train_policy] epoch #551 | Obtaining samples for iteration 551...
2021-06-04 13:55:11 | [train_policy] epoch #551 | Logging diagnostics...
2021-06-04 13:55:11 | [train_policy] epoch #551 | Optimizing policy...
2021-06-04 13:55:11 | [train_policy] epoch #551 | Computing loss before
2021-06-04 13:55:11 | [train_policy] epoch #551 | Computing KL before
2021-06-04 13:55:11 | [train_policy] epoch #551 | Optimizing
2021-06-04 13:55:11 | [train_policy] epoch #551 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:11 | [train_policy] epoch #551 | computing loss before
2021-06-04 13:55:11 | [train_policy] epoch #551 | computing gradient
2021-06-04 13:55:11 | [train_policy] epoch #551 | gradient computed
2021-06-04 13:55:11 | [train_policy] epoch #551 | computing descent direction
2021-06-04 13:55:11 | [train_policy] epoch #551 | descent direction computed
2021-06-04 13:55:11 | [train_policy] epoch #551 | backtrack iters: 0
2021-06-04 13:55:11 | [train_policy] epoch #551 | optimization finished
2021-06-04 13:55:11 | [train_policy] epoch #551 | Computing KL after
2021-06-04 13:55:11 | [train_policy] epoch #551 | Computing loss after
2021-06-04 13:55:11 | [train_policy] epoch #551 | Fitting baseline...
2021-06-04 13:55:11 | [train_policy] epoch #551 | Saving snapshot...
2021-06-04 13:55:11 | [train_policy] epoch #551 | Saved
2021-06-04 13:55:11 | [train_policy] epoch #551 | Time 443.10 s
2021-06-04 13:55:11 | [train_policy] epoch #551 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284968
Evaluation/AverageDiscountedReturn          -41.568
Evaluation/AverageReturn                    -41.568
Evaluation/CompletionRate                     0
Evaluation/Iteration                        551
Evaluation/MaxReturn                        -28.6757
Evaluation/MinReturn                        -64.2606
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.14267
Extras/EpisodeRewardMean                    -41.3097
LinearFeatureBaseline/ExplainedVariance       0.91207
PolicyExecTime                                0.229559
ProcessExecTime                               0.0315223
TotalEnvSteps                            558624
policy/Entropy                               -1.50111
policy/KL                                     0.00992193
policy/KLBefore                               0
policy/LossAfter                             -0.0153073
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             0.222882
policy/dLoss                                  0.0153073
---------------------------------------  ----------------
2021-06-04 13:55:11 | [train_policy] epoch #552 | Obtaining samples for iteration 552...
2021-06-04 13:55:12 | [train_policy] epoch #552 | Logging diagnostics...
2021-06-04 13:55:12 | [train_policy] epoch #552 | Optimizing policy...
2021-06-04 13:55:12 | [train_policy] epoch #552 | Computing loss before
2021-06-04 13:55:12 | [train_policy] epoch #552 | Computing KL before
2021-06-04 13:55:12 | [train_policy] epoch #552 | Optimizing
2021-06-04 13:55:12 | [train_policy] epoch #552 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:12 | [train_policy] epoch #552 | computing loss before
2021-06-04 13:55:12 | [train_policy] epoch #552 | computing gradient
2021-06-04 13:55:12 | [train_policy] epoch #552 | gradient computed
2021-06-04 13:55:12 | [train_policy] epoch #552 | computing descent direction
2021-06-04 13:55:12 | [train_policy] epoch #552 | descent direction computed
2021-06-04 13:55:12 | [train_policy] epoch #552 | backtrack iters: 0
2021-06-04 13:55:12 | [train_policy] epoch #552 | optimization finished
2021-06-04 13:55:12 | [train_policy] epoch #552 | Computing KL after
2021-06-04 13:55:12 | [train_policy] epoch #552 | Computing loss after
2021-06-04 13:55:12 | [train_policy] epoch #552 | Fitting baseline...
2021-06-04 13:55:12 | [train_policy] epoch #552 | Saving snapshot...
2021-06-04 13:55:12 | [train_policy] epoch #552 | Saved
2021-06-04 13:55:12 | [train_policy] epoch #552 | Time 443.88 s
2021-06-04 13:55:12 | [train_policy] epoch #552 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285331
Evaluation/AverageDiscountedReturn          -43.153
Evaluation/AverageReturn                    -43.153
Evaluation/CompletionRate                     0
Evaluation/Iteration                        552
Evaluation/MaxReturn                        -29.2083
Evaluation/MinReturn                        -79.8004
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.11342
Extras/EpisodeRewardMean                    -43.2014
LinearFeatureBaseline/ExplainedVariance       0.890663
PolicyExecTime                                0.219796
ProcessExecTime                               0.0312939
TotalEnvSteps                            559636
policy/Entropy                               -1.5234
policy/KL                                     0.00995617
policy/KLBefore                               0
policy/LossAfter                             -0.0185016
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.217969
policy/dLoss                                  0.0185016
---------------------------------------  ----------------
2021-06-04 13:55:12 | [train_policy] epoch #553 | Obtaining samples for iteration 553...
2021-06-04 13:55:12 | [train_policy] epoch #553 | Logging diagnostics...
2021-06-04 13:55:12 | [train_policy] epoch #553 | Optimizing policy...
2021-06-04 13:55:12 | [train_policy] epoch #553 | Computing loss before
2021-06-04 13:55:12 | [train_policy] epoch #553 | Computing KL before
2021-06-04 13:55:12 | [train_policy] epoch #553 | Optimizing
2021-06-04 13:55:12 | [train_policy] epoch #553 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:12 | [train_policy] epoch #553 | computing loss before
2021-06-04 13:55:12 | [train_policy] epoch #553 | computing gradient
2021-06-04 13:55:12 | [train_policy] epoch #553 | gradient computed
2021-06-04 13:55:12 | [train_policy] epoch #553 | computing descent direction
2021-06-04 13:55:12 | [train_policy] epoch #553 | descent direction computed
2021-06-04 13:55:12 | [train_policy] epoch #553 | backtrack iters: 1
2021-06-04 13:55:12 | [train_policy] epoch #553 | optimization finished
2021-06-04 13:55:12 | [train_policy] epoch #553 | Computing KL after
2021-06-04 13:55:12 | [train_policy] epoch #553 | Computing loss after
2021-06-04 13:55:12 | [train_policy] epoch #553 | Fitting baseline...
2021-06-04 13:55:12 | [train_policy] epoch #553 | Saving snapshot...
2021-06-04 13:55:12 | [train_policy] epoch #553 | Saved
2021-06-04 13:55:12 | [train_policy] epoch #553 | Time 444.68 s
2021-06-04 13:55:12 | [train_policy] epoch #553 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286283
Evaluation/AverageDiscountedReturn          -40.7576
Evaluation/AverageReturn                    -40.7576
Evaluation/CompletionRate                     0
Evaluation/Iteration                        553
Evaluation/MaxReturn                        -29.1073
Evaluation/MinReturn                        -63.1094
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.31308
Extras/EpisodeRewardMean                    -40.9911
LinearFeatureBaseline/ExplainedVariance       0.852142
PolicyExecTime                                0.224955
ProcessExecTime                               0.0314298
TotalEnvSteps                            560648
policy/Entropy                               -1.52456
policy/KL                                     0.00649158
policy/KLBefore                               0
policy/LossAfter                             -0.0157072
policy/LossBefore                            -2.12032e-09
policy/Perplexity                             0.217717
policy/dLoss                                  0.0157072
---------------------------------------  ----------------
2021-06-04 13:55:12 | [train_policy] epoch #554 | Obtaining samples for iteration 554...
2021-06-04 13:55:13 | [train_policy] epoch #554 | Logging diagnostics...
2021-06-04 13:55:13 | [train_policy] epoch #554 | Optimizing policy...
2021-06-04 13:55:13 | [train_policy] epoch #554 | Computing loss before
2021-06-04 13:55:13 | [train_policy] epoch #554 | Computing KL before
2021-06-04 13:55:13 | [train_policy] epoch #554 | Optimizing
2021-06-04 13:55:13 | [train_policy] epoch #554 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:13 | [train_policy] epoch #554 | computing loss before
2021-06-04 13:55:13 | [train_policy] epoch #554 | computing gradient
2021-06-04 13:55:13 | [train_policy] epoch #554 | gradient computed
2021-06-04 13:55:13 | [train_policy] epoch #554 | computing descent direction
2021-06-04 13:55:13 | [train_policy] epoch #554 | descent direction computed
2021-06-04 13:55:13 | [train_policy] epoch #554 | backtrack iters: 0
2021-06-04 13:55:13 | [train_policy] epoch #554 | optimization finished
2021-06-04 13:55:13 | [train_policy] epoch #554 | Computing KL after
2021-06-04 13:55:13 | [train_policy] epoch #554 | Computing loss after
2021-06-04 13:55:13 | [train_policy] epoch #554 | Fitting baseline...
2021-06-04 13:55:13 | [train_policy] epoch #554 | Saving snapshot...
2021-06-04 13:55:13 | [train_policy] epoch #554 | Saved
2021-06-04 13:55:13 | [train_policy] epoch #554 | Time 445.50 s
2021-06-04 13:55:13 | [train_policy] epoch #554 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                   0.287785
Evaluation/AverageDiscountedReturn          -41.9662
Evaluation/AverageReturn                    -41.9662
Evaluation/CompletionRate                     0
Evaluation/Iteration                        554
Evaluation/MaxReturn                        -28.7217
Evaluation/MinReturn                        -63.1142
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.9583
Extras/EpisodeRewardMean                    -41.8212
LinearFeatureBaseline/ExplainedVariance       0.906006
PolicyExecTime                                0.235631
ProcessExecTime                               0.0315821
TotalEnvSteps                            561660
policy/Entropy                               -1.50711
policy/KL                                     0.00988172
policy/KLBefore                               0
policy/LossAfter                             -0.0203343
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.22155
policy/dLoss                                  0.0203343
---------------------------------------  ---------------
2021-06-04 13:55:13 | [train_policy] epoch #555 | Obtaining samples for iteration 555...
2021-06-04 13:55:14 | [train_policy] epoch #555 | Logging diagnostics...
2021-06-04 13:55:14 | [train_policy] epoch #555 | Optimizing policy...
2021-06-04 13:55:14 | [train_policy] epoch #555 | Computing loss before
2021-06-04 13:55:14 | [train_policy] epoch #555 | Computing KL before
2021-06-04 13:55:14 | [train_policy] epoch #555 | Optimizing
2021-06-04 13:55:14 | [train_policy] epoch #555 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:14 | [train_policy] epoch #555 | computing loss before
2021-06-04 13:55:14 | [train_policy] epoch #555 | computing gradient
2021-06-04 13:55:14 | [train_policy] epoch #555 | gradient computed
2021-06-04 13:55:14 | [train_policy] epoch #555 | computing descent direction
2021-06-04 13:55:14 | [train_policy] epoch #555 | descent direction computed
2021-06-04 13:55:14 | [train_policy] epoch #555 | backtrack iters: 0
2021-06-04 13:55:14 | [train_policy] epoch #555 | optimization finished
2021-06-04 13:55:14 | [train_policy] epoch #555 | Computing KL after
2021-06-04 13:55:14 | [train_policy] epoch #555 | Computing loss after
2021-06-04 13:55:14 | [train_policy] epoch #555 | Fitting baseline...
2021-06-04 13:55:14 | [train_policy] epoch #555 | Saving snapshot...
2021-06-04 13:55:14 | [train_policy] epoch #555 | Saved
2021-06-04 13:55:14 | [train_policy] epoch #555 | Time 446.28 s
2021-06-04 13:55:14 | [train_policy] epoch #555 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28466
Evaluation/AverageDiscountedReturn          -43.6529
Evaluation/AverageReturn                    -43.6529
Evaluation/CompletionRate                     0
Evaluation/Iteration                        555
Evaluation/MaxReturn                        -28.9847
Evaluation/MinReturn                        -80.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.33751
Extras/EpisodeRewardMean                    -43.5349
LinearFeatureBaseline/ExplainedVariance       0.879459
PolicyExecTime                                0.221587
ProcessExecTime                               0.0312066
TotalEnvSteps                            562672
policy/Entropy                               -1.51003
policy/KL                                     0.00996585
policy/KLBefore                               0
policy/LossAfter                             -0.029498
policy/LossBefore                            -2.35591e-10
policy/Perplexity                             0.220903
policy/dLoss                                  0.029498
---------------------------------------  ----------------
2021-06-04 13:55:14 | [train_policy] epoch #556 | Obtaining samples for iteration 556...
2021-06-04 13:55:15 | [train_policy] epoch #556 | Logging diagnostics...
2021-06-04 13:55:15 | [train_policy] epoch #556 | Optimizing policy...
2021-06-04 13:55:15 | [train_policy] epoch #556 | Computing loss before
2021-06-04 13:55:15 | [train_policy] epoch #556 | Computing KL before
2021-06-04 13:55:15 | [train_policy] epoch #556 | Optimizing
2021-06-04 13:55:15 | [train_policy] epoch #556 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:15 | [train_policy] epoch #556 | computing loss before
2021-06-04 13:55:15 | [train_policy] epoch #556 | computing gradient
2021-06-04 13:55:15 | [train_policy] epoch #556 | gradient computed
2021-06-04 13:55:15 | [train_policy] epoch #556 | computing descent direction
2021-06-04 13:55:15 | [train_policy] epoch #556 | descent direction computed
2021-06-04 13:55:15 | [train_policy] epoch #556 | backtrack iters: 0
2021-06-04 13:55:15 | [train_policy] epoch #556 | optimization finished
2021-06-04 13:55:15 | [train_policy] epoch #556 | Computing KL after
2021-06-04 13:55:15 | [train_policy] epoch #556 | Computing loss after
2021-06-04 13:55:15 | [train_policy] epoch #556 | Fitting baseline...
2021-06-04 13:55:15 | [train_policy] epoch #556 | Saving snapshot...
2021-06-04 13:55:15 | [train_policy] epoch #556 | Saved
2021-06-04 13:55:15 | [train_policy] epoch #556 | Time 447.07 s
2021-06-04 13:55:15 | [train_policy] epoch #556 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285455
Evaluation/AverageDiscountedReturn          -42.8056
Evaluation/AverageReturn                    -42.8056
Evaluation/CompletionRate                     0
Evaluation/Iteration                        556
Evaluation/MaxReturn                        -29.6281
Evaluation/MinReturn                        -63.275
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.57703
Extras/EpisodeRewardMean                    -42.813
LinearFeatureBaseline/ExplainedVariance       0.884483
PolicyExecTime                                0.222524
ProcessExecTime                               0.0312696
TotalEnvSteps                            563684
policy/Entropy                               -1.50505
policy/KL                                     0.00993465
policy/KLBefore                               0
policy/LossAfter                             -0.0164304
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.222007
policy/dLoss                                  0.0164304
---------------------------------------  ----------------
2021-06-04 13:55:15 | [train_policy] epoch #557 | Obtaining samples for iteration 557...
2021-06-04 13:55:16 | [train_policy] epoch #557 | Logging diagnostics...
2021-06-04 13:55:16 | [train_policy] epoch #557 | Optimizing policy...
2021-06-04 13:55:16 | [train_policy] epoch #557 | Computing loss before
2021-06-04 13:55:16 | [train_policy] epoch #557 | Computing KL before
2021-06-04 13:55:16 | [train_policy] epoch #557 | Optimizing
2021-06-04 13:55:16 | [train_policy] epoch #557 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:16 | [train_policy] epoch #557 | computing loss before
2021-06-04 13:55:16 | [train_policy] epoch #557 | computing gradient
2021-06-04 13:55:16 | [train_policy] epoch #557 | gradient computed
2021-06-04 13:55:16 | [train_policy] epoch #557 | computing descent direction
2021-06-04 13:55:16 | [train_policy] epoch #557 | descent direction computed
2021-06-04 13:55:16 | [train_policy] epoch #557 | backtrack iters: 1
2021-06-04 13:55:16 | [train_policy] epoch #557 | optimization finished
2021-06-04 13:55:16 | [train_policy] epoch #557 | Computing KL after
2021-06-04 13:55:16 | [train_policy] epoch #557 | Computing loss after
2021-06-04 13:55:16 | [train_policy] epoch #557 | Fitting baseline...
2021-06-04 13:55:16 | [train_policy] epoch #557 | Saving snapshot...
2021-06-04 13:55:16 | [train_policy] epoch #557 | Saved
2021-06-04 13:55:16 | [train_policy] epoch #557 | Time 447.88 s
2021-06-04 13:55:16 | [train_policy] epoch #557 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287636
Evaluation/AverageDiscountedReturn          -43.8911
Evaluation/AverageReturn                    -43.8911
Evaluation/CompletionRate                     0
Evaluation/Iteration                        557
Evaluation/MaxReturn                        -28.8618
Evaluation/MinReturn                        -82.6246
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2021
Extras/EpisodeRewardMean                    -43.7953
LinearFeatureBaseline/ExplainedVariance       0.854715
PolicyExecTime                                0.232206
ProcessExecTime                               0.0314243
TotalEnvSteps                            564696
policy/Entropy                               -1.46635
policy/KL                                     0.00649492
policy/KLBefore                               0
policy/LossAfter                             -0.0156164
policy/LossBefore                             1.17796e-09
policy/Perplexity                             0.230766
policy/dLoss                                  0.0156164
---------------------------------------  ----------------
2021-06-04 13:55:16 | [train_policy] epoch #558 | Obtaining samples for iteration 558...
2021-06-04 13:55:16 | [train_policy] epoch #558 | Logging diagnostics...
2021-06-04 13:55:16 | [train_policy] epoch #558 | Optimizing policy...
2021-06-04 13:55:16 | [train_policy] epoch #558 | Computing loss before
2021-06-04 13:55:16 | [train_policy] epoch #558 | Computing KL before
2021-06-04 13:55:16 | [train_policy] epoch #558 | Optimizing
2021-06-04 13:55:16 | [train_policy] epoch #558 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:16 | [train_policy] epoch #558 | computing loss before
2021-06-04 13:55:16 | [train_policy] epoch #558 | computing gradient
2021-06-04 13:55:16 | [train_policy] epoch #558 | gradient computed
2021-06-04 13:55:16 | [train_policy] epoch #558 | computing descent direction
2021-06-04 13:55:16 | [train_policy] epoch #558 | descent direction computed
2021-06-04 13:55:16 | [train_policy] epoch #558 | backtrack iters: 0
2021-06-04 13:55:16 | [train_policy] epoch #558 | optimization finished
2021-06-04 13:55:16 | [train_policy] epoch #558 | Computing KL after
2021-06-04 13:55:16 | [train_policy] epoch #558 | Computing loss after
2021-06-04 13:55:16 | [train_policy] epoch #558 | Fitting baseline...
2021-06-04 13:55:16 | [train_policy] epoch #558 | Saving snapshot...
2021-06-04 13:55:16 | [train_policy] epoch #558 | Saved
2021-06-04 13:55:16 | [train_policy] epoch #558 | Time 448.68 s
2021-06-04 13:55:16 | [train_policy] epoch #558 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285975
Evaluation/AverageDiscountedReturn          -42.4544
Evaluation/AverageReturn                    -42.4544
Evaluation/CompletionRate                     0
Evaluation/Iteration                        558
Evaluation/MaxReturn                        -27.764
Evaluation/MinReturn                        -82.4626
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3117
Extras/EpisodeRewardMean                    -42.5355
LinearFeatureBaseline/ExplainedVariance       0.84416
PolicyExecTime                                0.235308
ProcessExecTime                               0.031239
TotalEnvSteps                            565708
policy/Entropy                               -1.46713
policy/KL                                     0.00973015
policy/KLBefore                               0
policy/LossAfter                             -0.0160094
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.230587
policy/dLoss                                  0.0160094
---------------------------------------  ----------------
2021-06-04 13:55:16 | [train_policy] epoch #559 | Obtaining samples for iteration 559...
2021-06-04 13:55:17 | [train_policy] epoch #559 | Logging diagnostics...
2021-06-04 13:55:17 | [train_policy] epoch #559 | Optimizing policy...
2021-06-04 13:55:17 | [train_policy] epoch #559 | Computing loss before
2021-06-04 13:55:17 | [train_policy] epoch #559 | Computing KL before
2021-06-04 13:55:17 | [train_policy] epoch #559 | Optimizing
2021-06-04 13:55:17 | [train_policy] epoch #559 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:17 | [train_policy] epoch #559 | computing loss before
2021-06-04 13:55:17 | [train_policy] epoch #559 | computing gradient
2021-06-04 13:55:17 | [train_policy] epoch #559 | gradient computed
2021-06-04 13:55:17 | [train_policy] epoch #559 | computing descent direction
2021-06-04 13:55:17 | [train_policy] epoch #559 | descent direction computed
2021-06-04 13:55:17 | [train_policy] epoch #559 | backtrack iters: 0
2021-06-04 13:55:17 | [train_policy] epoch #559 | optimization finished
2021-06-04 13:55:17 | [train_policy] epoch #559 | Computing KL after
2021-06-04 13:55:17 | [train_policy] epoch #559 | Computing loss after
2021-06-04 13:55:17 | [train_policy] epoch #559 | Fitting baseline...
2021-06-04 13:55:17 | [train_policy] epoch #559 | Saving snapshot...
2021-06-04 13:55:17 | [train_policy] epoch #559 | Saved
2021-06-04 13:55:17 | [train_policy] epoch #559 | Time 449.47 s
2021-06-04 13:55:17 | [train_policy] epoch #559 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285128
Evaluation/AverageDiscountedReturn          -43.4374
Evaluation/AverageReturn                    -43.4374
Evaluation/CompletionRate                     0
Evaluation/Iteration                        559
Evaluation/MaxReturn                        -32.929
Evaluation/MinReturn                        -64.2187
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.10804
Extras/EpisodeRewardMean                    -43.5955
LinearFeatureBaseline/ExplainedVariance       0.899084
PolicyExecTime                                0.218422
ProcessExecTime                               0.0312026
TotalEnvSteps                            566720
policy/Entropy                               -1.42908
policy/KL                                     0.00976778
policy/KLBefore                               0
policy/LossAfter                             -0.0166636
policy/LossBefore                            -1.97897e-08
policy/Perplexity                             0.239528
policy/dLoss                                  0.0166636
---------------------------------------  ----------------
2021-06-04 13:55:17 | [train_policy] epoch #560 | Obtaining samples for iteration 560...
2021-06-04 13:55:18 | [train_policy] epoch #560 | Logging diagnostics...
2021-06-04 13:55:18 | [train_policy] epoch #560 | Optimizing policy...
2021-06-04 13:55:18 | [train_policy] epoch #560 | Computing loss before
2021-06-04 13:55:18 | [train_policy] epoch #560 | Computing KL before
2021-06-04 13:55:18 | [train_policy] epoch #560 | Optimizing
2021-06-04 13:55:18 | [train_policy] epoch #560 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:18 | [train_policy] epoch #560 | computing loss before
2021-06-04 13:55:18 | [train_policy] epoch #560 | computing gradient
2021-06-04 13:55:18 | [train_policy] epoch #560 | gradient computed
2021-06-04 13:55:18 | [train_policy] epoch #560 | computing descent direction
2021-06-04 13:55:18 | [train_policy] epoch #560 | descent direction computed
2021-06-04 13:55:18 | [train_policy] epoch #560 | backtrack iters: 0
2021-06-04 13:55:18 | [train_policy] epoch #560 | optimization finished
2021-06-04 13:55:18 | [train_policy] epoch #560 | Computing KL after
2021-06-04 13:55:18 | [train_policy] epoch #560 | Computing loss after
2021-06-04 13:55:18 | [train_policy] epoch #560 | Fitting baseline...
2021-06-04 13:55:18 | [train_policy] epoch #560 | Saving snapshot...
2021-06-04 13:55:18 | [train_policy] epoch #560 | Saved
2021-06-04 13:55:18 | [train_policy] epoch #560 | Time 450.25 s
2021-06-04 13:55:18 | [train_policy] epoch #560 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.284173
Evaluation/AverageDiscountedReturn          -42.8486
Evaluation/AverageReturn                    -42.8486
Evaluation/CompletionRate                     0
Evaluation/Iteration                        560
Evaluation/MaxReturn                        -29.9284
Evaluation/MinReturn                        -79.9053
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.44729
Extras/EpisodeRewardMean                    -43.2243
LinearFeatureBaseline/ExplainedVariance       0.852232
PolicyExecTime                                0.205456
ProcessExecTime                               0.0311348
TotalEnvSteps                            567732
policy/Entropy                               -1.43908
policy/KL                                     0.00996903
policy/KLBefore                               0
policy/LossAfter                             -0.0173034
policy/LossBefore                            -8.3635e-09
policy/Perplexity                             0.237147
policy/dLoss                                  0.0173034
---------------------------------------  ---------------
2021-06-04 13:55:18 | [train_policy] epoch #561 | Obtaining samples for iteration 561...
2021-06-04 13:55:19 | [train_policy] epoch #561 | Logging diagnostics...
2021-06-04 13:55:19 | [train_policy] epoch #561 | Optimizing policy...
2021-06-04 13:55:19 | [train_policy] epoch #561 | Computing loss before
2021-06-04 13:55:19 | [train_policy] epoch #561 | Computing KL before
2021-06-04 13:55:19 | [train_policy] epoch #561 | Optimizing
2021-06-04 13:55:19 | [train_policy] epoch #561 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:19 | [train_policy] epoch #561 | computing loss before
2021-06-04 13:55:19 | [train_policy] epoch #561 | computing gradient
2021-06-04 13:55:19 | [train_policy] epoch #561 | gradient computed
2021-06-04 13:55:19 | [train_policy] epoch #561 | computing descent direction
2021-06-04 13:55:19 | [train_policy] epoch #561 | descent direction computed
2021-06-04 13:55:19 | [train_policy] epoch #561 | backtrack iters: 1
2021-06-04 13:55:19 | [train_policy] epoch #561 | optimization finished
2021-06-04 13:55:19 | [train_policy] epoch #561 | Computing KL after
2021-06-04 13:55:19 | [train_policy] epoch #561 | Computing loss after
2021-06-04 13:55:19 | [train_policy] epoch #561 | Fitting baseline...
2021-06-04 13:55:19 | [train_policy] epoch #561 | Saving snapshot...
2021-06-04 13:55:19 | [train_policy] epoch #561 | Saved
2021-06-04 13:55:19 | [train_policy] epoch #561 | Time 451.05 s
2021-06-04 13:55:19 | [train_policy] epoch #561 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284424
Evaluation/AverageDiscountedReturn          -41.2936
Evaluation/AverageReturn                    -41.2936
Evaluation/CompletionRate                     0
Evaluation/Iteration                        561
Evaluation/MaxReturn                        -28.7788
Evaluation/MinReturn                        -79.2765
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12988
Extras/EpisodeRewardMean                    -41.4096
LinearFeatureBaseline/ExplainedVariance       0.891103
PolicyExecTime                                0.23104
ProcessExecTime                               0.0311236
TotalEnvSteps                            568744
policy/Entropy                               -1.47752
policy/KL                                     0.00663145
policy/KLBefore                               0
policy/LossAfter                             -0.0208411
policy/LossBefore                             1.63736e-08
policy/Perplexity                             0.228203
policy/dLoss                                  0.0208411
---------------------------------------  ----------------
2021-06-04 13:55:19 | [train_policy] epoch #562 | Obtaining samples for iteration 562...
2021-06-04 13:55:19 | [train_policy] epoch #562 | Logging diagnostics...
2021-06-04 13:55:19 | [train_policy] epoch #562 | Optimizing policy...
2021-06-04 13:55:19 | [train_policy] epoch #562 | Computing loss before
2021-06-04 13:55:19 | [train_policy] epoch #562 | Computing KL before
2021-06-04 13:55:19 | [train_policy] epoch #562 | Optimizing
2021-06-04 13:55:19 | [train_policy] epoch #562 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:19 | [train_policy] epoch #562 | computing loss before
2021-06-04 13:55:19 | [train_policy] epoch #562 | computing gradient
2021-06-04 13:55:19 | [train_policy] epoch #562 | gradient computed
2021-06-04 13:55:19 | [train_policy] epoch #562 | computing descent direction
2021-06-04 13:55:20 | [train_policy] epoch #562 | descent direction computed
2021-06-04 13:55:20 | [train_policy] epoch #562 | backtrack iters: 1
2021-06-04 13:55:20 | [train_policy] epoch #562 | optimization finished
2021-06-04 13:55:20 | [train_policy] epoch #562 | Computing KL after
2021-06-04 13:55:20 | [train_policy] epoch #562 | Computing loss after
2021-06-04 13:55:20 | [train_policy] epoch #562 | Fitting baseline...
2021-06-04 13:55:20 | [train_policy] epoch #562 | Saving snapshot...
2021-06-04 13:55:20 | [train_policy] epoch #562 | Saved
2021-06-04 13:55:20 | [train_policy] epoch #562 | Time 451.85 s
2021-06-04 13:55:20 | [train_policy] epoch #562 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285275
Evaluation/AverageDiscountedReturn          -41.7233
Evaluation/AverageReturn                    -41.7233
Evaluation/CompletionRate                     0
Evaluation/Iteration                        562
Evaluation/MaxReturn                        -29.2724
Evaluation/MinReturn                        -58.4494
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.76881
Extras/EpisodeRewardMean                    -41.5374
LinearFeatureBaseline/ExplainedVariance       0.918635
PolicyExecTime                                0.231334
ProcessExecTime                               0.0312824
TotalEnvSteps                            569756
policy/Entropy                               -1.43007
policy/KL                                     0.00686283
policy/KLBefore                               0
policy/LossAfter                             -0.0133851
policy/LossBefore                            -4.24065e-08
policy/Perplexity                             0.239293
policy/dLoss                                  0.013385
---------------------------------------  ----------------
2021-06-04 13:55:20 | [train_policy] epoch #563 | Obtaining samples for iteration 563...
2021-06-04 13:55:20 | [train_policy] epoch #563 | Logging diagnostics...
2021-06-04 13:55:20 | [train_policy] epoch #563 | Optimizing policy...
2021-06-04 13:55:20 | [train_policy] epoch #563 | Computing loss before
2021-06-04 13:55:20 | [train_policy] epoch #563 | Computing KL before
2021-06-04 13:55:20 | [train_policy] epoch #563 | Optimizing
2021-06-04 13:55:20 | [train_policy] epoch #563 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:20 | [train_policy] epoch #563 | computing loss before
2021-06-04 13:55:20 | [train_policy] epoch #563 | computing gradient
2021-06-04 13:55:20 | [train_policy] epoch #563 | gradient computed
2021-06-04 13:55:20 | [train_policy] epoch #563 | computing descent direction
2021-06-04 13:55:20 | [train_policy] epoch #563 | descent direction computed
2021-06-04 13:55:20 | [train_policy] epoch #563 | backtrack iters: 0
2021-06-04 13:55:20 | [train_policy] epoch #563 | optimization finished
2021-06-04 13:55:20 | [train_policy] epoch #563 | Computing KL after
2021-06-04 13:55:20 | [train_policy] epoch #563 | Computing loss after
2021-06-04 13:55:20 | [train_policy] epoch #563 | Fitting baseline...
2021-06-04 13:55:20 | [train_policy] epoch #563 | Saving snapshot...
2021-06-04 13:55:20 | [train_policy] epoch #563 | Saved
2021-06-04 13:55:20 | [train_policy] epoch #563 | Time 452.66 s
2021-06-04 13:55:20 | [train_policy] epoch #563 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28828
Evaluation/AverageDiscountedReturn          -41.7037
Evaluation/AverageReturn                    -41.7037
Evaluation/CompletionRate                     0
Evaluation/Iteration                        563
Evaluation/MaxReturn                        -28.9067
Evaluation/MinReturn                        -64.1975
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.32106
Extras/EpisodeRewardMean                    -41.599
LinearFeatureBaseline/ExplainedVariance       0.918082
PolicyExecTime                                0.234445
ProcessExecTime                               0.0315857
TotalEnvSteps                            570768
policy/Entropy                               -1.43447
policy/KL                                     0.00991975
policy/KLBefore                               0
policy/LossAfter                             -0.020559
policy/LossBefore                            -1.60202e-08
policy/Perplexity                             0.238241
policy/dLoss                                  0.0205589
---------------------------------------  ----------------
2021-06-04 13:55:20 | [train_policy] epoch #564 | Obtaining samples for iteration 564...
2021-06-04 13:55:21 | [train_policy] epoch #564 | Logging diagnostics...
2021-06-04 13:55:21 | [train_policy] epoch #564 | Optimizing policy...
2021-06-04 13:55:21 | [train_policy] epoch #564 | Computing loss before
2021-06-04 13:55:21 | [train_policy] epoch #564 | Computing KL before
2021-06-04 13:55:21 | [train_policy] epoch #564 | Optimizing
2021-06-04 13:55:21 | [train_policy] epoch #564 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:21 | [train_policy] epoch #564 | computing loss before
2021-06-04 13:55:21 | [train_policy] epoch #564 | computing gradient
2021-06-04 13:55:21 | [train_policy] epoch #564 | gradient computed
2021-06-04 13:55:21 | [train_policy] epoch #564 | computing descent direction
2021-06-04 13:55:21 | [train_policy] epoch #564 | descent direction computed
2021-06-04 13:55:21 | [train_policy] epoch #564 | backtrack iters: 0
2021-06-04 13:55:21 | [train_policy] epoch #564 | optimization finished
2021-06-04 13:55:21 | [train_policy] epoch #564 | Computing KL after
2021-06-04 13:55:21 | [train_policy] epoch #564 | Computing loss after
2021-06-04 13:55:21 | [train_policy] epoch #564 | Fitting baseline...
2021-06-04 13:55:21 | [train_policy] epoch #564 | Saving snapshot...
2021-06-04 13:55:21 | [train_policy] epoch #564 | Saved
2021-06-04 13:55:21 | [train_policy] epoch #564 | Time 453.46 s
2021-06-04 13:55:21 | [train_policy] epoch #564 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286191
Evaluation/AverageDiscountedReturn          -43.43
Evaluation/AverageReturn                    -43.43
Evaluation/CompletionRate                     0
Evaluation/Iteration                        564
Evaluation/MaxReturn                        -31.4567
Evaluation/MinReturn                        -80.4855
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.98896
Extras/EpisodeRewardMean                    -43.2911
LinearFeatureBaseline/ExplainedVariance       0.880632
PolicyExecTime                                0.232901
ProcessExecTime                               0.0313447
TotalEnvSteps                            571780
policy/Entropy                               -1.41819
policy/KL                                     0.00992046
policy/KLBefore                               0
policy/LossAfter                             -0.031301
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.242152
policy/dLoss                                  0.031301
---------------------------------------  ----------------
2021-06-04 13:55:21 | [train_policy] epoch #565 | Obtaining samples for iteration 565...
2021-06-04 13:55:22 | [train_policy] epoch #565 | Logging diagnostics...
2021-06-04 13:55:22 | [train_policy] epoch #565 | Optimizing policy...
2021-06-04 13:55:22 | [train_policy] epoch #565 | Computing loss before
2021-06-04 13:55:22 | [train_policy] epoch #565 | Computing KL before
2021-06-04 13:55:22 | [train_policy] epoch #565 | Optimizing
2021-06-04 13:55:22 | [train_policy] epoch #565 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:22 | [train_policy] epoch #565 | computing loss before
2021-06-04 13:55:22 | [train_policy] epoch #565 | computing gradient
2021-06-04 13:55:22 | [train_policy] epoch #565 | gradient computed
2021-06-04 13:55:22 | [train_policy] epoch #565 | computing descent direction
2021-06-04 13:55:22 | [train_policy] epoch #565 | descent direction computed
2021-06-04 13:55:22 | [train_policy] epoch #565 | backtrack iters: 0
2021-06-04 13:55:22 | [train_policy] epoch #565 | optimization finished
2021-06-04 13:55:22 | [train_policy] epoch #565 | Computing KL after
2021-06-04 13:55:22 | [train_policy] epoch #565 | Computing loss after
2021-06-04 13:55:22 | [train_policy] epoch #565 | Fitting baseline...
2021-06-04 13:55:22 | [train_policy] epoch #565 | Saving snapshot...
2021-06-04 13:55:22 | [train_policy] epoch #565 | Saved
2021-06-04 13:55:22 | [train_policy] epoch #565 | Time 454.26 s
2021-06-04 13:55:22 | [train_policy] epoch #565 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28503
Evaluation/AverageDiscountedReturn          -43.4576
Evaluation/AverageReturn                    -43.4576
Evaluation/CompletionRate                     0
Evaluation/Iteration                        565
Evaluation/MaxReturn                        -32.4947
Evaluation/MinReturn                        -80.06
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.39229
Extras/EpisodeRewardMean                    -43.6321
LinearFeatureBaseline/ExplainedVariance       0.90575
PolicyExecTime                                0.227868
ProcessExecTime                               0.0312781
TotalEnvSteps                            572792
policy/Entropy                               -1.40815
policy/KL                                     0.00888275
policy/KLBefore                               0
policy/LossAfter                             -0.0229324
policy/LossBefore                             1.31931e-08
policy/Perplexity                             0.244595
policy/dLoss                                  0.0229324
---------------------------------------  ----------------
2021-06-04 13:55:22 | [train_policy] epoch #566 | Obtaining samples for iteration 566...
2021-06-04 13:55:23 | [train_policy] epoch #566 | Logging diagnostics...
2021-06-04 13:55:23 | [train_policy] epoch #566 | Optimizing policy...
2021-06-04 13:55:23 | [train_policy] epoch #566 | Computing loss before
2021-06-04 13:55:23 | [train_policy] epoch #566 | Computing KL before
2021-06-04 13:55:23 | [train_policy] epoch #566 | Optimizing
2021-06-04 13:55:23 | [train_policy] epoch #566 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:23 | [train_policy] epoch #566 | computing loss before
2021-06-04 13:55:23 | [train_policy] epoch #566 | computing gradient
2021-06-04 13:55:23 | [train_policy] epoch #566 | gradient computed
2021-06-04 13:55:23 | [train_policy] epoch #566 | computing descent direction
2021-06-04 13:55:23 | [train_policy] epoch #566 | descent direction computed
2021-06-04 13:55:23 | [train_policy] epoch #566 | backtrack iters: 0
2021-06-04 13:55:23 | [train_policy] epoch #566 | optimization finished
2021-06-04 13:55:23 | [train_policy] epoch #566 | Computing KL after
2021-06-04 13:55:23 | [train_policy] epoch #566 | Computing loss after
2021-06-04 13:55:23 | [train_policy] epoch #566 | Fitting baseline...
2021-06-04 13:55:23 | [train_policy] epoch #566 | Saving snapshot...
2021-06-04 13:55:23 | [train_policy] epoch #566 | Saved
2021-06-04 13:55:23 | [train_policy] epoch #566 | Time 455.04 s
2021-06-04 13:55:23 | [train_policy] epoch #566 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284682
Evaluation/AverageDiscountedReturn          -42.1785
Evaluation/AverageReturn                    -42.1785
Evaluation/CompletionRate                     0
Evaluation/Iteration                        566
Evaluation/MaxReturn                        -32.2489
Evaluation/MinReturn                        -79.3228
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.93482
Extras/EpisodeRewardMean                    -42.253
LinearFeatureBaseline/ExplainedVariance       0.890568
PolicyExecTime                                0.21528
ProcessExecTime                               0.031251
TotalEnvSteps                            573804
policy/Entropy                               -1.39586
policy/KL                                     0.00958945
policy/KLBefore                               0
policy/LossAfter                             -0.0171016
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.24762
policy/dLoss                                  0.0171016
---------------------------------------  ----------------
2021-06-04 13:55:23 | [train_policy] epoch #567 | Obtaining samples for iteration 567...
2021-06-04 13:55:23 | [train_policy] epoch #567 | Logging diagnostics...
2021-06-04 13:55:23 | [train_policy] epoch #567 | Optimizing policy...
2021-06-04 13:55:23 | [train_policy] epoch #567 | Computing loss before
2021-06-04 13:55:23 | [train_policy] epoch #567 | Computing KL before
2021-06-04 13:55:23 | [train_policy] epoch #567 | Optimizing
2021-06-04 13:55:23 | [train_policy] epoch #567 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:23 | [train_policy] epoch #567 | computing loss before
2021-06-04 13:55:23 | [train_policy] epoch #567 | computing gradient
2021-06-04 13:55:23 | [train_policy] epoch #567 | gradient computed
2021-06-04 13:55:23 | [train_policy] epoch #567 | computing descent direction
2021-06-04 13:55:24 | [train_policy] epoch #567 | descent direction computed
2021-06-04 13:55:24 | [train_policy] epoch #567 | backtrack iters: 0
2021-06-04 13:55:24 | [train_policy] epoch #567 | optimization finished
2021-06-04 13:55:24 | [train_policy] epoch #567 | Computing KL after
2021-06-04 13:55:24 | [train_policy] epoch #567 | Computing loss after
2021-06-04 13:55:24 | [train_policy] epoch #567 | Fitting baseline...
2021-06-04 13:55:24 | [train_policy] epoch #567 | Saving snapshot...
2021-06-04 13:55:24 | [train_policy] epoch #567 | Saved
2021-06-04 13:55:24 | [train_policy] epoch #567 | Time 455.84 s
2021-06-04 13:55:24 | [train_policy] epoch #567 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287779
Evaluation/AverageDiscountedReturn          -42.1908
Evaluation/AverageReturn                    -42.1908
Evaluation/CompletionRate                     0
Evaluation/Iteration                        567
Evaluation/MaxReturn                        -28.9566
Evaluation/MinReturn                        -78.7831
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77565
Extras/EpisodeRewardMean                    -42.1041
LinearFeatureBaseline/ExplainedVariance       0.890802
PolicyExecTime                                0.234892
ProcessExecTime                               0.0313752
TotalEnvSteps                            574816
policy/Entropy                               -1.41571
policy/KL                                     0.00989812
policy/KLBefore                               0
policy/LossAfter                             -0.0157701
policy/LossBefore                             1.22508e-08
policy/Perplexity                             0.242753
policy/dLoss                                  0.0157701
---------------------------------------  ----------------
2021-06-04 13:55:24 | [train_policy] epoch #568 | Obtaining samples for iteration 568...
2021-06-04 13:55:24 | [train_policy] epoch #568 | Logging diagnostics...
2021-06-04 13:55:24 | [train_policy] epoch #568 | Optimizing policy...
2021-06-04 13:55:24 | [train_policy] epoch #568 | Computing loss before
2021-06-04 13:55:24 | [train_policy] epoch #568 | Computing KL before
2021-06-04 13:55:24 | [train_policy] epoch #568 | Optimizing
2021-06-04 13:55:24 | [train_policy] epoch #568 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:24 | [train_policy] epoch #568 | computing loss before
2021-06-04 13:55:24 | [train_policy] epoch #568 | computing gradient
2021-06-04 13:55:24 | [train_policy] epoch #568 | gradient computed
2021-06-04 13:55:24 | [train_policy] epoch #568 | computing descent direction
2021-06-04 13:55:24 | [train_policy] epoch #568 | descent direction computed
2021-06-04 13:55:24 | [train_policy] epoch #568 | backtrack iters: 1
2021-06-04 13:55:24 | [train_policy] epoch #568 | optimization finished
2021-06-04 13:55:24 | [train_policy] epoch #568 | Computing KL after
2021-06-04 13:55:24 | [train_policy] epoch #568 | Computing loss after
2021-06-04 13:55:24 | [train_policy] epoch #568 | Fitting baseline...
2021-06-04 13:55:24 | [train_policy] epoch #568 | Saving snapshot...
2021-06-04 13:55:24 | [train_policy] epoch #568 | Saved
2021-06-04 13:55:24 | [train_policy] epoch #568 | Time 456.63 s
2021-06-04 13:55:24 | [train_policy] epoch #568 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285811
Evaluation/AverageDiscountedReturn          -42.5036
Evaluation/AverageReturn                    -42.5036
Evaluation/CompletionRate                     0
Evaluation/Iteration                        568
Evaluation/MaxReturn                        -28.0473
Evaluation/MinReturn                        -62.9873
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.96241
Extras/EpisodeRewardMean                    -43.1334
LinearFeatureBaseline/ExplainedVariance       0.900861
PolicyExecTime                                0.215269
ProcessExecTime                               0.031239
TotalEnvSteps                            575828
policy/Entropy                               -1.41713
policy/KL                                     0.00640193
policy/KLBefore                               0
policy/LossAfter                             -0.0179633
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.242409
policy/dLoss                                  0.0179633
---------------------------------------  ----------------
2021-06-04 13:55:24 | [train_policy] epoch #569 | Obtaining samples for iteration 569...
2021-06-04 13:55:25 | [train_policy] epoch #569 | Logging diagnostics...
2021-06-04 13:55:25 | [train_policy] epoch #569 | Optimizing policy...
2021-06-04 13:55:25 | [train_policy] epoch #569 | Computing loss before
2021-06-04 13:55:25 | [train_policy] epoch #569 | Computing KL before
2021-06-04 13:55:25 | [train_policy] epoch #569 | Optimizing
2021-06-04 13:55:25 | [train_policy] epoch #569 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:25 | [train_policy] epoch #569 | computing loss before
2021-06-04 13:55:25 | [train_policy] epoch #569 | computing gradient
2021-06-04 13:55:25 | [train_policy] epoch #569 | gradient computed
2021-06-04 13:55:25 | [train_policy] epoch #569 | computing descent direction
2021-06-04 13:55:25 | [train_policy] epoch #569 | descent direction computed
2021-06-04 13:55:25 | [train_policy] epoch #569 | backtrack iters: 0
2021-06-04 13:55:25 | [train_policy] epoch #569 | optimization finished
2021-06-04 13:55:25 | [train_policy] epoch #569 | Computing KL after
2021-06-04 13:55:25 | [train_policy] epoch #569 | Computing loss after
2021-06-04 13:55:25 | [train_policy] epoch #569 | Fitting baseline...
2021-06-04 13:55:25 | [train_policy] epoch #569 | Saving snapshot...
2021-06-04 13:55:25 | [train_policy] epoch #569 | Saved
2021-06-04 13:55:25 | [train_policy] epoch #569 | Time 457.41 s
2021-06-04 13:55:25 | [train_policy] epoch #569 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285962
Evaluation/AverageDiscountedReturn          -44.6142
Evaluation/AverageReturn                    -44.6142
Evaluation/CompletionRate                     0
Evaluation/Iteration                        569
Evaluation/MaxReturn                        -29.3851
Evaluation/MinReturn                        -83.8903
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.87284
Extras/EpisodeRewardMean                    -44.2315
LinearFeatureBaseline/ExplainedVariance       0.883271
PolicyExecTime                                0.210884
ProcessExecTime                               0.031312
TotalEnvSteps                            576840
policy/Entropy                               -1.41413
policy/KL                                     0.00952704
policy/KLBefore                               0
policy/LossAfter                             -0.019671
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             0.243137
policy/dLoss                                  0.019671
---------------------------------------  ----------------
2021-06-04 13:55:25 | [train_policy] epoch #570 | Obtaining samples for iteration 570...
2021-06-04 13:55:26 | [train_policy] epoch #570 | Logging diagnostics...
2021-06-04 13:55:26 | [train_policy] epoch #570 | Optimizing policy...
2021-06-04 13:55:26 | [train_policy] epoch #570 | Computing loss before
2021-06-04 13:55:26 | [train_policy] epoch #570 | Computing KL before
2021-06-04 13:55:26 | [train_policy] epoch #570 | Optimizing
2021-06-04 13:55:26 | [train_policy] epoch #570 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:26 | [train_policy] epoch #570 | computing loss before
2021-06-04 13:55:26 | [train_policy] epoch #570 | computing gradient
2021-06-04 13:55:26 | [train_policy] epoch #570 | gradient computed
2021-06-04 13:55:26 | [train_policy] epoch #570 | computing descent direction
2021-06-04 13:55:26 | [train_policy] epoch #570 | descent direction computed
2021-06-04 13:55:26 | [train_policy] epoch #570 | backtrack iters: 1
2021-06-04 13:55:26 | [train_policy] epoch #570 | optimization finished
2021-06-04 13:55:26 | [train_policy] epoch #570 | Computing KL after
2021-06-04 13:55:26 | [train_policy] epoch #570 | Computing loss after
2021-06-04 13:55:26 | [train_policy] epoch #570 | Fitting baseline...
2021-06-04 13:55:26 | [train_policy] epoch #570 | Saving snapshot...
2021-06-04 13:55:26 | [train_policy] epoch #570 | Saved
2021-06-04 13:55:26 | [train_policy] epoch #570 | Time 458.20 s
2021-06-04 13:55:26 | [train_policy] epoch #570 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28616
Evaluation/AverageDiscountedReturn          -41.3979
Evaluation/AverageReturn                    -41.3979
Evaluation/CompletionRate                     0
Evaluation/Iteration                        570
Evaluation/MaxReturn                        -29.3419
Evaluation/MinReturn                        -63.3254
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.86499
Extras/EpisodeRewardMean                    -41.9439
LinearFeatureBaseline/ExplainedVariance       0.848989
PolicyExecTime                                0.22395
ProcessExecTime                               0.0312984
TotalEnvSteps                            577852
policy/Entropy                               -1.45523
policy/KL                                     0.00666664
policy/KLBefore                               0
policy/LossAfter                             -0.0172992
policy/LossBefore                             1.28397e-08
policy/Perplexity                             0.233347
policy/dLoss                                  0.0172993
---------------------------------------  ----------------
2021-06-04 13:55:26 | [train_policy] epoch #571 | Obtaining samples for iteration 571...
2021-06-04 13:55:27 | [train_policy] epoch #571 | Logging diagnostics...
2021-06-04 13:55:27 | [train_policy] epoch #571 | Optimizing policy...
2021-06-04 13:55:27 | [train_policy] epoch #571 | Computing loss before
2021-06-04 13:55:27 | [train_policy] epoch #571 | Computing KL before
2021-06-04 13:55:27 | [train_policy] epoch #571 | Optimizing
2021-06-04 13:55:27 | [train_policy] epoch #571 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:27 | [train_policy] epoch #571 | computing loss before
2021-06-04 13:55:27 | [train_policy] epoch #571 | computing gradient
2021-06-04 13:55:27 | [train_policy] epoch #571 | gradient computed
2021-06-04 13:55:27 | [train_policy] epoch #571 | computing descent direction
2021-06-04 13:55:27 | [train_policy] epoch #571 | descent direction computed
2021-06-04 13:55:27 | [train_policy] epoch #571 | backtrack iters: 0
2021-06-04 13:55:27 | [train_policy] epoch #571 | optimization finished
2021-06-04 13:55:27 | [train_policy] epoch #571 | Computing KL after
2021-06-04 13:55:27 | [train_policy] epoch #571 | Computing loss after
2021-06-04 13:55:27 | [train_policy] epoch #571 | Fitting baseline...
2021-06-04 13:55:27 | [train_policy] epoch #571 | Saving snapshot...
2021-06-04 13:55:27 | [train_policy] epoch #571 | Saved
2021-06-04 13:55:27 | [train_policy] epoch #571 | Time 458.97 s
2021-06-04 13:55:27 | [train_policy] epoch #571 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.283832
Evaluation/AverageDiscountedReturn          -41.9008
Evaluation/AverageReturn                    -41.9008
Evaluation/CompletionRate                     0
Evaluation/Iteration                        571
Evaluation/MaxReturn                        -28.4902
Evaluation/MinReturn                        -58.5776
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43602
Extras/EpisodeRewardMean                    -42.1952
LinearFeatureBaseline/ExplainedVariance       0.924243
PolicyExecTime                                0.229582
ProcessExecTime                               0.0314019
TotalEnvSteps                            578864
policy/Entropy                               -1.44274
policy/KL                                     0.00995161
policy/KLBefore                               0
policy/LossAfter                             -0.0243934
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.23628
policy/dLoss                                  0.0243934
---------------------------------------  ----------------
2021-06-04 13:55:27 | [train_policy] epoch #572 | Obtaining samples for iteration 572...
2021-06-04 13:55:27 | [train_policy] epoch #572 | Logging diagnostics...
2021-06-04 13:55:27 | [train_policy] epoch #572 | Optimizing policy...
2021-06-04 13:55:27 | [train_policy] epoch #572 | Computing loss before
2021-06-04 13:55:27 | [train_policy] epoch #572 | Computing KL before
2021-06-04 13:55:27 | [train_policy] epoch #572 | Optimizing
2021-06-04 13:55:27 | [train_policy] epoch #572 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:27 | [train_policy] epoch #572 | computing loss before
2021-06-04 13:55:27 | [train_policy] epoch #572 | computing gradient
2021-06-04 13:55:27 | [train_policy] epoch #572 | gradient computed
2021-06-04 13:55:27 | [train_policy] epoch #572 | computing descent direction
2021-06-04 13:55:27 | [train_policy] epoch #572 | descent direction computed
2021-06-04 13:55:27 | [train_policy] epoch #572 | backtrack iters: 1
2021-06-04 13:55:27 | [train_policy] epoch #572 | optimization finished
2021-06-04 13:55:27 | [train_policy] epoch #572 | Computing KL after
2021-06-04 13:55:27 | [train_policy] epoch #572 | Computing loss after
2021-06-04 13:55:28 | [train_policy] epoch #572 | Fitting baseline...
2021-06-04 13:55:28 | [train_policy] epoch #572 | Saving snapshot...
2021-06-04 13:55:28 | [train_policy] epoch #572 | Saved
2021-06-04 13:55:28 | [train_policy] epoch #572 | Time 459.76 s
2021-06-04 13:55:28 | [train_policy] epoch #572 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283141
Evaluation/AverageDiscountedReturn          -41.1461
Evaluation/AverageReturn                    -41.1461
Evaluation/CompletionRate                     0
Evaluation/Iteration                        572
Evaluation/MaxReturn                        -28.4084
Evaluation/MinReturn                        -64.1532
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.3631
Extras/EpisodeRewardMean                    -41.2925
LinearFeatureBaseline/ExplainedVariance       0.909878
PolicyExecTime                                0.219469
ProcessExecTime                               0.0313385
TotalEnvSteps                            579876
policy/Entropy                               -1.47773
policy/KL                                     0.00650025
policy/KLBefore                               0
policy/LossAfter                             -0.0152423
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.228156
policy/dLoss                                  0.0152423
---------------------------------------  ----------------
2021-06-04 13:55:28 | [train_policy] epoch #573 | Obtaining samples for iteration 573...
2021-06-04 13:55:28 | [train_policy] epoch #573 | Logging diagnostics...
2021-06-04 13:55:28 | [train_policy] epoch #573 | Optimizing policy...
2021-06-04 13:55:28 | [train_policy] epoch #573 | Computing loss before
2021-06-04 13:55:28 | [train_policy] epoch #573 | Computing KL before
2021-06-04 13:55:28 | [train_policy] epoch #573 | Optimizing
2021-06-04 13:55:28 | [train_policy] epoch #573 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:28 | [train_policy] epoch #573 | computing loss before
2021-06-04 13:55:28 | [train_policy] epoch #573 | computing gradient
2021-06-04 13:55:28 | [train_policy] epoch #573 | gradient computed
2021-06-04 13:55:28 | [train_policy] epoch #573 | computing descent direction
2021-06-04 13:55:28 | [train_policy] epoch #573 | descent direction computed
2021-06-04 13:55:28 | [train_policy] epoch #573 | backtrack iters: 0
2021-06-04 13:55:28 | [train_policy] epoch #573 | optimization finished
2021-06-04 13:55:28 | [train_policy] epoch #573 | Computing KL after
2021-06-04 13:55:28 | [train_policy] epoch #573 | Computing loss after
2021-06-04 13:55:28 | [train_policy] epoch #573 | Fitting baseline...
2021-06-04 13:55:28 | [train_policy] epoch #573 | Saving snapshot...
2021-06-04 13:55:28 | [train_policy] epoch #573 | Saved
2021-06-04 13:55:28 | [train_policy] epoch #573 | Time 460.53 s
2021-06-04 13:55:28 | [train_policy] epoch #573 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283198
Evaluation/AverageDiscountedReturn          -41.9625
Evaluation/AverageReturn                    -41.9625
Evaluation/CompletionRate                     0
Evaluation/Iteration                        573
Evaluation/MaxReturn                        -28.7827
Evaluation/MinReturn                        -64.1046
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.56078
Extras/EpisodeRewardMean                    -41.7167
LinearFeatureBaseline/ExplainedVariance       0.908637
PolicyExecTime                                0.227292
ProcessExecTime                               0.0311968
TotalEnvSteps                            580888
policy/Entropy                               -1.44424
policy/KL                                     0.00956263
policy/KLBefore                               0
policy/LossAfter                             -0.0126184
policy/LossBefore                            -1.17796e-10
policy/Perplexity                             0.235926
policy/dLoss                                  0.0126184
---------------------------------------  ----------------
2021-06-04 13:55:28 | [train_policy] epoch #574 | Obtaining samples for iteration 574...
2021-06-04 13:55:29 | [train_policy] epoch #574 | Logging diagnostics...
2021-06-04 13:55:29 | [train_policy] epoch #574 | Optimizing policy...
2021-06-04 13:55:29 | [train_policy] epoch #574 | Computing loss before
2021-06-04 13:55:29 | [train_policy] epoch #574 | Computing KL before
2021-06-04 13:55:29 | [train_policy] epoch #574 | Optimizing
2021-06-04 13:55:29 | [train_policy] epoch #574 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:29 | [train_policy] epoch #574 | computing loss before
2021-06-04 13:55:29 | [train_policy] epoch #574 | computing gradient
2021-06-04 13:55:29 | [train_policy] epoch #574 | gradient computed
2021-06-04 13:55:29 | [train_policy] epoch #574 | computing descent direction
2021-06-04 13:55:29 | [train_policy] epoch #574 | descent direction computed
2021-06-04 13:55:29 | [train_policy] epoch #574 | backtrack iters: 0
2021-06-04 13:55:29 | [train_policy] epoch #574 | optimization finished
2021-06-04 13:55:29 | [train_policy] epoch #574 | Computing KL after
2021-06-04 13:55:29 | [train_policy] epoch #574 | Computing loss after
2021-06-04 13:55:29 | [train_policy] epoch #574 | Fitting baseline...
2021-06-04 13:55:29 | [train_policy] epoch #574 | Saving snapshot...
2021-06-04 13:55:29 | [train_policy] epoch #574 | Saved
2021-06-04 13:55:29 | [train_policy] epoch #574 | Time 461.33 s
2021-06-04 13:55:29 | [train_policy] epoch #574 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284004
Evaluation/AverageDiscountedReturn          -40.4968
Evaluation/AverageReturn                    -40.4968
Evaluation/CompletionRate                     0
Evaluation/Iteration                        574
Evaluation/MaxReturn                        -28.8977
Evaluation/MinReturn                        -58.4145
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.0999
Extras/EpisodeRewardMean                    -40.5047
LinearFeatureBaseline/ExplainedVariance       0.916318
PolicyExecTime                                0.235462
ProcessExecTime                               0.0313885
TotalEnvSteps                            581900
policy/Entropy                               -1.36791
policy/KL                                     0.00949358
policy/KLBefore                               0
policy/LossAfter                             -0.0194669
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             0.254639
policy/dLoss                                  0.0194669
---------------------------------------  ----------------
2021-06-04 13:55:29 | [train_policy] epoch #575 | Obtaining samples for iteration 575...
2021-06-04 13:55:30 | [train_policy] epoch #575 | Logging diagnostics...
2021-06-04 13:55:30 | [train_policy] epoch #575 | Optimizing policy...
2021-06-04 13:55:30 | [train_policy] epoch #575 | Computing loss before
2021-06-04 13:55:30 | [train_policy] epoch #575 | Computing KL before
2021-06-04 13:55:30 | [train_policy] epoch #575 | Optimizing
2021-06-04 13:55:30 | [train_policy] epoch #575 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:30 | [train_policy] epoch #575 | computing loss before
2021-06-04 13:55:30 | [train_policy] epoch #575 | computing gradient
2021-06-04 13:55:30 | [train_policy] epoch #575 | gradient computed
2021-06-04 13:55:30 | [train_policy] epoch #575 | computing descent direction
2021-06-04 13:55:30 | [train_policy] epoch #575 | descent direction computed
2021-06-04 13:55:30 | [train_policy] epoch #575 | backtrack iters: 0
2021-06-04 13:55:30 | [train_policy] epoch #575 | optimization finished
2021-06-04 13:55:30 | [train_policy] epoch #575 | Computing KL after
2021-06-04 13:55:30 | [train_policy] epoch #575 | Computing loss after
2021-06-04 13:55:30 | [train_policy] epoch #575 | Fitting baseline...
2021-06-04 13:55:30 | [train_policy] epoch #575 | Saving snapshot...
2021-06-04 13:55:30 | [train_policy] epoch #575 | Saved
2021-06-04 13:55:30 | [train_policy] epoch #575 | Time 462.16 s
2021-06-04 13:55:30 | [train_policy] epoch #575 | EpochTime 0.81 s
---------------------------------------  ----------------
EnvExecTime                                   0.301754
Evaluation/AverageDiscountedReturn          -41.6616
Evaluation/AverageReturn                    -41.6616
Evaluation/CompletionRate                     0
Evaluation/Iteration                        575
Evaluation/MaxReturn                        -29.0036
Evaluation/MinReturn                        -81.8585
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77879
Extras/EpisodeRewardMean                    -41.5061
LinearFeatureBaseline/ExplainedVariance       0.817756
PolicyExecTime                                0.252315
ProcessExecTime                               0.0333362
TotalEnvSteps                            582912
policy/Entropy                               -1.32782
policy/KL                                     0.00970529
policy/KLBefore                               0
policy/LossAfter                             -0.0204197
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             0.265055
policy/dLoss                                  0.0204197
---------------------------------------  ----------------
2021-06-04 13:55:30 | [train_policy] epoch #576 | Obtaining samples for iteration 576...
2021-06-04 13:55:31 | [train_policy] epoch #576 | Logging diagnostics...
2021-06-04 13:55:31 | [train_policy] epoch #576 | Optimizing policy...
2021-06-04 13:55:31 | [train_policy] epoch #576 | Computing loss before
2021-06-04 13:55:31 | [train_policy] epoch #576 | Computing KL before
2021-06-04 13:55:31 | [train_policy] epoch #576 | Optimizing
2021-06-04 13:55:31 | [train_policy] epoch #576 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:31 | [train_policy] epoch #576 | computing loss before
2021-06-04 13:55:31 | [train_policy] epoch #576 | computing gradient
2021-06-04 13:55:31 | [train_policy] epoch #576 | gradient computed
2021-06-04 13:55:31 | [train_policy] epoch #576 | computing descent direction
2021-06-04 13:55:31 | [train_policy] epoch #576 | descent direction computed
2021-06-04 13:55:31 | [train_policy] epoch #576 | backtrack iters: 1
2021-06-04 13:55:31 | [train_policy] epoch #576 | optimization finished
2021-06-04 13:55:31 | [train_policy] epoch #576 | Computing KL after
2021-06-04 13:55:31 | [train_policy] epoch #576 | Computing loss after
2021-06-04 13:55:31 | [train_policy] epoch #576 | Fitting baseline...
2021-06-04 13:55:31 | [train_policy] epoch #576 | Saving snapshot...
2021-06-04 13:55:31 | [train_policy] epoch #576 | Saved
2021-06-04 13:55:31 | [train_policy] epoch #576 | Time 462.95 s
2021-06-04 13:55:31 | [train_policy] epoch #576 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283401
Evaluation/AverageDiscountedReturn          -42.4044
Evaluation/AverageReturn                    -42.4044
Evaluation/CompletionRate                     0
Evaluation/Iteration                        576
Evaluation/MaxReturn                        -29.187
Evaluation/MinReturn                        -80.8093
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.59286
Extras/EpisodeRewardMean                    -42.2537
LinearFeatureBaseline/ExplainedVariance       0.875239
PolicyExecTime                                0.233618
ProcessExecTime                               0.0313201
TotalEnvSteps                            583924
policy/Entropy                               -1.37154
policy/KL                                     0.00663785
policy/KLBefore                               0
policy/LossAfter                             -0.0140143
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.253716
policy/dLoss                                  0.0140143
---------------------------------------  ----------------
2021-06-04 13:55:31 | [train_policy] epoch #577 | Obtaining samples for iteration 577...
2021-06-04 13:55:31 | [train_policy] epoch #577 | Logging diagnostics...
2021-06-04 13:55:31 | [train_policy] epoch #577 | Optimizing policy...
2021-06-04 13:55:31 | [train_policy] epoch #577 | Computing loss before
2021-06-04 13:55:31 | [train_policy] epoch #577 | Computing KL before
2021-06-04 13:55:31 | [train_policy] epoch #577 | Optimizing
2021-06-04 13:55:31 | [train_policy] epoch #577 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:31 | [train_policy] epoch #577 | computing loss before
2021-06-04 13:55:31 | [train_policy] epoch #577 | computing gradient
2021-06-04 13:55:31 | [train_policy] epoch #577 | gradient computed
2021-06-04 13:55:31 | [train_policy] epoch #577 | computing descent direction
2021-06-04 13:55:31 | [train_policy] epoch #577 | descent direction computed
2021-06-04 13:55:31 | [train_policy] epoch #577 | backtrack iters: 1
2021-06-04 13:55:31 | [train_policy] epoch #577 | optimization finished
2021-06-04 13:55:31 | [train_policy] epoch #577 | Computing KL after
2021-06-04 13:55:31 | [train_policy] epoch #577 | Computing loss after
2021-06-04 13:55:31 | [train_policy] epoch #577 | Fitting baseline...
2021-06-04 13:55:32 | [train_policy] epoch #577 | Saving snapshot...
2021-06-04 13:55:32 | [train_policy] epoch #577 | Saved
2021-06-04 13:55:32 | [train_policy] epoch #577 | Time 463.74 s
2021-06-04 13:55:32 | [train_policy] epoch #577 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283035
Evaluation/AverageDiscountedReturn          -42.4367
Evaluation/AverageReturn                    -42.4367
Evaluation/CompletionRate                     0
Evaluation/Iteration                        577
Evaluation/MaxReturn                        -29.5963
Evaluation/MinReturn                        -81.1875
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.61564
Extras/EpisodeRewardMean                    -42.3868
LinearFeatureBaseline/ExplainedVariance       0.889519
PolicyExecTime                                0.230442
ProcessExecTime                               0.0313401
TotalEnvSteps                            584936
policy/Entropy                               -1.38281
policy/KL                                     0.00654797
policy/KLBefore                               0
policy/LossAfter                             -0.0163699
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.250872
policy/dLoss                                  0.0163699
---------------------------------------  ----------------
2021-06-04 13:55:32 | [train_policy] epoch #578 | Obtaining samples for iteration 578...
2021-06-04 13:55:32 | [train_policy] epoch #578 | Logging diagnostics...
2021-06-04 13:55:32 | [train_policy] epoch #578 | Optimizing policy...
2021-06-04 13:55:32 | [train_policy] epoch #578 | Computing loss before
2021-06-04 13:55:32 | [train_policy] epoch #578 | Computing KL before
2021-06-04 13:55:32 | [train_policy] epoch #578 | Optimizing
2021-06-04 13:55:32 | [train_policy] epoch #578 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:32 | [train_policy] epoch #578 | computing loss before
2021-06-04 13:55:32 | [train_policy] epoch #578 | computing gradient
2021-06-04 13:55:32 | [train_policy] epoch #578 | gradient computed
2021-06-04 13:55:32 | [train_policy] epoch #578 | computing descent direction
2021-06-04 13:55:32 | [train_policy] epoch #578 | descent direction computed
2021-06-04 13:55:32 | [train_policy] epoch #578 | backtrack iters: 0
2021-06-04 13:55:32 | [train_policy] epoch #578 | optimization finished
2021-06-04 13:55:32 | [train_policy] epoch #578 | Computing KL after
2021-06-04 13:55:32 | [train_policy] epoch #578 | Computing loss after
2021-06-04 13:55:32 | [train_policy] epoch #578 | Fitting baseline...
2021-06-04 13:55:32 | [train_policy] epoch #578 | Saving snapshot...
2021-06-04 13:55:32 | [train_policy] epoch #578 | Saved
2021-06-04 13:55:32 | [train_policy] epoch #578 | Time 464.51 s
2021-06-04 13:55:32 | [train_policy] epoch #578 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283115
Evaluation/AverageDiscountedReturn          -41.5678
Evaluation/AverageReturn                    -41.5678
Evaluation/CompletionRate                     0
Evaluation/Iteration                        578
Evaluation/MaxReturn                        -29.4195
Evaluation/MinReturn                        -64.0565
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5584
Extras/EpisodeRewardMean                    -41.6155
LinearFeatureBaseline/ExplainedVariance       0.912269
PolicyExecTime                                0.229722
ProcessExecTime                               0.0313315
TotalEnvSteps                            585948
policy/Entropy                               -1.39592
policy/KL                                     0.00945321
policy/KLBefore                               0
policy/LossAfter                             -0.0245879
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.247605
policy/dLoss                                  0.0245879
---------------------------------------  ----------------
2021-06-04 13:55:32 | [train_policy] epoch #579 | Obtaining samples for iteration 579...
2021-06-04 13:55:33 | [train_policy] epoch #579 | Logging diagnostics...
2021-06-04 13:55:33 | [train_policy] epoch #579 | Optimizing policy...
2021-06-04 13:55:33 | [train_policy] epoch #579 | Computing loss before
2021-06-04 13:55:33 | [train_policy] epoch #579 | Computing KL before
2021-06-04 13:55:33 | [train_policy] epoch #579 | Optimizing
2021-06-04 13:55:33 | [train_policy] epoch #579 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:33 | [train_policy] epoch #579 | computing loss before
2021-06-04 13:55:33 | [train_policy] epoch #579 | computing gradient
2021-06-04 13:55:33 | [train_policy] epoch #579 | gradient computed
2021-06-04 13:55:33 | [train_policy] epoch #579 | computing descent direction
2021-06-04 13:55:33 | [train_policy] epoch #579 | descent direction computed
2021-06-04 13:55:33 | [train_policy] epoch #579 | backtrack iters: 0
2021-06-04 13:55:33 | [train_policy] epoch #579 | optimization finished
2021-06-04 13:55:33 | [train_policy] epoch #579 | Computing KL after
2021-06-04 13:55:33 | [train_policy] epoch #579 | Computing loss after
2021-06-04 13:55:33 | [train_policy] epoch #579 | Fitting baseline...
2021-06-04 13:55:33 | [train_policy] epoch #579 | Saving snapshot...
2021-06-04 13:55:33 | [train_policy] epoch #579 | Saved
2021-06-04 13:55:33 | [train_policy] epoch #579 | Time 465.30 s
2021-06-04 13:55:33 | [train_policy] epoch #579 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28309
Evaluation/AverageDiscountedReturn          -41.9171
Evaluation/AverageReturn                    -41.9171
Evaluation/CompletionRate                     0
Evaluation/Iteration                        579
Evaluation/MaxReturn                        -28.1843
Evaluation/MinReturn                        -64.1393
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.9674
Extras/EpisodeRewardMean                    -42.0296
LinearFeatureBaseline/ExplainedVariance       0.908325
PolicyExecTime                                0.233987
ProcessExecTime                               0.0312834
TotalEnvSteps                            586960
policy/Entropy                               -1.3894
policy/KL                                     0.00936517
policy/KLBefore                               0
policy/LossAfter                             -0.0189664
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.249225
policy/dLoss                                  0.0189664
---------------------------------------  ----------------
2021-06-04 13:55:33 | [train_policy] epoch #580 | Obtaining samples for iteration 580...
2021-06-04 13:55:34 | [train_policy] epoch #580 | Logging diagnostics...
2021-06-04 13:55:34 | [train_policy] epoch #580 | Optimizing policy...
2021-06-04 13:55:34 | [train_policy] epoch #580 | Computing loss before
2021-06-04 13:55:34 | [train_policy] epoch #580 | Computing KL before
2021-06-04 13:55:34 | [train_policy] epoch #580 | Optimizing
2021-06-04 13:55:34 | [train_policy] epoch #580 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:34 | [train_policy] epoch #580 | computing loss before
2021-06-04 13:55:34 | [train_policy] epoch #580 | computing gradient
2021-06-04 13:55:34 | [train_policy] epoch #580 | gradient computed
2021-06-04 13:55:34 | [train_policy] epoch #580 | computing descent direction
2021-06-04 13:55:34 | [train_policy] epoch #580 | descent direction computed
2021-06-04 13:55:34 | [train_policy] epoch #580 | backtrack iters: 1
2021-06-04 13:55:34 | [train_policy] epoch #580 | optimization finished
2021-06-04 13:55:34 | [train_policy] epoch #580 | Computing KL after
2021-06-04 13:55:34 | [train_policy] epoch #580 | Computing loss after
2021-06-04 13:55:34 | [train_policy] epoch #580 | Fitting baseline...
2021-06-04 13:55:34 | [train_policy] epoch #580 | Saving snapshot...
2021-06-04 13:55:34 | [train_policy] epoch #580 | Saved
2021-06-04 13:55:34 | [train_policy] epoch #580 | Time 466.07 s
2021-06-04 13:55:34 | [train_policy] epoch #580 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                   0.283431
Evaluation/AverageDiscountedReturn          -43.2661
Evaluation/AverageReturn                    -43.2661
Evaluation/CompletionRate                     0
Evaluation/Iteration                        580
Evaluation/MaxReturn                        -28.5511
Evaluation/MinReturn                        -79.2169
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.28267
Extras/EpisodeRewardMean                    -42.7635
LinearFeatureBaseline/ExplainedVariance       0.885947
PolicyExecTime                                0.227934
ProcessExecTime                               0.0313153
TotalEnvSteps                            587972
policy/Entropy                               -1.41289
policy/KL                                     0.00664887
policy/KLBefore                               0
policy/LossAfter                             -0.0172621
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.243438
policy/dLoss                                  0.0172621
---------------------------------------  ---------------
2021-06-04 13:55:34 | [train_policy] epoch #581 | Obtaining samples for iteration 581...
2021-06-04 13:55:34 | [train_policy] epoch #581 | Logging diagnostics...
2021-06-04 13:55:34 | [train_policy] epoch #581 | Optimizing policy...
2021-06-04 13:55:34 | [train_policy] epoch #581 | Computing loss before
2021-06-04 13:55:34 | [train_policy] epoch #581 | Computing KL before
2021-06-04 13:55:35 | [train_policy] epoch #581 | Optimizing
2021-06-04 13:55:35 | [train_policy] epoch #581 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:35 | [train_policy] epoch #581 | computing loss before
2021-06-04 13:55:35 | [train_policy] epoch #581 | computing gradient
2021-06-04 13:55:35 | [train_policy] epoch #581 | gradient computed
2021-06-04 13:55:35 | [train_policy] epoch #581 | computing descent direction
2021-06-04 13:55:35 | [train_policy] epoch #581 | descent direction computed
2021-06-04 13:55:35 | [train_policy] epoch #581 | backtrack iters: 1
2021-06-04 13:55:35 | [train_policy] epoch #581 | optimization finished
2021-06-04 13:55:35 | [train_policy] epoch #581 | Computing KL after
2021-06-04 13:55:35 | [train_policy] epoch #581 | Computing loss after
2021-06-04 13:55:35 | [train_policy] epoch #581 | Fitting baseline...
2021-06-04 13:55:35 | [train_policy] epoch #581 | Saving snapshot...
2021-06-04 13:55:35 | [train_policy] epoch #581 | Saved
2021-06-04 13:55:35 | [train_policy] epoch #581 | Time 466.86 s
2021-06-04 13:55:35 | [train_policy] epoch #581 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28384
Evaluation/AverageDiscountedReturn          -42.7908
Evaluation/AverageReturn                    -42.7908
Evaluation/CompletionRate                     0
Evaluation/Iteration                        581
Evaluation/MaxReturn                        -29.2988
Evaluation/MinReturn                        -64.0325
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.36948
Extras/EpisodeRewardMean                    -42.5286
LinearFeatureBaseline/ExplainedVariance       0.891638
PolicyExecTime                                0.230669
ProcessExecTime                               0.0314293
TotalEnvSteps                            588984
policy/Entropy                               -1.42939
policy/KL                                     0.00651101
policy/KLBefore                               0
policy/LossAfter                             -0.0141046
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.239455
policy/dLoss                                  0.0141046
---------------------------------------  ----------------
2021-06-04 13:55:35 | [train_policy] epoch #582 | Obtaining samples for iteration 582...
2021-06-04 13:55:35 | [train_policy] epoch #582 | Logging diagnostics...
2021-06-04 13:55:35 | [train_policy] epoch #582 | Optimizing policy...
2021-06-04 13:55:35 | [train_policy] epoch #582 | Computing loss before
2021-06-04 13:55:35 | [train_policy] epoch #582 | Computing KL before
2021-06-04 13:55:35 | [train_policy] epoch #582 | Optimizing
2021-06-04 13:55:35 | [train_policy] epoch #582 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:35 | [train_policy] epoch #582 | computing loss before
2021-06-04 13:55:35 | [train_policy] epoch #582 | computing gradient
2021-06-04 13:55:35 | [train_policy] epoch #582 | gradient computed
2021-06-04 13:55:35 | [train_policy] epoch #582 | computing descent direction
2021-06-04 13:55:35 | [train_policy] epoch #582 | descent direction computed
2021-06-04 13:55:35 | [train_policy] epoch #582 | backtrack iters: 1
2021-06-04 13:55:35 | [train_policy] epoch #582 | optimization finished
2021-06-04 13:55:35 | [train_policy] epoch #582 | Computing KL after
2021-06-04 13:55:35 | [train_policy] epoch #582 | Computing loss after
2021-06-04 13:55:35 | [train_policy] epoch #582 | Fitting baseline...
2021-06-04 13:55:35 | [train_policy] epoch #582 | Saving snapshot...
2021-06-04 13:55:35 | [train_policy] epoch #582 | Saved
2021-06-04 13:55:35 | [train_policy] epoch #582 | Time 467.65 s
2021-06-04 13:55:35 | [train_policy] epoch #582 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283192
Evaluation/AverageDiscountedReturn          -43.3125
Evaluation/AverageReturn                    -43.3125
Evaluation/CompletionRate                     0
Evaluation/Iteration                        582
Evaluation/MaxReturn                        -32.8154
Evaluation/MinReturn                        -79.2539
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.5863
Extras/EpisodeRewardMean                    -43.5087
LinearFeatureBaseline/ExplainedVariance       0.857426
PolicyExecTime                                0.235409
ProcessExecTime                               0.0312901
TotalEnvSteps                            589996
policy/Entropy                               -1.45818
policy/KL                                     0.00643887
policy/KLBefore                               0
policy/LossAfter                             -0.01779
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.232659
policy/dLoss                                  0.01779
---------------------------------------  ----------------
2021-06-04 13:55:35 | [train_policy] epoch #583 | Obtaining samples for iteration 583...
2021-06-04 13:55:36 | [train_policy] epoch #583 | Logging diagnostics...
2021-06-04 13:55:36 | [train_policy] epoch #583 | Optimizing policy...
2021-06-04 13:55:36 | [train_policy] epoch #583 | Computing loss before
2021-06-04 13:55:36 | [train_policy] epoch #583 | Computing KL before
2021-06-04 13:55:36 | [train_policy] epoch #583 | Optimizing
2021-06-04 13:55:36 | [train_policy] epoch #583 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:36 | [train_policy] epoch #583 | computing loss before
2021-06-04 13:55:36 | [train_policy] epoch #583 | computing gradient
2021-06-04 13:55:36 | [train_policy] epoch #583 | gradient computed
2021-06-04 13:55:36 | [train_policy] epoch #583 | computing descent direction
2021-06-04 13:55:36 | [train_policy] epoch #583 | descent direction computed
2021-06-04 13:55:36 | [train_policy] epoch #583 | backtrack iters: 1
2021-06-04 13:55:36 | [train_policy] epoch #583 | optimization finished
2021-06-04 13:55:36 | [train_policy] epoch #583 | Computing KL after
2021-06-04 13:55:36 | [train_policy] epoch #583 | Computing loss after
2021-06-04 13:55:36 | [train_policy] epoch #583 | Fitting baseline...
2021-06-04 13:55:36 | [train_policy] epoch #583 | Saving snapshot...
2021-06-04 13:55:36 | [train_policy] epoch #583 | Saved
2021-06-04 13:55:36 | [train_policy] epoch #583 | Time 468.44 s
2021-06-04 13:55:36 | [train_policy] epoch #583 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283127
Evaluation/AverageDiscountedReturn          -41.9001
Evaluation/AverageReturn                    -41.9001
Evaluation/CompletionRate                     0
Evaluation/Iteration                        583
Evaluation/MaxReturn                        -30.5676
Evaluation/MinReturn                        -81.4235
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.96467
Extras/EpisodeRewardMean                    -42.1138
LinearFeatureBaseline/ExplainedVariance       0.87555
PolicyExecTime                                0.224907
ProcessExecTime                               0.0313094
TotalEnvSteps                            591008
policy/Entropy                               -1.43151
policy/KL                                     0.00656776
policy/KLBefore                               0
policy/LossAfter                             -0.0117641
policy/LossBefore                             3.53387e-09
policy/Perplexity                             0.238947
policy/dLoss                                  0.0117641
---------------------------------------  ----------------
2021-06-04 13:55:36 | [train_policy] epoch #584 | Obtaining samples for iteration 584...
2021-06-04 13:55:37 | [train_policy] epoch #584 | Logging diagnostics...
2021-06-04 13:55:37 | [train_policy] epoch #584 | Optimizing policy...
2021-06-04 13:55:37 | [train_policy] epoch #584 | Computing loss before
2021-06-04 13:55:37 | [train_policy] epoch #584 | Computing KL before
2021-06-04 13:55:37 | [train_policy] epoch #584 | Optimizing
2021-06-04 13:55:37 | [train_policy] epoch #584 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:37 | [train_policy] epoch #584 | computing loss before
2021-06-04 13:55:37 | [train_policy] epoch #584 | computing gradient
2021-06-04 13:55:37 | [train_policy] epoch #584 | gradient computed
2021-06-04 13:55:37 | [train_policy] epoch #584 | computing descent direction
2021-06-04 13:55:37 | [train_policy] epoch #584 | descent direction computed
2021-06-04 13:55:37 | [train_policy] epoch #584 | backtrack iters: 0
2021-06-04 13:55:37 | [train_policy] epoch #584 | optimization finished
2021-06-04 13:55:37 | [train_policy] epoch #584 | Computing KL after
2021-06-04 13:55:37 | [train_policy] epoch #584 | Computing loss after
2021-06-04 13:55:37 | [train_policy] epoch #584 | Fitting baseline...
2021-06-04 13:55:37 | [train_policy] epoch #584 | Saving snapshot...
2021-06-04 13:55:37 | [train_policy] epoch #584 | Saved
2021-06-04 13:55:37 | [train_policy] epoch #584 | Time 469.23 s
2021-06-04 13:55:37 | [train_policy] epoch #584 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286912
Evaluation/AverageDiscountedReturn          -43.2462
Evaluation/AverageReturn                    -43.2462
Evaluation/CompletionRate                     0
Evaluation/Iteration                        584
Evaluation/MaxReturn                        -28.8151
Evaluation/MinReturn                        -79.9481
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.15579
Extras/EpisodeRewardMean                    -42.7959
LinearFeatureBaseline/ExplainedVariance       0.885485
PolicyExecTime                                0.230644
ProcessExecTime                               0.0317912
TotalEnvSteps                            592020
policy/Entropy                               -1.44458
policy/KL                                     0.00973987
policy/KLBefore                               0
policy/LossAfter                             -0.0149641
policy/LossBefore                            -2.16744e-08
policy/Perplexity                             0.235845
policy/dLoss                                  0.0149641
---------------------------------------  ----------------
2021-06-04 13:55:37 | [train_policy] epoch #585 | Obtaining samples for iteration 585...
2021-06-04 13:55:38 | [train_policy] epoch #585 | Logging diagnostics...
2021-06-04 13:55:38 | [train_policy] epoch #585 | Optimizing policy...
2021-06-04 13:55:38 | [train_policy] epoch #585 | Computing loss before
2021-06-04 13:55:38 | [train_policy] epoch #585 | Computing KL before
2021-06-04 13:55:38 | [train_policy] epoch #585 | Optimizing
2021-06-04 13:55:38 | [train_policy] epoch #585 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:38 | [train_policy] epoch #585 | computing loss before
2021-06-04 13:55:38 | [train_policy] epoch #585 | computing gradient
2021-06-04 13:55:38 | [train_policy] epoch #585 | gradient computed
2021-06-04 13:55:38 | [train_policy] epoch #585 | computing descent direction
2021-06-04 13:55:38 | [train_policy] epoch #585 | descent direction computed
2021-06-04 13:55:38 | [train_policy] epoch #585 | backtrack iters: 0
2021-06-04 13:55:38 | [train_policy] epoch #585 | optimization finished
2021-06-04 13:55:38 | [train_policy] epoch #585 | Computing KL after
2021-06-04 13:55:38 | [train_policy] epoch #585 | Computing loss after
2021-06-04 13:55:38 | [train_policy] epoch #585 | Fitting baseline...
2021-06-04 13:55:38 | [train_policy] epoch #585 | Saving snapshot...
2021-06-04 13:55:38 | [train_policy] epoch #585 | Saved
2021-06-04 13:55:38 | [train_policy] epoch #585 | Time 470.02 s
2021-06-04 13:55:38 | [train_policy] epoch #585 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.28441
Evaluation/AverageDiscountedReturn          -41.8317
Evaluation/AverageReturn                    -41.8317
Evaluation/CompletionRate                     0
Evaluation/Iteration                        585
Evaluation/MaxReturn                        -29.2499
Evaluation/MinReturn                        -77.7986
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.54763
Extras/EpisodeRewardMean                    -42.4581
LinearFeatureBaseline/ExplainedVariance       0.871314
PolicyExecTime                                0.227838
ProcessExecTime                               0.0313346
TotalEnvSteps                            593032
policy/Entropy                               -1.43268
policy/KL                                     0.00978635
policy/KLBefore                               0
policy/LossAfter                             -0.0239907
policy/LossBefore                             1.5549e-08
policy/Perplexity                             0.238669
policy/dLoss                                  0.0239907
---------------------------------------  ---------------
2021-06-04 13:55:38 | [train_policy] epoch #586 | Obtaining samples for iteration 586...
2021-06-04 13:55:38 | [train_policy] epoch #586 | Logging diagnostics...
2021-06-04 13:55:38 | [train_policy] epoch #586 | Optimizing policy...
2021-06-04 13:55:38 | [train_policy] epoch #586 | Computing loss before
2021-06-04 13:55:38 | [train_policy] epoch #586 | Computing KL before
2021-06-04 13:55:38 | [train_policy] epoch #586 | Optimizing
2021-06-04 13:55:38 | [train_policy] epoch #586 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:38 | [train_policy] epoch #586 | computing loss before
2021-06-04 13:55:38 | [train_policy] epoch #586 | computing gradient
2021-06-04 13:55:38 | [train_policy] epoch #586 | gradient computed
2021-06-04 13:55:38 | [train_policy] epoch #586 | computing descent direction
2021-06-04 13:55:39 | [train_policy] epoch #586 | descent direction computed
2021-06-04 13:55:39 | [train_policy] epoch #586 | backtrack iters: 0
2021-06-04 13:55:39 | [train_policy] epoch #586 | optimization finished
2021-06-04 13:55:39 | [train_policy] epoch #586 | Computing KL after
2021-06-04 13:55:39 | [train_policy] epoch #586 | Computing loss after
2021-06-04 13:55:39 | [train_policy] epoch #586 | Fitting baseline...
2021-06-04 13:55:39 | [train_policy] epoch #586 | Saving snapshot...
2021-06-04 13:55:39 | [train_policy] epoch #586 | Saved
2021-06-04 13:55:39 | [train_policy] epoch #586 | Time 470.80 s
2021-06-04 13:55:39 | [train_policy] epoch #586 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283556
Evaluation/AverageDiscountedReturn          -42.1511
Evaluation/AverageReturn                    -42.1511
Evaluation/CompletionRate                     0
Evaluation/Iteration                        586
Evaluation/MaxReturn                        -29.5666
Evaluation/MinReturn                        -78.6566
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.35895
Extras/EpisodeRewardMean                    -41.9855
LinearFeatureBaseline/ExplainedVariance       0.821119
PolicyExecTime                                0.231659
ProcessExecTime                               0.0312829
TotalEnvSteps                            594044
policy/Entropy                               -1.40856
policy/KL                                     0.00984963
policy/KLBefore                               0
policy/LossAfter                             -0.0189266
policy/LossBefore                            -8.01011e-09
policy/Perplexity                             0.244495
policy/dLoss                                  0.0189266
---------------------------------------  ----------------
2021-06-04 13:55:39 | [train_policy] epoch #587 | Obtaining samples for iteration 587...
2021-06-04 13:55:39 | [train_policy] epoch #587 | Logging diagnostics...
2021-06-04 13:55:39 | [train_policy] epoch #587 | Optimizing policy...
2021-06-04 13:55:39 | [train_policy] epoch #587 | Computing loss before
2021-06-04 13:55:39 | [train_policy] epoch #587 | Computing KL before
2021-06-04 13:55:39 | [train_policy] epoch #587 | Optimizing
2021-06-04 13:55:39 | [train_policy] epoch #587 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:39 | [train_policy] epoch #587 | computing loss before
2021-06-04 13:55:39 | [train_policy] epoch #587 | computing gradient
2021-06-04 13:55:39 | [train_policy] epoch #587 | gradient computed
2021-06-04 13:55:39 | [train_policy] epoch #587 | computing descent direction
2021-06-04 13:55:39 | [train_policy] epoch #587 | descent direction computed
2021-06-04 13:55:39 | [train_policy] epoch #587 | backtrack iters: 0
2021-06-04 13:55:39 | [train_policy] epoch #587 | optimization finished
2021-06-04 13:55:39 | [train_policy] epoch #587 | Computing KL after
2021-06-04 13:55:39 | [train_policy] epoch #587 | Computing loss after
2021-06-04 13:55:39 | [train_policy] epoch #587 | Fitting baseline...
2021-06-04 13:55:39 | [train_policy] epoch #587 | Saving snapshot...
2021-06-04 13:55:39 | [train_policy] epoch #587 | Saved
2021-06-04 13:55:39 | [train_policy] epoch #587 | Time 471.59 s
2021-06-04 13:55:39 | [train_policy] epoch #587 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283854
Evaluation/AverageDiscountedReturn          -43.2627
Evaluation/AverageReturn                    -43.2627
Evaluation/CompletionRate                     0
Evaluation/Iteration                        587
Evaluation/MaxReturn                        -29.8917
Evaluation/MinReturn                        -64.1884
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5668
Extras/EpisodeRewardMean                    -43.2965
LinearFeatureBaseline/ExplainedVariance       0.909988
PolicyExecTime                                0.220918
ProcessExecTime                               0.0315642
TotalEnvSteps                            595056
policy/Entropy                               -1.39147
policy/KL                                     0.00987215
policy/KLBefore                               0
policy/LossAfter                             -0.0165259
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.24871
policy/dLoss                                  0.0165259
---------------------------------------  ----------------
2021-06-04 13:55:39 | [train_policy] epoch #588 | Obtaining samples for iteration 588...
2021-06-04 13:55:40 | [train_policy] epoch #588 | Logging diagnostics...
2021-06-04 13:55:40 | [train_policy] epoch #588 | Optimizing policy...
2021-06-04 13:55:40 | [train_policy] epoch #588 | Computing loss before
2021-06-04 13:55:40 | [train_policy] epoch #588 | Computing KL before
2021-06-04 13:55:40 | [train_policy] epoch #588 | Optimizing
2021-06-04 13:55:40 | [train_policy] epoch #588 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:40 | [train_policy] epoch #588 | computing loss before
2021-06-04 13:55:40 | [train_policy] epoch #588 | computing gradient
2021-06-04 13:55:40 | [train_policy] epoch #588 | gradient computed
2021-06-04 13:55:40 | [train_policy] epoch #588 | computing descent direction
2021-06-04 13:55:40 | [train_policy] epoch #588 | descent direction computed
2021-06-04 13:55:40 | [train_policy] epoch #588 | backtrack iters: 1
2021-06-04 13:55:40 | [train_policy] epoch #588 | optimization finished
2021-06-04 13:55:40 | [train_policy] epoch #588 | Computing KL after
2021-06-04 13:55:40 | [train_policy] epoch #588 | Computing loss after
2021-06-04 13:55:40 | [train_policy] epoch #588 | Fitting baseline...
2021-06-04 13:55:40 | [train_policy] epoch #588 | Saving snapshot...
2021-06-04 13:55:40 | [train_policy] epoch #588 | Saved
2021-06-04 13:55:40 | [train_policy] epoch #588 | Time 472.40 s
2021-06-04 13:55:40 | [train_policy] epoch #588 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.299252
Evaluation/AverageDiscountedReturn          -42.695
Evaluation/AverageReturn                    -42.695
Evaluation/CompletionRate                     0
Evaluation/Iteration                        588
Evaluation/MaxReturn                        -30.3846
Evaluation/MinReturn                        -64.2052
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.40976
Extras/EpisodeRewardMean                    -42.5312
LinearFeatureBaseline/ExplainedVariance       0.907635
PolicyExecTime                                0.237853
ProcessExecTime                               0.0337262
TotalEnvSteps                            596068
policy/Entropy                               -1.3991
policy/KL                                     0.00654897
policy/KLBefore                               0
policy/LossAfter                             -0.016043
policy/LossBefore                            -1.43711e-08
policy/Perplexity                             0.246819
policy/dLoss                                  0.016043
---------------------------------------  ----------------
2021-06-04 13:55:40 | [train_policy] epoch #589 | Obtaining samples for iteration 589...
2021-06-04 13:55:41 | [train_policy] epoch #589 | Logging diagnostics...
2021-06-04 13:55:41 | [train_policy] epoch #589 | Optimizing policy...
2021-06-04 13:55:41 | [train_policy] epoch #589 | Computing loss before
2021-06-04 13:55:41 | [train_policy] epoch #589 | Computing KL before
2021-06-04 13:55:41 | [train_policy] epoch #589 | Optimizing
2021-06-04 13:55:41 | [train_policy] epoch #589 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:41 | [train_policy] epoch #589 | computing loss before
2021-06-04 13:55:41 | [train_policy] epoch #589 | computing gradient
2021-06-04 13:55:41 | [train_policy] epoch #589 | gradient computed
2021-06-04 13:55:41 | [train_policy] epoch #589 | computing descent direction
2021-06-04 13:55:41 | [train_policy] epoch #589 | descent direction computed
2021-06-04 13:55:41 | [train_policy] epoch #589 | backtrack iters: 1
2021-06-04 13:55:41 | [train_policy] epoch #589 | optimization finished
2021-06-04 13:55:41 | [train_policy] epoch #589 | Computing KL after
2021-06-04 13:55:41 | [train_policy] epoch #589 | Computing loss after
2021-06-04 13:55:41 | [train_policy] epoch #589 | Fitting baseline...
2021-06-04 13:55:41 | [train_policy] epoch #589 | Saving snapshot...
2021-06-04 13:55:41 | [train_policy] epoch #589 | Saved
2021-06-04 13:55:41 | [train_policy] epoch #589 | Time 473.19 s
2021-06-04 13:55:41 | [train_policy] epoch #589 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283384
Evaluation/AverageDiscountedReturn          -42.3926
Evaluation/AverageReturn                    -42.3926
Evaluation/CompletionRate                     0
Evaluation/Iteration                        589
Evaluation/MaxReturn                        -29.4025
Evaluation/MinReturn                        -64.0611
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.50244
Extras/EpisodeRewardMean                    -42.3605
LinearFeatureBaseline/ExplainedVariance       0.919507
PolicyExecTime                                0.232457
ProcessExecTime                               0.0313537
TotalEnvSteps                            597080
policy/Entropy                               -1.40142
policy/KL                                     0.00661611
policy/KLBefore                               0
policy/LossAfter                             -0.0129703
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.246247
policy/dLoss                                  0.0129702
---------------------------------------  ----------------
2021-06-04 13:55:41 | [train_policy] epoch #590 | Obtaining samples for iteration 590...
2021-06-04 13:55:42 | [train_policy] epoch #590 | Logging diagnostics...
2021-06-04 13:55:42 | [train_policy] epoch #590 | Optimizing policy...
2021-06-04 13:55:42 | [train_policy] epoch #590 | Computing loss before
2021-06-04 13:55:42 | [train_policy] epoch #590 | Computing KL before
2021-06-04 13:55:42 | [train_policy] epoch #590 | Optimizing
2021-06-04 13:55:42 | [train_policy] epoch #590 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:42 | [train_policy] epoch #590 | computing loss before
2021-06-04 13:55:42 | [train_policy] epoch #590 | computing gradient
2021-06-04 13:55:42 | [train_policy] epoch #590 | gradient computed
2021-06-04 13:55:42 | [train_policy] epoch #590 | computing descent direction
2021-06-04 13:55:42 | [train_policy] epoch #590 | descent direction computed
2021-06-04 13:55:42 | [train_policy] epoch #590 | backtrack iters: 1
2021-06-04 13:55:42 | [train_policy] epoch #590 | optimization finished
2021-06-04 13:55:42 | [train_policy] epoch #590 | Computing KL after
2021-06-04 13:55:42 | [train_policy] epoch #590 | Computing loss after
2021-06-04 13:55:42 | [train_policy] epoch #590 | Fitting baseline...
2021-06-04 13:55:42 | [train_policy] epoch #590 | Saving snapshot...
2021-06-04 13:55:42 | [train_policy] epoch #590 | Saved
2021-06-04 13:55:42 | [train_policy] epoch #590 | Time 473.99 s
2021-06-04 13:55:42 | [train_policy] epoch #590 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283684
Evaluation/AverageDiscountedReturn          -41.4596
Evaluation/AverageReturn                    -41.4596
Evaluation/CompletionRate                     0
Evaluation/Iteration                        590
Evaluation/MaxReturn                        -29.2943
Evaluation/MinReturn                        -77.0364
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.88087
Extras/EpisodeRewardMean                    -41.9353
LinearFeatureBaseline/ExplainedVariance       0.89483
PolicyExecTime                                0.241321
ProcessExecTime                               0.031338
TotalEnvSteps                            598092
policy/Entropy                               -1.43138
policy/KL                                     0.00655668
policy/KLBefore                               0
policy/LossAfter                             -0.0189437
policy/LossBefore                             1.24863e-08
policy/Perplexity                             0.238979
policy/dLoss                                  0.0189437
---------------------------------------  ----------------
2021-06-04 13:55:42 | [train_policy] epoch #591 | Obtaining samples for iteration 591...
2021-06-04 13:55:42 | [train_policy] epoch #591 | Logging diagnostics...
2021-06-04 13:55:42 | [train_policy] epoch #591 | Optimizing policy...
2021-06-04 13:55:42 | [train_policy] epoch #591 | Computing loss before
2021-06-04 13:55:42 | [train_policy] epoch #591 | Computing KL before
2021-06-04 13:55:42 | [train_policy] epoch #591 | Optimizing
2021-06-04 13:55:42 | [train_policy] epoch #591 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:42 | [train_policy] epoch #591 | computing loss before
2021-06-04 13:55:42 | [train_policy] epoch #591 | computing gradient
2021-06-04 13:55:42 | [train_policy] epoch #591 | gradient computed
2021-06-04 13:55:42 | [train_policy] epoch #591 | computing descent direction
2021-06-04 13:55:43 | [train_policy] epoch #591 | descent direction computed
2021-06-04 13:55:43 | [train_policy] epoch #591 | backtrack iters: 1
2021-06-04 13:55:43 | [train_policy] epoch #591 | optimization finished
2021-06-04 13:55:43 | [train_policy] epoch #591 | Computing KL after
2021-06-04 13:55:43 | [train_policy] epoch #591 | Computing loss after
2021-06-04 13:55:43 | [train_policy] epoch #591 | Fitting baseline...
2021-06-04 13:55:43 | [train_policy] epoch #591 | Saving snapshot...
2021-06-04 13:55:43 | [train_policy] epoch #591 | Saved
2021-06-04 13:55:43 | [train_policy] epoch #591 | Time 474.79 s
2021-06-04 13:55:43 | [train_policy] epoch #591 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283063
Evaluation/AverageDiscountedReturn          -42.4787
Evaluation/AverageReturn                    -42.4787
Evaluation/CompletionRate                     0
Evaluation/Iteration                        591
Evaluation/MaxReturn                        -28.2935
Evaluation/MinReturn                        -79.9556
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.87815
Extras/EpisodeRewardMean                    -42.4704
LinearFeatureBaseline/ExplainedVariance       0.876515
PolicyExecTime                                0.224052
ProcessExecTime                               0.0312381
TotalEnvSteps                            599104
policy/Entropy                               -1.44
policy/KL                                     0.00672136
policy/KLBefore                               0
policy/LossAfter                             -0.0190722
policy/LossBefore                             1.01304e-08
policy/Perplexity                             0.236927
policy/dLoss                                  0.0190722
---------------------------------------  ----------------
2021-06-04 13:55:43 | [train_policy] epoch #592 | Obtaining samples for iteration 592...
2021-06-04 13:55:43 | [train_policy] epoch #592 | Logging diagnostics...
2021-06-04 13:55:43 | [train_policy] epoch #592 | Optimizing policy...
2021-06-04 13:55:43 | [train_policy] epoch #592 | Computing loss before
2021-06-04 13:55:43 | [train_policy] epoch #592 | Computing KL before
2021-06-04 13:55:43 | [train_policy] epoch #592 | Optimizing
2021-06-04 13:55:43 | [train_policy] epoch #592 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:43 | [train_policy] epoch #592 | computing loss before
2021-06-04 13:55:43 | [train_policy] epoch #592 | computing gradient
2021-06-04 13:55:43 | [train_policy] epoch #592 | gradient computed
2021-06-04 13:55:43 | [train_policy] epoch #592 | computing descent direction
2021-06-04 13:55:43 | [train_policy] epoch #592 | descent direction computed
2021-06-04 13:55:43 | [train_policy] epoch #592 | backtrack iters: 0
2021-06-04 13:55:43 | [train_policy] epoch #592 | optimization finished
2021-06-04 13:55:43 | [train_policy] epoch #592 | Computing KL after
2021-06-04 13:55:43 | [train_policy] epoch #592 | Computing loss after
2021-06-04 13:55:43 | [train_policy] epoch #592 | Fitting baseline...
2021-06-04 13:55:43 | [train_policy] epoch #592 | Saving snapshot...
2021-06-04 13:55:43 | [train_policy] epoch #592 | Saved
2021-06-04 13:55:43 | [train_policy] epoch #592 | Time 475.57 s
2021-06-04 13:55:43 | [train_policy] epoch #592 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.28329
Evaluation/AverageDiscountedReturn          -42.5961
Evaluation/AverageReturn                    -42.5961
Evaluation/CompletionRate                     0
Evaluation/Iteration                        592
Evaluation/MaxReturn                        -29.395
Evaluation/MinReturn                        -78.7461
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.20424
Extras/EpisodeRewardMean                    -42.3996
LinearFeatureBaseline/ExplainedVariance       0.882801
PolicyExecTime                                0.234647
ProcessExecTime                               0.0313909
TotalEnvSteps                            600116
policy/Entropy                               -1.42023
policy/KL                                     0.00941285
policy/KLBefore                               0
policy/LossAfter                             -0.0169956
policy/LossBefore                            -0
policy/Perplexity                             0.241658
policy/dLoss                                  0.0169956
---------------------------------------  ---------------
2021-06-04 13:55:43 | [train_policy] epoch #593 | Obtaining samples for iteration 593...
2021-06-04 13:55:44 | [train_policy] epoch #593 | Logging diagnostics...
2021-06-04 13:55:44 | [train_policy] epoch #593 | Optimizing policy...
2021-06-04 13:55:44 | [train_policy] epoch #593 | Computing loss before
2021-06-04 13:55:44 | [train_policy] epoch #593 | Computing KL before
2021-06-04 13:55:44 | [train_policy] epoch #593 | Optimizing
2021-06-04 13:55:44 | [train_policy] epoch #593 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:44 | [train_policy] epoch #593 | computing loss before
2021-06-04 13:55:44 | [train_policy] epoch #593 | computing gradient
2021-06-04 13:55:44 | [train_policy] epoch #593 | gradient computed
2021-06-04 13:55:44 | [train_policy] epoch #593 | computing descent direction
2021-06-04 13:55:44 | [train_policy] epoch #593 | descent direction computed
2021-06-04 13:55:44 | [train_policy] epoch #593 | backtrack iters: 1
2021-06-04 13:55:44 | [train_policy] epoch #593 | optimization finished
2021-06-04 13:55:44 | [train_policy] epoch #593 | Computing KL after
2021-06-04 13:55:44 | [train_policy] epoch #593 | Computing loss after
2021-06-04 13:55:44 | [train_policy] epoch #593 | Fitting baseline...
2021-06-04 13:55:44 | [train_policy] epoch #593 | Saving snapshot...
2021-06-04 13:55:44 | [train_policy] epoch #593 | Saved
2021-06-04 13:55:44 | [train_policy] epoch #593 | Time 476.37 s
2021-06-04 13:55:44 | [train_policy] epoch #593 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283841
Evaluation/AverageDiscountedReturn          -42.7946
Evaluation/AverageReturn                    -42.7946
Evaluation/CompletionRate                     0
Evaluation/Iteration                        593
Evaluation/MaxReturn                        -29.5464
Evaluation/MinReturn                        -76.7692
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.84006
Extras/EpisodeRewardMean                    -42.3439
LinearFeatureBaseline/ExplainedVariance       0.874078
PolicyExecTime                                0.240405
ProcessExecTime                               0.0313113
TotalEnvSteps                            601128
policy/Entropy                               -1.40395
policy/KL                                     0.00644505
policy/KLBefore                               0
policy/LossAfter                             -0.0174906
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.245624
policy/dLoss                                  0.0174906
---------------------------------------  ----------------
2021-06-04 13:55:44 | [train_policy] epoch #594 | Obtaining samples for iteration 594...
2021-06-04 13:55:45 | [train_policy] epoch #594 | Logging diagnostics...
2021-06-04 13:55:45 | [train_policy] epoch #594 | Optimizing policy...
2021-06-04 13:55:45 | [train_policy] epoch #594 | Computing loss before
2021-06-04 13:55:45 | [train_policy] epoch #594 | Computing KL before
2021-06-04 13:55:45 | [train_policy] epoch #594 | Optimizing
2021-06-04 13:55:45 | [train_policy] epoch #594 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:45 | [train_policy] epoch #594 | computing loss before
2021-06-04 13:55:45 | [train_policy] epoch #594 | computing gradient
2021-06-04 13:55:45 | [train_policy] epoch #594 | gradient computed
2021-06-04 13:55:45 | [train_policy] epoch #594 | computing descent direction
2021-06-04 13:55:45 | [train_policy] epoch #594 | descent direction computed
2021-06-04 13:55:45 | [train_policy] epoch #594 | backtrack iters: 0
2021-06-04 13:55:45 | [train_policy] epoch #594 | optimization finished
2021-06-04 13:55:45 | [train_policy] epoch #594 | Computing KL after
2021-06-04 13:55:45 | [train_policy] epoch #594 | Computing loss after
2021-06-04 13:55:45 | [train_policy] epoch #594 | Fitting baseline...
2021-06-04 13:55:45 | [train_policy] epoch #594 | Saving snapshot...
2021-06-04 13:55:45 | [train_policy] epoch #594 | Saved
2021-06-04 13:55:45 | [train_policy] epoch #594 | Time 477.15 s
2021-06-04 13:55:45 | [train_policy] epoch #594 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283352
Evaluation/AverageDiscountedReturn          -43.2017
Evaluation/AverageReturn                    -43.2017
Evaluation/CompletionRate                     0
Evaluation/Iteration                        594
Evaluation/MaxReturn                        -28.9315
Evaluation/MinReturn                        -77.4108
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.28063
Extras/EpisodeRewardMean                    -43.2522
LinearFeatureBaseline/ExplainedVariance       0.897452
PolicyExecTime                                0.22965
ProcessExecTime                               0.0312431
TotalEnvSteps                            602140
policy/Entropy                               -1.38802
policy/KL                                     0.00991314
policy/KLBefore                               0
policy/LossAfter                             -0.0187192
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.249568
policy/dLoss                                  0.0187192
---------------------------------------  ----------------
2021-06-04 13:55:45 | [train_policy] epoch #595 | Obtaining samples for iteration 595...
2021-06-04 13:55:46 | [train_policy] epoch #595 | Logging diagnostics...
2021-06-04 13:55:46 | [train_policy] epoch #595 | Optimizing policy...
2021-06-04 13:55:46 | [train_policy] epoch #595 | Computing loss before
2021-06-04 13:55:46 | [train_policy] epoch #595 | Computing KL before
2021-06-04 13:55:46 | [train_policy] epoch #595 | Optimizing
2021-06-04 13:55:46 | [train_policy] epoch #595 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:46 | [train_policy] epoch #595 | computing loss before
2021-06-04 13:55:46 | [train_policy] epoch #595 | computing gradient
2021-06-04 13:55:46 | [train_policy] epoch #595 | gradient computed
2021-06-04 13:55:46 | [train_policy] epoch #595 | computing descent direction
2021-06-04 13:55:46 | [train_policy] epoch #595 | descent direction computed
2021-06-04 13:55:46 | [train_policy] epoch #595 | backtrack iters: 1
2021-06-04 13:55:46 | [train_policy] epoch #595 | optimization finished
2021-06-04 13:55:46 | [train_policy] epoch #595 | Computing KL after
2021-06-04 13:55:46 | [train_policy] epoch #595 | Computing loss after
2021-06-04 13:55:46 | [train_policy] epoch #595 | Fitting baseline...
2021-06-04 13:55:46 | [train_policy] epoch #595 | Saving snapshot...
2021-06-04 13:55:46 | [train_policy] epoch #595 | Saved
2021-06-04 13:55:46 | [train_policy] epoch #595 | Time 477.99 s
2021-06-04 13:55:46 | [train_policy] epoch #595 | EpochTime 0.82 s
---------------------------------------  ----------------
EnvExecTime                                   0.301811
Evaluation/AverageDiscountedReturn          -43.5695
Evaluation/AverageReturn                    -43.5695
Evaluation/CompletionRate                     0
Evaluation/Iteration                        595
Evaluation/MaxReturn                        -29.3606
Evaluation/MinReturn                        -77.2034
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.67087
Extras/EpisodeRewardMean                    -43.4938
LinearFeatureBaseline/ExplainedVariance       0.887472
PolicyExecTime                                0.236437
ProcessExecTime                               0.0335643
TotalEnvSteps                            603152
policy/Entropy                               -1.38104
policy/KL                                     0.0064494
policy/KLBefore                               0
policy/LossAfter                             -0.0144793
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.251317
policy/dLoss                                  0.0144793
---------------------------------------  ----------------
2021-06-04 13:55:46 | [train_policy] epoch #596 | Obtaining samples for iteration 596...
2021-06-04 13:55:46 | [train_policy] epoch #596 | Logging diagnostics...
2021-06-04 13:55:46 | [train_policy] epoch #596 | Optimizing policy...
2021-06-04 13:55:46 | [train_policy] epoch #596 | Computing loss before
2021-06-04 13:55:46 | [train_policy] epoch #596 | Computing KL before
2021-06-04 13:55:46 | [train_policy] epoch #596 | Optimizing
2021-06-04 13:55:46 | [train_policy] epoch #596 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:46 | [train_policy] epoch #596 | computing loss before
2021-06-04 13:55:46 | [train_policy] epoch #596 | computing gradient
2021-06-04 13:55:46 | [train_policy] epoch #596 | gradient computed
2021-06-04 13:55:46 | [train_policy] epoch #596 | computing descent direction
2021-06-04 13:55:47 | [train_policy] epoch #596 | descent direction computed
2021-06-04 13:55:47 | [train_policy] epoch #596 | backtrack iters: 1
2021-06-04 13:55:47 | [train_policy] epoch #596 | optimization finished
2021-06-04 13:55:47 | [train_policy] epoch #596 | Computing KL after
2021-06-04 13:55:47 | [train_policy] epoch #596 | Computing loss after
2021-06-04 13:55:47 | [train_policy] epoch #596 | Fitting baseline...
2021-06-04 13:55:47 | [train_policy] epoch #596 | Saving snapshot...
2021-06-04 13:55:47 | [train_policy] epoch #596 | Saved
2021-06-04 13:55:47 | [train_policy] epoch #596 | Time 478.80 s
2021-06-04 13:55:47 | [train_policy] epoch #596 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283827
Evaluation/AverageDiscountedReturn          -40.4928
Evaluation/AverageReturn                    -40.4928
Evaluation/CompletionRate                     0
Evaluation/Iteration                        596
Evaluation/MaxReturn                        -30.0263
Evaluation/MinReturn                        -63.5085
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43639
Extras/EpisodeRewardMean                    -40.7803
LinearFeatureBaseline/ExplainedVariance       0.910634
PolicyExecTime                                0.233
ProcessExecTime                               0.0313809
TotalEnvSteps                            604164
policy/Entropy                               -1.40866
policy/KL                                     0.00670159
policy/KLBefore                               0
policy/LossAfter                             -0.0167159
policy/LossBefore                            -1.17796e-10
policy/Perplexity                             0.244471
policy/dLoss                                  0.0167159
---------------------------------------  ----------------
2021-06-04 13:55:47 | [train_policy] epoch #597 | Obtaining samples for iteration 597...
2021-06-04 13:55:47 | [train_policy] epoch #597 | Logging diagnostics...
2021-06-04 13:55:47 | [train_policy] epoch #597 | Optimizing policy...
2021-06-04 13:55:47 | [train_policy] epoch #597 | Computing loss before
2021-06-04 13:55:47 | [train_policy] epoch #597 | Computing KL before
2021-06-04 13:55:47 | [train_policy] epoch #597 | Optimizing
2021-06-04 13:55:47 | [train_policy] epoch #597 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:47 | [train_policy] epoch #597 | computing loss before
2021-06-04 13:55:47 | [train_policy] epoch #597 | computing gradient
2021-06-04 13:55:47 | [train_policy] epoch #597 | gradient computed
2021-06-04 13:55:47 | [train_policy] epoch #597 | computing descent direction
2021-06-04 13:55:47 | [train_policy] epoch #597 | descent direction computed
2021-06-04 13:55:47 | [train_policy] epoch #597 | backtrack iters: 0
2021-06-04 13:55:47 | [train_policy] epoch #597 | optimization finished
2021-06-04 13:55:47 | [train_policy] epoch #597 | Computing KL after
2021-06-04 13:55:47 | [train_policy] epoch #597 | Computing loss after
2021-06-04 13:55:47 | [train_policy] epoch #597 | Fitting baseline...
2021-06-04 13:55:47 | [train_policy] epoch #597 | Saving snapshot...
2021-06-04 13:55:47 | [train_policy] epoch #597 | Saved
2021-06-04 13:55:47 | [train_policy] epoch #597 | Time 479.57 s
2021-06-04 13:55:47 | [train_policy] epoch #597 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.283426
Evaluation/AverageDiscountedReturn          -43.6238
Evaluation/AverageReturn                    -43.6238
Evaluation/CompletionRate                     0
Evaluation/Iteration                        597
Evaluation/MaxReturn                        -29.4526
Evaluation/MinReturn                        -78.7586
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.40547
Extras/EpisodeRewardMean                    -43.5289
LinearFeatureBaseline/ExplainedVariance       0.873284
PolicyExecTime                                0.220035
ProcessExecTime                               0.0313339
TotalEnvSteps                            605176
policy/Entropy                               -1.40693
policy/KL                                     0.00955343
policy/KLBefore                               0
policy/LossAfter                             -0.0182412
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.244895
policy/dLoss                                  0.0182412
---------------------------------------  ----------------
2021-06-04 13:55:47 | [train_policy] epoch #598 | Obtaining samples for iteration 598...
2021-06-04 13:55:48 | [train_policy] epoch #598 | Logging diagnostics...
2021-06-04 13:55:48 | [train_policy] epoch #598 | Optimizing policy...
2021-06-04 13:55:48 | [train_policy] epoch #598 | Computing loss before
2021-06-04 13:55:48 | [train_policy] epoch #598 | Computing KL before
2021-06-04 13:55:48 | [train_policy] epoch #598 | Optimizing
2021-06-04 13:55:48 | [train_policy] epoch #598 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:48 | [train_policy] epoch #598 | computing loss before
2021-06-04 13:55:48 | [train_policy] epoch #598 | computing gradient
2021-06-04 13:55:48 | [train_policy] epoch #598 | gradient computed
2021-06-04 13:55:48 | [train_policy] epoch #598 | computing descent direction
2021-06-04 13:55:48 | [train_policy] epoch #598 | descent direction computed
2021-06-04 13:55:48 | [train_policy] epoch #598 | backtrack iters: 1
2021-06-04 13:55:48 | [train_policy] epoch #598 | optimization finished
2021-06-04 13:55:48 | [train_policy] epoch #598 | Computing KL after
2021-06-04 13:55:48 | [train_policy] epoch #598 | Computing loss after
2021-06-04 13:55:48 | [train_policy] epoch #598 | Fitting baseline...
2021-06-04 13:55:48 | [train_policy] epoch #598 | Saving snapshot...
2021-06-04 13:55:48 | [train_policy] epoch #598 | Saved
2021-06-04 13:55:48 | [train_policy] epoch #598 | Time 480.34 s
2021-06-04 13:55:48 | [train_policy] epoch #598 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.283339
Evaluation/AverageDiscountedReturn          -42.1917
Evaluation/AverageReturn                    -42.1917
Evaluation/CompletionRate                     0
Evaluation/Iteration                        598
Evaluation/MaxReturn                        -28.5439
Evaluation/MinReturn                        -75.4529
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.23296
Extras/EpisodeRewardMean                    -41.9954
LinearFeatureBaseline/ExplainedVariance       0.90251
PolicyExecTime                                0.222382
ProcessExecTime                               0.0313129
TotalEnvSteps                            606188
policy/Entropy                               -1.41534
policy/KL                                     0.00646678
policy/KLBefore                               0
policy/LossAfter                             -0.014192
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.242843
policy/dLoss                                  0.014192
---------------------------------------  ----------------
2021-06-04 13:55:48 | [train_policy] epoch #599 | Obtaining samples for iteration 599...
2021-06-04 13:55:49 | [train_policy] epoch #599 | Logging diagnostics...
2021-06-04 13:55:49 | [train_policy] epoch #599 | Optimizing policy...
2021-06-04 13:55:49 | [train_policy] epoch #599 | Computing loss before
2021-06-04 13:55:49 | [train_policy] epoch #599 | Computing KL before
2021-06-04 13:55:49 | [train_policy] epoch #599 | Optimizing
2021-06-04 13:55:49 | [train_policy] epoch #599 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:49 | [train_policy] epoch #599 | computing loss before
2021-06-04 13:55:49 | [train_policy] epoch #599 | computing gradient
2021-06-04 13:55:49 | [train_policy] epoch #599 | gradient computed
2021-06-04 13:55:49 | [train_policy] epoch #599 | computing descent direction
2021-06-04 13:55:49 | [train_policy] epoch #599 | descent direction computed
2021-06-04 13:55:49 | [train_policy] epoch #599 | backtrack iters: 0
2021-06-04 13:55:49 | [train_policy] epoch #599 | optimization finished
2021-06-04 13:55:49 | [train_policy] epoch #599 | Computing KL after
2021-06-04 13:55:49 | [train_policy] epoch #599 | Computing loss after
2021-06-04 13:55:49 | [train_policy] epoch #599 | Fitting baseline...
2021-06-04 13:55:49 | [train_policy] epoch #599 | Saving snapshot...
2021-06-04 13:55:49 | [train_policy] epoch #599 | Saved
2021-06-04 13:55:49 | [train_policy] epoch #599 | Time 481.12 s
2021-06-04 13:55:49 | [train_policy] epoch #599 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283928
Evaluation/AverageDiscountedReturn          -42.1097
Evaluation/AverageReturn                    -42.1097
Evaluation/CompletionRate                     0
Evaluation/Iteration                        599
Evaluation/MaxReturn                        -28.4072
Evaluation/MinReturn                        -77.2616
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.89752
Extras/EpisodeRewardMean                    -42.4895
LinearFeatureBaseline/ExplainedVariance       0.854488
PolicyExecTime                                0.227236
ProcessExecTime                               0.0314677
TotalEnvSteps                            607200
policy/Entropy                               -1.42063
policy/KL                                     0.00992427
policy/KLBefore                               0
policy/LossAfter                             -0.0163545
policy/LossBefore                            -3.53387e-09
policy/Perplexity                             0.241562
policy/dLoss                                  0.0163545
---------------------------------------  ----------------
2021-06-04 13:55:49 | [train_policy] epoch #600 | Obtaining samples for iteration 600...
2021-06-04 13:55:50 | [train_policy] epoch #600 | Logging diagnostics...
2021-06-04 13:55:50 | [train_policy] epoch #600 | Optimizing policy...
2021-06-04 13:55:50 | [train_policy] epoch #600 | Computing loss before
2021-06-04 13:55:50 | [train_policy] epoch #600 | Computing KL before
2021-06-04 13:55:50 | [train_policy] epoch #600 | Optimizing
2021-06-04 13:55:50 | [train_policy] epoch #600 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:50 | [train_policy] epoch #600 | computing loss before
2021-06-04 13:55:50 | [train_policy] epoch #600 | computing gradient
2021-06-04 13:55:50 | [train_policy] epoch #600 | gradient computed
2021-06-04 13:55:50 | [train_policy] epoch #600 | computing descent direction
2021-06-04 13:55:50 | [train_policy] epoch #600 | descent direction computed
2021-06-04 13:55:50 | [train_policy] epoch #600 | backtrack iters: 0
2021-06-04 13:55:50 | [train_policy] epoch #600 | optimization finished
2021-06-04 13:55:50 | [train_policy] epoch #600 | Computing KL after
2021-06-04 13:55:50 | [train_policy] epoch #600 | Computing loss after
2021-06-04 13:55:50 | [train_policy] epoch #600 | Fitting baseline...
2021-06-04 13:55:50 | [train_policy] epoch #600 | Saving snapshot...
2021-06-04 13:55:50 | [train_policy] epoch #600 | Saved
2021-06-04 13:55:50 | [train_policy] epoch #600 | Time 481.94 s
2021-06-04 13:55:50 | [train_policy] epoch #600 | EpochTime 0.81 s
---------------------------------------  ----------------
EnvExecTime                                   0.28292
Evaluation/AverageDiscountedReturn          -42.7978
Evaluation/AverageReturn                    -42.7978
Evaluation/CompletionRate                     0
Evaluation/Iteration                        600
Evaluation/MaxReturn                        -28.547
Evaluation/MinReturn                        -78.0613
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.54611
Extras/EpisodeRewardMean                    -42.8104
LinearFeatureBaseline/ExplainedVariance       0.900385
PolicyExecTime                                0.229673
ProcessExecTime                               0.0312655
TotalEnvSteps                            608212
policy/Entropy                               -1.41672
policy/KL                                     0.00988612
policy/KLBefore                               0
policy/LossAfter                             -0.021275
policy/LossBefore                            -2.47371e-09
policy/Perplexity                             0.242509
policy/dLoss                                  0.021275
---------------------------------------  ----------------
2021-06-04 13:55:50 | [train_policy] epoch #601 | Obtaining samples for iteration 601...
2021-06-04 13:55:50 | [train_policy] epoch #601 | Logging diagnostics...
2021-06-04 13:55:50 | [train_policy] epoch #601 | Optimizing policy...
2021-06-04 13:55:50 | [train_policy] epoch #601 | Computing loss before
2021-06-04 13:55:50 | [train_policy] epoch #601 | Computing KL before
2021-06-04 13:55:50 | [train_policy] epoch #601 | Optimizing
2021-06-04 13:55:50 | [train_policy] epoch #601 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:50 | [train_policy] epoch #601 | computing loss before
2021-06-04 13:55:50 | [train_policy] epoch #601 | computing gradient
2021-06-04 13:55:50 | [train_policy] epoch #601 | gradient computed
2021-06-04 13:55:50 | [train_policy] epoch #601 | computing descent direction
2021-06-04 13:55:50 | [train_policy] epoch #601 | descent direction computed
2021-06-04 13:55:50 | [train_policy] epoch #601 | backtrack iters: 1
2021-06-04 13:55:50 | [train_policy] epoch #601 | optimization finished
2021-06-04 13:55:50 | [train_policy] epoch #601 | Computing KL after
2021-06-04 13:55:50 | [train_policy] epoch #601 | Computing loss after
2021-06-04 13:55:50 | [train_policy] epoch #601 | Fitting baseline...
2021-06-04 13:55:51 | [train_policy] epoch #601 | Saving snapshot...
2021-06-04 13:55:51 | [train_policy] epoch #601 | Saved
2021-06-04 13:55:51 | [train_policy] epoch #601 | Time 482.76 s
2021-06-04 13:55:51 | [train_policy] epoch #601 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284988
Evaluation/AverageDiscountedReturn          -42.304
Evaluation/AverageReturn                    -42.304
Evaluation/CompletionRate                     0
Evaluation/Iteration                        601
Evaluation/MaxReturn                        -28.5918
Evaluation/MinReturn                        -76.7494
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.30113
Extras/EpisodeRewardMean                    -42.5726
LinearFeatureBaseline/ExplainedVariance       0.89204
PolicyExecTime                                0.239455
ProcessExecTime                               0.0313842
TotalEnvSteps                            609224
policy/Entropy                               -1.44808
policy/KL                                     0.00660871
policy/KLBefore                               0
policy/LossAfter                             -0.0129635
policy/LossBefore                            -7.89231e-09
policy/Perplexity                             0.235021
policy/dLoss                                  0.0129635
---------------------------------------  ----------------
2021-06-04 13:55:51 | [train_policy] epoch #602 | Obtaining samples for iteration 602...
2021-06-04 13:55:51 | [train_policy] epoch #602 | Logging diagnostics...
2021-06-04 13:55:51 | [train_policy] epoch #602 | Optimizing policy...
2021-06-04 13:55:51 | [train_policy] epoch #602 | Computing loss before
2021-06-04 13:55:51 | [train_policy] epoch #602 | Computing KL before
2021-06-04 13:55:51 | [train_policy] epoch #602 | Optimizing
2021-06-04 13:55:51 | [train_policy] epoch #602 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:51 | [train_policy] epoch #602 | computing loss before
2021-06-04 13:55:51 | [train_policy] epoch #602 | computing gradient
2021-06-04 13:55:51 | [train_policy] epoch #602 | gradient computed
2021-06-04 13:55:51 | [train_policy] epoch #602 | computing descent direction
2021-06-04 13:55:51 | [train_policy] epoch #602 | descent direction computed
2021-06-04 13:55:51 | [train_policy] epoch #602 | backtrack iters: 0
2021-06-04 13:55:51 | [train_policy] epoch #602 | optimization finished
2021-06-04 13:55:51 | [train_policy] epoch #602 | Computing KL after
2021-06-04 13:55:51 | [train_policy] epoch #602 | Computing loss after
2021-06-04 13:55:51 | [train_policy] epoch #602 | Fitting baseline...
2021-06-04 13:55:51 | [train_policy] epoch #602 | Saving snapshot...
2021-06-04 13:55:51 | [train_policy] epoch #602 | Saved
2021-06-04 13:55:51 | [train_policy] epoch #602 | Time 483.58 s
2021-06-04 13:55:51 | [train_policy] epoch #602 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.285861
Evaluation/AverageDiscountedReturn          -42.318
Evaluation/AverageReturn                    -42.318
Evaluation/CompletionRate                     0
Evaluation/Iteration                        602
Evaluation/MaxReturn                        -27.6592
Evaluation/MinReturn                        -64.1166
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.97286
Extras/EpisodeRewardMean                    -42.1653
LinearFeatureBaseline/ExplainedVariance       0.904903
PolicyExecTime                                0.229908
ProcessExecTime                               0.031353
TotalEnvSteps                            610236
policy/Entropy                               -1.46483
policy/KL                                     0.00991601
policy/KLBefore                               0
policy/LossAfter                             -0.022885
policy/LossBefore                             2.00253e-09
policy/Perplexity                             0.231116
policy/dLoss                                  0.022885
---------------------------------------  ----------------
2021-06-04 13:55:51 | [train_policy] epoch #603 | Obtaining samples for iteration 603...
2021-06-04 13:55:52 | [train_policy] epoch #603 | Logging diagnostics...
2021-06-04 13:55:52 | [train_policy] epoch #603 | Optimizing policy...
2021-06-04 13:55:52 | [train_policy] epoch #603 | Computing loss before
2021-06-04 13:55:52 | [train_policy] epoch #603 | Computing KL before
2021-06-04 13:55:52 | [train_policy] epoch #603 | Optimizing
2021-06-04 13:55:52 | [train_policy] epoch #603 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:52 | [train_policy] epoch #603 | computing loss before
2021-06-04 13:55:52 | [train_policy] epoch #603 | computing gradient
2021-06-04 13:55:52 | [train_policy] epoch #603 | gradient computed
2021-06-04 13:55:52 | [train_policy] epoch #603 | computing descent direction
2021-06-04 13:55:52 | [train_policy] epoch #603 | descent direction computed
2021-06-04 13:55:52 | [train_policy] epoch #603 | backtrack iters: 1
2021-06-04 13:55:52 | [train_policy] epoch #603 | optimization finished
2021-06-04 13:55:52 | [train_policy] epoch #603 | Computing KL after
2021-06-04 13:55:52 | [train_policy] epoch #603 | Computing loss after
2021-06-04 13:55:52 | [train_policy] epoch #603 | Fitting baseline...
2021-06-04 13:55:52 | [train_policy] epoch #603 | Saving snapshot...
2021-06-04 13:55:52 | [train_policy] epoch #603 | Saved
2021-06-04 13:55:52 | [train_policy] epoch #603 | Time 484.38 s
2021-06-04 13:55:52 | [train_policy] epoch #603 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283395
Evaluation/AverageDiscountedReturn          -42.7254
Evaluation/AverageReturn                    -42.7254
Evaluation/CompletionRate                     0
Evaluation/Iteration                        603
Evaluation/MaxReturn                        -28.2037
Evaluation/MinReturn                        -80.3237
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.49358
Extras/EpisodeRewardMean                    -43.1163
LinearFeatureBaseline/ExplainedVariance       0.87345
PolicyExecTime                                0.222706
ProcessExecTime                               0.0310965
TotalEnvSteps                            611248
policy/Entropy                               -1.48252
policy/KL                                     0.00651786
policy/KLBefore                               0
policy/LossAfter                             -0.0207016
policy/LossBefore                            -4.18175e-09
policy/Perplexity                             0.227066
policy/dLoss                                  0.0207015
---------------------------------------  ----------------
2021-06-04 13:55:52 | [train_policy] epoch #604 | Obtaining samples for iteration 604...
2021-06-04 13:55:53 | [train_policy] epoch #604 | Logging diagnostics...
2021-06-04 13:55:53 | [train_policy] epoch #604 | Optimizing policy...
2021-06-04 13:55:53 | [train_policy] epoch #604 | Computing loss before
2021-06-04 13:55:53 | [train_policy] epoch #604 | Computing KL before
2021-06-04 13:55:53 | [train_policy] epoch #604 | Optimizing
2021-06-04 13:55:53 | [train_policy] epoch #604 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:53 | [train_policy] epoch #604 | computing loss before
2021-06-04 13:55:53 | [train_policy] epoch #604 | computing gradient
2021-06-04 13:55:53 | [train_policy] epoch #604 | gradient computed
2021-06-04 13:55:53 | [train_policy] epoch #604 | computing descent direction
2021-06-04 13:55:53 | [train_policy] epoch #604 | descent direction computed
2021-06-04 13:55:53 | [train_policy] epoch #604 | backtrack iters: 1
2021-06-04 13:55:53 | [train_policy] epoch #604 | optimization finished
2021-06-04 13:55:53 | [train_policy] epoch #604 | Computing KL after
2021-06-04 13:55:53 | [train_policy] epoch #604 | Computing loss after
2021-06-04 13:55:53 | [train_policy] epoch #604 | Fitting baseline...
2021-06-04 13:55:53 | [train_policy] epoch #604 | Saving snapshot...
2021-06-04 13:55:53 | [train_policy] epoch #604 | Saved
2021-06-04 13:55:53 | [train_policy] epoch #604 | Time 485.18 s
2021-06-04 13:55:53 | [train_policy] epoch #604 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283206
Evaluation/AverageDiscountedReturn          -41.3897
Evaluation/AverageReturn                    -41.3897
Evaluation/CompletionRate                     0
Evaluation/Iteration                        604
Evaluation/MaxReturn                        -32.3444
Evaluation/MinReturn                        -76.7132
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.21794
Extras/EpisodeRewardMean                    -41.6101
LinearFeatureBaseline/ExplainedVariance       0.890291
PolicyExecTime                                0.220622
ProcessExecTime                               0.0311165
TotalEnvSteps                            612260
policy/Entropy                               -1.46957
policy/KL                                     0.00642293
policy/KLBefore                               0
policy/LossAfter                             -0.0171654
policy/LossBefore                            -2.23812e-08
policy/Perplexity                             0.230024
policy/dLoss                                  0.0171654
---------------------------------------  ----------------
2021-06-04 13:55:53 | [train_policy] epoch #605 | Obtaining samples for iteration 605...
2021-06-04 13:55:54 | [train_policy] epoch #605 | Logging diagnostics...
2021-06-04 13:55:54 | [train_policy] epoch #605 | Optimizing policy...
2021-06-04 13:55:54 | [train_policy] epoch #605 | Computing loss before
2021-06-04 13:55:54 | [train_policy] epoch #605 | Computing KL before
2021-06-04 13:55:54 | [train_policy] epoch #605 | Optimizing
2021-06-04 13:55:54 | [train_policy] epoch #605 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:54 | [train_policy] epoch #605 | computing loss before
2021-06-04 13:55:54 | [train_policy] epoch #605 | computing gradient
2021-06-04 13:55:54 | [train_policy] epoch #605 | gradient computed
2021-06-04 13:55:54 | [train_policy] epoch #605 | computing descent direction
2021-06-04 13:55:54 | [train_policy] epoch #605 | descent direction computed
2021-06-04 13:55:54 | [train_policy] epoch #605 | backtrack iters: 1
2021-06-04 13:55:54 | [train_policy] epoch #605 | optimization finished
2021-06-04 13:55:54 | [train_policy] epoch #605 | Computing KL after
2021-06-04 13:55:54 | [train_policy] epoch #605 | Computing loss after
2021-06-04 13:55:54 | [train_policy] epoch #605 | Fitting baseline...
2021-06-04 13:55:54 | [train_policy] epoch #605 | Saving snapshot...
2021-06-04 13:55:54 | [train_policy] epoch #605 | Saved
2021-06-04 13:55:54 | [train_policy] epoch #605 | Time 485.99 s
2021-06-04 13:55:54 | [train_policy] epoch #605 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28523
Evaluation/AverageDiscountedReturn          -42.9554
Evaluation/AverageReturn                    -42.9554
Evaluation/CompletionRate                     0
Evaluation/Iteration                        605
Evaluation/MaxReturn                        -28.5158
Evaluation/MinReturn                        -76.8464
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.40852
Extras/EpisodeRewardMean                    -42.4199
LinearFeatureBaseline/ExplainedVariance       0.881045
PolicyExecTime                                0.225033
ProcessExecTime                               0.03124
TotalEnvSteps                            613272
policy/Entropy                               -1.50361
policy/KL                                     0.0065617
policy/KLBefore                               0
policy/LossAfter                             -0.0158395
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.222327
policy/dLoss                                  0.0158395
---------------------------------------  ----------------
2021-06-04 13:55:54 | [train_policy] epoch #606 | Obtaining samples for iteration 606...
2021-06-04 13:55:54 | [train_policy] epoch #606 | Logging diagnostics...
2021-06-04 13:55:54 | [train_policy] epoch #606 | Optimizing policy...
2021-06-04 13:55:54 | [train_policy] epoch #606 | Computing loss before
2021-06-04 13:55:54 | [train_policy] epoch #606 | Computing KL before
2021-06-04 13:55:54 | [train_policy] epoch #606 | Optimizing
2021-06-04 13:55:54 | [train_policy] epoch #606 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:54 | [train_policy] epoch #606 | computing loss before
2021-06-04 13:55:54 | [train_policy] epoch #606 | computing gradient
2021-06-04 13:55:54 | [train_policy] epoch #606 | gradient computed
2021-06-04 13:55:54 | [train_policy] epoch #606 | computing descent direction
2021-06-04 13:55:54 | [train_policy] epoch #606 | descent direction computed
2021-06-04 13:55:55 | [train_policy] epoch #606 | backtrack iters: 1
2021-06-04 13:55:55 | [train_policy] epoch #606 | optimization finished
2021-06-04 13:55:55 | [train_policy] epoch #606 | Computing KL after
2021-06-04 13:55:55 | [train_policy] epoch #606 | Computing loss after
2021-06-04 13:55:55 | [train_policy] epoch #606 | Fitting baseline...
2021-06-04 13:55:55 | [train_policy] epoch #606 | Saving snapshot...
2021-06-04 13:55:55 | [train_policy] epoch #606 | Saved
2021-06-04 13:55:55 | [train_policy] epoch #606 | Time 486.79 s
2021-06-04 13:55:55 | [train_policy] epoch #606 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283159
Evaluation/AverageDiscountedReturn          -40.9017
Evaluation/AverageReturn                    -40.9017
Evaluation/CompletionRate                     0
Evaluation/Iteration                        606
Evaluation/MaxReturn                        -30.5991
Evaluation/MinReturn                        -64.1784
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.86802
Extras/EpisodeRewardMean                    -41.505
LinearFeatureBaseline/ExplainedVariance       0.877814
PolicyExecTime                                0.212361
ProcessExecTime                               0.0311947
TotalEnvSteps                            614284
policy/Entropy                               -1.51797
policy/KL                                     0.00655395
policy/KLBefore                               0
policy/LossAfter                             -0.0172462
policy/LossBefore                            -1.38999e-08
policy/Perplexity                             0.219155
policy/dLoss                                  0.0172462
---------------------------------------  ----------------
2021-06-04 13:55:55 | [train_policy] epoch #607 | Obtaining samples for iteration 607...
2021-06-04 13:55:55 | [train_policy] epoch #607 | Logging diagnostics...
2021-06-04 13:55:55 | [train_policy] epoch #607 | Optimizing policy...
2021-06-04 13:55:55 | [train_policy] epoch #607 | Computing loss before
2021-06-04 13:55:55 | [train_policy] epoch #607 | Computing KL before
2021-06-04 13:55:55 | [train_policy] epoch #607 | Optimizing
2021-06-04 13:55:55 | [train_policy] epoch #607 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:55 | [train_policy] epoch #607 | computing loss before
2021-06-04 13:55:55 | [train_policy] epoch #607 | computing gradient
2021-06-04 13:55:55 | [train_policy] epoch #607 | gradient computed
2021-06-04 13:55:55 | [train_policy] epoch #607 | computing descent direction
2021-06-04 13:55:55 | [train_policy] epoch #607 | descent direction computed
2021-06-04 13:55:55 | [train_policy] epoch #607 | backtrack iters: 0
2021-06-04 13:55:55 | [train_policy] epoch #607 | optimization finished
2021-06-04 13:55:55 | [train_policy] epoch #607 | Computing KL after
2021-06-04 13:55:55 | [train_policy] epoch #607 | Computing loss after
2021-06-04 13:55:55 | [train_policy] epoch #607 | Fitting baseline...
2021-06-04 13:55:55 | [train_policy] epoch #607 | Saving snapshot...
2021-06-04 13:55:55 | [train_policy] epoch #607 | Saved
2021-06-04 13:55:55 | [train_policy] epoch #607 | Time 487.59 s
2021-06-04 13:55:55 | [train_policy] epoch #607 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28587
Evaluation/AverageDiscountedReturn          -41.6863
Evaluation/AverageReturn                    -41.6863
Evaluation/CompletionRate                     0
Evaluation/Iteration                        607
Evaluation/MaxReturn                        -29.4299
Evaluation/MinReturn                        -76.5421
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.73587
Extras/EpisodeRewardMean                    -41.9483
LinearFeatureBaseline/ExplainedVariance       0.883067
PolicyExecTime                                0.22406
ProcessExecTime                               0.0312743
TotalEnvSteps                            615296
policy/Entropy                               -1.50883
policy/KL                                     0.00968155
policy/KLBefore                               0
policy/LossAfter                             -0.0226662
policy/LossBefore                            -1.64914e-09
policy/Perplexity                             0.221169
policy/dLoss                                  0.0226662
---------------------------------------  ----------------
2021-06-04 13:55:55 | [train_policy] epoch #608 | Obtaining samples for iteration 608...
2021-06-04 13:55:56 | [train_policy] epoch #608 | Logging diagnostics...
2021-06-04 13:55:56 | [train_policy] epoch #608 | Optimizing policy...
2021-06-04 13:55:56 | [train_policy] epoch #608 | Computing loss before
2021-06-04 13:55:56 | [train_policy] epoch #608 | Computing KL before
2021-06-04 13:55:56 | [train_policy] epoch #608 | Optimizing
2021-06-04 13:55:56 | [train_policy] epoch #608 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:56 | [train_policy] epoch #608 | computing loss before
2021-06-04 13:55:56 | [train_policy] epoch #608 | computing gradient
2021-06-04 13:55:56 | [train_policy] epoch #608 | gradient computed
2021-06-04 13:55:56 | [train_policy] epoch #608 | computing descent direction
2021-06-04 13:55:56 | [train_policy] epoch #608 | descent direction computed
2021-06-04 13:55:56 | [train_policy] epoch #608 | backtrack iters: 0
2021-06-04 13:55:56 | [train_policy] epoch #608 | optimization finished
2021-06-04 13:55:56 | [train_policy] epoch #608 | Computing KL after
2021-06-04 13:55:56 | [train_policy] epoch #608 | Computing loss after
2021-06-04 13:55:56 | [train_policy] epoch #608 | Fitting baseline...
2021-06-04 13:55:56 | [train_policy] epoch #608 | Saving snapshot...
2021-06-04 13:55:56 | [train_policy] epoch #608 | Saved
2021-06-04 13:55:56 | [train_policy] epoch #608 | Time 488.39 s
2021-06-04 13:55:56 | [train_policy] epoch #608 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285189
Evaluation/AverageDiscountedReturn          -42.1771
Evaluation/AverageReturn                    -42.1771
Evaluation/CompletionRate                     0
Evaluation/Iteration                        608
Evaluation/MaxReturn                        -30.8045
Evaluation/MinReturn                        -60.3738
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.78654
Extras/EpisodeRewardMean                    -42.2372
LinearFeatureBaseline/ExplainedVariance       0.882558
PolicyExecTime                                0.230682
ProcessExecTime                               0.031147
TotalEnvSteps                            616308
policy/Entropy                               -1.50948
policy/KL                                     0.00977985
policy/KLBefore                               0
policy/LossAfter                             -0.0272112
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.221024
policy/dLoss                                  0.0272112
---------------------------------------  ----------------
2021-06-04 13:55:56 | [train_policy] epoch #609 | Obtaining samples for iteration 609...
2021-06-04 13:55:57 | [train_policy] epoch #609 | Logging diagnostics...
2021-06-04 13:55:57 | [train_policy] epoch #609 | Optimizing policy...
2021-06-04 13:55:57 | [train_policy] epoch #609 | Computing loss before
2021-06-04 13:55:57 | [train_policy] epoch #609 | Computing KL before
2021-06-04 13:55:57 | [train_policy] epoch #609 | Optimizing
2021-06-04 13:55:57 | [train_policy] epoch #609 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:57 | [train_policy] epoch #609 | computing loss before
2021-06-04 13:55:57 | [train_policy] epoch #609 | computing gradient
2021-06-04 13:55:57 | [train_policy] epoch #609 | gradient computed
2021-06-04 13:55:57 | [train_policy] epoch #609 | computing descent direction
2021-06-04 13:55:57 | [train_policy] epoch #609 | descent direction computed
2021-06-04 13:55:57 | [train_policy] epoch #609 | backtrack iters: 1
2021-06-04 13:55:57 | [train_policy] epoch #609 | optimization finished
2021-06-04 13:55:57 | [train_policy] epoch #609 | Computing KL after
2021-06-04 13:55:57 | [train_policy] epoch #609 | Computing loss after
2021-06-04 13:55:57 | [train_policy] epoch #609 | Fitting baseline...
2021-06-04 13:55:57 | [train_policy] epoch #609 | Saving snapshot...
2021-06-04 13:55:57 | [train_policy] epoch #609 | Saved
2021-06-04 13:55:57 | [train_policy] epoch #609 | Time 489.22 s
2021-06-04 13:55:57 | [train_policy] epoch #609 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                   0.283262
Evaluation/AverageDiscountedReturn          -44.2556
Evaluation/AverageReturn                    -44.2556
Evaluation/CompletionRate                     0
Evaluation/Iteration                        609
Evaluation/MaxReturn                        -29.988
Evaluation/MinReturn                        -63.3827
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.66346
Extras/EpisodeRewardMean                    -44.0776
LinearFeatureBaseline/ExplainedVariance       0.887298
PolicyExecTime                                0.233742
ProcessExecTime                               0.0309737
TotalEnvSteps                            617320
policy/Entropy                               -1.52141
policy/KL                                     0.00643473
policy/KLBefore                               0
policy/LossAfter                             -0.0181601
policy/LossBefore                            -5.4186e-09
policy/Perplexity                             0.218404
policy/dLoss                                  0.0181601
---------------------------------------  ---------------
2021-06-04 13:55:57 | [train_policy] epoch #610 | Obtaining samples for iteration 610...
2021-06-04 13:55:58 | [train_policy] epoch #610 | Logging diagnostics...
2021-06-04 13:55:58 | [train_policy] epoch #610 | Optimizing policy...
2021-06-04 13:55:58 | [train_policy] epoch #610 | Computing loss before
2021-06-04 13:55:58 | [train_policy] epoch #610 | Computing KL before
2021-06-04 13:55:58 | [train_policy] epoch #610 | Optimizing
2021-06-04 13:55:58 | [train_policy] epoch #610 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:58 | [train_policy] epoch #610 | computing loss before
2021-06-04 13:55:58 | [train_policy] epoch #610 | computing gradient
2021-06-04 13:55:58 | [train_policy] epoch #610 | gradient computed
2021-06-04 13:55:58 | [train_policy] epoch #610 | computing descent direction
2021-06-04 13:55:58 | [train_policy] epoch #610 | descent direction computed
2021-06-04 13:55:58 | [train_policy] epoch #610 | backtrack iters: 0
2021-06-04 13:55:58 | [train_policy] epoch #610 | optimization finished
2021-06-04 13:55:58 | [train_policy] epoch #610 | Computing KL after
2021-06-04 13:55:58 | [train_policy] epoch #610 | Computing loss after
2021-06-04 13:55:58 | [train_policy] epoch #610 | Fitting baseline...
2021-06-04 13:55:58 | [train_policy] epoch #610 | Saving snapshot...
2021-06-04 13:55:58 | [train_policy] epoch #610 | Saved
2021-06-04 13:55:58 | [train_policy] epoch #610 | Time 490.03 s
2021-06-04 13:55:58 | [train_policy] epoch #610 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.28489
Evaluation/AverageDiscountedReturn          -42.5584
Evaluation/AverageReturn                    -42.5584
Evaluation/CompletionRate                     0
Evaluation/Iteration                        610
Evaluation/MaxReturn                        -30.6459
Evaluation/MinReturn                        -76.8167
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.35517
Extras/EpisodeRewardMean                    -42.8223
LinearFeatureBaseline/ExplainedVariance       0.853264
PolicyExecTime                                0.237788
ProcessExecTime                               0.0312786
TotalEnvSteps                            618332
policy/Entropy                               -1.51003
policy/KL                                     0.00988296
policy/KLBefore                               0
policy/LossAfter                             -0.0214281
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             0.220903
policy/dLoss                                  0.0214281
---------------------------------------  ----------------
2021-06-04 13:55:58 | [train_policy] epoch #611 | Obtaining samples for iteration 611...
2021-06-04 13:55:58 | [train_policy] epoch #611 | Logging diagnostics...
2021-06-04 13:55:58 | [train_policy] epoch #611 | Optimizing policy...
2021-06-04 13:55:58 | [train_policy] epoch #611 | Computing loss before
2021-06-04 13:55:58 | [train_policy] epoch #611 | Computing KL before
2021-06-04 13:55:58 | [train_policy] epoch #611 | Optimizing
2021-06-04 13:55:58 | [train_policy] epoch #611 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:58 | [train_policy] epoch #611 | computing loss before
2021-06-04 13:55:58 | [train_policy] epoch #611 | computing gradient
2021-06-04 13:55:58 | [train_policy] epoch #611 | gradient computed
2021-06-04 13:55:58 | [train_policy] epoch #611 | computing descent direction
2021-06-04 13:55:59 | [train_policy] epoch #611 | descent direction computed
2021-06-04 13:55:59 | [train_policy] epoch #611 | backtrack iters: 1
2021-06-04 13:55:59 | [train_policy] epoch #611 | optimization finished
2021-06-04 13:55:59 | [train_policy] epoch #611 | Computing KL after
2021-06-04 13:55:59 | [train_policy] epoch #611 | Computing loss after
2021-06-04 13:55:59 | [train_policy] epoch #611 | Fitting baseline...
2021-06-04 13:55:59 | [train_policy] epoch #611 | Saving snapshot...
2021-06-04 13:55:59 | [train_policy] epoch #611 | Saved
2021-06-04 13:55:59 | [train_policy] epoch #611 | Time 490.84 s
2021-06-04 13:55:59 | [train_policy] epoch #611 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284925
Evaluation/AverageDiscountedReturn          -42.1169
Evaluation/AverageReturn                    -42.1169
Evaluation/CompletionRate                     0
Evaluation/Iteration                        611
Evaluation/MaxReturn                        -30.675
Evaluation/MinReturn                        -60.6103
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.64891
Extras/EpisodeRewardMean                    -41.8632
LinearFeatureBaseline/ExplainedVariance       0.907097
PolicyExecTime                                0.233935
ProcessExecTime                               0.0312245
TotalEnvSteps                            619344
policy/Entropy                               -1.51123
policy/KL                                     0.00647942
policy/KLBefore                               0
policy/LossAfter                             -0.0186498
policy/LossBefore                             1.17796e-09
policy/Perplexity                             0.220638
policy/dLoss                                  0.0186498
---------------------------------------  ----------------
2021-06-04 13:55:59 | [train_policy] epoch #612 | Obtaining samples for iteration 612...
2021-06-04 13:55:59 | [train_policy] epoch #612 | Logging diagnostics...
2021-06-04 13:55:59 | [train_policy] epoch #612 | Optimizing policy...
2021-06-04 13:55:59 | [train_policy] epoch #612 | Computing loss before
2021-06-04 13:55:59 | [train_policy] epoch #612 | Computing KL before
2021-06-04 13:55:59 | [train_policy] epoch #612 | Optimizing
2021-06-04 13:55:59 | [train_policy] epoch #612 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:55:59 | [train_policy] epoch #612 | computing loss before
2021-06-04 13:55:59 | [train_policy] epoch #612 | computing gradient
2021-06-04 13:55:59 | [train_policy] epoch #612 | gradient computed
2021-06-04 13:55:59 | [train_policy] epoch #612 | computing descent direction
2021-06-04 13:55:59 | [train_policy] epoch #612 | descent direction computed
2021-06-04 13:55:59 | [train_policy] epoch #612 | backtrack iters: 0
2021-06-04 13:55:59 | [train_policy] epoch #612 | optimization finished
2021-06-04 13:55:59 | [train_policy] epoch #612 | Computing KL after
2021-06-04 13:55:59 | [train_policy] epoch #612 | Computing loss after
2021-06-04 13:55:59 | [train_policy] epoch #612 | Fitting baseline...
2021-06-04 13:55:59 | [train_policy] epoch #612 | Saving snapshot...
2021-06-04 13:55:59 | [train_policy] epoch #612 | Saved
2021-06-04 13:55:59 | [train_policy] epoch #612 | Time 491.64 s
2021-06-04 13:55:59 | [train_policy] epoch #612 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285589
Evaluation/AverageDiscountedReturn          -42.5225
Evaluation/AverageReturn                    -42.5225
Evaluation/CompletionRate                     0
Evaluation/Iteration                        612
Evaluation/MaxReturn                        -31.7354
Evaluation/MinReturn                        -75.1291
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.76983
Extras/EpisodeRewardMean                    -42.2818
LinearFeatureBaseline/ExplainedVariance       0.885248
PolicyExecTime                                0.226732
ProcessExecTime                               0.0312617
TotalEnvSteps                            620356
policy/Entropy                               -1.50768
policy/KL                                     0.00996275
policy/KLBefore                               0
policy/LossAfter                             -0.0226928
policy/LossBefore                             1.88473e-08
policy/Perplexity                             0.221423
policy/dLoss                                  0.0226928
---------------------------------------  ----------------
2021-06-04 13:55:59 | [train_policy] epoch #613 | Obtaining samples for iteration 613...
2021-06-04 13:56:00 | [train_policy] epoch #613 | Logging diagnostics...
2021-06-04 13:56:00 | [train_policy] epoch #613 | Optimizing policy...
2021-06-04 13:56:00 | [train_policy] epoch #613 | Computing loss before
2021-06-04 13:56:00 | [train_policy] epoch #613 | Computing KL before
2021-06-04 13:56:00 | [train_policy] epoch #613 | Optimizing
2021-06-04 13:56:00 | [train_policy] epoch #613 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:00 | [train_policy] epoch #613 | computing loss before
2021-06-04 13:56:00 | [train_policy] epoch #613 | computing gradient
2021-06-04 13:56:00 | [train_policy] epoch #613 | gradient computed
2021-06-04 13:56:00 | [train_policy] epoch #613 | computing descent direction
2021-06-04 13:56:00 | [train_policy] epoch #613 | descent direction computed
2021-06-04 13:56:00 | [train_policy] epoch #613 | backtrack iters: 0
2021-06-04 13:56:00 | [train_policy] epoch #613 | optimization finished
2021-06-04 13:56:00 | [train_policy] epoch #613 | Computing KL after
2021-06-04 13:56:00 | [train_policy] epoch #613 | Computing loss after
2021-06-04 13:56:00 | [train_policy] epoch #613 | Fitting baseline...
2021-06-04 13:56:00 | [train_policy] epoch #613 | Saving snapshot...
2021-06-04 13:56:00 | [train_policy] epoch #613 | Saved
2021-06-04 13:56:00 | [train_policy] epoch #613 | Time 492.44 s
2021-06-04 13:56:00 | [train_policy] epoch #613 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287038
Evaluation/AverageDiscountedReturn          -41.7903
Evaluation/AverageReturn                    -41.7903
Evaluation/CompletionRate                     0
Evaluation/Iteration                        613
Evaluation/MaxReturn                        -29.6986
Evaluation/MinReturn                        -64.1494
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.82325
Extras/EpisodeRewardMean                    -41.8104
LinearFeatureBaseline/ExplainedVariance       0.896646
PolicyExecTime                                0.226825
ProcessExecTime                               0.0314159
TotalEnvSteps                            621368
policy/Entropy                               -1.51744
policy/KL                                     0.00982733
policy/KLBefore                               0
policy/LossAfter                             -0.0182968
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.219273
policy/dLoss                                  0.0182968
---------------------------------------  ----------------
2021-06-04 13:56:00 | [train_policy] epoch #614 | Obtaining samples for iteration 614...
2021-06-04 13:56:01 | [train_policy] epoch #614 | Logging diagnostics...
2021-06-04 13:56:01 | [train_policy] epoch #614 | Optimizing policy...
2021-06-04 13:56:01 | [train_policy] epoch #614 | Computing loss before
2021-06-04 13:56:01 | [train_policy] epoch #614 | Computing KL before
2021-06-04 13:56:01 | [train_policy] epoch #614 | Optimizing
2021-06-04 13:56:01 | [train_policy] epoch #614 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:01 | [train_policy] epoch #614 | computing loss before
2021-06-04 13:56:01 | [train_policy] epoch #614 | computing gradient
2021-06-04 13:56:01 | [train_policy] epoch #614 | gradient computed
2021-06-04 13:56:01 | [train_policy] epoch #614 | computing descent direction
2021-06-04 13:56:01 | [train_policy] epoch #614 | descent direction computed
2021-06-04 13:56:01 | [train_policy] epoch #614 | backtrack iters: 0
2021-06-04 13:56:01 | [train_policy] epoch #614 | optimization finished
2021-06-04 13:56:01 | [train_policy] epoch #614 | Computing KL after
2021-06-04 13:56:01 | [train_policy] epoch #614 | Computing loss after
2021-06-04 13:56:01 | [train_policy] epoch #614 | Fitting baseline...
2021-06-04 13:56:01 | [train_policy] epoch #614 | Saving snapshot...
2021-06-04 13:56:01 | [train_policy] epoch #614 | Saved
2021-06-04 13:56:01 | [train_policy] epoch #614 | Time 493.25 s
2021-06-04 13:56:01 | [train_policy] epoch #614 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285363
Evaluation/AverageDiscountedReturn          -42.7762
Evaluation/AverageReturn                    -42.7762
Evaluation/CompletionRate                     0
Evaluation/Iteration                        614
Evaluation/MaxReturn                        -31.2853
Evaluation/MinReturn                        -64.324
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.76343
Extras/EpisodeRewardMean                    -42.681
LinearFeatureBaseline/ExplainedVariance       0.906895
PolicyExecTime                                0.228256
ProcessExecTime                               0.0311317
TotalEnvSteps                            622380
policy/Entropy                               -1.4983
policy/KL                                     0.00963431
policy/KLBefore                               0
policy/LossAfter                             -0.0166178
policy/LossBefore                             1.00126e-09
policy/Perplexity                             0.223509
policy/dLoss                                  0.0166178
---------------------------------------  ----------------
2021-06-04 13:56:01 | [train_policy] epoch #615 | Obtaining samples for iteration 615...
2021-06-04 13:56:02 | [train_policy] epoch #615 | Logging diagnostics...
2021-06-04 13:56:02 | [train_policy] epoch #615 | Optimizing policy...
2021-06-04 13:56:02 | [train_policy] epoch #615 | Computing loss before
2021-06-04 13:56:02 | [train_policy] epoch #615 | Computing KL before
2021-06-04 13:56:02 | [train_policy] epoch #615 | Optimizing
2021-06-04 13:56:02 | [train_policy] epoch #615 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:02 | [train_policy] epoch #615 | computing loss before
2021-06-04 13:56:02 | [train_policy] epoch #615 | computing gradient
2021-06-04 13:56:02 | [train_policy] epoch #615 | gradient computed
2021-06-04 13:56:02 | [train_policy] epoch #615 | computing descent direction
2021-06-04 13:56:02 | [train_policy] epoch #615 | descent direction computed
2021-06-04 13:56:02 | [train_policy] epoch #615 | backtrack iters: 1
2021-06-04 13:56:02 | [train_policy] epoch #615 | optimization finished
2021-06-04 13:56:02 | [train_policy] epoch #615 | Computing KL after
2021-06-04 13:56:02 | [train_policy] epoch #615 | Computing loss after
2021-06-04 13:56:02 | [train_policy] epoch #615 | Fitting baseline...
2021-06-04 13:56:02 | [train_policy] epoch #615 | Saving snapshot...
2021-06-04 13:56:02 | [train_policy] epoch #615 | Saved
2021-06-04 13:56:02 | [train_policy] epoch #615 | Time 494.09 s
2021-06-04 13:56:02 | [train_policy] epoch #615 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.287467
Evaluation/AverageDiscountedReturn          -42.2598
Evaluation/AverageReturn                    -42.2598
Evaluation/CompletionRate                     0
Evaluation/Iteration                        615
Evaluation/MaxReturn                        -30.7624
Evaluation/MinReturn                        -76.8325
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.4917
Extras/EpisodeRewardMean                    -42.0803
LinearFeatureBaseline/ExplainedVariance       0.899385
PolicyExecTime                                0.236041
ProcessExecTime                               0.0314586
TotalEnvSteps                            623392
policy/Entropy                               -1.52754
policy/KL                                     0.00664551
policy/KLBefore                               0
policy/LossAfter                             -0.019122
policy/LossBefore                            -1.62558e-08
policy/Perplexity                             0.217069
policy/dLoss                                  0.019122
---------------------------------------  ----------------
2021-06-04 13:56:02 | [train_policy] epoch #616 | Obtaining samples for iteration 616...
2021-06-04 13:56:03 | [train_policy] epoch #616 | Logging diagnostics...
2021-06-04 13:56:03 | [train_policy] epoch #616 | Optimizing policy...
2021-06-04 13:56:03 | [train_policy] epoch #616 | Computing loss before
2021-06-04 13:56:03 | [train_policy] epoch #616 | Computing KL before
2021-06-04 13:56:03 | [train_policy] epoch #616 | Optimizing
2021-06-04 13:56:03 | [train_policy] epoch #616 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:03 | [train_policy] epoch #616 | computing loss before
2021-06-04 13:56:03 | [train_policy] epoch #616 | computing gradient
2021-06-04 13:56:03 | [train_policy] epoch #616 | gradient computed
2021-06-04 13:56:03 | [train_policy] epoch #616 | computing descent direction
2021-06-04 13:56:03 | [train_policy] epoch #616 | descent direction computed
2021-06-04 13:56:03 | [train_policy] epoch #616 | backtrack iters: 0
2021-06-04 13:56:03 | [train_policy] epoch #616 | optimization finished
2021-06-04 13:56:03 | [train_policy] epoch #616 | Computing KL after
2021-06-04 13:56:03 | [train_policy] epoch #616 | Computing loss after
2021-06-04 13:56:03 | [train_policy] epoch #616 | Fitting baseline...
2021-06-04 13:56:03 | [train_policy] epoch #616 | Saving snapshot...
2021-06-04 13:56:03 | [train_policy] epoch #616 | Saved
2021-06-04 13:56:03 | [train_policy] epoch #616 | Time 494.89 s
2021-06-04 13:56:03 | [train_policy] epoch #616 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283351
Evaluation/AverageDiscountedReturn          -43.4222
Evaluation/AverageReturn                    -43.4222
Evaluation/CompletionRate                     0
Evaluation/Iteration                        616
Evaluation/MaxReturn                        -30.2178
Evaluation/MinReturn                        -76.8069
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.69016
Extras/EpisodeRewardMean                    -43.5423
LinearFeatureBaseline/ExplainedVariance       0.828974
PolicyExecTime                                0.22804
ProcessExecTime                               0.0311334
TotalEnvSteps                            624404
policy/Entropy                               -1.5252
policy/KL                                     0.00979765
policy/KLBefore                               0
policy/LossAfter                             -0.0164488
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             0.217578
policy/dLoss                                  0.0164488
---------------------------------------  ----------------
2021-06-04 13:56:03 | [train_policy] epoch #617 | Obtaining samples for iteration 617...
2021-06-04 13:56:03 | [train_policy] epoch #617 | Logging diagnostics...
2021-06-04 13:56:03 | [train_policy] epoch #617 | Optimizing policy...
2021-06-04 13:56:03 | [train_policy] epoch #617 | Computing loss before
2021-06-04 13:56:03 | [train_policy] epoch #617 | Computing KL before
2021-06-04 13:56:03 | [train_policy] epoch #617 | Optimizing
2021-06-04 13:56:03 | [train_policy] epoch #617 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:03 | [train_policy] epoch #617 | computing loss before
2021-06-04 13:56:03 | [train_policy] epoch #617 | computing gradient
2021-06-04 13:56:03 | [train_policy] epoch #617 | gradient computed
2021-06-04 13:56:03 | [train_policy] epoch #617 | computing descent direction
2021-06-04 13:56:03 | [train_policy] epoch #617 | descent direction computed
2021-06-04 13:56:03 | [train_policy] epoch #617 | backtrack iters: 0
2021-06-04 13:56:03 | [train_policy] epoch #617 | optimization finished
2021-06-04 13:56:03 | [train_policy] epoch #617 | Computing KL after
2021-06-04 13:56:03 | [train_policy] epoch #617 | Computing loss after
2021-06-04 13:56:03 | [train_policy] epoch #617 | Fitting baseline...
2021-06-04 13:56:03 | [train_policy] epoch #617 | Saving snapshot...
2021-06-04 13:56:03 | [train_policy] epoch #617 | Saved
2021-06-04 13:56:03 | [train_policy] epoch #617 | Time 495.69 s
2021-06-04 13:56:03 | [train_policy] epoch #617 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.283483
Evaluation/AverageDiscountedReturn          -42.0389
Evaluation/AverageReturn                    -42.0389
Evaluation/CompletionRate                     0
Evaluation/Iteration                        617
Evaluation/MaxReturn                        -30.4537
Evaluation/MinReturn                        -63.1433
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.44151
Extras/EpisodeRewardMean                    -42.0567
LinearFeatureBaseline/ExplainedVariance       0.892335
PolicyExecTime                                0.221185
ProcessExecTime                               0.0312345
TotalEnvSteps                            625416
policy/Entropy                               -1.47535
policy/KL                                     0.00942335
policy/KLBefore                               0
policy/LossAfter                             -0.014116
policy/LossBefore                            -1.0955e-08
policy/Perplexity                             0.228698
policy/dLoss                                  0.014116
---------------------------------------  ---------------
2021-06-04 13:56:03 | [train_policy] epoch #618 | Obtaining samples for iteration 618...
2021-06-04 13:56:04 | [train_policy] epoch #618 | Logging diagnostics...
2021-06-04 13:56:04 | [train_policy] epoch #618 | Optimizing policy...
2021-06-04 13:56:04 | [train_policy] epoch #618 | Computing loss before
2021-06-04 13:56:04 | [train_policy] epoch #618 | Computing KL before
2021-06-04 13:56:04 | [train_policy] epoch #618 | Optimizing
2021-06-04 13:56:04 | [train_policy] epoch #618 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:04 | [train_policy] epoch #618 | computing loss before
2021-06-04 13:56:04 | [train_policy] epoch #618 | computing gradient
2021-06-04 13:56:04 | [train_policy] epoch #618 | gradient computed
2021-06-04 13:56:04 | [train_policy] epoch #618 | computing descent direction
2021-06-04 13:56:04 | [train_policy] epoch #618 | descent direction computed
2021-06-04 13:56:04 | [train_policy] epoch #618 | backtrack iters: 0
2021-06-04 13:56:04 | [train_policy] epoch #618 | optimization finished
2021-06-04 13:56:04 | [train_policy] epoch #618 | Computing KL after
2021-06-04 13:56:04 | [train_policy] epoch #618 | Computing loss after
2021-06-04 13:56:04 | [train_policy] epoch #618 | Fitting baseline...
2021-06-04 13:56:04 | [train_policy] epoch #618 | Saving snapshot...
2021-06-04 13:56:04 | [train_policy] epoch #618 | Saved
2021-06-04 13:56:04 | [train_policy] epoch #618 | Time 496.47 s
2021-06-04 13:56:04 | [train_policy] epoch #618 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.28282
Evaluation/AverageDiscountedReturn          -42.62
Evaluation/AverageReturn                    -42.62
Evaluation/CompletionRate                     0
Evaluation/Iteration                        618
Evaluation/MaxReturn                        -31.1047
Evaluation/MinReturn                        -64.1622
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.01235
Extras/EpisodeRewardMean                    -42.5119
LinearFeatureBaseline/ExplainedVariance       0.907224
PolicyExecTime                                0.212681
ProcessExecTime                               0.0310142
TotalEnvSteps                            626428
policy/Entropy                               -1.47403
policy/KL                                     0.00989936
policy/KLBefore                               0
policy/LossAfter                             -0.0195996
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.229002
policy/dLoss                                  0.0195996
---------------------------------------  ----------------
2021-06-04 13:56:04 | [train_policy] epoch #619 | Obtaining samples for iteration 619...
2021-06-04 13:56:05 | [train_policy] epoch #619 | Logging diagnostics...
2021-06-04 13:56:05 | [train_policy] epoch #619 | Optimizing policy...
2021-06-04 13:56:05 | [train_policy] epoch #619 | Computing loss before
2021-06-04 13:56:05 | [train_policy] epoch #619 | Computing KL before
2021-06-04 13:56:05 | [train_policy] epoch #619 | Optimizing
2021-06-04 13:56:05 | [train_policy] epoch #619 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:05 | [train_policy] epoch #619 | computing loss before
2021-06-04 13:56:05 | [train_policy] epoch #619 | computing gradient
2021-06-04 13:56:05 | [train_policy] epoch #619 | gradient computed
2021-06-04 13:56:05 | [train_policy] epoch #619 | computing descent direction
2021-06-04 13:56:05 | [train_policy] epoch #619 | descent direction computed
2021-06-04 13:56:05 | [train_policy] epoch #619 | backtrack iters: 0
2021-06-04 13:56:05 | [train_policy] epoch #619 | optimization finished
2021-06-04 13:56:05 | [train_policy] epoch #619 | Computing KL after
2021-06-04 13:56:05 | [train_policy] epoch #619 | Computing loss after
2021-06-04 13:56:05 | [train_policy] epoch #619 | Fitting baseline...
2021-06-04 13:56:05 | [train_policy] epoch #619 | Saving snapshot...
2021-06-04 13:56:05 | [train_policy] epoch #619 | Saved
2021-06-04 13:56:05 | [train_policy] epoch #619 | Time 497.28 s
2021-06-04 13:56:05 | [train_policy] epoch #619 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285402
Evaluation/AverageDiscountedReturn          -44.0074
Evaluation/AverageReturn                    -44.0074
Evaluation/CompletionRate                     0
Evaluation/Iteration                        619
Evaluation/MaxReturn                        -30.5413
Evaluation/MinReturn                        -76.1846
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.99208
Extras/EpisodeRewardMean                    -43.8235
LinearFeatureBaseline/ExplainedVariance       0.896363
PolicyExecTime                                0.229219
ProcessExecTime                               0.0314035
TotalEnvSteps                            627440
policy/Entropy                               -1.50888
policy/KL                                     0.00998554
policy/KLBefore                               0
policy/LossAfter                             -0.0185802
policy/LossBefore                            -2.45015e-08
policy/Perplexity                             0.221157
policy/dLoss                                  0.0185802
---------------------------------------  ----------------
2021-06-04 13:56:05 | [train_policy] epoch #620 | Obtaining samples for iteration 620...
2021-06-04 13:56:06 | [train_policy] epoch #620 | Logging diagnostics...
2021-06-04 13:56:06 | [train_policy] epoch #620 | Optimizing policy...
2021-06-04 13:56:06 | [train_policy] epoch #620 | Computing loss before
2021-06-04 13:56:06 | [train_policy] epoch #620 | Computing KL before
2021-06-04 13:56:06 | [train_policy] epoch #620 | Optimizing
2021-06-04 13:56:06 | [train_policy] epoch #620 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:06 | [train_policy] epoch #620 | computing loss before
2021-06-04 13:56:06 | [train_policy] epoch #620 | computing gradient
2021-06-04 13:56:06 | [train_policy] epoch #620 | gradient computed
2021-06-04 13:56:06 | [train_policy] epoch #620 | computing descent direction
2021-06-04 13:56:06 | [train_policy] epoch #620 | descent direction computed
2021-06-04 13:56:06 | [train_policy] epoch #620 | backtrack iters: 1
2021-06-04 13:56:06 | [train_policy] epoch #620 | optimization finished
2021-06-04 13:56:06 | [train_policy] epoch #620 | Computing KL after
2021-06-04 13:56:06 | [train_policy] epoch #620 | Computing loss after
2021-06-04 13:56:06 | [train_policy] epoch #620 | Fitting baseline...
2021-06-04 13:56:06 | [train_policy] epoch #620 | Saving snapshot...
2021-06-04 13:56:06 | [train_policy] epoch #620 | Saved
2021-06-04 13:56:06 | [train_policy] epoch #620 | Time 498.11 s
2021-06-04 13:56:06 | [train_policy] epoch #620 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.28653
Evaluation/AverageDiscountedReturn          -42.7508
Evaluation/AverageReturn                    -42.7508
Evaluation/CompletionRate                     0
Evaluation/Iteration                        620
Evaluation/MaxReturn                        -30.7693
Evaluation/MinReturn                        -79.647
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.87535
Extras/EpisodeRewardMean                    -42.6207
LinearFeatureBaseline/ExplainedVariance       0.854395
PolicyExecTime                                0.234873
ProcessExecTime                               0.0315063
TotalEnvSteps                            628452
policy/Entropy                               -1.53821
policy/KL                                     0.00671627
policy/KLBefore                               0
policy/LossAfter                             -0.0148621
policy/LossBefore                             2.63862e-08
policy/Perplexity                             0.214766
policy/dLoss                                  0.0148622
---------------------------------------  ----------------
2021-06-04 13:56:06 | [train_policy] epoch #621 | Obtaining samples for iteration 621...
2021-06-04 13:56:07 | [train_policy] epoch #621 | Logging diagnostics...
2021-06-04 13:56:07 | [train_policy] epoch #621 | Optimizing policy...
2021-06-04 13:56:07 | [train_policy] epoch #621 | Computing loss before
2021-06-04 13:56:07 | [train_policy] epoch #621 | Computing KL before
2021-06-04 13:56:07 | [train_policy] epoch #621 | Optimizing
2021-06-04 13:56:07 | [train_policy] epoch #621 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:07 | [train_policy] epoch #621 | computing loss before
2021-06-04 13:56:07 | [train_policy] epoch #621 | computing gradient
2021-06-04 13:56:07 | [train_policy] epoch #621 | gradient computed
2021-06-04 13:56:07 | [train_policy] epoch #621 | computing descent direction
2021-06-04 13:56:07 | [train_policy] epoch #621 | descent direction computed
2021-06-04 13:56:07 | [train_policy] epoch #621 | backtrack iters: 0
2021-06-04 13:56:07 | [train_policy] epoch #621 | optimization finished
2021-06-04 13:56:07 | [train_policy] epoch #621 | Computing KL after
2021-06-04 13:56:07 | [train_policy] epoch #621 | Computing loss after
2021-06-04 13:56:07 | [train_policy] epoch #621 | Fitting baseline...
2021-06-04 13:56:07 | [train_policy] epoch #621 | Saving snapshot...
2021-06-04 13:56:07 | [train_policy] epoch #621 | Saved
2021-06-04 13:56:07 | [train_policy] epoch #621 | Time 498.92 s
2021-06-04 13:56:07 | [train_policy] epoch #621 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28319
Evaluation/AverageDiscountedReturn          -41.4801
Evaluation/AverageReturn                    -41.4801
Evaluation/CompletionRate                     0
Evaluation/Iteration                        621
Evaluation/MaxReturn                        -30.4326
Evaluation/MinReturn                        -77.3051
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.15656
Extras/EpisodeRewardMean                    -41.8477
LinearFeatureBaseline/ExplainedVariance       0.89275
PolicyExecTime                                0.222164
ProcessExecTime                               0.0310431
TotalEnvSteps                            629464
policy/Entropy                               -1.53042
policy/KL                                     0.00962419
policy/KLBefore                               0
policy/LossAfter                             -0.014336
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.216445
policy/dLoss                                  0.0143361
---------------------------------------  ----------------
2021-06-04 13:56:07 | [train_policy] epoch #622 | Obtaining samples for iteration 622...
2021-06-04 13:56:07 | [train_policy] epoch #622 | Logging diagnostics...
2021-06-04 13:56:07 | [train_policy] epoch #622 | Optimizing policy...
2021-06-04 13:56:07 | [train_policy] epoch #622 | Computing loss before
2021-06-04 13:56:07 | [train_policy] epoch #622 | Computing KL before
2021-06-04 13:56:07 | [train_policy] epoch #622 | Optimizing
2021-06-04 13:56:07 | [train_policy] epoch #622 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:07 | [train_policy] epoch #622 | computing loss before
2021-06-04 13:56:07 | [train_policy] epoch #622 | computing gradient
2021-06-04 13:56:07 | [train_policy] epoch #622 | gradient computed
2021-06-04 13:56:07 | [train_policy] epoch #622 | computing descent direction
2021-06-04 13:56:07 | [train_policy] epoch #622 | descent direction computed
2021-06-04 13:56:07 | [train_policy] epoch #622 | backtrack iters: 1
2021-06-04 13:56:07 | [train_policy] epoch #622 | optimization finished
2021-06-04 13:56:07 | [train_policy] epoch #622 | Computing KL after
2021-06-04 13:56:07 | [train_policy] epoch #622 | Computing loss after
2021-06-04 13:56:07 | [train_policy] epoch #622 | Fitting baseline...
2021-06-04 13:56:07 | [train_policy] epoch #622 | Saving snapshot...
2021-06-04 13:56:08 | [train_policy] epoch #622 | Saved
2021-06-04 13:56:08 | [train_policy] epoch #622 | Time 499.72 s
2021-06-04 13:56:08 | [train_policy] epoch #622 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284301
Evaluation/AverageDiscountedReturn          -41.2757
Evaluation/AverageReturn                    -41.2757
Evaluation/CompletionRate                     0
Evaluation/Iteration                        622
Evaluation/MaxReturn                        -31.8412
Evaluation/MinReturn                        -64.3128
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.50016
Extras/EpisodeRewardMean                    -41.3782
LinearFeatureBaseline/ExplainedVariance       0.92611
PolicyExecTime                                0.217096
ProcessExecTime                               0.0312092
TotalEnvSteps                            630476
policy/Entropy                               -1.53629
policy/KL                                     0.00645359
policy/KLBefore                               0
policy/LossAfter                             -0.0106386
policy/LossBefore                            -1.71982e-08
policy/Perplexity                             0.215177
policy/dLoss                                  0.0106386
---------------------------------------  ----------------
2021-06-04 13:56:08 | [train_policy] epoch #623 | Obtaining samples for iteration 623...
2021-06-04 13:56:08 | [train_policy] epoch #623 | Logging diagnostics...
2021-06-04 13:56:08 | [train_policy] epoch #623 | Optimizing policy...
2021-06-04 13:56:08 | [train_policy] epoch #623 | Computing loss before
2021-06-04 13:56:08 | [train_policy] epoch #623 | Computing KL before
2021-06-04 13:56:08 | [train_policy] epoch #623 | Optimizing
2021-06-04 13:56:08 | [train_policy] epoch #623 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:08 | [train_policy] epoch #623 | computing loss before
2021-06-04 13:56:08 | [train_policy] epoch #623 | computing gradient
2021-06-04 13:56:08 | [train_policy] epoch #623 | gradient computed
2021-06-04 13:56:08 | [train_policy] epoch #623 | computing descent direction
2021-06-04 13:56:08 | [train_policy] epoch #623 | descent direction computed
2021-06-04 13:56:08 | [train_policy] epoch #623 | backtrack iters: 0
2021-06-04 13:56:08 | [train_policy] epoch #623 | optimization finished
2021-06-04 13:56:08 | [train_policy] epoch #623 | Computing KL after
2021-06-04 13:56:08 | [train_policy] epoch #623 | Computing loss after
2021-06-04 13:56:08 | [train_policy] epoch #623 | Fitting baseline...
2021-06-04 13:56:08 | [train_policy] epoch #623 | Saving snapshot...
2021-06-04 13:56:08 | [train_policy] epoch #623 | Saved
2021-06-04 13:56:08 | [train_policy] epoch #623 | Time 500.51 s
2021-06-04 13:56:08 | [train_policy] epoch #623 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.286422
Evaluation/AverageDiscountedReturn          -41.4664
Evaluation/AverageReturn                    -41.4664
Evaluation/CompletionRate                     0
Evaluation/Iteration                        623
Evaluation/MaxReturn                        -31.2862
Evaluation/MinReturn                        -75.456
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.78118
Extras/EpisodeRewardMean                    -41.7735
LinearFeatureBaseline/ExplainedVariance       0.894263
PolicyExecTime                                0.210427
ProcessExecTime                               0.0313241
TotalEnvSteps                            631488
policy/Entropy                               -1.50445
policy/KL                                     0.00944289
policy/KLBefore                               0
policy/LossAfter                             -0.0243257
policy/LossBefore                             1.74338e-08
policy/Perplexity                             0.22214
policy/dLoss                                  0.0243258
---------------------------------------  ----------------
2021-06-04 13:56:08 | [train_policy] epoch #624 | Obtaining samples for iteration 624...
2021-06-04 13:56:09 | [train_policy] epoch #624 | Logging diagnostics...
2021-06-04 13:56:09 | [train_policy] epoch #624 | Optimizing policy...
2021-06-04 13:56:09 | [train_policy] epoch #624 | Computing loss before
2021-06-04 13:56:09 | [train_policy] epoch #624 | Computing KL before
2021-06-04 13:56:09 | [train_policy] epoch #624 | Optimizing
2021-06-04 13:56:09 | [train_policy] epoch #624 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:09 | [train_policy] epoch #624 | computing loss before
2021-06-04 13:56:09 | [train_policy] epoch #624 | computing gradient
2021-06-04 13:56:09 | [train_policy] epoch #624 | gradient computed
2021-06-04 13:56:09 | [train_policy] epoch #624 | computing descent direction
2021-06-04 13:56:09 | [train_policy] epoch #624 | descent direction computed
2021-06-04 13:56:09 | [train_policy] epoch #624 | backtrack iters: 0
2021-06-04 13:56:09 | [train_policy] epoch #624 | optimization finished
2021-06-04 13:56:09 | [train_policy] epoch #624 | Computing KL after
2021-06-04 13:56:09 | [train_policy] epoch #624 | Computing loss after
2021-06-04 13:56:09 | [train_policy] epoch #624 | Fitting baseline...
2021-06-04 13:56:09 | [train_policy] epoch #624 | Saving snapshot...
2021-06-04 13:56:09 | [train_policy] epoch #624 | Saved
2021-06-04 13:56:09 | [train_policy] epoch #624 | Time 501.32 s
2021-06-04 13:56:09 | [train_policy] epoch #624 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283805
Evaluation/AverageDiscountedReturn          -41.0913
Evaluation/AverageReturn                    -41.0913
Evaluation/CompletionRate                     0
Evaluation/Iteration                        624
Evaluation/MaxReturn                        -29.1654
Evaluation/MinReturn                        -64.0643
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.70229
Extras/EpisodeRewardMean                    -40.9321
LinearFeatureBaseline/ExplainedVariance       0.883053
PolicyExecTime                                0.234515
ProcessExecTime                               0.0312035
TotalEnvSteps                            632500
policy/Entropy                               -1.52703
policy/KL                                     0.00982895
policy/KLBefore                               0
policy/LossAfter                             -0.0180396
policy/LossBefore                             8.12791e-09
policy/Perplexity                             0.217179
policy/dLoss                                  0.0180396
---------------------------------------  ----------------
2021-06-04 13:56:09 | [train_policy] epoch #625 | Obtaining samples for iteration 625...
2021-06-04 13:56:10 | [train_policy] epoch #625 | Logging diagnostics...
2021-06-04 13:56:10 | [train_policy] epoch #625 | Optimizing policy...
2021-06-04 13:56:10 | [train_policy] epoch #625 | Computing loss before
2021-06-04 13:56:10 | [train_policy] epoch #625 | Computing KL before
2021-06-04 13:56:10 | [train_policy] epoch #625 | Optimizing
2021-06-04 13:56:10 | [train_policy] epoch #625 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:10 | [train_policy] epoch #625 | computing loss before
2021-06-04 13:56:10 | [train_policy] epoch #625 | computing gradient
2021-06-04 13:56:10 | [train_policy] epoch #625 | gradient computed
2021-06-04 13:56:10 | [train_policy] epoch #625 | computing descent direction
2021-06-04 13:56:10 | [train_policy] epoch #625 | descent direction computed
2021-06-04 13:56:10 | [train_policy] epoch #625 | backtrack iters: 0
2021-06-04 13:56:10 | [train_policy] epoch #625 | optimization finished
2021-06-04 13:56:10 | [train_policy] epoch #625 | Computing KL after
2021-06-04 13:56:10 | [train_policy] epoch #625 | Computing loss after
2021-06-04 13:56:10 | [train_policy] epoch #625 | Fitting baseline...
2021-06-04 13:56:10 | [train_policy] epoch #625 | Saving snapshot...
2021-06-04 13:56:10 | [train_policy] epoch #625 | Saved
2021-06-04 13:56:10 | [train_policy] epoch #625 | Time 502.13 s
2021-06-04 13:56:10 | [train_policy] epoch #625 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285638
Evaluation/AverageDiscountedReturn          -42.4654
Evaluation/AverageReturn                    -42.4654
Evaluation/CompletionRate                     0
Evaluation/Iteration                        625
Evaluation/MaxReturn                        -31.4188
Evaluation/MinReturn                        -76.3267
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.31943
Extras/EpisodeRewardMean                    -42.0244
LinearFeatureBaseline/ExplainedVariance       0.888207
PolicyExecTime                                0.239404
ProcessExecTime                               0.0312407
TotalEnvSteps                            633512
policy/Entropy                               -1.51492
policy/KL                                     0.0099872
policy/KLBefore                               0
policy/LossAfter                             -0.0204718
policy/LossBefore                            -7.77452e-09
policy/Perplexity                             0.219825
policy/dLoss                                  0.0204717
---------------------------------------  ----------------
2021-06-04 13:56:10 | [train_policy] epoch #626 | Obtaining samples for iteration 626...
2021-06-04 13:56:11 | [train_policy] epoch #626 | Logging diagnostics...
2021-06-04 13:56:11 | [train_policy] epoch #626 | Optimizing policy...
2021-06-04 13:56:11 | [train_policy] epoch #626 | Computing loss before
2021-06-04 13:56:11 | [train_policy] epoch #626 | Computing KL before
2021-06-04 13:56:11 | [train_policy] epoch #626 | Optimizing
2021-06-04 13:56:11 | [train_policy] epoch #626 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:11 | [train_policy] epoch #626 | computing loss before
2021-06-04 13:56:11 | [train_policy] epoch #626 | computing gradient
2021-06-04 13:56:11 | [train_policy] epoch #626 | gradient computed
2021-06-04 13:56:11 | [train_policy] epoch #626 | computing descent direction
2021-06-04 13:56:11 | [train_policy] epoch #626 | descent direction computed
2021-06-04 13:56:11 | [train_policy] epoch #626 | backtrack iters: 0
2021-06-04 13:56:11 | [train_policy] epoch #626 | optimization finished
2021-06-04 13:56:11 | [train_policy] epoch #626 | Computing KL after
2021-06-04 13:56:11 | [train_policy] epoch #626 | Computing loss after
2021-06-04 13:56:11 | [train_policy] epoch #626 | Fitting baseline...
2021-06-04 13:56:11 | [train_policy] epoch #626 | Saving snapshot...
2021-06-04 13:56:11 | [train_policy] epoch #626 | Saved
2021-06-04 13:56:11 | [train_policy] epoch #626 | Time 502.94 s
2021-06-04 13:56:11 | [train_policy] epoch #626 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284726
Evaluation/AverageDiscountedReturn          -42.217
Evaluation/AverageReturn                    -42.217
Evaluation/CompletionRate                     0
Evaluation/Iteration                        626
Evaluation/MaxReturn                        -30.959
Evaluation/MinReturn                        -77.4579
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.98024
Extras/EpisodeRewardMean                    -41.8773
LinearFeatureBaseline/ExplainedVariance       0.876451
PolicyExecTime                                0.226639
ProcessExecTime                               0.0313563
TotalEnvSteps                            634524
policy/Entropy                               -1.51023
policy/KL                                     0.00949074
policy/KLBefore                               0
policy/LossAfter                             -0.0207471
policy/LossBefore                             1.36643e-08
policy/Perplexity                             0.220859
policy/dLoss                                  0.0207471
---------------------------------------  ----------------
2021-06-04 13:56:11 | [train_policy] epoch #627 | Obtaining samples for iteration 627...
2021-06-04 13:56:11 | [train_policy] epoch #627 | Logging diagnostics...
2021-06-04 13:56:11 | [train_policy] epoch #627 | Optimizing policy...
2021-06-04 13:56:11 | [train_policy] epoch #627 | Computing loss before
2021-06-04 13:56:11 | [train_policy] epoch #627 | Computing KL before
2021-06-04 13:56:11 | [train_policy] epoch #627 | Optimizing
2021-06-04 13:56:11 | [train_policy] epoch #627 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:11 | [train_policy] epoch #627 | computing loss before
2021-06-04 13:56:11 | [train_policy] epoch #627 | computing gradient
2021-06-04 13:56:11 | [train_policy] epoch #627 | gradient computed
2021-06-04 13:56:11 | [train_policy] epoch #627 | computing descent direction
2021-06-04 13:56:11 | [train_policy] epoch #627 | descent direction computed
2021-06-04 13:56:11 | [train_policy] epoch #627 | backtrack iters: 1
2021-06-04 13:56:11 | [train_policy] epoch #627 | optimization finished
2021-06-04 13:56:11 | [train_policy] epoch #627 | Computing KL after
2021-06-04 13:56:11 | [train_policy] epoch #627 | Computing loss after
2021-06-04 13:56:11 | [train_policy] epoch #627 | Fitting baseline...
2021-06-04 13:56:11 | [train_policy] epoch #627 | Saving snapshot...
2021-06-04 13:56:12 | [train_policy] epoch #627 | Saved
2021-06-04 13:56:12 | [train_policy] epoch #627 | Time 503.73 s
2021-06-04 13:56:12 | [train_policy] epoch #627 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28479
Evaluation/AverageDiscountedReturn          -43.0954
Evaluation/AverageReturn                    -43.0954
Evaluation/CompletionRate                     0
Evaluation/Iteration                        627
Evaluation/MaxReturn                        -30.9063
Evaluation/MinReturn                        -76.0441
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.85748
Extras/EpisodeRewardMean                    -43.3608
LinearFeatureBaseline/ExplainedVariance       0.890428
PolicyExecTime                                0.209681
ProcessExecTime                               0.0311666
TotalEnvSteps                            635536
policy/Entropy                               -1.53792
policy/KL                                     0.00646302
policy/KLBefore                               0
policy/LossAfter                             -0.0198464
policy/LossBefore                             1.64914e-08
policy/Perplexity                             0.214827
policy/dLoss                                  0.0198464
---------------------------------------  ----------------
2021-06-04 13:56:12 | [train_policy] epoch #628 | Obtaining samples for iteration 628...
2021-06-04 13:56:12 | [train_policy] epoch #628 | Logging diagnostics...
2021-06-04 13:56:12 | [train_policy] epoch #628 | Optimizing policy...
2021-06-04 13:56:12 | [train_policy] epoch #628 | Computing loss before
2021-06-04 13:56:12 | [train_policy] epoch #628 | Computing KL before
2021-06-04 13:56:12 | [train_policy] epoch #628 | Optimizing
2021-06-04 13:56:12 | [train_policy] epoch #628 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:12 | [train_policy] epoch #628 | computing loss before
2021-06-04 13:56:12 | [train_policy] epoch #628 | computing gradient
2021-06-04 13:56:12 | [train_policy] epoch #628 | gradient computed
2021-06-04 13:56:12 | [train_policy] epoch #628 | computing descent direction
2021-06-04 13:56:12 | [train_policy] epoch #628 | descent direction computed
2021-06-04 13:56:12 | [train_policy] epoch #628 | backtrack iters: 1
2021-06-04 13:56:12 | [train_policy] epoch #628 | optimization finished
2021-06-04 13:56:12 | [train_policy] epoch #628 | Computing KL after
2021-06-04 13:56:12 | [train_policy] epoch #628 | Computing loss after
2021-06-04 13:56:12 | [train_policy] epoch #628 | Fitting baseline...
2021-06-04 13:56:12 | [train_policy] epoch #628 | Saving snapshot...
2021-06-04 13:56:12 | [train_policy] epoch #628 | Saved
2021-06-04 13:56:12 | [train_policy] epoch #628 | Time 504.53 s
2021-06-04 13:56:12 | [train_policy] epoch #628 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28473
Evaluation/AverageDiscountedReturn          -40.9377
Evaluation/AverageReturn                    -40.9377
Evaluation/CompletionRate                     0
Evaluation/Iteration                        628
Evaluation/MaxReturn                        -30.3463
Evaluation/MinReturn                        -64.0668
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83318
Extras/EpisodeRewardMean                    -40.7118
LinearFeatureBaseline/ExplainedVariance       0.885286
PolicyExecTime                                0.216034
ProcessExecTime                               0.0311873
TotalEnvSteps                            636548
policy/Entropy                               -1.54667
policy/KL                                     0.00645436
policy/KLBefore                               0
policy/LossAfter                             -0.0129067
policy/LossBefore                             7.65672e-09
policy/Perplexity                             0.212957
policy/dLoss                                  0.0129067
---------------------------------------  ----------------
2021-06-04 13:56:12 | [train_policy] epoch #629 | Obtaining samples for iteration 629...
2021-06-04 13:56:13 | [train_policy] epoch #629 | Logging diagnostics...
2021-06-04 13:56:13 | [train_policy] epoch #629 | Optimizing policy...
2021-06-04 13:56:13 | [train_policy] epoch #629 | Computing loss before
2021-06-04 13:56:13 | [train_policy] epoch #629 | Computing KL before
2021-06-04 13:56:13 | [train_policy] epoch #629 | Optimizing
2021-06-04 13:56:13 | [train_policy] epoch #629 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:13 | [train_policy] epoch #629 | computing loss before
2021-06-04 13:56:13 | [train_policy] epoch #629 | computing gradient
2021-06-04 13:56:13 | [train_policy] epoch #629 | gradient computed
2021-06-04 13:56:13 | [train_policy] epoch #629 | computing descent direction
2021-06-04 13:56:13 | [train_policy] epoch #629 | descent direction computed
2021-06-04 13:56:13 | [train_policy] epoch #629 | backtrack iters: 1
2021-06-04 13:56:13 | [train_policy] epoch #629 | optimization finished
2021-06-04 13:56:13 | [train_policy] epoch #629 | Computing KL after
2021-06-04 13:56:13 | [train_policy] epoch #629 | Computing loss after
2021-06-04 13:56:13 | [train_policy] epoch #629 | Fitting baseline...
2021-06-04 13:56:13 | [train_policy] epoch #629 | Saving snapshot...
2021-06-04 13:56:13 | [train_policy] epoch #629 | Saved
2021-06-04 13:56:13 | [train_policy] epoch #629 | Time 505.35 s
2021-06-04 13:56:13 | [train_policy] epoch #629 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284419
Evaluation/AverageDiscountedReturn          -42.7089
Evaluation/AverageReturn                    -42.7089
Evaluation/CompletionRate                     0
Evaluation/Iteration                        629
Evaluation/MaxReturn                        -31.8641
Evaluation/MinReturn                        -78.3525
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.97132
Extras/EpisodeRewardMean                    -42.3482
LinearFeatureBaseline/ExplainedVariance       0.866521
PolicyExecTime                                0.232512
ProcessExecTime                               0.031203
TotalEnvSteps                            637560
policy/Entropy                               -1.56098
policy/KL                                     0.00648037
policy/KLBefore                               0
policy/LossAfter                             -0.0152456
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.209931
policy/dLoss                                  0.0152456
---------------------------------------  ----------------
2021-06-04 13:56:13 | [train_policy] epoch #630 | Obtaining samples for iteration 630...
2021-06-04 13:56:14 | [train_policy] epoch #630 | Logging diagnostics...
2021-06-04 13:56:14 | [train_policy] epoch #630 | Optimizing policy...
2021-06-04 13:56:14 | [train_policy] epoch #630 | Computing loss before
2021-06-04 13:56:14 | [train_policy] epoch #630 | Computing KL before
2021-06-04 13:56:14 | [train_policy] epoch #630 | Optimizing
2021-06-04 13:56:14 | [train_policy] epoch #630 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:14 | [train_policy] epoch #630 | computing loss before
2021-06-04 13:56:14 | [train_policy] epoch #630 | computing gradient
2021-06-04 13:56:14 | [train_policy] epoch #630 | gradient computed
2021-06-04 13:56:14 | [train_policy] epoch #630 | computing descent direction
2021-06-04 13:56:14 | [train_policy] epoch #630 | descent direction computed
2021-06-04 13:56:14 | [train_policy] epoch #630 | backtrack iters: 0
2021-06-04 13:56:14 | [train_policy] epoch #630 | optimization finished
2021-06-04 13:56:14 | [train_policy] epoch #630 | Computing KL after
2021-06-04 13:56:14 | [train_policy] epoch #630 | Computing loss after
2021-06-04 13:56:14 | [train_policy] epoch #630 | Fitting baseline...
2021-06-04 13:56:14 | [train_policy] epoch #630 | Saving snapshot...
2021-06-04 13:56:14 | [train_policy] epoch #630 | Saved
2021-06-04 13:56:14 | [train_policy] epoch #630 | Time 506.13 s
2021-06-04 13:56:14 | [train_policy] epoch #630 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284674
Evaluation/AverageDiscountedReturn          -41.7438
Evaluation/AverageReturn                    -41.7438
Evaluation/CompletionRate                     0
Evaluation/Iteration                        630
Evaluation/MaxReturn                        -30.3252
Evaluation/MinReturn                        -77.9086
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.26209
Extras/EpisodeRewardMean                    -41.7631
LinearFeatureBaseline/ExplainedVariance       0.88874
PolicyExecTime                                0.211011
ProcessExecTime                               0.0311406
TotalEnvSteps                            638572
policy/Entropy                               -1.53268
policy/KL                                     0.00971642
policy/KLBefore                               0
policy/LossAfter                             -0.0232423
policy/LossBefore                            -2.73875e-09
policy/Perplexity                             0.215956
policy/dLoss                                  0.0232423
---------------------------------------  ----------------
2021-06-04 13:56:14 | [train_policy] epoch #631 | Obtaining samples for iteration 631...
2021-06-04 13:56:15 | [train_policy] epoch #631 | Logging diagnostics...
2021-06-04 13:56:15 | [train_policy] epoch #631 | Optimizing policy...
2021-06-04 13:56:15 | [train_policy] epoch #631 | Computing loss before
2021-06-04 13:56:15 | [train_policy] epoch #631 | Computing KL before
2021-06-04 13:56:15 | [train_policy] epoch #631 | Optimizing
2021-06-04 13:56:15 | [train_policy] epoch #631 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:15 | [train_policy] epoch #631 | computing loss before
2021-06-04 13:56:15 | [train_policy] epoch #631 | computing gradient
2021-06-04 13:56:15 | [train_policy] epoch #631 | gradient computed
2021-06-04 13:56:15 | [train_policy] epoch #631 | computing descent direction
2021-06-04 13:56:15 | [train_policy] epoch #631 | descent direction computed
2021-06-04 13:56:15 | [train_policy] epoch #631 | backtrack iters: 1
2021-06-04 13:56:15 | [train_policy] epoch #631 | optimization finished
2021-06-04 13:56:15 | [train_policy] epoch #631 | Computing KL after
2021-06-04 13:56:15 | [train_policy] epoch #631 | Computing loss after
2021-06-04 13:56:15 | [train_policy] epoch #631 | Fitting baseline...
2021-06-04 13:56:15 | [train_policy] epoch #631 | Saving snapshot...
2021-06-04 13:56:15 | [train_policy] epoch #631 | Saved
2021-06-04 13:56:15 | [train_policy] epoch #631 | Time 506.95 s
2021-06-04 13:56:15 | [train_policy] epoch #631 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.285239
Evaluation/AverageDiscountedReturn          -42.7491
Evaluation/AverageReturn                    -42.7491
Evaluation/CompletionRate                     0
Evaluation/Iteration                        631
Evaluation/MaxReturn                        -29.8212
Evaluation/MinReturn                        -76.309
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.86546
Extras/EpisodeRewardMean                    -42.7937
LinearFeatureBaseline/ExplainedVariance       0.873532
PolicyExecTime                                0.232434
ProcessExecTime                               0.0312765
TotalEnvSteps                            639584
policy/Entropy                               -1.55876
policy/KL                                     0.00654009
policy/KLBefore                               0
policy/LossAfter                             -0.0187007
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.210398
policy/dLoss                                  0.0187007
---------------------------------------  ---------------
2021-06-04 13:56:15 | [train_policy] epoch #632 | Obtaining samples for iteration 632...
2021-06-04 13:56:15 | [train_policy] epoch #632 | Logging diagnostics...
2021-06-04 13:56:15 | [train_policy] epoch #632 | Optimizing policy...
2021-06-04 13:56:15 | [train_policy] epoch #632 | Computing loss before
2021-06-04 13:56:15 | [train_policy] epoch #632 | Computing KL before
2021-06-04 13:56:15 | [train_policy] epoch #632 | Optimizing
2021-06-04 13:56:15 | [train_policy] epoch #632 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:15 | [train_policy] epoch #632 | computing loss before
2021-06-04 13:56:15 | [train_policy] epoch #632 | computing gradient
2021-06-04 13:56:15 | [train_policy] epoch #632 | gradient computed
2021-06-04 13:56:15 | [train_policy] epoch #632 | computing descent direction
2021-06-04 13:56:15 | [train_policy] epoch #632 | descent direction computed
2021-06-04 13:56:15 | [train_policy] epoch #632 | backtrack iters: 0
2021-06-04 13:56:15 | [train_policy] epoch #632 | optimization finished
2021-06-04 13:56:15 | [train_policy] epoch #632 | Computing KL after
2021-06-04 13:56:15 | [train_policy] epoch #632 | Computing loss after
2021-06-04 13:56:15 | [train_policy] epoch #632 | Fitting baseline...
2021-06-04 13:56:15 | [train_policy] epoch #632 | Saving snapshot...
2021-06-04 13:56:16 | [train_policy] epoch #632 | Saved
2021-06-04 13:56:16 | [train_policy] epoch #632 | Time 507.74 s
2021-06-04 13:56:16 | [train_policy] epoch #632 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283825
Evaluation/AverageDiscountedReturn          -42.1187
Evaluation/AverageReturn                    -42.1187
Evaluation/CompletionRate                     0
Evaluation/Iteration                        632
Evaluation/MaxReturn                        -31.4723
Evaluation/MinReturn                        -76.0996
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.58076
Extras/EpisodeRewardMean                    -42.3164
LinearFeatureBaseline/ExplainedVariance       0.885481
PolicyExecTime                                0.216807
ProcessExecTime                               0.0311623
TotalEnvSteps                            640596
policy/Entropy                               -1.53481
policy/KL                                     0.0097838
policy/KLBefore                               0
policy/LossAfter                             -0.028021
policy/LossBefore                             7.77452e-09
policy/Perplexity                             0.215497
policy/dLoss                                  0.028021
---------------------------------------  ----------------
2021-06-04 13:56:16 | [train_policy] epoch #633 | Obtaining samples for iteration 633...
2021-06-04 13:56:16 | [train_policy] epoch #633 | Logging diagnostics...
2021-06-04 13:56:16 | [train_policy] epoch #633 | Optimizing policy...
2021-06-04 13:56:16 | [train_policy] epoch #633 | Computing loss before
2021-06-04 13:56:16 | [train_policy] epoch #633 | Computing KL before
2021-06-04 13:56:16 | [train_policy] epoch #633 | Optimizing
2021-06-04 13:56:16 | [train_policy] epoch #633 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:16 | [train_policy] epoch #633 | computing loss before
2021-06-04 13:56:16 | [train_policy] epoch #633 | computing gradient
2021-06-04 13:56:16 | [train_policy] epoch #633 | gradient computed
2021-06-04 13:56:16 | [train_policy] epoch #633 | computing descent direction
2021-06-04 13:56:16 | [train_policy] epoch #633 | descent direction computed
2021-06-04 13:56:16 | [train_policy] epoch #633 | backtrack iters: 1
2021-06-04 13:56:16 | [train_policy] epoch #633 | optimization finished
2021-06-04 13:56:16 | [train_policy] epoch #633 | Computing KL after
2021-06-04 13:56:16 | [train_policy] epoch #633 | Computing loss after
2021-06-04 13:56:16 | [train_policy] epoch #633 | Fitting baseline...
2021-06-04 13:56:16 | [train_policy] epoch #633 | Saving snapshot...
2021-06-04 13:56:16 | [train_policy] epoch #633 | Saved
2021-06-04 13:56:16 | [train_policy] epoch #633 | Time 508.55 s
2021-06-04 13:56:16 | [train_policy] epoch #633 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283909
Evaluation/AverageDiscountedReturn          -41.5259
Evaluation/AverageReturn                    -41.5259
Evaluation/CompletionRate                     0
Evaluation/Iteration                        633
Evaluation/MaxReturn                        -28.9622
Evaluation/MinReturn                        -78.6762
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.36773
Extras/EpisodeRewardMean                    -42.2677
LinearFeatureBaseline/ExplainedVariance       0.886361
PolicyExecTime                                0.229729
ProcessExecTime                               0.0311489
TotalEnvSteps                            641608
policy/Entropy                               -1.52774
policy/KL                                     0.006708
policy/KLBefore                               0
policy/LossAfter                             -0.0142735
policy/LossBefore                             8.71688e-09
policy/Perplexity                             0.217026
policy/dLoss                                  0.0142735
---------------------------------------  ----------------
2021-06-04 13:56:16 | [train_policy] epoch #634 | Obtaining samples for iteration 634...
2021-06-04 13:56:17 | [train_policy] epoch #634 | Logging diagnostics...
2021-06-04 13:56:17 | [train_policy] epoch #634 | Optimizing policy...
2021-06-04 13:56:17 | [train_policy] epoch #634 | Computing loss before
2021-06-04 13:56:17 | [train_policy] epoch #634 | Computing KL before
2021-06-04 13:56:17 | [train_policy] epoch #634 | Optimizing
2021-06-04 13:56:17 | [train_policy] epoch #634 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:17 | [train_policy] epoch #634 | computing loss before
2021-06-04 13:56:17 | [train_policy] epoch #634 | computing gradient
2021-06-04 13:56:17 | [train_policy] epoch #634 | gradient computed
2021-06-04 13:56:17 | [train_policy] epoch #634 | computing descent direction
2021-06-04 13:56:17 | [train_policy] epoch #634 | descent direction computed
2021-06-04 13:56:17 | [train_policy] epoch #634 | backtrack iters: 0
2021-06-04 13:56:17 | [train_policy] epoch #634 | optimization finished
2021-06-04 13:56:17 | [train_policy] epoch #634 | Computing KL after
2021-06-04 13:56:17 | [train_policy] epoch #634 | Computing loss after
2021-06-04 13:56:17 | [train_policy] epoch #634 | Fitting baseline...
2021-06-04 13:56:17 | [train_policy] epoch #634 | Saving snapshot...
2021-06-04 13:56:17 | [train_policy] epoch #634 | Saved
2021-06-04 13:56:17 | [train_policy] epoch #634 | Time 509.35 s
2021-06-04 13:56:17 | [train_policy] epoch #634 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283175
Evaluation/AverageDiscountedReturn          -41.8083
Evaluation/AverageReturn                    -41.8083
Evaluation/CompletionRate                     0
Evaluation/Iteration                        634
Evaluation/MaxReturn                        -30.0437
Evaluation/MinReturn                        -76.0508
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05053
Extras/EpisodeRewardMean                    -41.6225
LinearFeatureBaseline/ExplainedVariance       0.883002
PolicyExecTime                                0.223574
ProcessExecTime                               0.0310936
TotalEnvSteps                            642620
policy/Entropy                               -1.53948
policy/KL                                     0.00992491
policy/KLBefore                               0
policy/LossAfter                             -0.0192437
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.214492
policy/dLoss                                  0.0192437
---------------------------------------  ----------------
2021-06-04 13:56:17 | [train_policy] epoch #635 | Obtaining samples for iteration 635...
2021-06-04 13:56:18 | [train_policy] epoch #635 | Logging diagnostics...
2021-06-04 13:56:18 | [train_policy] epoch #635 | Optimizing policy...
2021-06-04 13:56:18 | [train_policy] epoch #635 | Computing loss before
2021-06-04 13:56:18 | [train_policy] epoch #635 | Computing KL before
2021-06-04 13:56:18 | [train_policy] epoch #635 | Optimizing
2021-06-04 13:56:18 | [train_policy] epoch #635 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:18 | [train_policy] epoch #635 | computing loss before
2021-06-04 13:56:18 | [train_policy] epoch #635 | computing gradient
2021-06-04 13:56:18 | [train_policy] epoch #635 | gradient computed
2021-06-04 13:56:18 | [train_policy] epoch #635 | computing descent direction
2021-06-04 13:56:18 | [train_policy] epoch #635 | descent direction computed
2021-06-04 13:56:18 | [train_policy] epoch #635 | backtrack iters: 1
2021-06-04 13:56:18 | [train_policy] epoch #635 | optimization finished
2021-06-04 13:56:18 | [train_policy] epoch #635 | Computing KL after
2021-06-04 13:56:18 | [train_policy] epoch #635 | Computing loss after
2021-06-04 13:56:18 | [train_policy] epoch #635 | Fitting baseline...
2021-06-04 13:56:18 | [train_policy] epoch #635 | Saving snapshot...
2021-06-04 13:56:18 | [train_policy] epoch #635 | Saved
2021-06-04 13:56:18 | [train_policy] epoch #635 | Time 510.17 s
2021-06-04 13:56:18 | [train_policy] epoch #635 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284538
Evaluation/AverageDiscountedReturn          -42.211
Evaluation/AverageReturn                    -42.211
Evaluation/CompletionRate                     0
Evaluation/Iteration                        635
Evaluation/MaxReturn                        -29.3562
Evaluation/MinReturn                        -78.785
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.5225
Extras/EpisodeRewardMean                    -42.2097
LinearFeatureBaseline/ExplainedVariance       0.863999
PolicyExecTime                                0.22733
ProcessExecTime                               0.0312257
TotalEnvSteps                            643632
policy/Entropy                               -1.54939
policy/KL                                     0.00653317
policy/KLBefore                               0
policy/LossAfter                             -0.0244448
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             0.212378
policy/dLoss                                  0.0244448
---------------------------------------  ----------------
2021-06-04 13:56:18 | [train_policy] epoch #636 | Obtaining samples for iteration 636...
2021-06-04 13:56:19 | [train_policy] epoch #636 | Logging diagnostics...
2021-06-04 13:56:19 | [train_policy] epoch #636 | Optimizing policy...
2021-06-04 13:56:19 | [train_policy] epoch #636 | Computing loss before
2021-06-04 13:56:19 | [train_policy] epoch #636 | Computing KL before
2021-06-04 13:56:19 | [train_policy] epoch #636 | Optimizing
2021-06-04 13:56:19 | [train_policy] epoch #636 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:19 | [train_policy] epoch #636 | computing loss before
2021-06-04 13:56:19 | [train_policy] epoch #636 | computing gradient
2021-06-04 13:56:19 | [train_policy] epoch #636 | gradient computed
2021-06-04 13:56:19 | [train_policy] epoch #636 | computing descent direction
2021-06-04 13:56:19 | [train_policy] epoch #636 | descent direction computed
2021-06-04 13:56:19 | [train_policy] epoch #636 | backtrack iters: 1
2021-06-04 13:56:19 | [train_policy] epoch #636 | optimization finished
2021-06-04 13:56:19 | [train_policy] epoch #636 | Computing KL after
2021-06-04 13:56:19 | [train_policy] epoch #636 | Computing loss after
2021-06-04 13:56:19 | [train_policy] epoch #636 | Fitting baseline...
2021-06-04 13:56:19 | [train_policy] epoch #636 | Saving snapshot...
2021-06-04 13:56:19 | [train_policy] epoch #636 | Saved
2021-06-04 13:56:19 | [train_policy] epoch #636 | Time 510.98 s
2021-06-04 13:56:19 | [train_policy] epoch #636 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284859
Evaluation/AverageDiscountedReturn          -41.7341
Evaluation/AverageReturn                    -41.7341
Evaluation/CompletionRate                     0
Evaluation/Iteration                        636
Evaluation/MaxReturn                        -29.6651
Evaluation/MinReturn                        -80.7233
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77895
Extras/EpisodeRewardMean                    -41.6034
LinearFeatureBaseline/ExplainedVariance       0.83882
PolicyExecTime                                0.228009
ProcessExecTime                               0.0312128
TotalEnvSteps                            644644
policy/Entropy                               -1.56136
policy/KL                                     0.00653551
policy/KLBefore                               0
policy/LossAfter                             -0.0146753
policy/LossBefore                             7.77452e-09
policy/Perplexity                             0.209851
policy/dLoss                                  0.0146753
---------------------------------------  ----------------
2021-06-04 13:56:19 | [train_policy] epoch #637 | Obtaining samples for iteration 637...
2021-06-04 13:56:19 | [train_policy] epoch #637 | Logging diagnostics...
2021-06-04 13:56:19 | [train_policy] epoch #637 | Optimizing policy...
2021-06-04 13:56:19 | [train_policy] epoch #637 | Computing loss before
2021-06-04 13:56:19 | [train_policy] epoch #637 | Computing KL before
2021-06-04 13:56:19 | [train_policy] epoch #637 | Optimizing
2021-06-04 13:56:19 | [train_policy] epoch #637 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:19 | [train_policy] epoch #637 | computing loss before
2021-06-04 13:56:19 | [train_policy] epoch #637 | computing gradient
2021-06-04 13:56:19 | [train_policy] epoch #637 | gradient computed
2021-06-04 13:56:19 | [train_policy] epoch #637 | computing descent direction
2021-06-04 13:56:19 | [train_policy] epoch #637 | descent direction computed
2021-06-04 13:56:20 | [train_policy] epoch #637 | backtrack iters: 0
2021-06-04 13:56:20 | [train_policy] epoch #637 | optimization finished
2021-06-04 13:56:20 | [train_policy] epoch #637 | Computing KL after
2021-06-04 13:56:20 | [train_policy] epoch #637 | Computing loss after
2021-06-04 13:56:20 | [train_policy] epoch #637 | Fitting baseline...
2021-06-04 13:56:20 | [train_policy] epoch #637 | Saving snapshot...
2021-06-04 13:56:20 | [train_policy] epoch #637 | Saved
2021-06-04 13:56:20 | [train_policy] epoch #637 | Time 511.78 s
2021-06-04 13:56:20 | [train_policy] epoch #637 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28515
Evaluation/AverageDiscountedReturn          -40.9204
Evaluation/AverageReturn                    -40.9204
Evaluation/CompletionRate                     0
Evaluation/Iteration                        637
Evaluation/MaxReturn                        -29.3238
Evaluation/MinReturn                        -63.6044
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.89041
Extras/EpisodeRewardMean                    -40.7801
LinearFeatureBaseline/ExplainedVariance       0.895383
PolicyExecTime                                0.22859
ProcessExecTime                               0.0313249
TotalEnvSteps                            645656
policy/Entropy                               -1.4978
policy/KL                                     0.00934852
policy/KLBefore                               0
policy/LossAfter                             -0.0168425
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.223621
policy/dLoss                                  0.0168425
---------------------------------------  ----------------
2021-06-04 13:56:20 | [train_policy] epoch #638 | Obtaining samples for iteration 638...
2021-06-04 13:56:20 | [train_policy] epoch #638 | Logging diagnostics...
2021-06-04 13:56:20 | [train_policy] epoch #638 | Optimizing policy...
2021-06-04 13:56:20 | [train_policy] epoch #638 | Computing loss before
2021-06-04 13:56:20 | [train_policy] epoch #638 | Computing KL before
2021-06-04 13:56:20 | [train_policy] epoch #638 | Optimizing
2021-06-04 13:56:20 | [train_policy] epoch #638 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:20 | [train_policy] epoch #638 | computing loss before
2021-06-04 13:56:20 | [train_policy] epoch #638 | computing gradient
2021-06-04 13:56:20 | [train_policy] epoch #638 | gradient computed
2021-06-04 13:56:20 | [train_policy] epoch #638 | computing descent direction
2021-06-04 13:56:20 | [train_policy] epoch #638 | descent direction computed
2021-06-04 13:56:20 | [train_policy] epoch #638 | backtrack iters: 0
2021-06-04 13:56:20 | [train_policy] epoch #638 | optimization finished
2021-06-04 13:56:20 | [train_policy] epoch #638 | Computing KL after
2021-06-04 13:56:20 | [train_policy] epoch #638 | Computing loss after
2021-06-04 13:56:20 | [train_policy] epoch #638 | Fitting baseline...
2021-06-04 13:56:20 | [train_policy] epoch #638 | Saving snapshot...
2021-06-04 13:56:20 | [train_policy] epoch #638 | Saved
2021-06-04 13:56:20 | [train_policy] epoch #638 | Time 512.59 s
2021-06-04 13:56:20 | [train_policy] epoch #638 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284582
Evaluation/AverageDiscountedReturn          -42.269
Evaluation/AverageReturn                    -42.269
Evaluation/CompletionRate                     0
Evaluation/Iteration                        638
Evaluation/MaxReturn                        -29.1019
Evaluation/MinReturn                        -75.8522
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.42856
Extras/EpisodeRewardMean                    -42.3255
LinearFeatureBaseline/ExplainedVariance       0.881052
PolicyExecTime                                0.217632
ProcessExecTime                               0.0312767
TotalEnvSteps                            646668
policy/Entropy                               -1.48329
policy/KL                                     0.00981734
policy/KLBefore                               0
policy/LossAfter                             -0.0199187
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.22689
policy/dLoss                                  0.0199187
---------------------------------------  ----------------
2021-06-04 13:56:20 | [train_policy] epoch #639 | Obtaining samples for iteration 639...
2021-06-04 13:56:21 | [train_policy] epoch #639 | Logging diagnostics...
2021-06-04 13:56:21 | [train_policy] epoch #639 | Optimizing policy...
2021-06-04 13:56:21 | [train_policy] epoch #639 | Computing loss before
2021-06-04 13:56:21 | [train_policy] epoch #639 | Computing KL before
2021-06-04 13:56:21 | [train_policy] epoch #639 | Optimizing
2021-06-04 13:56:21 | [train_policy] epoch #639 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:21 | [train_policy] epoch #639 | computing loss before
2021-06-04 13:56:21 | [train_policy] epoch #639 | computing gradient
2021-06-04 13:56:21 | [train_policy] epoch #639 | gradient computed
2021-06-04 13:56:21 | [train_policy] epoch #639 | computing descent direction
2021-06-04 13:56:21 | [train_policy] epoch #639 | descent direction computed
2021-06-04 13:56:21 | [train_policy] epoch #639 | backtrack iters: 0
2021-06-04 13:56:21 | [train_policy] epoch #639 | optimization finished
2021-06-04 13:56:21 | [train_policy] epoch #639 | Computing KL after
2021-06-04 13:56:21 | [train_policy] epoch #639 | Computing loss after
2021-06-04 13:56:21 | [train_policy] epoch #639 | Fitting baseline...
2021-06-04 13:56:21 | [train_policy] epoch #639 | Saving snapshot...
2021-06-04 13:56:21 | [train_policy] epoch #639 | Saved
2021-06-04 13:56:21 | [train_policy] epoch #639 | Time 513.40 s
2021-06-04 13:56:21 | [train_policy] epoch #639 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28742
Evaluation/AverageDiscountedReturn          -41.2206
Evaluation/AverageReturn                    -41.2206
Evaluation/CompletionRate                     0
Evaluation/Iteration                        639
Evaluation/MaxReturn                        -29.1184
Evaluation/MinReturn                        -63.1606
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.06364
Extras/EpisodeRewardMean                    -41.3445
LinearFeatureBaseline/ExplainedVariance       0.899068
PolicyExecTime                                0.2383
ProcessExecTime                               0.0313866
TotalEnvSteps                            647680
policy/Entropy                               -1.45587
policy/KL                                     0.00970707
policy/KLBefore                               0
policy/LossAfter                             -0.0175723
policy/LossBefore                            -9.71815e-09
policy/Perplexity                             0.233199
policy/dLoss                                  0.0175722
---------------------------------------  ----------------
2021-06-04 13:56:21 | [train_policy] epoch #640 | Obtaining samples for iteration 640...
2021-06-04 13:56:22 | [train_policy] epoch #640 | Logging diagnostics...
2021-06-04 13:56:22 | [train_policy] epoch #640 | Optimizing policy...
2021-06-04 13:56:22 | [train_policy] epoch #640 | Computing loss before
2021-06-04 13:56:22 | [train_policy] epoch #640 | Computing KL before
2021-06-04 13:56:22 | [train_policy] epoch #640 | Optimizing
2021-06-04 13:56:22 | [train_policy] epoch #640 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:22 | [train_policy] epoch #640 | computing loss before
2021-06-04 13:56:22 | [train_policy] epoch #640 | computing gradient
2021-06-04 13:56:22 | [train_policy] epoch #640 | gradient computed
2021-06-04 13:56:22 | [train_policy] epoch #640 | computing descent direction
2021-06-04 13:56:22 | [train_policy] epoch #640 | descent direction computed
2021-06-04 13:56:22 | [train_policy] epoch #640 | backtrack iters: 1
2021-06-04 13:56:22 | [train_policy] epoch #640 | optimization finished
2021-06-04 13:56:22 | [train_policy] epoch #640 | Computing KL after
2021-06-04 13:56:22 | [train_policy] epoch #640 | Computing loss after
2021-06-04 13:56:22 | [train_policy] epoch #640 | Fitting baseline...
2021-06-04 13:56:22 | [train_policy] epoch #640 | Saving snapshot...
2021-06-04 13:56:22 | [train_policy] epoch #640 | Saved
2021-06-04 13:56:22 | [train_policy] epoch #640 | Time 514.21 s
2021-06-04 13:56:22 | [train_policy] epoch #640 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283849
Evaluation/AverageDiscountedReturn          -41.2376
Evaluation/AverageReturn                    -41.2376
Evaluation/CompletionRate                     0
Evaluation/Iteration                        640
Evaluation/MaxReturn                        -30.1334
Evaluation/MinReturn                        -63.457
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.48153
Extras/EpisodeRewardMean                    -41.0278
LinearFeatureBaseline/ExplainedVariance       0.907577
PolicyExecTime                                0.227024
ProcessExecTime                               0.0311856
TotalEnvSteps                            648692
policy/Entropy                               -1.46896
policy/KL                                     0.00654111
policy/KLBefore                               0
policy/LossAfter                             -0.00940487
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.230164
policy/dLoss                                  0.00940488
---------------------------------------  ----------------
2021-06-04 13:56:22 | [train_policy] epoch #641 | Obtaining samples for iteration 641...
2021-06-04 13:56:23 | [train_policy] epoch #641 | Logging diagnostics...
2021-06-04 13:56:23 | [train_policy] epoch #641 | Optimizing policy...
2021-06-04 13:56:23 | [train_policy] epoch #641 | Computing loss before
2021-06-04 13:56:23 | [train_policy] epoch #641 | Computing KL before
2021-06-04 13:56:23 | [train_policy] epoch #641 | Optimizing
2021-06-04 13:56:23 | [train_policy] epoch #641 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:23 | [train_policy] epoch #641 | computing loss before
2021-06-04 13:56:23 | [train_policy] epoch #641 | computing gradient
2021-06-04 13:56:23 | [train_policy] epoch #641 | gradient computed
2021-06-04 13:56:23 | [train_policy] epoch #641 | computing descent direction
2021-06-04 13:56:23 | [train_policy] epoch #641 | descent direction computed
2021-06-04 13:56:23 | [train_policy] epoch #641 | backtrack iters: 0
2021-06-04 13:56:23 | [train_policy] epoch #641 | optimization finished
2021-06-04 13:56:23 | [train_policy] epoch #641 | Computing KL after
2021-06-04 13:56:23 | [train_policy] epoch #641 | Computing loss after
2021-06-04 13:56:23 | [train_policy] epoch #641 | Fitting baseline...
2021-06-04 13:56:23 | [train_policy] epoch #641 | Saving snapshot...
2021-06-04 13:56:23 | [train_policy] epoch #641 | Saved
2021-06-04 13:56:23 | [train_policy] epoch #641 | Time 515.00 s
2021-06-04 13:56:23 | [train_policy] epoch #641 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.282868
Evaluation/AverageDiscountedReturn          -42.6529
Evaluation/AverageReturn                    -42.6529
Evaluation/CompletionRate                     0
Evaluation/Iteration                        641
Evaluation/MaxReturn                        -28.8977
Evaluation/MinReturn                        -81.1152
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.7845
Extras/EpisodeRewardMean                    -42.6505
LinearFeatureBaseline/ExplainedVariance       0.850109
PolicyExecTime                                0.216586
ProcessExecTime                               0.0311077
TotalEnvSteps                            649704
policy/Entropy                               -1.48713
policy/KL                                     0.00989914
policy/KLBefore                               0
policy/LossAfter                             -0.0227016
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.22602
policy/dLoss                                  0.0227016
---------------------------------------  ----------------
2021-06-04 13:56:23 | [train_policy] epoch #642 | Obtaining samples for iteration 642...
2021-06-04 13:56:23 | [train_policy] epoch #642 | Logging diagnostics...
2021-06-04 13:56:23 | [train_policy] epoch #642 | Optimizing policy...
2021-06-04 13:56:23 | [train_policy] epoch #642 | Computing loss before
2021-06-04 13:56:23 | [train_policy] epoch #642 | Computing KL before
2021-06-04 13:56:23 | [train_policy] epoch #642 | Optimizing
2021-06-04 13:56:23 | [train_policy] epoch #642 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:23 | [train_policy] epoch #642 | computing loss before
2021-06-04 13:56:23 | [train_policy] epoch #642 | computing gradient
2021-06-04 13:56:23 | [train_policy] epoch #642 | gradient computed
2021-06-04 13:56:23 | [train_policy] epoch #642 | computing descent direction
2021-06-04 13:56:24 | [train_policy] epoch #642 | descent direction computed
2021-06-04 13:56:24 | [train_policy] epoch #642 | backtrack iters: 0
2021-06-04 13:56:24 | [train_policy] epoch #642 | optimization finished
2021-06-04 13:56:24 | [train_policy] epoch #642 | Computing KL after
2021-06-04 13:56:24 | [train_policy] epoch #642 | Computing loss after
2021-06-04 13:56:24 | [train_policy] epoch #642 | Fitting baseline...
2021-06-04 13:56:24 | [train_policy] epoch #642 | Saving snapshot...
2021-06-04 13:56:24 | [train_policy] epoch #642 | Saved
2021-06-04 13:56:24 | [train_policy] epoch #642 | Time 515.79 s
2021-06-04 13:56:24 | [train_policy] epoch #642 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283684
Evaluation/AverageDiscountedReturn          -41.7667
Evaluation/AverageReturn                    -41.7667
Evaluation/CompletionRate                     0
Evaluation/Iteration                        642
Evaluation/MaxReturn                        -29.778
Evaluation/MinReturn                        -77.2577
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25906
Extras/EpisodeRewardMean                    -41.8014
LinearFeatureBaseline/ExplainedVariance       0.89984
PolicyExecTime                                0.210741
ProcessExecTime                               0.0311069
TotalEnvSteps                            650716
policy/Entropy                               -1.48688
policy/KL                                     0.00996371
policy/KLBefore                               0
policy/LossAfter                             -0.0166328
policy/LossBefore                             1.81405e-08
policy/Perplexity                             0.226077
policy/dLoss                                  0.0166328
---------------------------------------  ----------------
2021-06-04 13:56:24 | [train_policy] epoch #643 | Obtaining samples for iteration 643...
2021-06-04 13:56:24 | [train_policy] epoch #643 | Logging diagnostics...
2021-06-04 13:56:24 | [train_policy] epoch #643 | Optimizing policy...
2021-06-04 13:56:24 | [train_policy] epoch #643 | Computing loss before
2021-06-04 13:56:24 | [train_policy] epoch #643 | Computing KL before
2021-06-04 13:56:24 | [train_policy] epoch #643 | Optimizing
2021-06-04 13:56:24 | [train_policy] epoch #643 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:24 | [train_policy] epoch #643 | computing loss before
2021-06-04 13:56:24 | [train_policy] epoch #643 | computing gradient
2021-06-04 13:56:24 | [train_policy] epoch #643 | gradient computed
2021-06-04 13:56:24 | [train_policy] epoch #643 | computing descent direction
2021-06-04 13:56:24 | [train_policy] epoch #643 | descent direction computed
2021-06-04 13:56:24 | [train_policy] epoch #643 | backtrack iters: 0
2021-06-04 13:56:24 | [train_policy] epoch #643 | optimization finished
2021-06-04 13:56:24 | [train_policy] epoch #643 | Computing KL after
2021-06-04 13:56:24 | [train_policy] epoch #643 | Computing loss after
2021-06-04 13:56:24 | [train_policy] epoch #643 | Fitting baseline...
2021-06-04 13:56:24 | [train_policy] epoch #643 | Saving snapshot...
2021-06-04 13:56:24 | [train_policy] epoch #643 | Saved
2021-06-04 13:56:24 | [train_policy] epoch #643 | Time 516.59 s
2021-06-04 13:56:24 | [train_policy] epoch #643 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283722
Evaluation/AverageDiscountedReturn          -41.0623
Evaluation/AverageReturn                    -41.0623
Evaluation/CompletionRate                     0
Evaluation/Iteration                        643
Evaluation/MaxReturn                        -28.2543
Evaluation/MinReturn                        -78.8052
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.58906
Extras/EpisodeRewardMean                    -41.3945
LinearFeatureBaseline/ExplainedVariance       0.827576
PolicyExecTime                                0.225221
ProcessExecTime                               0.0311916
TotalEnvSteps                            651728
policy/Entropy                               -1.48658
policy/KL                                     0.00979255
policy/KLBefore                               0
policy/LossAfter                             -0.0209604
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.226144
policy/dLoss                                  0.0209604
---------------------------------------  ----------------
2021-06-04 13:56:24 | [train_policy] epoch #644 | Obtaining samples for iteration 644...
2021-06-04 13:56:25 | [train_policy] epoch #644 | Logging diagnostics...
2021-06-04 13:56:25 | [train_policy] epoch #644 | Optimizing policy...
2021-06-04 13:56:25 | [train_policy] epoch #644 | Computing loss before
2021-06-04 13:56:25 | [train_policy] epoch #644 | Computing KL before
2021-06-04 13:56:25 | [train_policy] epoch #644 | Optimizing
2021-06-04 13:56:25 | [train_policy] epoch #644 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:25 | [train_policy] epoch #644 | computing loss before
2021-06-04 13:56:25 | [train_policy] epoch #644 | computing gradient
2021-06-04 13:56:25 | [train_policy] epoch #644 | gradient computed
2021-06-04 13:56:25 | [train_policy] epoch #644 | computing descent direction
2021-06-04 13:56:25 | [train_policy] epoch #644 | descent direction computed
2021-06-04 13:56:25 | [train_policy] epoch #644 | backtrack iters: 1
2021-06-04 13:56:25 | [train_policy] epoch #644 | optimization finished
2021-06-04 13:56:25 | [train_policy] epoch #644 | Computing KL after
2021-06-04 13:56:25 | [train_policy] epoch #644 | Computing loss after
2021-06-04 13:56:25 | [train_policy] epoch #644 | Fitting baseline...
2021-06-04 13:56:25 | [train_policy] epoch #644 | Saving snapshot...
2021-06-04 13:56:25 | [train_policy] epoch #644 | Saved
2021-06-04 13:56:25 | [train_policy] epoch #644 | Time 517.39 s
2021-06-04 13:56:25 | [train_policy] epoch #644 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285291
Evaluation/AverageDiscountedReturn          -42.3994
Evaluation/AverageReturn                    -42.3994
Evaluation/CompletionRate                     0
Evaluation/Iteration                        644
Evaluation/MaxReturn                        -30.5543
Evaluation/MinReturn                        -76.6375
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.02527
Extras/EpisodeRewardMean                    -42.3456
LinearFeatureBaseline/ExplainedVariance       0.890134
PolicyExecTime                                0.214027
ProcessExecTime                               0.0312798
TotalEnvSteps                            652740
policy/Entropy                               -1.50971
policy/KL                                     0.00680214
policy/KLBefore                               0
policy/LossAfter                             -0.0248367
policy/LossBefore                            -1.13084e-08
policy/Perplexity                             0.220975
policy/dLoss                                  0.0248367
---------------------------------------  ----------------
2021-06-04 13:56:25 | [train_policy] epoch #645 | Obtaining samples for iteration 645...
2021-06-04 13:56:26 | [train_policy] epoch #645 | Logging diagnostics...
2021-06-04 13:56:26 | [train_policy] epoch #645 | Optimizing policy...
2021-06-04 13:56:26 | [train_policy] epoch #645 | Computing loss before
2021-06-04 13:56:26 | [train_policy] epoch #645 | Computing KL before
2021-06-04 13:56:26 | [train_policy] epoch #645 | Optimizing
2021-06-04 13:56:26 | [train_policy] epoch #645 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:26 | [train_policy] epoch #645 | computing loss before
2021-06-04 13:56:26 | [train_policy] epoch #645 | computing gradient
2021-06-04 13:56:26 | [train_policy] epoch #645 | gradient computed
2021-06-04 13:56:26 | [train_policy] epoch #645 | computing descent direction
2021-06-04 13:56:26 | [train_policy] epoch #645 | descent direction computed
2021-06-04 13:56:26 | [train_policy] epoch #645 | backtrack iters: 1
2021-06-04 13:56:26 | [train_policy] epoch #645 | optimization finished
2021-06-04 13:56:26 | [train_policy] epoch #645 | Computing KL after
2021-06-04 13:56:26 | [train_policy] epoch #645 | Computing loss after
2021-06-04 13:56:26 | [train_policy] epoch #645 | Fitting baseline...
2021-06-04 13:56:26 | [train_policy] epoch #645 | Saving snapshot...
2021-06-04 13:56:26 | [train_policy] epoch #645 | Saved
2021-06-04 13:56:26 | [train_policy] epoch #645 | Time 518.20 s
2021-06-04 13:56:26 | [train_policy] epoch #645 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.28556
Evaluation/AverageDiscountedReturn          -41.8532
Evaluation/AverageReturn                    -41.8532
Evaluation/CompletionRate                     0
Evaluation/Iteration                        645
Evaluation/MaxReturn                        -28.9478
Evaluation/MinReturn                        -64.1063
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05632
Extras/EpisodeRewardMean                    -41.9344
LinearFeatureBaseline/ExplainedVariance       0.899194
PolicyExecTime                                0.221564
ProcessExecTime                               0.0312247
TotalEnvSteps                            653752
policy/Entropy                               -1.52678
policy/KL                                     0.00678559
policy/KLBefore                               0
policy/LossAfter                             -0.0171401
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.217233
policy/dLoss                                  0.0171401
---------------------------------------  ----------------
2021-06-04 13:56:26 | [train_policy] epoch #646 | Obtaining samples for iteration 646...
2021-06-04 13:56:27 | [train_policy] epoch #646 | Logging diagnostics...
2021-06-04 13:56:27 | [train_policy] epoch #646 | Optimizing policy...
2021-06-04 13:56:27 | [train_policy] epoch #646 | Computing loss before
2021-06-04 13:56:27 | [train_policy] epoch #646 | Computing KL before
2021-06-04 13:56:27 | [train_policy] epoch #646 | Optimizing
2021-06-04 13:56:27 | [train_policy] epoch #646 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:27 | [train_policy] epoch #646 | computing loss before
2021-06-04 13:56:27 | [train_policy] epoch #646 | computing gradient
2021-06-04 13:56:27 | [train_policy] epoch #646 | gradient computed
2021-06-04 13:56:27 | [train_policy] epoch #646 | computing descent direction
2021-06-04 13:56:27 | [train_policy] epoch #646 | descent direction computed
2021-06-04 13:56:27 | [train_policy] epoch #646 | backtrack iters: 1
2021-06-04 13:56:27 | [train_policy] epoch #646 | optimization finished
2021-06-04 13:56:27 | [train_policy] epoch #646 | Computing KL after
2021-06-04 13:56:27 | [train_policy] epoch #646 | Computing loss after
2021-06-04 13:56:27 | [train_policy] epoch #646 | Fitting baseline...
2021-06-04 13:56:27 | [train_policy] epoch #646 | Saving snapshot...
2021-06-04 13:56:27 | [train_policy] epoch #646 | Saved
2021-06-04 13:56:27 | [train_policy] epoch #646 | Time 519.02 s
2021-06-04 13:56:27 | [train_policy] epoch #646 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283477
Evaluation/AverageDiscountedReturn          -41.7622
Evaluation/AverageReturn                    -41.7622
Evaluation/CompletionRate                     0
Evaluation/Iteration                        646
Evaluation/MaxReturn                        -28.8809
Evaluation/MinReturn                        -79.5346
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.34528
Extras/EpisodeRewardMean                    -41.8314
LinearFeatureBaseline/ExplainedVariance       0.878663
PolicyExecTime                                0.225523
ProcessExecTime                               0.0312064
TotalEnvSteps                            654764
policy/Entropy                               -1.55635
policy/KL                                     0.00672979
policy/KLBefore                               0
policy/LossAfter                             -0.0224132
policy/LossBefore                            -8.71688e-09
policy/Perplexity                             0.210905
policy/dLoss                                  0.0224132
---------------------------------------  ----------------
2021-06-04 13:56:27 | [train_policy] epoch #647 | Obtaining samples for iteration 647...
2021-06-04 13:56:27 | [train_policy] epoch #647 | Logging diagnostics...
2021-06-04 13:56:27 | [train_policy] epoch #647 | Optimizing policy...
2021-06-04 13:56:27 | [train_policy] epoch #647 | Computing loss before
2021-06-04 13:56:27 | [train_policy] epoch #647 | Computing KL before
2021-06-04 13:56:27 | [train_policy] epoch #647 | Optimizing
2021-06-04 13:56:27 | [train_policy] epoch #647 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:27 | [train_policy] epoch #647 | computing loss before
2021-06-04 13:56:27 | [train_policy] epoch #647 | computing gradient
2021-06-04 13:56:27 | [train_policy] epoch #647 | gradient computed
2021-06-04 13:56:27 | [train_policy] epoch #647 | computing descent direction
2021-06-04 13:56:28 | [train_policy] epoch #647 | descent direction computed
2021-06-04 13:56:28 | [train_policy] epoch #647 | backtrack iters: 1
2021-06-04 13:56:28 | [train_policy] epoch #647 | optimization finished
2021-06-04 13:56:28 | [train_policy] epoch #647 | Computing KL after
2021-06-04 13:56:28 | [train_policy] epoch #647 | Computing loss after
2021-06-04 13:56:28 | [train_policy] epoch #647 | Fitting baseline...
2021-06-04 13:56:28 | [train_policy] epoch #647 | Saving snapshot...
2021-06-04 13:56:28 | [train_policy] epoch #647 | Saved
2021-06-04 13:56:28 | [train_policy] epoch #647 | Time 519.82 s
2021-06-04 13:56:28 | [train_policy] epoch #647 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.28357
Evaluation/AverageDiscountedReturn          -40.9352
Evaluation/AverageReturn                    -40.9352
Evaluation/CompletionRate                     0
Evaluation/Iteration                        647
Evaluation/MaxReturn                        -31.8571
Evaluation/MinReturn                        -57.9044
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.53477
Extras/EpisodeRewardMean                    -41.2619
LinearFeatureBaseline/ExplainedVariance       0.914348
PolicyExecTime                                0.226445
ProcessExecTime                               0.031111
TotalEnvSteps                            655776
policy/Entropy                               -1.562
policy/KL                                     0.00671593
policy/KLBefore                               0
policy/LossAfter                             -0.0209894
policy/LossBefore                             1.2663e-08
policy/Perplexity                             0.209716
policy/dLoss                                  0.0209894
---------------------------------------  ---------------
2021-06-04 13:56:28 | [train_policy] epoch #648 | Obtaining samples for iteration 648...
2021-06-04 13:56:28 | [train_policy] epoch #648 | Logging diagnostics...
2021-06-04 13:56:28 | [train_policy] epoch #648 | Optimizing policy...
2021-06-04 13:56:28 | [train_policy] epoch #648 | Computing loss before
2021-06-04 13:56:28 | [train_policy] epoch #648 | Computing KL before
2021-06-04 13:56:28 | [train_policy] epoch #648 | Optimizing
2021-06-04 13:56:28 | [train_policy] epoch #648 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:28 | [train_policy] epoch #648 | computing loss before
2021-06-04 13:56:28 | [train_policy] epoch #648 | computing gradient
2021-06-04 13:56:28 | [train_policy] epoch #648 | gradient computed
2021-06-04 13:56:28 | [train_policy] epoch #648 | computing descent direction
2021-06-04 13:56:28 | [train_policy] epoch #648 | descent direction computed
2021-06-04 13:56:28 | [train_policy] epoch #648 | backtrack iters: 1
2021-06-04 13:56:28 | [train_policy] epoch #648 | optimization finished
2021-06-04 13:56:28 | [train_policy] epoch #648 | Computing KL after
2021-06-04 13:56:28 | [train_policy] epoch #648 | Computing loss after
2021-06-04 13:56:28 | [train_policy] epoch #648 | Fitting baseline...
2021-06-04 13:56:28 | [train_policy] epoch #648 | Saving snapshot...
2021-06-04 13:56:28 | [train_policy] epoch #648 | Saved
2021-06-04 13:56:28 | [train_policy] epoch #648 | Time 520.62 s
2021-06-04 13:56:28 | [train_policy] epoch #648 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284332
Evaluation/AverageDiscountedReturn          -41.059
Evaluation/AverageReturn                    -41.059
Evaluation/CompletionRate                     0
Evaluation/Iteration                        648
Evaluation/MaxReturn                        -28.7182
Evaluation/MinReturn                        -64.0207
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.24768
Extras/EpisodeRewardMean                    -41.2808
LinearFeatureBaseline/ExplainedVariance       0.893457
PolicyExecTime                                0.223562
ProcessExecTime                               0.0311601
TotalEnvSteps                            656788
policy/Entropy                               -1.54939
policy/KL                                     0.00649246
policy/KLBefore                               0
policy/LossAfter                             -0.0144778
policy/LossBefore                            -1.93185e-08
policy/Perplexity                             0.212378
policy/dLoss                                  0.0144778
---------------------------------------  ----------------
2021-06-04 13:56:28 | [train_policy] epoch #649 | Obtaining samples for iteration 649...
2021-06-04 13:56:29 | [train_policy] epoch #649 | Logging diagnostics...
2021-06-04 13:56:29 | [train_policy] epoch #649 | Optimizing policy...
2021-06-04 13:56:29 | [train_policy] epoch #649 | Computing loss before
2021-06-04 13:56:29 | [train_policy] epoch #649 | Computing KL before
2021-06-04 13:56:29 | [train_policy] epoch #649 | Optimizing
2021-06-04 13:56:29 | [train_policy] epoch #649 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:29 | [train_policy] epoch #649 | computing loss before
2021-06-04 13:56:29 | [train_policy] epoch #649 | computing gradient
2021-06-04 13:56:29 | [train_policy] epoch #649 | gradient computed
2021-06-04 13:56:29 | [train_policy] epoch #649 | computing descent direction
2021-06-04 13:56:29 | [train_policy] epoch #649 | descent direction computed
2021-06-04 13:56:29 | [train_policy] epoch #649 | backtrack iters: 1
2021-06-04 13:56:29 | [train_policy] epoch #649 | optimization finished
2021-06-04 13:56:29 | [train_policy] epoch #649 | Computing KL after
2021-06-04 13:56:29 | [train_policy] epoch #649 | Computing loss after
2021-06-04 13:56:29 | [train_policy] epoch #649 | Fitting baseline...
2021-06-04 13:56:29 | [train_policy] epoch #649 | Saving snapshot...
2021-06-04 13:56:29 | [train_policy] epoch #649 | Saved
2021-06-04 13:56:29 | [train_policy] epoch #649 | Time 521.45 s
2021-06-04 13:56:29 | [train_policy] epoch #649 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.288977
Evaluation/AverageDiscountedReturn          -42.3161
Evaluation/AverageReturn                    -42.3161
Evaluation/CompletionRate                     0
Evaluation/Iteration                        649
Evaluation/MaxReturn                        -28.8402
Evaluation/MinReturn                        -60.6107
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.77632
Extras/EpisodeRewardMean                    -41.9552
LinearFeatureBaseline/ExplainedVariance       0.899604
PolicyExecTime                                0.234334
ProcessExecTime                               0.0316851
TotalEnvSteps                            657800
policy/Entropy                               -1.58193
policy/KL                                     0.00661884
policy/KLBefore                               0
policy/LossAfter                             -0.0129092
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.205578
policy/dLoss                                  0.0129092
---------------------------------------  ----------------
2021-06-04 13:56:29 | [train_policy] epoch #650 | Obtaining samples for iteration 650...
2021-06-04 13:56:30 | [train_policy] epoch #650 | Logging diagnostics...
2021-06-04 13:56:30 | [train_policy] epoch #650 | Optimizing policy...
2021-06-04 13:56:30 | [train_policy] epoch #650 | Computing loss before
2021-06-04 13:56:30 | [train_policy] epoch #650 | Computing KL before
2021-06-04 13:56:30 | [train_policy] epoch #650 | Optimizing
2021-06-04 13:56:30 | [train_policy] epoch #650 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:30 | [train_policy] epoch #650 | computing loss before
2021-06-04 13:56:30 | [train_policy] epoch #650 | computing gradient
2021-06-04 13:56:30 | [train_policy] epoch #650 | gradient computed
2021-06-04 13:56:30 | [train_policy] epoch #650 | computing descent direction
2021-06-04 13:56:30 | [train_policy] epoch #650 | descent direction computed
2021-06-04 13:56:30 | [train_policy] epoch #650 | backtrack iters: 1
2021-06-04 13:56:30 | [train_policy] epoch #650 | optimization finished
2021-06-04 13:56:30 | [train_policy] epoch #650 | Computing KL after
2021-06-04 13:56:30 | [train_policy] epoch #650 | Computing loss after
2021-06-04 13:56:30 | [train_policy] epoch #650 | Fitting baseline...
2021-06-04 13:56:30 | [train_policy] epoch #650 | Saving snapshot...
2021-06-04 13:56:30 | [train_policy] epoch #650 | Saved
2021-06-04 13:56:30 | [train_policy] epoch #650 | Time 522.25 s
2021-06-04 13:56:30 | [train_policy] epoch #650 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285085
Evaluation/AverageDiscountedReturn          -42.3321
Evaluation/AverageReturn                    -42.3321
Evaluation/CompletionRate                     0
Evaluation/Iteration                        650
Evaluation/MaxReturn                        -28.4333
Evaluation/MinReturn                        -79.1907
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.81506
Extras/EpisodeRewardMean                    -42.4468
LinearFeatureBaseline/ExplainedVariance       0.889316
PolicyExecTime                                0.218317
ProcessExecTime                               0.0311677
TotalEnvSteps                            658812
policy/Entropy                               -1.62128
policy/KL                                     0.00662261
policy/KLBefore                               0
policy/LossAfter                             -0.00922932
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.197645
policy/dLoss                                  0.00922932
---------------------------------------  ----------------
2021-06-04 13:56:30 | [train_policy] epoch #651 | Obtaining samples for iteration 651...
2021-06-04 13:56:31 | [train_policy] epoch #651 | Logging diagnostics...
2021-06-04 13:56:31 | [train_policy] epoch #651 | Optimizing policy...
2021-06-04 13:56:31 | [train_policy] epoch #651 | Computing loss before
2021-06-04 13:56:31 | [train_policy] epoch #651 | Computing KL before
2021-06-04 13:56:31 | [train_policy] epoch #651 | Optimizing
2021-06-04 13:56:31 | [train_policy] epoch #651 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:31 | [train_policy] epoch #651 | computing loss before
2021-06-04 13:56:31 | [train_policy] epoch #651 | computing gradient
2021-06-04 13:56:31 | [train_policy] epoch #651 | gradient computed
2021-06-04 13:56:31 | [train_policy] epoch #651 | computing descent direction
2021-06-04 13:56:31 | [train_policy] epoch #651 | descent direction computed
2021-06-04 13:56:31 | [train_policy] epoch #651 | backtrack iters: 1
2021-06-04 13:56:31 | [train_policy] epoch #651 | optimization finished
2021-06-04 13:56:31 | [train_policy] epoch #651 | Computing KL after
2021-06-04 13:56:31 | [train_policy] epoch #651 | Computing loss after
2021-06-04 13:56:31 | [train_policy] epoch #651 | Fitting baseline...
2021-06-04 13:56:31 | [train_policy] epoch #651 | Saving snapshot...
2021-06-04 13:56:31 | [train_policy] epoch #651 | Saved
2021-06-04 13:56:31 | [train_policy] epoch #651 | Time 523.05 s
2021-06-04 13:56:31 | [train_policy] epoch #651 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283262
Evaluation/AverageDiscountedReturn          -41.7439
Evaluation/AverageReturn                    -41.7439
Evaluation/CompletionRate                     0
Evaluation/Iteration                        651
Evaluation/MaxReturn                        -27.8702
Evaluation/MinReturn                        -78.0073
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.96643
Extras/EpisodeRewardMean                    -41.731
LinearFeatureBaseline/ExplainedVariance       0.874282
PolicyExecTime                                0.220488
ProcessExecTime                               0.0310519
TotalEnvSteps                            659824
policy/Entropy                               -1.6112
policy/KL                                     0.00644081
policy/KLBefore                               0
policy/LossAfter                             -0.0176123
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.199648
policy/dLoss                                  0.0176123
---------------------------------------  ----------------
2021-06-04 13:56:31 | [train_policy] epoch #652 | Obtaining samples for iteration 652...
2021-06-04 13:56:31 | [train_policy] epoch #652 | Logging diagnostics...
2021-06-04 13:56:31 | [train_policy] epoch #652 | Optimizing policy...
2021-06-04 13:56:31 | [train_policy] epoch #652 | Computing loss before
2021-06-04 13:56:31 | [train_policy] epoch #652 | Computing KL before
2021-06-04 13:56:31 | [train_policy] epoch #652 | Optimizing
2021-06-04 13:56:31 | [train_policy] epoch #652 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:31 | [train_policy] epoch #652 | computing loss before
2021-06-04 13:56:31 | [train_policy] epoch #652 | computing gradient
2021-06-04 13:56:31 | [train_policy] epoch #652 | gradient computed
2021-06-04 13:56:31 | [train_policy] epoch #652 | computing descent direction
2021-06-04 13:56:32 | [train_policy] epoch #652 | descent direction computed
2021-06-04 13:56:32 | [train_policy] epoch #652 | backtrack iters: 1
2021-06-04 13:56:32 | [train_policy] epoch #652 | optimization finished
2021-06-04 13:56:32 | [train_policy] epoch #652 | Computing KL after
2021-06-04 13:56:32 | [train_policy] epoch #652 | Computing loss after
2021-06-04 13:56:32 | [train_policy] epoch #652 | Fitting baseline...
2021-06-04 13:56:32 | [train_policy] epoch #652 | Saving snapshot...
2021-06-04 13:56:32 | [train_policy] epoch #652 | Saved
2021-06-04 13:56:32 | [train_policy] epoch #652 | Time 523.86 s
2021-06-04 13:56:32 | [train_policy] epoch #652 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284011
Evaluation/AverageDiscountedReturn          -42.8209
Evaluation/AverageReturn                    -42.8209
Evaluation/CompletionRate                     0
Evaluation/Iteration                        652
Evaluation/MaxReturn                        -31.2646
Evaluation/MinReturn                        -78.4522
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.32363
Extras/EpisodeRewardMean                    -42.5104
LinearFeatureBaseline/ExplainedVariance       0.806209
PolicyExecTime                                0.23115
ProcessExecTime                               0.0310819
TotalEnvSteps                            660836
policy/Entropy                               -1.62753
policy/KL                                     0.00642164
policy/KLBefore                               0
policy/LossAfter                             -0.0152984
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.196413
policy/dLoss                                  0.0152984
---------------------------------------  ----------------
2021-06-04 13:56:32 | [train_policy] epoch #653 | Obtaining samples for iteration 653...
2021-06-04 13:56:32 | [train_policy] epoch #653 | Logging diagnostics...
2021-06-04 13:56:32 | [train_policy] epoch #653 | Optimizing policy...
2021-06-04 13:56:32 | [train_policy] epoch #653 | Computing loss before
2021-06-04 13:56:32 | [train_policy] epoch #653 | Computing KL before
2021-06-04 13:56:32 | [train_policy] epoch #653 | Optimizing
2021-06-04 13:56:32 | [train_policy] epoch #653 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:32 | [train_policy] epoch #653 | computing loss before
2021-06-04 13:56:32 | [train_policy] epoch #653 | computing gradient
2021-06-04 13:56:32 | [train_policy] epoch #653 | gradient computed
2021-06-04 13:56:32 | [train_policy] epoch #653 | computing descent direction
2021-06-04 13:56:32 | [train_policy] epoch #653 | descent direction computed
2021-06-04 13:56:32 | [train_policy] epoch #653 | backtrack iters: 0
2021-06-04 13:56:32 | [train_policy] epoch #653 | optimization finished
2021-06-04 13:56:32 | [train_policy] epoch #653 | Computing KL after
2021-06-04 13:56:32 | [train_policy] epoch #653 | Computing loss after
2021-06-04 13:56:32 | [train_policy] epoch #653 | Fitting baseline...
2021-06-04 13:56:32 | [train_policy] epoch #653 | Saving snapshot...
2021-06-04 13:56:32 | [train_policy] epoch #653 | Saved
2021-06-04 13:56:32 | [train_policy] epoch #653 | Time 524.66 s
2021-06-04 13:56:32 | [train_policy] epoch #653 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.283507
Evaluation/AverageDiscountedReturn          -41.9957
Evaluation/AverageReturn                    -41.9957
Evaluation/CompletionRate                     0
Evaluation/Iteration                        653
Evaluation/MaxReturn                        -28.5966
Evaluation/MinReturn                        -76.428
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.53028
Extras/EpisodeRewardMean                    -41.9226
LinearFeatureBaseline/ExplainedVariance       0.867919
PolicyExecTime                                0.219643
ProcessExecTime                               0.0311766
TotalEnvSteps                            661848
policy/Entropy                               -1.62191
policy/KL                                     0.0098932
policy/KLBefore                               0
policy/LossAfter                             -0.0223473
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.19752
policy/dLoss                                  0.0223473
---------------------------------------  ---------------
2021-06-04 13:56:32 | [train_policy] epoch #654 | Obtaining samples for iteration 654...
2021-06-04 13:56:33 | [train_policy] epoch #654 | Logging diagnostics...
2021-06-04 13:56:33 | [train_policy] epoch #654 | Optimizing policy...
2021-06-04 13:56:33 | [train_policy] epoch #654 | Computing loss before
2021-06-04 13:56:33 | [train_policy] epoch #654 | Computing KL before
2021-06-04 13:56:33 | [train_policy] epoch #654 | Optimizing
2021-06-04 13:56:33 | [train_policy] epoch #654 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:33 | [train_policy] epoch #654 | computing loss before
2021-06-04 13:56:33 | [train_policy] epoch #654 | computing gradient
2021-06-04 13:56:33 | [train_policy] epoch #654 | gradient computed
2021-06-04 13:56:33 | [train_policy] epoch #654 | computing descent direction
2021-06-04 13:56:33 | [train_policy] epoch #654 | descent direction computed
2021-06-04 13:56:33 | [train_policy] epoch #654 | backtrack iters: 1
2021-06-04 13:56:33 | [train_policy] epoch #654 | optimization finished
2021-06-04 13:56:33 | [train_policy] epoch #654 | Computing KL after
2021-06-04 13:56:33 | [train_policy] epoch #654 | Computing loss after
2021-06-04 13:56:33 | [train_policy] epoch #654 | Fitting baseline...
2021-06-04 13:56:33 | [train_policy] epoch #654 | Saving snapshot...
2021-06-04 13:56:33 | [train_policy] epoch #654 | Saved
2021-06-04 13:56:33 | [train_policy] epoch #654 | Time 525.46 s
2021-06-04 13:56:33 | [train_policy] epoch #654 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284805
Evaluation/AverageDiscountedReturn          -41.6777
Evaluation/AverageReturn                    -41.6777
Evaluation/CompletionRate                     0
Evaluation/Iteration                        654
Evaluation/MaxReturn                        -28.0767
Evaluation/MinReturn                        -79.4099
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.46898
Extras/EpisodeRewardMean                    -42.3758
LinearFeatureBaseline/ExplainedVariance       0.871228
PolicyExecTime                                0.223653
ProcessExecTime                               0.0310922
TotalEnvSteps                            662860
policy/Entropy                               -1.65449
policy/KL                                     0.00667915
policy/KLBefore                               0
policy/LossAfter                             -0.0155462
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.191189
policy/dLoss                                  0.0155462
---------------------------------------  ----------------
2021-06-04 13:56:33 | [train_policy] epoch #655 | Obtaining samples for iteration 655...
2021-06-04 13:56:34 | [train_policy] epoch #655 | Logging diagnostics...
2021-06-04 13:56:34 | [train_policy] epoch #655 | Optimizing policy...
2021-06-04 13:56:34 | [train_policy] epoch #655 | Computing loss before
2021-06-04 13:56:34 | [train_policy] epoch #655 | Computing KL before
2021-06-04 13:56:34 | [train_policy] epoch #655 | Optimizing
2021-06-04 13:56:34 | [train_policy] epoch #655 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:34 | [train_policy] epoch #655 | computing loss before
2021-06-04 13:56:34 | [train_policy] epoch #655 | computing gradient
2021-06-04 13:56:34 | [train_policy] epoch #655 | gradient computed
2021-06-04 13:56:34 | [train_policy] epoch #655 | computing descent direction
2021-06-04 13:56:34 | [train_policy] epoch #655 | descent direction computed
2021-06-04 13:56:34 | [train_policy] epoch #655 | backtrack iters: 0
2021-06-04 13:56:34 | [train_policy] epoch #655 | optimization finished
2021-06-04 13:56:34 | [train_policy] epoch #655 | Computing KL after
2021-06-04 13:56:34 | [train_policy] epoch #655 | Computing loss after
2021-06-04 13:56:34 | [train_policy] epoch #655 | Fitting baseline...
2021-06-04 13:56:34 | [train_policy] epoch #655 | Saving snapshot...
2021-06-04 13:56:34 | [train_policy] epoch #655 | Saved
2021-06-04 13:56:34 | [train_policy] epoch #655 | Time 526.26 s
2021-06-04 13:56:34 | [train_policy] epoch #655 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284288
Evaluation/AverageDiscountedReturn          -42.6294
Evaluation/AverageReturn                    -42.6294
Evaluation/CompletionRate                     0
Evaluation/Iteration                        655
Evaluation/MaxReturn                        -28.0429
Evaluation/MinReturn                        -77.3948
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2891
Extras/EpisodeRewardMean                    -42.5802
LinearFeatureBaseline/ExplainedVariance       0.867788
PolicyExecTime                                0.222345
ProcessExecTime                               0.0312011
TotalEnvSteps                            663872
policy/Entropy                               -1.63038
policy/KL                                     0.00961783
policy/KLBefore                               0
policy/LossAfter                             -0.0210508
policy/LossBefore                             7.89231e-09
policy/Perplexity                             0.195854
policy/dLoss                                  0.0210508
---------------------------------------  ----------------
2021-06-04 13:56:34 | [train_policy] epoch #656 | Obtaining samples for iteration 656...
2021-06-04 13:56:35 | [train_policy] epoch #656 | Logging diagnostics...
2021-06-04 13:56:35 | [train_policy] epoch #656 | Optimizing policy...
2021-06-04 13:56:35 | [train_policy] epoch #656 | Computing loss before
2021-06-04 13:56:35 | [train_policy] epoch #656 | Computing KL before
2021-06-04 13:56:35 | [train_policy] epoch #656 | Optimizing
2021-06-04 13:56:35 | [train_policy] epoch #656 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:35 | [train_policy] epoch #656 | computing loss before
2021-06-04 13:56:35 | [train_policy] epoch #656 | computing gradient
2021-06-04 13:56:35 | [train_policy] epoch #656 | gradient computed
2021-06-04 13:56:35 | [train_policy] epoch #656 | computing descent direction
2021-06-04 13:56:35 | [train_policy] epoch #656 | descent direction computed
2021-06-04 13:56:35 | [train_policy] epoch #656 | backtrack iters: 1
2021-06-04 13:56:35 | [train_policy] epoch #656 | optimization finished
2021-06-04 13:56:35 | [train_policy] epoch #656 | Computing KL after
2021-06-04 13:56:35 | [train_policy] epoch #656 | Computing loss after
2021-06-04 13:56:35 | [train_policy] epoch #656 | Fitting baseline...
2021-06-04 13:56:35 | [train_policy] epoch #656 | Saving snapshot...
2021-06-04 13:56:35 | [train_policy] epoch #656 | Saved
2021-06-04 13:56:35 | [train_policy] epoch #656 | Time 527.05 s
2021-06-04 13:56:35 | [train_policy] epoch #656 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285509
Evaluation/AverageDiscountedReturn          -41.4707
Evaluation/AverageReturn                    -41.4707
Evaluation/CompletionRate                     0
Evaluation/Iteration                        656
Evaluation/MaxReturn                        -27.7336
Evaluation/MinReturn                        -63.9008
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.96839
Extras/EpisodeRewardMean                    -41.3463
LinearFeatureBaseline/ExplainedVariance       0.879866
PolicyExecTime                                0.21146
ProcessExecTime                               0.0313914
TotalEnvSteps                            664884
policy/Entropy                               -1.64965
policy/KL                                     0.00646389
policy/KLBefore                               0
policy/LossAfter                             -0.0180102
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.192118
policy/dLoss                                  0.0180102
---------------------------------------  ----------------
2021-06-04 13:56:35 | [train_policy] epoch #657 | Obtaining samples for iteration 657...
2021-06-04 13:56:35 | [train_policy] epoch #657 | Logging diagnostics...
2021-06-04 13:56:35 | [train_policy] epoch #657 | Optimizing policy...
2021-06-04 13:56:35 | [train_policy] epoch #657 | Computing loss before
2021-06-04 13:56:35 | [train_policy] epoch #657 | Computing KL before
2021-06-04 13:56:35 | [train_policy] epoch #657 | Optimizing
2021-06-04 13:56:35 | [train_policy] epoch #657 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:35 | [train_policy] epoch #657 | computing loss before
2021-06-04 13:56:35 | [train_policy] epoch #657 | computing gradient
2021-06-04 13:56:35 | [train_policy] epoch #657 | gradient computed
2021-06-04 13:56:35 | [train_policy] epoch #657 | computing descent direction
2021-06-04 13:56:36 | [train_policy] epoch #657 | descent direction computed
2021-06-04 13:56:36 | [train_policy] epoch #657 | backtrack iters: 0
2021-06-04 13:56:36 | [train_policy] epoch #657 | optimization finished
2021-06-04 13:56:36 | [train_policy] epoch #657 | Computing KL after
2021-06-04 13:56:36 | [train_policy] epoch #657 | Computing loss after
2021-06-04 13:56:36 | [train_policy] epoch #657 | Fitting baseline...
2021-06-04 13:56:36 | [train_policy] epoch #657 | Saving snapshot...
2021-06-04 13:56:36 | [train_policy] epoch #657 | Saved
2021-06-04 13:56:36 | [train_policy] epoch #657 | Time 527.87 s
2021-06-04 13:56:36 | [train_policy] epoch #657 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.284868
Evaluation/AverageDiscountedReturn          -41.6304
Evaluation/AverageReturn                    -41.6304
Evaluation/CompletionRate                     0
Evaluation/Iteration                        657
Evaluation/MaxReturn                        -30.1433
Evaluation/MinReturn                        -63.9958
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.27425
Extras/EpisodeRewardMean                    -41.818
LinearFeatureBaseline/ExplainedVariance       0.901032
PolicyExecTime                                0.237798
ProcessExecTime                               0.0310359
TotalEnvSteps                            665896
policy/Entropy                               -1.63706
policy/KL                                     0.00946474
policy/KLBefore                               0
policy/LossAfter                             -0.0187671
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             0.194551
policy/dLoss                                  0.0187671
---------------------------------------  ----------------
2021-06-04 13:56:36 | [train_policy] epoch #658 | Obtaining samples for iteration 658...
2021-06-04 13:56:36 | [train_policy] epoch #658 | Logging diagnostics...
2021-06-04 13:56:36 | [train_policy] epoch #658 | Optimizing policy...
2021-06-04 13:56:36 | [train_policy] epoch #658 | Computing loss before
2021-06-04 13:56:36 | [train_policy] epoch #658 | Computing KL before
2021-06-04 13:56:36 | [train_policy] epoch #658 | Optimizing
2021-06-04 13:56:36 | [train_policy] epoch #658 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:36 | [train_policy] epoch #658 | computing loss before
2021-06-04 13:56:36 | [train_policy] epoch #658 | computing gradient
2021-06-04 13:56:36 | [train_policy] epoch #658 | gradient computed
2021-06-04 13:56:36 | [train_policy] epoch #658 | computing descent direction
2021-06-04 13:56:36 | [train_policy] epoch #658 | descent direction computed
2021-06-04 13:56:36 | [train_policy] epoch #658 | backtrack iters: 1
2021-06-04 13:56:36 | [train_policy] epoch #658 | optimization finished
2021-06-04 13:56:36 | [train_policy] epoch #658 | Computing KL after
2021-06-04 13:56:36 | [train_policy] epoch #658 | Computing loss after
2021-06-04 13:56:36 | [train_policy] epoch #658 | Fitting baseline...
2021-06-04 13:56:36 | [train_policy] epoch #658 | Saving snapshot...
2021-06-04 13:56:36 | [train_policy] epoch #658 | Saved
2021-06-04 13:56:36 | [train_policy] epoch #658 | Time 528.69 s
2021-06-04 13:56:36 | [train_policy] epoch #658 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.288363
Evaluation/AverageDiscountedReturn          -42.006
Evaluation/AverageReturn                    -42.006
Evaluation/CompletionRate                     0
Evaluation/Iteration                        658
Evaluation/MaxReturn                        -27.925
Evaluation/MinReturn                        -77.9105
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.63106
Extras/EpisodeRewardMean                    -42.0023
LinearFeatureBaseline/ExplainedVariance       0.875243
PolicyExecTime                                0.234469
ProcessExecTime                               0.0315406
TotalEnvSteps                            666908
policy/Entropy                               -1.63316
policy/KL                                     0.00657484
policy/KLBefore                               0
policy/LossAfter                             -0.0123596
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             0.195311
policy/dLoss                                  0.0123595
---------------------------------------  ----------------
2021-06-04 13:56:36 | [train_policy] epoch #659 | Obtaining samples for iteration 659...
2021-06-04 13:56:37 | [train_policy] epoch #659 | Logging diagnostics...
2021-06-04 13:56:37 | [train_policy] epoch #659 | Optimizing policy...
2021-06-04 13:56:37 | [train_policy] epoch #659 | Computing loss before
2021-06-04 13:56:37 | [train_policy] epoch #659 | Computing KL before
2021-06-04 13:56:37 | [train_policy] epoch #659 | Optimizing
2021-06-04 13:56:37 | [train_policy] epoch #659 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:37 | [train_policy] epoch #659 | computing loss before
2021-06-04 13:56:37 | [train_policy] epoch #659 | computing gradient
2021-06-04 13:56:37 | [train_policy] epoch #659 | gradient computed
2021-06-04 13:56:37 | [train_policy] epoch #659 | computing descent direction
2021-06-04 13:56:37 | [train_policy] epoch #659 | descent direction computed
2021-06-04 13:56:37 | [train_policy] epoch #659 | backtrack iters: 1
2021-06-04 13:56:37 | [train_policy] epoch #659 | optimization finished
2021-06-04 13:56:37 | [train_policy] epoch #659 | Computing KL after
2021-06-04 13:56:37 | [train_policy] epoch #659 | Computing loss after
2021-06-04 13:56:37 | [train_policy] epoch #659 | Fitting baseline...
2021-06-04 13:56:37 | [train_policy] epoch #659 | Saving snapshot...
2021-06-04 13:56:37 | [train_policy] epoch #659 | Saved
2021-06-04 13:56:37 | [train_policy] epoch #659 | Time 529.50 s
2021-06-04 13:56:37 | [train_policy] epoch #659 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285769
Evaluation/AverageDiscountedReturn          -42.3964
Evaluation/AverageReturn                    -42.3964
Evaluation/CompletionRate                     0
Evaluation/Iteration                        659
Evaluation/MaxReturn                        -31.6241
Evaluation/MinReturn                        -78.1857
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.78995
Extras/EpisodeRewardMean                    -42.1053
LinearFeatureBaseline/ExplainedVariance       0.833359
PolicyExecTime                                0.2216
ProcessExecTime                               0.0312533
TotalEnvSteps                            667920
policy/Entropy                               -1.6405
policy/KL                                     0.00653188
policy/KLBefore                               0
policy/LossAfter                             -0.0128409
policy/LossBefore                            -7.30334e-09
policy/Perplexity                             0.193884
policy/dLoss                                  0.0128409
---------------------------------------  ----------------
2021-06-04 13:56:37 | [train_policy] epoch #660 | Obtaining samples for iteration 660...
2021-06-04 13:56:38 | [train_policy] epoch #660 | Logging diagnostics...
2021-06-04 13:56:38 | [train_policy] epoch #660 | Optimizing policy...
2021-06-04 13:56:38 | [train_policy] epoch #660 | Computing loss before
2021-06-04 13:56:38 | [train_policy] epoch #660 | Computing KL before
2021-06-04 13:56:38 | [train_policy] epoch #660 | Optimizing
2021-06-04 13:56:38 | [train_policy] epoch #660 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:38 | [train_policy] epoch #660 | computing loss before
2021-06-04 13:56:38 | [train_policy] epoch #660 | computing gradient
2021-06-04 13:56:38 | [train_policy] epoch #660 | gradient computed
2021-06-04 13:56:38 | [train_policy] epoch #660 | computing descent direction
2021-06-04 13:56:38 | [train_policy] epoch #660 | descent direction computed
2021-06-04 13:56:38 | [train_policy] epoch #660 | backtrack iters: 0
2021-06-04 13:56:38 | [train_policy] epoch #660 | optimization finished
2021-06-04 13:56:38 | [train_policy] epoch #660 | Computing KL after
2021-06-04 13:56:38 | [train_policy] epoch #660 | Computing loss after
2021-06-04 13:56:38 | [train_policy] epoch #660 | Fitting baseline...
2021-06-04 13:56:38 | [train_policy] epoch #660 | Saving snapshot...
2021-06-04 13:56:38 | [train_policy] epoch #660 | Saved
2021-06-04 13:56:38 | [train_policy] epoch #660 | Time 530.29 s
2021-06-04 13:56:38 | [train_policy] epoch #660 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283028
Evaluation/AverageDiscountedReturn          -41.8743
Evaluation/AverageReturn                    -41.8743
Evaluation/CompletionRate                     0
Evaluation/Iteration                        660
Evaluation/MaxReturn                        -27.8702
Evaluation/MinReturn                        -79.4168
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.56983
Extras/EpisodeRewardMean                    -41.9974
LinearFeatureBaseline/ExplainedVariance       0.852531
PolicyExecTime                                0.215247
ProcessExecTime                               0.0310962
TotalEnvSteps                            668932
policy/Entropy                               -1.62682
policy/KL                                     0.00994149
policy/KLBefore                               0
policy/LossAfter                             -0.0162622
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             0.196554
policy/dLoss                                  0.0162622
---------------------------------------  ----------------
2021-06-04 13:56:38 | [train_policy] epoch #661 | Obtaining samples for iteration 661...
2021-06-04 13:56:39 | [train_policy] epoch #661 | Logging diagnostics...
2021-06-04 13:56:39 | [train_policy] epoch #661 | Optimizing policy...
2021-06-04 13:56:39 | [train_policy] epoch #661 | Computing loss before
2021-06-04 13:56:39 | [train_policy] epoch #661 | Computing KL before
2021-06-04 13:56:39 | [train_policy] epoch #661 | Optimizing
2021-06-04 13:56:39 | [train_policy] epoch #661 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:39 | [train_policy] epoch #661 | computing loss before
2021-06-04 13:56:39 | [train_policy] epoch #661 | computing gradient
2021-06-04 13:56:39 | [train_policy] epoch #661 | gradient computed
2021-06-04 13:56:39 | [train_policy] epoch #661 | computing descent direction
2021-06-04 13:56:39 | [train_policy] epoch #661 | descent direction computed
2021-06-04 13:56:39 | [train_policy] epoch #661 | backtrack iters: 0
2021-06-04 13:56:39 | [train_policy] epoch #661 | optimization finished
2021-06-04 13:56:39 | [train_policy] epoch #661 | Computing KL after
2021-06-04 13:56:39 | [train_policy] epoch #661 | Computing loss after
2021-06-04 13:56:39 | [train_policy] epoch #661 | Fitting baseline...
2021-06-04 13:56:39 | [train_policy] epoch #661 | Saving snapshot...
2021-06-04 13:56:39 | [train_policy] epoch #661 | Saved
2021-06-04 13:56:39 | [train_policy] epoch #661 | Time 531.11 s
2021-06-04 13:56:39 | [train_policy] epoch #661 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286739
Evaluation/AverageDiscountedReturn          -39.7245
Evaluation/AverageReturn                    -39.7245
Evaluation/CompletionRate                     0
Evaluation/Iteration                        661
Evaluation/MaxReturn                        -28.5922
Evaluation/MinReturn                        -64.0471
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.39897
Extras/EpisodeRewardMean                    -39.365
LinearFeatureBaseline/ExplainedVariance       0.889595
PolicyExecTime                                0.237359
ProcessExecTime                               0.0313966
TotalEnvSteps                            669944
policy/Entropy                               -1.62872
policy/KL                                     0.00996337
policy/KLBefore                               0
policy/LossAfter                             -0.056107
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             0.196181
policy/dLoss                                  0.056107
---------------------------------------  ----------------
2021-06-04 13:56:39 | [train_policy] epoch #662 | Obtaining samples for iteration 662...
2021-06-04 13:56:40 | [train_policy] epoch #662 | Logging diagnostics...
2021-06-04 13:56:40 | [train_policy] epoch #662 | Optimizing policy...
2021-06-04 13:56:40 | [train_policy] epoch #662 | Computing loss before
2021-06-04 13:56:40 | [train_policy] epoch #662 | Computing KL before
2021-06-04 13:56:40 | [train_policy] epoch #662 | Optimizing
2021-06-04 13:56:40 | [train_policy] epoch #662 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:40 | [train_policy] epoch #662 | computing loss before
2021-06-04 13:56:40 | [train_policy] epoch #662 | computing gradient
2021-06-04 13:56:40 | [train_policy] epoch #662 | gradient computed
2021-06-04 13:56:40 | [train_policy] epoch #662 | computing descent direction
2021-06-04 13:56:40 | [train_policy] epoch #662 | descent direction computed
2021-06-04 13:56:40 | [train_policy] epoch #662 | backtrack iters: 1
2021-06-04 13:56:40 | [train_policy] epoch #662 | optimization finished
2021-06-04 13:56:40 | [train_policy] epoch #662 | Computing KL after
2021-06-04 13:56:40 | [train_policy] epoch #662 | Computing loss after
2021-06-04 13:56:40 | [train_policy] epoch #662 | Fitting baseline...
2021-06-04 13:56:40 | [train_policy] epoch #662 | Saving snapshot...
2021-06-04 13:56:40 | [train_policy] epoch #662 | Saved
2021-06-04 13:56:40 | [train_policy] epoch #662 | Time 531.92 s
2021-06-04 13:56:40 | [train_policy] epoch #662 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287032
Evaluation/AverageDiscountedReturn          -41.126
Evaluation/AverageReturn                    -41.126
Evaluation/CompletionRate                     0
Evaluation/Iteration                        662
Evaluation/MaxReturn                        -30.7078
Evaluation/MinReturn                        -63.9702
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.35735
Extras/EpisodeRewardMean                    -41.1314
LinearFeatureBaseline/ExplainedVariance       0.91766
PolicyExecTime                                0.232892
ProcessExecTime                               0.0314655
TotalEnvSteps                            670956
policy/Entropy                               -1.63962
policy/KL                                     0.00641376
policy/KLBefore                               0
policy/LossAfter                             -0.0130084
policy/LossBefore                            -2.35591e-10
policy/Perplexity                             0.194054
policy/dLoss                                  0.0130084
---------------------------------------  ----------------
2021-06-04 13:56:40 | [train_policy] epoch #663 | Obtaining samples for iteration 663...
2021-06-04 13:56:40 | [train_policy] epoch #663 | Logging diagnostics...
2021-06-04 13:56:40 | [train_policy] epoch #663 | Optimizing policy...
2021-06-04 13:56:40 | [train_policy] epoch #663 | Computing loss before
2021-06-04 13:56:40 | [train_policy] epoch #663 | Computing KL before
2021-06-04 13:56:40 | [train_policy] epoch #663 | Optimizing
2021-06-04 13:56:40 | [train_policy] epoch #663 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:40 | [train_policy] epoch #663 | computing loss before
2021-06-04 13:56:40 | [train_policy] epoch #663 | computing gradient
2021-06-04 13:56:40 | [train_policy] epoch #663 | gradient computed
2021-06-04 13:56:40 | [train_policy] epoch #663 | computing descent direction
2021-06-04 13:56:40 | [train_policy] epoch #663 | descent direction computed
2021-06-04 13:56:40 | [train_policy] epoch #663 | backtrack iters: 0
2021-06-04 13:56:40 | [train_policy] epoch #663 | optimization finished
2021-06-04 13:56:40 | [train_policy] epoch #663 | Computing KL after
2021-06-04 13:56:40 | [train_policy] epoch #663 | Computing loss after
2021-06-04 13:56:40 | [train_policy] epoch #663 | Fitting baseline...
2021-06-04 13:56:40 | [train_policy] epoch #663 | Saving snapshot...
2021-06-04 13:56:41 | [train_policy] epoch #663 | Saved
2021-06-04 13:56:41 | [train_policy] epoch #663 | Time 532.72 s
2021-06-04 13:56:41 | [train_policy] epoch #663 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287504
Evaluation/AverageDiscountedReturn          -41.2432
Evaluation/AverageReturn                    -41.2432
Evaluation/CompletionRate                     0
Evaluation/Iteration                        663
Evaluation/MaxReturn                        -29.4673
Evaluation/MinReturn                        -77.6176
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.06735
Extras/EpisodeRewardMean                    -41.0176
LinearFeatureBaseline/ExplainedVariance       0.892072
PolicyExecTime                                0.225632
ProcessExecTime                               0.0313635
TotalEnvSteps                            671968
policy/Entropy                               -1.61936
policy/KL                                     0.00980621
policy/KLBefore                               0
policy/LossAfter                             -0.0222189
policy/LossBefore                             2.94489e-09
policy/Perplexity                             0.198026
policy/dLoss                                  0.0222189
---------------------------------------  ----------------
2021-06-04 13:56:41 | [train_policy] epoch #664 | Obtaining samples for iteration 664...
2021-06-04 13:56:41 | [train_policy] epoch #664 | Logging diagnostics...
2021-06-04 13:56:41 | [train_policy] epoch #664 | Optimizing policy...
2021-06-04 13:56:41 | [train_policy] epoch #664 | Computing loss before
2021-06-04 13:56:41 | [train_policy] epoch #664 | Computing KL before
2021-06-04 13:56:41 | [train_policy] epoch #664 | Optimizing
2021-06-04 13:56:41 | [train_policy] epoch #664 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:41 | [train_policy] epoch #664 | computing loss before
2021-06-04 13:56:41 | [train_policy] epoch #664 | computing gradient
2021-06-04 13:56:41 | [train_policy] epoch #664 | gradient computed
2021-06-04 13:56:41 | [train_policy] epoch #664 | computing descent direction
2021-06-04 13:56:41 | [train_policy] epoch #664 | descent direction computed
2021-06-04 13:56:41 | [train_policy] epoch #664 | backtrack iters: 0
2021-06-04 13:56:41 | [train_policy] epoch #664 | optimization finished
2021-06-04 13:56:41 | [train_policy] epoch #664 | Computing KL after
2021-06-04 13:56:41 | [train_policy] epoch #664 | Computing loss after
2021-06-04 13:56:41 | [train_policy] epoch #664 | Fitting baseline...
2021-06-04 13:56:41 | [train_policy] epoch #664 | Saving snapshot...
2021-06-04 13:56:41 | [train_policy] epoch #664 | Saved
2021-06-04 13:56:41 | [train_policy] epoch #664 | Time 533.51 s
2021-06-04 13:56:41 | [train_policy] epoch #664 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.283727
Evaluation/AverageDiscountedReturn          -41.9983
Evaluation/AverageReturn                    -41.9983
Evaluation/CompletionRate                     0
Evaluation/Iteration                        664
Evaluation/MaxReturn                        -29.4576
Evaluation/MinReturn                        -79.7775
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.6338
Extras/EpisodeRewardMean                    -41.7524
LinearFeatureBaseline/ExplainedVariance       0.873647
PolicyExecTime                                0.209846
ProcessExecTime                               0.031146
TotalEnvSteps                            672980
policy/Entropy                               -1.61539
policy/KL                                     0.0097721
policy/KLBefore                               0
policy/LossAfter                             -0.0168818
policy/LossBefore                            -8.2457e-09
policy/Perplexity                             0.198813
policy/dLoss                                  0.0168818
---------------------------------------  ---------------
2021-06-04 13:56:41 | [train_policy] epoch #665 | Obtaining samples for iteration 665...
2021-06-04 13:56:42 | [train_policy] epoch #665 | Logging diagnostics...
2021-06-04 13:56:42 | [train_policy] epoch #665 | Optimizing policy...
2021-06-04 13:56:42 | [train_policy] epoch #665 | Computing loss before
2021-06-04 13:56:42 | [train_policy] epoch #665 | Computing KL before
2021-06-04 13:56:42 | [train_policy] epoch #665 | Optimizing
2021-06-04 13:56:42 | [train_policy] epoch #665 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:42 | [train_policy] epoch #665 | computing loss before
2021-06-04 13:56:42 | [train_policy] epoch #665 | computing gradient
2021-06-04 13:56:42 | [train_policy] epoch #665 | gradient computed
2021-06-04 13:56:42 | [train_policy] epoch #665 | computing descent direction
2021-06-04 13:56:42 | [train_policy] epoch #665 | descent direction computed
2021-06-04 13:56:42 | [train_policy] epoch #665 | backtrack iters: 0
2021-06-04 13:56:42 | [train_policy] epoch #665 | optimization finished
2021-06-04 13:56:42 | [train_policy] epoch #665 | Computing KL after
2021-06-04 13:56:42 | [train_policy] epoch #665 | Computing loss after
2021-06-04 13:56:42 | [train_policy] epoch #665 | Fitting baseline...
2021-06-04 13:56:42 | [train_policy] epoch #665 | Saving snapshot...
2021-06-04 13:56:42 | [train_policy] epoch #665 | Saved
2021-06-04 13:56:42 | [train_policy] epoch #665 | Time 534.32 s
2021-06-04 13:56:42 | [train_policy] epoch #665 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284291
Evaluation/AverageDiscountedReturn          -42.4375
Evaluation/AverageReturn                    -42.4375
Evaluation/CompletionRate                     0
Evaluation/Iteration                        665
Evaluation/MaxReturn                        -29.864
Evaluation/MinReturn                        -57.8329
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.44707
Extras/EpisodeRewardMean                    -42.2658
LinearFeatureBaseline/ExplainedVariance       0.855539
PolicyExecTime                                0.229037
ProcessExecTime                               0.0311704
TotalEnvSteps                            673992
policy/Entropy                               -1.6219
policy/KL                                     0.00996045
policy/KLBefore                               0
policy/LossAfter                             -0.0166064
policy/LossBefore                            -1.48423e-08
policy/Perplexity                             0.197523
policy/dLoss                                  0.0166064
---------------------------------------  ----------------
2021-06-04 13:56:42 | [train_policy] epoch #666 | Obtaining samples for iteration 666...
2021-06-04 13:56:43 | [train_policy] epoch #666 | Logging diagnostics...
2021-06-04 13:56:43 | [train_policy] epoch #666 | Optimizing policy...
2021-06-04 13:56:43 | [train_policy] epoch #666 | Computing loss before
2021-06-04 13:56:43 | [train_policy] epoch #666 | Computing KL before
2021-06-04 13:56:43 | [train_policy] epoch #666 | Optimizing
2021-06-04 13:56:43 | [train_policy] epoch #666 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:43 | [train_policy] epoch #666 | computing loss before
2021-06-04 13:56:43 | [train_policy] epoch #666 | computing gradient
2021-06-04 13:56:43 | [train_policy] epoch #666 | gradient computed
2021-06-04 13:56:43 | [train_policy] epoch #666 | computing descent direction
2021-06-04 13:56:43 | [train_policy] epoch #666 | descent direction computed
2021-06-04 13:56:43 | [train_policy] epoch #666 | backtrack iters: 0
2021-06-04 13:56:43 | [train_policy] epoch #666 | optimization finished
2021-06-04 13:56:43 | [train_policy] epoch #666 | Computing KL after
2021-06-04 13:56:43 | [train_policy] epoch #666 | Computing loss after
2021-06-04 13:56:43 | [train_policy] epoch #666 | Fitting baseline...
2021-06-04 13:56:43 | [train_policy] epoch #666 | Saving snapshot...
2021-06-04 13:56:43 | [train_policy] epoch #666 | Saved
2021-06-04 13:56:43 | [train_policy] epoch #666 | Time 535.13 s
2021-06-04 13:56:43 | [train_policy] epoch #666 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285666
Evaluation/AverageDiscountedReturn          -43.1515
Evaluation/AverageReturn                    -43.1515
Evaluation/CompletionRate                     0
Evaluation/Iteration                        666
Evaluation/MaxReturn                        -29.1807
Evaluation/MinReturn                        -77.3832
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.73645
Extras/EpisodeRewardMean                    -43.0796
LinearFeatureBaseline/ExplainedVariance       0.87655
PolicyExecTime                                0.234046
ProcessExecTime                               0.0312819
TotalEnvSteps                            675004
policy/Entropy                               -1.58256
policy/KL                                     0.00957484
policy/KLBefore                               0
policy/LossAfter                             -0.0127214
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.205448
policy/dLoss                                  0.0127214
---------------------------------------  ----------------
2021-06-04 13:56:43 | [train_policy] epoch #667 | Obtaining samples for iteration 667...
2021-06-04 13:56:44 | [train_policy] epoch #667 | Logging diagnostics...
2021-06-04 13:56:44 | [train_policy] epoch #667 | Optimizing policy...
2021-06-04 13:56:44 | [train_policy] epoch #667 | Computing loss before
2021-06-04 13:56:44 | [train_policy] epoch #667 | Computing KL before
2021-06-04 13:56:44 | [train_policy] epoch #667 | Optimizing
2021-06-04 13:56:44 | [train_policy] epoch #667 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:44 | [train_policy] epoch #667 | computing loss before
2021-06-04 13:56:44 | [train_policy] epoch #667 | computing gradient
2021-06-04 13:56:44 | [train_policy] epoch #667 | gradient computed
2021-06-04 13:56:44 | [train_policy] epoch #667 | computing descent direction
2021-06-04 13:56:44 | [train_policy] epoch #667 | descent direction computed
2021-06-04 13:56:44 | [train_policy] epoch #667 | backtrack iters: 1
2021-06-04 13:56:44 | [train_policy] epoch #667 | optimization finished
2021-06-04 13:56:44 | [train_policy] epoch #667 | Computing KL after
2021-06-04 13:56:44 | [train_policy] epoch #667 | Computing loss after
2021-06-04 13:56:44 | [train_policy] epoch #667 | Fitting baseline...
2021-06-04 13:56:44 | [train_policy] epoch #667 | Saving snapshot...
2021-06-04 13:56:44 | [train_policy] epoch #667 | Saved
2021-06-04 13:56:44 | [train_policy] epoch #667 | Time 535.94 s
2021-06-04 13:56:44 | [train_policy] epoch #667 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284092
Evaluation/AverageDiscountedReturn          -41.6938
Evaluation/AverageReturn                    -41.6938
Evaluation/CompletionRate                     0
Evaluation/Iteration                        667
Evaluation/MaxReturn                        -28.7441
Evaluation/MinReturn                        -78.4139
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.59119
Extras/EpisodeRewardMean                    -41.6967
LinearFeatureBaseline/ExplainedVariance       0.859184
PolicyExecTime                                0.227569
ProcessExecTime                               0.0312028
TotalEnvSteps                            676016
policy/Entropy                               -1.61364
policy/KL                                     0.00660517
policy/KLBefore                               0
policy/LossAfter                             -0.0176214
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.199161
policy/dLoss                                  0.0176214
---------------------------------------  ----------------
2021-06-04 13:56:44 | [train_policy] epoch #668 | Obtaining samples for iteration 668...
2021-06-04 13:56:44 | [train_policy] epoch #668 | Logging diagnostics...
2021-06-04 13:56:44 | [train_policy] epoch #668 | Optimizing policy...
2021-06-04 13:56:44 | [train_policy] epoch #668 | Computing loss before
2021-06-04 13:56:44 | [train_policy] epoch #668 | Computing KL before
2021-06-04 13:56:44 | [train_policy] epoch #668 | Optimizing
2021-06-04 13:56:44 | [train_policy] epoch #668 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:44 | [train_policy] epoch #668 | computing loss before
2021-06-04 13:56:44 | [train_policy] epoch #668 | computing gradient
2021-06-04 13:56:44 | [train_policy] epoch #668 | gradient computed
2021-06-04 13:56:44 | [train_policy] epoch #668 | computing descent direction
2021-06-04 13:56:44 | [train_policy] epoch #668 | descent direction computed
2021-06-04 13:56:44 | [train_policy] epoch #668 | backtrack iters: 1
2021-06-04 13:56:44 | [train_policy] epoch #668 | optimization finished
2021-06-04 13:56:44 | [train_policy] epoch #668 | Computing KL after
2021-06-04 13:56:44 | [train_policy] epoch #668 | Computing loss after
2021-06-04 13:56:44 | [train_policy] epoch #668 | Fitting baseline...
2021-06-04 13:56:45 | [train_policy] epoch #668 | Saving snapshot...
2021-06-04 13:56:45 | [train_policy] epoch #668 | Saved
2021-06-04 13:56:45 | [train_policy] epoch #668 | Time 536.76 s
2021-06-04 13:56:45 | [train_policy] epoch #668 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285377
Evaluation/AverageDiscountedReturn          -41.6825
Evaluation/AverageReturn                    -41.6825
Evaluation/CompletionRate                     0
Evaluation/Iteration                        668
Evaluation/MaxReturn                        -31.3149
Evaluation/MinReturn                        -81.9428
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.10749
Extras/EpisodeRewardMean                    -41.5946
LinearFeatureBaseline/ExplainedVariance       0.858119
PolicyExecTime                                0.234868
ProcessExecTime                               0.0312929
TotalEnvSteps                            677028
policy/Entropy                               -1.61486
policy/KL                                     0.00640272
policy/KLBefore                               0
policy/LossAfter                             -0.0223152
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.198919
policy/dLoss                                  0.0223152
---------------------------------------  ----------------
2021-06-04 13:56:45 | [train_policy] epoch #669 | Obtaining samples for iteration 669...
2021-06-04 13:56:45 | [train_policy] epoch #669 | Logging diagnostics...
2021-06-04 13:56:45 | [train_policy] epoch #669 | Optimizing policy...
2021-06-04 13:56:45 | [train_policy] epoch #669 | Computing loss before
2021-06-04 13:56:45 | [train_policy] epoch #669 | Computing KL before
2021-06-04 13:56:45 | [train_policy] epoch #669 | Optimizing
2021-06-04 13:56:45 | [train_policy] epoch #669 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:45 | [train_policy] epoch #669 | computing loss before
2021-06-04 13:56:45 | [train_policy] epoch #669 | computing gradient
2021-06-04 13:56:45 | [train_policy] epoch #669 | gradient computed
2021-06-04 13:56:45 | [train_policy] epoch #669 | computing descent direction
2021-06-04 13:56:45 | [train_policy] epoch #669 | descent direction computed
2021-06-04 13:56:45 | [train_policy] epoch #669 | backtrack iters: 1
2021-06-04 13:56:45 | [train_policy] epoch #669 | optimization finished
2021-06-04 13:56:45 | [train_policy] epoch #669 | Computing KL after
2021-06-04 13:56:45 | [train_policy] epoch #669 | Computing loss after
2021-06-04 13:56:45 | [train_policy] epoch #669 | Fitting baseline...
2021-06-04 13:56:45 | [train_policy] epoch #669 | Saving snapshot...
2021-06-04 13:56:45 | [train_policy] epoch #669 | Saved
2021-06-04 13:56:45 | [train_policy] epoch #669 | Time 537.58 s
2021-06-04 13:56:45 | [train_policy] epoch #669 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.282803
Evaluation/AverageDiscountedReturn          -40.9154
Evaluation/AverageReturn                    -40.9154
Evaluation/CompletionRate                     0
Evaluation/Iteration                        669
Evaluation/MaxReturn                        -28.9472
Evaluation/MinReturn                        -80.1226
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1811
Extras/EpisodeRewardMean                    -40.8537
LinearFeatureBaseline/ExplainedVariance       0.847881
PolicyExecTime                                0.232572
ProcessExecTime                               0.0310748
TotalEnvSteps                            678040
policy/Entropy                               -1.65384
policy/KL                                     0.0066715
policy/KLBefore                               0
policy/LossAfter                             -0.0131604
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.191313
policy/dLoss                                  0.0131604
---------------------------------------  ----------------
2021-06-04 13:56:45 | [train_policy] epoch #670 | Obtaining samples for iteration 670...
2021-06-04 13:56:46 | [train_policy] epoch #670 | Logging diagnostics...
2021-06-04 13:56:46 | [train_policy] epoch #670 | Optimizing policy...
2021-06-04 13:56:46 | [train_policy] epoch #670 | Computing loss before
2021-06-04 13:56:46 | [train_policy] epoch #670 | Computing KL before
2021-06-04 13:56:46 | [train_policy] epoch #670 | Optimizing
2021-06-04 13:56:46 | [train_policy] epoch #670 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:46 | [train_policy] epoch #670 | computing loss before
2021-06-04 13:56:46 | [train_policy] epoch #670 | computing gradient
2021-06-04 13:56:46 | [train_policy] epoch #670 | gradient computed
2021-06-04 13:56:46 | [train_policy] epoch #670 | computing descent direction
2021-06-04 13:56:46 | [train_policy] epoch #670 | descent direction computed
2021-06-04 13:56:46 | [train_policy] epoch #670 | backtrack iters: 1
2021-06-04 13:56:46 | [train_policy] epoch #670 | optimization finished
2021-06-04 13:56:46 | [train_policy] epoch #670 | Computing KL after
2021-06-04 13:56:46 | [train_policy] epoch #670 | Computing loss after
2021-06-04 13:56:46 | [train_policy] epoch #670 | Fitting baseline...
2021-06-04 13:56:46 | [train_policy] epoch #670 | Saving snapshot...
2021-06-04 13:56:46 | [train_policy] epoch #670 | Saved
2021-06-04 13:56:46 | [train_policy] epoch #670 | Time 538.37 s
2021-06-04 13:56:46 | [train_policy] epoch #670 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.283507
Evaluation/AverageDiscountedReturn          -42.0627
Evaluation/AverageReturn                    -42.0627
Evaluation/CompletionRate                     0
Evaluation/Iteration                        670
Evaluation/MaxReturn                        -28.7777
Evaluation/MinReturn                        -77.0052
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.65277
Extras/EpisodeRewardMean                    -41.8424
LinearFeatureBaseline/ExplainedVariance       0.886014
PolicyExecTime                                0.217919
ProcessExecTime                               0.0310838
TotalEnvSteps                            679052
policy/Entropy                               -1.6558
policy/KL                                     0.00656213
policy/KLBefore                               0
policy/LossAfter                             -0.0152186
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.19094
policy/dLoss                                  0.0152186
---------------------------------------  ---------------
2021-06-04 13:56:46 | [train_policy] epoch #671 | Obtaining samples for iteration 671...
2021-06-04 13:56:47 | [train_policy] epoch #671 | Logging diagnostics...
2021-06-04 13:56:47 | [train_policy] epoch #671 | Optimizing policy...
2021-06-04 13:56:47 | [train_policy] epoch #671 | Computing loss before
2021-06-04 13:56:47 | [train_policy] epoch #671 | Computing KL before
2021-06-04 13:56:47 | [train_policy] epoch #671 | Optimizing
2021-06-04 13:56:47 | [train_policy] epoch #671 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:47 | [train_policy] epoch #671 | computing loss before
2021-06-04 13:56:47 | [train_policy] epoch #671 | computing gradient
2021-06-04 13:56:47 | [train_policy] epoch #671 | gradient computed
2021-06-04 13:56:47 | [train_policy] epoch #671 | computing descent direction
2021-06-04 13:56:47 | [train_policy] epoch #671 | descent direction computed
2021-06-04 13:56:47 | [train_policy] epoch #671 | backtrack iters: 0
2021-06-04 13:56:47 | [train_policy] epoch #671 | optimization finished
2021-06-04 13:56:47 | [train_policy] epoch #671 | Computing KL after
2021-06-04 13:56:47 | [train_policy] epoch #671 | Computing loss after
2021-06-04 13:56:47 | [train_policy] epoch #671 | Fitting baseline...
2021-06-04 13:56:47 | [train_policy] epoch #671 | Saving snapshot...
2021-06-04 13:56:47 | [train_policy] epoch #671 | Saved
2021-06-04 13:56:47 | [train_policy] epoch #671 | Time 539.17 s
2021-06-04 13:56:47 | [train_policy] epoch #671 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286634
Evaluation/AverageDiscountedReturn          -43.1182
Evaluation/AverageReturn                    -43.1182
Evaluation/CompletionRate                     0
Evaluation/Iteration                        671
Evaluation/MaxReturn                        -30.7661
Evaluation/MinReturn                        -78.8316
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.27414
Extras/EpisodeRewardMean                    -43.1827
LinearFeatureBaseline/ExplainedVariance       0.868598
PolicyExecTime                                0.226818
ProcessExecTime                               0.0314703
TotalEnvSteps                            680064
policy/Entropy                               -1.65058
policy/KL                                     0.00987501
policy/KLBefore                               0
policy/LossAfter                             -0.0202557
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.191939
policy/dLoss                                  0.0202557
---------------------------------------  ----------------
2021-06-04 13:56:47 | [train_policy] epoch #672 | Obtaining samples for iteration 672...
2021-06-04 13:56:48 | [train_policy] epoch #672 | Logging diagnostics...
2021-06-04 13:56:48 | [train_policy] epoch #672 | Optimizing policy...
2021-06-04 13:56:48 | [train_policy] epoch #672 | Computing loss before
2021-06-04 13:56:48 | [train_policy] epoch #672 | Computing KL before
2021-06-04 13:56:48 | [train_policy] epoch #672 | Optimizing
2021-06-04 13:56:48 | [train_policy] epoch #672 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:48 | [train_policy] epoch #672 | computing loss before
2021-06-04 13:56:48 | [train_policy] epoch #672 | computing gradient
2021-06-04 13:56:48 | [train_policy] epoch #672 | gradient computed
2021-06-04 13:56:48 | [train_policy] epoch #672 | computing descent direction
2021-06-04 13:56:48 | [train_policy] epoch #672 | descent direction computed
2021-06-04 13:56:48 | [train_policy] epoch #672 | backtrack iters: 1
2021-06-04 13:56:48 | [train_policy] epoch #672 | optimization finished
2021-06-04 13:56:48 | [train_policy] epoch #672 | Computing KL after
2021-06-04 13:56:48 | [train_policy] epoch #672 | Computing loss after
2021-06-04 13:56:48 | [train_policy] epoch #672 | Fitting baseline...
2021-06-04 13:56:48 | [train_policy] epoch #672 | Saving snapshot...
2021-06-04 13:56:48 | [train_policy] epoch #672 | Saved
2021-06-04 13:56:48 | [train_policy] epoch #672 | Time 539.98 s
2021-06-04 13:56:48 | [train_policy] epoch #672 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284365
Evaluation/AverageDiscountedReturn          -42.1038
Evaluation/AverageReturn                    -42.1038
Evaluation/CompletionRate                     0
Evaluation/Iteration                        672
Evaluation/MaxReturn                        -29.0684
Evaluation/MinReturn                        -77.8441
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.0571
Extras/EpisodeRewardMean                    -41.8316
LinearFeatureBaseline/ExplainedVariance       0.898792
PolicyExecTime                                0.222288
ProcessExecTime                               0.0311451
TotalEnvSteps                            681076
policy/Entropy                               -1.65269
policy/KL                                     0.00647349
policy/KLBefore                               0
policy/LossAfter                             -0.0191019
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.191534
policy/dLoss                                  0.0191018
---------------------------------------  ----------------
2021-06-04 13:56:48 | [train_policy] epoch #673 | Obtaining samples for iteration 673...
2021-06-04 13:56:48 | [train_policy] epoch #673 | Logging diagnostics...
2021-06-04 13:56:48 | [train_policy] epoch #673 | Optimizing policy...
2021-06-04 13:56:48 | [train_policy] epoch #673 | Computing loss before
2021-06-04 13:56:48 | [train_policy] epoch #673 | Computing KL before
2021-06-04 13:56:48 | [train_policy] epoch #673 | Optimizing
2021-06-04 13:56:48 | [train_policy] epoch #673 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:48 | [train_policy] epoch #673 | computing loss before
2021-06-04 13:56:48 | [train_policy] epoch #673 | computing gradient
2021-06-04 13:56:48 | [train_policy] epoch #673 | gradient computed
2021-06-04 13:56:48 | [train_policy] epoch #673 | computing descent direction
2021-06-04 13:56:49 | [train_policy] epoch #673 | descent direction computed
2021-06-04 13:56:49 | [train_policy] epoch #673 | backtrack iters: 0
2021-06-04 13:56:49 | [train_policy] epoch #673 | optimization finished
2021-06-04 13:56:49 | [train_policy] epoch #673 | Computing KL after
2021-06-04 13:56:49 | [train_policy] epoch #673 | Computing loss after
2021-06-04 13:56:49 | [train_policy] epoch #673 | Fitting baseline...
2021-06-04 13:56:49 | [train_policy] epoch #673 | Saving snapshot...
2021-06-04 13:56:49 | [train_policy] epoch #673 | Saved
2021-06-04 13:56:49 | [train_policy] epoch #673 | Time 540.79 s
2021-06-04 13:56:49 | [train_policy] epoch #673 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287641
Evaluation/AverageDiscountedReturn          -41.9416
Evaluation/AverageReturn                    -41.9416
Evaluation/CompletionRate                     0
Evaluation/Iteration                        673
Evaluation/MaxReturn                        -29.6994
Evaluation/MinReturn                        -63.3727
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.65909
Extras/EpisodeRewardMean                    -41.9904
LinearFeatureBaseline/ExplainedVariance       0.88729
PolicyExecTime                                0.241596
ProcessExecTime                               0.0315142
TotalEnvSteps                            682088
policy/Entropy                               -1.59141
policy/KL                                     0.00934856
policy/KLBefore                               0
policy/LossAfter                             -0.0226685
policy/LossBefore                            -1.07194e-08
policy/Perplexity                             0.203639
policy/dLoss                                  0.0226685
---------------------------------------  ----------------
2021-06-04 13:56:49 | [train_policy] epoch #674 | Obtaining samples for iteration 674...
2021-06-04 13:56:49 | [train_policy] epoch #674 | Logging diagnostics...
2021-06-04 13:56:49 | [train_policy] epoch #674 | Optimizing policy...
2021-06-04 13:56:49 | [train_policy] epoch #674 | Computing loss before
2021-06-04 13:56:49 | [train_policy] epoch #674 | Computing KL before
2021-06-04 13:56:49 | [train_policy] epoch #674 | Optimizing
2021-06-04 13:56:49 | [train_policy] epoch #674 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:49 | [train_policy] epoch #674 | computing loss before
2021-06-04 13:56:49 | [train_policy] epoch #674 | computing gradient
2021-06-04 13:56:49 | [train_policy] epoch #674 | gradient computed
2021-06-04 13:56:49 | [train_policy] epoch #674 | computing descent direction
2021-06-04 13:56:49 | [train_policy] epoch #674 | descent direction computed
2021-06-04 13:56:49 | [train_policy] epoch #674 | backtrack iters: 1
2021-06-04 13:56:49 | [train_policy] epoch #674 | optimization finished
2021-06-04 13:56:49 | [train_policy] epoch #674 | Computing KL after
2021-06-04 13:56:49 | [train_policy] epoch #674 | Computing loss after
2021-06-04 13:56:49 | [train_policy] epoch #674 | Fitting baseline...
2021-06-04 13:56:49 | [train_policy] epoch #674 | Saving snapshot...
2021-06-04 13:56:49 | [train_policy] epoch #674 | Saved
2021-06-04 13:56:49 | [train_policy] epoch #674 | Time 541.60 s
2021-06-04 13:56:49 | [train_policy] epoch #674 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284744
Evaluation/AverageDiscountedReturn          -41.6202
Evaluation/AverageReturn                    -41.6202
Evaluation/CompletionRate                     0
Evaluation/Iteration                        674
Evaluation/MaxReturn                        -28.6359
Evaluation/MinReturn                        -63.2459
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.93619
Extras/EpisodeRewardMean                    -41.7099
LinearFeatureBaseline/ExplainedVariance       0.890615
PolicyExecTime                                0.226573
ProcessExecTime                               0.0311711
TotalEnvSteps                            683100
policy/Entropy                               -1.5868
policy/KL                                     0.00659387
policy/KLBefore                               0
policy/LossAfter                             -0.0165841
policy/LossBefore                            -2.63862e-08
policy/Perplexity                             0.204579
policy/dLoss                                  0.0165841
---------------------------------------  ----------------
2021-06-04 13:56:49 | [train_policy] epoch #675 | Obtaining samples for iteration 675...
2021-06-04 13:56:50 | [train_policy] epoch #675 | Logging diagnostics...
2021-06-04 13:56:50 | [train_policy] epoch #675 | Optimizing policy...
2021-06-04 13:56:50 | [train_policy] epoch #675 | Computing loss before
2021-06-04 13:56:50 | [train_policy] epoch #675 | Computing KL before
2021-06-04 13:56:50 | [train_policy] epoch #675 | Optimizing
2021-06-04 13:56:50 | [train_policy] epoch #675 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:50 | [train_policy] epoch #675 | computing loss before
2021-06-04 13:56:50 | [train_policy] epoch #675 | computing gradient
2021-06-04 13:56:50 | [train_policy] epoch #675 | gradient computed
2021-06-04 13:56:50 | [train_policy] epoch #675 | computing descent direction
2021-06-04 13:56:50 | [train_policy] epoch #675 | descent direction computed
2021-06-04 13:56:50 | [train_policy] epoch #675 | backtrack iters: 1
2021-06-04 13:56:50 | [train_policy] epoch #675 | optimization finished
2021-06-04 13:56:50 | [train_policy] epoch #675 | Computing KL after
2021-06-04 13:56:50 | [train_policy] epoch #675 | Computing loss after
2021-06-04 13:56:50 | [train_policy] epoch #675 | Fitting baseline...
2021-06-04 13:56:50 | [train_policy] epoch #675 | Saving snapshot...
2021-06-04 13:56:50 | [train_policy] epoch #675 | Saved
2021-06-04 13:56:50 | [train_policy] epoch #675 | Time 542.41 s
2021-06-04 13:56:50 | [train_policy] epoch #675 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285287
Evaluation/AverageDiscountedReturn          -41.2704
Evaluation/AverageReturn                    -41.2704
Evaluation/CompletionRate                     0
Evaluation/Iteration                        675
Evaluation/MaxReturn                        -28.5864
Evaluation/MinReturn                        -79.2164
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.51454
Extras/EpisodeRewardMean                    -41.1241
LinearFeatureBaseline/ExplainedVariance       0.863989
PolicyExecTime                                0.226865
ProcessExecTime                               0.0311275
TotalEnvSteps                            684112
policy/Entropy                               -1.60442
policy/KL                                     0.00646048
policy/KLBefore                               0
policy/LossAfter                             -0.0157074
policy/LossBefore                            -1.17796e-09
policy/Perplexity                             0.201006
policy/dLoss                                  0.0157074
---------------------------------------  ----------------
2021-06-04 13:56:50 | [train_policy] epoch #676 | Obtaining samples for iteration 676...
2021-06-04 13:56:51 | [train_policy] epoch #676 | Logging diagnostics...
2021-06-04 13:56:51 | [train_policy] epoch #676 | Optimizing policy...
2021-06-04 13:56:51 | [train_policy] epoch #676 | Computing loss before
2021-06-04 13:56:51 | [train_policy] epoch #676 | Computing KL before
2021-06-04 13:56:51 | [train_policy] epoch #676 | Optimizing
2021-06-04 13:56:51 | [train_policy] epoch #676 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:51 | [train_policy] epoch #676 | computing loss before
2021-06-04 13:56:51 | [train_policy] epoch #676 | computing gradient
2021-06-04 13:56:51 | [train_policy] epoch #676 | gradient computed
2021-06-04 13:56:51 | [train_policy] epoch #676 | computing descent direction
2021-06-04 13:56:51 | [train_policy] epoch #676 | descent direction computed
2021-06-04 13:56:51 | [train_policy] epoch #676 | backtrack iters: 0
2021-06-04 13:56:51 | [train_policy] epoch #676 | optimization finished
2021-06-04 13:56:51 | [train_policy] epoch #676 | Computing KL after
2021-06-04 13:56:51 | [train_policy] epoch #676 | Computing loss after
2021-06-04 13:56:51 | [train_policy] epoch #676 | Fitting baseline...
2021-06-04 13:56:51 | [train_policy] epoch #676 | Saving snapshot...
2021-06-04 13:56:51 | [train_policy] epoch #676 | Saved
2021-06-04 13:56:51 | [train_policy] epoch #676 | Time 543.22 s
2021-06-04 13:56:51 | [train_policy] epoch #676 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284123
Evaluation/AverageDiscountedReturn          -41.0648
Evaluation/AverageReturn                    -41.0648
Evaluation/CompletionRate                     0
Evaluation/Iteration                        676
Evaluation/MaxReturn                        -28.517
Evaluation/MinReturn                        -60.4863
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.86058
Extras/EpisodeRewardMean                    -40.7995
LinearFeatureBaseline/ExplainedVariance       0.915401
PolicyExecTime                                0.23294
ProcessExecTime                               0.0311468
TotalEnvSteps                            685124
policy/Entropy                               -1.59054
policy/KL                                     0.00954091
policy/KLBefore                               0
policy/LossAfter                             -0.0265228
policy/LossBefore                            -3.53387e-10
policy/Perplexity                             0.203816
policy/dLoss                                  0.0265228
---------------------------------------  ----------------
2021-06-04 13:56:51 | [train_policy] epoch #677 | Obtaining samples for iteration 677...
2021-06-04 13:56:52 | [train_policy] epoch #677 | Logging diagnostics...
2021-06-04 13:56:52 | [train_policy] epoch #677 | Optimizing policy...
2021-06-04 13:56:52 | [train_policy] epoch #677 | Computing loss before
2021-06-04 13:56:52 | [train_policy] epoch #677 | Computing KL before
2021-06-04 13:56:52 | [train_policy] epoch #677 | Optimizing
2021-06-04 13:56:52 | [train_policy] epoch #677 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:52 | [train_policy] epoch #677 | computing loss before
2021-06-04 13:56:52 | [train_policy] epoch #677 | computing gradient
2021-06-04 13:56:52 | [train_policy] epoch #677 | gradient computed
2021-06-04 13:56:52 | [train_policy] epoch #677 | computing descent direction
2021-06-04 13:56:52 | [train_policy] epoch #677 | descent direction computed
2021-06-04 13:56:52 | [train_policy] epoch #677 | backtrack iters: 1
2021-06-04 13:56:52 | [train_policy] epoch #677 | optimization finished
2021-06-04 13:56:52 | [train_policy] epoch #677 | Computing KL after
2021-06-04 13:56:52 | [train_policy] epoch #677 | Computing loss after
2021-06-04 13:56:52 | [train_policy] epoch #677 | Fitting baseline...
2021-06-04 13:56:52 | [train_policy] epoch #677 | Saving snapshot...
2021-06-04 13:56:52 | [train_policy] epoch #677 | Saved
2021-06-04 13:56:52 | [train_policy] epoch #677 | Time 544.00 s
2021-06-04 13:56:52 | [train_policy] epoch #677 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283997
Evaluation/AverageDiscountedReturn          -41.7846
Evaluation/AverageReturn                    -41.7846
Evaluation/CompletionRate                     0
Evaluation/Iteration                        677
Evaluation/MaxReturn                        -28.4305
Evaluation/MinReturn                        -66.5324
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25263
Extras/EpisodeRewardMean                    -41.6726
LinearFeatureBaseline/ExplainedVariance       0.888617
PolicyExecTime                                0.210344
ProcessExecTime                               0.0311604
TotalEnvSteps                            686136
policy/Entropy                               -1.60418
policy/KL                                     0.00658309
policy/KLBefore                               0
policy/LossAfter                             -0.0193584
policy/LossBefore                             1.00126e-08
policy/Perplexity                             0.201055
policy/dLoss                                  0.0193584
---------------------------------------  ----------------
2021-06-04 13:56:52 | [train_policy] epoch #678 | Obtaining samples for iteration 678...
2021-06-04 13:56:52 | [train_policy] epoch #678 | Logging diagnostics...
2021-06-04 13:56:52 | [train_policy] epoch #678 | Optimizing policy...
2021-06-04 13:56:52 | [train_policy] epoch #678 | Computing loss before
2021-06-04 13:56:52 | [train_policy] epoch #678 | Computing KL before
2021-06-04 13:56:52 | [train_policy] epoch #678 | Optimizing
2021-06-04 13:56:52 | [train_policy] epoch #678 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:52 | [train_policy] epoch #678 | computing loss before
2021-06-04 13:56:52 | [train_policy] epoch #678 | computing gradient
2021-06-04 13:56:52 | [train_policy] epoch #678 | gradient computed
2021-06-04 13:56:52 | [train_policy] epoch #678 | computing descent direction
2021-06-04 13:56:53 | [train_policy] epoch #678 | descent direction computed
2021-06-04 13:56:53 | [train_policy] epoch #678 | backtrack iters: 1
2021-06-04 13:56:53 | [train_policy] epoch #678 | optimization finished
2021-06-04 13:56:53 | [train_policy] epoch #678 | Computing KL after
2021-06-04 13:56:53 | [train_policy] epoch #678 | Computing loss after
2021-06-04 13:56:53 | [train_policy] epoch #678 | Fitting baseline...
2021-06-04 13:56:53 | [train_policy] epoch #678 | Saving snapshot...
2021-06-04 13:56:53 | [train_policy] epoch #678 | Saved
2021-06-04 13:56:53 | [train_policy] epoch #678 | Time 544.81 s
2021-06-04 13:56:53 | [train_policy] epoch #678 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.2881
Evaluation/AverageDiscountedReturn          -42.2962
Evaluation/AverageReturn                    -42.2962
Evaluation/CompletionRate                     0
Evaluation/Iteration                        678
Evaluation/MaxReturn                        -28.626
Evaluation/MinReturn                        -77.2927
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.16809
Extras/EpisodeRewardMean                    -42.413
LinearFeatureBaseline/ExplainedVariance       0.86844
PolicyExecTime                                0.226765
ProcessExecTime                               0.0315371
TotalEnvSteps                            687148
policy/Entropy                               -1.61435
policy/KL                                     0.00652922
policy/KLBefore                               0
policy/LossAfter                             -0.0212916
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.19902
policy/dLoss                                  0.0212916
---------------------------------------  ----------------
2021-06-04 13:56:53 | [train_policy] epoch #679 | Obtaining samples for iteration 679...
2021-06-04 13:56:53 | [train_policy] epoch #679 | Logging diagnostics...
2021-06-04 13:56:53 | [train_policy] epoch #679 | Optimizing policy...
2021-06-04 13:56:53 | [train_policy] epoch #679 | Computing loss before
2021-06-04 13:56:53 | [train_policy] epoch #679 | Computing KL before
2021-06-04 13:56:53 | [train_policy] epoch #679 | Optimizing
2021-06-04 13:56:53 | [train_policy] epoch #679 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:53 | [train_policy] epoch #679 | computing loss before
2021-06-04 13:56:53 | [train_policy] epoch #679 | computing gradient
2021-06-04 13:56:53 | [train_policy] epoch #679 | gradient computed
2021-06-04 13:56:53 | [train_policy] epoch #679 | computing descent direction
2021-06-04 13:56:53 | [train_policy] epoch #679 | descent direction computed
2021-06-04 13:56:53 | [train_policy] epoch #679 | backtrack iters: 0
2021-06-04 13:56:53 | [train_policy] epoch #679 | optimization finished
2021-06-04 13:56:53 | [train_policy] epoch #679 | Computing KL after
2021-06-04 13:56:53 | [train_policy] epoch #679 | Computing loss after
2021-06-04 13:56:53 | [train_policy] epoch #679 | Fitting baseline...
2021-06-04 13:56:53 | [train_policy] epoch #679 | Saving snapshot...
2021-06-04 13:56:53 | [train_policy] epoch #679 | Saved
2021-06-04 13:56:53 | [train_policy] epoch #679 | Time 545.62 s
2021-06-04 13:56:53 | [train_policy] epoch #679 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284678
Evaluation/AverageDiscountedReturn          -42.7235
Evaluation/AverageReturn                    -42.7235
Evaluation/CompletionRate                     0
Evaluation/Iteration                        679
Evaluation/MaxReturn                        -28.5129
Evaluation/MinReturn                        -76.2702
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.74227
Extras/EpisodeRewardMean                    -42.6315
LinearFeatureBaseline/ExplainedVariance       0.873608
PolicyExecTime                                0.229065
ProcessExecTime                               0.0311544
TotalEnvSteps                            688160
policy/Entropy                               -1.59974
policy/KL                                     0.00914425
policy/KLBefore                               0
policy/LossAfter                             -0.0181722
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.20195
policy/dLoss                                  0.0181722
---------------------------------------  ----------------
2021-06-04 13:56:53 | [train_policy] epoch #680 | Obtaining samples for iteration 680...
2021-06-04 13:56:54 | [train_policy] epoch #680 | Logging diagnostics...
2021-06-04 13:56:54 | [train_policy] epoch #680 | Optimizing policy...
2021-06-04 13:56:54 | [train_policy] epoch #680 | Computing loss before
2021-06-04 13:56:54 | [train_policy] epoch #680 | Computing KL before
2021-06-04 13:56:54 | [train_policy] epoch #680 | Optimizing
2021-06-04 13:56:54 | [train_policy] epoch #680 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:54 | [train_policy] epoch #680 | computing loss before
2021-06-04 13:56:54 | [train_policy] epoch #680 | computing gradient
2021-06-04 13:56:54 | [train_policy] epoch #680 | gradient computed
2021-06-04 13:56:54 | [train_policy] epoch #680 | computing descent direction
2021-06-04 13:56:54 | [train_policy] epoch #680 | descent direction computed
2021-06-04 13:56:54 | [train_policy] epoch #680 | backtrack iters: 0
2021-06-04 13:56:54 | [train_policy] epoch #680 | optimization finished
2021-06-04 13:56:54 | [train_policy] epoch #680 | Computing KL after
2021-06-04 13:56:54 | [train_policy] epoch #680 | Computing loss after
2021-06-04 13:56:54 | [train_policy] epoch #680 | Fitting baseline...
2021-06-04 13:56:54 | [train_policy] epoch #680 | Saving snapshot...
2021-06-04 13:56:54 | [train_policy] epoch #680 | Saved
2021-06-04 13:56:54 | [train_policy] epoch #680 | Time 546.43 s
2021-06-04 13:56:54 | [train_policy] epoch #680 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286116
Evaluation/AverageDiscountedReturn          -42.1446
Evaluation/AverageReturn                    -42.1446
Evaluation/CompletionRate                     0
Evaluation/Iteration                        680
Evaluation/MaxReturn                        -29.2874
Evaluation/MinReturn                        -76.4953
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.86683
Extras/EpisodeRewardMean                    -41.9174
LinearFeatureBaseline/ExplainedVariance       0.883194
PolicyExecTime                                0.229827
ProcessExecTime                               0.0313444
TotalEnvSteps                            689172
policy/Entropy                               -1.56464
policy/KL                                     0.00982481
policy/KLBefore                               0
policy/LossAfter                             -0.0246865
policy/LossBefore                             9.89484e-09
policy/Perplexity                             0.209162
policy/dLoss                                  0.0246865
---------------------------------------  ----------------
2021-06-04 13:56:54 | [train_policy] epoch #681 | Obtaining samples for iteration 681...
2021-06-04 13:56:55 | [train_policy] epoch #681 | Logging diagnostics...
2021-06-04 13:56:55 | [train_policy] epoch #681 | Optimizing policy...
2021-06-04 13:56:55 | [train_policy] epoch #681 | Computing loss before
2021-06-04 13:56:55 | [train_policy] epoch #681 | Computing KL before
2021-06-04 13:56:55 | [train_policy] epoch #681 | Optimizing
2021-06-04 13:56:55 | [train_policy] epoch #681 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:55 | [train_policy] epoch #681 | computing loss before
2021-06-04 13:56:55 | [train_policy] epoch #681 | computing gradient
2021-06-04 13:56:55 | [train_policy] epoch #681 | gradient computed
2021-06-04 13:56:55 | [train_policy] epoch #681 | computing descent direction
2021-06-04 13:56:55 | [train_policy] epoch #681 | descent direction computed
2021-06-04 13:56:55 | [train_policy] epoch #681 | backtrack iters: 0
2021-06-04 13:56:55 | [train_policy] epoch #681 | optimization finished
2021-06-04 13:56:55 | [train_policy] epoch #681 | Computing KL after
2021-06-04 13:56:55 | [train_policy] epoch #681 | Computing loss after
2021-06-04 13:56:55 | [train_policy] epoch #681 | Fitting baseline...
2021-06-04 13:56:55 | [train_policy] epoch #681 | Saving snapshot...
2021-06-04 13:56:55 | [train_policy] epoch #681 | Saved
2021-06-04 13:56:55 | [train_policy] epoch #681 | Time 547.24 s
2021-06-04 13:56:55 | [train_policy] epoch #681 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283251
Evaluation/AverageDiscountedReturn          -40.322
Evaluation/AverageReturn                    -40.322
Evaluation/CompletionRate                     0
Evaluation/Iteration                        681
Evaluation/MaxReturn                        -30.2225
Evaluation/MinReturn                        -75.8018
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.76742
Extras/EpisodeRewardMean                    -40.4561
LinearFeatureBaseline/ExplainedVariance       0.87599
PolicyExecTime                                0.231099
ProcessExecTime                               0.0311322
TotalEnvSteps                            690184
policy/Entropy                               -1.54341
policy/KL                                     0.00997334
policy/KLBefore                               0
policy/LossAfter                             -0.0250429
policy/LossBefore                             2.66218e-08
policy/Perplexity                             0.21365
policy/dLoss                                  0.0250429
---------------------------------------  ----------------
2021-06-04 13:56:55 | [train_policy] epoch #682 | Obtaining samples for iteration 682...
2021-06-04 13:56:56 | [train_policy] epoch #682 | Logging diagnostics...
2021-06-04 13:56:56 | [train_policy] epoch #682 | Optimizing policy...
2021-06-04 13:56:56 | [train_policy] epoch #682 | Computing loss before
2021-06-04 13:56:56 | [train_policy] epoch #682 | Computing KL before
2021-06-04 13:56:56 | [train_policy] epoch #682 | Optimizing
2021-06-04 13:56:56 | [train_policy] epoch #682 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:56 | [train_policy] epoch #682 | computing loss before
2021-06-04 13:56:56 | [train_policy] epoch #682 | computing gradient
2021-06-04 13:56:56 | [train_policy] epoch #682 | gradient computed
2021-06-04 13:56:56 | [train_policy] epoch #682 | computing descent direction
2021-06-04 13:56:56 | [train_policy] epoch #682 | descent direction computed
2021-06-04 13:56:56 | [train_policy] epoch #682 | backtrack iters: 1
2021-06-04 13:56:56 | [train_policy] epoch #682 | optimization finished
2021-06-04 13:56:56 | [train_policy] epoch #682 | Computing KL after
2021-06-04 13:56:56 | [train_policy] epoch #682 | Computing loss after
2021-06-04 13:56:56 | [train_policy] epoch #682 | Fitting baseline...
2021-06-04 13:56:56 | [train_policy] epoch #682 | Saving snapshot...
2021-06-04 13:56:56 | [train_policy] epoch #682 | Saved
2021-06-04 13:56:56 | [train_policy] epoch #682 | Time 548.06 s
2021-06-04 13:56:56 | [train_policy] epoch #682 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284006
Evaluation/AverageDiscountedReturn          -42.5905
Evaluation/AverageReturn                    -42.5905
Evaluation/CompletionRate                     0
Evaluation/Iteration                        682
Evaluation/MaxReturn                        -31.8243
Evaluation/MinReturn                        -75.9311
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.95546
Extras/EpisodeRewardMean                    -42.6454
LinearFeatureBaseline/ExplainedVariance       0.876188
PolicyExecTime                                0.230542
ProcessExecTime                               0.0311422
TotalEnvSteps                            691196
policy/Entropy                               -1.53638
policy/KL                                     0.00648479
policy/KLBefore                               0
policy/LossAfter                             -0.0117088
policy/LossBefore                             2.23812e-09
policy/Perplexity                             0.21516
policy/dLoss                                  0.0117088
---------------------------------------  ----------------
2021-06-04 13:56:56 | [train_policy] epoch #683 | Obtaining samples for iteration 683...
2021-06-04 13:56:56 | [train_policy] epoch #683 | Logging diagnostics...
2021-06-04 13:56:56 | [train_policy] epoch #683 | Optimizing policy...
2021-06-04 13:56:56 | [train_policy] epoch #683 | Computing loss before
2021-06-04 13:56:56 | [train_policy] epoch #683 | Computing KL before
2021-06-04 13:56:56 | [train_policy] epoch #683 | Optimizing
2021-06-04 13:56:56 | [train_policy] epoch #683 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:56 | [train_policy] epoch #683 | computing loss before
2021-06-04 13:56:56 | [train_policy] epoch #683 | computing gradient
2021-06-04 13:56:56 | [train_policy] epoch #683 | gradient computed
2021-06-04 13:56:57 | [train_policy] epoch #683 | computing descent direction
2021-06-04 13:56:57 | [train_policy] epoch #683 | descent direction computed
2021-06-04 13:56:57 | [train_policy] epoch #683 | backtrack iters: 1
2021-06-04 13:56:57 | [train_policy] epoch #683 | optimization finished
2021-06-04 13:56:57 | [train_policy] epoch #683 | Computing KL after
2021-06-04 13:56:57 | [train_policy] epoch #683 | Computing loss after
2021-06-04 13:56:57 | [train_policy] epoch #683 | Fitting baseline...
2021-06-04 13:56:57 | [train_policy] epoch #683 | Saving snapshot...
2021-06-04 13:56:57 | [train_policy] epoch #683 | Saved
2021-06-04 13:56:57 | [train_policy] epoch #683 | Time 548.86 s
2021-06-04 13:56:57 | [train_policy] epoch #683 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284267
Evaluation/AverageDiscountedReturn          -41.7313
Evaluation/AverageReturn                    -41.7313
Evaluation/CompletionRate                     0
Evaluation/Iteration                        683
Evaluation/MaxReturn                        -29.0249
Evaluation/MinReturn                        -78.2186
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.12396
Extras/EpisodeRewardMean                    -42.0068
LinearFeatureBaseline/ExplainedVariance       0.875667
PolicyExecTime                                0.226623
ProcessExecTime                               0.0313346
TotalEnvSteps                            692208
policy/Entropy                               -1.55546
policy/KL                                     0.0066324
policy/KLBefore                               0
policy/LossAfter                             -0.0173437
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.211092
policy/dLoss                                  0.0173436
---------------------------------------  ----------------
2021-06-04 13:56:57 | [train_policy] epoch #684 | Obtaining samples for iteration 684...
2021-06-04 13:56:57 | [train_policy] epoch #684 | Logging diagnostics...
2021-06-04 13:56:57 | [train_policy] epoch #684 | Optimizing policy...
2021-06-04 13:56:57 | [train_policy] epoch #684 | Computing loss before
2021-06-04 13:56:57 | [train_policy] epoch #684 | Computing KL before
2021-06-04 13:56:57 | [train_policy] epoch #684 | Optimizing
2021-06-04 13:56:57 | [train_policy] epoch #684 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:57 | [train_policy] epoch #684 | computing loss before
2021-06-04 13:56:57 | [train_policy] epoch #684 | computing gradient
2021-06-04 13:56:57 | [train_policy] epoch #684 | gradient computed
2021-06-04 13:56:57 | [train_policy] epoch #684 | computing descent direction
2021-06-04 13:56:57 | [train_policy] epoch #684 | descent direction computed
2021-06-04 13:56:57 | [train_policy] epoch #684 | backtrack iters: 0
2021-06-04 13:56:57 | [train_policy] epoch #684 | optimization finished
2021-06-04 13:56:57 | [train_policy] epoch #684 | Computing KL after
2021-06-04 13:56:57 | [train_policy] epoch #684 | Computing loss after
2021-06-04 13:56:57 | [train_policy] epoch #684 | Fitting baseline...
2021-06-04 13:56:57 | [train_policy] epoch #684 | Saving snapshot...
2021-06-04 13:56:57 | [train_policy] epoch #684 | Saved
2021-06-04 13:56:57 | [train_policy] epoch #684 | Time 549.67 s
2021-06-04 13:56:57 | [train_policy] epoch #684 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285728
Evaluation/AverageDiscountedReturn          -40.4212
Evaluation/AverageReturn                    -40.4212
Evaluation/CompletionRate                     0
Evaluation/Iteration                        684
Evaluation/MaxReturn                        -31.2241
Evaluation/MinReturn                        -77.0139
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.33477
Extras/EpisodeRewardMean                    -40.6051
LinearFeatureBaseline/ExplainedVariance       0.900494
PolicyExecTime                                0.232488
ProcessExecTime                               0.0312564
TotalEnvSteps                            693220
policy/Entropy                               -1.58938
policy/KL                                     0.00988404
policy/KLBefore                               0
policy/LossAfter                             -0.0210646
policy/LossBefore                             1.29575e-08
policy/Perplexity                             0.204052
policy/dLoss                                  0.0210646
---------------------------------------  ----------------
2021-06-04 13:56:57 | [train_policy] epoch #685 | Obtaining samples for iteration 685...
2021-06-04 13:56:58 | [train_policy] epoch #685 | Logging diagnostics...
2021-06-04 13:56:58 | [train_policy] epoch #685 | Optimizing policy...
2021-06-04 13:56:58 | [train_policy] epoch #685 | Computing loss before
2021-06-04 13:56:58 | [train_policy] epoch #685 | Computing KL before
2021-06-04 13:56:58 | [train_policy] epoch #685 | Optimizing
2021-06-04 13:56:58 | [train_policy] epoch #685 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:58 | [train_policy] epoch #685 | computing loss before
2021-06-04 13:56:58 | [train_policy] epoch #685 | computing gradient
2021-06-04 13:56:58 | [train_policy] epoch #685 | gradient computed
2021-06-04 13:56:58 | [train_policy] epoch #685 | computing descent direction
2021-06-04 13:56:58 | [train_policy] epoch #685 | descent direction computed
2021-06-04 13:56:58 | [train_policy] epoch #685 | backtrack iters: 1
2021-06-04 13:56:58 | [train_policy] epoch #685 | optimization finished
2021-06-04 13:56:58 | [train_policy] epoch #685 | Computing KL after
2021-06-04 13:56:58 | [train_policy] epoch #685 | Computing loss after
2021-06-04 13:56:58 | [train_policy] epoch #685 | Fitting baseline...
2021-06-04 13:56:58 | [train_policy] epoch #685 | Saving snapshot...
2021-06-04 13:56:58 | [train_policy] epoch #685 | Saved
2021-06-04 13:56:58 | [train_policy] epoch #685 | Time 550.47 s
2021-06-04 13:56:58 | [train_policy] epoch #685 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284388
Evaluation/AverageDiscountedReturn          -42.0276
Evaluation/AverageReturn                    -42.0276
Evaluation/CompletionRate                     0
Evaluation/Iteration                        685
Evaluation/MaxReturn                        -28.5121
Evaluation/MinReturn                        -64.0118
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.84686
Extras/EpisodeRewardMean                    -41.6504
LinearFeatureBaseline/ExplainedVariance       0.885149
PolicyExecTime                                0.222579
ProcessExecTime                               0.0312243
TotalEnvSteps                            694232
policy/Entropy                               -1.60733
policy/KL                                     0.00662792
policy/KLBefore                               0
policy/LossAfter                             -0.010539
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.200422
policy/dLoss                                  0.010539
---------------------------------------  ----------------
2021-06-04 13:56:58 | [train_policy] epoch #686 | Obtaining samples for iteration 686...
2021-06-04 13:56:59 | [train_policy] epoch #686 | Logging diagnostics...
2021-06-04 13:56:59 | [train_policy] epoch #686 | Optimizing policy...
2021-06-04 13:56:59 | [train_policy] epoch #686 | Computing loss before
2021-06-04 13:56:59 | [train_policy] epoch #686 | Computing KL before
2021-06-04 13:56:59 | [train_policy] epoch #686 | Optimizing
2021-06-04 13:56:59 | [train_policy] epoch #686 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:56:59 | [train_policy] epoch #686 | computing loss before
2021-06-04 13:56:59 | [train_policy] epoch #686 | computing gradient
2021-06-04 13:56:59 | [train_policy] epoch #686 | gradient computed
2021-06-04 13:56:59 | [train_policy] epoch #686 | computing descent direction
2021-06-04 13:56:59 | [train_policy] epoch #686 | descent direction computed
2021-06-04 13:56:59 | [train_policy] epoch #686 | backtrack iters: 0
2021-06-04 13:56:59 | [train_policy] epoch #686 | optimization finished
2021-06-04 13:56:59 | [train_policy] epoch #686 | Computing KL after
2021-06-04 13:56:59 | [train_policy] epoch #686 | Computing loss after
2021-06-04 13:56:59 | [train_policy] epoch #686 | Fitting baseline...
2021-06-04 13:56:59 | [train_policy] epoch #686 | Saving snapshot...
2021-06-04 13:56:59 | [train_policy] epoch #686 | Saved
2021-06-04 13:56:59 | [train_policy] epoch #686 | Time 551.27 s
2021-06-04 13:56:59 | [train_policy] epoch #686 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286053
Evaluation/AverageDiscountedReturn          -41.974
Evaluation/AverageReturn                    -41.974
Evaluation/CompletionRate                     0
Evaluation/Iteration                        686
Evaluation/MaxReturn                        -28.8539
Evaluation/MinReturn                        -64.0836
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.13064
Extras/EpisodeRewardMean                    -42.1089
LinearFeatureBaseline/ExplainedVariance       0.890802
PolicyExecTime                                0.227578
ProcessExecTime                               0.0313137
TotalEnvSteps                            695244
policy/Entropy                               -1.58857
policy/KL                                     0.00983668
policy/KLBefore                               0
policy/LossAfter                             -0.0178266
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             0.204217
policy/dLoss                                  0.0178266
---------------------------------------  ----------------
2021-06-04 13:56:59 | [train_policy] epoch #687 | Obtaining samples for iteration 687...
2021-06-04 13:57:00 | [train_policy] epoch #687 | Logging diagnostics...
2021-06-04 13:57:00 | [train_policy] epoch #687 | Optimizing policy...
2021-06-04 13:57:00 | [train_policy] epoch #687 | Computing loss before
2021-06-04 13:57:00 | [train_policy] epoch #687 | Computing KL before
2021-06-04 13:57:00 | [train_policy] epoch #687 | Optimizing
2021-06-04 13:57:00 | [train_policy] epoch #687 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:00 | [train_policy] epoch #687 | computing loss before
2021-06-04 13:57:00 | [train_policy] epoch #687 | computing gradient
2021-06-04 13:57:00 | [train_policy] epoch #687 | gradient computed
2021-06-04 13:57:00 | [train_policy] epoch #687 | computing descent direction
2021-06-04 13:57:00 | [train_policy] epoch #687 | descent direction computed
2021-06-04 13:57:00 | [train_policy] epoch #687 | backtrack iters: 1
2021-06-04 13:57:00 | [train_policy] epoch #687 | optimization finished
2021-06-04 13:57:00 | [train_policy] epoch #687 | Computing KL after
2021-06-04 13:57:00 | [train_policy] epoch #687 | Computing loss after
2021-06-04 13:57:00 | [train_policy] epoch #687 | Fitting baseline...
2021-06-04 13:57:00 | [train_policy] epoch #687 | Saving snapshot...
2021-06-04 13:57:00 | [train_policy] epoch #687 | Saved
2021-06-04 13:57:00 | [train_policy] epoch #687 | Time 552.08 s
2021-06-04 13:57:00 | [train_policy] epoch #687 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286246
Evaluation/AverageDiscountedReturn          -41.9125
Evaluation/AverageReturn                    -41.9125
Evaluation/CompletionRate                     0
Evaluation/Iteration                        687
Evaluation/MaxReturn                        -30.7779
Evaluation/MinReturn                        -77.6175
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.80462
Extras/EpisodeRewardMean                    -42.0909
LinearFeatureBaseline/ExplainedVariance       0.903567
PolicyExecTime                                0.23554
ProcessExecTime                               0.0312209
TotalEnvSteps                            696256
policy/Entropy                               -1.59296
policy/KL                                     0.00683111
policy/KLBefore                               0
policy/LossAfter                             -0.0192011
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.203323
policy/dLoss                                  0.0192011
---------------------------------------  ----------------
2021-06-04 13:57:00 | [train_policy] epoch #688 | Obtaining samples for iteration 688...
2021-06-04 13:57:01 | [train_policy] epoch #688 | Logging diagnostics...
2021-06-04 13:57:01 | [train_policy] epoch #688 | Optimizing policy...
2021-06-04 13:57:01 | [train_policy] epoch #688 | Computing loss before
2021-06-04 13:57:01 | [train_policy] epoch #688 | Computing KL before
2021-06-04 13:57:01 | [train_policy] epoch #688 | Optimizing
2021-06-04 13:57:01 | [train_policy] epoch #688 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:01 | [train_policy] epoch #688 | computing loss before
2021-06-04 13:57:01 | [train_policy] epoch #688 | computing gradient
2021-06-04 13:57:01 | [train_policy] epoch #688 | gradient computed
2021-06-04 13:57:01 | [train_policy] epoch #688 | computing descent direction
2021-06-04 13:57:01 | [train_policy] epoch #688 | descent direction computed
2021-06-04 13:57:01 | [train_policy] epoch #688 | backtrack iters: 1
2021-06-04 13:57:01 | [train_policy] epoch #688 | optimization finished
2021-06-04 13:57:01 | [train_policy] epoch #688 | Computing KL after
2021-06-04 13:57:01 | [train_policy] epoch #688 | Computing loss after
2021-06-04 13:57:01 | [train_policy] epoch #688 | Fitting baseline...
2021-06-04 13:57:01 | [train_policy] epoch #688 | Saving snapshot...
2021-06-04 13:57:01 | [train_policy] epoch #688 | Saved
2021-06-04 13:57:01 | [train_policy] epoch #688 | Time 552.91 s
2021-06-04 13:57:01 | [train_policy] epoch #688 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.289913
Evaluation/AverageDiscountedReturn          -42.6837
Evaluation/AverageReturn                    -42.6837
Evaluation/CompletionRate                     0
Evaluation/Iteration                        688
Evaluation/MaxReturn                        -28.931
Evaluation/MinReturn                        -76.7889
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.16825
Extras/EpisodeRewardMean                    -42.3756
LinearFeatureBaseline/ExplainedVariance       0.866342
PolicyExecTime                                0.235941
ProcessExecTime                               0.0317848
TotalEnvSteps                            697268
policy/Entropy                               -1.65935
policy/KL                                     0.00672983
policy/KLBefore                               0
policy/LossAfter                             -0.020521
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             0.190263
policy/dLoss                                  0.020521
---------------------------------------  ----------------
2021-06-04 13:57:01 | [train_policy] epoch #689 | Obtaining samples for iteration 689...
2021-06-04 13:57:01 | [train_policy] epoch #689 | Logging diagnostics...
2021-06-04 13:57:01 | [train_policy] epoch #689 | Optimizing policy...
2021-06-04 13:57:01 | [train_policy] epoch #689 | Computing loss before
2021-06-04 13:57:01 | [train_policy] epoch #689 | Computing KL before
2021-06-04 13:57:01 | [train_policy] epoch #689 | Optimizing
2021-06-04 13:57:01 | [train_policy] epoch #689 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:01 | [train_policy] epoch #689 | computing loss before
2021-06-04 13:57:01 | [train_policy] epoch #689 | computing gradient
2021-06-04 13:57:01 | [train_policy] epoch #689 | gradient computed
2021-06-04 13:57:01 | [train_policy] epoch #689 | computing descent direction
2021-06-04 13:57:01 | [train_policy] epoch #689 | descent direction computed
2021-06-04 13:57:01 | [train_policy] epoch #689 | backtrack iters: 0
2021-06-04 13:57:01 | [train_policy] epoch #689 | optimization finished
2021-06-04 13:57:01 | [train_policy] epoch #689 | Computing KL after
2021-06-04 13:57:01 | [train_policy] epoch #689 | Computing loss after
2021-06-04 13:57:01 | [train_policy] epoch #689 | Fitting baseline...
2021-06-04 13:57:01 | [train_policy] epoch #689 | Saving snapshot...
2021-06-04 13:57:01 | [train_policy] epoch #689 | Saved
2021-06-04 13:57:01 | [train_policy] epoch #689 | Time 553.70 s
2021-06-04 13:57:01 | [train_policy] epoch #689 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.285846
Evaluation/AverageDiscountedReturn          -39.8444
Evaluation/AverageReturn                    -39.8444
Evaluation/CompletionRate                     0
Evaluation/Iteration                        689
Evaluation/MaxReturn                        -27.8008
Evaluation/MinReturn                        -58.0746
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.62666
Extras/EpisodeRewardMean                    -39.8806
LinearFeatureBaseline/ExplainedVariance       0.906058
PolicyExecTime                                0.219614
ProcessExecTime                               0.0311625
TotalEnvSteps                            698280
policy/Entropy                               -1.66341
policy/KL                                     0.00996001
policy/KLBefore                               0
policy/LossAfter                             -0.0235325
policy/LossBefore                             8.2457e-09
policy/Perplexity                             0.189492
policy/dLoss                                  0.0235325
---------------------------------------  ---------------
2021-06-04 13:57:02 | [train_policy] epoch #690 | Obtaining samples for iteration 690...
2021-06-04 13:57:02 | [train_policy] epoch #690 | Logging diagnostics...
2021-06-04 13:57:02 | [train_policy] epoch #690 | Optimizing policy...
2021-06-04 13:57:02 | [train_policy] epoch #690 | Computing loss before
2021-06-04 13:57:02 | [train_policy] epoch #690 | Computing KL before
2021-06-04 13:57:02 | [train_policy] epoch #690 | Optimizing
2021-06-04 13:57:02 | [train_policy] epoch #690 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:02 | [train_policy] epoch #690 | computing loss before
2021-06-04 13:57:02 | [train_policy] epoch #690 | computing gradient
2021-06-04 13:57:02 | [train_policy] epoch #690 | gradient computed
2021-06-04 13:57:02 | [train_policy] epoch #690 | computing descent direction
2021-06-04 13:57:02 | [train_policy] epoch #690 | descent direction computed
2021-06-04 13:57:02 | [train_policy] epoch #690 | backtrack iters: 1
2021-06-04 13:57:02 | [train_policy] epoch #690 | optimization finished
2021-06-04 13:57:02 | [train_policy] epoch #690 | Computing KL after
2021-06-04 13:57:02 | [train_policy] epoch #690 | Computing loss after
2021-06-04 13:57:02 | [train_policy] epoch #690 | Fitting baseline...
2021-06-04 13:57:02 | [train_policy] epoch #690 | Saving snapshot...
2021-06-04 13:57:02 | [train_policy] epoch #690 | Saved
2021-06-04 13:57:02 | [train_policy] epoch #690 | Time 554.51 s
2021-06-04 13:57:02 | [train_policy] epoch #690 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287191
Evaluation/AverageDiscountedReturn          -42.194
Evaluation/AverageReturn                    -42.194
Evaluation/CompletionRate                     0
Evaluation/Iteration                        690
Evaluation/MaxReturn                        -29.1564
Evaluation/MinReturn                        -63.9971
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.44895
Extras/EpisodeRewardMean                    -42.125
LinearFeatureBaseline/ExplainedVariance       0.895689
PolicyExecTime                                0.231436
ProcessExecTime                               0.0313621
TotalEnvSteps                            699292
policy/Entropy                               -1.65339
policy/KL                                     0.00642069
policy/KLBefore                               0
policy/LossAfter                             -0.0143641
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.191401
policy/dLoss                                  0.0143641
---------------------------------------  ----------------
2021-06-04 13:57:02 | [train_policy] epoch #691 | Obtaining samples for iteration 691...
2021-06-04 13:57:03 | [train_policy] epoch #691 | Logging diagnostics...
2021-06-04 13:57:03 | [train_policy] epoch #691 | Optimizing policy...
2021-06-04 13:57:03 | [train_policy] epoch #691 | Computing loss before
2021-06-04 13:57:03 | [train_policy] epoch #691 | Computing KL before
2021-06-04 13:57:03 | [train_policy] epoch #691 | Optimizing
2021-06-04 13:57:03 | [train_policy] epoch #691 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:03 | [train_policy] epoch #691 | computing loss before
2021-06-04 13:57:03 | [train_policy] epoch #691 | computing gradient
2021-06-04 13:57:03 | [train_policy] epoch #691 | gradient computed
2021-06-04 13:57:03 | [train_policy] epoch #691 | computing descent direction
2021-06-04 13:57:03 | [train_policy] epoch #691 | descent direction computed
2021-06-04 13:57:03 | [train_policy] epoch #691 | backtrack iters: 0
2021-06-04 13:57:03 | [train_policy] epoch #691 | optimization finished
2021-06-04 13:57:03 | [train_policy] epoch #691 | Computing KL after
2021-06-04 13:57:03 | [train_policy] epoch #691 | Computing loss after
2021-06-04 13:57:03 | [train_policy] epoch #691 | Fitting baseline...
2021-06-04 13:57:03 | [train_policy] epoch #691 | Saving snapshot...
2021-06-04 13:57:03 | [train_policy] epoch #691 | Saved
2021-06-04 13:57:03 | [train_policy] epoch #691 | Time 555.29 s
2021-06-04 13:57:03 | [train_policy] epoch #691 | EpochTime 0.75 s
---------------------------------------  ---------------
EnvExecTime                                   0.284974
Evaluation/AverageDiscountedReturn          -41.1014
Evaluation/AverageReturn                    -41.1014
Evaluation/CompletionRate                     0
Evaluation/Iteration                        691
Evaluation/MaxReturn                        -28.7628
Evaluation/MinReturn                        -60.7001
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.17256
Extras/EpisodeRewardMean                    -41.2622
LinearFeatureBaseline/ExplainedVariance       0.897906
PolicyExecTime                                0.209679
ProcessExecTime                               0.0311711
TotalEnvSteps                            700304
policy/Entropy                               -1.66606
policy/KL                                     0.00964923
policy/KLBefore                               0
policy/LossAfter                             -0.00969359
policy/LossBefore                            -0
policy/Perplexity                             0.188989
policy/dLoss                                  0.00969359
---------------------------------------  ---------------
2021-06-04 13:57:03 | [train_policy] epoch #692 | Obtaining samples for iteration 692...
2021-06-04 13:57:04 | [train_policy] epoch #692 | Logging diagnostics...
2021-06-04 13:57:04 | [train_policy] epoch #692 | Optimizing policy...
2021-06-04 13:57:04 | [train_policy] epoch #692 | Computing loss before
2021-06-04 13:57:04 | [train_policy] epoch #692 | Computing KL before
2021-06-04 13:57:04 | [train_policy] epoch #692 | Optimizing
2021-06-04 13:57:04 | [train_policy] epoch #692 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:04 | [train_policy] epoch #692 | computing loss before
2021-06-04 13:57:04 | [train_policy] epoch #692 | computing gradient
2021-06-04 13:57:04 | [train_policy] epoch #692 | gradient computed
2021-06-04 13:57:04 | [train_policy] epoch #692 | computing descent direction
2021-06-04 13:57:04 | [train_policy] epoch #692 | descent direction computed
2021-06-04 13:57:04 | [train_policy] epoch #692 | backtrack iters: 1
2021-06-04 13:57:04 | [train_policy] epoch #692 | optimization finished
2021-06-04 13:57:04 | [train_policy] epoch #692 | Computing KL after
2021-06-04 13:57:04 | [train_policy] epoch #692 | Computing loss after
2021-06-04 13:57:04 | [train_policy] epoch #692 | Fitting baseline...
2021-06-04 13:57:04 | [train_policy] epoch #692 | Saving snapshot...
2021-06-04 13:57:04 | [train_policy] epoch #692 | Saved
2021-06-04 13:57:04 | [train_policy] epoch #692 | Time 556.09 s
2021-06-04 13:57:04 | [train_policy] epoch #692 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.284982
Evaluation/AverageDiscountedReturn          -43.3494
Evaluation/AverageReturn                    -43.3494
Evaluation/CompletionRate                     0
Evaluation/Iteration                        692
Evaluation/MaxReturn                        -28.8959
Evaluation/MinReturn                        -63.9982
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.66157
Extras/EpisodeRewardMean                    -42.882
LinearFeatureBaseline/ExplainedVariance       0.912412
PolicyExecTime                                0.211615
ProcessExecTime                               0.0318646
TotalEnvSteps                            701316
policy/Entropy                               -1.68356
policy/KL                                     0.00641474
policy/KLBefore                               0
policy/LossAfter                             -0.0136302
policy/LossBefore                             8.2457e-09
policy/Perplexity                             0.185712
policy/dLoss                                  0.0136302
---------------------------------------  ---------------
2021-06-04 13:57:04 | [train_policy] epoch #693 | Obtaining samples for iteration 693...
2021-06-04 13:57:05 | [train_policy] epoch #693 | Logging diagnostics...
2021-06-04 13:57:05 | [train_policy] epoch #693 | Optimizing policy...
2021-06-04 13:57:05 | [train_policy] epoch #693 | Computing loss before
2021-06-04 13:57:05 | [train_policy] epoch #693 | Computing KL before
2021-06-04 13:57:05 | [train_policy] epoch #693 | Optimizing
2021-06-04 13:57:05 | [train_policy] epoch #693 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:05 | [train_policy] epoch #693 | computing loss before
2021-06-04 13:57:05 | [train_policy] epoch #693 | computing gradient
2021-06-04 13:57:05 | [train_policy] epoch #693 | gradient computed
2021-06-04 13:57:05 | [train_policy] epoch #693 | computing descent direction
2021-06-04 13:57:05 | [train_policy] epoch #693 | descent direction computed
2021-06-04 13:57:05 | [train_policy] epoch #693 | backtrack iters: 0
2021-06-04 13:57:05 | [train_policy] epoch #693 | optimization finished
2021-06-04 13:57:05 | [train_policy] epoch #693 | Computing KL after
2021-06-04 13:57:05 | [train_policy] epoch #693 | Computing loss after
2021-06-04 13:57:05 | [train_policy] epoch #693 | Fitting baseline...
2021-06-04 13:57:05 | [train_policy] epoch #693 | Saving snapshot...
2021-06-04 13:57:05 | [train_policy] epoch #693 | Saved
2021-06-04 13:57:05 | [train_policy] epoch #693 | Time 556.89 s
2021-06-04 13:57:05 | [train_policy] epoch #693 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286677
Evaluation/AverageDiscountedReturn          -40.3788
Evaluation/AverageReturn                    -40.3788
Evaluation/CompletionRate                     0
Evaluation/Iteration                        693
Evaluation/MaxReturn                        -28.1528
Evaluation/MinReturn                        -79.5382
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.99898
Extras/EpisodeRewardMean                    -40.5945
LinearFeatureBaseline/ExplainedVariance       0.87455
PolicyExecTime                                0.233069
ProcessExecTime                               0.0313122
TotalEnvSteps                            702328
policy/Entropy                               -1.68576
policy/KL                                     0.00982406
policy/KLBefore                               0
policy/LossAfter                             -0.0114085
policy/LossBefore                             9.65925e-09
policy/Perplexity                             0.185303
policy/dLoss                                  0.0114085
---------------------------------------  ----------------
2021-06-04 13:57:05 | [train_policy] epoch #694 | Obtaining samples for iteration 694...
2021-06-04 13:57:05 | [train_policy] epoch #694 | Logging diagnostics...
2021-06-04 13:57:05 | [train_policy] epoch #694 | Optimizing policy...
2021-06-04 13:57:05 | [train_policy] epoch #694 | Computing loss before
2021-06-04 13:57:05 | [train_policy] epoch #694 | Computing KL before
2021-06-04 13:57:05 | [train_policy] epoch #694 | Optimizing
2021-06-04 13:57:05 | [train_policy] epoch #694 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:05 | [train_policy] epoch #694 | computing loss before
2021-06-04 13:57:05 | [train_policy] epoch #694 | computing gradient
2021-06-04 13:57:05 | [train_policy] epoch #694 | gradient computed
2021-06-04 13:57:05 | [train_policy] epoch #694 | computing descent direction
2021-06-04 13:57:05 | [train_policy] epoch #694 | descent direction computed
2021-06-04 13:57:05 | [train_policy] epoch #694 | backtrack iters: 1
2021-06-04 13:57:05 | [train_policy] epoch #694 | optimization finished
2021-06-04 13:57:05 | [train_policy] epoch #694 | Computing KL after
2021-06-04 13:57:05 | [train_policy] epoch #694 | Computing loss after
2021-06-04 13:57:05 | [train_policy] epoch #694 | Fitting baseline...
2021-06-04 13:57:05 | [train_policy] epoch #694 | Saving snapshot...
2021-06-04 13:57:05 | [train_policy] epoch #694 | Saved
2021-06-04 13:57:05 | [train_policy] epoch #694 | Time 557.71 s
2021-06-04 13:57:05 | [train_policy] epoch #694 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287606
Evaluation/AverageDiscountedReturn          -43.6786
Evaluation/AverageReturn                    -43.6786
Evaluation/CompletionRate                     0
Evaluation/Iteration                        694
Evaluation/MaxReturn                        -29.3812
Evaluation/MinReturn                        -79.4288
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.5397
Extras/EpisodeRewardMean                    -43.4463
LinearFeatureBaseline/ExplainedVariance       0.869766
PolicyExecTime                                0.221485
ProcessExecTime                               0.0314114
TotalEnvSteps                            703340
policy/Entropy                               -1.74444
policy/KL                                     0.00665312
policy/KLBefore                               0
policy/LossAfter                             -0.0110466
policy/LossBefore                             1.26041e-08
policy/Perplexity                             0.174744
policy/dLoss                                  0.0110466
---------------------------------------  ----------------
2021-06-04 13:57:06 | [train_policy] epoch #695 | Obtaining samples for iteration 695...
2021-06-04 13:57:06 | [train_policy] epoch #695 | Logging diagnostics...
2021-06-04 13:57:06 | [train_policy] epoch #695 | Optimizing policy...
2021-06-04 13:57:06 | [train_policy] epoch #695 | Computing loss before
2021-06-04 13:57:06 | [train_policy] epoch #695 | Computing KL before
2021-06-04 13:57:06 | [train_policy] epoch #695 | Optimizing
2021-06-04 13:57:06 | [train_policy] epoch #695 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:06 | [train_policy] epoch #695 | computing loss before
2021-06-04 13:57:06 | [train_policy] epoch #695 | computing gradient
2021-06-04 13:57:06 | [train_policy] epoch #695 | gradient computed
2021-06-04 13:57:06 | [train_policy] epoch #695 | computing descent direction
2021-06-04 13:57:06 | [train_policy] epoch #695 | descent direction computed
2021-06-04 13:57:06 | [train_policy] epoch #695 | backtrack iters: 1
2021-06-04 13:57:06 | [train_policy] epoch #695 | optimization finished
2021-06-04 13:57:06 | [train_policy] epoch #695 | Computing KL after
2021-06-04 13:57:06 | [train_policy] epoch #695 | Computing loss after
2021-06-04 13:57:06 | [train_policy] epoch #695 | Fitting baseline...
2021-06-04 13:57:06 | [train_policy] epoch #695 | Saving snapshot...
2021-06-04 13:57:06 | [train_policy] epoch #695 | Saved
2021-06-04 13:57:06 | [train_policy] epoch #695 | Time 558.52 s
2021-06-04 13:57:06 | [train_policy] epoch #695 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.285652
Evaluation/AverageDiscountedReturn          -41.875
Evaluation/AverageReturn                    -41.875
Evaluation/CompletionRate                     0
Evaluation/Iteration                        695
Evaluation/MaxReturn                        -31.8067
Evaluation/MinReturn                        -63.9828
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.51649
Extras/EpisodeRewardMean                    -42.076
LinearFeatureBaseline/ExplainedVariance       0.904675
PolicyExecTime                                0.234516
ProcessExecTime                               0.03109
TotalEnvSteps                            704352
policy/Entropy                               -1.77582
policy/KL                                     0.00648314
policy/KLBefore                               0
policy/LossAfter                             -0.0197206
policy/LossBefore                             1.1544e-08
policy/Perplexity                             0.169344
policy/dLoss                                  0.0197206
---------------------------------------  ---------------
2021-06-04 13:57:06 | [train_policy] epoch #696 | Obtaining samples for iteration 696...
2021-06-04 13:57:07 | [train_policy] epoch #696 | Logging diagnostics...
2021-06-04 13:57:07 | [train_policy] epoch #696 | Optimizing policy...
2021-06-04 13:57:07 | [train_policy] epoch #696 | Computing loss before
2021-06-04 13:57:07 | [train_policy] epoch #696 | Computing KL before
2021-06-04 13:57:07 | [train_policy] epoch #696 | Optimizing
2021-06-04 13:57:07 | [train_policy] epoch #696 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:07 | [train_policy] epoch #696 | computing loss before
2021-06-04 13:57:07 | [train_policy] epoch #696 | computing gradient
2021-06-04 13:57:07 | [train_policy] epoch #696 | gradient computed
2021-06-04 13:57:07 | [train_policy] epoch #696 | computing descent direction
2021-06-04 13:57:07 | [train_policy] epoch #696 | descent direction computed
2021-06-04 13:57:07 | [train_policy] epoch #696 | backtrack iters: 0
2021-06-04 13:57:07 | [train_policy] epoch #696 | optimization finished
2021-06-04 13:57:07 | [train_policy] epoch #696 | Computing KL after
2021-06-04 13:57:07 | [train_policy] epoch #696 | Computing loss after
2021-06-04 13:57:07 | [train_policy] epoch #696 | Fitting baseline...
2021-06-04 13:57:07 | [train_policy] epoch #696 | Saving snapshot...
2021-06-04 13:57:07 | [train_policy] epoch #696 | Saved
2021-06-04 13:57:07 | [train_policy] epoch #696 | Time 559.31 s
2021-06-04 13:57:07 | [train_policy] epoch #696 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284717
Evaluation/AverageDiscountedReturn          -41.7572
Evaluation/AverageReturn                    -41.7572
Evaluation/CompletionRate                     0
Evaluation/Iteration                        696
Evaluation/MaxReturn                        -29.1973
Evaluation/MinReturn                        -63.9644
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.74229
Extras/EpisodeRewardMean                    -41.4882
LinearFeatureBaseline/ExplainedVariance       0.913087
PolicyExecTime                                0.219996
ProcessExecTime                               0.0311275
TotalEnvSteps                            705364
policy/Entropy                               -1.71626
policy/KL                                     0.00959278
policy/KLBefore                               0
policy/LossAfter                             -0.0207194
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             0.179737
policy/dLoss                                  0.0207194
---------------------------------------  ----------------
2021-06-04 13:57:07 | [train_policy] epoch #697 | Obtaining samples for iteration 697...
2021-06-04 13:57:08 | [train_policy] epoch #697 | Logging diagnostics...
2021-06-04 13:57:08 | [train_policy] epoch #697 | Optimizing policy...
2021-06-04 13:57:08 | [train_policy] epoch #697 | Computing loss before
2021-06-04 13:57:08 | [train_policy] epoch #697 | Computing KL before
2021-06-04 13:57:08 | [train_policy] epoch #697 | Optimizing
2021-06-04 13:57:08 | [train_policy] epoch #697 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:08 | [train_policy] epoch #697 | computing loss before
2021-06-04 13:57:08 | [train_policy] epoch #697 | computing gradient
2021-06-04 13:57:08 | [train_policy] epoch #697 | gradient computed
2021-06-04 13:57:08 | [train_policy] epoch #697 | computing descent direction
2021-06-04 13:57:08 | [train_policy] epoch #697 | descent direction computed
2021-06-04 13:57:08 | [train_policy] epoch #697 | backtrack iters: 0
2021-06-04 13:57:08 | [train_policy] epoch #697 | optimization finished
2021-06-04 13:57:08 | [train_policy] epoch #697 | Computing KL after
2021-06-04 13:57:08 | [train_policy] epoch #697 | Computing loss after
2021-06-04 13:57:08 | [train_policy] epoch #697 | Fitting baseline...
2021-06-04 13:57:08 | [train_policy] epoch #697 | Saving snapshot...
2021-06-04 13:57:08 | [train_policy] epoch #697 | Saved
2021-06-04 13:57:08 | [train_policy] epoch #697 | Time 560.12 s
2021-06-04 13:57:08 | [train_policy] epoch #697 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283702
Evaluation/AverageDiscountedReturn          -41.6242
Evaluation/AverageReturn                    -41.6242
Evaluation/CompletionRate                     0
Evaluation/Iteration                        697
Evaluation/MaxReturn                        -28.9835
Evaluation/MinReturn                        -64.0246
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.71036
Extras/EpisodeRewardMean                    -41.4495
LinearFeatureBaseline/ExplainedVariance       0.810362
PolicyExecTime                                0.233789
ProcessExecTime                               0.0311801
TotalEnvSteps                            706376
policy/Entropy                               -1.72643
policy/KL                                     0.00993378
policy/KLBefore                               0
policy/LossAfter                             -0.0274597
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.177919
policy/dLoss                                  0.0274597
---------------------------------------  ----------------
2021-06-04 13:57:08 | [train_policy] epoch #698 | Obtaining samples for iteration 698...
2021-06-04 13:57:09 | [train_policy] epoch #698 | Logging diagnostics...
2021-06-04 13:57:09 | [train_policy] epoch #698 | Optimizing policy...
2021-06-04 13:57:09 | [train_policy] epoch #698 | Computing loss before
2021-06-04 13:57:09 | [train_policy] epoch #698 | Computing KL before
2021-06-04 13:57:09 | [train_policy] epoch #698 | Optimizing
2021-06-04 13:57:09 | [train_policy] epoch #698 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:09 | [train_policy] epoch #698 | computing loss before
2021-06-04 13:57:09 | [train_policy] epoch #698 | computing gradient
2021-06-04 13:57:09 | [train_policy] epoch #698 | gradient computed
2021-06-04 13:57:09 | [train_policy] epoch #698 | computing descent direction
2021-06-04 13:57:09 | [train_policy] epoch #698 | descent direction computed
2021-06-04 13:57:09 | [train_policy] epoch #698 | backtrack iters: 0
2021-06-04 13:57:09 | [train_policy] epoch #698 | optimization finished
2021-06-04 13:57:09 | [train_policy] epoch #698 | Computing KL after
2021-06-04 13:57:09 | [train_policy] epoch #698 | Computing loss after
2021-06-04 13:57:09 | [train_policy] epoch #698 | Fitting baseline...
2021-06-04 13:57:09 | [train_policy] epoch #698 | Saving snapshot...
2021-06-04 13:57:09 | [train_policy] epoch #698 | Saved
2021-06-04 13:57:09 | [train_policy] epoch #698 | Time 560.92 s
2021-06-04 13:57:09 | [train_policy] epoch #698 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287281
Evaluation/AverageDiscountedReturn          -41.6341
Evaluation/AverageReturn                    -41.6341
Evaluation/CompletionRate                     0
Evaluation/Iteration                        698
Evaluation/MaxReturn                        -28.1618
Evaluation/MinReturn                        -63.3103
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.29121
Extras/EpisodeRewardMean                    -41.7026
LinearFeatureBaseline/ExplainedVariance       0.888088
PolicyExecTime                                0.231812
ProcessExecTime                               0.0314023
TotalEnvSteps                            707388
policy/Entropy                               -1.71622
policy/KL                                     0.00978673
policy/KLBefore                               0
policy/LossAfter                             -0.0110024
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.179745
policy/dLoss                                  0.0110024
---------------------------------------  ----------------
2021-06-04 13:57:09 | [train_policy] epoch #699 | Obtaining samples for iteration 699...
2021-06-04 13:57:09 | [train_policy] epoch #699 | Logging diagnostics...
2021-06-04 13:57:09 | [train_policy] epoch #699 | Optimizing policy...
2021-06-04 13:57:09 | [train_policy] epoch #699 | Computing loss before
2021-06-04 13:57:09 | [train_policy] epoch #699 | Computing KL before
2021-06-04 13:57:09 | [train_policy] epoch #699 | Optimizing
2021-06-04 13:57:09 | [train_policy] epoch #699 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:09 | [train_policy] epoch #699 | computing loss before
2021-06-04 13:57:09 | [train_policy] epoch #699 | computing gradient
2021-06-04 13:57:09 | [train_policy] epoch #699 | gradient computed
2021-06-04 13:57:09 | [train_policy] epoch #699 | computing descent direction
2021-06-04 13:57:09 | [train_policy] epoch #699 | descent direction computed
2021-06-04 13:57:09 | [train_policy] epoch #699 | backtrack iters: 0
2021-06-04 13:57:09 | [train_policy] epoch #699 | optimization finished
2021-06-04 13:57:09 | [train_policy] epoch #699 | Computing KL after
2021-06-04 13:57:09 | [train_policy] epoch #699 | Computing loss after
2021-06-04 13:57:09 | [train_policy] epoch #699 | Fitting baseline...
2021-06-04 13:57:09 | [train_policy] epoch #699 | Saving snapshot...
2021-06-04 13:57:09 | [train_policy] epoch #699 | Saved
2021-06-04 13:57:09 | [train_policy] epoch #699 | Time 561.72 s
2021-06-04 13:57:09 | [train_policy] epoch #699 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285034
Evaluation/AverageDiscountedReturn          -41.7338
Evaluation/AverageReturn                    -41.7338
Evaluation/CompletionRate                     0
Evaluation/Iteration                        699
Evaluation/MaxReturn                        -29.0285
Evaluation/MinReturn                        -77.7299
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.8323
Extras/EpisodeRewardMean                    -41.5365
LinearFeatureBaseline/ExplainedVariance       0.875716
PolicyExecTime                                0.211607
ProcessExecTime                               0.031224
TotalEnvSteps                            708400
policy/Entropy                               -1.72771
policy/KL                                     0.00963691
policy/KLBefore                               0
policy/LossAfter                             -0.0221912
policy/LossBefore                            -2.59151e-09
policy/Perplexity                             0.17769
policy/dLoss                                  0.0221912
---------------------------------------  ----------------
2021-06-04 13:57:10 | [train_policy] epoch #700 | Obtaining samples for iteration 700...
2021-06-04 13:57:10 | [train_policy] epoch #700 | Logging diagnostics...
2021-06-04 13:57:10 | [train_policy] epoch #700 | Optimizing policy...
2021-06-04 13:57:10 | [train_policy] epoch #700 | Computing loss before
2021-06-04 13:57:10 | [train_policy] epoch #700 | Computing KL before
2021-06-04 13:57:10 | [train_policy] epoch #700 | Optimizing
2021-06-04 13:57:10 | [train_policy] epoch #700 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:10 | [train_policy] epoch #700 | computing loss before
2021-06-04 13:57:10 | [train_policy] epoch #700 | computing gradient
2021-06-04 13:57:10 | [train_policy] epoch #700 | gradient computed
2021-06-04 13:57:10 | [train_policy] epoch #700 | computing descent direction
2021-06-04 13:57:10 | [train_policy] epoch #700 | descent direction computed
2021-06-04 13:57:10 | [train_policy] epoch #700 | backtrack iters: 1
2021-06-04 13:57:10 | [train_policy] epoch #700 | optimization finished
2021-06-04 13:57:10 | [train_policy] epoch #700 | Computing KL after
2021-06-04 13:57:10 | [train_policy] epoch #700 | Computing loss after
2021-06-04 13:57:10 | [train_policy] epoch #700 | Fitting baseline...
2021-06-04 13:57:10 | [train_policy] epoch #700 | Saving snapshot...
2021-06-04 13:57:10 | [train_policy] epoch #700 | Saved
2021-06-04 13:57:10 | [train_policy] epoch #700 | Time 562.53 s
2021-06-04 13:57:10 | [train_policy] epoch #700 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28364
Evaluation/AverageDiscountedReturn          -40.4181
Evaluation/AverageReturn                    -40.4181
Evaluation/CompletionRate                     0
Evaluation/Iteration                        700
Evaluation/MaxReturn                        -29.4167
Evaluation/MinReturn                        -76.8197
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.01507
Extras/EpisodeRewardMean                    -40.3755
LinearFeatureBaseline/ExplainedVariance       0.878141
PolicyExecTime                                0.220459
ProcessExecTime                               0.0311954
TotalEnvSteps                            709412
policy/Entropy                               -1.74987
policy/KL                                     0.00662863
policy/KLBefore                               0
policy/LossAfter                             -0.0158793
policy/LossBefore                            -9.18807e-09
policy/Perplexity                             0.173797
policy/dLoss                                  0.0158793
---------------------------------------  ----------------
2021-06-04 13:57:10 | [train_policy] epoch #701 | Obtaining samples for iteration 701...
2021-06-04 13:57:11 | [train_policy] epoch #701 | Logging diagnostics...
2021-06-04 13:57:11 | [train_policy] epoch #701 | Optimizing policy...
2021-06-04 13:57:11 | [train_policy] epoch #701 | Computing loss before
2021-06-04 13:57:11 | [train_policy] epoch #701 | Computing KL before
2021-06-04 13:57:11 | [train_policy] epoch #701 | Optimizing
2021-06-04 13:57:11 | [train_policy] epoch #701 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:11 | [train_policy] epoch #701 | computing loss before
2021-06-04 13:57:11 | [train_policy] epoch #701 | computing gradient
2021-06-04 13:57:11 | [train_policy] epoch #701 | gradient computed
2021-06-04 13:57:11 | [train_policy] epoch #701 | computing descent direction
2021-06-04 13:57:11 | [train_policy] epoch #701 | descent direction computed
2021-06-04 13:57:11 | [train_policy] epoch #701 | backtrack iters: 0
2021-06-04 13:57:11 | [train_policy] epoch #701 | optimization finished
2021-06-04 13:57:11 | [train_policy] epoch #701 | Computing KL after
2021-06-04 13:57:11 | [train_policy] epoch #701 | Computing loss after
2021-06-04 13:57:11 | [train_policy] epoch #701 | Fitting baseline...
2021-06-04 13:57:11 | [train_policy] epoch #701 | Saving snapshot...
2021-06-04 13:57:11 | [train_policy] epoch #701 | Saved
2021-06-04 13:57:11 | [train_policy] epoch #701 | Time 563.31 s
2021-06-04 13:57:11 | [train_policy] epoch #701 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285344
Evaluation/AverageDiscountedReturn          -41.49
Evaluation/AverageReturn                    -41.49
Evaluation/CompletionRate                     0
Evaluation/Iteration                        701
Evaluation/MaxReturn                        -30.5384
Evaluation/MinReturn                        -75.9631
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.77597
Extras/EpisodeRewardMean                    -41.7041
LinearFeatureBaseline/ExplainedVariance       0.832791
PolicyExecTime                                0.220232
ProcessExecTime                               0.0312765
TotalEnvSteps                            710424
policy/Entropy                               -1.74934
policy/KL                                     0.00986444
policy/KLBefore                               0
policy/LossAfter                             -0.0622752
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.173888
policy/dLoss                                  0.0622752
---------------------------------------  ----------------
2021-06-04 13:57:11 | [train_policy] epoch #702 | Obtaining samples for iteration 702...
2021-06-04 13:57:12 | [train_policy] epoch #702 | Logging diagnostics...
2021-06-04 13:57:12 | [train_policy] epoch #702 | Optimizing policy...
2021-06-04 13:57:12 | [train_policy] epoch #702 | Computing loss before
2021-06-04 13:57:12 | [train_policy] epoch #702 | Computing KL before
2021-06-04 13:57:12 | [train_policy] epoch #702 | Optimizing
2021-06-04 13:57:12 | [train_policy] epoch #702 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:12 | [train_policy] epoch #702 | computing loss before
2021-06-04 13:57:12 | [train_policy] epoch #702 | computing gradient
2021-06-04 13:57:12 | [train_policy] epoch #702 | gradient computed
2021-06-04 13:57:12 | [train_policy] epoch #702 | computing descent direction
2021-06-04 13:57:12 | [train_policy] epoch #702 | descent direction computed
2021-06-04 13:57:12 | [train_policy] epoch #702 | backtrack iters: 1
2021-06-04 13:57:12 | [train_policy] epoch #702 | optimization finished
2021-06-04 13:57:12 | [train_policy] epoch #702 | Computing KL after
2021-06-04 13:57:12 | [train_policy] epoch #702 | Computing loss after
2021-06-04 13:57:12 | [train_policy] epoch #702 | Fitting baseline...
2021-06-04 13:57:12 | [train_policy] epoch #702 | Saving snapshot...
2021-06-04 13:57:12 | [train_policy] epoch #702 | Saved
2021-06-04 13:57:12 | [train_policy] epoch #702 | Time 564.12 s
2021-06-04 13:57:12 | [train_policy] epoch #702 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284698
Evaluation/AverageDiscountedReturn          -41.7885
Evaluation/AverageReturn                    -41.7885
Evaluation/CompletionRate                     0
Evaluation/Iteration                        702
Evaluation/MaxReturn                        -28.5188
Evaluation/MinReturn                        -78.2639
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.48208
Extras/EpisodeRewardMean                    -41.7716
LinearFeatureBaseline/ExplainedVariance       0.888129
PolicyExecTime                                0.229219
ProcessExecTime                               0.0311956
TotalEnvSteps                            711436
policy/Entropy                               -1.75044
policy/KL                                     0.00641553
policy/KLBefore                               0
policy/LossAfter                             -0.0143711
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.173698
policy/dLoss                                  0.0143711
---------------------------------------  ----------------
2021-06-04 13:57:12 | [train_policy] epoch #703 | Obtaining samples for iteration 703...
2021-06-04 13:57:13 | [train_policy] epoch #703 | Logging diagnostics...
2021-06-04 13:57:13 | [train_policy] epoch #703 | Optimizing policy...
2021-06-04 13:57:13 | [train_policy] epoch #703 | Computing loss before
2021-06-04 13:57:13 | [train_policy] epoch #703 | Computing KL before
2021-06-04 13:57:13 | [train_policy] epoch #703 | Optimizing
2021-06-04 13:57:13 | [train_policy] epoch #703 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:13 | [train_policy] epoch #703 | computing loss before
2021-06-04 13:57:13 | [train_policy] epoch #703 | computing gradient
2021-06-04 13:57:13 | [train_policy] epoch #703 | gradient computed
2021-06-04 13:57:13 | [train_policy] epoch #703 | computing descent direction
2021-06-04 13:57:13 | [train_policy] epoch #703 | descent direction computed
2021-06-04 13:57:13 | [train_policy] epoch #703 | backtrack iters: 1
2021-06-04 13:57:13 | [train_policy] epoch #703 | optimization finished
2021-06-04 13:57:13 | [train_policy] epoch #703 | Computing KL after
2021-06-04 13:57:13 | [train_policy] epoch #703 | Computing loss after
2021-06-04 13:57:13 | [train_policy] epoch #703 | Fitting baseline...
2021-06-04 13:57:13 | [train_policy] epoch #703 | Saving snapshot...
2021-06-04 13:57:13 | [train_policy] epoch #703 | Saved
2021-06-04 13:57:13 | [train_policy] epoch #703 | Time 564.91 s
2021-06-04 13:57:13 | [train_policy] epoch #703 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285445
Evaluation/AverageDiscountedReturn          -41.7025
Evaluation/AverageReturn                    -41.7025
Evaluation/CompletionRate                     0
Evaluation/Iteration                        703
Evaluation/MaxReturn                        -32.103
Evaluation/MinReturn                        -63.9715
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5793
Extras/EpisodeRewardMean                    -41.9391
LinearFeatureBaseline/ExplainedVariance       0.910231
PolicyExecTime                                0.216356
ProcessExecTime                               0.0313079
TotalEnvSteps                            712448
policy/Entropy                               -1.77033
policy/KL                                     0.00647713
policy/KLBefore                               0
policy/LossAfter                             -0.0164042
policy/LossBefore                             1.74338e-08
policy/Perplexity                             0.170277
policy/dLoss                                  0.0164043
---------------------------------------  ----------------
2021-06-04 13:57:13 | [train_policy] epoch #704 | Obtaining samples for iteration 704...
2021-06-04 13:57:13 | [train_policy] epoch #704 | Logging diagnostics...
2021-06-04 13:57:13 | [train_policy] epoch #704 | Optimizing policy...
2021-06-04 13:57:13 | [train_policy] epoch #704 | Computing loss before
2021-06-04 13:57:13 | [train_policy] epoch #704 | Computing KL before
2021-06-04 13:57:13 | [train_policy] epoch #704 | Optimizing
2021-06-04 13:57:13 | [train_policy] epoch #704 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:13 | [train_policy] epoch #704 | computing loss before
2021-06-04 13:57:13 | [train_policy] epoch #704 | computing gradient
2021-06-04 13:57:13 | [train_policy] epoch #704 | gradient computed
2021-06-04 13:57:13 | [train_policy] epoch #704 | computing descent direction
2021-06-04 13:57:13 | [train_policy] epoch #704 | descent direction computed
2021-06-04 13:57:13 | [train_policy] epoch #704 | backtrack iters: 1
2021-06-04 13:57:13 | [train_policy] epoch #704 | optimization finished
2021-06-04 13:57:13 | [train_policy] epoch #704 | Computing KL after
2021-06-04 13:57:13 | [train_policy] epoch #704 | Computing loss after
2021-06-04 13:57:13 | [train_policy] epoch #704 | Fitting baseline...
2021-06-04 13:57:13 | [train_policy] epoch #704 | Saving snapshot...
2021-06-04 13:57:13 | [train_policy] epoch #704 | Saved
2021-06-04 13:57:13 | [train_policy] epoch #704 | Time 565.70 s
2021-06-04 13:57:13 | [train_policy] epoch #704 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284056
Evaluation/AverageDiscountedReturn          -41.228
Evaluation/AverageReturn                    -41.228
Evaluation/CompletionRate                     0
Evaluation/Iteration                        704
Evaluation/MaxReturn                        -28.7855
Evaluation/MinReturn                        -64.0382
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.65337
Extras/EpisodeRewardMean                    -41.3409
LinearFeatureBaseline/ExplainedVariance       0.894605
PolicyExecTime                                0.211978
ProcessExecTime                               0.0311172
TotalEnvSteps                            713460
policy/Entropy                               -1.78825
policy/KL                                     0.00665576
policy/KLBefore                               0
policy/LossAfter                             -0.0200341
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.167252
policy/dLoss                                  0.0200341
---------------------------------------  ----------------
2021-06-04 13:57:14 | [train_policy] epoch #705 | Obtaining samples for iteration 705...
2021-06-04 13:57:14 | [train_policy] epoch #705 | Logging diagnostics...
2021-06-04 13:57:14 | [train_policy] epoch #705 | Optimizing policy...
2021-06-04 13:57:14 | [train_policy] epoch #705 | Computing loss before
2021-06-04 13:57:14 | [train_policy] epoch #705 | Computing KL before
2021-06-04 13:57:14 | [train_policy] epoch #705 | Optimizing
2021-06-04 13:57:14 | [train_policy] epoch #705 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:14 | [train_policy] epoch #705 | computing loss before
2021-06-04 13:57:14 | [train_policy] epoch #705 | computing gradient
2021-06-04 13:57:14 | [train_policy] epoch #705 | gradient computed
2021-06-04 13:57:14 | [train_policy] epoch #705 | computing descent direction
2021-06-04 13:57:14 | [train_policy] epoch #705 | descent direction computed
2021-06-04 13:57:14 | [train_policy] epoch #705 | backtrack iters: 0
2021-06-04 13:57:14 | [train_policy] epoch #705 | optimization finished
2021-06-04 13:57:14 | [train_policy] epoch #705 | Computing KL after
2021-06-04 13:57:14 | [train_policy] epoch #705 | Computing loss after
2021-06-04 13:57:14 | [train_policy] epoch #705 | Fitting baseline...
2021-06-04 13:57:14 | [train_policy] epoch #705 | Saving snapshot...
2021-06-04 13:57:14 | [train_policy] epoch #705 | Saved
2021-06-04 13:57:14 | [train_policy] epoch #705 | Time 566.49 s
2021-06-04 13:57:14 | [train_policy] epoch #705 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285734
Evaluation/AverageDiscountedReturn          -42.0477
Evaluation/AverageReturn                    -42.0477
Evaluation/CompletionRate                     0
Evaluation/Iteration                        705
Evaluation/MaxReturn                        -27.5668
Evaluation/MinReturn                        -76.6607
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.296
Extras/EpisodeRewardMean                    -41.9092
LinearFeatureBaseline/ExplainedVariance       0.876378
PolicyExecTime                                0.217033
ProcessExecTime                               0.0313332
TotalEnvSteps                            714472
policy/Entropy                               -1.78621
policy/KL                                     0.00952882
policy/KLBefore                               0
policy/LossAfter                             -0.0203669
policy/LossBefore                            -1.01304e-08
policy/Perplexity                             0.167594
policy/dLoss                                  0.0203669
---------------------------------------  ----------------
2021-06-04 13:57:14 | [train_policy] epoch #706 | Obtaining samples for iteration 706...
2021-06-04 13:57:15 | [train_policy] epoch #706 | Logging diagnostics...
2021-06-04 13:57:15 | [train_policy] epoch #706 | Optimizing policy...
2021-06-04 13:57:15 | [train_policy] epoch #706 | Computing loss before
2021-06-04 13:57:15 | [train_policy] epoch #706 | Computing KL before
2021-06-04 13:57:15 | [train_policy] epoch #706 | Optimizing
2021-06-04 13:57:15 | [train_policy] epoch #706 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:15 | [train_policy] epoch #706 | computing loss before
2021-06-04 13:57:15 | [train_policy] epoch #706 | computing gradient
2021-06-04 13:57:15 | [train_policy] epoch #706 | gradient computed
2021-06-04 13:57:15 | [train_policy] epoch #706 | computing descent direction
2021-06-04 13:57:15 | [train_policy] epoch #706 | descent direction computed
2021-06-04 13:57:15 | [train_policy] epoch #706 | backtrack iters: 1
2021-06-04 13:57:15 | [train_policy] epoch #706 | optimization finished
2021-06-04 13:57:15 | [train_policy] epoch #706 | Computing KL after
2021-06-04 13:57:15 | [train_policy] epoch #706 | Computing loss after
2021-06-04 13:57:15 | [train_policy] epoch #706 | Fitting baseline...
2021-06-04 13:57:15 | [train_policy] epoch #706 | Saving snapshot...
2021-06-04 13:57:15 | [train_policy] epoch #706 | Saved
2021-06-04 13:57:15 | [train_policy] epoch #706 | Time 567.29 s
2021-06-04 13:57:15 | [train_policy] epoch #706 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284436
Evaluation/AverageDiscountedReturn          -40.5037
Evaluation/AverageReturn                    -40.5037
Evaluation/CompletionRate                     0
Evaluation/Iteration                        706
Evaluation/MaxReturn                        -28.3292
Evaluation/MinReturn                        -60.2693
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.98836
Extras/EpisodeRewardMean                    -40.4463
LinearFeatureBaseline/ExplainedVariance       0.888966
PolicyExecTime                                0.230834
ProcessExecTime                               0.0311747
TotalEnvSteps                            715484
policy/Entropy                               -1.77796
policy/KL                                     0.00642368
policy/KLBefore                               0
policy/LossAfter                             -0.0168952
policy/LossBefore                            -6.36097e-09
policy/Perplexity                             0.168982
policy/dLoss                                  0.0168952
---------------------------------------  ----------------
2021-06-04 13:57:15 | [train_policy] epoch #707 | Obtaining samples for iteration 707...
2021-06-04 13:57:16 | [train_policy] epoch #707 | Logging diagnostics...
2021-06-04 13:57:16 | [train_policy] epoch #707 | Optimizing policy...
2021-06-04 13:57:16 | [train_policy] epoch #707 | Computing loss before
2021-06-04 13:57:16 | [train_policy] epoch #707 | Computing KL before
2021-06-04 13:57:16 | [train_policy] epoch #707 | Optimizing
2021-06-04 13:57:16 | [train_policy] epoch #707 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:16 | [train_policy] epoch #707 | computing loss before
2021-06-04 13:57:16 | [train_policy] epoch #707 | computing gradient
2021-06-04 13:57:16 | [train_policy] epoch #707 | gradient computed
2021-06-04 13:57:16 | [train_policy] epoch #707 | computing descent direction
2021-06-04 13:57:16 | [train_policy] epoch #707 | descent direction computed
2021-06-04 13:57:16 | [train_policy] epoch #707 | backtrack iters: 0
2021-06-04 13:57:16 | [train_policy] epoch #707 | optimization finished
2021-06-04 13:57:16 | [train_policy] epoch #707 | Computing KL after
2021-06-04 13:57:16 | [train_policy] epoch #707 | Computing loss after
2021-06-04 13:57:16 | [train_policy] epoch #707 | Fitting baseline...
2021-06-04 13:57:16 | [train_policy] epoch #707 | Saving snapshot...
2021-06-04 13:57:16 | [train_policy] epoch #707 | Saved
2021-06-04 13:57:16 | [train_policy] epoch #707 | Time 568.10 s
2021-06-04 13:57:16 | [train_policy] epoch #707 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284773
Evaluation/AverageDiscountedReturn          -40.8722
Evaluation/AverageReturn                    -40.8722
Evaluation/CompletionRate                     0
Evaluation/Iteration                        707
Evaluation/MaxReturn                        -29.6037
Evaluation/MinReturn                        -77.2414
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.8811
Extras/EpisodeRewardMean                    -40.9406
LinearFeatureBaseline/ExplainedVariance       0.887346
PolicyExecTime                                0.227895
ProcessExecTime                               0.0315378
TotalEnvSteps                            716496
policy/Entropy                               -1.81418
policy/KL                                     0.00996001
policy/KLBefore                               0
policy/LossAfter                             -0.0179223
policy/LossBefore                            -1.99075e-08
policy/Perplexity                             0.162971
policy/dLoss                                  0.0179223
---------------------------------------  ----------------
2021-06-04 13:57:16 | [train_policy] epoch #708 | Obtaining samples for iteration 708...
2021-06-04 13:57:17 | [train_policy] epoch #708 | Logging diagnostics...
2021-06-04 13:57:17 | [train_policy] epoch #708 | Optimizing policy...
2021-06-04 13:57:17 | [train_policy] epoch #708 | Computing loss before
2021-06-04 13:57:17 | [train_policy] epoch #708 | Computing KL before
2021-06-04 13:57:17 | [train_policy] epoch #708 | Optimizing
2021-06-04 13:57:17 | [train_policy] epoch #708 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:17 | [train_policy] epoch #708 | computing loss before
2021-06-04 13:57:17 | [train_policy] epoch #708 | computing gradient
2021-06-04 13:57:17 | [train_policy] epoch #708 | gradient computed
2021-06-04 13:57:17 | [train_policy] epoch #708 | computing descent direction
2021-06-04 13:57:17 | [train_policy] epoch #708 | descent direction computed
2021-06-04 13:57:17 | [train_policy] epoch #708 | backtrack iters: 1
2021-06-04 13:57:17 | [train_policy] epoch #708 | optimization finished
2021-06-04 13:57:17 | [train_policy] epoch #708 | Computing KL after
2021-06-04 13:57:17 | [train_policy] epoch #708 | Computing loss after
2021-06-04 13:57:17 | [train_policy] epoch #708 | Fitting baseline...
2021-06-04 13:57:17 | [train_policy] epoch #708 | Saving snapshot...
2021-06-04 13:57:17 | [train_policy] epoch #708 | Saved
2021-06-04 13:57:17 | [train_policy] epoch #708 | Time 568.89 s
2021-06-04 13:57:17 | [train_policy] epoch #708 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285372
Evaluation/AverageDiscountedReturn          -41.9163
Evaluation/AverageReturn                    -41.9163
Evaluation/CompletionRate                     0
Evaluation/Iteration                        708
Evaluation/MaxReturn                        -28.3248
Evaluation/MinReturn                        -86.9408
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.65947
Extras/EpisodeRewardMean                    -41.9277
LinearFeatureBaseline/ExplainedVariance       0.859058
PolicyExecTime                                0.22316
ProcessExecTime                               0.0312324
TotalEnvSteps                            717508
policy/Entropy                               -1.80875
policy/KL                                     0.00642726
policy/KLBefore                               0
policy/LossAfter                             -0.0215928
policy/LossBefore                             1.17796e-09
policy/Perplexity                             0.163859
policy/dLoss                                  0.0215928
---------------------------------------  ----------------
2021-06-04 13:57:17 | [train_policy] epoch #709 | Obtaining samples for iteration 709...
2021-06-04 13:57:17 | [train_policy] epoch #709 | Logging diagnostics...
2021-06-04 13:57:17 | [train_policy] epoch #709 | Optimizing policy...
2021-06-04 13:57:17 | [train_policy] epoch #709 | Computing loss before
2021-06-04 13:57:17 | [train_policy] epoch #709 | Computing KL before
2021-06-04 13:57:17 | [train_policy] epoch #709 | Optimizing
2021-06-04 13:57:17 | [train_policy] epoch #709 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:17 | [train_policy] epoch #709 | computing loss before
2021-06-04 13:57:17 | [train_policy] epoch #709 | computing gradient
2021-06-04 13:57:17 | [train_policy] epoch #709 | gradient computed
2021-06-04 13:57:17 | [train_policy] epoch #709 | computing descent direction
2021-06-04 13:57:17 | [train_policy] epoch #709 | descent direction computed
2021-06-04 13:57:17 | [train_policy] epoch #709 | backtrack iters: 1
2021-06-04 13:57:17 | [train_policy] epoch #709 | optimization finished
2021-06-04 13:57:17 | [train_policy] epoch #709 | Computing KL after
2021-06-04 13:57:17 | [train_policy] epoch #709 | Computing loss after
2021-06-04 13:57:17 | [train_policy] epoch #709 | Fitting baseline...
2021-06-04 13:57:17 | [train_policy] epoch #709 | Saving snapshot...
2021-06-04 13:57:17 | [train_policy] epoch #709 | Saved
2021-06-04 13:57:17 | [train_policy] epoch #709 | Time 569.72 s
2021-06-04 13:57:17 | [train_policy] epoch #709 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                   0.285387
Evaluation/AverageDiscountedReturn          -42.2096
Evaluation/AverageReturn                    -42.2096
Evaluation/CompletionRate                     0
Evaluation/Iteration                        709
Evaluation/MaxReturn                        -27.4891
Evaluation/MinReturn                        -78.4747
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2268
Extras/EpisodeRewardMean                    -41.9879
LinearFeatureBaseline/ExplainedVariance       0.831633
PolicyExecTime                                0.234363
ProcessExecTime                               0.0312693
TotalEnvSteps                            718520
policy/Entropy                               -1.78982
policy/KL                                     0.00647687
policy/KLBefore                               0
policy/LossAfter                             -0.0143361
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.16699
policy/dLoss                                  0.0143361
---------------------------------------  ---------------
2021-06-04 13:57:18 | [train_policy] epoch #710 | Obtaining samples for iteration 710...
2021-06-04 13:57:18 | [train_policy] epoch #710 | Logging diagnostics...
2021-06-04 13:57:18 | [train_policy] epoch #710 | Optimizing policy...
2021-06-04 13:57:18 | [train_policy] epoch #710 | Computing loss before
2021-06-04 13:57:18 | [train_policy] epoch #710 | Computing KL before
2021-06-04 13:57:18 | [train_policy] epoch #710 | Optimizing
2021-06-04 13:57:18 | [train_policy] epoch #710 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:18 | [train_policy] epoch #710 | computing loss before
2021-06-04 13:57:18 | [train_policy] epoch #710 | computing gradient
2021-06-04 13:57:18 | [train_policy] epoch #710 | gradient computed
2021-06-04 13:57:18 | [train_policy] epoch #710 | computing descent direction
2021-06-04 13:57:18 | [train_policy] epoch #710 | descent direction computed
2021-06-04 13:57:18 | [train_policy] epoch #710 | backtrack iters: 0
2021-06-04 13:57:18 | [train_policy] epoch #710 | optimization finished
2021-06-04 13:57:18 | [train_policy] epoch #710 | Computing KL after
2021-06-04 13:57:18 | [train_policy] epoch #710 | Computing loss after
2021-06-04 13:57:18 | [train_policy] epoch #710 | Fitting baseline...
2021-06-04 13:57:18 | [train_policy] epoch #710 | Saving snapshot...
2021-06-04 13:57:18 | [train_policy] epoch #710 | Saved
2021-06-04 13:57:18 | [train_policy] epoch #710 | Time 570.50 s
2021-06-04 13:57:18 | [train_policy] epoch #710 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.283958
Evaluation/AverageDiscountedReturn          -42.5376
Evaluation/AverageReturn                    -42.5376
Evaluation/CompletionRate                     0
Evaluation/Iteration                        710
Evaluation/MaxReturn                        -27.9794
Evaluation/MinReturn                        -76.4523
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.81745
Extras/EpisodeRewardMean                    -42.3722
LinearFeatureBaseline/ExplainedVariance       0.869034
PolicyExecTime                                0.225976
ProcessExecTime                               0.0312338
TotalEnvSteps                            719532
policy/Entropy                               -1.77337
policy/KL                                     0.00985874
policy/KLBefore                               0
policy/LossAfter                             -0.0164538
policy/LossBefore                            -8.01011e-09
policy/Perplexity                             0.16976
policy/dLoss                                  0.0164538
---------------------------------------  ----------------
2021-06-04 13:57:18 | [train_policy] epoch #711 | Obtaining samples for iteration 711...
2021-06-04 13:57:19 | [train_policy] epoch #711 | Logging diagnostics...
2021-06-04 13:57:19 | [train_policy] epoch #711 | Optimizing policy...
2021-06-04 13:57:19 | [train_policy] epoch #711 | Computing loss before
2021-06-04 13:57:19 | [train_policy] epoch #711 | Computing KL before
2021-06-04 13:57:19 | [train_policy] epoch #711 | Optimizing
2021-06-04 13:57:19 | [train_policy] epoch #711 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:19 | [train_policy] epoch #711 | computing loss before
2021-06-04 13:57:19 | [train_policy] epoch #711 | computing gradient
2021-06-04 13:57:19 | [train_policy] epoch #711 | gradient computed
2021-06-04 13:57:19 | [train_policy] epoch #711 | computing descent direction
2021-06-04 13:57:19 | [train_policy] epoch #711 | descent direction computed
2021-06-04 13:57:19 | [train_policy] epoch #711 | backtrack iters: 1
2021-06-04 13:57:19 | [train_policy] epoch #711 | optimization finished
2021-06-04 13:57:19 | [train_policy] epoch #711 | Computing KL after
2021-06-04 13:57:19 | [train_policy] epoch #711 | Computing loss after
2021-06-04 13:57:19 | [train_policy] epoch #711 | Fitting baseline...
2021-06-04 13:57:19 | [train_policy] epoch #711 | Saving snapshot...
2021-06-04 13:57:19 | [train_policy] epoch #711 | Saved
2021-06-04 13:57:19 | [train_policy] epoch #711 | Time 571.29 s
2021-06-04 13:57:19 | [train_policy] epoch #711 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284648
Evaluation/AverageDiscountedReturn          -41.2244
Evaluation/AverageReturn                    -41.2244
Evaluation/CompletionRate                     0
Evaluation/Iteration                        711
Evaluation/MaxReturn                        -28.7725
Evaluation/MinReturn                        -76.447
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.27652
Extras/EpisodeRewardMean                    -41.0711
LinearFeatureBaseline/ExplainedVariance       0.874747
PolicyExecTime                                0.213144
ProcessExecTime                               0.0311532
TotalEnvSteps                            720544
policy/Entropy                               -1.7786
policy/KL                                     0.00655613
policy/KLBefore                               0
policy/LossAfter                             -0.0225313
policy/LossBefore                            -1.69626e-08
policy/Perplexity                             0.168875
policy/dLoss                                  0.0225313
---------------------------------------  ----------------
2021-06-04 13:57:19 | [train_policy] epoch #712 | Obtaining samples for iteration 712...
2021-06-04 13:57:20 | [train_policy] epoch #712 | Logging diagnostics...
2021-06-04 13:57:20 | [train_policy] epoch #712 | Optimizing policy...
2021-06-04 13:57:20 | [train_policy] epoch #712 | Computing loss before
2021-06-04 13:57:20 | [train_policy] epoch #712 | Computing KL before
2021-06-04 13:57:20 | [train_policy] epoch #712 | Optimizing
2021-06-04 13:57:20 | [train_policy] epoch #712 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:20 | [train_policy] epoch #712 | computing loss before
2021-06-04 13:57:20 | [train_policy] epoch #712 | computing gradient
2021-06-04 13:57:20 | [train_policy] epoch #712 | gradient computed
2021-06-04 13:57:20 | [train_policy] epoch #712 | computing descent direction
2021-06-04 13:57:20 | [train_policy] epoch #712 | descent direction computed
2021-06-04 13:57:20 | [train_policy] epoch #712 | backtrack iters: 1
2021-06-04 13:57:20 | [train_policy] epoch #712 | optimization finished
2021-06-04 13:57:20 | [train_policy] epoch #712 | Computing KL after
2021-06-04 13:57:20 | [train_policy] epoch #712 | Computing loss after
2021-06-04 13:57:20 | [train_policy] epoch #712 | Fitting baseline...
2021-06-04 13:57:20 | [train_policy] epoch #712 | Saving snapshot...
2021-06-04 13:57:20 | [train_policy] epoch #712 | Saved
2021-06-04 13:57:20 | [train_policy] epoch #712 | Time 572.10 s
2021-06-04 13:57:20 | [train_policy] epoch #712 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.28701
Evaluation/AverageDiscountedReturn          -41.676
Evaluation/AverageReturn                    -41.676
Evaluation/CompletionRate                     0
Evaluation/Iteration                        712
Evaluation/MaxReturn                        -29.6779
Evaluation/MinReturn                        -76.4287
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.81685
Extras/EpisodeRewardMean                    -41.8792
LinearFeatureBaseline/ExplainedVariance       0.864546
PolicyExecTime                                0.234236
ProcessExecTime                               0.0315659
TotalEnvSteps                            721556
policy/Entropy                               -1.78695
policy/KL                                     0.00652738
policy/KLBefore                               0
policy/LossAfter                             -0.019634
policy/LossBefore                             3.53387e-10
policy/Perplexity                             0.16747
policy/dLoss                                  0.019634
---------------------------------------  ----------------
2021-06-04 13:57:20 | [train_policy] epoch #713 | Obtaining samples for iteration 713...
2021-06-04 13:57:21 | [train_policy] epoch #713 | Logging diagnostics...
2021-06-04 13:57:21 | [train_policy] epoch #713 | Optimizing policy...
2021-06-04 13:57:21 | [train_policy] epoch #713 | Computing loss before
2021-06-04 13:57:21 | [train_policy] epoch #713 | Computing KL before
2021-06-04 13:57:21 | [train_policy] epoch #713 | Optimizing
2021-06-04 13:57:21 | [train_policy] epoch #713 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:21 | [train_policy] epoch #713 | computing loss before
2021-06-04 13:57:21 | [train_policy] epoch #713 | computing gradient
2021-06-04 13:57:21 | [train_policy] epoch #713 | gradient computed
2021-06-04 13:57:21 | [train_policy] epoch #713 | computing descent direction
2021-06-04 13:57:21 | [train_policy] epoch #713 | descent direction computed
2021-06-04 13:57:21 | [train_policy] epoch #713 | backtrack iters: 0
2021-06-04 13:57:21 | [train_policy] epoch #713 | optimization finished
2021-06-04 13:57:21 | [train_policy] epoch #713 | Computing KL after
2021-06-04 13:57:21 | [train_policy] epoch #713 | Computing loss after
2021-06-04 13:57:21 | [train_policy] epoch #713 | Fitting baseline...
2021-06-04 13:57:21 | [train_policy] epoch #713 | Saving snapshot...
2021-06-04 13:57:21 | [train_policy] epoch #713 | Saved
2021-06-04 13:57:21 | [train_policy] epoch #713 | Time 572.89 s
2021-06-04 13:57:21 | [train_policy] epoch #713 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284303
Evaluation/AverageDiscountedReturn          -41.8154
Evaluation/AverageReturn                    -41.8154
Evaluation/CompletionRate                     0
Evaluation/Iteration                        713
Evaluation/MaxReturn                        -27.9566
Evaluation/MinReturn                        -78.2182
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.91433
Extras/EpisodeRewardMean                    -41.7608
LinearFeatureBaseline/ExplainedVariance       0.877257
PolicyExecTime                                0.226209
ProcessExecTime                               0.0313444
TotalEnvSteps                            722568
policy/Entropy                               -1.78022
policy/KL                                     0.00993357
policy/KLBefore                               0
policy/LossAfter                             -0.0262065
policy/LossBefore                            -1.10728e-08
policy/Perplexity                             0.168601
policy/dLoss                                  0.0262065
---------------------------------------  ----------------
2021-06-04 13:57:21 | [train_policy] epoch #714 | Obtaining samples for iteration 714...
2021-06-04 13:57:21 | [train_policy] epoch #714 | Logging diagnostics...
2021-06-04 13:57:21 | [train_policy] epoch #714 | Optimizing policy...
2021-06-04 13:57:21 | [train_policy] epoch #714 | Computing loss before
2021-06-04 13:57:21 | [train_policy] epoch #714 | Computing KL before
2021-06-04 13:57:21 | [train_policy] epoch #714 | Optimizing
2021-06-04 13:57:21 | [train_policy] epoch #714 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:21 | [train_policy] epoch #714 | computing loss before
2021-06-04 13:57:21 | [train_policy] epoch #714 | computing gradient
2021-06-04 13:57:21 | [train_policy] epoch #714 | gradient computed
2021-06-04 13:57:21 | [train_policy] epoch #714 | computing descent direction
2021-06-04 13:57:21 | [train_policy] epoch #714 | descent direction computed
2021-06-04 13:57:21 | [train_policy] epoch #714 | backtrack iters: 0
2021-06-04 13:57:21 | [train_policy] epoch #714 | optimization finished
2021-06-04 13:57:21 | [train_policy] epoch #714 | Computing KL after
2021-06-04 13:57:21 | [train_policy] epoch #714 | Computing loss after
2021-06-04 13:57:21 | [train_policy] epoch #714 | Fitting baseline...
2021-06-04 13:57:21 | [train_policy] epoch #714 | Saving snapshot...
2021-06-04 13:57:21 | [train_policy] epoch #714 | Saved
2021-06-04 13:57:21 | [train_policy] epoch #714 | Time 573.68 s
2021-06-04 13:57:21 | [train_policy] epoch #714 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284938
Evaluation/AverageDiscountedReturn          -42.1036
Evaluation/AverageReturn                    -42.1036
Evaluation/CompletionRate                     0
Evaluation/Iteration                        714
Evaluation/MaxReturn                        -27.6895
Evaluation/MinReturn                        -77.9208
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1478
Extras/EpisodeRewardMean                    -41.7953
LinearFeatureBaseline/ExplainedVariance       0.845857
PolicyExecTime                                0.219215
ProcessExecTime                               0.0311782
TotalEnvSteps                            723580
policy/Entropy                               -1.77345
policy/KL                                     0.00974258
policy/KLBefore                               0
policy/LossAfter                             -0.015629
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.169746
policy/dLoss                                  0.015629
---------------------------------------  ----------------
2021-06-04 13:57:21 | [train_policy] epoch #715 | Obtaining samples for iteration 715...
2021-06-04 13:57:22 | [train_policy] epoch #715 | Logging diagnostics...
2021-06-04 13:57:22 | [train_policy] epoch #715 | Optimizing policy...
2021-06-04 13:57:22 | [train_policy] epoch #715 | Computing loss before
2021-06-04 13:57:22 | [train_policy] epoch #715 | Computing KL before
2021-06-04 13:57:22 | [train_policy] epoch #715 | Optimizing
2021-06-04 13:57:22 | [train_policy] epoch #715 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:22 | [train_policy] epoch #715 | computing loss before
2021-06-04 13:57:22 | [train_policy] epoch #715 | computing gradient
2021-06-04 13:57:22 | [train_policy] epoch #715 | gradient computed
2021-06-04 13:57:22 | [train_policy] epoch #715 | computing descent direction
2021-06-04 13:57:22 | [train_policy] epoch #715 | descent direction computed
2021-06-04 13:57:22 | [train_policy] epoch #715 | backtrack iters: 0
2021-06-04 13:57:22 | [train_policy] epoch #715 | optimization finished
2021-06-04 13:57:22 | [train_policy] epoch #715 | Computing KL after
2021-06-04 13:57:22 | [train_policy] epoch #715 | Computing loss after
2021-06-04 13:57:22 | [train_policy] epoch #715 | Fitting baseline...
2021-06-04 13:57:22 | [train_policy] epoch #715 | Saving snapshot...
2021-06-04 13:57:22 | [train_policy] epoch #715 | Saved
2021-06-04 13:57:22 | [train_policy] epoch #715 | Time 574.48 s
2021-06-04 13:57:22 | [train_policy] epoch #715 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284088
Evaluation/AverageDiscountedReturn          -42.4814
Evaluation/AverageReturn                    -42.4814
Evaluation/CompletionRate                     0
Evaluation/Iteration                        715
Evaluation/MaxReturn                        -29.937
Evaluation/MinReturn                        -64.2641
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.70564
Extras/EpisodeRewardMean                    -42.1992
LinearFeatureBaseline/ExplainedVariance       0.902016
PolicyExecTime                                0.233014
ProcessExecTime                               0.0311689
TotalEnvSteps                            724592
policy/Entropy                               -1.75859
policy/KL                                     0.00984177
policy/KLBefore                               0
policy/LossAfter                             -0.0198213
policy/LossBefore                             2.77998e-08
policy/Perplexity                             0.172288
policy/dLoss                                  0.0198213
---------------------------------------  ----------------
2021-06-04 13:57:22 | [train_policy] epoch #716 | Obtaining samples for iteration 716...
2021-06-04 13:57:23 | [train_policy] epoch #716 | Logging diagnostics...
2021-06-04 13:57:23 | [train_policy] epoch #716 | Optimizing policy...
2021-06-04 13:57:23 | [train_policy] epoch #716 | Computing loss before
2021-06-04 13:57:23 | [train_policy] epoch #716 | Computing KL before
2021-06-04 13:57:23 | [train_policy] epoch #716 | Optimizing
2021-06-04 13:57:23 | [train_policy] epoch #716 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:23 | [train_policy] epoch #716 | computing loss before
2021-06-04 13:57:23 | [train_policy] epoch #716 | computing gradient
2021-06-04 13:57:23 | [train_policy] epoch #716 | gradient computed
2021-06-04 13:57:23 | [train_policy] epoch #716 | computing descent direction
2021-06-04 13:57:23 | [train_policy] epoch #716 | descent direction computed
2021-06-04 13:57:23 | [train_policy] epoch #716 | backtrack iters: 1
2021-06-04 13:57:23 | [train_policy] epoch #716 | optimization finished
2021-06-04 13:57:23 | [train_policy] epoch #716 | Computing KL after
2021-06-04 13:57:23 | [train_policy] epoch #716 | Computing loss after
2021-06-04 13:57:23 | [train_policy] epoch #716 | Fitting baseline...
2021-06-04 13:57:23 | [train_policy] epoch #716 | Saving snapshot...
2021-06-04 13:57:23 | [train_policy] epoch #716 | Saved
2021-06-04 13:57:23 | [train_policy] epoch #716 | Time 575.29 s
2021-06-04 13:57:23 | [train_policy] epoch #716 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286144
Evaluation/AverageDiscountedReturn          -41.2807
Evaluation/AverageReturn                    -41.2807
Evaluation/CompletionRate                     0
Evaluation/Iteration                        716
Evaluation/MaxReturn                        -29.3398
Evaluation/MinReturn                        -76.5188
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.63544
Extras/EpisodeRewardMean                    -41.2489
LinearFeatureBaseline/ExplainedVariance       0.888121
PolicyExecTime                                0.222942
ProcessExecTime                               0.0313716
TotalEnvSteps                            725604
policy/Entropy                               -1.77606
policy/KL                                     0.00653209
policy/KLBefore                               0
policy/LossAfter                             -0.014609
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             0.169304
policy/dLoss                                  0.014609
---------------------------------------  ----------------
2021-06-04 13:57:23 | [train_policy] epoch #717 | Obtaining samples for iteration 717...
2021-06-04 13:57:24 | [train_policy] epoch #717 | Logging diagnostics...
2021-06-04 13:57:24 | [train_policy] epoch #717 | Optimizing policy...
2021-06-04 13:57:24 | [train_policy] epoch #717 | Computing loss before
2021-06-04 13:57:24 | [train_policy] epoch #717 | Computing KL before
2021-06-04 13:57:24 | [train_policy] epoch #717 | Optimizing
2021-06-04 13:57:24 | [train_policy] epoch #717 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:24 | [train_policy] epoch #717 | computing loss before
2021-06-04 13:57:24 | [train_policy] epoch #717 | computing gradient
2021-06-04 13:57:24 | [train_policy] epoch #717 | gradient computed
2021-06-04 13:57:24 | [train_policy] epoch #717 | computing descent direction
2021-06-04 13:57:24 | [train_policy] epoch #717 | descent direction computed
2021-06-04 13:57:24 | [train_policy] epoch #717 | backtrack iters: 1
2021-06-04 13:57:24 | [train_policy] epoch #717 | optimization finished
2021-06-04 13:57:24 | [train_policy] epoch #717 | Computing KL after
2021-06-04 13:57:24 | [train_policy] epoch #717 | Computing loss after
2021-06-04 13:57:24 | [train_policy] epoch #717 | Fitting baseline...
2021-06-04 13:57:24 | [train_policy] epoch #717 | Saving snapshot...
2021-06-04 13:57:24 | [train_policy] epoch #717 | Saved
2021-06-04 13:57:24 | [train_policy] epoch #717 | Time 576.10 s
2021-06-04 13:57:24 | [train_policy] epoch #717 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.286375
Evaluation/AverageDiscountedReturn          -40.7444
Evaluation/AverageReturn                    -40.7444
Evaluation/CompletionRate                     0
Evaluation/Iteration                        717
Evaluation/MaxReturn                        -29.064
Evaluation/MinReturn                        -64.1145
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.16839
Extras/EpisodeRewardMean                    -40.9263
LinearFeatureBaseline/ExplainedVariance       0.893804
PolicyExecTime                                0.225506
ProcessExecTime                               0.0314481
TotalEnvSteps                            726616
policy/Entropy                               -1.78957
policy/KL                                     0.00650061
policy/KLBefore                               0
policy/LossAfter                             -0.0143555
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.167031
policy/dLoss                                  0.0143555
---------------------------------------  ---------------
2021-06-04 13:57:24 | [train_policy] epoch #718 | Obtaining samples for iteration 718...
2021-06-04 13:57:25 | [train_policy] epoch #718 | Logging diagnostics...
2021-06-04 13:57:25 | [train_policy] epoch #718 | Optimizing policy...
2021-06-04 13:57:25 | [train_policy] epoch #718 | Computing loss before
2021-06-04 13:57:25 | [train_policy] epoch #718 | Computing KL before
2021-06-04 13:57:25 | [train_policy] epoch #718 | Optimizing
2021-06-04 13:57:25 | [train_policy] epoch #718 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:25 | [train_policy] epoch #718 | computing loss before
2021-06-04 13:57:25 | [train_policy] epoch #718 | computing gradient
2021-06-04 13:57:25 | [train_policy] epoch #718 | gradient computed
2021-06-04 13:57:25 | [train_policy] epoch #718 | computing descent direction
2021-06-04 13:57:25 | [train_policy] epoch #718 | descent direction computed
2021-06-04 13:57:25 | [train_policy] epoch #718 | backtrack iters: 1
2021-06-04 13:57:25 | [train_policy] epoch #718 | optimization finished
2021-06-04 13:57:25 | [train_policy] epoch #718 | Computing KL after
2021-06-04 13:57:25 | [train_policy] epoch #718 | Computing loss after
2021-06-04 13:57:25 | [train_policy] epoch #718 | Fitting baseline...
2021-06-04 13:57:25 | [train_policy] epoch #718 | Saving snapshot...
2021-06-04 13:57:25 | [train_policy] epoch #718 | Saved
2021-06-04 13:57:25 | [train_policy] epoch #718 | Time 576.89 s
2021-06-04 13:57:25 | [train_policy] epoch #718 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284354
Evaluation/AverageDiscountedReturn          -42.1707
Evaluation/AverageReturn                    -42.1707
Evaluation/CompletionRate                     0
Evaluation/Iteration                        718
Evaluation/MaxReturn                        -27.8635
Evaluation/MinReturn                        -78.2314
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.80039
Extras/EpisodeRewardMean                    -42.1603
LinearFeatureBaseline/ExplainedVariance       0.872636
PolicyExecTime                                0.223802
ProcessExecTime                               0.0311658
TotalEnvSteps                            727628
policy/Entropy                               -1.79761
policy/KL                                     0.00658933
policy/KLBefore                               0
policy/LossAfter                             -0.017887
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.165695
policy/dLoss                                  0.017887
---------------------------------------  ----------------
2021-06-04 13:57:25 | [train_policy] epoch #719 | Obtaining samples for iteration 719...
2021-06-04 13:57:25 | [train_policy] epoch #719 | Logging diagnostics...
2021-06-04 13:57:25 | [train_policy] epoch #719 | Optimizing policy...
2021-06-04 13:57:25 | [train_policy] epoch #719 | Computing loss before
2021-06-04 13:57:25 | [train_policy] epoch #719 | Computing KL before
2021-06-04 13:57:25 | [train_policy] epoch #719 | Optimizing
2021-06-04 13:57:25 | [train_policy] epoch #719 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:25 | [train_policy] epoch #719 | computing loss before
2021-06-04 13:57:25 | [train_policy] epoch #719 | computing gradient
2021-06-04 13:57:25 | [train_policy] epoch #719 | gradient computed
2021-06-04 13:57:25 | [train_policy] epoch #719 | computing descent direction
2021-06-04 13:57:25 | [train_policy] epoch #719 | descent direction computed
2021-06-04 13:57:25 | [train_policy] epoch #719 | backtrack iters: 0
2021-06-04 13:57:25 | [train_policy] epoch #719 | optimization finished
2021-06-04 13:57:25 | [train_policy] epoch #719 | Computing KL after
2021-06-04 13:57:25 | [train_policy] epoch #719 | Computing loss after
2021-06-04 13:57:25 | [train_policy] epoch #719 | Fitting baseline...
2021-06-04 13:57:25 | [train_policy] epoch #719 | Saving snapshot...
2021-06-04 13:57:25 | [train_policy] epoch #719 | Saved
2021-06-04 13:57:25 | [train_policy] epoch #719 | Time 577.69 s
2021-06-04 13:57:25 | [train_policy] epoch #719 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28413
Evaluation/AverageDiscountedReturn          -41.9205
Evaluation/AverageReturn                    -41.9205
Evaluation/CompletionRate                     0
Evaluation/Iteration                        719
Evaluation/MaxReturn                        -31.8916
Evaluation/MinReturn                        -77.5798
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.72163
Extras/EpisodeRewardMean                    -41.8107
LinearFeatureBaseline/ExplainedVariance       0.866699
PolicyExecTime                                0.227145
ProcessExecTime                               0.0312116
TotalEnvSteps                            728640
policy/Entropy                               -1.80769
policy/KL                                     0.00998491
policy/KLBefore                               0
policy/LossAfter                             -0.0200077
policy/LossBefore                            -2.07321e-08
policy/Perplexity                             0.164032
policy/dLoss                                  0.0200077
---------------------------------------  ----------------
2021-06-04 13:57:25 | [train_policy] epoch #720 | Obtaining samples for iteration 720...
2021-06-04 13:57:26 | [train_policy] epoch #720 | Logging diagnostics...
2021-06-04 13:57:26 | [train_policy] epoch #720 | Optimizing policy...
2021-06-04 13:57:26 | [train_policy] epoch #720 | Computing loss before
2021-06-04 13:57:26 | [train_policy] epoch #720 | Computing KL before
2021-06-04 13:57:26 | [train_policy] epoch #720 | Optimizing
2021-06-04 13:57:26 | [train_policy] epoch #720 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:26 | [train_policy] epoch #720 | computing loss before
2021-06-04 13:57:26 | [train_policy] epoch #720 | computing gradient
2021-06-04 13:57:26 | [train_policy] epoch #720 | gradient computed
2021-06-04 13:57:26 | [train_policy] epoch #720 | computing descent direction
2021-06-04 13:57:26 | [train_policy] epoch #720 | descent direction computed
2021-06-04 13:57:26 | [train_policy] epoch #720 | backtrack iters: 1
2021-06-04 13:57:26 | [train_policy] epoch #720 | optimization finished
2021-06-04 13:57:26 | [train_policy] epoch #720 | Computing KL after
2021-06-04 13:57:26 | [train_policy] epoch #720 | Computing loss after
2021-06-04 13:57:26 | [train_policy] epoch #720 | Fitting baseline...
2021-06-04 13:57:26 | [train_policy] epoch #720 | Saving snapshot...
2021-06-04 13:57:26 | [train_policy] epoch #720 | Saved
2021-06-04 13:57:26 | [train_policy] epoch #720 | Time 578.49 s
2021-06-04 13:57:26 | [train_policy] epoch #720 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285488
Evaluation/AverageDiscountedReturn          -41.8368
Evaluation/AverageReturn                    -41.8368
Evaluation/CompletionRate                     0
Evaluation/Iteration                        720
Evaluation/MaxReturn                        -27.7504
Evaluation/MinReturn                        -79.4612
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.34891
Extras/EpisodeRewardMean                    -41.9315
LinearFeatureBaseline/ExplainedVariance       0.863366
PolicyExecTime                                0.228362
ProcessExecTime                               0.0313065
TotalEnvSteps                            729652
policy/Entropy                               -1.86382
policy/KL                                     0.00667678
policy/KLBefore                               0
policy/LossAfter                             -0.0133238
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.155078
policy/dLoss                                  0.0133238
---------------------------------------  ----------------
2021-06-04 13:57:26 | [train_policy] epoch #721 | Obtaining samples for iteration 721...
2021-06-04 13:57:27 | [train_policy] epoch #721 | Logging diagnostics...
2021-06-04 13:57:27 | [train_policy] epoch #721 | Optimizing policy...
2021-06-04 13:57:27 | [train_policy] epoch #721 | Computing loss before
2021-06-04 13:57:27 | [train_policy] epoch #721 | Computing KL before
2021-06-04 13:57:27 | [train_policy] epoch #721 | Optimizing
2021-06-04 13:57:27 | [train_policy] epoch #721 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:27 | [train_policy] epoch #721 | computing loss before
2021-06-04 13:57:27 | [train_policy] epoch #721 | computing gradient
2021-06-04 13:57:27 | [train_policy] epoch #721 | gradient computed
2021-06-04 13:57:27 | [train_policy] epoch #721 | computing descent direction
2021-06-04 13:57:27 | [train_policy] epoch #721 | descent direction computed
2021-06-04 13:57:27 | [train_policy] epoch #721 | backtrack iters: 0
2021-06-04 13:57:27 | [train_policy] epoch #721 | optimization finished
2021-06-04 13:57:27 | [train_policy] epoch #721 | Computing KL after
2021-06-04 13:57:27 | [train_policy] epoch #721 | Computing loss after
2021-06-04 13:57:27 | [train_policy] epoch #721 | Fitting baseline...
2021-06-04 13:57:27 | [train_policy] epoch #721 | Saving snapshot...
2021-06-04 13:57:27 | [train_policy] epoch #721 | Saved
2021-06-04 13:57:27 | [train_policy] epoch #721 | Time 579.27 s
2021-06-04 13:57:27 | [train_policy] epoch #721 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28463
Evaluation/AverageDiscountedReturn          -41.8204
Evaluation/AverageReturn                    -41.8204
Evaluation/CompletionRate                     0
Evaluation/Iteration                        721
Evaluation/MaxReturn                        -31.763
Evaluation/MinReturn                        -64.1172
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.00229
Extras/EpisodeRewardMean                    -41.908
LinearFeatureBaseline/ExplainedVariance       0.889679
PolicyExecTime                                0.210327
ProcessExecTime                               0.0311182
TotalEnvSteps                            730664
policy/Entropy                               -1.84952
policy/KL                                     0.0098317
policy/KLBefore                               0
policy/LossAfter                             -0.0231223
policy/LossBefore                             2.12032e-08
policy/Perplexity                             0.157313
policy/dLoss                                  0.0231224
---------------------------------------  ----------------
2021-06-04 13:57:27 | [train_policy] epoch #722 | Obtaining samples for iteration 722...
2021-06-04 13:57:28 | [train_policy] epoch #722 | Logging diagnostics...
2021-06-04 13:57:28 | [train_policy] epoch #722 | Optimizing policy...
2021-06-04 13:57:28 | [train_policy] epoch #722 | Computing loss before
2021-06-04 13:57:28 | [train_policy] epoch #722 | Computing KL before
2021-06-04 13:57:28 | [train_policy] epoch #722 | Optimizing
2021-06-04 13:57:28 | [train_policy] epoch #722 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:28 | [train_policy] epoch #722 | computing loss before
2021-06-04 13:57:28 | [train_policy] epoch #722 | computing gradient
2021-06-04 13:57:28 | [train_policy] epoch #722 | gradient computed
2021-06-04 13:57:28 | [train_policy] epoch #722 | computing descent direction
2021-06-04 13:57:28 | [train_policy] epoch #722 | descent direction computed
2021-06-04 13:57:28 | [train_policy] epoch #722 | backtrack iters: 1
2021-06-04 13:57:28 | [train_policy] epoch #722 | optimization finished
2021-06-04 13:57:28 | [train_policy] epoch #722 | Computing KL after
2021-06-04 13:57:28 | [train_policy] epoch #722 | Computing loss after
2021-06-04 13:57:28 | [train_policy] epoch #722 | Fitting baseline...
2021-06-04 13:57:28 | [train_policy] epoch #722 | Saving snapshot...
2021-06-04 13:57:28 | [train_policy] epoch #722 | Saved
2021-06-04 13:57:28 | [train_policy] epoch #722 | Time 580.07 s
2021-06-04 13:57:28 | [train_policy] epoch #722 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284535
Evaluation/AverageDiscountedReturn          -40.4945
Evaluation/AverageReturn                    -40.4945
Evaluation/CompletionRate                     0
Evaluation/Iteration                        722
Evaluation/MaxReturn                        -27.4187
Evaluation/MinReturn                        -76.7375
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.22921
Extras/EpisodeRewardMean                    -40.4462
LinearFeatureBaseline/ExplainedVariance       0.875917
PolicyExecTime                                0.225638
ProcessExecTime                               0.0312462
TotalEnvSteps                            731676
policy/Entropy                               -1.8759
policy/KL                                     0.0065109
policy/KLBefore                               0
policy/LossAfter                             -0.0170442
policy/LossBefore                             1.13084e-08
policy/Perplexity                             0.153217
policy/dLoss                                  0.0170442
---------------------------------------  ----------------
2021-06-04 13:57:28 | [train_policy] epoch #723 | Obtaining samples for iteration 723...
2021-06-04 13:57:28 | [train_policy] epoch #723 | Logging diagnostics...
2021-06-04 13:57:28 | [train_policy] epoch #723 | Optimizing policy...
2021-06-04 13:57:28 | [train_policy] epoch #723 | Computing loss before
2021-06-04 13:57:29 | [train_policy] epoch #723 | Computing KL before
2021-06-04 13:57:29 | [train_policy] epoch #723 | Optimizing
2021-06-04 13:57:29 | [train_policy] epoch #723 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:29 | [train_policy] epoch #723 | computing loss before
2021-06-04 13:57:29 | [train_policy] epoch #723 | computing gradient
2021-06-04 13:57:29 | [train_policy] epoch #723 | gradient computed
2021-06-04 13:57:29 | [train_policy] epoch #723 | computing descent direction
2021-06-04 13:57:29 | [train_policy] epoch #723 | descent direction computed
2021-06-04 13:57:29 | [train_policy] epoch #723 | backtrack iters: 1
2021-06-04 13:57:29 | [train_policy] epoch #723 | optimization finished
2021-06-04 13:57:29 | [train_policy] epoch #723 | Computing KL after
2021-06-04 13:57:29 | [train_policy] epoch #723 | Computing loss after
2021-06-04 13:57:29 | [train_policy] epoch #723 | Fitting baseline...
2021-06-04 13:57:29 | [train_policy] epoch #723 | Saving snapshot...
2021-06-04 13:57:29 | [train_policy] epoch #723 | Saved
2021-06-04 13:57:29 | [train_policy] epoch #723 | Time 580.87 s
2021-06-04 13:57:29 | [train_policy] epoch #723 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.28568
Evaluation/AverageDiscountedReturn          -41.9129
Evaluation/AverageReturn                    -41.9129
Evaluation/CompletionRate                     0
Evaluation/Iteration                        723
Evaluation/MaxReturn                        -28.1434
Evaluation/MinReturn                        -63.981
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.73736
Extras/EpisodeRewardMean                    -42.0989
LinearFeatureBaseline/ExplainedVariance       0.880464
PolicyExecTime                                0.228579
ProcessExecTime                               0.0312512
TotalEnvSteps                            732688
policy/Entropy                               -1.89569
policy/KL                                     0.00659612
policy/KLBefore                               0
policy/LossAfter                             -0.0141485
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             0.150215
policy/dLoss                                  0.0141485
---------------------------------------  ---------------
2021-06-04 13:57:29 | [train_policy] epoch #724 | Obtaining samples for iteration 724...
2021-06-04 13:57:29 | [train_policy] epoch #724 | Logging diagnostics...
2021-06-04 13:57:29 | [train_policy] epoch #724 | Optimizing policy...
2021-06-04 13:57:29 | [train_policy] epoch #724 | Computing loss before
2021-06-04 13:57:29 | [train_policy] epoch #724 | Computing KL before
2021-06-04 13:57:29 | [train_policy] epoch #724 | Optimizing
2021-06-04 13:57:29 | [train_policy] epoch #724 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:29 | [train_policy] epoch #724 | computing loss before
2021-06-04 13:57:29 | [train_policy] epoch #724 | computing gradient
2021-06-04 13:57:29 | [train_policy] epoch #724 | gradient computed
2021-06-04 13:57:29 | [train_policy] epoch #724 | computing descent direction
2021-06-04 13:57:29 | [train_policy] epoch #724 | descent direction computed
2021-06-04 13:57:29 | [train_policy] epoch #724 | backtrack iters: 0
2021-06-04 13:57:29 | [train_policy] epoch #724 | optimization finished
2021-06-04 13:57:29 | [train_policy] epoch #724 | Computing KL after
2021-06-04 13:57:29 | [train_policy] epoch #724 | Computing loss after
2021-06-04 13:57:29 | [train_policy] epoch #724 | Fitting baseline...
2021-06-04 13:57:29 | [train_policy] epoch #724 | Saving snapshot...
2021-06-04 13:57:29 | [train_policy] epoch #724 | Saved
2021-06-04 13:57:29 | [train_policy] epoch #724 | Time 581.67 s
2021-06-04 13:57:29 | [train_policy] epoch #724 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283693
Evaluation/AverageDiscountedReturn          -42.1283
Evaluation/AverageReturn                    -42.1283
Evaluation/CompletionRate                     0
Evaluation/Iteration                        724
Evaluation/MaxReturn                        -28.4735
Evaluation/MinReturn                        -81.9221
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1803
Extras/EpisodeRewardMean                    -42.3506
LinearFeatureBaseline/ExplainedVariance       0.850583
PolicyExecTime                                0.22795
ProcessExecTime                               0.0311909
TotalEnvSteps                            733700
policy/Entropy                               -1.88955
policy/KL                                     0.00971246
policy/KLBefore                               0
policy/LossAfter                             -0.013876
policy/LossBefore                            -1.71982e-08
policy/Perplexity                             0.151139
policy/dLoss                                  0.013876
---------------------------------------  ----------------
2021-06-04 13:57:29 | [train_policy] epoch #725 | Obtaining samples for iteration 725...
2021-06-04 13:57:30 | [train_policy] epoch #725 | Logging diagnostics...
2021-06-04 13:57:30 | [train_policy] epoch #725 | Optimizing policy...
2021-06-04 13:57:30 | [train_policy] epoch #725 | Computing loss before
2021-06-04 13:57:30 | [train_policy] epoch #725 | Computing KL before
2021-06-04 13:57:30 | [train_policy] epoch #725 | Optimizing
2021-06-04 13:57:30 | [train_policy] epoch #725 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:30 | [train_policy] epoch #725 | computing loss before
2021-06-04 13:57:30 | [train_policy] epoch #725 | computing gradient
2021-06-04 13:57:30 | [train_policy] epoch #725 | gradient computed
2021-06-04 13:57:30 | [train_policy] epoch #725 | computing descent direction
2021-06-04 13:57:30 | [train_policy] epoch #725 | descent direction computed
2021-06-04 13:57:30 | [train_policy] epoch #725 | backtrack iters: 1
2021-06-04 13:57:30 | [train_policy] epoch #725 | optimization finished
2021-06-04 13:57:30 | [train_policy] epoch #725 | Computing KL after
2021-06-04 13:57:30 | [train_policy] epoch #725 | Computing loss after
2021-06-04 13:57:30 | [train_policy] epoch #725 | Fitting baseline...
2021-06-04 13:57:30 | [train_policy] epoch #725 | Saving snapshot...
2021-06-04 13:57:30 | [train_policy] epoch #725 | Saved
2021-06-04 13:57:30 | [train_policy] epoch #725 | Time 582.46 s
2021-06-04 13:57:30 | [train_policy] epoch #725 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.285108
Evaluation/AverageDiscountedReturn          -42.7315
Evaluation/AverageReturn                    -42.7315
Evaluation/CompletionRate                     0
Evaluation/Iteration                        725
Evaluation/MaxReturn                        -28.0862
Evaluation/MinReturn                        -83.8055
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.4066
Extras/EpisodeRewardMean                    -42.1111
LinearFeatureBaseline/ExplainedVariance       0.798389
PolicyExecTime                                0.217149
ProcessExecTime                               0.0313101
TotalEnvSteps                            734712
policy/Entropy                               -1.8852
policy/KL                                     0.00654936
policy/KLBefore                               0
policy/LossAfter                             -0.0114171
policy/LossBefore                            -2.191e-08
policy/Perplexity                             0.151799
policy/dLoss                                  0.011417
---------------------------------------  ---------------
2021-06-04 13:57:30 | [train_policy] epoch #726 | Obtaining samples for iteration 726...
2021-06-04 13:57:31 | [train_policy] epoch #726 | Logging diagnostics...
2021-06-04 13:57:31 | [train_policy] epoch #726 | Optimizing policy...
2021-06-04 13:57:31 | [train_policy] epoch #726 | Computing loss before
2021-06-04 13:57:31 | [train_policy] epoch #726 | Computing KL before
2021-06-04 13:57:31 | [train_policy] epoch #726 | Optimizing
2021-06-04 13:57:31 | [train_policy] epoch #726 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:31 | [train_policy] epoch #726 | computing loss before
2021-06-04 13:57:31 | [train_policy] epoch #726 | computing gradient
2021-06-04 13:57:31 | [train_policy] epoch #726 | gradient computed
2021-06-04 13:57:31 | [train_policy] epoch #726 | computing descent direction
2021-06-04 13:57:31 | [train_policy] epoch #726 | descent direction computed
2021-06-04 13:57:31 | [train_policy] epoch #726 | backtrack iters: 1
2021-06-04 13:57:31 | [train_policy] epoch #726 | optimization finished
2021-06-04 13:57:31 | [train_policy] epoch #726 | Computing KL after
2021-06-04 13:57:31 | [train_policy] epoch #726 | Computing loss after
2021-06-04 13:57:31 | [train_policy] epoch #726 | Fitting baseline...
2021-06-04 13:57:31 | [train_policy] epoch #726 | Saving snapshot...
2021-06-04 13:57:31 | [train_policy] epoch #726 | Saved
2021-06-04 13:57:31 | [train_policy] epoch #726 | Time 583.26 s
2021-06-04 13:57:31 | [train_policy] epoch #726 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285389
Evaluation/AverageDiscountedReturn          -39.7953
Evaluation/AverageReturn                    -39.7953
Evaluation/CompletionRate                     0
Evaluation/Iteration                        726
Evaluation/MaxReturn                        -27.9532
Evaluation/MinReturn                        -63.9967
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.26633
Extras/EpisodeRewardMean                    -40.1367
LinearFeatureBaseline/ExplainedVariance       0.898213
PolicyExecTime                                0.226631
ProcessExecTime                               0.0312934
TotalEnvSteps                            735724
policy/Entropy                               -1.89424
policy/KL                                     0.00642757
policy/KLBefore                               0
policy/LossAfter                             -0.0135079
policy/LossBefore                             3.19226e-08
policy/Perplexity                             0.150432
policy/dLoss                                  0.013508
---------------------------------------  ----------------
2021-06-04 13:57:31 | [train_policy] epoch #727 | Obtaining samples for iteration 727...
2021-06-04 13:57:32 | [train_policy] epoch #727 | Logging diagnostics...
2021-06-04 13:57:32 | [train_policy] epoch #727 | Optimizing policy...
2021-06-04 13:57:32 | [train_policy] epoch #727 | Computing loss before
2021-06-04 13:57:32 | [train_policy] epoch #727 | Computing KL before
2021-06-04 13:57:32 | [train_policy] epoch #727 | Optimizing
2021-06-04 13:57:32 | [train_policy] epoch #727 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:32 | [train_policy] epoch #727 | computing loss before
2021-06-04 13:57:32 | [train_policy] epoch #727 | computing gradient
2021-06-04 13:57:32 | [train_policy] epoch #727 | gradient computed
2021-06-04 13:57:32 | [train_policy] epoch #727 | computing descent direction
2021-06-04 13:57:32 | [train_policy] epoch #727 | descent direction computed
2021-06-04 13:57:32 | [train_policy] epoch #727 | backtrack iters: 1
2021-06-04 13:57:32 | [train_policy] epoch #727 | optimization finished
2021-06-04 13:57:32 | [train_policy] epoch #727 | Computing KL after
2021-06-04 13:57:32 | [train_policy] epoch #727 | Computing loss after
2021-06-04 13:57:32 | [train_policy] epoch #727 | Fitting baseline...
2021-06-04 13:57:32 | [train_policy] epoch #727 | Saving snapshot...
2021-06-04 13:57:32 | [train_policy] epoch #727 | Saved
2021-06-04 13:57:32 | [train_policy] epoch #727 | Time 584.08 s
2021-06-04 13:57:32 | [train_policy] epoch #727 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285398
Evaluation/AverageDiscountedReturn          -41.7502
Evaluation/AverageReturn                    -41.7502
Evaluation/CompletionRate                     0
Evaluation/Iteration                        727
Evaluation/MaxReturn                        -29.1316
Evaluation/MinReturn                        -77.9543
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.95481
Extras/EpisodeRewardMean                    -41.4322
LinearFeatureBaseline/ExplainedVariance       0.889234
PolicyExecTime                                0.230671
ProcessExecTime                               0.0313203
TotalEnvSteps                            736736
policy/Entropy                               -1.89991
policy/KL                                     0.00678221
policy/KLBefore                               0
policy/LossAfter                             -0.0163731
policy/LossBefore                            -1.01304e-08
policy/Perplexity                             0.149582
policy/dLoss                                  0.0163731
---------------------------------------  ----------------
2021-06-04 13:57:32 | [train_policy] epoch #728 | Obtaining samples for iteration 728...
2021-06-04 13:57:32 | [train_policy] epoch #728 | Logging diagnostics...
2021-06-04 13:57:32 | [train_policy] epoch #728 | Optimizing policy...
2021-06-04 13:57:32 | [train_policy] epoch #728 | Computing loss before
2021-06-04 13:57:32 | [train_policy] epoch #728 | Computing KL before
2021-06-04 13:57:32 | [train_policy] epoch #728 | Optimizing
2021-06-04 13:57:32 | [train_policy] epoch #728 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:32 | [train_policy] epoch #728 | computing loss before
2021-06-04 13:57:32 | [train_policy] epoch #728 | computing gradient
2021-06-04 13:57:33 | [train_policy] epoch #728 | gradient computed
2021-06-04 13:57:33 | [train_policy] epoch #728 | computing descent direction
2021-06-04 13:57:33 | [train_policy] epoch #728 | descent direction computed
2021-06-04 13:57:33 | [train_policy] epoch #728 | backtrack iters: 0
2021-06-04 13:57:33 | [train_policy] epoch #728 | optimization finished
2021-06-04 13:57:33 | [train_policy] epoch #728 | Computing KL after
2021-06-04 13:57:33 | [train_policy] epoch #728 | Computing loss after
2021-06-04 13:57:33 | [train_policy] epoch #728 | Fitting baseline...
2021-06-04 13:57:33 | [train_policy] epoch #728 | Saving snapshot...
2021-06-04 13:57:33 | [train_policy] epoch #728 | Saved
2021-06-04 13:57:33 | [train_policy] epoch #728 | Time 584.85 s
2021-06-04 13:57:33 | [train_policy] epoch #728 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.286451
Evaluation/AverageDiscountedReturn          -42.5136
Evaluation/AverageReturn                    -42.5136
Evaluation/CompletionRate                     0
Evaluation/Iteration                        728
Evaluation/MaxReturn                        -27.7538
Evaluation/MinReturn                        -82.1531
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.89387
Extras/EpisodeRewardMean                    -42.7572
LinearFeatureBaseline/ExplainedVariance       0.789977
PolicyExecTime                                0.209662
ProcessExecTime                               0.0313153
TotalEnvSteps                            737748
policy/Entropy                               -1.88415
policy/KL                                     0.00978779
policy/KLBefore                               0
policy/LossAfter                             -0.0225401
policy/LossBefore                             1.46067e-08
policy/Perplexity                             0.151958
policy/dLoss                                  0.0225401
---------------------------------------  ----------------
2021-06-04 13:57:33 | [train_policy] epoch #729 | Obtaining samples for iteration 729...
2021-06-04 13:57:33 | [train_policy] epoch #729 | Logging diagnostics...
2021-06-04 13:57:33 | [train_policy] epoch #729 | Optimizing policy...
2021-06-04 13:57:33 | [train_policy] epoch #729 | Computing loss before
2021-06-04 13:57:33 | [train_policy] epoch #729 | Computing KL before
2021-06-04 13:57:33 | [train_policy] epoch #729 | Optimizing
2021-06-04 13:57:33 | [train_policy] epoch #729 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:33 | [train_policy] epoch #729 | computing loss before
2021-06-04 13:57:33 | [train_policy] epoch #729 | computing gradient
2021-06-04 13:57:33 | [train_policy] epoch #729 | gradient computed
2021-06-04 13:57:33 | [train_policy] epoch #729 | computing descent direction
2021-06-04 13:57:33 | [train_policy] epoch #729 | descent direction computed
2021-06-04 13:57:33 | [train_policy] epoch #729 | backtrack iters: 1
2021-06-04 13:57:33 | [train_policy] epoch #729 | optimization finished
2021-06-04 13:57:33 | [train_policy] epoch #729 | Computing KL after
2021-06-04 13:57:33 | [train_policy] epoch #729 | Computing loss after
2021-06-04 13:57:33 | [train_policy] epoch #729 | Fitting baseline...
2021-06-04 13:57:33 | [train_policy] epoch #729 | Saving snapshot...
2021-06-04 13:57:33 | [train_policy] epoch #729 | Saved
2021-06-04 13:57:33 | [train_policy] epoch #729 | Time 585.66 s
2021-06-04 13:57:33 | [train_policy] epoch #729 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285052
Evaluation/AverageDiscountedReturn          -41.9452
Evaluation/AverageReturn                    -41.9452
Evaluation/CompletionRate                     0
Evaluation/Iteration                        729
Evaluation/MaxReturn                        -29.6737
Evaluation/MinReturn                        -86.3456
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.97734
Extras/EpisodeRewardMean                    -41.4817
LinearFeatureBaseline/ExplainedVariance       0.879539
PolicyExecTime                                0.234101
ProcessExecTime                               0.0312366
TotalEnvSteps                            738760
policy/Entropy                               -1.89471
policy/KL                                     0.00659046
policy/KLBefore                               0
policy/LossAfter                             -0.0157077
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.150362
policy/dLoss                                  0.0157077
---------------------------------------  ----------------
2021-06-04 13:57:33 | [train_policy] epoch #730 | Obtaining samples for iteration 730...
2021-06-04 13:57:34 | [train_policy] epoch #730 | Logging diagnostics...
2021-06-04 13:57:34 | [train_policy] epoch #730 | Optimizing policy...
2021-06-04 13:57:34 | [train_policy] epoch #730 | Computing loss before
2021-06-04 13:57:34 | [train_policy] epoch #730 | Computing KL before
2021-06-04 13:57:34 | [train_policy] epoch #730 | Optimizing
2021-06-04 13:57:34 | [train_policy] epoch #730 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:34 | [train_policy] epoch #730 | computing loss before
2021-06-04 13:57:34 | [train_policy] epoch #730 | computing gradient
2021-06-04 13:57:34 | [train_policy] epoch #730 | gradient computed
2021-06-04 13:57:34 | [train_policy] epoch #730 | computing descent direction
2021-06-04 13:57:34 | [train_policy] epoch #730 | descent direction computed
2021-06-04 13:57:34 | [train_policy] epoch #730 | backtrack iters: 0
2021-06-04 13:57:34 | [train_policy] epoch #730 | optimization finished
2021-06-04 13:57:34 | [train_policy] epoch #730 | Computing KL after
2021-06-04 13:57:34 | [train_policy] epoch #730 | Computing loss after
2021-06-04 13:57:34 | [train_policy] epoch #730 | Fitting baseline...
2021-06-04 13:57:34 | [train_policy] epoch #730 | Saving snapshot...
2021-06-04 13:57:34 | [train_policy] epoch #730 | Saved
2021-06-04 13:57:34 | [train_policy] epoch #730 | Time 586.45 s
2021-06-04 13:57:34 | [train_policy] epoch #730 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285531
Evaluation/AverageDiscountedReturn          -42.7632
Evaluation/AverageReturn                    -42.7632
Evaluation/CompletionRate                     0
Evaluation/Iteration                        730
Evaluation/MaxReturn                        -29.3439
Evaluation/MinReturn                        -83.5977
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2081
Extras/EpisodeRewardMean                    -42.8474
LinearFeatureBaseline/ExplainedVariance       0.865751
PolicyExecTime                                0.227102
ProcessExecTime                               0.0312593
TotalEnvSteps                            739772
policy/Entropy                               -1.88507
policy/KL                                     0.00952602
policy/KLBefore                               0
policy/LossAfter                             -0.018482
policy/LossBefore                            -6.00758e-09
policy/Perplexity                             0.151818
policy/dLoss                                  0.018482
---------------------------------------  ----------------
2021-06-04 13:57:34 | [train_policy] epoch #731 | Obtaining samples for iteration 731...
2021-06-04 13:57:35 | [train_policy] epoch #731 | Logging diagnostics...
2021-06-04 13:57:35 | [train_policy] epoch #731 | Optimizing policy...
2021-06-04 13:57:35 | [train_policy] epoch #731 | Computing loss before
2021-06-04 13:57:35 | [train_policy] epoch #731 | Computing KL before
2021-06-04 13:57:35 | [train_policy] epoch #731 | Optimizing
2021-06-04 13:57:35 | [train_policy] epoch #731 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:35 | [train_policy] epoch #731 | computing loss before
2021-06-04 13:57:35 | [train_policy] epoch #731 | computing gradient
2021-06-04 13:57:35 | [train_policy] epoch #731 | gradient computed
2021-06-04 13:57:35 | [train_policy] epoch #731 | computing descent direction
2021-06-04 13:57:35 | [train_policy] epoch #731 | descent direction computed
2021-06-04 13:57:35 | [train_policy] epoch #731 | backtrack iters: 0
2021-06-04 13:57:35 | [train_policy] epoch #731 | optimization finished
2021-06-04 13:57:35 | [train_policy] epoch #731 | Computing KL after
2021-06-04 13:57:35 | [train_policy] epoch #731 | Computing loss after
2021-06-04 13:57:35 | [train_policy] epoch #731 | Fitting baseline...
2021-06-04 13:57:35 | [train_policy] epoch #731 | Saving snapshot...
2021-06-04 13:57:35 | [train_policy] epoch #731 | Saved
2021-06-04 13:57:35 | [train_policy] epoch #731 | Time 587.24 s
2021-06-04 13:57:35 | [train_policy] epoch #731 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.289134
Evaluation/AverageDiscountedReturn          -40.0942
Evaluation/AverageReturn                    -40.0942
Evaluation/CompletionRate                     0
Evaluation/Iteration                        731
Evaluation/MaxReturn                        -27.5802
Evaluation/MinReturn                        -81.5711
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.73285
Extras/EpisodeRewardMean                    -39.8554
LinearFeatureBaseline/ExplainedVariance       0.853604
PolicyExecTime                                0.213253
ProcessExecTime                               0.0316169
TotalEnvSteps                            740784
policy/Entropy                               -1.85035
policy/KL                                     0.0092549
policy/KLBefore                               0
policy/LossAfter                             -0.0192012
policy/LossBefore                            -1.64914e-08
policy/Perplexity                             0.157182
policy/dLoss                                  0.0192012
---------------------------------------  ----------------
2021-06-04 13:57:35 | [train_policy] epoch #732 | Obtaining samples for iteration 732...
2021-06-04 13:57:36 | [train_policy] epoch #732 | Logging diagnostics...
2021-06-04 13:57:36 | [train_policy] epoch #732 | Optimizing policy...
2021-06-04 13:57:36 | [train_policy] epoch #732 | Computing loss before
2021-06-04 13:57:36 | [train_policy] epoch #732 | Computing KL before
2021-06-04 13:57:36 | [train_policy] epoch #732 | Optimizing
2021-06-04 13:57:36 | [train_policy] epoch #732 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:36 | [train_policy] epoch #732 | computing loss before
2021-06-04 13:57:36 | [train_policy] epoch #732 | computing gradient
2021-06-04 13:57:36 | [train_policy] epoch #732 | gradient computed
2021-06-04 13:57:36 | [train_policy] epoch #732 | computing descent direction
2021-06-04 13:57:36 | [train_policy] epoch #732 | descent direction computed
2021-06-04 13:57:36 | [train_policy] epoch #732 | backtrack iters: 1
2021-06-04 13:57:36 | [train_policy] epoch #732 | optimization finished
2021-06-04 13:57:36 | [train_policy] epoch #732 | Computing KL after
2021-06-04 13:57:36 | [train_policy] epoch #732 | Computing loss after
2021-06-04 13:57:36 | [train_policy] epoch #732 | Fitting baseline...
2021-06-04 13:57:36 | [train_policy] epoch #732 | Saving snapshot...
2021-06-04 13:57:36 | [train_policy] epoch #732 | Saved
2021-06-04 13:57:36 | [train_policy] epoch #732 | Time 588.05 s
2021-06-04 13:57:36 | [train_policy] epoch #732 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287843
Evaluation/AverageDiscountedReturn          -41.9247
Evaluation/AverageReturn                    -41.9247
Evaluation/CompletionRate                     0
Evaluation/Iteration                        732
Evaluation/MaxReturn                        -31.3721
Evaluation/MinReturn                        -84.846
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.44214
Extras/EpisodeRewardMean                    -41.7708
LinearFeatureBaseline/ExplainedVariance       0.86613
PolicyExecTime                                0.229618
ProcessExecTime                               0.031513
TotalEnvSteps                            741796
policy/Entropy                               -1.87991
policy/KL                                     0.00678928
policy/KLBefore                               0
policy/LossAfter                             -0.0158392
policy/LossBefore                             2.35591e-10
policy/Perplexity                             0.152604
policy/dLoss                                  0.0158392
---------------------------------------  ----------------
2021-06-04 13:57:36 | [train_policy] epoch #733 | Obtaining samples for iteration 733...
2021-06-04 13:57:36 | [train_policy] epoch #733 | Logging diagnostics...
2021-06-04 13:57:36 | [train_policy] epoch #733 | Optimizing policy...
2021-06-04 13:57:36 | [train_policy] epoch #733 | Computing loss before
2021-06-04 13:57:36 | [train_policy] epoch #733 | Computing KL before
2021-06-04 13:57:36 | [train_policy] epoch #733 | Optimizing
2021-06-04 13:57:36 | [train_policy] epoch #733 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:36 | [train_policy] epoch #733 | computing loss before
2021-06-04 13:57:36 | [train_policy] epoch #733 | computing gradient
2021-06-04 13:57:36 | [train_policy] epoch #733 | gradient computed
2021-06-04 13:57:36 | [train_policy] epoch #733 | computing descent direction
2021-06-04 13:57:37 | [train_policy] epoch #733 | descent direction computed
2021-06-04 13:57:37 | [train_policy] epoch #733 | backtrack iters: 0
2021-06-04 13:57:37 | [train_policy] epoch #733 | optimization finished
2021-06-04 13:57:37 | [train_policy] epoch #733 | Computing KL after
2021-06-04 13:57:37 | [train_policy] epoch #733 | Computing loss after
2021-06-04 13:57:37 | [train_policy] epoch #733 | Fitting baseline...
2021-06-04 13:57:37 | [train_policy] epoch #733 | Saving snapshot...
2021-06-04 13:57:37 | [train_policy] epoch #733 | Saved
2021-06-04 13:57:37 | [train_policy] epoch #733 | Time 588.85 s
2021-06-04 13:57:37 | [train_policy] epoch #733 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286007
Evaluation/AverageDiscountedReturn          -42.501
Evaluation/AverageReturn                    -42.501
Evaluation/CompletionRate                     0
Evaluation/Iteration                        733
Evaluation/MaxReturn                        -27.6486
Evaluation/MinReturn                        -84.0313
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.62742
Extras/EpisodeRewardMean                    -42.3809
LinearFeatureBaseline/ExplainedVariance       0.856732
PolicyExecTime                                0.226054
ProcessExecTime                               0.0312958
TotalEnvSteps                            742808
policy/Entropy                               -1.85922
policy/KL                                     0.00945133
policy/KLBefore                               0
policy/LossAfter                             -0.0286571
policy/LossBefore                             1.36643e-08
policy/Perplexity                             0.155795
policy/dLoss                                  0.0286572
---------------------------------------  ----------------
2021-06-04 13:57:37 | [train_policy] epoch #734 | Obtaining samples for iteration 734...
2021-06-04 13:57:37 | [train_policy] epoch #734 | Logging diagnostics...
2021-06-04 13:57:37 | [train_policy] epoch #734 | Optimizing policy...
2021-06-04 13:57:37 | [train_policy] epoch #734 | Computing loss before
2021-06-04 13:57:37 | [train_policy] epoch #734 | Computing KL before
2021-06-04 13:57:37 | [train_policy] epoch #734 | Optimizing
2021-06-04 13:57:37 | [train_policy] epoch #734 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:37 | [train_policy] epoch #734 | computing loss before
2021-06-04 13:57:37 | [train_policy] epoch #734 | computing gradient
2021-06-04 13:57:37 | [train_policy] epoch #734 | gradient computed
2021-06-04 13:57:37 | [train_policy] epoch #734 | computing descent direction
2021-06-04 13:57:37 | [train_policy] epoch #734 | descent direction computed
2021-06-04 13:57:37 | [train_policy] epoch #734 | backtrack iters: 1
2021-06-04 13:57:37 | [train_policy] epoch #734 | optimization finished
2021-06-04 13:57:37 | [train_policy] epoch #734 | Computing KL after
2021-06-04 13:57:37 | [train_policy] epoch #734 | Computing loss after
2021-06-04 13:57:37 | [train_policy] epoch #734 | Fitting baseline...
2021-06-04 13:57:37 | [train_policy] epoch #734 | Saving snapshot...
2021-06-04 13:57:37 | [train_policy] epoch #734 | Saved
2021-06-04 13:57:37 | [train_policy] epoch #734 | Time 589.66 s
2021-06-04 13:57:37 | [train_policy] epoch #734 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284491
Evaluation/AverageDiscountedReturn          -44.3031
Evaluation/AverageReturn                    -44.3031
Evaluation/CompletionRate                     0
Evaluation/Iteration                        734
Evaluation/MaxReturn                        -28.8344
Evaluation/MinReturn                       -176.607
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         18.2971
Extras/EpisodeRewardMean                    -44.6772
LinearFeatureBaseline/ExplainedVariance       0.522919
PolicyExecTime                                0.233856
ProcessExecTime                               0.0312426
TotalEnvSteps                            743820
policy/Entropy                               -1.86119
policy/KL                                     0.00658061
policy/KLBefore                               0
policy/LossAfter                             -0.0232724
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             0.155487
policy/dLoss                                  0.0232724
---------------------------------------  ----------------
2021-06-04 13:57:37 | [train_policy] epoch #735 | Obtaining samples for iteration 735...
2021-06-04 13:57:38 | [train_policy] epoch #735 | Logging diagnostics...
2021-06-04 13:57:38 | [train_policy] epoch #735 | Optimizing policy...
2021-06-04 13:57:38 | [train_policy] epoch #735 | Computing loss before
2021-06-04 13:57:38 | [train_policy] epoch #735 | Computing KL before
2021-06-04 13:57:38 | [train_policy] epoch #735 | Optimizing
2021-06-04 13:57:38 | [train_policy] epoch #735 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:38 | [train_policy] epoch #735 | computing loss before
2021-06-04 13:57:38 | [train_policy] epoch #735 | computing gradient
2021-06-04 13:57:38 | [train_policy] epoch #735 | gradient computed
2021-06-04 13:57:38 | [train_policy] epoch #735 | computing descent direction
2021-06-04 13:57:38 | [train_policy] epoch #735 | descent direction computed
2021-06-04 13:57:38 | [train_policy] epoch #735 | backtrack iters: 0
2021-06-04 13:57:38 | [train_policy] epoch #735 | optimization finished
2021-06-04 13:57:38 | [train_policy] epoch #735 | Computing KL after
2021-06-04 13:57:38 | [train_policy] epoch #735 | Computing loss after
2021-06-04 13:57:38 | [train_policy] epoch #735 | Fitting baseline...
2021-06-04 13:57:38 | [train_policy] epoch #735 | Saving snapshot...
2021-06-04 13:57:38 | [train_policy] epoch #735 | Saved
2021-06-04 13:57:38 | [train_policy] epoch #735 | Time 590.44 s
2021-06-04 13:57:38 | [train_policy] epoch #735 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285794
Evaluation/AverageDiscountedReturn          -40.5406
Evaluation/AverageReturn                    -40.5406
Evaluation/CompletionRate                     0
Evaluation/Iteration                        735
Evaluation/MaxReturn                        -28.0128
Evaluation/MinReturn                        -56.4417
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.0855
Extras/EpisodeRewardMean                    -40.4235
LinearFeatureBaseline/ExplainedVariance       0.73919
PolicyExecTime                                0.217906
ProcessExecTime                               0.0312779
TotalEnvSteps                            744832
policy/Entropy                               -1.84541
policy/KL                                     0.00995328
policy/KLBefore                               0
policy/LossAfter                             -0.0444142
policy/LossBefore                            -8.01011e-09
policy/Perplexity                             0.15796
policy/dLoss                                  0.0444142
---------------------------------------  ----------------
2021-06-04 13:57:38 | [train_policy] epoch #736 | Obtaining samples for iteration 736...
2021-06-04 13:57:39 | [train_policy] epoch #736 | Logging diagnostics...
2021-06-04 13:57:39 | [train_policy] epoch #736 | Optimizing policy...
2021-06-04 13:57:39 | [train_policy] epoch #736 | Computing loss before
2021-06-04 13:57:39 | [train_policy] epoch #736 | Computing KL before
2021-06-04 13:57:39 | [train_policy] epoch #736 | Optimizing
2021-06-04 13:57:39 | [train_policy] epoch #736 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:39 | [train_policy] epoch #736 | computing loss before
2021-06-04 13:57:39 | [train_policy] epoch #736 | computing gradient
2021-06-04 13:57:39 | [train_policy] epoch #736 | gradient computed
2021-06-04 13:57:39 | [train_policy] epoch #736 | computing descent direction
2021-06-04 13:57:39 | [train_policy] epoch #736 | descent direction computed
2021-06-04 13:57:39 | [train_policy] epoch #736 | backtrack iters: 1
2021-06-04 13:57:39 | [train_policy] epoch #736 | optimization finished
2021-06-04 13:57:39 | [train_policy] epoch #736 | Computing KL after
2021-06-04 13:57:39 | [train_policy] epoch #736 | Computing loss after
2021-06-04 13:57:39 | [train_policy] epoch #736 | Fitting baseline...
2021-06-04 13:57:39 | [train_policy] epoch #736 | Saving snapshot...
2021-06-04 13:57:39 | [train_policy] epoch #736 | Saved
2021-06-04 13:57:39 | [train_policy] epoch #736 | Time 591.25 s
2021-06-04 13:57:39 | [train_policy] epoch #736 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285986
Evaluation/AverageDiscountedReturn          -43.2404
Evaluation/AverageReturn                    -43.2404
Evaluation/CompletionRate                     0
Evaluation/Iteration                        736
Evaluation/MaxReturn                        -28.5266
Evaluation/MinReturn                       -159.298
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         14.3091
Extras/EpisodeRewardMean                    -42.7684
LinearFeatureBaseline/ExplainedVariance       0.567939
PolicyExecTime                                0.227592
ProcessExecTime                               0.0314367
TotalEnvSteps                            745844
policy/Entropy                               -1.84598
policy/KL                                     0.00663091
policy/KLBefore                               0
policy/LossAfter                             -0.0180109
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.15787
policy/dLoss                                  0.0180108
---------------------------------------  ----------------
2021-06-04 13:57:39 | [train_policy] epoch #737 | Obtaining samples for iteration 737...
2021-06-04 13:57:40 | [train_policy] epoch #737 | Logging diagnostics...
2021-06-04 13:57:40 | [train_policy] epoch #737 | Optimizing policy...
2021-06-04 13:57:40 | [train_policy] epoch #737 | Computing loss before
2021-06-04 13:57:40 | [train_policy] epoch #737 | Computing KL before
2021-06-04 13:57:40 | [train_policy] epoch #737 | Optimizing
2021-06-04 13:57:40 | [train_policy] epoch #737 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:40 | [train_policy] epoch #737 | computing loss before
2021-06-04 13:57:40 | [train_policy] epoch #737 | computing gradient
2021-06-04 13:57:40 | [train_policy] epoch #737 | gradient computed
2021-06-04 13:57:40 | [train_policy] epoch #737 | computing descent direction
2021-06-04 13:57:40 | [train_policy] epoch #737 | descent direction computed
2021-06-04 13:57:40 | [train_policy] epoch #737 | backtrack iters: 1
2021-06-04 13:57:40 | [train_policy] epoch #737 | optimization finished
2021-06-04 13:57:40 | [train_policy] epoch #737 | Computing KL after
2021-06-04 13:57:40 | [train_policy] epoch #737 | Computing loss after
2021-06-04 13:57:40 | [train_policy] epoch #737 | Fitting baseline...
2021-06-04 13:57:40 | [train_policy] epoch #737 | Saving snapshot...
2021-06-04 13:57:40 | [train_policy] epoch #737 | Saved
2021-06-04 13:57:40 | [train_policy] epoch #737 | Time 592.04 s
2021-06-04 13:57:40 | [train_policy] epoch #737 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28774
Evaluation/AverageDiscountedReturn          -40.2247
Evaluation/AverageReturn                    -40.2247
Evaluation/CompletionRate                     0
Evaluation/Iteration                        737
Evaluation/MaxReturn                        -28.046
Evaluation/MinReturn                        -80.8545
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.18163
Extras/EpisodeRewardMean                    -40.5667
LinearFeatureBaseline/ExplainedVariance       0.823796
PolicyExecTime                                0.225854
ProcessExecTime                               0.0316103
TotalEnvSteps                            746856
policy/Entropy                               -1.86757
policy/KL                                     0.00654529
policy/KLBefore                               0
policy/LossAfter                             -0.0166572
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.154498
policy/dLoss                                  0.0166572
---------------------------------------  ----------------
2021-06-04 13:57:40 | [train_policy] epoch #738 | Obtaining samples for iteration 738...
2021-06-04 13:57:40 | [train_policy] epoch #738 | Logging diagnostics...
2021-06-04 13:57:40 | [train_policy] epoch #738 | Optimizing policy...
2021-06-04 13:57:40 | [train_policy] epoch #738 | Computing loss before
2021-06-04 13:57:40 | [train_policy] epoch #738 | Computing KL before
2021-06-04 13:57:40 | [train_policy] epoch #738 | Optimizing
2021-06-04 13:57:40 | [train_policy] epoch #738 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:40 | [train_policy] epoch #738 | computing loss before
2021-06-04 13:57:40 | [train_policy] epoch #738 | computing gradient
2021-06-04 13:57:40 | [train_policy] epoch #738 | gradient computed
2021-06-04 13:57:40 | [train_policy] epoch #738 | computing descent direction
2021-06-04 13:57:41 | [train_policy] epoch #738 | descent direction computed
2021-06-04 13:57:41 | [train_policy] epoch #738 | backtrack iters: 1
2021-06-04 13:57:41 | [train_policy] epoch #738 | optimization finished
2021-06-04 13:57:41 | [train_policy] epoch #738 | Computing KL after
2021-06-04 13:57:41 | [train_policy] epoch #738 | Computing loss after
2021-06-04 13:57:41 | [train_policy] epoch #738 | Fitting baseline...
2021-06-04 13:57:41 | [train_policy] epoch #738 | Saving snapshot...
2021-06-04 13:57:41 | [train_policy] epoch #738 | Saved
2021-06-04 13:57:41 | [train_policy] epoch #738 | Time 592.85 s
2021-06-04 13:57:41 | [train_policy] epoch #738 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284937
Evaluation/AverageDiscountedReturn          -41.0158
Evaluation/AverageReturn                    -41.0158
Evaluation/CompletionRate                     0
Evaluation/Iteration                        738
Evaluation/MaxReturn                        -28.1208
Evaluation/MinReturn                        -63.9042
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.15809
Extras/EpisodeRewardMean                    -41.5567
LinearFeatureBaseline/ExplainedVariance       0.900549
PolicyExecTime                                0.234231
ProcessExecTime                               0.0311866
TotalEnvSteps                            747868
policy/Entropy                               -1.90071
policy/KL                                     0.00648593
policy/KLBefore                               0
policy/LossAfter                             -0.0135925
policy/LossBefore                            -3.65167e-09
policy/Perplexity                             0.149463
policy/dLoss                                  0.0135925
---------------------------------------  ----------------
2021-06-04 13:57:41 | [train_policy] epoch #739 | Obtaining samples for iteration 739...
2021-06-04 13:57:41 | [train_policy] epoch #739 | Logging diagnostics...
2021-06-04 13:57:41 | [train_policy] epoch #739 | Optimizing policy...
2021-06-04 13:57:41 | [train_policy] epoch #739 | Computing loss before
2021-06-04 13:57:41 | [train_policy] epoch #739 | Computing KL before
2021-06-04 13:57:41 | [train_policy] epoch #739 | Optimizing
2021-06-04 13:57:41 | [train_policy] epoch #739 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:41 | [train_policy] epoch #739 | computing loss before
2021-06-04 13:57:41 | [train_policy] epoch #739 | computing gradient
2021-06-04 13:57:41 | [train_policy] epoch #739 | gradient computed
2021-06-04 13:57:41 | [train_policy] epoch #739 | computing descent direction
2021-06-04 13:57:41 | [train_policy] epoch #739 | descent direction computed
2021-06-04 13:57:41 | [train_policy] epoch #739 | backtrack iters: 0
2021-06-04 13:57:41 | [train_policy] epoch #739 | optimization finished
2021-06-04 13:57:41 | [train_policy] epoch #739 | Computing KL after
2021-06-04 13:57:41 | [train_policy] epoch #739 | Computing loss after
2021-06-04 13:57:41 | [train_policy] epoch #739 | Fitting baseline...
2021-06-04 13:57:41 | [train_policy] epoch #739 | Saving snapshot...
2021-06-04 13:57:41 | [train_policy] epoch #739 | Saved
2021-06-04 13:57:41 | [train_policy] epoch #739 | Time 593.64 s
2021-06-04 13:57:41 | [train_policy] epoch #739 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28499
Evaluation/AverageDiscountedReturn          -41.2627
Evaluation/AverageReturn                    -41.2627
Evaluation/CompletionRate                     0
Evaluation/Iteration                        739
Evaluation/MaxReturn                        -29.2194
Evaluation/MinReturn                        -85.9319
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.79184
Extras/EpisodeRewardMean                    -41.2416
LinearFeatureBaseline/ExplainedVariance       0.841272
PolicyExecTime                                0.218464
ProcessExecTime                               0.0313003
TotalEnvSteps                            748880
policy/Entropy                               -1.85923
policy/KL                                     0.00955351
policy/KLBefore                               0
policy/LossAfter                             -0.0165714
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.155792
policy/dLoss                                  0.0165714
---------------------------------------  ----------------
2021-06-04 13:57:41 | [train_policy] epoch #740 | Obtaining samples for iteration 740...
2021-06-04 13:57:42 | [train_policy] epoch #740 | Logging diagnostics...
2021-06-04 13:57:42 | [train_policy] epoch #740 | Optimizing policy...
2021-06-04 13:57:42 | [train_policy] epoch #740 | Computing loss before
2021-06-04 13:57:42 | [train_policy] epoch #740 | Computing KL before
2021-06-04 13:57:42 | [train_policy] epoch #740 | Optimizing
2021-06-04 13:57:42 | [train_policy] epoch #740 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:42 | [train_policy] epoch #740 | computing loss before
2021-06-04 13:57:42 | [train_policy] epoch #740 | computing gradient
2021-06-04 13:57:42 | [train_policy] epoch #740 | gradient computed
2021-06-04 13:57:42 | [train_policy] epoch #740 | computing descent direction
2021-06-04 13:57:42 | [train_policy] epoch #740 | descent direction computed
2021-06-04 13:57:42 | [train_policy] epoch #740 | backtrack iters: 0
2021-06-04 13:57:42 | [train_policy] epoch #740 | optimization finished
2021-06-04 13:57:42 | [train_policy] epoch #740 | Computing KL after
2021-06-04 13:57:42 | [train_policy] epoch #740 | Computing loss after
2021-06-04 13:57:42 | [train_policy] epoch #740 | Fitting baseline...
2021-06-04 13:57:42 | [train_policy] epoch #740 | Saving snapshot...
2021-06-04 13:57:42 | [train_policy] epoch #740 | Saved
2021-06-04 13:57:42 | [train_policy] epoch #740 | Time 594.46 s
2021-06-04 13:57:42 | [train_policy] epoch #740 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.288894
Evaluation/AverageDiscountedReturn          -40.5913
Evaluation/AverageReturn                    -40.5913
Evaluation/CompletionRate                     0
Evaluation/Iteration                        740
Evaluation/MaxReturn                        -27.872
Evaluation/MinReturn                        -62.9732
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.68981
Extras/EpisodeRewardMean                    -40.6783
LinearFeatureBaseline/ExplainedVariance       0.900185
PolicyExecTime                                0.242022
ProcessExecTime                               0.0316498
TotalEnvSteps                            749892
policy/Entropy                               -1.86069
policy/KL                                     0.00988056
policy/KLBefore                               0
policy/LossAfter                             -0.0241925
policy/LossBefore                             5.4186e-09
policy/Perplexity                             0.155565
policy/dLoss                                  0.0241925
---------------------------------------  ---------------
2021-06-04 13:57:42 | [train_policy] epoch #741 | Obtaining samples for iteration 741...
2021-06-04 13:57:43 | [train_policy] epoch #741 | Logging diagnostics...
2021-06-04 13:57:43 | [train_policy] epoch #741 | Optimizing policy...
2021-06-04 13:57:43 | [train_policy] epoch #741 | Computing loss before
2021-06-04 13:57:43 | [train_policy] epoch #741 | Computing KL before
2021-06-04 13:57:43 | [train_policy] epoch #741 | Optimizing
2021-06-04 13:57:43 | [train_policy] epoch #741 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:43 | [train_policy] epoch #741 | computing loss before
2021-06-04 13:57:43 | [train_policy] epoch #741 | computing gradient
2021-06-04 13:57:43 | [train_policy] epoch #741 | gradient computed
2021-06-04 13:57:43 | [train_policy] epoch #741 | computing descent direction
2021-06-04 13:57:43 | [train_policy] epoch #741 | descent direction computed
2021-06-04 13:57:43 | [train_policy] epoch #741 | backtrack iters: 0
2021-06-04 13:57:43 | [train_policy] epoch #741 | optimization finished
2021-06-04 13:57:43 | [train_policy] epoch #741 | Computing KL after
2021-06-04 13:57:43 | [train_policy] epoch #741 | Computing loss after
2021-06-04 13:57:43 | [train_policy] epoch #741 | Fitting baseline...
2021-06-04 13:57:43 | [train_policy] epoch #741 | Saving snapshot...
2021-06-04 13:57:43 | [train_policy] epoch #741 | Saved
2021-06-04 13:57:43 | [train_policy] epoch #741 | Time 595.24 s
2021-06-04 13:57:43 | [train_policy] epoch #741 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.286246
Evaluation/AverageDiscountedReturn          -41.1398
Evaluation/AverageReturn                    -41.1398
Evaluation/CompletionRate                     0
Evaluation/Iteration                        741
Evaluation/MaxReturn                        -28.5046
Evaluation/MinReturn                        -78.9881
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.75438
Extras/EpisodeRewardMean                    -41.0785
LinearFeatureBaseline/ExplainedVariance       0.859156
PolicyExecTime                                0.213897
ProcessExecTime                               0.0312595
TotalEnvSteps                            750904
policy/Entropy                               -1.90738
policy/KL                                     0.00994985
policy/KLBefore                               0
policy/LossAfter                             -0.0176858
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.148469
policy/dLoss                                  0.0176858
---------------------------------------  ----------------
2021-06-04 13:57:43 | [train_policy] epoch #742 | Obtaining samples for iteration 742...
2021-06-04 13:57:44 | [train_policy] epoch #742 | Logging diagnostics...
2021-06-04 13:57:44 | [train_policy] epoch #742 | Optimizing policy...
2021-06-04 13:57:44 | [train_policy] epoch #742 | Computing loss before
2021-06-04 13:57:44 | [train_policy] epoch #742 | Computing KL before
2021-06-04 13:57:44 | [train_policy] epoch #742 | Optimizing
2021-06-04 13:57:44 | [train_policy] epoch #742 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:44 | [train_policy] epoch #742 | computing loss before
2021-06-04 13:57:44 | [train_policy] epoch #742 | computing gradient
2021-06-04 13:57:44 | [train_policy] epoch #742 | gradient computed
2021-06-04 13:57:44 | [train_policy] epoch #742 | computing descent direction
2021-06-04 13:57:44 | [train_policy] epoch #742 | descent direction computed
2021-06-04 13:57:44 | [train_policy] epoch #742 | backtrack iters: 1
2021-06-04 13:57:44 | [train_policy] epoch #742 | optimization finished
2021-06-04 13:57:44 | [train_policy] epoch #742 | Computing KL after
2021-06-04 13:57:44 | [train_policy] epoch #742 | Computing loss after
2021-06-04 13:57:44 | [train_policy] epoch #742 | Fitting baseline...
2021-06-04 13:57:44 | [train_policy] epoch #742 | Saving snapshot...
2021-06-04 13:57:44 | [train_policy] epoch #742 | Saved
2021-06-04 13:57:44 | [train_policy] epoch #742 | Time 596.04 s
2021-06-04 13:57:44 | [train_policy] epoch #742 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.28523
Evaluation/AverageDiscountedReturn          -64.1338
Evaluation/AverageReturn                    -64.1338
Evaluation/CompletionRate                     0
Evaluation/Iteration                        742
Evaluation/MaxReturn                        -28.0989
Evaluation/MinReturn                      -2062.25
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.675
Extras/EpisodeRewardMean                    -62.099
LinearFeatureBaseline/ExplainedVariance       0.0143187
PolicyExecTime                                0.221161
ProcessExecTime                               0.031184
TotalEnvSteps                            751916
policy/Entropy                               -1.90515
policy/KL                                     0.00648385
policy/KLBefore                               0
policy/LossAfter                             -0.0152628
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.1488
policy/dLoss                                  0.0152628
---------------------------------------  ---------------
2021-06-04 13:57:44 | [train_policy] epoch #743 | Obtaining samples for iteration 743...
2021-06-04 13:57:44 | [train_policy] epoch #743 | Logging diagnostics...
2021-06-04 13:57:44 | [train_policy] epoch #743 | Optimizing policy...
2021-06-04 13:57:44 | [train_policy] epoch #743 | Computing loss before
2021-06-04 13:57:44 | [train_policy] epoch #743 | Computing KL before
2021-06-04 13:57:44 | [train_policy] epoch #743 | Optimizing
2021-06-04 13:57:44 | [train_policy] epoch #743 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:44 | [train_policy] epoch #743 | computing loss before
2021-06-04 13:57:44 | [train_policy] epoch #743 | computing gradient
2021-06-04 13:57:44 | [train_policy] epoch #743 | gradient computed
2021-06-04 13:57:44 | [train_policy] epoch #743 | computing descent direction
2021-06-04 13:57:45 | [train_policy] epoch #743 | descent direction computed
2021-06-04 13:57:45 | [train_policy] epoch #743 | backtrack iters: 0
2021-06-04 13:57:45 | [train_policy] epoch #743 | optimization finished
2021-06-04 13:57:45 | [train_policy] epoch #743 | Computing KL after
2021-06-04 13:57:45 | [train_policy] epoch #743 | Computing loss after
2021-06-04 13:57:45 | [train_policy] epoch #743 | Fitting baseline...
2021-06-04 13:57:45 | [train_policy] epoch #743 | Saving snapshot...
2021-06-04 13:57:45 | [train_policy] epoch #743 | Saved
2021-06-04 13:57:45 | [train_policy] epoch #743 | Time 596.83 s
2021-06-04 13:57:45 | [train_policy] epoch #743 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285401
Evaluation/AverageDiscountedReturn          -41.1163
Evaluation/AverageReturn                    -41.1163
Evaluation/CompletionRate                     0
Evaluation/Iteration                        743
Evaluation/MaxReturn                        -28.6211
Evaluation/MinReturn                        -84.7146
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.41925
Extras/EpisodeRewardMean                    -41.083
LinearFeatureBaseline/ExplainedVariance     -17.6683
PolicyExecTime                                0.223827
ProcessExecTime                               0.0313444
TotalEnvSteps                            752928
policy/Entropy                               -1.89406
policy/KL                                     0.00998361
policy/KLBefore                               0
policy/LossAfter                             -0.0274204
policy/LossBefore                            -4.24065e-08
policy/Perplexity                             0.150459
policy/dLoss                                  0.0274204
---------------------------------------  ----------------
2021-06-04 13:57:45 | [train_policy] epoch #744 | Obtaining samples for iteration 744...
2021-06-04 13:57:45 | [train_policy] epoch #744 | Logging diagnostics...
2021-06-04 13:57:45 | [train_policy] epoch #744 | Optimizing policy...
2021-06-04 13:57:45 | [train_policy] epoch #744 | Computing loss before
2021-06-04 13:57:45 | [train_policy] epoch #744 | Computing KL before
2021-06-04 13:57:45 | [train_policy] epoch #744 | Optimizing
2021-06-04 13:57:45 | [train_policy] epoch #744 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:45 | [train_policy] epoch #744 | computing loss before
2021-06-04 13:57:45 | [train_policy] epoch #744 | computing gradient
2021-06-04 13:57:45 | [train_policy] epoch #744 | gradient computed
2021-06-04 13:57:45 | [train_policy] epoch #744 | computing descent direction
2021-06-04 13:57:45 | [train_policy] epoch #744 | descent direction computed
2021-06-04 13:57:45 | [train_policy] epoch #744 | backtrack iters: 1
2021-06-04 13:57:45 | [train_policy] epoch #744 | optimization finished
2021-06-04 13:57:45 | [train_policy] epoch #744 | Computing KL after
2021-06-04 13:57:45 | [train_policy] epoch #744 | Computing loss after
2021-06-04 13:57:45 | [train_policy] epoch #744 | Fitting baseline...
2021-06-04 13:57:45 | [train_policy] epoch #744 | Saving snapshot...
2021-06-04 13:57:45 | [train_policy] epoch #744 | Saved
2021-06-04 13:57:45 | [train_policy] epoch #744 | Time 597.64 s
2021-06-04 13:57:45 | [train_policy] epoch #744 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284578
Evaluation/AverageDiscountedReturn          -41.464
Evaluation/AverageReturn                    -41.464
Evaluation/CompletionRate                     0
Evaluation/Iteration                        744
Evaluation/MaxReturn                        -28.3753
Evaluation/MinReturn                        -63.9057
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.92466
Extras/EpisodeRewardMean                    -41.4985
LinearFeatureBaseline/ExplainedVariance       0.911914
PolicyExecTime                                0.227753
ProcessExecTime                               0.0312057
TotalEnvSteps                            753940
policy/Entropy                               -1.9205
policy/KL                                     0.00662979
policy/KLBefore                               0
policy/LossAfter                             -0.0128558
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             0.146534
policy/dLoss                                  0.0128558
---------------------------------------  ----------------
2021-06-04 13:57:45 | [train_policy] epoch #745 | Obtaining samples for iteration 745...
2021-06-04 13:57:46 | [train_policy] epoch #745 | Logging diagnostics...
2021-06-04 13:57:46 | [train_policy] epoch #745 | Optimizing policy...
2021-06-04 13:57:46 | [train_policy] epoch #745 | Computing loss before
2021-06-04 13:57:46 | [train_policy] epoch #745 | Computing KL before
2021-06-04 13:57:46 | [train_policy] epoch #745 | Optimizing
2021-06-04 13:57:46 | [train_policy] epoch #745 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:46 | [train_policy] epoch #745 | computing loss before
2021-06-04 13:57:46 | [train_policy] epoch #745 | computing gradient
2021-06-04 13:57:46 | [train_policy] epoch #745 | gradient computed
2021-06-04 13:57:46 | [train_policy] epoch #745 | computing descent direction
2021-06-04 13:57:46 | [train_policy] epoch #745 | descent direction computed
2021-06-04 13:57:46 | [train_policy] epoch #745 | backtrack iters: 0
2021-06-04 13:57:46 | [train_policy] epoch #745 | optimization finished
2021-06-04 13:57:46 | [train_policy] epoch #745 | Computing KL after
2021-06-04 13:57:46 | [train_policy] epoch #745 | Computing loss after
2021-06-04 13:57:46 | [train_policy] epoch #745 | Fitting baseline...
2021-06-04 13:57:46 | [train_policy] epoch #745 | Saving snapshot...
2021-06-04 13:57:46 | [train_policy] epoch #745 | Saved
2021-06-04 13:57:46 | [train_policy] epoch #745 | Time 598.42 s
2021-06-04 13:57:46 | [train_policy] epoch #745 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.284328
Evaluation/AverageDiscountedReturn          -39.6661
Evaluation/AverageReturn                    -39.6661
Evaluation/CompletionRate                     0
Evaluation/Iteration                        745
Evaluation/MaxReturn                        -27.9796
Evaluation/MinReturn                        -82.1941
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.11758
Extras/EpisodeRewardMean                    -39.5632
LinearFeatureBaseline/ExplainedVariance       0.87645
PolicyExecTime                                0.213846
ProcessExecTime                               0.0312092
TotalEnvSteps                            754952
policy/Entropy                               -1.90083
policy/KL                                     0.00982869
policy/KLBefore                               0
policy/LossAfter                             -0.0198487
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.149445
policy/dLoss                                  0.0198487
---------------------------------------  ---------------
2021-06-04 13:57:46 | [train_policy] epoch #746 | Obtaining samples for iteration 746...
2021-06-04 13:57:47 | [train_policy] epoch #746 | Logging diagnostics...
2021-06-04 13:57:47 | [train_policy] epoch #746 | Optimizing policy...
2021-06-04 13:57:47 | [train_policy] epoch #746 | Computing loss before
2021-06-04 13:57:47 | [train_policy] epoch #746 | Computing KL before
2021-06-04 13:57:47 | [train_policy] epoch #746 | Optimizing
2021-06-04 13:57:47 | [train_policy] epoch #746 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:47 | [train_policy] epoch #746 | computing loss before
2021-06-04 13:57:47 | [train_policy] epoch #746 | computing gradient
2021-06-04 13:57:47 | [train_policy] epoch #746 | gradient computed
2021-06-04 13:57:47 | [train_policy] epoch #746 | computing descent direction
2021-06-04 13:57:47 | [train_policy] epoch #746 | descent direction computed
2021-06-04 13:57:47 | [train_policy] epoch #746 | backtrack iters: 1
2021-06-04 13:57:47 | [train_policy] epoch #746 | optimization finished
2021-06-04 13:57:47 | [train_policy] epoch #746 | Computing KL after
2021-06-04 13:57:47 | [train_policy] epoch #746 | Computing loss after
2021-06-04 13:57:47 | [train_policy] epoch #746 | Fitting baseline...
2021-06-04 13:57:47 | [train_policy] epoch #746 | Saving snapshot...
2021-06-04 13:57:47 | [train_policy] epoch #746 | Saved
2021-06-04 13:57:47 | [train_policy] epoch #746 | Time 599.21 s
2021-06-04 13:57:47 | [train_policy] epoch #746 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285095
Evaluation/AverageDiscountedReturn          -40.6233
Evaluation/AverageReturn                    -40.6233
Evaluation/CompletionRate                     0
Evaluation/Iteration                        746
Evaluation/MaxReturn                        -28.2236
Evaluation/MinReturn                        -78.8678
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.97484
Extras/EpisodeRewardMean                    -40.7045
LinearFeatureBaseline/ExplainedVariance       0.876283
PolicyExecTime                                0.211433
ProcessExecTime                               0.0314312
TotalEnvSteps                            755964
policy/Entropy                               -1.88237
policy/KL                                     0.0065053
policy/KLBefore                               0
policy/LossAfter                             -0.0136173
policy/LossBefore                             1.24863e-08
policy/Perplexity                             0.152229
policy/dLoss                                  0.0136173
---------------------------------------  ----------------
2021-06-04 13:57:47 | [train_policy] epoch #747 | Obtaining samples for iteration 747...
2021-06-04 13:57:48 | [train_policy] epoch #747 | Logging diagnostics...
2021-06-04 13:57:48 | [train_policy] epoch #747 | Optimizing policy...
2021-06-04 13:57:48 | [train_policy] epoch #747 | Computing loss before
2021-06-04 13:57:48 | [train_policy] epoch #747 | Computing KL before
2021-06-04 13:57:48 | [train_policy] epoch #747 | Optimizing
2021-06-04 13:57:48 | [train_policy] epoch #747 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:48 | [train_policy] epoch #747 | computing loss before
2021-06-04 13:57:48 | [train_policy] epoch #747 | computing gradient
2021-06-04 13:57:48 | [train_policy] epoch #747 | gradient computed
2021-06-04 13:57:48 | [train_policy] epoch #747 | computing descent direction
2021-06-04 13:57:48 | [train_policy] epoch #747 | descent direction computed
2021-06-04 13:57:48 | [train_policy] epoch #747 | backtrack iters: 0
2021-06-04 13:57:48 | [train_policy] epoch #747 | optimization finished
2021-06-04 13:57:48 | [train_policy] epoch #747 | Computing KL after
2021-06-04 13:57:48 | [train_policy] epoch #747 | Computing loss after
2021-06-04 13:57:48 | [train_policy] epoch #747 | Fitting baseline...
2021-06-04 13:57:48 | [train_policy] epoch #747 | Saving snapshot...
2021-06-04 13:57:48 | [train_policy] epoch #747 | Saved
2021-06-04 13:57:48 | [train_policy] epoch #747 | Time 600.00 s
2021-06-04 13:57:48 | [train_policy] epoch #747 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284068
Evaluation/AverageDiscountedReturn          -42.7724
Evaluation/AverageReturn                    -42.7724
Evaluation/CompletionRate                     0
Evaluation/Iteration                        747
Evaluation/MaxReturn                        -29.6927
Evaluation/MinReturn                        -84.7745
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.31841
Extras/EpisodeRewardMean                    -42.8545
LinearFeatureBaseline/ExplainedVariance       0.84214
PolicyExecTime                                0.226934
ProcessExecTime                               0.0311253
TotalEnvSteps                            756976
policy/Entropy                               -1.88392
policy/KL                                     0.00984046
policy/KLBefore                               0
policy/LossAfter                             -0.0224474
policy/LossBefore                             3.18048e-09
policy/Perplexity                             0.151993
policy/dLoss                                  0.0224474
---------------------------------------  ----------------
2021-06-04 13:57:48 | [train_policy] epoch #748 | Obtaining samples for iteration 748...
2021-06-04 13:57:48 | [train_policy] epoch #748 | Logging diagnostics...
2021-06-04 13:57:48 | [train_policy] epoch #748 | Optimizing policy...
2021-06-04 13:57:48 | [train_policy] epoch #748 | Computing loss before
2021-06-04 13:57:48 | [train_policy] epoch #748 | Computing KL before
2021-06-04 13:57:48 | [train_policy] epoch #748 | Optimizing
2021-06-04 13:57:48 | [train_policy] epoch #748 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:48 | [train_policy] epoch #748 | computing loss before
2021-06-04 13:57:48 | [train_policy] epoch #748 | computing gradient
2021-06-04 13:57:48 | [train_policy] epoch #748 | gradient computed
2021-06-04 13:57:48 | [train_policy] epoch #748 | computing descent direction
2021-06-04 13:57:49 | [train_policy] epoch #748 | descent direction computed
2021-06-04 13:57:49 | [train_policy] epoch #748 | backtrack iters: 0
2021-06-04 13:57:49 | [train_policy] epoch #748 | optimization finished
2021-06-04 13:57:49 | [train_policy] epoch #748 | Computing KL after
2021-06-04 13:57:49 | [train_policy] epoch #748 | Computing loss after
2021-06-04 13:57:49 | [train_policy] epoch #748 | Fitting baseline...
2021-06-04 13:57:49 | [train_policy] epoch #748 | Saving snapshot...
2021-06-04 13:57:49 | [train_policy] epoch #748 | Saved
2021-06-04 13:57:49 | [train_policy] epoch #748 | Time 600.80 s
2021-06-04 13:57:49 | [train_policy] epoch #748 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287028
Evaluation/AverageDiscountedReturn          -43.0892
Evaluation/AverageReturn                    -43.0892
Evaluation/CompletionRate                     0
Evaluation/Iteration                        748
Evaluation/MaxReturn                        -27.4734
Evaluation/MinReturn                        -81.4816
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.12794
Extras/EpisodeRewardMean                    -43.5495
LinearFeatureBaseline/ExplainedVariance       0.873478
PolicyExecTime                                0.232288
ProcessExecTime                               0.0314767
TotalEnvSteps                            757988
policy/Entropy                               -1.86313
policy/KL                                     0.00974252
policy/KLBefore                               0
policy/LossAfter                             -0.0197275
policy/LossBefore                            -3.06269e-09
policy/Perplexity                             0.155186
policy/dLoss                                  0.0197275
---------------------------------------  ----------------
2021-06-04 13:57:49 | [train_policy] epoch #749 | Obtaining samples for iteration 749...
2021-06-04 13:57:49 | [train_policy] epoch #749 | Logging diagnostics...
2021-06-04 13:57:49 | [train_policy] epoch #749 | Optimizing policy...
2021-06-04 13:57:49 | [train_policy] epoch #749 | Computing loss before
2021-06-04 13:57:49 | [train_policy] epoch #749 | Computing KL before
2021-06-04 13:57:49 | [train_policy] epoch #749 | Optimizing
2021-06-04 13:57:49 | [train_policy] epoch #749 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:49 | [train_policy] epoch #749 | computing loss before
2021-06-04 13:57:49 | [train_policy] epoch #749 | computing gradient
2021-06-04 13:57:49 | [train_policy] epoch #749 | gradient computed
2021-06-04 13:57:49 | [train_policy] epoch #749 | computing descent direction
2021-06-04 13:57:49 | [train_policy] epoch #749 | descent direction computed
2021-06-04 13:57:49 | [train_policy] epoch #749 | backtrack iters: 1
2021-06-04 13:57:49 | [train_policy] epoch #749 | optimization finished
2021-06-04 13:57:49 | [train_policy] epoch #749 | Computing KL after
2021-06-04 13:57:49 | [train_policy] epoch #749 | Computing loss after
2021-06-04 13:57:49 | [train_policy] epoch #749 | Fitting baseline...
2021-06-04 13:57:49 | [train_policy] epoch #749 | Saving snapshot...
2021-06-04 13:57:49 | [train_policy] epoch #749 | Saved
2021-06-04 13:57:49 | [train_policy] epoch #749 | Time 601.62 s
2021-06-04 13:57:49 | [train_policy] epoch #749 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285876
Evaluation/AverageDiscountedReturn          -41.2733
Evaluation/AverageReturn                    -41.2733
Evaluation/CompletionRate                     0
Evaluation/Iteration                        749
Evaluation/MaxReturn                        -28.1729
Evaluation/MinReturn                        -63.9466
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.26192
Extras/EpisodeRewardMean                    -41.1501
LinearFeatureBaseline/ExplainedVariance       0.883578
PolicyExecTime                                0.23688
ProcessExecTime                               0.0313408
TotalEnvSteps                            759000
policy/Entropy                               -1.8552
policy/KL                                     0.00662607
policy/KLBefore                               0
policy/LossAfter                             -0.0156867
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.156422
policy/dLoss                                  0.0156867
---------------------------------------  ----------------
2021-06-04 13:57:49 | [train_policy] epoch #750 | Obtaining samples for iteration 750...
2021-06-04 13:57:50 | [train_policy] epoch #750 | Logging diagnostics...
2021-06-04 13:57:50 | [train_policy] epoch #750 | Optimizing policy...
2021-06-04 13:57:50 | [train_policy] epoch #750 | Computing loss before
2021-06-04 13:57:50 | [train_policy] epoch #750 | Computing KL before
2021-06-04 13:57:50 | [train_policy] epoch #750 | Optimizing
2021-06-04 13:57:50 | [train_policy] epoch #750 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:50 | [train_policy] epoch #750 | computing loss before
2021-06-04 13:57:50 | [train_policy] epoch #750 | computing gradient
2021-06-04 13:57:50 | [train_policy] epoch #750 | gradient computed
2021-06-04 13:57:50 | [train_policy] epoch #750 | computing descent direction
2021-06-04 13:57:50 | [train_policy] epoch #750 | descent direction computed
2021-06-04 13:57:50 | [train_policy] epoch #750 | backtrack iters: 1
2021-06-04 13:57:50 | [train_policy] epoch #750 | optimization finished
2021-06-04 13:57:50 | [train_policy] epoch #750 | Computing KL after
2021-06-04 13:57:50 | [train_policy] epoch #750 | Computing loss after
2021-06-04 13:57:50 | [train_policy] epoch #750 | Fitting baseline...
2021-06-04 13:57:50 | [train_policy] epoch #750 | Saving snapshot...
2021-06-04 13:57:50 | [train_policy] epoch #750 | Saved
2021-06-04 13:57:50 | [train_policy] epoch #750 | Time 602.43 s
2021-06-04 13:57:50 | [train_policy] epoch #750 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285933
Evaluation/AverageDiscountedReturn          -41.3189
Evaluation/AverageReturn                    -41.3189
Evaluation/CompletionRate                     0
Evaluation/Iteration                        750
Evaluation/MaxReturn                        -28.4764
Evaluation/MinReturn                        -63.9199
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.5031
Extras/EpisodeRewardMean                    -41.2467
LinearFeatureBaseline/ExplainedVariance       0.898773
PolicyExecTime                                0.226596
ProcessExecTime                               0.0312731
TotalEnvSteps                            760012
policy/Entropy                               -1.8567
policy/KL                                     0.00641591
policy/KLBefore                               0
policy/LossAfter                             -0.0137818
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.156187
policy/dLoss                                  0.0137818
---------------------------------------  ----------------
2021-06-04 13:57:50 | [train_policy] epoch #751 | Obtaining samples for iteration 751...
2021-06-04 13:57:51 | [train_policy] epoch #751 | Logging diagnostics...
2021-06-04 13:57:51 | [train_policy] epoch #751 | Optimizing policy...
2021-06-04 13:57:51 | [train_policy] epoch #751 | Computing loss before
2021-06-04 13:57:51 | [train_policy] epoch #751 | Computing KL before
2021-06-04 13:57:51 | [train_policy] epoch #751 | Optimizing
2021-06-04 13:57:51 | [train_policy] epoch #751 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:51 | [train_policy] epoch #751 | computing loss before
2021-06-04 13:57:51 | [train_policy] epoch #751 | computing gradient
2021-06-04 13:57:51 | [train_policy] epoch #751 | gradient computed
2021-06-04 13:57:51 | [train_policy] epoch #751 | computing descent direction
2021-06-04 13:57:51 | [train_policy] epoch #751 | descent direction computed
2021-06-04 13:57:51 | [train_policy] epoch #751 | backtrack iters: 1
2021-06-04 13:57:51 | [train_policy] epoch #751 | optimization finished
2021-06-04 13:57:51 | [train_policy] epoch #751 | Computing KL after
2021-06-04 13:57:51 | [train_policy] epoch #751 | Computing loss after
2021-06-04 13:57:51 | [train_policy] epoch #751 | Fitting baseline...
2021-06-04 13:57:51 | [train_policy] epoch #751 | Saving snapshot...
2021-06-04 13:57:51 | [train_policy] epoch #751 | Saved
2021-06-04 13:57:51 | [train_policy] epoch #751 | Time 603.23 s
2021-06-04 13:57:51 | [train_policy] epoch #751 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.287467
Evaluation/AverageDiscountedReturn          -40.5484
Evaluation/AverageReturn                    -40.5484
Evaluation/CompletionRate                     0
Evaluation/Iteration                        751
Evaluation/MaxReturn                        -28.8319
Evaluation/MinReturn                        -77.2697
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.21179
Extras/EpisodeRewardMean                    -40.7154
LinearFeatureBaseline/ExplainedVariance       0.894095
PolicyExecTime                                0.22535
ProcessExecTime                               0.0315359
TotalEnvSteps                            761024
policy/Entropy                               -1.84449
policy/KL                                     0.00650114
policy/KLBefore                               0
policy/LossAfter                             -0.0174886
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             0.158106
policy/dLoss                                  0.0174886
---------------------------------------  ---------------
2021-06-04 13:57:51 | [train_policy] epoch #752 | Obtaining samples for iteration 752...
2021-06-04 13:57:52 | [train_policy] epoch #752 | Logging diagnostics...
2021-06-04 13:57:52 | [train_policy] epoch #752 | Optimizing policy...
2021-06-04 13:57:52 | [train_policy] epoch #752 | Computing loss before
2021-06-04 13:57:52 | [train_policy] epoch #752 | Computing KL before
2021-06-04 13:57:52 | [train_policy] epoch #752 | Optimizing
2021-06-04 13:57:52 | [train_policy] epoch #752 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:52 | [train_policy] epoch #752 | computing loss before
2021-06-04 13:57:52 | [train_policy] epoch #752 | computing gradient
2021-06-04 13:57:52 | [train_policy] epoch #752 | gradient computed
2021-06-04 13:57:52 | [train_policy] epoch #752 | computing descent direction
2021-06-04 13:57:52 | [train_policy] epoch #752 | descent direction computed
2021-06-04 13:57:52 | [train_policy] epoch #752 | backtrack iters: 1
2021-06-04 13:57:52 | [train_policy] epoch #752 | optimization finished
2021-06-04 13:57:52 | [train_policy] epoch #752 | Computing KL after
2021-06-04 13:57:52 | [train_policy] epoch #752 | Computing loss after
2021-06-04 13:57:52 | [train_policy] epoch #752 | Fitting baseline...
2021-06-04 13:57:52 | [train_policy] epoch #752 | Saving snapshot...
2021-06-04 13:57:52 | [train_policy] epoch #752 | Saved
2021-06-04 13:57:52 | [train_policy] epoch #752 | Time 604.04 s
2021-06-04 13:57:52 | [train_policy] epoch #752 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28517
Evaluation/AverageDiscountedReturn          -42.1647
Evaluation/AverageReturn                    -42.1647
Evaluation/CompletionRate                     0
Evaluation/Iteration                        752
Evaluation/MaxReturn                        -27.8945
Evaluation/MinReturn                        -78.8242
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.59939
Extras/EpisodeRewardMean                    -42.0721
LinearFeatureBaseline/ExplainedVariance       0.867301
PolicyExecTime                                0.233784
ProcessExecTime                               0.0313029
TotalEnvSteps                            762036
policy/Entropy                               -1.84904
policy/KL                                     0.00649427
policy/KLBefore                               0
policy/LossAfter                             -0.0180892
policy/LossBefore                            -7.06774e-10
policy/Perplexity                             0.157388
policy/dLoss                                  0.0180892
---------------------------------------  ----------------
2021-06-04 13:57:52 | [train_policy] epoch #753 | Obtaining samples for iteration 753...
2021-06-04 13:57:52 | [train_policy] epoch #753 | Logging diagnostics...
2021-06-04 13:57:52 | [train_policy] epoch #753 | Optimizing policy...
2021-06-04 13:57:52 | [train_policy] epoch #753 | Computing loss before
2021-06-04 13:57:52 | [train_policy] epoch #753 | Computing KL before
2021-06-04 13:57:52 | [train_policy] epoch #753 | Optimizing
2021-06-04 13:57:52 | [train_policy] epoch #753 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:52 | [train_policy] epoch #753 | computing loss before
2021-06-04 13:57:52 | [train_policy] epoch #753 | computing gradient
2021-06-04 13:57:52 | [train_policy] epoch #753 | gradient computed
2021-06-04 13:57:52 | [train_policy] epoch #753 | computing descent direction
2021-06-04 13:57:53 | [train_policy] epoch #753 | descent direction computed
2021-06-04 13:57:53 | [train_policy] epoch #753 | backtrack iters: 1
2021-06-04 13:57:53 | [train_policy] epoch #753 | optimization finished
2021-06-04 13:57:53 | [train_policy] epoch #753 | Computing KL after
2021-06-04 13:57:53 | [train_policy] epoch #753 | Computing loss after
2021-06-04 13:57:53 | [train_policy] epoch #753 | Fitting baseline...
2021-06-04 13:57:53 | [train_policy] epoch #753 | Saving snapshot...
2021-06-04 13:57:53 | [train_policy] epoch #753 | Saved
2021-06-04 13:57:53 | [train_policy] epoch #753 | Time 604.84 s
2021-06-04 13:57:53 | [train_policy] epoch #753 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.284344
Evaluation/AverageDiscountedReturn          -41.0438
Evaluation/AverageReturn                    -41.0438
Evaluation/CompletionRate                     0
Evaluation/Iteration                        753
Evaluation/MaxReturn                        -28.2175
Evaluation/MinReturn                        -63.9304
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.13248
Extras/EpisodeRewardMean                    -40.9179
LinearFeatureBaseline/ExplainedVariance       0.902016
PolicyExecTime                                0.233786
ProcessExecTime                               0.0311966
TotalEnvSteps                            763048
policy/Entropy                               -1.85224
policy/KL                                     0.00647946
policy/KLBefore                               0
policy/LossAfter                             -0.0161173
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.156886
policy/dLoss                                  0.0161173
---------------------------------------  ---------------
2021-06-04 13:57:53 | [train_policy] epoch #754 | Obtaining samples for iteration 754...
2021-06-04 13:57:53 | [train_policy] epoch #754 | Logging diagnostics...
2021-06-04 13:57:53 | [train_policy] epoch #754 | Optimizing policy...
2021-06-04 13:57:53 | [train_policy] epoch #754 | Computing loss before
2021-06-04 13:57:53 | [train_policy] epoch #754 | Computing KL before
2021-06-04 13:57:53 | [train_policy] epoch #754 | Optimizing
2021-06-04 13:57:53 | [train_policy] epoch #754 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:53 | [train_policy] epoch #754 | computing loss before
2021-06-04 13:57:53 | [train_policy] epoch #754 | computing gradient
2021-06-04 13:57:53 | [train_policy] epoch #754 | gradient computed
2021-06-04 13:57:53 | [train_policy] epoch #754 | computing descent direction
2021-06-04 13:57:53 | [train_policy] epoch #754 | descent direction computed
2021-06-04 13:57:53 | [train_policy] epoch #754 | backtrack iters: 1
2021-06-04 13:57:53 | [train_policy] epoch #754 | optimization finished
2021-06-04 13:57:53 | [train_policy] epoch #754 | Computing KL after
2021-06-04 13:57:53 | [train_policy] epoch #754 | Computing loss after
2021-06-04 13:57:53 | [train_policy] epoch #754 | Fitting baseline...
2021-06-04 13:57:53 | [train_policy] epoch #754 | Saving snapshot...
2021-06-04 13:57:53 | [train_policy] epoch #754 | Saved
2021-06-04 13:57:53 | [train_policy] epoch #754 | Time 605.65 s
2021-06-04 13:57:53 | [train_policy] epoch #754 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28504
Evaluation/AverageDiscountedReturn          -41.574
Evaluation/AverageReturn                    -41.574
Evaluation/CompletionRate                     0
Evaluation/Iteration                        754
Evaluation/MaxReturn                        -29.5729
Evaluation/MinReturn                        -78.3911
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.91207
Extras/EpisodeRewardMean                    -41.6231
LinearFeatureBaseline/ExplainedVariance       0.881023
PolicyExecTime                                0.211809
ProcessExecTime                               0.0311944
TotalEnvSteps                            764060
policy/Entropy                               -1.86017
policy/KL                                     0.00642932
policy/KLBefore                               0
policy/LossAfter                             -0.0153616
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.155646
policy/dLoss                                  0.0153616
---------------------------------------  ----------------
2021-06-04 13:57:53 | [train_policy] epoch #755 | Obtaining samples for iteration 755...
2021-06-04 13:57:54 | [train_policy] epoch #755 | Logging diagnostics...
2021-06-04 13:57:54 | [train_policy] epoch #755 | Optimizing policy...
2021-06-04 13:57:54 | [train_policy] epoch #755 | Computing loss before
2021-06-04 13:57:54 | [train_policy] epoch #755 | Computing KL before
2021-06-04 13:57:54 | [train_policy] epoch #755 | Optimizing
2021-06-04 13:57:54 | [train_policy] epoch #755 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:54 | [train_policy] epoch #755 | computing loss before
2021-06-04 13:57:54 | [train_policy] epoch #755 | computing gradient
2021-06-04 13:57:54 | [train_policy] epoch #755 | gradient computed
2021-06-04 13:57:54 | [train_policy] epoch #755 | computing descent direction
2021-06-04 13:57:54 | [train_policy] epoch #755 | descent direction computed
2021-06-04 13:57:54 | [train_policy] epoch #755 | backtrack iters: 1
2021-06-04 13:57:54 | [train_policy] epoch #755 | optimization finished
2021-06-04 13:57:54 | [train_policy] epoch #755 | Computing KL after
2021-06-04 13:57:54 | [train_policy] epoch #755 | Computing loss after
2021-06-04 13:57:54 | [train_policy] epoch #755 | Fitting baseline...
2021-06-04 13:57:54 | [train_policy] epoch #755 | Saving snapshot...
2021-06-04 13:57:54 | [train_policy] epoch #755 | Saved
2021-06-04 13:57:54 | [train_policy] epoch #755 | Time 606.45 s
2021-06-04 13:57:54 | [train_policy] epoch #755 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28742
Evaluation/AverageDiscountedReturn          -42.3489
Evaluation/AverageReturn                    -42.3489
Evaluation/CompletionRate                     0
Evaluation/Iteration                        755
Evaluation/MaxReturn                        -29.0468
Evaluation/MinReturn                        -77.6719
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.59428
Extras/EpisodeRewardMean                    -42.1807
LinearFeatureBaseline/ExplainedVariance       0.891601
PolicyExecTime                                0.228323
ProcessExecTime                               0.0315185
TotalEnvSteps                            765072
policy/Entropy                               -1.86303
policy/KL                                     0.00641929
policy/KLBefore                               0
policy/LossAfter                             -0.0140913
policy/LossBefore                            -3.53387e-09
policy/Perplexity                             0.155201
policy/dLoss                                  0.0140913
---------------------------------------  ----------------
2021-06-04 13:57:54 | [train_policy] epoch #756 | Obtaining samples for iteration 756...
2021-06-04 13:57:55 | [train_policy] epoch #756 | Logging diagnostics...
2021-06-04 13:57:55 | [train_policy] epoch #756 | Optimizing policy...
2021-06-04 13:57:55 | [train_policy] epoch #756 | Computing loss before
2021-06-04 13:57:55 | [train_policy] epoch #756 | Computing KL before
2021-06-04 13:57:55 | [train_policy] epoch #756 | Optimizing
2021-06-04 13:57:55 | [train_policy] epoch #756 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:55 | [train_policy] epoch #756 | computing loss before
2021-06-04 13:57:55 | [train_policy] epoch #756 | computing gradient
2021-06-04 13:57:55 | [train_policy] epoch #756 | gradient computed
2021-06-04 13:57:55 | [train_policy] epoch #756 | computing descent direction
2021-06-04 13:57:55 | [train_policy] epoch #756 | descent direction computed
2021-06-04 13:57:55 | [train_policy] epoch #756 | backtrack iters: 0
2021-06-04 13:57:55 | [train_policy] epoch #756 | optimization finished
2021-06-04 13:57:55 | [train_policy] epoch #756 | Computing KL after
2021-06-04 13:57:55 | [train_policy] epoch #756 | Computing loss after
2021-06-04 13:57:55 | [train_policy] epoch #756 | Fitting baseline...
2021-06-04 13:57:55 | [train_policy] epoch #756 | Saving snapshot...
2021-06-04 13:57:55 | [train_policy] epoch #756 | Saved
2021-06-04 13:57:55 | [train_policy] epoch #756 | Time 607.24 s
2021-06-04 13:57:55 | [train_policy] epoch #756 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28572
Evaluation/AverageDiscountedReturn          -42.477
Evaluation/AverageReturn                    -42.477
Evaluation/CompletionRate                     0
Evaluation/Iteration                        756
Evaluation/MaxReturn                        -29.3841
Evaluation/MinReturn                        -63.8942
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.2525
Extras/EpisodeRewardMean                    -42.5906
LinearFeatureBaseline/ExplainedVariance       0.894385
PolicyExecTime                                0.220462
ProcessExecTime                               0.0313015
TotalEnvSteps                            766084
policy/Entropy                               -1.83564
policy/KL                                     0.00991553
policy/KLBefore                               0
policy/LossAfter                             -0.0205605
policy/LossBefore                             3.65167e-09
policy/Perplexity                             0.159511
policy/dLoss                                  0.0205605
---------------------------------------  ----------------
2021-06-04 13:57:55 | [train_policy] epoch #757 | Obtaining samples for iteration 757...
2021-06-04 13:57:56 | [train_policy] epoch #757 | Logging diagnostics...
2021-06-04 13:57:56 | [train_policy] epoch #757 | Optimizing policy...
2021-06-04 13:57:56 | [train_policy] epoch #757 | Computing loss before
2021-06-04 13:57:56 | [train_policy] epoch #757 | Computing KL before
2021-06-04 13:57:56 | [train_policy] epoch #757 | Optimizing
2021-06-04 13:57:56 | [train_policy] epoch #757 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:56 | [train_policy] epoch #757 | computing loss before
2021-06-04 13:57:56 | [train_policy] epoch #757 | computing gradient
2021-06-04 13:57:56 | [train_policy] epoch #757 | gradient computed
2021-06-04 13:57:56 | [train_policy] epoch #757 | computing descent direction
2021-06-04 13:57:56 | [train_policy] epoch #757 | descent direction computed
2021-06-04 13:57:56 | [train_policy] epoch #757 | backtrack iters: 0
2021-06-04 13:57:56 | [train_policy] epoch #757 | optimization finished
2021-06-04 13:57:56 | [train_policy] epoch #757 | Computing KL after
2021-06-04 13:57:56 | [train_policy] epoch #757 | Computing loss after
2021-06-04 13:57:56 | [train_policy] epoch #757 | Fitting baseline...
2021-06-04 13:57:56 | [train_policy] epoch #757 | Saving snapshot...
2021-06-04 13:57:56 | [train_policy] epoch #757 | Saved
2021-06-04 13:57:56 | [train_policy] epoch #757 | Time 608.04 s
2021-06-04 13:57:56 | [train_policy] epoch #757 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284853
Evaluation/AverageDiscountedReturn          -42.0682
Evaluation/AverageReturn                    -42.0682
Evaluation/CompletionRate                     0
Evaluation/Iteration                        757
Evaluation/MaxReturn                        -28.4191
Evaluation/MinReturn                        -87.709
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.00398
Extras/EpisodeRewardMean                    -42.3944
LinearFeatureBaseline/ExplainedVariance       0.864858
PolicyExecTime                                0.227987
ProcessExecTime                               0.0313048
TotalEnvSteps                            767096
policy/Entropy                               -1.82469
policy/KL                                     0.00994519
policy/KLBefore                               0
policy/LossAfter                             -0.0252039
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             0.161268
policy/dLoss                                  0.0252039
---------------------------------------  ----------------
2021-06-04 13:57:56 | [train_policy] epoch #758 | Obtaining samples for iteration 758...
2021-06-04 13:57:56 | [train_policy] epoch #758 | Logging diagnostics...
2021-06-04 13:57:56 | [train_policy] epoch #758 | Optimizing policy...
2021-06-04 13:57:56 | [train_policy] epoch #758 | Computing loss before
2021-06-04 13:57:56 | [train_policy] epoch #758 | Computing KL before
2021-06-04 13:57:56 | [train_policy] epoch #758 | Optimizing
2021-06-04 13:57:56 | [train_policy] epoch #758 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:56 | [train_policy] epoch #758 | computing loss before
2021-06-04 13:57:56 | [train_policy] epoch #758 | computing gradient
2021-06-04 13:57:56 | [train_policy] epoch #758 | gradient computed
2021-06-04 13:57:56 | [train_policy] epoch #758 | computing descent direction
2021-06-04 13:57:57 | [train_policy] epoch #758 | descent direction computed
2021-06-04 13:57:57 | [train_policy] epoch #758 | backtrack iters: 1
2021-06-04 13:57:57 | [train_policy] epoch #758 | optimization finished
2021-06-04 13:57:57 | [train_policy] epoch #758 | Computing KL after
2021-06-04 13:57:57 | [train_policy] epoch #758 | Computing loss after
2021-06-04 13:57:57 | [train_policy] epoch #758 | Fitting baseline...
2021-06-04 13:57:57 | [train_policy] epoch #758 | Saving snapshot...
2021-06-04 13:57:57 | [train_policy] epoch #758 | Saved
2021-06-04 13:57:57 | [train_policy] epoch #758 | Time 608.83 s
2021-06-04 13:57:57 | [train_policy] epoch #758 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284083
Evaluation/AverageDiscountedReturn          -40.6186
Evaluation/AverageReturn                    -40.6186
Evaluation/CompletionRate                     0
Evaluation/Iteration                        758
Evaluation/MaxReturn                        -28.9295
Evaluation/MinReturn                        -84.5544
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.50724
Extras/EpisodeRewardMean                    -40.6211
LinearFeatureBaseline/ExplainedVariance       0.850997
PolicyExecTime                                0.219741
ProcessExecTime                               0.031178
TotalEnvSteps                            768108
policy/Entropy                               -1.83215
policy/KL                                     0.00647014
policy/KLBefore                               0
policy/LossAfter                             -0.0173549
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.160068
policy/dLoss                                  0.0173549
---------------------------------------  ----------------
2021-06-04 13:57:57 | [train_policy] epoch #759 | Obtaining samples for iteration 759...
2021-06-04 13:57:57 | [train_policy] epoch #759 | Logging diagnostics...
2021-06-04 13:57:57 | [train_policy] epoch #759 | Optimizing policy...
2021-06-04 13:57:57 | [train_policy] epoch #759 | Computing loss before
2021-06-04 13:57:57 | [train_policy] epoch #759 | Computing KL before
2021-06-04 13:57:57 | [train_policy] epoch #759 | Optimizing
2021-06-04 13:57:57 | [train_policy] epoch #759 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:57 | [train_policy] epoch #759 | computing loss before
2021-06-04 13:57:57 | [train_policy] epoch #759 | computing gradient
2021-06-04 13:57:57 | [train_policy] epoch #759 | gradient computed
2021-06-04 13:57:57 | [train_policy] epoch #759 | computing descent direction
2021-06-04 13:57:57 | [train_policy] epoch #759 | descent direction computed
2021-06-04 13:57:57 | [train_policy] epoch #759 | backtrack iters: 0
2021-06-04 13:57:57 | [train_policy] epoch #759 | optimization finished
2021-06-04 13:57:57 | [train_policy] epoch #759 | Computing KL after
2021-06-04 13:57:57 | [train_policy] epoch #759 | Computing loss after
2021-06-04 13:57:57 | [train_policy] epoch #759 | Fitting baseline...
2021-06-04 13:57:57 | [train_policy] epoch #759 | Saving snapshot...
2021-06-04 13:57:57 | [train_policy] epoch #759 | Saved
2021-06-04 13:57:57 | [train_policy] epoch #759 | Time 609.61 s
2021-06-04 13:57:57 | [train_policy] epoch #759 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285114
Evaluation/AverageDiscountedReturn          -40.3475
Evaluation/AverageReturn                    -40.3475
Evaluation/CompletionRate                     0
Evaluation/Iteration                        759
Evaluation/MaxReturn                        -31.5401
Evaluation/MinReturn                        -63.947
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.39625
Extras/EpisodeRewardMean                    -40.7446
LinearFeatureBaseline/ExplainedVariance       0.908141
PolicyExecTime                                0.222848
ProcessExecTime                               0.0312133
TotalEnvSteps                            769120
policy/Entropy                               -1.83288
policy/KL                                     0.00994033
policy/KLBefore                               0
policy/LossAfter                             -0.0178812
policy/LossBefore                            -3.06269e-09
policy/Perplexity                             0.159952
policy/dLoss                                  0.0178812
---------------------------------------  ----------------
2021-06-04 13:57:57 | [train_policy] epoch #760 | Obtaining samples for iteration 760...
2021-06-04 13:57:58 | [train_policy] epoch #760 | Logging diagnostics...
2021-06-04 13:57:58 | [train_policy] epoch #760 | Optimizing policy...
2021-06-04 13:57:58 | [train_policy] epoch #760 | Computing loss before
2021-06-04 13:57:58 | [train_policy] epoch #760 | Computing KL before
2021-06-04 13:57:58 | [train_policy] epoch #760 | Optimizing
2021-06-04 13:57:58 | [train_policy] epoch #760 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:58 | [train_policy] epoch #760 | computing loss before
2021-06-04 13:57:58 | [train_policy] epoch #760 | computing gradient
2021-06-04 13:57:58 | [train_policy] epoch #760 | gradient computed
2021-06-04 13:57:58 | [train_policy] epoch #760 | computing descent direction
2021-06-04 13:57:58 | [train_policy] epoch #760 | descent direction computed
2021-06-04 13:57:58 | [train_policy] epoch #760 | backtrack iters: 0
2021-06-04 13:57:58 | [train_policy] epoch #760 | optimization finished
2021-06-04 13:57:58 | [train_policy] epoch #760 | Computing KL after
2021-06-04 13:57:58 | [train_policy] epoch #760 | Computing loss after
2021-06-04 13:57:58 | [train_policy] epoch #760 | Fitting baseline...
2021-06-04 13:57:58 | [train_policy] epoch #760 | Saving snapshot...
2021-06-04 13:57:58 | [train_policy] epoch #760 | Saved
2021-06-04 13:57:58 | [train_policy] epoch #760 | Time 610.42 s
2021-06-04 13:57:58 | [train_policy] epoch #760 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286206
Evaluation/AverageDiscountedReturn          -40.764
Evaluation/AverageReturn                    -40.764
Evaluation/CompletionRate                     0
Evaluation/Iteration                        760
Evaluation/MaxReturn                        -27.5528
Evaluation/MinReturn                        -98.9629
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2328
Extras/EpisodeRewardMean                    -41.2826
LinearFeatureBaseline/ExplainedVariance       0.816525
PolicyExecTime                                0.225774
ProcessExecTime                               0.0313737
TotalEnvSteps                            770132
policy/Entropy                               -1.85071
policy/KL                                     0.00994282
policy/KLBefore                               0
policy/LossAfter                             -0.0266069
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             0.157125
policy/dLoss                                  0.0266069
---------------------------------------  ----------------
2021-06-04 13:57:58 | [train_policy] epoch #761 | Obtaining samples for iteration 761...
2021-06-04 13:57:59 | [train_policy] epoch #761 | Logging diagnostics...
2021-06-04 13:57:59 | [train_policy] epoch #761 | Optimizing policy...
2021-06-04 13:57:59 | [train_policy] epoch #761 | Computing loss before
2021-06-04 13:57:59 | [train_policy] epoch #761 | Computing KL before
2021-06-04 13:57:59 | [train_policy] epoch #761 | Optimizing
2021-06-04 13:57:59 | [train_policy] epoch #761 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:57:59 | [train_policy] epoch #761 | computing loss before
2021-06-04 13:57:59 | [train_policy] epoch #761 | computing gradient
2021-06-04 13:57:59 | [train_policy] epoch #761 | gradient computed
2021-06-04 13:57:59 | [train_policy] epoch #761 | computing descent direction
2021-06-04 13:57:59 | [train_policy] epoch #761 | descent direction computed
2021-06-04 13:57:59 | [train_policy] epoch #761 | backtrack iters: 1
2021-06-04 13:57:59 | [train_policy] epoch #761 | optimization finished
2021-06-04 13:57:59 | [train_policy] epoch #761 | Computing KL after
2021-06-04 13:57:59 | [train_policy] epoch #761 | Computing loss after
2021-06-04 13:57:59 | [train_policy] epoch #761 | Fitting baseline...
2021-06-04 13:57:59 | [train_policy] epoch #761 | Saving snapshot...
2021-06-04 13:57:59 | [train_policy] epoch #761 | Saved
2021-06-04 13:57:59 | [train_policy] epoch #761 | Time 611.20 s
2021-06-04 13:57:59 | [train_policy] epoch #761 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.284195
Evaluation/AverageDiscountedReturn          -41.4345
Evaluation/AverageReturn                    -41.4345
Evaluation/CompletionRate                     0
Evaluation/Iteration                        761
Evaluation/MaxReturn                        -28.4217
Evaluation/MinReturn                        -63.9944
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.61041
Extras/EpisodeRewardMean                    -41.1041
LinearFeatureBaseline/ExplainedVariance       0.886048
PolicyExecTime                                0.216868
ProcessExecTime                               0.0313692
TotalEnvSteps                            771144
policy/Entropy                               -1.84989
policy/KL                                     0.00644488
policy/KLBefore                               0
policy/LossAfter                             -0.019269
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.157254
policy/dLoss                                  0.019269
---------------------------------------  ---------------
2021-06-04 13:57:59 | [train_policy] epoch #762 | Obtaining samples for iteration 762...
2021-06-04 13:58:00 | [train_policy] epoch #762 | Logging diagnostics...
2021-06-04 13:58:00 | [train_policy] epoch #762 | Optimizing policy...
2021-06-04 13:58:00 | [train_policy] epoch #762 | Computing loss before
2021-06-04 13:58:00 | [train_policy] epoch #762 | Computing KL before
2021-06-04 13:58:00 | [train_policy] epoch #762 | Optimizing
2021-06-04 13:58:00 | [train_policy] epoch #762 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:00 | [train_policy] epoch #762 | computing loss before
2021-06-04 13:58:00 | [train_policy] epoch #762 | computing gradient
2021-06-04 13:58:00 | [train_policy] epoch #762 | gradient computed
2021-06-04 13:58:00 | [train_policy] epoch #762 | computing descent direction
2021-06-04 13:58:00 | [train_policy] epoch #762 | descent direction computed
2021-06-04 13:58:00 | [train_policy] epoch #762 | backtrack iters: 1
2021-06-04 13:58:00 | [train_policy] epoch #762 | optimization finished
2021-06-04 13:58:00 | [train_policy] epoch #762 | Computing KL after
2021-06-04 13:58:00 | [train_policy] epoch #762 | Computing loss after
2021-06-04 13:58:00 | [train_policy] epoch #762 | Fitting baseline...
2021-06-04 13:58:00 | [train_policy] epoch #762 | Saving snapshot...
2021-06-04 13:58:00 | [train_policy] epoch #762 | Saved
2021-06-04 13:58:00 | [train_policy] epoch #762 | Time 611.98 s
2021-06-04 13:58:00 | [train_policy] epoch #762 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284908
Evaluation/AverageDiscountedReturn          -41.5811
Evaluation/AverageReturn                    -41.5811
Evaluation/CompletionRate                     0
Evaluation/Iteration                        762
Evaluation/MaxReturn                        -28.2108
Evaluation/MinReturn                        -88.5017
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.82915
Extras/EpisodeRewardMean                    -41.5808
LinearFeatureBaseline/ExplainedVariance       0.844386
PolicyExecTime                                0.209181
ProcessExecTime                               0.0313697
TotalEnvSteps                            772156
policy/Entropy                               -1.85141
policy/KL                                     0.00650279
policy/KLBefore                               0
policy/LossAfter                             -0.0191209
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.157015
policy/dLoss                                  0.0191209
---------------------------------------  ----------------
2021-06-04 13:58:00 | [train_policy] epoch #763 | Obtaining samples for iteration 763...
2021-06-04 13:58:00 | [train_policy] epoch #763 | Logging diagnostics...
2021-06-04 13:58:00 | [train_policy] epoch #763 | Optimizing policy...
2021-06-04 13:58:00 | [train_policy] epoch #763 | Computing loss before
2021-06-04 13:58:00 | [train_policy] epoch #763 | Computing KL before
2021-06-04 13:58:00 | [train_policy] epoch #763 | Optimizing
2021-06-04 13:58:00 | [train_policy] epoch #763 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:00 | [train_policy] epoch #763 | computing loss before
2021-06-04 13:58:00 | [train_policy] epoch #763 | computing gradient
2021-06-04 13:58:00 | [train_policy] epoch #763 | gradient computed
2021-06-04 13:58:00 | [train_policy] epoch #763 | computing descent direction
2021-06-04 13:58:00 | [train_policy] epoch #763 | descent direction computed
2021-06-04 13:58:00 | [train_policy] epoch #763 | backtrack iters: 0
2021-06-04 13:58:00 | [train_policy] epoch #763 | optimization finished
2021-06-04 13:58:00 | [train_policy] epoch #763 | Computing KL after
2021-06-04 13:58:01 | [train_policy] epoch #763 | Computing loss after
2021-06-04 13:58:01 | [train_policy] epoch #763 | Fitting baseline...
2021-06-04 13:58:01 | [train_policy] epoch #763 | Saving snapshot...
2021-06-04 13:58:01 | [train_policy] epoch #763 | Saved
2021-06-04 13:58:01 | [train_policy] epoch #763 | Time 612.77 s
2021-06-04 13:58:01 | [train_policy] epoch #763 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28428
Evaluation/AverageDiscountedReturn          -40.409
Evaluation/AverageReturn                    -40.409
Evaluation/CompletionRate                     0
Evaluation/Iteration                        763
Evaluation/MaxReturn                        -27.9389
Evaluation/MinReturn                        -63.8655
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.8234
Extras/EpisodeRewardMean                    -40.4977
LinearFeatureBaseline/ExplainedVariance       0.890718
PolicyExecTime                                0.222712
ProcessExecTime                               0.0312209
TotalEnvSteps                            773168
policy/Entropy                               -1.83869
policy/KL                                     0.00974503
policy/KLBefore                               0
policy/LossAfter                             -0.0164286
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.159025
policy/dLoss                                  0.0164286
---------------------------------------  ----------------
2021-06-04 13:58:01 | [train_policy] epoch #764 | Obtaining samples for iteration 764...
2021-06-04 13:58:01 | [train_policy] epoch #764 | Logging diagnostics...
2021-06-04 13:58:01 | [train_policy] epoch #764 | Optimizing policy...
2021-06-04 13:58:01 | [train_policy] epoch #764 | Computing loss before
2021-06-04 13:58:01 | [train_policy] epoch #764 | Computing KL before
2021-06-04 13:58:01 | [train_policy] epoch #764 | Optimizing
2021-06-04 13:58:01 | [train_policy] epoch #764 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:01 | [train_policy] epoch #764 | computing loss before
2021-06-04 13:58:01 | [train_policy] epoch #764 | computing gradient
2021-06-04 13:58:01 | [train_policy] epoch #764 | gradient computed
2021-06-04 13:58:01 | [train_policy] epoch #764 | computing descent direction
2021-06-04 13:58:01 | [train_policy] epoch #764 | descent direction computed
2021-06-04 13:58:01 | [train_policy] epoch #764 | backtrack iters: 0
2021-06-04 13:58:01 | [train_policy] epoch #764 | optimization finished
2021-06-04 13:58:01 | [train_policy] epoch #764 | Computing KL after
2021-06-04 13:58:01 | [train_policy] epoch #764 | Computing loss after
2021-06-04 13:58:01 | [train_policy] epoch #764 | Fitting baseline...
2021-06-04 13:58:01 | [train_policy] epoch #764 | Saving snapshot...
2021-06-04 13:58:01 | [train_policy] epoch #764 | Saved
2021-06-04 13:58:01 | [train_policy] epoch #764 | Time 613.57 s
2021-06-04 13:58:01 | [train_policy] epoch #764 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285372
Evaluation/AverageDiscountedReturn          -42.6989
Evaluation/AverageReturn                    -42.6989
Evaluation/CompletionRate                     0
Evaluation/Iteration                        764
Evaluation/MaxReturn                        -28.1895
Evaluation/MinReturn                        -92.2512
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.7521
Extras/EpisodeRewardMean                    -42.6613
LinearFeatureBaseline/ExplainedVariance       0.801294
PolicyExecTime                                0.224153
ProcessExecTime                               0.0314677
TotalEnvSteps                            774180
policy/Entropy                               -1.82666
policy/KL                                     0.00963544
policy/KLBefore                               0
policy/LossAfter                             -0.0190723
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.16095
policy/dLoss                                  0.0190723
---------------------------------------  ----------------
2021-06-04 13:58:01 | [train_policy] epoch #765 | Obtaining samples for iteration 765...
2021-06-04 13:58:02 | [train_policy] epoch #765 | Logging diagnostics...
2021-06-04 13:58:02 | [train_policy] epoch #765 | Optimizing policy...
2021-06-04 13:58:02 | [train_policy] epoch #765 | Computing loss before
2021-06-04 13:58:02 | [train_policy] epoch #765 | Computing KL before
2021-06-04 13:58:02 | [train_policy] epoch #765 | Optimizing
2021-06-04 13:58:02 | [train_policy] epoch #765 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:02 | [train_policy] epoch #765 | computing loss before
2021-06-04 13:58:02 | [train_policy] epoch #765 | computing gradient
2021-06-04 13:58:02 | [train_policy] epoch #765 | gradient computed
2021-06-04 13:58:02 | [train_policy] epoch #765 | computing descent direction
2021-06-04 13:58:02 | [train_policy] epoch #765 | descent direction computed
2021-06-04 13:58:02 | [train_policy] epoch #765 | backtrack iters: 1
2021-06-04 13:58:02 | [train_policy] epoch #765 | optimization finished
2021-06-04 13:58:02 | [train_policy] epoch #765 | Computing KL after
2021-06-04 13:58:02 | [train_policy] epoch #765 | Computing loss after
2021-06-04 13:58:02 | [train_policy] epoch #765 | Fitting baseline...
2021-06-04 13:58:02 | [train_policy] epoch #765 | Saving snapshot...
2021-06-04 13:58:02 | [train_policy] epoch #765 | Saved
2021-06-04 13:58:02 | [train_policy] epoch #765 | Time 614.37 s
2021-06-04 13:58:02 | [train_policy] epoch #765 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284957
Evaluation/AverageDiscountedReturn          -41.0645
Evaluation/AverageReturn                    -41.0645
Evaluation/CompletionRate                     0
Evaluation/Iteration                        765
Evaluation/MaxReturn                        -30.1718
Evaluation/MinReturn                        -62.9598
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.69258
Extras/EpisodeRewardMean                    -41.3057
LinearFeatureBaseline/ExplainedVariance       0.881382
PolicyExecTime                                0.221954
ProcessExecTime                               0.031271
TotalEnvSteps                            775192
policy/Entropy                               -1.83316
policy/KL                                     0.00654722
policy/KLBefore                               0
policy/LossAfter                             -0.0152082
policy/LossBefore                            -1.48423e-08
policy/Perplexity                             0.159908
policy/dLoss                                  0.0152082
---------------------------------------  ----------------
2021-06-04 13:58:02 | [train_policy] epoch #766 | Obtaining samples for iteration 766...
2021-06-04 13:58:03 | [train_policy] epoch #766 | Logging diagnostics...
2021-06-04 13:58:03 | [train_policy] epoch #766 | Optimizing policy...
2021-06-04 13:58:03 | [train_policy] epoch #766 | Computing loss before
2021-06-04 13:58:03 | [train_policy] epoch #766 | Computing KL before
2021-06-04 13:58:03 | [train_policy] epoch #766 | Optimizing
2021-06-04 13:58:03 | [train_policy] epoch #766 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:03 | [train_policy] epoch #766 | computing loss before
2021-06-04 13:58:03 | [train_policy] epoch #766 | computing gradient
2021-06-04 13:58:03 | [train_policy] epoch #766 | gradient computed
2021-06-04 13:58:03 | [train_policy] epoch #766 | computing descent direction
2021-06-04 13:58:03 | [train_policy] epoch #766 | descent direction computed
2021-06-04 13:58:03 | [train_policy] epoch #766 | backtrack iters: 1
2021-06-04 13:58:03 | [train_policy] epoch #766 | optimization finished
2021-06-04 13:58:03 | [train_policy] epoch #766 | Computing KL after
2021-06-04 13:58:03 | [train_policy] epoch #766 | Computing loss after
2021-06-04 13:58:03 | [train_policy] epoch #766 | Fitting baseline...
2021-06-04 13:58:03 | [train_policy] epoch #766 | Saving snapshot...
2021-06-04 13:58:03 | [train_policy] epoch #766 | Saved
2021-06-04 13:58:03 | [train_policy] epoch #766 | Time 615.18 s
2021-06-04 13:58:03 | [train_policy] epoch #766 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286864
Evaluation/AverageDiscountedReturn          -40.3604
Evaluation/AverageReturn                    -40.3604
Evaluation/CompletionRate                     0
Evaluation/Iteration                        766
Evaluation/MaxReturn                        -28.1668
Evaluation/MinReturn                        -64.2986
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.0634
Extras/EpisodeRewardMean                    -40.1547
LinearFeatureBaseline/ExplainedVariance       0.890143
PolicyExecTime                                0.228501
ProcessExecTime                               0.0314281
TotalEnvSteps                            776204
policy/Entropy                               -1.85215
policy/KL                                     0.00639869
policy/KLBefore                               0
policy/LossAfter                             -0.0155775
policy/LossBefore                            -1.53134e-09
policy/Perplexity                             0.156899
policy/dLoss                                  0.0155775
---------------------------------------  ----------------
2021-06-04 13:58:03 | [train_policy] epoch #767 | Obtaining samples for iteration 767...
2021-06-04 13:58:04 | [train_policy] epoch #767 | Logging diagnostics...
2021-06-04 13:58:04 | [train_policy] epoch #767 | Optimizing policy...
2021-06-04 13:58:04 | [train_policy] epoch #767 | Computing loss before
2021-06-04 13:58:04 | [train_policy] epoch #767 | Computing KL before
2021-06-04 13:58:04 | [train_policy] epoch #767 | Optimizing
2021-06-04 13:58:04 | [train_policy] epoch #767 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:04 | [train_policy] epoch #767 | computing loss before
2021-06-04 13:58:04 | [train_policy] epoch #767 | computing gradient
2021-06-04 13:58:04 | [train_policy] epoch #767 | gradient computed
2021-06-04 13:58:04 | [train_policy] epoch #767 | computing descent direction
2021-06-04 13:58:04 | [train_policy] epoch #767 | descent direction computed
2021-06-04 13:58:04 | [train_policy] epoch #767 | backtrack iters: 0
2021-06-04 13:58:04 | [train_policy] epoch #767 | optimization finished
2021-06-04 13:58:04 | [train_policy] epoch #767 | Computing KL after
2021-06-04 13:58:04 | [train_policy] epoch #767 | Computing loss after
2021-06-04 13:58:04 | [train_policy] epoch #767 | Fitting baseline...
2021-06-04 13:58:04 | [train_policy] epoch #767 | Saving snapshot...
2021-06-04 13:58:04 | [train_policy] epoch #767 | Saved
2021-06-04 13:58:04 | [train_policy] epoch #767 | Time 615.97 s
2021-06-04 13:58:04 | [train_policy] epoch #767 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284321
Evaluation/AverageDiscountedReturn          -42.2612
Evaluation/AverageReturn                    -42.2612
Evaluation/CompletionRate                     0
Evaluation/Iteration                        767
Evaluation/MaxReturn                        -28.1521
Evaluation/MinReturn                        -64.1464
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.64209
Extras/EpisodeRewardMean                    -42.2074
LinearFeatureBaseline/ExplainedVariance       0.885951
PolicyExecTime                                0.22468
ProcessExecTime                               0.0311508
TotalEnvSteps                            777216
policy/Entropy                               -1.83318
policy/KL                                     0.00987211
policy/KLBefore                               0
policy/LossAfter                             -0.0172498
policy/LossBefore                            -1.06016e-09
policy/Perplexity                             0.159904
policy/dLoss                                  0.0172498
---------------------------------------  ----------------
2021-06-04 13:58:04 | [train_policy] epoch #768 | Obtaining samples for iteration 768...
2021-06-04 13:58:04 | [train_policy] epoch #768 | Logging diagnostics...
2021-06-04 13:58:04 | [train_policy] epoch #768 | Optimizing policy...
2021-06-04 13:58:04 | [train_policy] epoch #768 | Computing loss before
2021-06-04 13:58:04 | [train_policy] epoch #768 | Computing KL before
2021-06-04 13:58:04 | [train_policy] epoch #768 | Optimizing
2021-06-04 13:58:04 | [train_policy] epoch #768 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:04 | [train_policy] epoch #768 | computing loss before
2021-06-04 13:58:04 | [train_policy] epoch #768 | computing gradient
2021-06-04 13:58:04 | [train_policy] epoch #768 | gradient computed
2021-06-04 13:58:04 | [train_policy] epoch #768 | computing descent direction
2021-06-04 13:58:04 | [train_policy] epoch #768 | descent direction computed
2021-06-04 13:58:04 | [train_policy] epoch #768 | backtrack iters: 0
2021-06-04 13:58:04 | [train_policy] epoch #768 | optimization finished
2021-06-04 13:58:04 | [train_policy] epoch #768 | Computing KL after
2021-06-04 13:58:04 | [train_policy] epoch #768 | Computing loss after
2021-06-04 13:58:04 | [train_policy] epoch #768 | Fitting baseline...
2021-06-04 13:58:04 | [train_policy] epoch #768 | Saving snapshot...
2021-06-04 13:58:05 | [train_policy] epoch #768 | Saved
2021-06-04 13:58:05 | [train_policy] epoch #768 | Time 616.74 s
2021-06-04 13:58:05 | [train_policy] epoch #768 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284448
Evaluation/AverageDiscountedReturn          -63.3495
Evaluation/AverageReturn                    -63.3495
Evaluation/CompletionRate                     0
Evaluation/Iteration                        768
Evaluation/MaxReturn                        -27.9024
Evaluation/MinReturn                      -2062.3
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.732
Extras/EpisodeRewardMean                    -61.5301
LinearFeatureBaseline/ExplainedVariance       0.0138752
PolicyExecTime                                0.210031
ProcessExecTime                               0.0311656
TotalEnvSteps                            778228
policy/Entropy                               -1.84375
policy/KL                                     0.00981873
policy/KLBefore                               0
policy/LossAfter                             -0.0326115
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.158223
policy/dLoss                                  0.0326115
---------------------------------------  ----------------
2021-06-04 13:58:05 | [train_policy] epoch #769 | Obtaining samples for iteration 769...
2021-06-04 13:58:05 | [train_policy] epoch #769 | Logging diagnostics...
2021-06-04 13:58:05 | [train_policy] epoch #769 | Optimizing policy...
2021-06-04 13:58:05 | [train_policy] epoch #769 | Computing loss before
2021-06-04 13:58:05 | [train_policy] epoch #769 | Computing KL before
2021-06-04 13:58:05 | [train_policy] epoch #769 | Optimizing
2021-06-04 13:58:05 | [train_policy] epoch #769 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:05 | [train_policy] epoch #769 | computing loss before
2021-06-04 13:58:05 | [train_policy] epoch #769 | computing gradient
2021-06-04 13:58:05 | [train_policy] epoch #769 | gradient computed
2021-06-04 13:58:05 | [train_policy] epoch #769 | computing descent direction
2021-06-04 13:58:05 | [train_policy] epoch #769 | descent direction computed
2021-06-04 13:58:05 | [train_policy] epoch #769 | backtrack iters: 1
2021-06-04 13:58:05 | [train_policy] epoch #769 | optimization finished
2021-06-04 13:58:05 | [train_policy] epoch #769 | Computing KL after
2021-06-04 13:58:05 | [train_policy] epoch #769 | Computing loss after
2021-06-04 13:58:05 | [train_policy] epoch #769 | Fitting baseline...
2021-06-04 13:58:05 | [train_policy] epoch #769 | Saving snapshot...
2021-06-04 13:58:05 | [train_policy] epoch #769 | Saved
2021-06-04 13:58:05 | [train_policy] epoch #769 | Time 617.55 s
2021-06-04 13:58:05 | [train_policy] epoch #769 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285362
Evaluation/AverageDiscountedReturn          -41.7053
Evaluation/AverageReturn                    -41.7053
Evaluation/CompletionRate                     0
Evaluation/Iteration                        769
Evaluation/MaxReturn                        -27.7427
Evaluation/MinReturn                        -63.923
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.09396
Extras/EpisodeRewardMean                    -41.6121
LinearFeatureBaseline/ExplainedVariance     -17.9987
PolicyExecTime                                0.228504
ProcessExecTime                               0.0311832
TotalEnvSteps                            779240
policy/Entropy                               -1.85578
policy/KL                                     0.00655524
policy/KLBefore                               0
policy/LossAfter                             -0.021481
policy/LossBefore                             1.93185e-08
policy/Perplexity                             0.156331
policy/dLoss                                  0.021481
---------------------------------------  ----------------
2021-06-04 13:58:05 | [train_policy] epoch #770 | Obtaining samples for iteration 770...
2021-06-04 13:58:06 | [train_policy] epoch #770 | Logging diagnostics...
2021-06-04 13:58:06 | [train_policy] epoch #770 | Optimizing policy...
2021-06-04 13:58:06 | [train_policy] epoch #770 | Computing loss before
2021-06-04 13:58:06 | [train_policy] epoch #770 | Computing KL before
2021-06-04 13:58:06 | [train_policy] epoch #770 | Optimizing
2021-06-04 13:58:06 | [train_policy] epoch #770 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:06 | [train_policy] epoch #770 | computing loss before
2021-06-04 13:58:06 | [train_policy] epoch #770 | computing gradient
2021-06-04 13:58:06 | [train_policy] epoch #770 | gradient computed
2021-06-04 13:58:06 | [train_policy] epoch #770 | computing descent direction
2021-06-04 13:58:06 | [train_policy] epoch #770 | descent direction computed
2021-06-04 13:58:06 | [train_policy] epoch #770 | backtrack iters: 1
2021-06-04 13:58:06 | [train_policy] epoch #770 | optimization finished
2021-06-04 13:58:06 | [train_policy] epoch #770 | Computing KL after
2021-06-04 13:58:06 | [train_policy] epoch #770 | Computing loss after
2021-06-04 13:58:06 | [train_policy] epoch #770 | Fitting baseline...
2021-06-04 13:58:06 | [train_policy] epoch #770 | Saving snapshot...
2021-06-04 13:58:06 | [train_policy] epoch #770 | Saved
2021-06-04 13:58:06 | [train_policy] epoch #770 | Time 618.36 s
2021-06-04 13:58:06 | [train_policy] epoch #770 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285806
Evaluation/AverageDiscountedReturn          -39.9258
Evaluation/AverageReturn                    -39.9258
Evaluation/CompletionRate                     0
Evaluation/Iteration                        770
Evaluation/MaxReturn                        -28.2602
Evaluation/MinReturn                        -78.407
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05171
Extras/EpisodeRewardMean                    -39.9825
LinearFeatureBaseline/ExplainedVariance       0.879345
PolicyExecTime                                0.227285
ProcessExecTime                               0.0312309
TotalEnvSteps                            780252
policy/Entropy                               -1.87655
policy/KL                                     0.00648382
policy/KLBefore                               0
policy/LossAfter                             -0.0176316
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             0.153117
policy/dLoss                                  0.0176316
---------------------------------------  ----------------
2021-06-04 13:58:06 | [train_policy] epoch #771 | Obtaining samples for iteration 771...
2021-06-04 13:58:07 | [train_policy] epoch #771 | Logging diagnostics...
2021-06-04 13:58:07 | [train_policy] epoch #771 | Optimizing policy...
2021-06-04 13:58:07 | [train_policy] epoch #771 | Computing loss before
2021-06-04 13:58:07 | [train_policy] epoch #771 | Computing KL before
2021-06-04 13:58:07 | [train_policy] epoch #771 | Optimizing
2021-06-04 13:58:07 | [train_policy] epoch #771 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:07 | [train_policy] epoch #771 | computing loss before
2021-06-04 13:58:07 | [train_policy] epoch #771 | computing gradient
2021-06-04 13:58:07 | [train_policy] epoch #771 | gradient computed
2021-06-04 13:58:07 | [train_policy] epoch #771 | computing descent direction
2021-06-04 13:58:07 | [train_policy] epoch #771 | descent direction computed
2021-06-04 13:58:07 | [train_policy] epoch #771 | backtrack iters: 1
2021-06-04 13:58:07 | [train_policy] epoch #771 | optimization finished
2021-06-04 13:58:07 | [train_policy] epoch #771 | Computing KL after
2021-06-04 13:58:07 | [train_policy] epoch #771 | Computing loss after
2021-06-04 13:58:07 | [train_policy] epoch #771 | Fitting baseline...
2021-06-04 13:58:07 | [train_policy] epoch #771 | Saving snapshot...
2021-06-04 13:58:07 | [train_policy] epoch #771 | Saved
2021-06-04 13:58:07 | [train_policy] epoch #771 | Time 619.14 s
2021-06-04 13:58:07 | [train_policy] epoch #771 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284002
Evaluation/AverageDiscountedReturn          -40.8398
Evaluation/AverageReturn                    -40.8398
Evaluation/CompletionRate                     0
Evaluation/Iteration                        771
Evaluation/MaxReturn                        -27.9082
Evaluation/MinReturn                       -102.917
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3021
Extras/EpisodeRewardMean                    -40.4784
LinearFeatureBaseline/ExplainedVariance       0.809665
PolicyExecTime                                0.210967
ProcessExecTime                               0.0311453
TotalEnvSteps                            781264
policy/Entropy                               -1.89021
policy/KL                                     0.00652133
policy/KLBefore                               0
policy/LossAfter                             -0.0234388
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.15104
policy/dLoss                                  0.0234388
---------------------------------------  ----------------
2021-06-04 13:58:07 | [train_policy] epoch #772 | Obtaining samples for iteration 772...
2021-06-04 13:58:08 | [train_policy] epoch #772 | Logging diagnostics...
2021-06-04 13:58:08 | [train_policy] epoch #772 | Optimizing policy...
2021-06-04 13:58:08 | [train_policy] epoch #772 | Computing loss before
2021-06-04 13:58:08 | [train_policy] epoch #772 | Computing KL before
2021-06-04 13:58:08 | [train_policy] epoch #772 | Optimizing
2021-06-04 13:58:08 | [train_policy] epoch #772 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:08 | [train_policy] epoch #772 | computing loss before
2021-06-04 13:58:08 | [train_policy] epoch #772 | computing gradient
2021-06-04 13:58:08 | [train_policy] epoch #772 | gradient computed
2021-06-04 13:58:08 | [train_policy] epoch #772 | computing descent direction
2021-06-04 13:58:08 | [train_policy] epoch #772 | descent direction computed
2021-06-04 13:58:08 | [train_policy] epoch #772 | backtrack iters: 0
2021-06-04 13:58:08 | [train_policy] epoch #772 | optimization finished
2021-06-04 13:58:08 | [train_policy] epoch #772 | Computing KL after
2021-06-04 13:58:08 | [train_policy] epoch #772 | Computing loss after
2021-06-04 13:58:08 | [train_policy] epoch #772 | Fitting baseline...
2021-06-04 13:58:08 | [train_policy] epoch #772 | Saving snapshot...
2021-06-04 13:58:08 | [train_policy] epoch #772 | Saved
2021-06-04 13:58:08 | [train_policy] epoch #772 | Time 619.93 s
2021-06-04 13:58:08 | [train_policy] epoch #772 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284329
Evaluation/AverageDiscountedReturn          -40.1057
Evaluation/AverageReturn                    -40.1057
Evaluation/CompletionRate                     0
Evaluation/Iteration                        772
Evaluation/MaxReturn                        -27.9425
Evaluation/MinReturn                        -63.9494
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.29089
Extras/EpisodeRewardMean                    -40.4648
LinearFeatureBaseline/ExplainedVariance       0.886095
PolicyExecTime                                0.229701
ProcessExecTime                               0.0311792
TotalEnvSteps                            782276
policy/Entropy                               -1.88981
policy/KL                                     0.0098473
policy/KLBefore                               0
policy/LossAfter                             -0.0239415
policy/LossBefore                            -3.15693e-08
policy/Perplexity                             0.1511
policy/dLoss                                  0.0239415
---------------------------------------  ----------------
2021-06-04 13:58:08 | [train_policy] epoch #773 | Obtaining samples for iteration 773...
2021-06-04 13:58:08 | [train_policy] epoch #773 | Logging diagnostics...
2021-06-04 13:58:08 | [train_policy] epoch #773 | Optimizing policy...
2021-06-04 13:58:08 | [train_policy] epoch #773 | Computing loss before
2021-06-04 13:58:08 | [train_policy] epoch #773 | Computing KL before
2021-06-04 13:58:08 | [train_policy] epoch #773 | Optimizing
2021-06-04 13:58:08 | [train_policy] epoch #773 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:08 | [train_policy] epoch #773 | computing loss before
2021-06-04 13:58:08 | [train_policy] epoch #773 | computing gradient
2021-06-04 13:58:08 | [train_policy] epoch #773 | gradient computed
2021-06-04 13:58:08 | [train_policy] epoch #773 | computing descent direction
2021-06-04 13:58:08 | [train_policy] epoch #773 | descent direction computed
2021-06-04 13:58:08 | [train_policy] epoch #773 | backtrack iters: 0
2021-06-04 13:58:08 | [train_policy] epoch #773 | optimization finished
2021-06-04 13:58:08 | [train_policy] epoch #773 | Computing KL after
2021-06-04 13:58:08 | [train_policy] epoch #773 | Computing loss after
2021-06-04 13:58:08 | [train_policy] epoch #773 | Fitting baseline...
2021-06-04 13:58:08 | [train_policy] epoch #773 | Saving snapshot...
2021-06-04 13:58:08 | [train_policy] epoch #773 | Saved
2021-06-04 13:58:08 | [train_policy] epoch #773 | Time 620.71 s
2021-06-04 13:58:08 | [train_policy] epoch #773 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285063
Evaluation/AverageDiscountedReturn          -39.9527
Evaluation/AverageReturn                    -39.9527
Evaluation/CompletionRate                     0
Evaluation/Iteration                        773
Evaluation/MaxReturn                        -27.8025
Evaluation/MinReturn                        -63.9023
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05224
Extras/EpisodeRewardMean                    -39.9399
LinearFeatureBaseline/ExplainedVariance       0.887767
PolicyExecTime                                0.218894
ProcessExecTime                               0.0312159
TotalEnvSteps                            783288
policy/Entropy                               -1.90362
policy/KL                                     0.00971723
policy/KLBefore                               0
policy/LossAfter                             -0.0211317
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.149028
policy/dLoss                                  0.0211317
---------------------------------------  ----------------
2021-06-04 13:58:09 | [train_policy] epoch #774 | Obtaining samples for iteration 774...
2021-06-04 13:58:09 | [train_policy] epoch #774 | Logging diagnostics...
2021-06-04 13:58:09 | [train_policy] epoch #774 | Optimizing policy...
2021-06-04 13:58:09 | [train_policy] epoch #774 | Computing loss before
2021-06-04 13:58:09 | [train_policy] epoch #774 | Computing KL before
2021-06-04 13:58:09 | [train_policy] epoch #774 | Optimizing
2021-06-04 13:58:09 | [train_policy] epoch #774 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:09 | [train_policy] epoch #774 | computing loss before
2021-06-04 13:58:09 | [train_policy] epoch #774 | computing gradient
2021-06-04 13:58:09 | [train_policy] epoch #774 | gradient computed
2021-06-04 13:58:09 | [train_policy] epoch #774 | computing descent direction
2021-06-04 13:58:09 | [train_policy] epoch #774 | descent direction computed
2021-06-04 13:58:09 | [train_policy] epoch #774 | backtrack iters: 0
2021-06-04 13:58:09 | [train_policy] epoch #774 | optimization finished
2021-06-04 13:58:09 | [train_policy] epoch #774 | Computing KL after
2021-06-04 13:58:09 | [train_policy] epoch #774 | Computing loss after
2021-06-04 13:58:09 | [train_policy] epoch #774 | Fitting baseline...
2021-06-04 13:58:09 | [train_policy] epoch #774 | Saving snapshot...
2021-06-04 13:58:09 | [train_policy] epoch #774 | Saved
2021-06-04 13:58:09 | [train_policy] epoch #774 | Time 621.52 s
2021-06-04 13:58:09 | [train_policy] epoch #774 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284727
Evaluation/AverageDiscountedReturn          -41.7954
Evaluation/AverageReturn                    -41.7954
Evaluation/CompletionRate                     0
Evaluation/Iteration                        774
Evaluation/MaxReturn                        -27.8614
Evaluation/MinReturn                        -92.1736
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.74421
Extras/EpisodeRewardMean                    -41.6775
LinearFeatureBaseline/ExplainedVariance       0.838191
PolicyExecTime                                0.234836
ProcessExecTime                               0.0311487
TotalEnvSteps                            784300
policy/Entropy                               -1.91087
policy/KL                                     0.00996559
policy/KLBefore                               0
policy/LossAfter                             -0.0211717
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.147951
policy/dLoss                                  0.0211717
---------------------------------------  ----------------
2021-06-04 13:58:09 | [train_policy] epoch #775 | Obtaining samples for iteration 775...
2021-06-04 13:58:10 | [train_policy] epoch #775 | Logging diagnostics...
2021-06-04 13:58:10 | [train_policy] epoch #775 | Optimizing policy...
2021-06-04 13:58:10 | [train_policy] epoch #775 | Computing loss before
2021-06-04 13:58:10 | [train_policy] epoch #775 | Computing KL before
2021-06-04 13:58:10 | [train_policy] epoch #775 | Optimizing
2021-06-04 13:58:10 | [train_policy] epoch #775 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:10 | [train_policy] epoch #775 | computing loss before
2021-06-04 13:58:10 | [train_policy] epoch #775 | computing gradient
2021-06-04 13:58:10 | [train_policy] epoch #775 | gradient computed
2021-06-04 13:58:10 | [train_policy] epoch #775 | computing descent direction
2021-06-04 13:58:10 | [train_policy] epoch #775 | descent direction computed
2021-06-04 13:58:10 | [train_policy] epoch #775 | backtrack iters: 1
2021-06-04 13:58:10 | [train_policy] epoch #775 | optimization finished
2021-06-04 13:58:10 | [train_policy] epoch #775 | Computing KL after
2021-06-04 13:58:10 | [train_policy] epoch #775 | Computing loss after
2021-06-04 13:58:10 | [train_policy] epoch #775 | Fitting baseline...
2021-06-04 13:58:10 | [train_policy] epoch #775 | Saving snapshot...
2021-06-04 13:58:10 | [train_policy] epoch #775 | Saved
2021-06-04 13:58:10 | [train_policy] epoch #775 | Time 622.32 s
2021-06-04 13:58:10 | [train_policy] epoch #775 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284087
Evaluation/AverageDiscountedReturn          -40.9234
Evaluation/AverageReturn                    -40.9234
Evaluation/CompletionRate                     0
Evaluation/Iteration                        775
Evaluation/MaxReturn                        -28.17
Evaluation/MinReturn                        -63.0347
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.93776
Extras/EpisodeRewardMean                    -41.0118
LinearFeatureBaseline/ExplainedVariance       0.891714
PolicyExecTime                                0.21954
ProcessExecTime                               0.0311208
TotalEnvSteps                            785312
policy/Entropy                               -1.91416
policy/KL                                     0.00653473
policy/KLBefore                               0
policy/LossAfter                             -0.0188293
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.147465
policy/dLoss                                  0.0188294
---------------------------------------  ----------------
2021-06-04 13:58:10 | [train_policy] epoch #776 | Obtaining samples for iteration 776...
2021-06-04 13:58:11 | [train_policy] epoch #776 | Logging diagnostics...
2021-06-04 13:58:11 | [train_policy] epoch #776 | Optimizing policy...
2021-06-04 13:58:11 | [train_policy] epoch #776 | Computing loss before
2021-06-04 13:58:11 | [train_policy] epoch #776 | Computing KL before
2021-06-04 13:58:11 | [train_policy] epoch #776 | Optimizing
2021-06-04 13:58:11 | [train_policy] epoch #776 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:11 | [train_policy] epoch #776 | computing loss before
2021-06-04 13:58:11 | [train_policy] epoch #776 | computing gradient
2021-06-04 13:58:11 | [train_policy] epoch #776 | gradient computed
2021-06-04 13:58:11 | [train_policy] epoch #776 | computing descent direction
2021-06-04 13:58:11 | [train_policy] epoch #776 | descent direction computed
2021-06-04 13:58:11 | [train_policy] epoch #776 | backtrack iters: 1
2021-06-04 13:58:11 | [train_policy] epoch #776 | optimization finished
2021-06-04 13:58:11 | [train_policy] epoch #776 | Computing KL after
2021-06-04 13:58:11 | [train_policy] epoch #776 | Computing loss after
2021-06-04 13:58:11 | [train_policy] epoch #776 | Fitting baseline...
2021-06-04 13:58:11 | [train_policy] epoch #776 | Saving snapshot...
2021-06-04 13:58:11 | [train_policy] epoch #776 | Saved
2021-06-04 13:58:11 | [train_policy] epoch #776 | Time 623.12 s
2021-06-04 13:58:11 | [train_policy] epoch #776 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284562
Evaluation/AverageDiscountedReturn          -45.9824
Evaluation/AverageReturn                    -45.9824
Evaluation/CompletionRate                     0
Evaluation/Iteration                        776
Evaluation/MaxReturn                        -29.3215
Evaluation/MinReturn                       -489.498
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         47.177
Extras/EpisodeRewardMean                    -45.5845
LinearFeatureBaseline/ExplainedVariance       0.104829
PolicyExecTime                                0.226327
ProcessExecTime                               0.0310557
TotalEnvSteps                            786324
policy/Entropy                               -1.90598
policy/KL                                     0.00652739
policy/KLBefore                               0
policy/LossAfter                             -0.0214791
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.148676
policy/dLoss                                  0.0214791
---------------------------------------  ----------------
2021-06-04 13:58:11 | [train_policy] epoch #777 | Obtaining samples for iteration 777...
2021-06-04 13:58:12 | [train_policy] epoch #777 | Logging diagnostics...
2021-06-04 13:58:12 | [train_policy] epoch #777 | Optimizing policy...
2021-06-04 13:58:12 | [train_policy] epoch #777 | Computing loss before
2021-06-04 13:58:12 | [train_policy] epoch #777 | Computing KL before
2021-06-04 13:58:12 | [train_policy] epoch #777 | Optimizing
2021-06-04 13:58:12 | [train_policy] epoch #777 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:12 | [train_policy] epoch #777 | computing loss before
2021-06-04 13:58:12 | [train_policy] epoch #777 | computing gradient
2021-06-04 13:58:12 | [train_policy] epoch #777 | gradient computed
2021-06-04 13:58:12 | [train_policy] epoch #777 | computing descent direction
2021-06-04 13:58:12 | [train_policy] epoch #777 | descent direction computed
2021-06-04 13:58:12 | [train_policy] epoch #777 | backtrack iters: 1
2021-06-04 13:58:12 | [train_policy] epoch #777 | optimization finished
2021-06-04 13:58:12 | [train_policy] epoch #777 | Computing KL after
2021-06-04 13:58:12 | [train_policy] epoch #777 | Computing loss after
2021-06-04 13:58:12 | [train_policy] epoch #777 | Fitting baseline...
2021-06-04 13:58:12 | [train_policy] epoch #777 | Saving snapshot...
2021-06-04 13:58:12 | [train_policy] epoch #777 | Saved
2021-06-04 13:58:12 | [train_policy] epoch #777 | Time 623.90 s
2021-06-04 13:58:12 | [train_policy] epoch #777 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284642
Evaluation/AverageDiscountedReturn          -40.9925
Evaluation/AverageReturn                    -40.9925
Evaluation/CompletionRate                     0
Evaluation/Iteration                        777
Evaluation/MaxReturn                        -28.5389
Evaluation/MinReturn                        -63.9491
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.15435
Extras/EpisodeRewardMean                    -40.6353
LinearFeatureBaseline/ExplainedVariance      -0.418755
PolicyExecTime                                0.207711
ProcessExecTime                               0.031183
TotalEnvSteps                            787336
policy/Entropy                               -1.92483
policy/KL                                     0.00656807
policy/KLBefore                               0
policy/LossAfter                             -0.018032
policy/LossBefore                             2.12032e-08
policy/Perplexity                             0.145901
policy/dLoss                                  0.0180321
---------------------------------------  ----------------
2021-06-04 13:58:12 | [train_policy] epoch #778 | Obtaining samples for iteration 778...
2021-06-04 13:58:12 | [train_policy] epoch #778 | Logging diagnostics...
2021-06-04 13:58:12 | [train_policy] epoch #778 | Optimizing policy...
2021-06-04 13:58:12 | [train_policy] epoch #778 | Computing loss before
2021-06-04 13:58:12 | [train_policy] epoch #778 | Computing KL before
2021-06-04 13:58:12 | [train_policy] epoch #778 | Optimizing
2021-06-04 13:58:12 | [train_policy] epoch #778 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:12 | [train_policy] epoch #778 | computing loss before
2021-06-04 13:58:12 | [train_policy] epoch #778 | computing gradient
2021-06-04 13:58:12 | [train_policy] epoch #778 | gradient computed
2021-06-04 13:58:12 | [train_policy] epoch #778 | computing descent direction
2021-06-04 13:58:12 | [train_policy] epoch #778 | descent direction computed
2021-06-04 13:58:12 | [train_policy] epoch #778 | backtrack iters: 1
2021-06-04 13:58:12 | [train_policy] epoch #778 | optimization finished
2021-06-04 13:58:12 | [train_policy] epoch #778 | Computing KL after
2021-06-04 13:58:12 | [train_policy] epoch #778 | Computing loss after
2021-06-04 13:58:12 | [train_policy] epoch #778 | Fitting baseline...
2021-06-04 13:58:12 | [train_policy] epoch #778 | Saving snapshot...
2021-06-04 13:58:12 | [train_policy] epoch #778 | Saved
2021-06-04 13:58:12 | [train_policy] epoch #778 | Time 624.69 s
2021-06-04 13:58:12 | [train_policy] epoch #778 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285817
Evaluation/AverageDiscountedReturn          -42.525
Evaluation/AverageReturn                    -42.525
Evaluation/CompletionRate                     0
Evaluation/Iteration                        778
Evaluation/MaxReturn                        -28.1545
Evaluation/MinReturn                       -153.451
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         18.0821
Extras/EpisodeRewardMean                    -42.358
LinearFeatureBaseline/ExplainedVariance       0.473489
PolicyExecTime                                0.213331
ProcessExecTime                               0.0312917
TotalEnvSteps                            788348
policy/Entropy                               -1.92734
policy/KL                                     0.00641941
policy/KLBefore                               0
policy/LossAfter                             -0.0267415
policy/LossBefore                            -1.17796e-08
policy/Perplexity                             0.145535
policy/dLoss                                  0.0267415
---------------------------------------  ----------------
2021-06-04 13:58:12 | [train_policy] epoch #779 | Obtaining samples for iteration 779...
2021-06-04 13:58:13 | [train_policy] epoch #779 | Logging diagnostics...
2021-06-04 13:58:13 | [train_policy] epoch #779 | Optimizing policy...
2021-06-04 13:58:13 | [train_policy] epoch #779 | Computing loss before
2021-06-04 13:58:13 | [train_policy] epoch #779 | Computing KL before
2021-06-04 13:58:13 | [train_policy] epoch #779 | Optimizing
2021-06-04 13:58:13 | [train_policy] epoch #779 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:13 | [train_policy] epoch #779 | computing loss before
2021-06-04 13:58:13 | [train_policy] epoch #779 | computing gradient
2021-06-04 13:58:13 | [train_policy] epoch #779 | gradient computed
2021-06-04 13:58:13 | [train_policy] epoch #779 | computing descent direction
2021-06-04 13:58:13 | [train_policy] epoch #779 | descent direction computed
2021-06-04 13:58:13 | [train_policy] epoch #779 | backtrack iters: 0
2021-06-04 13:58:13 | [train_policy] epoch #779 | optimization finished
2021-06-04 13:58:13 | [train_policy] epoch #779 | Computing KL after
2021-06-04 13:58:13 | [train_policy] epoch #779 | Computing loss after
2021-06-04 13:58:13 | [train_policy] epoch #779 | Fitting baseline...
2021-06-04 13:58:13 | [train_policy] epoch #779 | Saving snapshot...
2021-06-04 13:58:13 | [train_policy] epoch #779 | Saved
2021-06-04 13:58:13 | [train_policy] epoch #779 | Time 625.49 s
2021-06-04 13:58:13 | [train_policy] epoch #779 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283951
Evaluation/AverageDiscountedReturn          -43.3274
Evaluation/AverageReturn                    -43.3274
Evaluation/CompletionRate                     0
Evaluation/Iteration                        779
Evaluation/MaxReturn                        -31.4222
Evaluation/MinReturn                       -106.249
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.1134
Extras/EpisodeRewardMean                    -44.2314
LinearFeatureBaseline/ExplainedVariance       0.691492
PolicyExecTime                                0.226308
ProcessExecTime                               0.0311067
TotalEnvSteps                            789360
policy/Entropy                               -1.91553
policy/KL                                     0.00980263
policy/KLBefore                               0
policy/LossAfter                             -0.0160926
policy/LossBefore                            -7.06774e-10
policy/Perplexity                             0.147263
policy/dLoss                                  0.0160926
---------------------------------------  ----------------
2021-06-04 13:58:13 | [train_policy] epoch #780 | Obtaining samples for iteration 780...
2021-06-04 13:58:14 | [train_policy] epoch #780 | Logging diagnostics...
2021-06-04 13:58:14 | [train_policy] epoch #780 | Optimizing policy...
2021-06-04 13:58:14 | [train_policy] epoch #780 | Computing loss before
2021-06-04 13:58:14 | [train_policy] epoch #780 | Computing KL before
2021-06-04 13:58:14 | [train_policy] epoch #780 | Optimizing
2021-06-04 13:58:14 | [train_policy] epoch #780 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:14 | [train_policy] epoch #780 | computing loss before
2021-06-04 13:58:14 | [train_policy] epoch #780 | computing gradient
2021-06-04 13:58:14 | [train_policy] epoch #780 | gradient computed
2021-06-04 13:58:14 | [train_policy] epoch #780 | computing descent direction
2021-06-04 13:58:14 | [train_policy] epoch #780 | descent direction computed
2021-06-04 13:58:14 | [train_policy] epoch #780 | backtrack iters: 0
2021-06-04 13:58:14 | [train_policy] epoch #780 | optimization finished
2021-06-04 13:58:14 | [train_policy] epoch #780 | Computing KL after
2021-06-04 13:58:14 | [train_policy] epoch #780 | Computing loss after
2021-06-04 13:58:14 | [train_policy] epoch #780 | Fitting baseline...
2021-06-04 13:58:14 | [train_policy] epoch #780 | Saving snapshot...
2021-06-04 13:58:14 | [train_policy] epoch #780 | Saved
2021-06-04 13:58:14 | [train_policy] epoch #780 | Time 626.30 s
2021-06-04 13:58:14 | [train_policy] epoch #780 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284836
Evaluation/AverageDiscountedReturn          -41.7661
Evaluation/AverageReturn                    -41.7661
Evaluation/CompletionRate                     0
Evaluation/Iteration                        780
Evaluation/MaxReturn                        -27.6375
Evaluation/MinReturn                        -82.8625
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.7864
Extras/EpisodeRewardMean                    -41.8067
LinearFeatureBaseline/ExplainedVariance       0.836914
PolicyExecTime                                0.234165
ProcessExecTime                               0.0311377
TotalEnvSteps                            790372
policy/Entropy                               -1.89581
policy/KL                                     0.00971584
policy/KLBefore                               0
policy/LossAfter                             -0.0283535
policy/LossBefore                            -6.36097e-09
policy/Perplexity                             0.150197
policy/dLoss                                  0.0283535
---------------------------------------  ----------------
2021-06-04 13:58:14 | [train_policy] epoch #781 | Obtaining samples for iteration 781...
2021-06-04 13:58:15 | [train_policy] epoch #781 | Logging diagnostics...
2021-06-04 13:58:15 | [train_policy] epoch #781 | Optimizing policy...
2021-06-04 13:58:15 | [train_policy] epoch #781 | Computing loss before
2021-06-04 13:58:15 | [train_policy] epoch #781 | Computing KL before
2021-06-04 13:58:15 | [train_policy] epoch #781 | Optimizing
2021-06-04 13:58:15 | [train_policy] epoch #781 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:15 | [train_policy] epoch #781 | computing loss before
2021-06-04 13:58:15 | [train_policy] epoch #781 | computing gradient
2021-06-04 13:58:15 | [train_policy] epoch #781 | gradient computed
2021-06-04 13:58:15 | [train_policy] epoch #781 | computing descent direction
2021-06-04 13:58:15 | [train_policy] epoch #781 | descent direction computed
2021-06-04 13:58:15 | [train_policy] epoch #781 | backtrack iters: 0
2021-06-04 13:58:15 | [train_policy] epoch #781 | optimization finished
2021-06-04 13:58:15 | [train_policy] epoch #781 | Computing KL after
2021-06-04 13:58:15 | [train_policy] epoch #781 | Computing loss after
2021-06-04 13:58:15 | [train_policy] epoch #781 | Fitting baseline...
2021-06-04 13:58:15 | [train_policy] epoch #781 | Saving snapshot...
2021-06-04 13:58:15 | [train_policy] epoch #781 | Saved
2021-06-04 13:58:15 | [train_policy] epoch #781 | Time 627.08 s
2021-06-04 13:58:15 | [train_policy] epoch #781 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284599
Evaluation/AverageDiscountedReturn          -63.4346
Evaluation/AverageReturn                    -63.4346
Evaluation/CompletionRate                     0
Evaluation/Iteration                        781
Evaluation/MaxReturn                        -30.078
Evaluation/MinReturn                      -2061.86
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.659
Extras/EpisodeRewardMean                    -61.253
LinearFeatureBaseline/ExplainedVariance       0.0155705
PolicyExecTime                                0.22103
ProcessExecTime                               0.0312016
TotalEnvSteps                            791384
policy/Entropy                               -1.87847
policy/KL                                     0.00958562
policy/KLBefore                               0
policy/LossAfter                             -0.0281583
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.152823
policy/dLoss                                  0.0281583
---------------------------------------  ----------------
2021-06-04 13:58:15 | [train_policy] epoch #782 | Obtaining samples for iteration 782...
2021-06-04 13:58:16 | [train_policy] epoch #782 | Logging diagnostics...
2021-06-04 13:58:16 | [train_policy] epoch #782 | Optimizing policy...
2021-06-04 13:58:16 | [train_policy] epoch #782 | Computing loss before
2021-06-04 13:58:16 | [train_policy] epoch #782 | Computing KL before
2021-06-04 13:58:16 | [train_policy] epoch #782 | Optimizing
2021-06-04 13:58:16 | [train_policy] epoch #782 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:16 | [train_policy] epoch #782 | computing loss before
2021-06-04 13:58:16 | [train_policy] epoch #782 | computing gradient
2021-06-04 13:58:16 | [train_policy] epoch #782 | gradient computed
2021-06-04 13:58:16 | [train_policy] epoch #782 | computing descent direction
2021-06-04 13:58:16 | [train_policy] epoch #782 | descent direction computed
2021-06-04 13:58:16 | [train_policy] epoch #782 | backtrack iters: 1
2021-06-04 13:58:16 | [train_policy] epoch #782 | optimization finished
2021-06-04 13:58:16 | [train_policy] epoch #782 | Computing KL after
2021-06-04 13:58:16 | [train_policy] epoch #782 | Computing loss after
2021-06-04 13:58:16 | [train_policy] epoch #782 | Fitting baseline...
2021-06-04 13:58:16 | [train_policy] epoch #782 | Saving snapshot...
2021-06-04 13:58:16 | [train_policy] epoch #782 | Saved
2021-06-04 13:58:16 | [train_policy] epoch #782 | Time 627.89 s
2021-06-04 13:58:16 | [train_policy] epoch #782 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.289402
Evaluation/AverageDiscountedReturn          -42.4377
Evaluation/AverageReturn                    -42.4377
Evaluation/CompletionRate                     0
Evaluation/Iteration                        782
Evaluation/MaxReturn                        -31.1618
Evaluation/MinReturn                        -88.1635
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.67146
Extras/EpisodeRewardMean                    -42.2279
LinearFeatureBaseline/ExplainedVariance     -66.7141
PolicyExecTime                                0.225277
ProcessExecTime                               0.0315905
TotalEnvSteps                            792396
policy/Entropy                               -1.89942
policy/KL                                     0.00661878
policy/KLBefore                               0
policy/LossAfter                             -0.00999565
policy/LossBefore                             8.71688e-09
policy/Perplexity                             0.149656
policy/dLoss                                  0.00999566
---------------------------------------  ----------------
2021-06-04 13:58:16 | [train_policy] epoch #783 | Obtaining samples for iteration 783...
2021-06-04 13:58:16 | [train_policy] epoch #783 | Logging diagnostics...
2021-06-04 13:58:16 | [train_policy] epoch #783 | Optimizing policy...
2021-06-04 13:58:16 | [train_policy] epoch #783 | Computing loss before
2021-06-04 13:58:16 | [train_policy] epoch #783 | Computing KL before
2021-06-04 13:58:16 | [train_policy] epoch #783 | Optimizing
2021-06-04 13:58:16 | [train_policy] epoch #783 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:16 | [train_policy] epoch #783 | computing loss before
2021-06-04 13:58:16 | [train_policy] epoch #783 | computing gradient
2021-06-04 13:58:16 | [train_policy] epoch #783 | gradient computed
2021-06-04 13:58:16 | [train_policy] epoch #783 | computing descent direction
2021-06-04 13:58:16 | [train_policy] epoch #783 | descent direction computed
2021-06-04 13:58:16 | [train_policy] epoch #783 | backtrack iters: 0
2021-06-04 13:58:16 | [train_policy] epoch #783 | optimization finished
2021-06-04 13:58:16 | [train_policy] epoch #783 | Computing KL after
2021-06-04 13:58:16 | [train_policy] epoch #783 | Computing loss after
2021-06-04 13:58:16 | [train_policy] epoch #783 | Fitting baseline...
2021-06-04 13:58:16 | [train_policy] epoch #783 | Saving snapshot...
2021-06-04 13:58:16 | [train_policy] epoch #783 | Saved
2021-06-04 13:58:16 | [train_policy] epoch #783 | Time 628.69 s
2021-06-04 13:58:16 | [train_policy] epoch #783 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286098
Evaluation/AverageDiscountedReturn          -40.8081
Evaluation/AverageReturn                    -40.8081
Evaluation/CompletionRate                     0
Evaluation/Iteration                        783
Evaluation/MaxReturn                        -27.7482
Evaluation/MinReturn                        -63.8807
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.52709
Extras/EpisodeRewardMean                    -40.917
LinearFeatureBaseline/ExplainedVariance       0.884059
PolicyExecTime                                0.227605
ProcessExecTime                               0.0314572
TotalEnvSteps                            793408
policy/Entropy                               -1.88622
policy/KL                                     0.0098738
policy/KLBefore                               0
policy/LossAfter                             -0.0143319
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.151643
policy/dLoss                                  0.0143319
---------------------------------------  ----------------
2021-06-04 13:58:16 | [train_policy] epoch #784 | Obtaining samples for iteration 784...
2021-06-04 13:58:17 | [train_policy] epoch #784 | Logging diagnostics...
2021-06-04 13:58:17 | [train_policy] epoch #784 | Optimizing policy...
2021-06-04 13:58:17 | [train_policy] epoch #784 | Computing loss before
2021-06-04 13:58:17 | [train_policy] epoch #784 | Computing KL before
2021-06-04 13:58:17 | [train_policy] epoch #784 | Optimizing
2021-06-04 13:58:17 | [train_policy] epoch #784 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:17 | [train_policy] epoch #784 | computing loss before
2021-06-04 13:58:17 | [train_policy] epoch #784 | computing gradient
2021-06-04 13:58:17 | [train_policy] epoch #784 | gradient computed
2021-06-04 13:58:17 | [train_policy] epoch #784 | computing descent direction
2021-06-04 13:58:17 | [train_policy] epoch #784 | descent direction computed
2021-06-04 13:58:17 | [train_policy] epoch #784 | backtrack iters: 0
2021-06-04 13:58:17 | [train_policy] epoch #784 | optimization finished
2021-06-04 13:58:17 | [train_policy] epoch #784 | Computing KL after
2021-06-04 13:58:17 | [train_policy] epoch #784 | Computing loss after
2021-06-04 13:58:17 | [train_policy] epoch #784 | Fitting baseline...
2021-06-04 13:58:17 | [train_policy] epoch #784 | Saving snapshot...
2021-06-04 13:58:17 | [train_policy] epoch #784 | Saved
2021-06-04 13:58:17 | [train_policy] epoch #784 | Time 629.49 s
2021-06-04 13:58:17 | [train_policy] epoch #784 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284728
Evaluation/AverageDiscountedReturn          -40.9841
Evaluation/AverageReturn                    -40.9841
Evaluation/CompletionRate                     0
Evaluation/Iteration                        784
Evaluation/MaxReturn                        -28.9091
Evaluation/MinReturn                        -63.8803
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.68472
Extras/EpisodeRewardMean                    -40.8857
LinearFeatureBaseline/ExplainedVariance       0.900771
PolicyExecTime                                0.231218
ProcessExecTime                               0.031132
TotalEnvSteps                            794420
policy/Entropy                               -1.83007
policy/KL                                     0.0095299
policy/KLBefore                               0
policy/LossAfter                             -0.0167721
policy/LossBefore                            -6.21373e-09
policy/Perplexity                             0.160402
policy/dLoss                                  0.0167721
---------------------------------------  ----------------
2021-06-04 13:58:17 | [train_policy] epoch #785 | Obtaining samples for iteration 785...
2021-06-04 13:58:18 | [train_policy] epoch #785 | Logging diagnostics...
2021-06-04 13:58:18 | [train_policy] epoch #785 | Optimizing policy...
2021-06-04 13:58:18 | [train_policy] epoch #785 | Computing loss before
2021-06-04 13:58:18 | [train_policy] epoch #785 | Computing KL before
2021-06-04 13:58:18 | [train_policy] epoch #785 | Optimizing
2021-06-04 13:58:18 | [train_policy] epoch #785 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:18 | [train_policy] epoch #785 | computing loss before
2021-06-04 13:58:18 | [train_policy] epoch #785 | computing gradient
2021-06-04 13:58:18 | [train_policy] epoch #785 | gradient computed
2021-06-04 13:58:18 | [train_policy] epoch #785 | computing descent direction
2021-06-04 13:58:18 | [train_policy] epoch #785 | descent direction computed
2021-06-04 13:58:18 | [train_policy] epoch #785 | backtrack iters: 1
2021-06-04 13:58:18 | [train_policy] epoch #785 | optimization finished
2021-06-04 13:58:18 | [train_policy] epoch #785 | Computing KL after
2021-06-04 13:58:18 | [train_policy] epoch #785 | Computing loss after
2021-06-04 13:58:18 | [train_policy] epoch #785 | Fitting baseline...
2021-06-04 13:58:18 | [train_policy] epoch #785 | Saving snapshot...
2021-06-04 13:58:18 | [train_policy] epoch #785 | Saved
2021-06-04 13:58:18 | [train_policy] epoch #785 | Time 630.29 s
2021-06-04 13:58:18 | [train_policy] epoch #785 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.284917
Evaluation/AverageDiscountedReturn          -41.869
Evaluation/AverageReturn                    -41.869
Evaluation/CompletionRate                     0
Evaluation/Iteration                        785
Evaluation/MaxReturn                        -28.6955
Evaluation/MinReturn                        -90.1588
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3819
Extras/EpisodeRewardMean                    -41.8349
LinearFeatureBaseline/ExplainedVariance       0.84105
PolicyExecTime                                0.228471
ProcessExecTime                               0.031167
TotalEnvSteps                            795432
policy/Entropy                               -1.83773
policy/KL                                     0.00651209
policy/KLBefore                               0
policy/LossAfter                             -0.0211782
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.159178
policy/dLoss                                  0.0211782
---------------------------------------  ---------------
2021-06-04 13:58:18 | [train_policy] epoch #786 | Obtaining samples for iteration 786...
2021-06-04 13:58:19 | [train_policy] epoch #786 | Logging diagnostics...
2021-06-04 13:58:19 | [train_policy] epoch #786 | Optimizing policy...
2021-06-04 13:58:19 | [train_policy] epoch #786 | Computing loss before
2021-06-04 13:58:19 | [train_policy] epoch #786 | Computing KL before
2021-06-04 13:58:19 | [train_policy] epoch #786 | Optimizing
2021-06-04 13:58:19 | [train_policy] epoch #786 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:19 | [train_policy] epoch #786 | computing loss before
2021-06-04 13:58:19 | [train_policy] epoch #786 | computing gradient
2021-06-04 13:58:19 | [train_policy] epoch #786 | gradient computed
2021-06-04 13:58:19 | [train_policy] epoch #786 | computing descent direction
2021-06-04 13:58:19 | [train_policy] epoch #786 | descent direction computed
2021-06-04 13:58:19 | [train_policy] epoch #786 | backtrack iters: 0
2021-06-04 13:58:19 | [train_policy] epoch #786 | optimization finished
2021-06-04 13:58:19 | [train_policy] epoch #786 | Computing KL after
2021-06-04 13:58:19 | [train_policy] epoch #786 | Computing loss after
2021-06-04 13:58:19 | [train_policy] epoch #786 | Fitting baseline...
2021-06-04 13:58:19 | [train_policy] epoch #786 | Saving snapshot...
2021-06-04 13:58:19 | [train_policy] epoch #786 | Saved
2021-06-04 13:58:19 | [train_policy] epoch #786 | Time 631.08 s
2021-06-04 13:58:19 | [train_policy] epoch #786 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.285364
Evaluation/AverageDiscountedReturn          -42.5333
Evaluation/AverageReturn                    -42.5333
Evaluation/CompletionRate                     0
Evaluation/Iteration                        786
Evaluation/MaxReturn                        -27.5865
Evaluation/MinReturn                        -86.5568
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.6526
Extras/EpisodeRewardMean                    -42.2092
LinearFeatureBaseline/ExplainedVariance       0.831233
PolicyExecTime                                0.222259
ProcessExecTime                               0.0312889
TotalEnvSteps                            796444
policy/Entropy                               -1.79202
policy/KL                                     0.0092088
policy/KLBefore                               0
policy/LossAfter                             -0.0173828
policy/LossBefore                             1.5549e-08
policy/Perplexity                             0.166623
policy/dLoss                                  0.0173828
---------------------------------------  ---------------
2021-06-04 13:58:19 | [train_policy] epoch #787 | Obtaining samples for iteration 787...
2021-06-04 13:58:19 | [train_policy] epoch #787 | Logging diagnostics...
2021-06-04 13:58:19 | [train_policy] epoch #787 | Optimizing policy...
2021-06-04 13:58:19 | [train_policy] epoch #787 | Computing loss before
2021-06-04 13:58:20 | [train_policy] epoch #787 | Computing KL before
2021-06-04 13:58:20 | [train_policy] epoch #787 | Optimizing
2021-06-04 13:58:20 | [train_policy] epoch #787 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:20 | [train_policy] epoch #787 | computing loss before
2021-06-04 13:58:20 | [train_policy] epoch #787 | computing gradient
2021-06-04 13:58:20 | [train_policy] epoch #787 | gradient computed
2021-06-04 13:58:20 | [train_policy] epoch #787 | computing descent direction
2021-06-04 13:58:20 | [train_policy] epoch #787 | descent direction computed
2021-06-04 13:58:20 | [train_policy] epoch #787 | backtrack iters: 1
2021-06-04 13:58:20 | [train_policy] epoch #787 | optimization finished
2021-06-04 13:58:20 | [train_policy] epoch #787 | Computing KL after
2021-06-04 13:58:20 | [train_policy] epoch #787 | Computing loss after
2021-06-04 13:58:20 | [train_policy] epoch #787 | Fitting baseline...
2021-06-04 13:58:20 | [train_policy] epoch #787 | Saving snapshot...
2021-06-04 13:58:20 | [train_policy] epoch #787 | Saved
2021-06-04 13:58:20 | [train_policy] epoch #787 | Time 631.87 s
2021-06-04 13:58:20 | [train_policy] epoch #787 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.288102
Evaluation/AverageDiscountedReturn          -40.9992
Evaluation/AverageReturn                    -40.9992
Evaluation/CompletionRate                     0
Evaluation/Iteration                        787
Evaluation/MaxReturn                        -28.4761
Evaluation/MinReturn                        -63.8794
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.58402
Extras/EpisodeRewardMean                    -41.2961
LinearFeatureBaseline/ExplainedVariance       0.854678
PolicyExecTime                                0.216995
ProcessExecTime                               0.0315182
TotalEnvSteps                            797456
policy/Entropy                               -1.82208
policy/KL                                     0.00667826
policy/KLBefore                               0
policy/LossAfter                             -0.016167
policy/LossBefore                            -0
policy/Perplexity                             0.161689
policy/dLoss                                  0.016167
---------------------------------------  ---------------
2021-06-04 13:58:20 | [train_policy] epoch #788 | Obtaining samples for iteration 788...
2021-06-04 13:58:20 | [train_policy] epoch #788 | Logging diagnostics...
2021-06-04 13:58:20 | [train_policy] epoch #788 | Optimizing policy...
2021-06-04 13:58:20 | [train_policy] epoch #788 | Computing loss before
2021-06-04 13:58:20 | [train_policy] epoch #788 | Computing KL before
2021-06-04 13:58:20 | [train_policy] epoch #788 | Optimizing
2021-06-04 13:58:20 | [train_policy] epoch #788 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:20 | [train_policy] epoch #788 | computing loss before
2021-06-04 13:58:20 | [train_policy] epoch #788 | computing gradient
2021-06-04 13:58:20 | [train_policy] epoch #788 | gradient computed
2021-06-04 13:58:20 | [train_policy] epoch #788 | computing descent direction
2021-06-04 13:58:20 | [train_policy] epoch #788 | descent direction computed
2021-06-04 13:58:20 | [train_policy] epoch #788 | backtrack iters: 0
2021-06-04 13:58:20 | [train_policy] epoch #788 | optimization finished
2021-06-04 13:58:20 | [train_policy] epoch #788 | Computing KL after
2021-06-04 13:58:20 | [train_policy] epoch #788 | Computing loss after
2021-06-04 13:58:20 | [train_policy] epoch #788 | Fitting baseline...
2021-06-04 13:58:20 | [train_policy] epoch #788 | Saving snapshot...
2021-06-04 13:58:20 | [train_policy] epoch #788 | Saved
2021-06-04 13:58:20 | [train_policy] epoch #788 | Time 632.68 s
2021-06-04 13:58:20 | [train_policy] epoch #788 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287543
Evaluation/AverageDiscountedReturn          -41.0052
Evaluation/AverageReturn                    -41.0052
Evaluation/CompletionRate                     0
Evaluation/Iteration                        788
Evaluation/MaxReturn                        -28.148
Evaluation/MinReturn                        -76.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.94474
Extras/EpisodeRewardMean                    -40.8329
LinearFeatureBaseline/ExplainedVariance       0.881565
PolicyExecTime                                0.229916
ProcessExecTime                               0.0314384
TotalEnvSteps                            798468
policy/Entropy                               -1.79239
policy/KL                                     0.00964374
policy/KLBefore                               0
policy/LossAfter                             -0.0230216
policy/LossBefore                             7.77452e-09
policy/Perplexity                             0.166561
policy/dLoss                                  0.0230216
---------------------------------------  ----------------
2021-06-04 13:58:20 | [train_policy] epoch #789 | Obtaining samples for iteration 789...
2021-06-04 13:58:21 | [train_policy] epoch #789 | Logging diagnostics...
2021-06-04 13:58:21 | [train_policy] epoch #789 | Optimizing policy...
2021-06-04 13:58:21 | [train_policy] epoch #789 | Computing loss before
2021-06-04 13:58:21 | [train_policy] epoch #789 | Computing KL before
2021-06-04 13:58:21 | [train_policy] epoch #789 | Optimizing
2021-06-04 13:58:21 | [train_policy] epoch #789 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:21 | [train_policy] epoch #789 | computing loss before
2021-06-04 13:58:21 | [train_policy] epoch #789 | computing gradient
2021-06-04 13:58:21 | [train_policy] epoch #789 | gradient computed
2021-06-04 13:58:21 | [train_policy] epoch #789 | computing descent direction
2021-06-04 13:58:21 | [train_policy] epoch #789 | descent direction computed
2021-06-04 13:58:21 | [train_policy] epoch #789 | backtrack iters: 1
2021-06-04 13:58:21 | [train_policy] epoch #789 | optimization finished
2021-06-04 13:58:21 | [train_policy] epoch #789 | Computing KL after
2021-06-04 13:58:21 | [train_policy] epoch #789 | Computing loss after
2021-06-04 13:58:21 | [train_policy] epoch #789 | Fitting baseline...
2021-06-04 13:58:21 | [train_policy] epoch #789 | Saving snapshot...
2021-06-04 13:58:21 | [train_policy] epoch #789 | Saved
2021-06-04 13:58:21 | [train_policy] epoch #789 | Time 633.49 s
2021-06-04 13:58:21 | [train_policy] epoch #789 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285107
Evaluation/AverageDiscountedReturn          -40.9382
Evaluation/AverageReturn                    -40.9382
Evaluation/CompletionRate                     0
Evaluation/Iteration                        789
Evaluation/MaxReturn                        -30.2111
Evaluation/MinReturn                        -78.4547
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.59991
Extras/EpisodeRewardMean                    -41.2377
LinearFeatureBaseline/ExplainedVariance       0.868662
PolicyExecTime                                0.229981
ProcessExecTime                               0.0311553
TotalEnvSteps                            799480
policy/Entropy                               -1.79086
policy/KL                                     0.00647608
policy/KLBefore                               0
policy/LossAfter                             -0.013484
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.166817
policy/dLoss                                  0.013484
---------------------------------------  ----------------
2021-06-04 13:58:21 | [train_policy] epoch #790 | Obtaining samples for iteration 790...
2021-06-04 13:58:22 | [train_policy] epoch #790 | Logging diagnostics...
2021-06-04 13:58:22 | [train_policy] epoch #790 | Optimizing policy...
2021-06-04 13:58:22 | [train_policy] epoch #790 | Computing loss before
2021-06-04 13:58:22 | [train_policy] epoch #790 | Computing KL before
2021-06-04 13:58:22 | [train_policy] epoch #790 | Optimizing
2021-06-04 13:58:22 | [train_policy] epoch #790 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:22 | [train_policy] epoch #790 | computing loss before
2021-06-04 13:58:22 | [train_policy] epoch #790 | computing gradient
2021-06-04 13:58:22 | [train_policy] epoch #790 | gradient computed
2021-06-04 13:58:22 | [train_policy] epoch #790 | computing descent direction
2021-06-04 13:58:22 | [train_policy] epoch #790 | descent direction computed
2021-06-04 13:58:22 | [train_policy] epoch #790 | backtrack iters: 0
2021-06-04 13:58:22 | [train_policy] epoch #790 | optimization finished
2021-06-04 13:58:22 | [train_policy] epoch #790 | Computing KL after
2021-06-04 13:58:22 | [train_policy] epoch #790 | Computing loss after
2021-06-04 13:58:22 | [train_policy] epoch #790 | Fitting baseline...
2021-06-04 13:58:22 | [train_policy] epoch #790 | Saving snapshot...
2021-06-04 13:58:22 | [train_policy] epoch #790 | Saved
2021-06-04 13:58:22 | [train_policy] epoch #790 | Time 634.28 s
2021-06-04 13:58:22 | [train_policy] epoch #790 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285437
Evaluation/AverageDiscountedReturn          -41.0011
Evaluation/AverageReturn                    -41.0011
Evaluation/CompletionRate                     0
Evaluation/Iteration                        790
Evaluation/MaxReturn                        -28.0019
Evaluation/MinReturn                        -64.0634
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.79761
Extras/EpisodeRewardMean                    -40.9579
LinearFeatureBaseline/ExplainedVariance       0.887842
PolicyExecTime                                0.22436
ProcessExecTime                               0.0312688
TotalEnvSteps                            800492
policy/Entropy                               -1.78654
policy/KL                                     0.00963577
policy/KLBefore                               0
policy/LossAfter                             -0.0216244
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.167538
policy/dLoss                                  0.0216244
---------------------------------------  ----------------
2021-06-04 13:58:22 | [train_policy] epoch #791 | Obtaining samples for iteration 791...
2021-06-04 13:58:23 | [train_policy] epoch #791 | Logging diagnostics...
2021-06-04 13:58:23 | [train_policy] epoch #791 | Optimizing policy...
2021-06-04 13:58:23 | [train_policy] epoch #791 | Computing loss before
2021-06-04 13:58:23 | [train_policy] epoch #791 | Computing KL before
2021-06-04 13:58:23 | [train_policy] epoch #791 | Optimizing
2021-06-04 13:58:23 | [train_policy] epoch #791 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:23 | [train_policy] epoch #791 | computing loss before
2021-06-04 13:58:23 | [train_policy] epoch #791 | computing gradient
2021-06-04 13:58:23 | [train_policy] epoch #791 | gradient computed
2021-06-04 13:58:23 | [train_policy] epoch #791 | computing descent direction
2021-06-04 13:58:23 | [train_policy] epoch #791 | descent direction computed
2021-06-04 13:58:23 | [train_policy] epoch #791 | backtrack iters: 1
2021-06-04 13:58:23 | [train_policy] epoch #791 | optimization finished
2021-06-04 13:58:23 | [train_policy] epoch #791 | Computing KL after
2021-06-04 13:58:23 | [train_policy] epoch #791 | Computing loss after
2021-06-04 13:58:23 | [train_policy] epoch #791 | Fitting baseline...
2021-06-04 13:58:23 | [train_policy] epoch #791 | Saving snapshot...
2021-06-04 13:58:23 | [train_policy] epoch #791 | Saved
2021-06-04 13:58:23 | [train_policy] epoch #791 | Time 635.09 s
2021-06-04 13:58:23 | [train_policy] epoch #791 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285999
Evaluation/AverageDiscountedReturn          -43.1258
Evaluation/AverageReturn                    -43.1258
Evaluation/CompletionRate                     0
Evaluation/Iteration                        791
Evaluation/MaxReturn                        -29.9162
Evaluation/MinReturn                        -86.2515
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.59861
Extras/EpisodeRewardMean                    -43.0222
LinearFeatureBaseline/ExplainedVariance       0.84322
PolicyExecTime                                0.225274
ProcessExecTime                               0.0312812
TotalEnvSteps                            801504
policy/Entropy                               -1.80571
policy/KL                                     0.00665683
policy/KLBefore                               0
policy/LossAfter                             -0.0187994
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             0.164357
policy/dLoss                                  0.0187994
---------------------------------------  ----------------
2021-06-04 13:58:23 | [train_policy] epoch #792 | Obtaining samples for iteration 792...
2021-06-04 13:58:24 | [train_policy] epoch #792 | Logging diagnostics...
2021-06-04 13:58:24 | [train_policy] epoch #792 | Optimizing policy...
2021-06-04 13:58:24 | [train_policy] epoch #792 | Computing loss before
2021-06-04 13:58:24 | [train_policy] epoch #792 | Computing KL before
2021-06-04 13:58:24 | [train_policy] epoch #792 | Optimizing
2021-06-04 13:58:24 | [train_policy] epoch #792 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:24 | [train_policy] epoch #792 | computing loss before
2021-06-04 13:58:24 | [train_policy] epoch #792 | computing gradient
2021-06-04 13:58:24 | [train_policy] epoch #792 | gradient computed
2021-06-04 13:58:24 | [train_policy] epoch #792 | computing descent direction
2021-06-04 13:58:24 | [train_policy] epoch #792 | descent direction computed
2021-06-04 13:58:24 | [train_policy] epoch #792 | backtrack iters: 1
2021-06-04 13:58:24 | [train_policy] epoch #792 | optimization finished
2021-06-04 13:58:24 | [train_policy] epoch #792 | Computing KL after
2021-06-04 13:58:24 | [train_policy] epoch #792 | Computing loss after
2021-06-04 13:58:24 | [train_policy] epoch #792 | Fitting baseline...
2021-06-04 13:58:24 | [train_policy] epoch #792 | Saving snapshot...
2021-06-04 13:58:24 | [train_policy] epoch #792 | Saved
2021-06-04 13:58:24 | [train_policy] epoch #792 | Time 635.89 s
2021-06-04 13:58:24 | [train_policy] epoch #792 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285394
Evaluation/AverageDiscountedReturn          -41.1319
Evaluation/AverageReturn                    -41.1319
Evaluation/CompletionRate                     0
Evaluation/Iteration                        792
Evaluation/MaxReturn                        -28.1902
Evaluation/MinReturn                        -86.545
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.81224
Extras/EpisodeRewardMean                    -41.565
LinearFeatureBaseline/ExplainedVariance       0.846798
PolicyExecTime                                0.225014
ProcessExecTime                               0.0312583
TotalEnvSteps                            802516
policy/Entropy                               -1.8426
policy/KL                                     0.00645094
policy/KLBefore                               0
policy/LossAfter                             -0.0185488
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.158406
policy/dLoss                                  0.0185488
---------------------------------------  ----------------
2021-06-04 13:58:24 | [train_policy] epoch #793 | Obtaining samples for iteration 793...
2021-06-04 13:58:24 | [train_policy] epoch #793 | Logging diagnostics...
2021-06-04 13:58:24 | [train_policy] epoch #793 | Optimizing policy...
2021-06-04 13:58:24 | [train_policy] epoch #793 | Computing loss before
2021-06-04 13:58:24 | [train_policy] epoch #793 | Computing KL before
2021-06-04 13:58:24 | [train_policy] epoch #793 | Optimizing
2021-06-04 13:58:24 | [train_policy] epoch #793 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:24 | [train_policy] epoch #793 | computing loss before
2021-06-04 13:58:24 | [train_policy] epoch #793 | computing gradient
2021-06-04 13:58:24 | [train_policy] epoch #793 | gradient computed
2021-06-04 13:58:24 | [train_policy] epoch #793 | computing descent direction
2021-06-04 13:58:24 | [train_policy] epoch #793 | descent direction computed
2021-06-04 13:58:24 | [train_policy] epoch #793 | backtrack iters: 0
2021-06-04 13:58:24 | [train_policy] epoch #793 | optimization finished
2021-06-04 13:58:24 | [train_policy] epoch #793 | Computing KL after
2021-06-04 13:58:24 | [train_policy] epoch #793 | Computing loss after
2021-06-04 13:58:24 | [train_policy] epoch #793 | Fitting baseline...
2021-06-04 13:58:24 | [train_policy] epoch #793 | Saving snapshot...
2021-06-04 13:58:24 | [train_policy] epoch #793 | Saved
2021-06-04 13:58:24 | [train_policy] epoch #793 | Time 636.68 s
2021-06-04 13:58:24 | [train_policy] epoch #793 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284726
Evaluation/AverageDiscountedReturn          -41.0096
Evaluation/AverageReturn                    -41.0096
Evaluation/CompletionRate                     0
Evaluation/Iteration                        793
Evaluation/MaxReturn                        -28.474
Evaluation/MinReturn                        -88.3657
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.59851
Extras/EpisodeRewardMean                    -41.0051
LinearFeatureBaseline/ExplainedVariance       0.846539
PolicyExecTime                                0.223149
ProcessExecTime                               0.0312552
TotalEnvSteps                            803528
policy/Entropy                               -1.8648
policy/KL                                     0.00985144
policy/KLBefore                               0
policy/LossAfter                             -0.0214769
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.154927
policy/dLoss                                  0.0214769
---------------------------------------  ----------------
2021-06-04 13:58:24 | [train_policy] epoch #794 | Obtaining samples for iteration 794...
2021-06-04 13:58:25 | [train_policy] epoch #794 | Logging diagnostics...
2021-06-04 13:58:25 | [train_policy] epoch #794 | Optimizing policy...
2021-06-04 13:58:25 | [train_policy] epoch #794 | Computing loss before
2021-06-04 13:58:25 | [train_policy] epoch #794 | Computing KL before
2021-06-04 13:58:25 | [train_policy] epoch #794 | Optimizing
2021-06-04 13:58:25 | [train_policy] epoch #794 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:25 | [train_policy] epoch #794 | computing loss before
2021-06-04 13:58:25 | [train_policy] epoch #794 | computing gradient
2021-06-04 13:58:25 | [train_policy] epoch #794 | gradient computed
2021-06-04 13:58:25 | [train_policy] epoch #794 | computing descent direction
2021-06-04 13:58:25 | [train_policy] epoch #794 | descent direction computed
2021-06-04 13:58:25 | [train_policy] epoch #794 | backtrack iters: 1
2021-06-04 13:58:25 | [train_policy] epoch #794 | optimization finished
2021-06-04 13:58:25 | [train_policy] epoch #794 | Computing KL after
2021-06-04 13:58:25 | [train_policy] epoch #794 | Computing loss after
2021-06-04 13:58:25 | [train_policy] epoch #794 | Fitting baseline...
2021-06-04 13:58:25 | [train_policy] epoch #794 | Saving snapshot...
2021-06-04 13:58:25 | [train_policy] epoch #794 | Saved
2021-06-04 13:58:25 | [train_policy] epoch #794 | Time 637.46 s
2021-06-04 13:58:25 | [train_policy] epoch #794 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284745
Evaluation/AverageDiscountedReturn          -42.5795
Evaluation/AverageReturn                    -42.5795
Evaluation/CompletionRate                     0
Evaluation/Iteration                        794
Evaluation/MaxReturn                        -30.3499
Evaluation/MinReturn                        -77.9525
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.99697
Extras/EpisodeRewardMean                    -42.1622
LinearFeatureBaseline/ExplainedVariance       0.856099
PolicyExecTime                                0.213074
ProcessExecTime                               0.0311754
TotalEnvSteps                            804540
policy/Entropy                               -1.88104
policy/KL                                     0.00662142
policy/KLBefore                               0
policy/LossAfter                             -0.0164855
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.152432
policy/dLoss                                  0.0164855
---------------------------------------  ----------------
2021-06-04 13:58:25 | [train_policy] epoch #795 | Obtaining samples for iteration 795...
2021-06-04 13:58:26 | [train_policy] epoch #795 | Logging diagnostics...
2021-06-04 13:58:26 | [train_policy] epoch #795 | Optimizing policy...
2021-06-04 13:58:26 | [train_policy] epoch #795 | Computing loss before
2021-06-04 13:58:26 | [train_policy] epoch #795 | Computing KL before
2021-06-04 13:58:26 | [train_policy] epoch #795 | Optimizing
2021-06-04 13:58:26 | [train_policy] epoch #795 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:26 | [train_policy] epoch #795 | computing loss before
2021-06-04 13:58:26 | [train_policy] epoch #795 | computing gradient
2021-06-04 13:58:26 | [train_policy] epoch #795 | gradient computed
2021-06-04 13:58:26 | [train_policy] epoch #795 | computing descent direction
2021-06-04 13:58:26 | [train_policy] epoch #795 | descent direction computed
2021-06-04 13:58:26 | [train_policy] epoch #795 | backtrack iters: 1
2021-06-04 13:58:26 | [train_policy] epoch #795 | optimization finished
2021-06-04 13:58:26 | [train_policy] epoch #795 | Computing KL after
2021-06-04 13:58:26 | [train_policy] epoch #795 | Computing loss after
2021-06-04 13:58:26 | [train_policy] epoch #795 | Fitting baseline...
2021-06-04 13:58:26 | [train_policy] epoch #795 | Saving snapshot...
2021-06-04 13:58:26 | [train_policy] epoch #795 | Saved
2021-06-04 13:58:26 | [train_policy] epoch #795 | Time 638.26 s
2021-06-04 13:58:26 | [train_policy] epoch #795 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284194
Evaluation/AverageDiscountedReturn          -41.5255
Evaluation/AverageReturn                    -41.5255
Evaluation/CompletionRate                     0
Evaluation/Iteration                        795
Evaluation/MaxReturn                        -31.4053
Evaluation/MinReturn                        -62.6708
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.38535
Extras/EpisodeRewardMean                    -41.254
LinearFeatureBaseline/ExplainedVariance       0.909335
PolicyExecTime                                0.224093
ProcessExecTime                               0.0311582
TotalEnvSteps                            805552
policy/Entropy                               -1.89459
policy/KL                                     0.00649042
policy/KLBefore                               0
policy/LossAfter                             -0.0166252
policy/LossBefore                             9.18807e-09
policy/Perplexity                             0.15038
policy/dLoss                                  0.0166252
---------------------------------------  ----------------
2021-06-04 13:58:26 | [train_policy] epoch #796 | Obtaining samples for iteration 796...
2021-06-04 13:58:27 | [train_policy] epoch #796 | Logging diagnostics...
2021-06-04 13:58:27 | [train_policy] epoch #796 | Optimizing policy...
2021-06-04 13:58:27 | [train_policy] epoch #796 | Computing loss before
2021-06-04 13:58:27 | [train_policy] epoch #796 | Computing KL before
2021-06-04 13:58:27 | [train_policy] epoch #796 | Optimizing
2021-06-04 13:58:27 | [train_policy] epoch #796 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:27 | [train_policy] epoch #796 | computing loss before
2021-06-04 13:58:27 | [train_policy] epoch #796 | computing gradient
2021-06-04 13:58:27 | [train_policy] epoch #796 | gradient computed
2021-06-04 13:58:27 | [train_policy] epoch #796 | computing descent direction
2021-06-04 13:58:27 | [train_policy] epoch #796 | descent direction computed
2021-06-04 13:58:27 | [train_policy] epoch #796 | backtrack iters: 1
2021-06-04 13:58:27 | [train_policy] epoch #796 | optimization finished
2021-06-04 13:58:27 | [train_policy] epoch #796 | Computing KL after
2021-06-04 13:58:27 | [train_policy] epoch #796 | Computing loss after
2021-06-04 13:58:27 | [train_policy] epoch #796 | Fitting baseline...
2021-06-04 13:58:27 | [train_policy] epoch #796 | Saving snapshot...
2021-06-04 13:58:27 | [train_policy] epoch #796 | Saved
2021-06-04 13:58:27 | [train_policy] epoch #796 | Time 639.07 s
2021-06-04 13:58:27 | [train_policy] epoch #796 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.290004
Evaluation/AverageDiscountedReturn          -41.9521
Evaluation/AverageReturn                    -41.9521
Evaluation/CompletionRate                     0
Evaluation/Iteration                        796
Evaluation/MaxReturn                        -31.7542
Evaluation/MinReturn                        -57.3737
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.26144
Extras/EpisodeRewardMean                    -42.067
LinearFeatureBaseline/ExplainedVariance       0.882337
PolicyExecTime                                0.224656
ProcessExecTime                               0.0316808
TotalEnvSteps                            806564
policy/Entropy                               -1.88794
policy/KL                                     0.00663674
policy/KLBefore                               0
policy/LossAfter                             -0.014504
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             0.151383
policy/dLoss                                  0.014504
---------------------------------------  ----------------
2021-06-04 13:58:27 | [train_policy] epoch #797 | Obtaining samples for iteration 797...
2021-06-04 13:58:27 | [train_policy] epoch #797 | Logging diagnostics...
2021-06-04 13:58:27 | [train_policy] epoch #797 | Optimizing policy...
2021-06-04 13:58:27 | [train_policy] epoch #797 | Computing loss before
2021-06-04 13:58:27 | [train_policy] epoch #797 | Computing KL before
2021-06-04 13:58:27 | [train_policy] epoch #797 | Optimizing
2021-06-04 13:58:27 | [train_policy] epoch #797 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:27 | [train_policy] epoch #797 | computing loss before
2021-06-04 13:58:27 | [train_policy] epoch #797 | computing gradient
2021-06-04 13:58:28 | [train_policy] epoch #797 | gradient computed
2021-06-04 13:58:28 | [train_policy] epoch #797 | computing descent direction
2021-06-04 13:58:28 | [train_policy] epoch #797 | descent direction computed
2021-06-04 13:58:28 | [train_policy] epoch #797 | backtrack iters: 0
2021-06-04 13:58:28 | [train_policy] epoch #797 | optimization finished
2021-06-04 13:58:28 | [train_policy] epoch #797 | Computing KL after
2021-06-04 13:58:28 | [train_policy] epoch #797 | Computing loss after
2021-06-04 13:58:28 | [train_policy] epoch #797 | Fitting baseline...
2021-06-04 13:58:28 | [train_policy] epoch #797 | Saving snapshot...
2021-06-04 13:58:28 | [train_policy] epoch #797 | Saved
2021-06-04 13:58:28 | [train_policy] epoch #797 | Time 639.86 s
2021-06-04 13:58:28 | [train_policy] epoch #797 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285321
Evaluation/AverageDiscountedReturn          -40.5096
Evaluation/AverageReturn                    -40.5096
Evaluation/CompletionRate                     0
Evaluation/Iteration                        797
Evaluation/MaxReturn                        -28.2904
Evaluation/MinReturn                        -63.83
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.07253
Extras/EpisodeRewardMean                    -40.547
LinearFeatureBaseline/ExplainedVariance       0.887077
PolicyExecTime                                0.220462
ProcessExecTime                               0.031199
TotalEnvSteps                            807576
policy/Entropy                               -1.86745
policy/KL                                     0.00949324
policy/KLBefore                               0
policy/LossAfter                             -0.0146052
policy/LossBefore                            -1.00126e-08
policy/Perplexity                             0.154517
policy/dLoss                                  0.0146052
---------------------------------------  ----------------
2021-06-04 13:58:28 | [train_policy] epoch #798 | Obtaining samples for iteration 798...
2021-06-04 13:58:28 | [train_policy] epoch #798 | Logging diagnostics...
2021-06-04 13:58:28 | [train_policy] epoch #798 | Optimizing policy...
2021-06-04 13:58:28 | [train_policy] epoch #798 | Computing loss before
2021-06-04 13:58:28 | [train_policy] epoch #798 | Computing KL before
2021-06-04 13:58:28 | [train_policy] epoch #798 | Optimizing
2021-06-04 13:58:28 | [train_policy] epoch #798 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:28 | [train_policy] epoch #798 | computing loss before
2021-06-04 13:58:28 | [train_policy] epoch #798 | computing gradient
2021-06-04 13:58:28 | [train_policy] epoch #798 | gradient computed
2021-06-04 13:58:28 | [train_policy] epoch #798 | computing descent direction
2021-06-04 13:58:28 | [train_policy] epoch #798 | descent direction computed
2021-06-04 13:58:28 | [train_policy] epoch #798 | backtrack iters: 1
2021-06-04 13:58:28 | [train_policy] epoch #798 | optimization finished
2021-06-04 13:58:28 | [train_policy] epoch #798 | Computing KL after
2021-06-04 13:58:28 | [train_policy] epoch #798 | Computing loss after
2021-06-04 13:58:28 | [train_policy] epoch #798 | Fitting baseline...
2021-06-04 13:58:28 | [train_policy] epoch #798 | Saving snapshot...
2021-06-04 13:58:28 | [train_policy] epoch #798 | Saved
2021-06-04 13:58:28 | [train_policy] epoch #798 | Time 640.67 s
2021-06-04 13:58:28 | [train_policy] epoch #798 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285045
Evaluation/AverageDiscountedReturn          -62.5805
Evaluation/AverageReturn                    -62.5805
Evaluation/CompletionRate                     0
Evaluation/Iteration                        798
Evaluation/MaxReturn                        -28.318
Evaluation/MinReturn                      -2061.97
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.74
Extras/EpisodeRewardMean                    -60.9432
LinearFeatureBaseline/ExplainedVariance       0.0124164
PolicyExecTime                                0.229438
ProcessExecTime                               0.0311573
TotalEnvSteps                            808588
policy/Entropy                               -1.87344
policy/KL                                     0.00659685
policy/KLBefore                               0
policy/LossAfter                             -0.0286539
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.153595
policy/dLoss                                  0.0286539
---------------------------------------  ----------------
2021-06-04 13:58:28 | [train_policy] epoch #799 | Obtaining samples for iteration 799...
2021-06-04 13:58:29 | [train_policy] epoch #799 | Logging diagnostics...
2021-06-04 13:58:29 | [train_policy] epoch #799 | Optimizing policy...
2021-06-04 13:58:29 | [train_policy] epoch #799 | Computing loss before
2021-06-04 13:58:29 | [train_policy] epoch #799 | Computing KL before
2021-06-04 13:58:29 | [train_policy] epoch #799 | Optimizing
2021-06-04 13:58:29 | [train_policy] epoch #799 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:29 | [train_policy] epoch #799 | computing loss before
2021-06-04 13:58:29 | [train_policy] epoch #799 | computing gradient
2021-06-04 13:58:29 | [train_policy] epoch #799 | gradient computed
2021-06-04 13:58:29 | [train_policy] epoch #799 | computing descent direction
2021-06-04 13:58:29 | [train_policy] epoch #799 | descent direction computed
2021-06-04 13:58:29 | [train_policy] epoch #799 | backtrack iters: 0
2021-06-04 13:58:29 | [train_policy] epoch #799 | optimization finished
2021-06-04 13:58:29 | [train_policy] epoch #799 | Computing KL after
2021-06-04 13:58:29 | [train_policy] epoch #799 | Computing loss after
2021-06-04 13:58:29 | [train_policy] epoch #799 | Fitting baseline...
2021-06-04 13:58:29 | [train_policy] epoch #799 | Saving snapshot...
2021-06-04 13:58:29 | [train_policy] epoch #799 | Saved
2021-06-04 13:58:29 | [train_policy] epoch #799 | Time 641.47 s
2021-06-04 13:58:29 | [train_policy] epoch #799 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285228
Evaluation/AverageDiscountedReturn          -41.167
Evaluation/AverageReturn                    -41.167
Evaluation/CompletionRate                     0
Evaluation/Iteration                        799
Evaluation/MaxReturn                        -29.0665
Evaluation/MinReturn                       -107.984
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.81686
Extras/EpisodeRewardMean                    -61.3964
LinearFeatureBaseline/ExplainedVariance     -27.7474
PolicyExecTime                                0.234422
ProcessExecTime                               0.0312304
TotalEnvSteps                            809600
policy/Entropy                               -1.8667
policy/KL                                     0.00970787
policy/KLBefore                               0
policy/LossAfter                             -0.0386681
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.154633
policy/dLoss                                  0.0386682
---------------------------------------  ----------------
2021-06-04 13:58:29 | [train_policy] epoch #800 | Obtaining samples for iteration 800...
2021-06-04 13:58:30 | [train_policy] epoch #800 | Logging diagnostics...
2021-06-04 13:58:30 | [train_policy] epoch #800 | Optimizing policy...
2021-06-04 13:58:30 | [train_policy] epoch #800 | Computing loss before
2021-06-04 13:58:30 | [train_policy] epoch #800 | Computing KL before
2021-06-04 13:58:30 | [train_policy] epoch #800 | Optimizing
2021-06-04 13:58:30 | [train_policy] epoch #800 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:30 | [train_policy] epoch #800 | computing loss before
2021-06-04 13:58:30 | [train_policy] epoch #800 | computing gradient
2021-06-04 13:58:30 | [train_policy] epoch #800 | gradient computed
2021-06-04 13:58:30 | [train_policy] epoch #800 | computing descent direction
2021-06-04 13:58:30 | [train_policy] epoch #800 | descent direction computed
2021-06-04 13:58:30 | [train_policy] epoch #800 | backtrack iters: 1
2021-06-04 13:58:30 | [train_policy] epoch #800 | optimization finished
2021-06-04 13:58:30 | [train_policy] epoch #800 | Computing KL after
2021-06-04 13:58:30 | [train_policy] epoch #800 | Computing loss after
2021-06-04 13:58:30 | [train_policy] epoch #800 | Fitting baseline...
2021-06-04 13:58:30 | [train_policy] epoch #800 | Saving snapshot...
2021-06-04 13:58:30 | [train_policy] epoch #800 | Saved
2021-06-04 13:58:30 | [train_policy] epoch #800 | Time 642.25 s
2021-06-04 13:58:30 | [train_policy] epoch #800 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.284975
Evaluation/AverageDiscountedReturn          -41.8822
Evaluation/AverageReturn                    -41.8822
Evaluation/CompletionRate                     0
Evaluation/Iteration                        800
Evaluation/MaxReturn                        -29.9883
Evaluation/MinReturn                        -63.9029
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.36797
Extras/EpisodeRewardMean                    -42.8434
LinearFeatureBaseline/ExplainedVariance       0.888534
PolicyExecTime                                0.212178
ProcessExecTime                               0.0313163
TotalEnvSteps                            810612
policy/Entropy                               -1.86521
policy/KL                                     0.00653142
policy/KLBefore                               0
policy/LossAfter                             -0.0165055
policy/LossBefore                             1.0366e-08
policy/Perplexity                             0.154864
policy/dLoss                                  0.0165055
---------------------------------------  ---------------
2021-06-04 13:58:30 | [train_policy] epoch #801 | Obtaining samples for iteration 801...
2021-06-04 13:58:31 | [train_policy] epoch #801 | Logging diagnostics...
2021-06-04 13:58:31 | [train_policy] epoch #801 | Optimizing policy...
2021-06-04 13:58:31 | [train_policy] epoch #801 | Computing loss before
2021-06-04 13:58:31 | [train_policy] epoch #801 | Computing KL before
2021-06-04 13:58:31 | [train_policy] epoch #801 | Optimizing
2021-06-04 13:58:31 | [train_policy] epoch #801 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:31 | [train_policy] epoch #801 | computing loss before
2021-06-04 13:58:31 | [train_policy] epoch #801 | computing gradient
2021-06-04 13:58:31 | [train_policy] epoch #801 | gradient computed
2021-06-04 13:58:31 | [train_policy] epoch #801 | computing descent direction
2021-06-04 13:58:31 | [train_policy] epoch #801 | descent direction computed
2021-06-04 13:58:31 | [train_policy] epoch #801 | backtrack iters: 1
2021-06-04 13:58:31 | [train_policy] epoch #801 | optimization finished
2021-06-04 13:58:31 | [train_policy] epoch #801 | Computing KL after
2021-06-04 13:58:31 | [train_policy] epoch #801 | Computing loss after
2021-06-04 13:58:31 | [train_policy] epoch #801 | Fitting baseline...
2021-06-04 13:58:31 | [train_policy] epoch #801 | Saving snapshot...
2021-06-04 13:58:31 | [train_policy] epoch #801 | Saved
2021-06-04 13:58:31 | [train_policy] epoch #801 | Time 643.05 s
2021-06-04 13:58:31 | [train_policy] epoch #801 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287932
Evaluation/AverageDiscountedReturn          -62.5484
Evaluation/AverageReturn                    -62.5484
Evaluation/CompletionRate                     0
Evaluation/Iteration                        801
Evaluation/MaxReturn                        -29.1951
Evaluation/MinReturn                      -2061.73
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.712
Extras/EpisodeRewardMean                    -61.2986
LinearFeatureBaseline/ExplainedVariance       0.0136464
PolicyExecTime                                0.214693
ProcessExecTime                               0.0315194
TotalEnvSteps                            811624
policy/Entropy                               -1.84685
policy/KL                                     0.0064168
policy/KLBefore                               0
policy/LossAfter                             -0.0187921
policy/LossBefore                             1.22508e-08
policy/Perplexity                             0.157734
policy/dLoss                                  0.0187921
---------------------------------------  ----------------
2021-06-04 13:58:31 | [train_policy] epoch #802 | Obtaining samples for iteration 802...
2021-06-04 13:58:31 | [train_policy] epoch #802 | Logging diagnostics...
2021-06-04 13:58:31 | [train_policy] epoch #802 | Optimizing policy...
2021-06-04 13:58:31 | [train_policy] epoch #802 | Computing loss before
2021-06-04 13:58:31 | [train_policy] epoch #802 | Computing KL before
2021-06-04 13:58:31 | [train_policy] epoch #802 | Optimizing
2021-06-04 13:58:31 | [train_policy] epoch #802 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:31 | [train_policy] epoch #802 | computing loss before
2021-06-04 13:58:31 | [train_policy] epoch #802 | computing gradient
2021-06-04 13:58:31 | [train_policy] epoch #802 | gradient computed
2021-06-04 13:58:31 | [train_policy] epoch #802 | computing descent direction
2021-06-04 13:58:32 | [train_policy] epoch #802 | descent direction computed
2021-06-04 13:58:32 | [train_policy] epoch #802 | backtrack iters: 1
2021-06-04 13:58:32 | [train_policy] epoch #802 | optimization finished
2021-06-04 13:58:32 | [train_policy] epoch #802 | Computing KL after
2021-06-04 13:58:32 | [train_policy] epoch #802 | Computing loss after
2021-06-04 13:58:32 | [train_policy] epoch #802 | Fitting baseline...
2021-06-04 13:58:32 | [train_policy] epoch #802 | Saving snapshot...
2021-06-04 13:58:32 | [train_policy] epoch #802 | Saved
2021-06-04 13:58:32 | [train_policy] epoch #802 | Time 643.86 s
2021-06-04 13:58:32 | [train_policy] epoch #802 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284979
Evaluation/AverageDiscountedReturn          -41.8615
Evaluation/AverageReturn                    -41.8615
Evaluation/CompletionRate                     0
Evaluation/Iteration                        802
Evaluation/MaxReturn                        -28.3124
Evaluation/MinReturn                        -97.0528
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.87141
Extras/EpisodeRewardMean                    -41.5589
LinearFeatureBaseline/ExplainedVariance     -32.8249
PolicyExecTime                                0.225544
ProcessExecTime                               0.0312641
TotalEnvSteps                            812636
policy/Entropy                               -1.84953
policy/KL                                     0.00644769
policy/KLBefore                               0
policy/LossAfter                             -0.00878586
policy/LossBefore                             1.88473e-08
policy/Perplexity                             0.157311
policy/dLoss                                  0.00878588
---------------------------------------  ----------------
2021-06-04 13:58:32 | [train_policy] epoch #803 | Obtaining samples for iteration 803...
2021-06-04 13:58:32 | [train_policy] epoch #803 | Logging diagnostics...
2021-06-04 13:58:32 | [train_policy] epoch #803 | Optimizing policy...
2021-06-04 13:58:32 | [train_policy] epoch #803 | Computing loss before
2021-06-04 13:58:32 | [train_policy] epoch #803 | Computing KL before
2021-06-04 13:58:32 | [train_policy] epoch #803 | Optimizing
2021-06-04 13:58:32 | [train_policy] epoch #803 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:32 | [train_policy] epoch #803 | computing loss before
2021-06-04 13:58:32 | [train_policy] epoch #803 | computing gradient
2021-06-04 13:58:32 | [train_policy] epoch #803 | gradient computed
2021-06-04 13:58:32 | [train_policy] epoch #803 | computing descent direction
2021-06-04 13:58:32 | [train_policy] epoch #803 | descent direction computed
2021-06-04 13:58:32 | [train_policy] epoch #803 | backtrack iters: 0
2021-06-04 13:58:32 | [train_policy] epoch #803 | optimization finished
2021-06-04 13:58:32 | [train_policy] epoch #803 | Computing KL after
2021-06-04 13:58:32 | [train_policy] epoch #803 | Computing loss after
2021-06-04 13:58:32 | [train_policy] epoch #803 | Fitting baseline...
2021-06-04 13:58:32 | [train_policy] epoch #803 | Saving snapshot...
2021-06-04 13:58:32 | [train_policy] epoch #803 | Saved
2021-06-04 13:58:32 | [train_policy] epoch #803 | Time 644.64 s
2021-06-04 13:58:32 | [train_policy] epoch #803 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285875
Evaluation/AverageDiscountedReturn          -64.1338
Evaluation/AverageReturn                    -64.1338
Evaluation/CompletionRate                     0
Evaluation/Iteration                        803
Evaluation/MaxReturn                        -28.2958
Evaluation/MinReturn                      -2061.81
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.593
Extras/EpisodeRewardMean                    -62.4379
LinearFeatureBaseline/ExplainedVariance       0.016999
PolicyExecTime                                0.219634
ProcessExecTime                               0.0312464
TotalEnvSteps                            813648
policy/Entropy                               -1.825
policy/KL                                     0.00987576
policy/KLBefore                               0
policy/LossAfter                             -0.0230603
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.161218
policy/dLoss                                  0.0230603
---------------------------------------  ----------------
2021-06-04 13:58:32 | [train_policy] epoch #804 | Obtaining samples for iteration 804...
2021-06-04 13:58:33 | [train_policy] epoch #804 | Logging diagnostics...
2021-06-04 13:58:33 | [train_policy] epoch #804 | Optimizing policy...
2021-06-04 13:58:33 | [train_policy] epoch #804 | Computing loss before
2021-06-04 13:58:33 | [train_policy] epoch #804 | Computing KL before
2021-06-04 13:58:33 | [train_policy] epoch #804 | Optimizing
2021-06-04 13:58:33 | [train_policy] epoch #804 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:33 | [train_policy] epoch #804 | computing loss before
2021-06-04 13:58:33 | [train_policy] epoch #804 | computing gradient
2021-06-04 13:58:33 | [train_policy] epoch #804 | gradient computed
2021-06-04 13:58:33 | [train_policy] epoch #804 | computing descent direction
2021-06-04 13:58:33 | [train_policy] epoch #804 | descent direction computed
2021-06-04 13:58:33 | [train_policy] epoch #804 | backtrack iters: 1
2021-06-04 13:58:33 | [train_policy] epoch #804 | optimization finished
2021-06-04 13:58:33 | [train_policy] epoch #804 | Computing KL after
2021-06-04 13:58:33 | [train_policy] epoch #804 | Computing loss after
2021-06-04 13:58:33 | [train_policy] epoch #804 | Fitting baseline...
2021-06-04 13:58:33 | [train_policy] epoch #804 | Saving snapshot...
2021-06-04 13:58:33 | [train_policy] epoch #804 | Saved
2021-06-04 13:58:33 | [train_policy] epoch #804 | Time 645.47 s
2021-06-04 13:58:33 | [train_policy] epoch #804 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.284381
Evaluation/AverageDiscountedReturn          -66.0805
Evaluation/AverageReturn                    -66.0805
Evaluation/CompletionRate                     0
Evaluation/Iteration                        804
Evaluation/MaxReturn                        -28.829
Evaluation/MinReturn                      -2061.88
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.471
Extras/EpisodeRewardMean                    -64.0995
LinearFeatureBaseline/ExplainedVariance       0.128295
PolicyExecTime                                0.250683
ProcessExecTime                               0.0310967
TotalEnvSteps                            814660
policy/Entropy                               -1.83064
policy/KL                                     0.00646374
policy/KLBefore                               0
policy/LossAfter                             -0.0250591
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             0.16031
policy/dLoss                                  0.0250591
---------------------------------------  ----------------
2021-06-04 13:58:33 | [train_policy] epoch #805 | Obtaining samples for iteration 805...
2021-06-04 13:58:34 | [train_policy] epoch #805 | Logging diagnostics...
2021-06-04 13:58:34 | [train_policy] epoch #805 | Optimizing policy...
2021-06-04 13:58:34 | [train_policy] epoch #805 | Computing loss before
2021-06-04 13:58:34 | [train_policy] epoch #805 | Computing KL before
2021-06-04 13:58:34 | [train_policy] epoch #805 | Optimizing
2021-06-04 13:58:34 | [train_policy] epoch #805 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:34 | [train_policy] epoch #805 | computing loss before
2021-06-04 13:58:34 | [train_policy] epoch #805 | computing gradient
2021-06-04 13:58:34 | [train_policy] epoch #805 | gradient computed
2021-06-04 13:58:34 | [train_policy] epoch #805 | computing descent direction
2021-06-04 13:58:34 | [train_policy] epoch #805 | descent direction computed
2021-06-04 13:58:34 | [train_policy] epoch #805 | backtrack iters: 1
2021-06-04 13:58:34 | [train_policy] epoch #805 | optimization finished
2021-06-04 13:58:34 | [train_policy] epoch #805 | Computing KL after
2021-06-04 13:58:34 | [train_policy] epoch #805 | Computing loss after
2021-06-04 13:58:34 | [train_policy] epoch #805 | Fitting baseline...
2021-06-04 13:58:34 | [train_policy] epoch #805 | Saving snapshot...
2021-06-04 13:58:34 | [train_policy] epoch #805 | Saved
2021-06-04 13:58:34 | [train_policy] epoch #805 | Time 646.26 s
2021-06-04 13:58:34 | [train_policy] epoch #805 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28619
Evaluation/AverageDiscountedReturn          -41.5543
Evaluation/AverageReturn                    -41.5543
Evaluation/CompletionRate                     0
Evaluation/Iteration                        805
Evaluation/MaxReturn                        -28.1613
Evaluation/MinReturn                        -63.8325
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.67036
Extras/EpisodeRewardMean                    -43.9988
LinearFeatureBaseline/ExplainedVariance     -19.7404
PolicyExecTime                                0.215732
ProcessExecTime                               0.0313573
TotalEnvSteps                            815672
policy/Entropy                               -1.8336
policy/KL                                     0.00661187
policy/KLBefore                               0
policy/LossAfter                             -0.0300069
policy/LossBefore                            -1.27219e-08
policy/Perplexity                             0.159837
policy/dLoss                                  0.0300069
---------------------------------------  ----------------
2021-06-04 13:58:34 | [train_policy] epoch #806 | Obtaining samples for iteration 806...
2021-06-04 13:58:35 | [train_policy] epoch #806 | Logging diagnostics...
2021-06-04 13:58:35 | [train_policy] epoch #806 | Optimizing policy...
2021-06-04 13:58:35 | [train_policy] epoch #806 | Computing loss before
2021-06-04 13:58:35 | [train_policy] epoch #806 | Computing KL before
2021-06-04 13:58:35 | [train_policy] epoch #806 | Optimizing
2021-06-04 13:58:35 | [train_policy] epoch #806 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:35 | [train_policy] epoch #806 | computing loss before
2021-06-04 13:58:35 | [train_policy] epoch #806 | computing gradient
2021-06-04 13:58:35 | [train_policy] epoch #806 | gradient computed
2021-06-04 13:58:35 | [train_policy] epoch #806 | computing descent direction
2021-06-04 13:58:35 | [train_policy] epoch #806 | descent direction computed
2021-06-04 13:58:35 | [train_policy] epoch #806 | backtrack iters: 0
2021-06-04 13:58:35 | [train_policy] epoch #806 | optimization finished
2021-06-04 13:58:35 | [train_policy] epoch #806 | Computing KL after
2021-06-04 13:58:35 | [train_policy] epoch #806 | Computing loss after
2021-06-04 13:58:35 | [train_policy] epoch #806 | Fitting baseline...
2021-06-04 13:58:35 | [train_policy] epoch #806 | Saving snapshot...
2021-06-04 13:58:35 | [train_policy] epoch #806 | Saved
2021-06-04 13:58:35 | [train_policy] epoch #806 | Time 647.06 s
2021-06-04 13:58:35 | [train_policy] epoch #806 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.287989
Evaluation/AverageDiscountedReturn          -63.896
Evaluation/AverageReturn                    -63.896
Evaluation/CompletionRate                     0
Evaluation/Iteration                        806
Evaluation/MaxReturn                        -28.6878
Evaluation/MinReturn                      -2061.78
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.672
Extras/EpisodeRewardMean                    -62.0458
LinearFeatureBaseline/ExplainedVariance       0.0135013
PolicyExecTime                                0.222594
ProcessExecTime                               0.0316119
TotalEnvSteps                            816684
policy/Entropy                               -1.81767
policy/KL                                     0.00920253
policy/KLBefore                               0
policy/LossAfter                             -0.0251546
policy/LossBefore                            -0
policy/Perplexity                             0.162403
policy/dLoss                                  0.0251546
---------------------------------------  ---------------
2021-06-04 13:58:35 | [train_policy] epoch #807 | Obtaining samples for iteration 807...
2021-06-04 13:58:35 | [train_policy] epoch #807 | Logging diagnostics...
2021-06-04 13:58:35 | [train_policy] epoch #807 | Optimizing policy...
2021-06-04 13:58:35 | [train_policy] epoch #807 | Computing loss before
2021-06-04 13:58:35 | [train_policy] epoch #807 | Computing KL before
2021-06-04 13:58:35 | [train_policy] epoch #807 | Optimizing
2021-06-04 13:58:35 | [train_policy] epoch #807 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:35 | [train_policy] epoch #807 | computing loss before
2021-06-04 13:58:35 | [train_policy] epoch #807 | computing gradient
2021-06-04 13:58:36 | [train_policy] epoch #807 | gradient computed
2021-06-04 13:58:36 | [train_policy] epoch #807 | computing descent direction
2021-06-04 13:58:36 | [train_policy] epoch #807 | descent direction computed
2021-06-04 13:58:36 | [train_policy] epoch #807 | backtrack iters: 1
2021-06-04 13:58:36 | [train_policy] epoch #807 | optimization finished
2021-06-04 13:58:36 | [train_policy] epoch #807 | Computing KL after
2021-06-04 13:58:36 | [train_policy] epoch #807 | Computing loss after
2021-06-04 13:58:36 | [train_policy] epoch #807 | Fitting baseline...
2021-06-04 13:58:36 | [train_policy] epoch #807 | Saving snapshot...
2021-06-04 13:58:36 | [train_policy] epoch #807 | Saved
2021-06-04 13:58:36 | [train_policy] epoch #807 | Time 647.86 s
2021-06-04 13:58:36 | [train_policy] epoch #807 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285034
Evaluation/AverageDiscountedReturn          -40.6383
Evaluation/AverageReturn                    -40.6383
Evaluation/CompletionRate                     0
Evaluation/Iteration                        807
Evaluation/MaxReturn                        -28.5301
Evaluation/MinReturn                        -60.1802
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.88115
Extras/EpisodeRewardMean                    -40.4232
LinearFeatureBaseline/ExplainedVariance     -28.8966
PolicyExecTime                                0.227005
ProcessExecTime                               0.0312486
TotalEnvSteps                            817696
policy/Entropy                               -1.81357
policy/KL                                     0.00645294
policy/KLBefore                               0
policy/LossAfter                             -0.0310839
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.163072
policy/dLoss                                  0.0310839
---------------------------------------  ----------------
2021-06-04 13:58:36 | [train_policy] epoch #808 | Obtaining samples for iteration 808...
2021-06-04 13:58:36 | [train_policy] epoch #808 | Logging diagnostics...
2021-06-04 13:58:36 | [train_policy] epoch #808 | Optimizing policy...
2021-06-04 13:58:36 | [train_policy] epoch #808 | Computing loss before
2021-06-04 13:58:36 | [train_policy] epoch #808 | Computing KL before
2021-06-04 13:58:36 | [train_policy] epoch #808 | Optimizing
2021-06-04 13:58:36 | [train_policy] epoch #808 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:36 | [train_policy] epoch #808 | computing loss before
2021-06-04 13:58:36 | [train_policy] epoch #808 | computing gradient
2021-06-04 13:58:36 | [train_policy] epoch #808 | gradient computed
2021-06-04 13:58:36 | [train_policy] epoch #808 | computing descent direction
2021-06-04 13:58:36 | [train_policy] epoch #808 | descent direction computed
2021-06-04 13:58:36 | [train_policy] epoch #808 | backtrack iters: 0
2021-06-04 13:58:36 | [train_policy] epoch #808 | optimization finished
2021-06-04 13:58:36 | [train_policy] epoch #808 | Computing KL after
2021-06-04 13:58:36 | [train_policy] epoch #808 | Computing loss after
2021-06-04 13:58:36 | [train_policy] epoch #808 | Fitting baseline...
2021-06-04 13:58:36 | [train_policy] epoch #808 | Saving snapshot...
2021-06-04 13:58:36 | [train_policy] epoch #808 | Saved
2021-06-04 13:58:36 | [train_policy] epoch #808 | Time 648.64 s
2021-06-04 13:58:36 | [train_policy] epoch #808 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28745
Evaluation/AverageDiscountedReturn          -40.7634
Evaluation/AverageReturn                    -40.7634
Evaluation/CompletionRate                     0
Evaluation/Iteration                        808
Evaluation/MaxReturn                        -28.8005
Evaluation/MinReturn                        -60.1814
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.27489
Extras/EpisodeRewardMean                    -40.9153
LinearFeatureBaseline/ExplainedVariance       0.896603
PolicyExecTime                                0.219553
ProcessExecTime                               0.0314231
TotalEnvSteps                            818708
policy/Entropy                               -1.79149
policy/KL                                     0.00970681
policy/KLBefore                               0
policy/LossAfter                             -0.019126
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.166711
policy/dLoss                                  0.019126
---------------------------------------  ----------------
2021-06-04 13:58:36 | [train_policy] epoch #809 | Obtaining samples for iteration 809...
2021-06-04 13:58:37 | [train_policy] epoch #809 | Logging diagnostics...
2021-06-04 13:58:37 | [train_policy] epoch #809 | Optimizing policy...
2021-06-04 13:58:37 | [train_policy] epoch #809 | Computing loss before
2021-06-04 13:58:37 | [train_policy] epoch #809 | Computing KL before
2021-06-04 13:58:37 | [train_policy] epoch #809 | Optimizing
2021-06-04 13:58:37 | [train_policy] epoch #809 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:37 | [train_policy] epoch #809 | computing loss before
2021-06-04 13:58:37 | [train_policy] epoch #809 | computing gradient
2021-06-04 13:58:37 | [train_policy] epoch #809 | gradient computed
2021-06-04 13:58:37 | [train_policy] epoch #809 | computing descent direction
2021-06-04 13:58:37 | [train_policy] epoch #809 | descent direction computed
2021-06-04 13:58:37 | [train_policy] epoch #809 | backtrack iters: 1
2021-06-04 13:58:37 | [train_policy] epoch #809 | optimization finished
2021-06-04 13:58:37 | [train_policy] epoch #809 | Computing KL after
2021-06-04 13:58:37 | [train_policy] epoch #809 | Computing loss after
2021-06-04 13:58:37 | [train_policy] epoch #809 | Fitting baseline...
2021-06-04 13:58:37 | [train_policy] epoch #809 | Saving snapshot...
2021-06-04 13:58:37 | [train_policy] epoch #809 | Saved
2021-06-04 13:58:37 | [train_policy] epoch #809 | Time 649.43 s
2021-06-04 13:58:37 | [train_policy] epoch #809 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284957
Evaluation/AverageDiscountedReturn          -42.7842
Evaluation/AverageReturn                    -42.7842
Evaluation/CompletionRate                     0
Evaluation/Iteration                        809
Evaluation/MaxReturn                        -28.8009
Evaluation/MinReturn                       -120.117
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.2126
Extras/EpisodeRewardMean                    -42.4014
LinearFeatureBaseline/ExplainedVariance       0.741645
PolicyExecTime                                0.217109
ProcessExecTime                               0.0311663
TotalEnvSteps                            819720
policy/Entropy                               -1.79858
policy/KL                                     0.00680697
policy/KLBefore                               0
policy/LossAfter                             -0.0201987
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             0.165533
policy/dLoss                                  0.0201987
---------------------------------------  ----------------
2021-06-04 13:58:37 | [train_policy] epoch #810 | Obtaining samples for iteration 810...
2021-06-04 13:58:38 | [train_policy] epoch #810 | Logging diagnostics...
2021-06-04 13:58:38 | [train_policy] epoch #810 | Optimizing policy...
2021-06-04 13:58:38 | [train_policy] epoch #810 | Computing loss before
2021-06-04 13:58:38 | [train_policy] epoch #810 | Computing KL before
2021-06-04 13:58:38 | [train_policy] epoch #810 | Optimizing
2021-06-04 13:58:38 | [train_policy] epoch #810 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:38 | [train_policy] epoch #810 | computing loss before
2021-06-04 13:58:38 | [train_policy] epoch #810 | computing gradient
2021-06-04 13:58:38 | [train_policy] epoch #810 | gradient computed
2021-06-04 13:58:38 | [train_policy] epoch #810 | computing descent direction
2021-06-04 13:58:38 | [train_policy] epoch #810 | descent direction computed
2021-06-04 13:58:38 | [train_policy] epoch #810 | backtrack iters: 1
2021-06-04 13:58:38 | [train_policy] epoch #810 | optimization finished
2021-06-04 13:58:38 | [train_policy] epoch #810 | Computing KL after
2021-06-04 13:58:38 | [train_policy] epoch #810 | Computing loss after
2021-06-04 13:58:38 | [train_policy] epoch #810 | Fitting baseline...
2021-06-04 13:58:38 | [train_policy] epoch #810 | Saving snapshot...
2021-06-04 13:58:38 | [train_policy] epoch #810 | Saved
2021-06-04 13:58:38 | [train_policy] epoch #810 | Time 650.24 s
2021-06-04 13:58:38 | [train_policy] epoch #810 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286943
Evaluation/AverageDiscountedReturn          -42.7723
Evaluation/AverageReturn                    -42.7723
Evaluation/CompletionRate                     0
Evaluation/Iteration                        810
Evaluation/MaxReturn                        -28.5834
Evaluation/MinReturn                       -139.027
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         13.0429
Extras/EpisodeRewardMean                    -42.6714
LinearFeatureBaseline/ExplainedVariance       0.678343
PolicyExecTime                                0.230841
ProcessExecTime                               0.0314081
TotalEnvSteps                            820732
policy/Entropy                               -1.81319
policy/KL                                     0.00646831
policy/KLBefore                               0
policy/LossAfter                             -0.0197119
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.163132
policy/dLoss                                  0.0197119
---------------------------------------  ----------------
2021-06-04 13:58:38 | [train_policy] epoch #811 | Obtaining samples for iteration 811...
2021-06-04 13:58:39 | [train_policy] epoch #811 | Logging diagnostics...
2021-06-04 13:58:39 | [train_policy] epoch #811 | Optimizing policy...
2021-06-04 13:58:39 | [train_policy] epoch #811 | Computing loss before
2021-06-04 13:58:39 | [train_policy] epoch #811 | Computing KL before
2021-06-04 13:58:39 | [train_policy] epoch #811 | Optimizing
2021-06-04 13:58:39 | [train_policy] epoch #811 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:39 | [train_policy] epoch #811 | computing loss before
2021-06-04 13:58:39 | [train_policy] epoch #811 | computing gradient
2021-06-04 13:58:39 | [train_policy] epoch #811 | gradient computed
2021-06-04 13:58:39 | [train_policy] epoch #811 | computing descent direction
2021-06-04 13:58:39 | [train_policy] epoch #811 | descent direction computed
2021-06-04 13:58:39 | [train_policy] epoch #811 | backtrack iters: 0
2021-06-04 13:58:39 | [train_policy] epoch #811 | optimization finished
2021-06-04 13:58:39 | [train_policy] epoch #811 | Computing KL after
2021-06-04 13:58:39 | [train_policy] epoch #811 | Computing loss after
2021-06-04 13:58:39 | [train_policy] epoch #811 | Fitting baseline...
2021-06-04 13:58:39 | [train_policy] epoch #811 | Saving snapshot...
2021-06-04 13:58:39 | [train_policy] epoch #811 | Saved
2021-06-04 13:58:39 | [train_policy] epoch #811 | Time 651.04 s
2021-06-04 13:58:39 | [train_policy] epoch #811 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284844
Evaluation/AverageDiscountedReturn          -41.4501
Evaluation/AverageReturn                    -41.4501
Evaluation/CompletionRate                     0
Evaluation/Iteration                        811
Evaluation/MaxReturn                        -29.1946
Evaluation/MinReturn                        -80.94
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.77778
Extras/EpisodeRewardMean                    -41.304
LinearFeatureBaseline/ExplainedVariance       0.72221
PolicyExecTime                                0.231715
ProcessExecTime                               0.0311918
TotalEnvSteps                            821744
policy/Entropy                               -1.75004
policy/KL                                     0.00936326
policy/KLBefore                               0
policy/LossAfter                             -0.0272414
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.173767
policy/dLoss                                  0.0272414
---------------------------------------  ----------------
2021-06-04 13:58:39 | [train_policy] epoch #812 | Obtaining samples for iteration 812...
2021-06-04 13:58:39 | [train_policy] epoch #812 | Logging diagnostics...
2021-06-04 13:58:39 | [train_policy] epoch #812 | Optimizing policy...
2021-06-04 13:58:39 | [train_policy] epoch #812 | Computing loss before
2021-06-04 13:58:39 | [train_policy] epoch #812 | Computing KL before
2021-06-04 13:58:39 | [train_policy] epoch #812 | Optimizing
2021-06-04 13:58:39 | [train_policy] epoch #812 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:39 | [train_policy] epoch #812 | computing loss before
2021-06-04 13:58:39 | [train_policy] epoch #812 | computing gradient
2021-06-04 13:58:39 | [train_policy] epoch #812 | gradient computed
2021-06-04 13:58:39 | [train_policy] epoch #812 | computing descent direction
2021-06-04 13:58:40 | [train_policy] epoch #812 | descent direction computed
2021-06-04 13:58:40 | [train_policy] epoch #812 | backtrack iters: 0
2021-06-04 13:58:40 | [train_policy] epoch #812 | optimization finished
2021-06-04 13:58:40 | [train_policy] epoch #812 | Computing KL after
2021-06-04 13:58:40 | [train_policy] epoch #812 | Computing loss after
2021-06-04 13:58:40 | [train_policy] epoch #812 | Fitting baseline...
2021-06-04 13:58:40 | [train_policy] epoch #812 | Saving snapshot...
2021-06-04 13:58:40 | [train_policy] epoch #812 | Saved
2021-06-04 13:58:40 | [train_policy] epoch #812 | Time 651.84 s
2021-06-04 13:58:40 | [train_policy] epoch #812 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.286758
Evaluation/AverageDiscountedReturn          -40.9592
Evaluation/AverageReturn                    -40.9592
Evaluation/CompletionRate                     0
Evaluation/Iteration                        812
Evaluation/MaxReturn                        -29.0104
Evaluation/MinReturn                        -63.8201
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.62393
Extras/EpisodeRewardMean                    -40.8432
LinearFeatureBaseline/ExplainedVariance       0.904693
PolicyExecTime                                0.225083
ProcessExecTime                               0.0314069
TotalEnvSteps                            822756
policy/Entropy                               -1.74376
policy/KL                                     0.00994343
policy/KLBefore                               0
policy/LossAfter                             -0.0215125
policy/LossBefore                            -2.7093e-09
policy/Perplexity                             0.174862
policy/dLoss                                  0.0215125
---------------------------------------  ---------------
2021-06-04 13:58:40 | [train_policy] epoch #813 | Obtaining samples for iteration 813...
2021-06-04 13:58:40 | [train_policy] epoch #813 | Logging diagnostics...
2021-06-04 13:58:40 | [train_policy] epoch #813 | Optimizing policy...
2021-06-04 13:58:40 | [train_policy] epoch #813 | Computing loss before
2021-06-04 13:58:40 | [train_policy] epoch #813 | Computing KL before
2021-06-04 13:58:40 | [train_policy] epoch #813 | Optimizing
2021-06-04 13:58:40 | [train_policy] epoch #813 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:40 | [train_policy] epoch #813 | computing loss before
2021-06-04 13:58:40 | [train_policy] epoch #813 | computing gradient
2021-06-04 13:58:40 | [train_policy] epoch #813 | gradient computed
2021-06-04 13:58:40 | [train_policy] epoch #813 | computing descent direction
2021-06-04 13:58:40 | [train_policy] epoch #813 | descent direction computed
2021-06-04 13:58:40 | [train_policy] epoch #813 | backtrack iters: 1
2021-06-04 13:58:40 | [train_policy] epoch #813 | optimization finished
2021-06-04 13:58:40 | [train_policy] epoch #813 | Computing KL after
2021-06-04 13:58:40 | [train_policy] epoch #813 | Computing loss after
2021-06-04 13:58:40 | [train_policy] epoch #813 | Fitting baseline...
2021-06-04 13:58:40 | [train_policy] epoch #813 | Saving snapshot...
2021-06-04 13:58:40 | [train_policy] epoch #813 | Saved
2021-06-04 13:58:40 | [train_policy] epoch #813 | Time 652.63 s
2021-06-04 13:58:40 | [train_policy] epoch #813 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.287426
Evaluation/AverageDiscountedReturn          -41.6286
Evaluation/AverageReturn                    -41.6286
Evaluation/CompletionRate                     0
Evaluation/Iteration                        813
Evaluation/MaxReturn                        -28.5792
Evaluation/MinReturn                        -62.7468
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.0055
Extras/EpisodeRewardMean                    -41.6884
LinearFeatureBaseline/ExplainedVariance       0.878742
PolicyExecTime                                0.211034
ProcessExecTime                               0.0315192
TotalEnvSteps                            823768
policy/Entropy                               -1.72915
policy/KL                                     0.00640607
policy/KLBefore                               0
policy/LossAfter                             -0.0145808
policy/LossBefore                            -8.83468e-09
policy/Perplexity                             0.177435
policy/dLoss                                  0.0145808
---------------------------------------  ----------------
2021-06-04 13:58:40 | [train_policy] epoch #814 | Obtaining samples for iteration 814...
2021-06-04 13:58:41 | [train_policy] epoch #814 | Logging diagnostics...
2021-06-04 13:58:41 | [train_policy] epoch #814 | Optimizing policy...
2021-06-04 13:58:41 | [train_policy] epoch #814 | Computing loss before
2021-06-04 13:58:41 | [train_policy] epoch #814 | Computing KL before
2021-06-04 13:58:41 | [train_policy] epoch #814 | Optimizing
2021-06-04 13:58:41 | [train_policy] epoch #814 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:41 | [train_policy] epoch #814 | computing loss before
2021-06-04 13:58:41 | [train_policy] epoch #814 | computing gradient
2021-06-04 13:58:41 | [train_policy] epoch #814 | gradient computed
2021-06-04 13:58:41 | [train_policy] epoch #814 | computing descent direction
2021-06-04 13:58:41 | [train_policy] epoch #814 | descent direction computed
2021-06-04 13:58:41 | [train_policy] epoch #814 | backtrack iters: 1
2021-06-04 13:58:41 | [train_policy] epoch #814 | optimization finished
2021-06-04 13:58:41 | [train_policy] epoch #814 | Computing KL after
2021-06-04 13:58:41 | [train_policy] epoch #814 | Computing loss after
2021-06-04 13:58:41 | [train_policy] epoch #814 | Fitting baseline...
2021-06-04 13:58:41 | [train_policy] epoch #814 | Saving snapshot...
2021-06-04 13:58:41 | [train_policy] epoch #814 | Saved
2021-06-04 13:58:41 | [train_policy] epoch #814 | Time 653.43 s
2021-06-04 13:58:41 | [train_policy] epoch #814 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286313
Evaluation/AverageDiscountedReturn          -42.8635
Evaluation/AverageReturn                    -42.8635
Evaluation/CompletionRate                     0
Evaluation/Iteration                        814
Evaluation/MaxReturn                        -29.9705
Evaluation/MinReturn                        -84.4912
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.51502
Extras/EpisodeRewardMean                    -43.3067
LinearFeatureBaseline/ExplainedVariance       0.830854
PolicyExecTime                                0.227962
ProcessExecTime                               0.0312943
TotalEnvSteps                            824780
policy/Entropy                               -1.75149
policy/KL                                     0.00661373
policy/KLBefore                               0
policy/LossAfter                             -0.0181282
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.173515
policy/dLoss                                  0.0181282
---------------------------------------  ----------------
2021-06-04 13:58:41 | [train_policy] epoch #815 | Obtaining samples for iteration 815...
2021-06-04 13:58:42 | [train_policy] epoch #815 | Logging diagnostics...
2021-06-04 13:58:42 | [train_policy] epoch #815 | Optimizing policy...
2021-06-04 13:58:42 | [train_policy] epoch #815 | Computing loss before
2021-06-04 13:58:42 | [train_policy] epoch #815 | Computing KL before
2021-06-04 13:58:42 | [train_policy] epoch #815 | Optimizing
2021-06-04 13:58:42 | [train_policy] epoch #815 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:42 | [train_policy] epoch #815 | computing loss before
2021-06-04 13:58:42 | [train_policy] epoch #815 | computing gradient
2021-06-04 13:58:42 | [train_policy] epoch #815 | gradient computed
2021-06-04 13:58:42 | [train_policy] epoch #815 | computing descent direction
2021-06-04 13:58:42 | [train_policy] epoch #815 | descent direction computed
2021-06-04 13:58:42 | [train_policy] epoch #815 | backtrack iters: 1
2021-06-04 13:58:42 | [train_policy] epoch #815 | optimization finished
2021-06-04 13:58:42 | [train_policy] epoch #815 | Computing KL after
2021-06-04 13:58:42 | [train_policy] epoch #815 | Computing loss after
2021-06-04 13:58:42 | [train_policy] epoch #815 | Fitting baseline...
2021-06-04 13:58:42 | [train_policy] epoch #815 | Saving snapshot...
2021-06-04 13:58:42 | [train_policy] epoch #815 | Saved
2021-06-04 13:58:42 | [train_policy] epoch #815 | Time 654.24 s
2021-06-04 13:58:42 | [train_policy] epoch #815 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286358
Evaluation/AverageDiscountedReturn          -43.5163
Evaluation/AverageReturn                    -43.5163
Evaluation/CompletionRate                     0
Evaluation/Iteration                        815
Evaluation/MaxReturn                        -29.0583
Evaluation/MinReturn                       -166.441
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         14.9752
Extras/EpisodeRewardMean                    -43.4203
LinearFeatureBaseline/ExplainedVariance       0.572825
PolicyExecTime                                0.21293
ProcessExecTime                               0.0312421
TotalEnvSteps                            825792
policy/Entropy                               -1.7303
policy/KL                                     0.006578
policy/KLBefore                               0
policy/LossAfter                             -0.0202471
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.177231
policy/dLoss                                  0.0202471
---------------------------------------  ----------------
2021-06-04 13:58:42 | [train_policy] epoch #816 | Obtaining samples for iteration 816...
2021-06-04 13:58:43 | [train_policy] epoch #816 | Logging diagnostics...
2021-06-04 13:58:43 | [train_policy] epoch #816 | Optimizing policy...
2021-06-04 13:58:43 | [train_policy] epoch #816 | Computing loss before
2021-06-04 13:58:43 | [train_policy] epoch #816 | Computing KL before
2021-06-04 13:58:43 | [train_policy] epoch #816 | Optimizing
2021-06-04 13:58:43 | [train_policy] epoch #816 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:43 | [train_policy] epoch #816 | computing loss before
2021-06-04 13:58:43 | [train_policy] epoch #816 | computing gradient
2021-06-04 13:58:43 | [train_policy] epoch #816 | gradient computed
2021-06-04 13:58:43 | [train_policy] epoch #816 | computing descent direction
2021-06-04 13:58:43 | [train_policy] epoch #816 | descent direction computed
2021-06-04 13:58:43 | [train_policy] epoch #816 | backtrack iters: 1
2021-06-04 13:58:43 | [train_policy] epoch #816 | optimization finished
2021-06-04 13:58:43 | [train_policy] epoch #816 | Computing KL after
2021-06-04 13:58:43 | [train_policy] epoch #816 | Computing loss after
2021-06-04 13:58:43 | [train_policy] epoch #816 | Fitting baseline...
2021-06-04 13:58:43 | [train_policy] epoch #816 | Saving snapshot...
2021-06-04 13:58:43 | [train_policy] epoch #816 | Saved
2021-06-04 13:58:43 | [train_policy] epoch #816 | Time 655.02 s
2021-06-04 13:58:43 | [train_policy] epoch #816 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285632
Evaluation/AverageDiscountedReturn          -46.2755
Evaluation/AverageReturn                    -46.2755
Evaluation/CompletionRate                     0
Evaluation/Iteration                        816
Evaluation/MaxReturn                        -28.0232
Evaluation/MinReturn                       -489.253
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         47.184
Extras/EpisodeRewardMean                    -45.7196
LinearFeatureBaseline/ExplainedVariance       0.1669
PolicyExecTime                                0.215692
ProcessExecTime                               0.0312369
TotalEnvSteps                            826804
policy/Entropy                               -1.72006
policy/KL                                     0.00645577
policy/KLBefore                               0
policy/LossAfter                             -0.0177862
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.179055
policy/dLoss                                  0.0177862
---------------------------------------  ----------------
2021-06-04 13:58:43 | [train_policy] epoch #817 | Obtaining samples for iteration 817...
2021-06-04 13:58:43 | [train_policy] epoch #817 | Logging diagnostics...
2021-06-04 13:58:43 | [train_policy] epoch #817 | Optimizing policy...
2021-06-04 13:58:43 | [train_policy] epoch #817 | Computing loss before
2021-06-04 13:58:43 | [train_policy] epoch #817 | Computing KL before
2021-06-04 13:58:43 | [train_policy] epoch #817 | Optimizing
2021-06-04 13:58:43 | [train_policy] epoch #817 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:43 | [train_policy] epoch #817 | computing loss before
2021-06-04 13:58:43 | [train_policy] epoch #817 | computing gradient
2021-06-04 13:58:43 | [train_policy] epoch #817 | gradient computed
2021-06-04 13:58:43 | [train_policy] epoch #817 | computing descent direction
2021-06-04 13:58:44 | [train_policy] epoch #817 | descent direction computed
2021-06-04 13:58:44 | [train_policy] epoch #817 | backtrack iters: 1
2021-06-04 13:58:44 | [train_policy] epoch #817 | optimization finished
2021-06-04 13:58:44 | [train_policy] epoch #817 | Computing KL after
2021-06-04 13:58:44 | [train_policy] epoch #817 | Computing loss after
2021-06-04 13:58:44 | [train_policy] epoch #817 | Fitting baseline...
2021-06-04 13:58:44 | [train_policy] epoch #817 | Saving snapshot...
2021-06-04 13:58:44 | [train_policy] epoch #817 | Saved
2021-06-04 13:58:44 | [train_policy] epoch #817 | Time 655.82 s
2021-06-04 13:58:44 | [train_policy] epoch #817 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284137
Evaluation/AverageDiscountedReturn          -40.9369
Evaluation/AverageReturn                    -40.9369
Evaluation/CompletionRate                     0
Evaluation/Iteration                        817
Evaluation/MaxReturn                        -28.7617
Evaluation/MinReturn                       -103.649
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.38718
Extras/EpisodeRewardMean                    -40.6974
LinearFeatureBaseline/ExplainedVariance       0.115169
PolicyExecTime                                0.222191
ProcessExecTime                               0.0310826
TotalEnvSteps                            827816
policy/Entropy                               -1.72407
policy/KL                                     0.00653355
policy/KLBefore                               0
policy/LossAfter                             -0.0199029
policy/LossBefore                            -1.24863e-08
policy/Perplexity                             0.178339
policy/dLoss                                  0.0199029
---------------------------------------  ----------------
2021-06-04 13:58:44 | [train_policy] epoch #818 | Obtaining samples for iteration 818...
2021-06-04 13:58:44 | [train_policy] epoch #818 | Logging diagnostics...
2021-06-04 13:58:44 | [train_policy] epoch #818 | Optimizing policy...
2021-06-04 13:58:44 | [train_policy] epoch #818 | Computing loss before
2021-06-04 13:58:44 | [train_policy] epoch #818 | Computing KL before
2021-06-04 13:58:44 | [train_policy] epoch #818 | Optimizing
2021-06-04 13:58:44 | [train_policy] epoch #818 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:44 | [train_policy] epoch #818 | computing loss before
2021-06-04 13:58:44 | [train_policy] epoch #818 | computing gradient
2021-06-04 13:58:44 | [train_policy] epoch #818 | gradient computed
2021-06-04 13:58:44 | [train_policy] epoch #818 | computing descent direction
2021-06-04 13:58:44 | [train_policy] epoch #818 | descent direction computed
2021-06-04 13:58:44 | [train_policy] epoch #818 | backtrack iters: 0
2021-06-04 13:58:44 | [train_policy] epoch #818 | optimization finished
2021-06-04 13:58:44 | [train_policy] epoch #818 | Computing KL after
2021-06-04 13:58:44 | [train_policy] epoch #818 | Computing loss after
2021-06-04 13:58:44 | [train_policy] epoch #818 | Fitting baseline...
2021-06-04 13:58:44 | [train_policy] epoch #818 | Saving snapshot...
2021-06-04 13:58:44 | [train_policy] epoch #818 | Saved
2021-06-04 13:58:44 | [train_policy] epoch #818 | Time 656.61 s
2021-06-04 13:58:44 | [train_policy] epoch #818 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284985
Evaluation/AverageDiscountedReturn          -41.642
Evaluation/AverageReturn                    -41.642
Evaluation/CompletionRate                     0
Evaluation/Iteration                        818
Evaluation/MaxReturn                        -28.6317
Evaluation/MinReturn                        -93.6931
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.16143
Extras/EpisodeRewardMean                    -41.5457
LinearFeatureBaseline/ExplainedVariance       0.813091
PolicyExecTime                                0.226656
ProcessExecTime                               0.0312171
TotalEnvSteps                            828828
policy/Entropy                               -1.71526
policy/KL                                     0.00983191
policy/KLBefore                               0
policy/LossAfter                             -0.0152848
policy/LossBefore                             4.47624e-09
policy/Perplexity                             0.179917
policy/dLoss                                  0.0152848
---------------------------------------  ----------------
2021-06-04 13:58:44 | [train_policy] epoch #819 | Obtaining samples for iteration 819...
2021-06-04 13:58:45 | [train_policy] epoch #819 | Logging diagnostics...
2021-06-04 13:58:45 | [train_policy] epoch #819 | Optimizing policy...
2021-06-04 13:58:45 | [train_policy] epoch #819 | Computing loss before
2021-06-04 13:58:45 | [train_policy] epoch #819 | Computing KL before
2021-06-04 13:58:45 | [train_policy] epoch #819 | Optimizing
2021-06-04 13:58:45 | [train_policy] epoch #819 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:45 | [train_policy] epoch #819 | computing loss before
2021-06-04 13:58:45 | [train_policy] epoch #819 | computing gradient
2021-06-04 13:58:45 | [train_policy] epoch #819 | gradient computed
2021-06-04 13:58:45 | [train_policy] epoch #819 | computing descent direction
2021-06-04 13:58:45 | [train_policy] epoch #819 | descent direction computed
2021-06-04 13:58:45 | [train_policy] epoch #819 | backtrack iters: 1
2021-06-04 13:58:45 | [train_policy] epoch #819 | optimization finished
2021-06-04 13:58:45 | [train_policy] epoch #819 | Computing KL after
2021-06-04 13:58:45 | [train_policy] epoch #819 | Computing loss after
2021-06-04 13:58:45 | [train_policy] epoch #819 | Fitting baseline...
2021-06-04 13:58:45 | [train_policy] epoch #819 | Saving snapshot...
2021-06-04 13:58:45 | [train_policy] epoch #819 | Saved
2021-06-04 13:58:45 | [train_policy] epoch #819 | Time 657.42 s
2021-06-04 13:58:45 | [train_policy] epoch #819 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283889
Evaluation/AverageDiscountedReturn          -42.4829
Evaluation/AverageReturn                    -42.4829
Evaluation/CompletionRate                     0
Evaluation/Iteration                        819
Evaluation/MaxReturn                        -29.7742
Evaluation/MinReturn                        -81.8987
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.95062
Extras/EpisodeRewardMean                    -42.5355
LinearFeatureBaseline/ExplainedVariance       0.869079
PolicyExecTime                                0.231379
ProcessExecTime                               0.0311027
TotalEnvSteps                            829840
policy/Entropy                               -1.73089
policy/KL                                     0.00644903
policy/KLBefore                               0
policy/LossAfter                             -0.0141655
policy/LossBefore                             6.94995e-09
policy/Perplexity                             0.177127
policy/dLoss                                  0.0141656
---------------------------------------  ----------------
2021-06-04 13:58:45 | [train_policy] epoch #820 | Obtaining samples for iteration 820...
2021-06-04 13:58:46 | [train_policy] epoch #820 | Logging diagnostics...
2021-06-04 13:58:46 | [train_policy] epoch #820 | Optimizing policy...
2021-06-04 13:58:46 | [train_policy] epoch #820 | Computing loss before
2021-06-04 13:58:46 | [train_policy] epoch #820 | Computing KL before
2021-06-04 13:58:46 | [train_policy] epoch #820 | Optimizing
2021-06-04 13:58:46 | [train_policy] epoch #820 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:46 | [train_policy] epoch #820 | computing loss before
2021-06-04 13:58:46 | [train_policy] epoch #820 | computing gradient
2021-06-04 13:58:46 | [train_policy] epoch #820 | gradient computed
2021-06-04 13:58:46 | [train_policy] epoch #820 | computing descent direction
2021-06-04 13:58:46 | [train_policy] epoch #820 | descent direction computed
2021-06-04 13:58:46 | [train_policy] epoch #820 | backtrack iters: 0
2021-06-04 13:58:46 | [train_policy] epoch #820 | optimization finished
2021-06-04 13:58:46 | [train_policy] epoch #820 | Computing KL after
2021-06-04 13:58:46 | [train_policy] epoch #820 | Computing loss after
2021-06-04 13:58:46 | [train_policy] epoch #820 | Fitting baseline...
2021-06-04 13:58:46 | [train_policy] epoch #820 | Saving snapshot...
2021-06-04 13:58:46 | [train_policy] epoch #820 | Saved
2021-06-04 13:58:46 | [train_policy] epoch #820 | Time 658.22 s
2021-06-04 13:58:46 | [train_policy] epoch #820 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286648
Evaluation/AverageDiscountedReturn          -42.5159
Evaluation/AverageReturn                    -42.5159
Evaluation/CompletionRate                     0
Evaluation/Iteration                        820
Evaluation/MaxReturn                        -28.0804
Evaluation/MinReturn                        -92.54
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.8453
Extras/EpisodeRewardMean                    -42.2848
LinearFeatureBaseline/ExplainedVariance       0.808627
PolicyExecTime                                0.229292
ProcessExecTime                               0.0314567
TotalEnvSteps                            830852
policy/Entropy                               -1.6777
policy/KL                                     0.00944132
policy/KLBefore                               0
policy/LossAfter                             -0.0208041
policy/LossBefore                             4.47624e-09
policy/Perplexity                             0.186804
policy/dLoss                                  0.0208041
---------------------------------------  ----------------
2021-06-04 13:58:46 | [train_policy] epoch #821 | Obtaining samples for iteration 821...
2021-06-04 13:58:47 | [train_policy] epoch #821 | Logging diagnostics...
2021-06-04 13:58:47 | [train_policy] epoch #821 | Optimizing policy...
2021-06-04 13:58:47 | [train_policy] epoch #821 | Computing loss before
2021-06-04 13:58:47 | [train_policy] epoch #821 | Computing KL before
2021-06-04 13:58:47 | [train_policy] epoch #821 | Optimizing
2021-06-04 13:58:47 | [train_policy] epoch #821 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:47 | [train_policy] epoch #821 | computing loss before
2021-06-04 13:58:47 | [train_policy] epoch #821 | computing gradient
2021-06-04 13:58:47 | [train_policy] epoch #821 | gradient computed
2021-06-04 13:58:47 | [train_policy] epoch #821 | computing descent direction
2021-06-04 13:58:47 | [train_policy] epoch #821 | descent direction computed
2021-06-04 13:58:47 | [train_policy] epoch #821 | backtrack iters: 1
2021-06-04 13:58:47 | [train_policy] epoch #821 | optimization finished
2021-06-04 13:58:47 | [train_policy] epoch #821 | Computing KL after
2021-06-04 13:58:47 | [train_policy] epoch #821 | Computing loss after
2021-06-04 13:58:47 | [train_policy] epoch #821 | Fitting baseline...
2021-06-04 13:58:47 | [train_policy] epoch #821 | Saving snapshot...
2021-06-04 13:58:47 | [train_policy] epoch #821 | Saved
2021-06-04 13:58:47 | [train_policy] epoch #821 | Time 659.04 s
2021-06-04 13:58:47 | [train_policy] epoch #821 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283804
Evaluation/AverageDiscountedReturn          -41.0168
Evaluation/AverageReturn                    -41.0168
Evaluation/CompletionRate                     0
Evaluation/Iteration                        821
Evaluation/MaxReturn                        -28.2757
Evaluation/MinReturn                        -63.8032
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.87981
Extras/EpisodeRewardMean                    -41.4482
LinearFeatureBaseline/ExplainedVariance       0.887526
PolicyExecTime                                0.226977
ProcessExecTime                               0.0311301
TotalEnvSteps                            831864
policy/Entropy                               -1.7095
policy/KL                                     0.00661006
policy/KLBefore                               0
policy/LossAfter                             -0.0212329
policy/LossBefore                            -1.88473e-08
policy/Perplexity                             0.180957
policy/dLoss                                  0.0212329
---------------------------------------  ----------------
2021-06-04 13:58:47 | [train_policy] epoch #822 | Obtaining samples for iteration 822...
2021-06-04 13:58:47 | [train_policy] epoch #822 | Logging diagnostics...
2021-06-04 13:58:47 | [train_policy] epoch #822 | Optimizing policy...
2021-06-04 13:58:47 | [train_policy] epoch #822 | Computing loss before
2021-06-04 13:58:47 | [train_policy] epoch #822 | Computing KL before
2021-06-04 13:58:47 | [train_policy] epoch #822 | Optimizing
2021-06-04 13:58:47 | [train_policy] epoch #822 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:47 | [train_policy] epoch #822 | computing loss before
2021-06-04 13:58:47 | [train_policy] epoch #822 | computing gradient
2021-06-04 13:58:47 | [train_policy] epoch #822 | gradient computed
2021-06-04 13:58:47 | [train_policy] epoch #822 | computing descent direction
2021-06-04 13:58:48 | [train_policy] epoch #822 | descent direction computed
2021-06-04 13:58:48 | [train_policy] epoch #822 | backtrack iters: 1
2021-06-04 13:58:48 | [train_policy] epoch #822 | optimization finished
2021-06-04 13:58:48 | [train_policy] epoch #822 | Computing KL after
2021-06-04 13:58:48 | [train_policy] epoch #822 | Computing loss after
2021-06-04 13:58:48 | [train_policy] epoch #822 | Fitting baseline...
2021-06-04 13:58:48 | [train_policy] epoch #822 | Saving snapshot...
2021-06-04 13:58:48 | [train_policy] epoch #822 | Saved
2021-06-04 13:58:48 | [train_policy] epoch #822 | Time 659.85 s
2021-06-04 13:58:48 | [train_policy] epoch #822 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.288163
Evaluation/AverageDiscountedReturn          -41.5771
Evaluation/AverageReturn                    -41.5771
Evaluation/CompletionRate                     0
Evaluation/Iteration                        822
Evaluation/MaxReturn                        -28.3935
Evaluation/MinReturn                        -87.0603
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.2808
Extras/EpisodeRewardMean                    -41.7028
LinearFeatureBaseline/ExplainedVariance       0.876857
PolicyExecTime                                0.235258
ProcessExecTime                               0.0315602
TotalEnvSteps                            832876
policy/Entropy                               -1.6974
policy/KL                                     0.00650659
policy/KLBefore                               0
policy/LossAfter                             -0.0206599
policy/LossBefore                             4.06395e-09
policy/Perplexity                             0.183159
policy/dLoss                                  0.0206599
---------------------------------------  ----------------
2021-06-04 13:58:48 | [train_policy] epoch #823 | Obtaining samples for iteration 823...
2021-06-04 13:58:48 | [train_policy] epoch #823 | Logging diagnostics...
2021-06-04 13:58:48 | [train_policy] epoch #823 | Optimizing policy...
2021-06-04 13:58:48 | [train_policy] epoch #823 | Computing loss before
2021-06-04 13:58:48 | [train_policy] epoch #823 | Computing KL before
2021-06-04 13:58:48 | [train_policy] epoch #823 | Optimizing
2021-06-04 13:58:48 | [train_policy] epoch #823 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:48 | [train_policy] epoch #823 | computing loss before
2021-06-04 13:58:48 | [train_policy] epoch #823 | computing gradient
2021-06-04 13:58:48 | [train_policy] epoch #823 | gradient computed
2021-06-04 13:58:48 | [train_policy] epoch #823 | computing descent direction
2021-06-04 13:58:48 | [train_policy] epoch #823 | descent direction computed
2021-06-04 13:58:48 | [train_policy] epoch #823 | backtrack iters: 1
2021-06-04 13:58:48 | [train_policy] epoch #823 | optimization finished
2021-06-04 13:58:48 | [train_policy] epoch #823 | Computing KL after
2021-06-04 13:58:48 | [train_policy] epoch #823 | Computing loss after
2021-06-04 13:58:48 | [train_policy] epoch #823 | Fitting baseline...
2021-06-04 13:58:48 | [train_policy] epoch #823 | Saving snapshot...
2021-06-04 13:58:48 | [train_policy] epoch #823 | Saved
2021-06-04 13:58:48 | [train_policy] epoch #823 | Time 660.63 s
2021-06-04 13:58:48 | [train_policy] epoch #823 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.28506
Evaluation/AverageDiscountedReturn          -40.7373
Evaluation/AverageReturn                    -40.7373
Evaluation/CompletionRate                     0
Evaluation/Iteration                        823
Evaluation/MaxReturn                        -28.0644
Evaluation/MinReturn                        -56.5268
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43914
Extras/EpisodeRewardMean                    -40.7033
LinearFeatureBaseline/ExplainedVariance       0.883929
PolicyExecTime                                0.2136
ProcessExecTime                               0.0312459
TotalEnvSteps                            833888
policy/Entropy                               -1.75359
policy/KL                                     0.00677898
policy/KLBefore                               0
policy/LossAfter                             -0.0219337
policy/LossBefore                            -0
policy/Perplexity                             0.173151
policy/dLoss                                  0.0219337
---------------------------------------  ---------------
2021-06-04 13:58:48 | [train_policy] epoch #824 | Obtaining samples for iteration 824...
2021-06-04 13:58:49 | [train_policy] epoch #824 | Logging diagnostics...
2021-06-04 13:58:49 | [train_policy] epoch #824 | Optimizing policy...
2021-06-04 13:58:49 | [train_policy] epoch #824 | Computing loss before
2021-06-04 13:58:49 | [train_policy] epoch #824 | Computing KL before
2021-06-04 13:58:49 | [train_policy] epoch #824 | Optimizing
2021-06-04 13:58:49 | [train_policy] epoch #824 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:49 | [train_policy] epoch #824 | computing loss before
2021-06-04 13:58:49 | [train_policy] epoch #824 | computing gradient
2021-06-04 13:58:49 | [train_policy] epoch #824 | gradient computed
2021-06-04 13:58:49 | [train_policy] epoch #824 | computing descent direction
2021-06-04 13:58:49 | [train_policy] epoch #824 | descent direction computed
2021-06-04 13:58:49 | [train_policy] epoch #824 | backtrack iters: 1
2021-06-04 13:58:49 | [train_policy] epoch #824 | optimization finished
2021-06-04 13:58:49 | [train_policy] epoch #824 | Computing KL after
2021-06-04 13:58:49 | [train_policy] epoch #824 | Computing loss after
2021-06-04 13:58:49 | [train_policy] epoch #824 | Fitting baseline...
2021-06-04 13:58:49 | [train_policy] epoch #824 | Saving snapshot...
2021-06-04 13:58:49 | [train_policy] epoch #824 | Saved
2021-06-04 13:58:49 | [train_policy] epoch #824 | Time 661.44 s
2021-06-04 13:58:49 | [train_policy] epoch #824 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.290531
Evaluation/AverageDiscountedReturn          -40.922
Evaluation/AverageReturn                    -40.922
Evaluation/CompletionRate                     0
Evaluation/Iteration                        824
Evaluation/MaxReturn                        -27.8297
Evaluation/MinReturn                       -107.519
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3517
Extras/EpisodeRewardMean                    -41.0684
LinearFeatureBaseline/ExplainedVariance       0.772443
PolicyExecTime                                0.229098
ProcessExecTime                               0.0317118
TotalEnvSteps                            834900
policy/Entropy                               -1.75081
policy/KL                                     0.00658017
policy/KLBefore                               0
policy/LossAfter                             -0.0277339
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.173633
policy/dLoss                                  0.0277339
---------------------------------------  ----------------
2021-06-04 13:58:49 | [train_policy] epoch #825 | Obtaining samples for iteration 825...
2021-06-04 13:58:50 | [train_policy] epoch #825 | Logging diagnostics...
2021-06-04 13:58:50 | [train_policy] epoch #825 | Optimizing policy...
2021-06-04 13:58:50 | [train_policy] epoch #825 | Computing loss before
2021-06-04 13:58:50 | [train_policy] epoch #825 | Computing KL before
2021-06-04 13:58:50 | [train_policy] epoch #825 | Optimizing
2021-06-04 13:58:50 | [train_policy] epoch #825 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:50 | [train_policy] epoch #825 | computing loss before
2021-06-04 13:58:50 | [train_policy] epoch #825 | computing gradient
2021-06-04 13:58:50 | [train_policy] epoch #825 | gradient computed
2021-06-04 13:58:50 | [train_policy] epoch #825 | computing descent direction
2021-06-04 13:58:50 | [train_policy] epoch #825 | descent direction computed
2021-06-04 13:58:50 | [train_policy] epoch #825 | backtrack iters: 1
2021-06-04 13:58:50 | [train_policy] epoch #825 | optimization finished
2021-06-04 13:58:50 | [train_policy] epoch #825 | Computing KL after
2021-06-04 13:58:50 | [train_policy] epoch #825 | Computing loss after
2021-06-04 13:58:50 | [train_policy] epoch #825 | Fitting baseline...
2021-06-04 13:58:50 | [train_policy] epoch #825 | Saving snapshot...
2021-06-04 13:58:50 | [train_policy] epoch #825 | Saved
2021-06-04 13:58:50 | [train_policy] epoch #825 | Time 662.26 s
2021-06-04 13:58:50 | [train_policy] epoch #825 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.289081
Evaluation/AverageDiscountedReturn          -42.8737
Evaluation/AverageReturn                    -42.8737
Evaluation/CompletionRate                     0
Evaluation/Iteration                        825
Evaluation/MaxReturn                        -30.4759
Evaluation/MinReturn                        -96.873
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.1091
Extras/EpisodeRewardMean                    -42.3448
LinearFeatureBaseline/ExplainedVariance       0.806825
PolicyExecTime                                0.231397
ProcessExecTime                               0.0317252
TotalEnvSteps                            835912
policy/Entropy                               -1.80494
policy/KL                                     0.00707217
policy/KLBefore                               0
policy/LossAfter                             -0.0124657
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.164485
policy/dLoss                                  0.0124657
---------------------------------------  ----------------
2021-06-04 13:58:50 | [train_policy] epoch #826 | Obtaining samples for iteration 826...
2021-06-04 13:58:51 | [train_policy] epoch #826 | Logging diagnostics...
2021-06-04 13:58:51 | [train_policy] epoch #826 | Optimizing policy...
2021-06-04 13:58:51 | [train_policy] epoch #826 | Computing loss before
2021-06-04 13:58:51 | [train_policy] epoch #826 | Computing KL before
2021-06-04 13:58:51 | [train_policy] epoch #826 | Optimizing
2021-06-04 13:58:51 | [train_policy] epoch #826 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:51 | [train_policy] epoch #826 | computing loss before
2021-06-04 13:58:51 | [train_policy] epoch #826 | computing gradient
2021-06-04 13:58:51 | [train_policy] epoch #826 | gradient computed
2021-06-04 13:58:51 | [train_policy] epoch #826 | computing descent direction
2021-06-04 13:58:51 | [train_policy] epoch #826 | descent direction computed
2021-06-04 13:58:51 | [train_policy] epoch #826 | backtrack iters: 0
2021-06-04 13:58:51 | [train_policy] epoch #826 | optimization finished
2021-06-04 13:58:51 | [train_policy] epoch #826 | Computing KL after
2021-06-04 13:58:51 | [train_policy] epoch #826 | Computing loss after
2021-06-04 13:58:51 | [train_policy] epoch #826 | Fitting baseline...
2021-06-04 13:58:51 | [train_policy] epoch #826 | Saving snapshot...
2021-06-04 13:58:51 | [train_policy] epoch #826 | Saved
2021-06-04 13:58:51 | [train_policy] epoch #826 | Time 663.06 s
2021-06-04 13:58:51 | [train_policy] epoch #826 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28428
Evaluation/AverageDiscountedReturn          -41.756
Evaluation/AverageReturn                    -41.756
Evaluation/CompletionRate                     0
Evaluation/Iteration                        826
Evaluation/MaxReturn                        -28.028
Evaluation/MinReturn                       -102.472
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.6308
Extras/EpisodeRewardMean                    -43.0645
LinearFeatureBaseline/ExplainedVariance       0.717996
PolicyExecTime                                0.225369
ProcessExecTime                               0.0311756
TotalEnvSteps                            836924
policy/Entropy                               -1.81535
policy/KL                                     0.00987625
policy/KLBefore                               0
policy/LossAfter                             -0.0208187
policy/LossBefore                             2.40303e-08
policy/Perplexity                             0.162781
policy/dLoss                                  0.0208187
---------------------------------------  ----------------
2021-06-04 13:58:51 | [train_policy] epoch #827 | Obtaining samples for iteration 827...
2021-06-04 13:58:51 | [train_policy] epoch #827 | Logging diagnostics...
2021-06-04 13:58:51 | [train_policy] epoch #827 | Optimizing policy...
2021-06-04 13:58:51 | [train_policy] epoch #827 | Computing loss before
2021-06-04 13:58:51 | [train_policy] epoch #827 | Computing KL before
2021-06-04 13:58:51 | [train_policy] epoch #827 | Optimizing
2021-06-04 13:58:51 | [train_policy] epoch #827 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:51 | [train_policy] epoch #827 | computing loss before
2021-06-04 13:58:51 | [train_policy] epoch #827 | computing gradient
2021-06-04 13:58:51 | [train_policy] epoch #827 | gradient computed
2021-06-04 13:58:51 | [train_policy] epoch #827 | computing descent direction
2021-06-04 13:58:52 | [train_policy] epoch #827 | descent direction computed
2021-06-04 13:58:52 | [train_policy] epoch #827 | backtrack iters: 3
2021-06-04 13:58:52 | [train_policy] epoch #827 | optimization finished
2021-06-04 13:58:52 | [train_policy] epoch #827 | Computing KL after
2021-06-04 13:58:52 | [train_policy] epoch #827 | Computing loss after
2021-06-04 13:58:52 | [train_policy] epoch #827 | Fitting baseline...
2021-06-04 13:58:52 | [train_policy] epoch #827 | Saving snapshot...
2021-06-04 13:58:52 | [train_policy] epoch #827 | Saved
2021-06-04 13:58:52 | [train_policy] epoch #827 | Time 663.85 s
2021-06-04 13:58:52 | [train_policy] epoch #827 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285475
Evaluation/AverageDiscountedReturn          -61.7769
Evaluation/AverageReturn                    -61.7769
Evaluation/CompletionRate                     0
Evaluation/Iteration                        827
Evaluation/MaxReturn                        -28.2592
Evaluation/MinReturn                      -2061.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.931
Extras/EpisodeRewardMean                    -60.0963
LinearFeatureBaseline/ExplainedVariance       0.02167
PolicyExecTime                                0.214651
ProcessExecTime                               0.0311792
TotalEnvSteps                            837936
policy/Entropy                               -1.82973
policy/KL                                     0.00265362
policy/KLBefore                               0
policy/LossAfter                             -6.3229e-05
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.160457
policy/dLoss                                  6.32252e-05
---------------------------------------  ----------------
2021-06-04 13:58:52 | [train_policy] epoch #828 | Obtaining samples for iteration 828...
2021-06-04 13:58:52 | [train_policy] epoch #828 | Logging diagnostics...
2021-06-04 13:58:52 | [train_policy] epoch #828 | Optimizing policy...
2021-06-04 13:58:52 | [train_policy] epoch #828 | Computing loss before
2021-06-04 13:58:52 | [train_policy] epoch #828 | Computing KL before
2021-06-04 13:58:52 | [train_policy] epoch #828 | Optimizing
2021-06-04 13:58:52 | [train_policy] epoch #828 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:52 | [train_policy] epoch #828 | computing loss before
2021-06-04 13:58:52 | [train_policy] epoch #828 | computing gradient
2021-06-04 13:58:52 | [train_policy] epoch #828 | gradient computed
2021-06-04 13:58:52 | [train_policy] epoch #828 | computing descent direction
2021-06-04 13:58:52 | [train_policy] epoch #828 | descent direction computed
2021-06-04 13:58:52 | [train_policy] epoch #828 | backtrack iters: 0
2021-06-04 13:58:52 | [train_policy] epoch #828 | optimization finished
2021-06-04 13:58:52 | [train_policy] epoch #828 | Computing KL after
2021-06-04 13:58:52 | [train_policy] epoch #828 | Computing loss after
2021-06-04 13:58:52 | [train_policy] epoch #828 | Fitting baseline...
2021-06-04 13:58:52 | [train_policy] epoch #828 | Saving snapshot...
2021-06-04 13:58:52 | [train_policy] epoch #828 | Saved
2021-06-04 13:58:52 | [train_policy] epoch #828 | Time 664.64 s
2021-06-04 13:58:52 | [train_policy] epoch #828 | EpochTime 0.76 s
---------------------------------------  ---------------
EnvExecTime                                   0.284497
Evaluation/AverageDiscountedReturn          -42.4658
Evaluation/AverageReturn                    -42.4658
Evaluation/CompletionRate                     0
Evaluation/Iteration                        828
Evaluation/MaxReturn                        -28.6736
Evaluation/MinReturn                       -123.999
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.9657
Extras/EpisodeRewardMean                    -42.3178
LinearFeatureBaseline/ExplainedVariance     -26.3682
PolicyExecTime                                0.224536
ProcessExecTime                               0.0311677
TotalEnvSteps                            838948
policy/Entropy                               -1.82117
policy/KL                                     0.00986755
policy/KLBefore                               0
policy/LossAfter                             -0.0213492
policy/LossBefore                            -4.382e-08
policy/Perplexity                             0.161836
policy/dLoss                                  0.0213491
---------------------------------------  ---------------
2021-06-04 13:58:52 | [train_policy] epoch #829 | Obtaining samples for iteration 829...
2021-06-04 13:58:53 | [train_policy] epoch #829 | Logging diagnostics...
2021-06-04 13:58:53 | [train_policy] epoch #829 | Optimizing policy...
2021-06-04 13:58:53 | [train_policy] epoch #829 | Computing loss before
2021-06-04 13:58:53 | [train_policy] epoch #829 | Computing KL before
2021-06-04 13:58:53 | [train_policy] epoch #829 | Optimizing
2021-06-04 13:58:53 | [train_policy] epoch #829 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:53 | [train_policy] epoch #829 | computing loss before
2021-06-04 13:58:53 | [train_policy] epoch #829 | computing gradient
2021-06-04 13:58:53 | [train_policy] epoch #829 | gradient computed
2021-06-04 13:58:53 | [train_policy] epoch #829 | computing descent direction
2021-06-04 13:58:53 | [train_policy] epoch #829 | descent direction computed
2021-06-04 13:58:53 | [train_policy] epoch #829 | backtrack iters: 1
2021-06-04 13:58:53 | [train_policy] epoch #829 | optimization finished
2021-06-04 13:58:53 | [train_policy] epoch #829 | Computing KL after
2021-06-04 13:58:53 | [train_policy] epoch #829 | Computing loss after
2021-06-04 13:58:53 | [train_policy] epoch #829 | Fitting baseline...
2021-06-04 13:58:53 | [train_policy] epoch #829 | Saving snapshot...
2021-06-04 13:58:53 | [train_policy] epoch #829 | Saved
2021-06-04 13:58:53 | [train_policy] epoch #829 | Time 665.45 s
2021-06-04 13:58:53 | [train_policy] epoch #829 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28644
Evaluation/AverageDiscountedReturn          -40.5106
Evaluation/AverageReturn                    -40.5106
Evaluation/CompletionRate                     0
Evaluation/Iteration                        829
Evaluation/MaxReturn                        -29.5145
Evaluation/MinReturn                        -63.9012
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.78152
Extras/EpisodeRewardMean                    -40.4117
LinearFeatureBaseline/ExplainedVariance       0.851591
PolicyExecTime                                0.224468
ProcessExecTime                               0.0314212
TotalEnvSteps                            839960
policy/Entropy                               -1.85091
policy/KL                                     0.00657853
policy/KLBefore                               0
policy/LossAfter                             -0.0157906
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.157094
policy/dLoss                                  0.0157906
---------------------------------------  ----------------
2021-06-04 13:58:53 | [train_policy] epoch #830 | Obtaining samples for iteration 830...
2021-06-04 13:58:54 | [train_policy] epoch #830 | Logging diagnostics...
2021-06-04 13:58:54 | [train_policy] epoch #830 | Optimizing policy...
2021-06-04 13:58:54 | [train_policy] epoch #830 | Computing loss before
2021-06-04 13:58:54 | [train_policy] epoch #830 | Computing KL before
2021-06-04 13:58:54 | [train_policy] epoch #830 | Optimizing
2021-06-04 13:58:54 | [train_policy] epoch #830 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:54 | [train_policy] epoch #830 | computing loss before
2021-06-04 13:58:54 | [train_policy] epoch #830 | computing gradient
2021-06-04 13:58:54 | [train_policy] epoch #830 | gradient computed
2021-06-04 13:58:54 | [train_policy] epoch #830 | computing descent direction
2021-06-04 13:58:54 | [train_policy] epoch #830 | descent direction computed
2021-06-04 13:58:54 | [train_policy] epoch #830 | backtrack iters: 1
2021-06-04 13:58:54 | [train_policy] epoch #830 | optimization finished
2021-06-04 13:58:54 | [train_policy] epoch #830 | Computing KL after
2021-06-04 13:58:54 | [train_policy] epoch #830 | Computing loss after
2021-06-04 13:58:54 | [train_policy] epoch #830 | Fitting baseline...
2021-06-04 13:58:54 | [train_policy] epoch #830 | Saving snapshot...
2021-06-04 13:58:54 | [train_policy] epoch #830 | Saved
2021-06-04 13:58:54 | [train_policy] epoch #830 | Time 666.24 s
2021-06-04 13:58:54 | [train_policy] epoch #830 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284536
Evaluation/AverageDiscountedReturn          -40.9595
Evaluation/AverageReturn                    -40.9595
Evaluation/CompletionRate                     0
Evaluation/Iteration                        830
Evaluation/MaxReturn                        -28.2892
Evaluation/MinReturn                       -114.051
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.8003
Extras/EpisodeRewardMean                    -40.6395
LinearFeatureBaseline/ExplainedVariance       0.717948
PolicyExecTime                                0.223502
ProcessExecTime                               0.0311098
TotalEnvSteps                            840972
policy/Entropy                               -1.85932
policy/KL                                     0.00650842
policy/KLBefore                               0
policy/LossAfter                             -0.0248719
policy/LossBefore                            -4.35844e-09
policy/Perplexity                             0.155779
policy/dLoss                                  0.0248719
---------------------------------------  ----------------
2021-06-04 13:58:54 | [train_policy] epoch #831 | Obtaining samples for iteration 831...
2021-06-04 13:58:55 | [train_policy] epoch #831 | Logging diagnostics...
2021-06-04 13:58:55 | [train_policy] epoch #831 | Optimizing policy...
2021-06-04 13:58:55 | [train_policy] epoch #831 | Computing loss before
2021-06-04 13:58:55 | [train_policy] epoch #831 | Computing KL before
2021-06-04 13:58:55 | [train_policy] epoch #831 | Optimizing
2021-06-04 13:58:55 | [train_policy] epoch #831 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:55 | [train_policy] epoch #831 | computing loss before
2021-06-04 13:58:55 | [train_policy] epoch #831 | computing gradient
2021-06-04 13:58:55 | [train_policy] epoch #831 | gradient computed
2021-06-04 13:58:55 | [train_policy] epoch #831 | computing descent direction
2021-06-04 13:58:55 | [train_policy] epoch #831 | descent direction computed
2021-06-04 13:58:55 | [train_policy] epoch #831 | backtrack iters: 1
2021-06-04 13:58:55 | [train_policy] epoch #831 | optimization finished
2021-06-04 13:58:55 | [train_policy] epoch #831 | Computing KL after
2021-06-04 13:58:55 | [train_policy] epoch #831 | Computing loss after
2021-06-04 13:58:55 | [train_policy] epoch #831 | Fitting baseline...
2021-06-04 13:58:55 | [train_policy] epoch #831 | Saving snapshot...
2021-06-04 13:58:55 | [train_policy] epoch #831 | Saved
2021-06-04 13:58:55 | [train_policy] epoch #831 | Time 667.04 s
2021-06-04 13:58:55 | [train_policy] epoch #831 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284638
Evaluation/AverageDiscountedReturn          -41.4145
Evaluation/AverageReturn                    -41.4145
Evaluation/CompletionRate                     0
Evaluation/Iteration                        831
Evaluation/MaxReturn                        -29.0299
Evaluation/MinReturn                       -101.783
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.6017
Extras/EpisodeRewardMean                    -42.0996
LinearFeatureBaseline/ExplainedVariance       0.799349
PolicyExecTime                                0.228964
ProcessExecTime                               0.0312419
TotalEnvSteps                            841984
policy/Entropy                               -1.86655
policy/KL                                     0.00648533
policy/KLBefore                               0
policy/LossAfter                             -0.0160696
policy/LossBefore                             1.59024e-09
policy/Perplexity                             0.154657
policy/dLoss                                  0.0160696
---------------------------------------  ----------------
2021-06-04 13:58:55 | [train_policy] epoch #832 | Obtaining samples for iteration 832...
2021-06-04 13:58:55 | [train_policy] epoch #832 | Logging diagnostics...
2021-06-04 13:58:55 | [train_policy] epoch #832 | Optimizing policy...
2021-06-04 13:58:55 | [train_policy] epoch #832 | Computing loss before
2021-06-04 13:58:55 | [train_policy] epoch #832 | Computing KL before
2021-06-04 13:58:55 | [train_policy] epoch #832 | Optimizing
2021-06-04 13:58:55 | [train_policy] epoch #832 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:55 | [train_policy] epoch #832 | computing loss before
2021-06-04 13:58:55 | [train_policy] epoch #832 | computing gradient
2021-06-04 13:58:55 | [train_policy] epoch #832 | gradient computed
2021-06-04 13:58:55 | [train_policy] epoch #832 | computing descent direction
2021-06-04 13:58:56 | [train_policy] epoch #832 | descent direction computed
2021-06-04 13:58:56 | [train_policy] epoch #832 | backtrack iters: 1
2021-06-04 13:58:56 | [train_policy] epoch #832 | optimization finished
2021-06-04 13:58:56 | [train_policy] epoch #832 | Computing KL after
2021-06-04 13:58:56 | [train_policy] epoch #832 | Computing loss after
2021-06-04 13:58:56 | [train_policy] epoch #832 | Fitting baseline...
2021-06-04 13:58:56 | [train_policy] epoch #832 | Saving snapshot...
2021-06-04 13:58:56 | [train_policy] epoch #832 | Saved
2021-06-04 13:58:56 | [train_policy] epoch #832 | Time 667.86 s
2021-06-04 13:58:56 | [train_policy] epoch #832 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.284971
Evaluation/AverageDiscountedReturn          -42.2373
Evaluation/AverageReturn                    -42.2373
Evaluation/CompletionRate                     0
Evaluation/Iteration                        832
Evaluation/MaxReturn                        -29.8278
Evaluation/MinReturn                        -91.1569
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.9538
Extras/EpisodeRewardMean                    -41.9404
LinearFeatureBaseline/ExplainedVariance       0.840275
PolicyExecTime                                0.237632
ProcessExecTime                               0.0312986
TotalEnvSteps                            842996
policy/Entropy                               -1.87246
policy/KL                                     0.00646697
policy/KLBefore                               0
policy/LossAfter                             -0.0146864
policy/LossBefore                             8.2457e-10
policy/Perplexity                             0.153745
policy/dLoss                                  0.0146864
---------------------------------------  ---------------
2021-06-04 13:58:56 | [train_policy] epoch #833 | Obtaining samples for iteration 833...
2021-06-04 13:58:56 | [train_policy] epoch #833 | Logging diagnostics...
2021-06-04 13:58:56 | [train_policy] epoch #833 | Optimizing policy...
2021-06-04 13:58:56 | [train_policy] epoch #833 | Computing loss before
2021-06-04 13:58:56 | [train_policy] epoch #833 | Computing KL before
2021-06-04 13:58:56 | [train_policy] epoch #833 | Optimizing
2021-06-04 13:58:56 | [train_policy] epoch #833 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:56 | [train_policy] epoch #833 | computing loss before
2021-06-04 13:58:56 | [train_policy] epoch #833 | computing gradient
2021-06-04 13:58:56 | [train_policy] epoch #833 | gradient computed
2021-06-04 13:58:56 | [train_policy] epoch #833 | computing descent direction
2021-06-04 13:58:56 | [train_policy] epoch #833 | descent direction computed
2021-06-04 13:58:56 | [train_policy] epoch #833 | backtrack iters: 1
2021-06-04 13:58:56 | [train_policy] epoch #833 | optimization finished
2021-06-04 13:58:56 | [train_policy] epoch #833 | Computing KL after
2021-06-04 13:58:56 | [train_policy] epoch #833 | Computing loss after
2021-06-04 13:58:56 | [train_policy] epoch #833 | Fitting baseline...
2021-06-04 13:58:56 | [train_policy] epoch #833 | Saving snapshot...
2021-06-04 13:58:56 | [train_policy] epoch #833 | Saved
2021-06-04 13:58:56 | [train_policy] epoch #833 | Time 668.66 s
2021-06-04 13:58:56 | [train_policy] epoch #833 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284458
Evaluation/AverageDiscountedReturn          -40.7396
Evaluation/AverageReturn                    -40.7396
Evaluation/CompletionRate                     0
Evaluation/Iteration                        833
Evaluation/MaxReturn                        -28.1689
Evaluation/MinReturn                        -85.0199
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.00451
Extras/EpisodeRewardMean                    -41.1986
LinearFeatureBaseline/ExplainedVariance       0.873926
PolicyExecTime                                0.225266
ProcessExecTime                               0.0310857
TotalEnvSteps                            844008
policy/Entropy                               -1.87988
policy/KL                                     0.00663672
policy/KLBefore                               0
policy/LossAfter                             -0.0189963
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             0.152608
policy/dLoss                                  0.0189963
---------------------------------------  ----------------
2021-06-04 13:58:56 | [train_policy] epoch #834 | Obtaining samples for iteration 834...
2021-06-04 13:58:57 | [train_policy] epoch #834 | Logging diagnostics...
2021-06-04 13:58:57 | [train_policy] epoch #834 | Optimizing policy...
2021-06-04 13:58:57 | [train_policy] epoch #834 | Computing loss before
2021-06-04 13:58:57 | [train_policy] epoch #834 | Computing KL before
2021-06-04 13:58:57 | [train_policy] epoch #834 | Optimizing
2021-06-04 13:58:57 | [train_policy] epoch #834 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:57 | [train_policy] epoch #834 | computing loss before
2021-06-04 13:58:57 | [train_policy] epoch #834 | computing gradient
2021-06-04 13:58:57 | [train_policy] epoch #834 | gradient computed
2021-06-04 13:58:57 | [train_policy] epoch #834 | computing descent direction
2021-06-04 13:58:57 | [train_policy] epoch #834 | descent direction computed
2021-06-04 13:58:57 | [train_policy] epoch #834 | backtrack iters: 1
2021-06-04 13:58:57 | [train_policy] epoch #834 | optimization finished
2021-06-04 13:58:57 | [train_policy] epoch #834 | Computing KL after
2021-06-04 13:58:57 | [train_policy] epoch #834 | Computing loss after
2021-06-04 13:58:57 | [train_policy] epoch #834 | Fitting baseline...
2021-06-04 13:58:57 | [train_policy] epoch #834 | Saving snapshot...
2021-06-04 13:58:57 | [train_policy] epoch #834 | Saved
2021-06-04 13:58:57 | [train_policy] epoch #834 | Time 669.46 s
2021-06-04 13:58:57 | [train_policy] epoch #834 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286154
Evaluation/AverageDiscountedReturn          -40.5765
Evaluation/AverageReturn                    -40.5765
Evaluation/CompletionRate                     0
Evaluation/Iteration                        834
Evaluation/MaxReturn                        -29.0544
Evaluation/MinReturn                        -62.9422
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.61139
Extras/EpisodeRewardMean                    -40.3228
LinearFeatureBaseline/ExplainedVariance       0.885154
PolicyExecTime                                0.230727
ProcessExecTime                               0.0313118
TotalEnvSteps                            845020
policy/Entropy                               -1.85721
policy/KL                                     0.00642854
policy/KLBefore                               0
policy/LossAfter                             -0.0135027
policy/LossBefore                             1.39441e-08
policy/Perplexity                             0.156107
policy/dLoss                                  0.0135027
---------------------------------------  ----------------
2021-06-04 13:58:57 | [train_policy] epoch #835 | Obtaining samples for iteration 835...
2021-06-04 13:58:58 | [train_policy] epoch #835 | Logging diagnostics...
2021-06-04 13:58:58 | [train_policy] epoch #835 | Optimizing policy...
2021-06-04 13:58:58 | [train_policy] epoch #835 | Computing loss before
2021-06-04 13:58:58 | [train_policy] epoch #835 | Computing KL before
2021-06-04 13:58:58 | [train_policy] epoch #835 | Optimizing
2021-06-04 13:58:58 | [train_policy] epoch #835 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:58 | [train_policy] epoch #835 | computing loss before
2021-06-04 13:58:58 | [train_policy] epoch #835 | computing gradient
2021-06-04 13:58:58 | [train_policy] epoch #835 | gradient computed
2021-06-04 13:58:58 | [train_policy] epoch #835 | computing descent direction
2021-06-04 13:58:58 | [train_policy] epoch #835 | descent direction computed
2021-06-04 13:58:58 | [train_policy] epoch #835 | backtrack iters: 0
2021-06-04 13:58:58 | [train_policy] epoch #835 | optimization finished
2021-06-04 13:58:58 | [train_policy] epoch #835 | Computing KL after
2021-06-04 13:58:58 | [train_policy] epoch #835 | Computing loss after
2021-06-04 13:58:58 | [train_policy] epoch #835 | Fitting baseline...
2021-06-04 13:58:58 | [train_policy] epoch #835 | Saving snapshot...
2021-06-04 13:58:58 | [train_policy] epoch #835 | Saved
2021-06-04 13:58:58 | [train_policy] epoch #835 | Time 670.25 s
2021-06-04 13:58:58 | [train_policy] epoch #835 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285032
Evaluation/AverageDiscountedReturn          -40.5852
Evaluation/AverageReturn                    -40.5852
Evaluation/CompletionRate                     0
Evaluation/Iteration                        835
Evaluation/MaxReturn                        -28.4623
Evaluation/MinReturn                        -63.7559
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.61389
Extras/EpisodeRewardMean                    -40.7539
LinearFeatureBaseline/ExplainedVariance       0.898683
PolicyExecTime                                0.224974
ProcessExecTime                               0.0312376
TotalEnvSteps                            846032
policy/Entropy                               -1.81535
policy/KL                                     0.00961032
policy/KLBefore                               0
policy/LossAfter                             -0.0225385
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.16278
policy/dLoss                                  0.0225385
---------------------------------------  ----------------
2021-06-04 13:58:58 | [train_policy] epoch #836 | Obtaining samples for iteration 836...
2021-06-04 13:58:59 | [train_policy] epoch #836 | Logging diagnostics...
2021-06-04 13:58:59 | [train_policy] epoch #836 | Optimizing policy...
2021-06-04 13:58:59 | [train_policy] epoch #836 | Computing loss before
2021-06-04 13:58:59 | [train_policy] epoch #836 | Computing KL before
2021-06-04 13:58:59 | [train_policy] epoch #836 | Optimizing
2021-06-04 13:58:59 | [train_policy] epoch #836 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:59 | [train_policy] epoch #836 | computing loss before
2021-06-04 13:58:59 | [train_policy] epoch #836 | computing gradient
2021-06-04 13:58:59 | [train_policy] epoch #836 | gradient computed
2021-06-04 13:58:59 | [train_policy] epoch #836 | computing descent direction
2021-06-04 13:58:59 | [train_policy] epoch #836 | descent direction computed
2021-06-04 13:58:59 | [train_policy] epoch #836 | backtrack iters: 1
2021-06-04 13:58:59 | [train_policy] epoch #836 | optimization finished
2021-06-04 13:58:59 | [train_policy] epoch #836 | Computing KL after
2021-06-04 13:58:59 | [train_policy] epoch #836 | Computing loss after
2021-06-04 13:58:59 | [train_policy] epoch #836 | Fitting baseline...
2021-06-04 13:58:59 | [train_policy] epoch #836 | Saving snapshot...
2021-06-04 13:58:59 | [train_policy] epoch #836 | Saved
2021-06-04 13:58:59 | [train_policy] epoch #836 | Time 671.05 s
2021-06-04 13:58:59 | [train_policy] epoch #836 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284555
Evaluation/AverageDiscountedReturn          -42.1048
Evaluation/AverageReturn                    -42.1048
Evaluation/CompletionRate                     0
Evaluation/Iteration                        836
Evaluation/MaxReturn                        -30.1384
Evaluation/MinReturn                        -99.4911
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.96943
Extras/EpisodeRewardMean                    -41.8069
LinearFeatureBaseline/ExplainedVariance       0.826112
PolicyExecTime                                0.226466
ProcessExecTime                               0.0312247
TotalEnvSteps                            847044
policy/Entropy                               -1.82212
policy/KL                                     0.0064248
policy/KLBefore                               0
policy/LossAfter                             -0.00201293
policy/LossBefore                             5.88979e-09
policy/Perplexity                             0.161682
policy/dLoss                                  0.00201294
---------------------------------------  ----------------
2021-06-04 13:58:59 | [train_policy] epoch #837 | Obtaining samples for iteration 837...
2021-06-04 13:58:59 | [train_policy] epoch #837 | Logging diagnostics...
2021-06-04 13:58:59 | [train_policy] epoch #837 | Optimizing policy...
2021-06-04 13:58:59 | [train_policy] epoch #837 | Computing loss before
2021-06-04 13:58:59 | [train_policy] epoch #837 | Computing KL before
2021-06-04 13:58:59 | [train_policy] epoch #837 | Optimizing
2021-06-04 13:58:59 | [train_policy] epoch #837 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:58:59 | [train_policy] epoch #837 | computing loss before
2021-06-04 13:58:59 | [train_policy] epoch #837 | computing gradient
2021-06-04 13:58:59 | [train_policy] epoch #837 | gradient computed
2021-06-04 13:58:59 | [train_policy] epoch #837 | computing descent direction
2021-06-04 13:59:00 | [train_policy] epoch #837 | descent direction computed
2021-06-04 13:59:00 | [train_policy] epoch #837 | backtrack iters: 0
2021-06-04 13:59:00 | [train_policy] epoch #837 | optimization finished
2021-06-04 13:59:00 | [train_policy] epoch #837 | Computing KL after
2021-06-04 13:59:00 | [train_policy] epoch #837 | Computing loss after
2021-06-04 13:59:00 | [train_policy] epoch #837 | Fitting baseline...
2021-06-04 13:59:00 | [train_policy] epoch #837 | Saving snapshot...
2021-06-04 13:59:00 | [train_policy] epoch #837 | Saved
2021-06-04 13:59:00 | [train_policy] epoch #837 | Time 671.84 s
2021-06-04 13:59:00 | [train_policy] epoch #837 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284599
Evaluation/AverageDiscountedReturn          -41.2654
Evaluation/AverageReturn                    -41.2654
Evaluation/CompletionRate                     0
Evaluation/Iteration                        837
Evaluation/MaxReturn                        -28.0788
Evaluation/MinReturn                        -63.8828
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.60285
Extras/EpisodeRewardMean                    -41.3608
LinearFeatureBaseline/ExplainedVariance       0.887925
PolicyExecTime                                0.212766
ProcessExecTime                               0.0313237
TotalEnvSteps                            848056
policy/Entropy                               -1.8046
policy/KL                                     0.00994767
policy/KLBefore                               0
policy/LossAfter                             -0.0227351
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.16454
policy/dLoss                                  0.0227351
---------------------------------------  ----------------
2021-06-04 13:59:00 | [train_policy] epoch #838 | Obtaining samples for iteration 838...
2021-06-04 13:59:00 | [train_policy] epoch #838 | Logging diagnostics...
2021-06-04 13:59:00 | [train_policy] epoch #838 | Optimizing policy...
2021-06-04 13:59:00 | [train_policy] epoch #838 | Computing loss before
2021-06-04 13:59:00 | [train_policy] epoch #838 | Computing KL before
2021-06-04 13:59:00 | [train_policy] epoch #838 | Optimizing
2021-06-04 13:59:00 | [train_policy] epoch #838 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:00 | [train_policy] epoch #838 | computing loss before
2021-06-04 13:59:00 | [train_policy] epoch #838 | computing gradient
2021-06-04 13:59:00 | [train_policy] epoch #838 | gradient computed
2021-06-04 13:59:00 | [train_policy] epoch #838 | computing descent direction
2021-06-04 13:59:00 | [train_policy] epoch #838 | descent direction computed
2021-06-04 13:59:00 | [train_policy] epoch #838 | backtrack iters: 0
2021-06-04 13:59:00 | [train_policy] epoch #838 | optimization finished
2021-06-04 13:59:00 | [train_policy] epoch #838 | Computing KL after
2021-06-04 13:59:00 | [train_policy] epoch #838 | Computing loss after
2021-06-04 13:59:00 | [train_policy] epoch #838 | Fitting baseline...
2021-06-04 13:59:00 | [train_policy] epoch #838 | Saving snapshot...
2021-06-04 13:59:00 | [train_policy] epoch #838 | Saved
2021-06-04 13:59:00 | [train_policy] epoch #838 | Time 672.65 s
2021-06-04 13:59:00 | [train_policy] epoch #838 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285362
Evaluation/AverageDiscountedReturn          -41.2612
Evaluation/AverageReturn                    -41.2612
Evaluation/CompletionRate                     0
Evaluation/Iteration                        838
Evaluation/MaxReturn                        -29.9223
Evaluation/MinReturn                        -85.7307
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.48233
Extras/EpisodeRewardMean                    -40.9947
LinearFeatureBaseline/ExplainedVariance       0.858914
PolicyExecTime                                0.231994
ProcessExecTime                               0.031301
TotalEnvSteps                            849068
policy/Entropy                               -1.81775
policy/KL                                     0.00946455
policy/KLBefore                               0
policy/LossAfter                             -0.0210014
policy/LossBefore                             1.17796e-09
policy/Perplexity                             0.162391
policy/dLoss                                  0.0210014
---------------------------------------  ----------------
2021-06-04 13:59:00 | [train_policy] epoch #839 | Obtaining samples for iteration 839...
2021-06-04 13:59:01 | [train_policy] epoch #839 | Logging diagnostics...
2021-06-04 13:59:01 | [train_policy] epoch #839 | Optimizing policy...
2021-06-04 13:59:01 | [train_policy] epoch #839 | Computing loss before
2021-06-04 13:59:01 | [train_policy] epoch #839 | Computing KL before
2021-06-04 13:59:01 | [train_policy] epoch #839 | Optimizing
2021-06-04 13:59:01 | [train_policy] epoch #839 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:01 | [train_policy] epoch #839 | computing loss before
2021-06-04 13:59:01 | [train_policy] epoch #839 | computing gradient
2021-06-04 13:59:01 | [train_policy] epoch #839 | gradient computed
2021-06-04 13:59:01 | [train_policy] epoch #839 | computing descent direction
2021-06-04 13:59:01 | [train_policy] epoch #839 | descent direction computed
2021-06-04 13:59:01 | [train_policy] epoch #839 | backtrack iters: 0
2021-06-04 13:59:01 | [train_policy] epoch #839 | optimization finished
2021-06-04 13:59:01 | [train_policy] epoch #839 | Computing KL after
2021-06-04 13:59:01 | [train_policy] epoch #839 | Computing loss after
2021-06-04 13:59:01 | [train_policy] epoch #839 | Fitting baseline...
2021-06-04 13:59:01 | [train_policy] epoch #839 | Saving snapshot...
2021-06-04 13:59:01 | [train_policy] epoch #839 | Saved
2021-06-04 13:59:01 | [train_policy] epoch #839 | Time 673.45 s
2021-06-04 13:59:01 | [train_policy] epoch #839 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286862
Evaluation/AverageDiscountedReturn          -40.5683
Evaluation/AverageReturn                    -40.5683
Evaluation/CompletionRate                     0
Evaluation/Iteration                        839
Evaluation/MaxReturn                        -28.6613
Evaluation/MinReturn                        -63.036
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.67532
Extras/EpisodeRewardMean                    -40.7211
LinearFeatureBaseline/ExplainedVariance       0.890892
PolicyExecTime                                0.233874
ProcessExecTime                               0.0313282
TotalEnvSteps                            850080
policy/Entropy                               -1.7759
policy/KL                                     0.00916362
policy/KLBefore                               0
policy/LossAfter                             -0.0155531
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.169331
policy/dLoss                                  0.0155531
---------------------------------------  ----------------
2021-06-04 13:59:01 | [train_policy] epoch #840 | Obtaining samples for iteration 840...
2021-06-04 13:59:02 | [train_policy] epoch #840 | Logging diagnostics...
2021-06-04 13:59:02 | [train_policy] epoch #840 | Optimizing policy...
2021-06-04 13:59:02 | [train_policy] epoch #840 | Computing loss before
2021-06-04 13:59:02 | [train_policy] epoch #840 | Computing KL before
2021-06-04 13:59:02 | [train_policy] epoch #840 | Optimizing
2021-06-04 13:59:02 | [train_policy] epoch #840 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:02 | [train_policy] epoch #840 | computing loss before
2021-06-04 13:59:02 | [train_policy] epoch #840 | computing gradient
2021-06-04 13:59:02 | [train_policy] epoch #840 | gradient computed
2021-06-04 13:59:02 | [train_policy] epoch #840 | computing descent direction
2021-06-04 13:59:02 | [train_policy] epoch #840 | descent direction computed
2021-06-04 13:59:02 | [train_policy] epoch #840 | backtrack iters: 1
2021-06-04 13:59:02 | [train_policy] epoch #840 | optimization finished
2021-06-04 13:59:02 | [train_policy] epoch #840 | Computing KL after
2021-06-04 13:59:02 | [train_policy] epoch #840 | Computing loss after
2021-06-04 13:59:02 | [train_policy] epoch #840 | Fitting baseline...
2021-06-04 13:59:02 | [train_policy] epoch #840 | Saving snapshot...
2021-06-04 13:59:02 | [train_policy] epoch #840 | Saved
2021-06-04 13:59:02 | [train_policy] epoch #840 | Time 674.29 s
2021-06-04 13:59:02 | [train_policy] epoch #840 | EpochTime 0.81 s
---------------------------------------  ---------------
EnvExecTime                                   0.287083
Evaluation/AverageDiscountedReturn          -42.0626
Evaluation/AverageReturn                    -42.0626
Evaluation/CompletionRate                     0
Evaluation/Iteration                        840
Evaluation/MaxReturn                        -28.5547
Evaluation/MinReturn                       -117.527
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.9922
Extras/EpisodeRewardMean                    -42.1293
LinearFeatureBaseline/ExplainedVariance       0.735639
PolicyExecTime                                0.246073
ProcessExecTime                               0.0314727
TotalEnvSteps                            851092
policy/Entropy                               -1.76519
policy/KL                                     0.00661868
policy/KLBefore                               0
policy/LossAfter                             -0.0198535
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.171154
policy/dLoss                                  0.0198535
---------------------------------------  ---------------
2021-06-04 13:59:02 | [train_policy] epoch #841 | Obtaining samples for iteration 841...
2021-06-04 13:59:03 | [train_policy] epoch #841 | Logging diagnostics...
2021-06-04 13:59:03 | [train_policy] epoch #841 | Optimizing policy...
2021-06-04 13:59:03 | [train_policy] epoch #841 | Computing loss before
2021-06-04 13:59:03 | [train_policy] epoch #841 | Computing KL before
2021-06-04 13:59:03 | [train_policy] epoch #841 | Optimizing
2021-06-04 13:59:03 | [train_policy] epoch #841 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:03 | [train_policy] epoch #841 | computing loss before
2021-06-04 13:59:03 | [train_policy] epoch #841 | computing gradient
2021-06-04 13:59:03 | [train_policy] epoch #841 | gradient computed
2021-06-04 13:59:03 | [train_policy] epoch #841 | computing descent direction
2021-06-04 13:59:03 | [train_policy] epoch #841 | descent direction computed
2021-06-04 13:59:03 | [train_policy] epoch #841 | backtrack iters: 0
2021-06-04 13:59:03 | [train_policy] epoch #841 | optimization finished
2021-06-04 13:59:03 | [train_policy] epoch #841 | Computing KL after
2021-06-04 13:59:03 | [train_policy] epoch #841 | Computing loss after
2021-06-04 13:59:03 | [train_policy] epoch #841 | Fitting baseline...
2021-06-04 13:59:03 | [train_policy] epoch #841 | Saving snapshot...
2021-06-04 13:59:03 | [train_policy] epoch #841 | Saved
2021-06-04 13:59:03 | [train_policy] epoch #841 | Time 675.08 s
2021-06-04 13:59:03 | [train_policy] epoch #841 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.285447
Evaluation/AverageDiscountedReturn          -44.488
Evaluation/AverageReturn                    -44.488
Evaluation/CompletionRate                     0
Evaluation/Iteration                        841
Evaluation/MaxReturn                        -28.7862
Evaluation/MinReturn                       -215.973
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         23.1134
Extras/EpisodeRewardMean                    -44.106
LinearFeatureBaseline/ExplainedVariance       0.399695
PolicyExecTime                                0.227286
ProcessExecTime                               0.031276
TotalEnvSteps                            852104
policy/Entropy                               -1.78353
policy/KL                                     0.00999379
policy/KLBefore                               0
policy/LossAfter                             -0.01962
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.168044
policy/dLoss                                  0.01962
---------------------------------------  ---------------
2021-06-04 13:59:03 | [train_policy] epoch #842 | Obtaining samples for iteration 842...
2021-06-04 13:59:04 | [train_policy] epoch #842 | Logging diagnostics...
2021-06-04 13:59:04 | [train_policy] epoch #842 | Optimizing policy...
2021-06-04 13:59:04 | [train_policy] epoch #842 | Computing loss before
2021-06-04 13:59:04 | [train_policy] epoch #842 | Computing KL before
2021-06-04 13:59:04 | [train_policy] epoch #842 | Optimizing
2021-06-04 13:59:04 | [train_policy] epoch #842 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:04 | [train_policy] epoch #842 | computing loss before
2021-06-04 13:59:04 | [train_policy] epoch #842 | computing gradient
2021-06-04 13:59:04 | [train_policy] epoch #842 | gradient computed
2021-06-04 13:59:04 | [train_policy] epoch #842 | computing descent direction
2021-06-04 13:59:04 | [train_policy] epoch #842 | descent direction computed
2021-06-04 13:59:04 | [train_policy] epoch #842 | backtrack iters: 1
2021-06-04 13:59:04 | [train_policy] epoch #842 | optimization finished
2021-06-04 13:59:04 | [train_policy] epoch #842 | Computing KL after
2021-06-04 13:59:04 | [train_policy] epoch #842 | Computing loss after
2021-06-04 13:59:04 | [train_policy] epoch #842 | Fitting baseline...
2021-06-04 13:59:04 | [train_policy] epoch #842 | Saving snapshot...
2021-06-04 13:59:04 | [train_policy] epoch #842 | Saved
2021-06-04 13:59:04 | [train_policy] epoch #842 | Time 675.88 s
2021-06-04 13:59:04 | [train_policy] epoch #842 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285512
Evaluation/AverageDiscountedReturn          -41.2446
Evaluation/AverageReturn                    -41.2446
Evaluation/CompletionRate                     0
Evaluation/Iteration                        842
Evaluation/MaxReturn                        -28.505
Evaluation/MinReturn                        -63.696
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.50759
Extras/EpisodeRewardMean                    -41.6124
LinearFeatureBaseline/ExplainedVariance       0.681088
PolicyExecTime                                0.230294
ProcessExecTime                               0.0312679
TotalEnvSteps                            853116
policy/Entropy                               -1.79077
policy/KL                                     0.00653409
policy/KLBefore                               0
policy/LossAfter                             -0.0134182
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             0.166832
policy/dLoss                                  0.0134182
---------------------------------------  ----------------
2021-06-04 13:59:04 | [train_policy] epoch #843 | Obtaining samples for iteration 843...
2021-06-04 13:59:04 | [train_policy] epoch #843 | Logging diagnostics...
2021-06-04 13:59:04 | [train_policy] epoch #843 | Optimizing policy...
2021-06-04 13:59:04 | [train_policy] epoch #843 | Computing loss before
2021-06-04 13:59:04 | [train_policy] epoch #843 | Computing KL before
2021-06-04 13:59:04 | [train_policy] epoch #843 | Optimizing
2021-06-04 13:59:04 | [train_policy] epoch #843 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:04 | [train_policy] epoch #843 | computing loss before
2021-06-04 13:59:04 | [train_policy] epoch #843 | computing gradient
2021-06-04 13:59:04 | [train_policy] epoch #843 | gradient computed
2021-06-04 13:59:04 | [train_policy] epoch #843 | computing descent direction
2021-06-04 13:59:04 | [train_policy] epoch #843 | descent direction computed
2021-06-04 13:59:04 | [train_policy] epoch #843 | backtrack iters: 1
2021-06-04 13:59:04 | [train_policy] epoch #843 | optimization finished
2021-06-04 13:59:04 | [train_policy] epoch #843 | Computing KL after
2021-06-04 13:59:04 | [train_policy] epoch #843 | Computing loss after
2021-06-04 13:59:04 | [train_policy] epoch #843 | Fitting baseline...
2021-06-04 13:59:04 | [train_policy] epoch #843 | Saving snapshot...
2021-06-04 13:59:04 | [train_policy] epoch #843 | Saved
2021-06-04 13:59:04 | [train_policy] epoch #843 | Time 676.68 s
2021-06-04 13:59:04 | [train_policy] epoch #843 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285352
Evaluation/AverageDiscountedReturn          -40.9632
Evaluation/AverageReturn                    -40.9632
Evaluation/CompletionRate                     0
Evaluation/Iteration                        843
Evaluation/MaxReturn                        -28.375
Evaluation/MinReturn                        -62.9392
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.52224
Extras/EpisodeRewardMean                    -40.866
LinearFeatureBaseline/ExplainedVariance       0.888918
PolicyExecTime                                0.23188
ProcessExecTime                               0.0313342
TotalEnvSteps                            854128
policy/Entropy                               -1.77273
policy/KL                                     0.00642985
policy/KLBefore                               0
policy/LossAfter                             -0.0141358
policy/LossBefore                             1.64914e-08
policy/Perplexity                             0.169868
policy/dLoss                                  0.0141358
---------------------------------------  ----------------
2021-06-04 13:59:04 | [train_policy] epoch #844 | Obtaining samples for iteration 844...
2021-06-04 13:59:05 | [train_policy] epoch #844 | Logging diagnostics...
2021-06-04 13:59:05 | [train_policy] epoch #844 | Optimizing policy...
2021-06-04 13:59:05 | [train_policy] epoch #844 | Computing loss before
2021-06-04 13:59:05 | [train_policy] epoch #844 | Computing KL before
2021-06-04 13:59:05 | [train_policy] epoch #844 | Optimizing
2021-06-04 13:59:05 | [train_policy] epoch #844 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:05 | [train_policy] epoch #844 | computing loss before
2021-06-04 13:59:05 | [train_policy] epoch #844 | computing gradient
2021-06-04 13:59:05 | [train_policy] epoch #844 | gradient computed
2021-06-04 13:59:05 | [train_policy] epoch #844 | computing descent direction
2021-06-04 13:59:05 | [train_policy] epoch #844 | descent direction computed
2021-06-04 13:59:05 | [train_policy] epoch #844 | backtrack iters: 1
2021-06-04 13:59:05 | [train_policy] epoch #844 | optimization finished
2021-06-04 13:59:05 | [train_policy] epoch #844 | Computing KL after
2021-06-04 13:59:05 | [train_policy] epoch #844 | Computing loss after
2021-06-04 13:59:05 | [train_policy] epoch #844 | Fitting baseline...
2021-06-04 13:59:05 | [train_policy] epoch #844 | Saving snapshot...
2021-06-04 13:59:05 | [train_policy] epoch #844 | Saved
2021-06-04 13:59:05 | [train_policy] epoch #844 | Time 677.48 s
2021-06-04 13:59:05 | [train_policy] epoch #844 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285516
Evaluation/AverageDiscountedReturn          -39.9596
Evaluation/AverageReturn                    -39.9596
Evaluation/CompletionRate                     0
Evaluation/Iteration                        844
Evaluation/MaxReturn                        -28.8265
Evaluation/MinReturn                        -59.6537
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.8369
Extras/EpisodeRewardMean                    -39.8525
LinearFeatureBaseline/ExplainedVariance       0.91201
PolicyExecTime                                0.223414
ProcessExecTime                               0.0312765
TotalEnvSteps                            855140
policy/Entropy                               -1.79185
policy/KL                                     0.00656626
policy/KLBefore                               0
policy/LossAfter                             -0.0179686
policy/LossBefore                            -2.56795e-08
policy/Perplexity                             0.166652
policy/dLoss                                  0.0179685
---------------------------------------  ----------------
2021-06-04 13:59:05 | [train_policy] epoch #845 | Obtaining samples for iteration 845...
2021-06-04 13:59:06 | [train_policy] epoch #845 | Logging diagnostics...
2021-06-04 13:59:06 | [train_policy] epoch #845 | Optimizing policy...
2021-06-04 13:59:06 | [train_policy] epoch #845 | Computing loss before
2021-06-04 13:59:06 | [train_policy] epoch #845 | Computing KL before
2021-06-04 13:59:06 | [train_policy] epoch #845 | Optimizing
2021-06-04 13:59:06 | [train_policy] epoch #845 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:06 | [train_policy] epoch #845 | computing loss before
2021-06-04 13:59:06 | [train_policy] epoch #845 | computing gradient
2021-06-04 13:59:06 | [train_policy] epoch #845 | gradient computed
2021-06-04 13:59:06 | [train_policy] epoch #845 | computing descent direction
2021-06-04 13:59:06 | [train_policy] epoch #845 | descent direction computed
2021-06-04 13:59:06 | [train_policy] epoch #845 | backtrack iters: 1
2021-06-04 13:59:06 | [train_policy] epoch #845 | optimization finished
2021-06-04 13:59:06 | [train_policy] epoch #845 | Computing KL after
2021-06-04 13:59:06 | [train_policy] epoch #845 | Computing loss after
2021-06-04 13:59:06 | [train_policy] epoch #845 | Fitting baseline...
2021-06-04 13:59:06 | [train_policy] epoch #845 | Saving snapshot...
2021-06-04 13:59:06 | [train_policy] epoch #845 | Saved
2021-06-04 13:59:06 | [train_policy] epoch #845 | Time 678.29 s
2021-06-04 13:59:06 | [train_policy] epoch #845 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.284115
Evaluation/AverageDiscountedReturn          -41.2146
Evaluation/AverageReturn                    -41.2146
Evaluation/CompletionRate                     0
Evaluation/Iteration                        845
Evaluation/MaxReturn                        -29.9978
Evaluation/MinReturn                        -65.8931
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.92937
Extras/EpisodeRewardMean                    -40.976
LinearFeatureBaseline/ExplainedVariance       0.900148
PolicyExecTime                                0.221203
ProcessExecTime                               0.0311201
TotalEnvSteps                            856152
policy/Entropy                               -1.8107
policy/KL                                     0.00642013
policy/KLBefore                               0
policy/LossAfter                             -0.0172221
policy/LossBefore                            -2.3088e-08
policy/Perplexity                             0.163539
policy/dLoss                                  0.017222
---------------------------------------  ---------------
2021-06-04 13:59:06 | [train_policy] epoch #846 | Obtaining samples for iteration 846...
2021-06-04 13:59:07 | [train_policy] epoch #846 | Logging diagnostics...
2021-06-04 13:59:07 | [train_policy] epoch #846 | Optimizing policy...
2021-06-04 13:59:07 | [train_policy] epoch #846 | Computing loss before
2021-06-04 13:59:07 | [train_policy] epoch #846 | Computing KL before
2021-06-04 13:59:07 | [train_policy] epoch #846 | Optimizing
2021-06-04 13:59:07 | [train_policy] epoch #846 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:07 | [train_policy] epoch #846 | computing loss before
2021-06-04 13:59:07 | [train_policy] epoch #846 | computing gradient
2021-06-04 13:59:07 | [train_policy] epoch #846 | gradient computed
2021-06-04 13:59:07 | [train_policy] epoch #846 | computing descent direction
2021-06-04 13:59:07 | [train_policy] epoch #846 | descent direction computed
2021-06-04 13:59:07 | [train_policy] epoch #846 | backtrack iters: 0
2021-06-04 13:59:07 | [train_policy] epoch #846 | optimization finished
2021-06-04 13:59:07 | [train_policy] epoch #846 | Computing KL after
2021-06-04 13:59:07 | [train_policy] epoch #846 | Computing loss after
2021-06-04 13:59:07 | [train_policy] epoch #846 | Fitting baseline...
2021-06-04 13:59:07 | [train_policy] epoch #846 | Saving snapshot...
2021-06-04 13:59:07 | [train_policy] epoch #846 | Saved
2021-06-04 13:59:07 | [train_policy] epoch #846 | Time 679.09 s
2021-06-04 13:59:07 | [train_policy] epoch #846 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285161
Evaluation/AverageDiscountedReturn          -42.4104
Evaluation/AverageReturn                    -42.4104
Evaluation/CompletionRate                     0
Evaluation/Iteration                        846
Evaluation/MaxReturn                        -31.4686
Evaluation/MinReturn                       -100.857
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.0515
Extras/EpisodeRewardMean                    -42.2342
LinearFeatureBaseline/ExplainedVariance       0.764758
PolicyExecTime                                0.229861
ProcessExecTime                               0.0313261
TotalEnvSteps                            857164
policy/Entropy                               -1.798
policy/KL                                     0.00970226
policy/KLBefore                               0
policy/LossAfter                             -0.0260045
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.165629
policy/dLoss                                  0.0260045
---------------------------------------  ----------------
2021-06-04 13:59:07 | [train_policy] epoch #847 | Obtaining samples for iteration 847...
2021-06-04 13:59:08 | [train_policy] epoch #847 | Logging diagnostics...
2021-06-04 13:59:08 | [train_policy] epoch #847 | Optimizing policy...
2021-06-04 13:59:08 | [train_policy] epoch #847 | Computing loss before
2021-06-04 13:59:08 | [train_policy] epoch #847 | Computing KL before
2021-06-04 13:59:08 | [train_policy] epoch #847 | Optimizing
2021-06-04 13:59:08 | [train_policy] epoch #847 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:08 | [train_policy] epoch #847 | computing loss before
2021-06-04 13:59:08 | [train_policy] epoch #847 | computing gradient
2021-06-04 13:59:08 | [train_policy] epoch #847 | gradient computed
2021-06-04 13:59:08 | [train_policy] epoch #847 | computing descent direction
2021-06-04 13:59:08 | [train_policy] epoch #847 | descent direction computed
2021-06-04 13:59:08 | [train_policy] epoch #847 | backtrack iters: 0
2021-06-04 13:59:08 | [train_policy] epoch #847 | optimization finished
2021-06-04 13:59:08 | [train_policy] epoch #847 | Computing KL after
2021-06-04 13:59:08 | [train_policy] epoch #847 | Computing loss after
2021-06-04 13:59:08 | [train_policy] epoch #847 | Fitting baseline...
2021-06-04 13:59:08 | [train_policy] epoch #847 | Saving snapshot...
2021-06-04 13:59:08 | [train_policy] epoch #847 | Saved
2021-06-04 13:59:08 | [train_policy] epoch #847 | Time 679.89 s
2021-06-04 13:59:08 | [train_policy] epoch #847 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28464
Evaluation/AverageDiscountedReturn          -41.8175
Evaluation/AverageReturn                    -41.8175
Evaluation/CompletionRate                     0
Evaluation/Iteration                        847
Evaluation/MaxReturn                        -30.0725
Evaluation/MinReturn                       -103.851
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.1056
Extras/EpisodeRewardMean                    -41.6866
LinearFeatureBaseline/ExplainedVariance       0.7537
PolicyExecTime                                0.22787
ProcessExecTime                               0.0311549
TotalEnvSteps                            858176
policy/Entropy                               -1.7614
policy/KL                                     0.00975052
policy/KLBefore                               0
policy/LossAfter                             -0.0161651
policy/LossBefore                            -7.06774e-10
policy/Perplexity                             0.171804
policy/dLoss                                  0.0161651
---------------------------------------  ----------------
2021-06-04 13:59:08 | [train_policy] epoch #848 | Obtaining samples for iteration 848...
2021-06-04 13:59:08 | [train_policy] epoch #848 | Logging diagnostics...
2021-06-04 13:59:08 | [train_policy] epoch #848 | Optimizing policy...
2021-06-04 13:59:08 | [train_policy] epoch #848 | Computing loss before
2021-06-04 13:59:08 | [train_policy] epoch #848 | Computing KL before
2021-06-04 13:59:08 | [train_policy] epoch #848 | Optimizing
2021-06-04 13:59:08 | [train_policy] epoch #848 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:08 | [train_policy] epoch #848 | computing loss before
2021-06-04 13:59:08 | [train_policy] epoch #848 | computing gradient
2021-06-04 13:59:08 | [train_policy] epoch #848 | gradient computed
2021-06-04 13:59:08 | [train_policy] epoch #848 | computing descent direction
2021-06-04 13:59:08 | [train_policy] epoch #848 | descent direction computed
2021-06-04 13:59:08 | [train_policy] epoch #848 | backtrack iters: 0
2021-06-04 13:59:08 | [train_policy] epoch #848 | optimization finished
2021-06-04 13:59:08 | [train_policy] epoch #848 | Computing KL after
2021-06-04 13:59:08 | [train_policy] epoch #848 | Computing loss after
2021-06-04 13:59:08 | [train_policy] epoch #848 | Fitting baseline...
2021-06-04 13:59:08 | [train_policy] epoch #848 | Saving snapshot...
2021-06-04 13:59:08 | [train_policy] epoch #848 | Saved
2021-06-04 13:59:08 | [train_policy] epoch #848 | Time 680.68 s
2021-06-04 13:59:08 | [train_policy] epoch #848 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287039
Evaluation/AverageDiscountedReturn          -41.3907
Evaluation/AverageReturn                    -41.3907
Evaluation/CompletionRate                     0
Evaluation/Iteration                        848
Evaluation/MaxReturn                        -27.9032
Evaluation/MinReturn                        -88.4357
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.18168
Extras/EpisodeRewardMean                    -41.2562
LinearFeatureBaseline/ExplainedVariance       0.830834
PolicyExecTime                                0.230698
ProcessExecTime                               0.0314541
TotalEnvSteps                            859188
policy/Entropy                               -1.74825
policy/KL                                     0.00955292
policy/KLBefore                               0
policy/LossAfter                             -0.02053
policy/LossBefore                            -9.95374e-09
policy/Perplexity                             0.174078
policy/dLoss                                  0.02053
---------------------------------------  ----------------
2021-06-04 13:59:08 | [train_policy] epoch #849 | Obtaining samples for iteration 849...
2021-06-04 13:59:09 | [train_policy] epoch #849 | Logging diagnostics...
2021-06-04 13:59:09 | [train_policy] epoch #849 | Optimizing policy...
2021-06-04 13:59:09 | [train_policy] epoch #849 | Computing loss before
2021-06-04 13:59:09 | [train_policy] epoch #849 | Computing KL before
2021-06-04 13:59:09 | [train_policy] epoch #849 | Optimizing
2021-06-04 13:59:09 | [train_policy] epoch #849 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:09 | [train_policy] epoch #849 | computing loss before
2021-06-04 13:59:09 | [train_policy] epoch #849 | computing gradient
2021-06-04 13:59:09 | [train_policy] epoch #849 | gradient computed
2021-06-04 13:59:09 | [train_policy] epoch #849 | computing descent direction
2021-06-04 13:59:09 | [train_policy] epoch #849 | descent direction computed
2021-06-04 13:59:09 | [train_policy] epoch #849 | backtrack iters: 1
2021-06-04 13:59:09 | [train_policy] epoch #849 | optimization finished
2021-06-04 13:59:09 | [train_policy] epoch #849 | Computing KL after
2021-06-04 13:59:09 | [train_policy] epoch #849 | Computing loss after
2021-06-04 13:59:09 | [train_policy] epoch #849 | Fitting baseline...
2021-06-04 13:59:09 | [train_policy] epoch #849 | Saving snapshot...
2021-06-04 13:59:09 | [train_policy] epoch #849 | Saved
2021-06-04 13:59:09 | [train_policy] epoch #849 | Time 681.51 s
2021-06-04 13:59:09 | [train_policy] epoch #849 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.285509
Evaluation/AverageDiscountedReturn          -40.5878
Evaluation/AverageReturn                    -40.5878
Evaluation/CompletionRate                     0
Evaluation/Iteration                        849
Evaluation/MaxReturn                        -28.6939
Evaluation/MinReturn                        -64.6961
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.28473
Extras/EpisodeRewardMean                    -40.7679
LinearFeatureBaseline/ExplainedVariance       0.892909
PolicyExecTime                                0.238882
ProcessExecTime                               0.0311906
TotalEnvSteps                            860200
policy/Entropy                               -1.77615
policy/KL                                     0.00644137
policy/KLBefore                               0
policy/LossAfter                             -0.0157371
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.169289
policy/dLoss                                  0.015737
---------------------------------------  ----------------
2021-06-04 13:59:09 | [train_policy] epoch #850 | Obtaining samples for iteration 850...
2021-06-04 13:59:10 | [train_policy] epoch #850 | Logging diagnostics...
2021-06-04 13:59:10 | [train_policy] epoch #850 | Optimizing policy...
2021-06-04 13:59:10 | [train_policy] epoch #850 | Computing loss before
2021-06-04 13:59:10 | [train_policy] epoch #850 | Computing KL before
2021-06-04 13:59:10 | [train_policy] epoch #850 | Optimizing
2021-06-04 13:59:10 | [train_policy] epoch #850 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:10 | [train_policy] epoch #850 | computing loss before
2021-06-04 13:59:10 | [train_policy] epoch #850 | computing gradient
2021-06-04 13:59:10 | [train_policy] epoch #850 | gradient computed
2021-06-04 13:59:10 | [train_policy] epoch #850 | computing descent direction
2021-06-04 13:59:10 | [train_policy] epoch #850 | descent direction computed
2021-06-04 13:59:10 | [train_policy] epoch #850 | backtrack iters: 1
2021-06-04 13:59:10 | [train_policy] epoch #850 | optimization finished
2021-06-04 13:59:10 | [train_policy] epoch #850 | Computing KL after
2021-06-04 13:59:10 | [train_policy] epoch #850 | Computing loss after
2021-06-04 13:59:10 | [train_policy] epoch #850 | Fitting baseline...
2021-06-04 13:59:10 | [train_policy] epoch #850 | Saving snapshot...
2021-06-04 13:59:10 | [train_policy] epoch #850 | Saved
2021-06-04 13:59:10 | [train_policy] epoch #850 | Time 682.30 s
2021-06-04 13:59:10 | [train_policy] epoch #850 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284606
Evaluation/AverageDiscountedReturn          -62.9656
Evaluation/AverageReturn                    -62.9656
Evaluation/CompletionRate                     0
Evaluation/Iteration                        850
Evaluation/MaxReturn                        -28.3454
Evaluation/MinReturn                      -2061.98
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.784
Extras/EpisodeRewardMean                    -61.1415
LinearFeatureBaseline/ExplainedVariance       0.0137773
PolicyExecTime                                0.208341
ProcessExecTime                               0.0311348
TotalEnvSteps                            861212
policy/Entropy                               -1.77403
policy/KL                                     0.0065973
policy/KLBefore                               0
policy/LossAfter                             -0.0265353
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.169648
policy/dLoss                                  0.0265353
---------------------------------------  ----------------
2021-06-04 13:59:10 | [train_policy] epoch #851 | Obtaining samples for iteration 851...
2021-06-04 13:59:11 | [train_policy] epoch #851 | Logging diagnostics...
2021-06-04 13:59:11 | [train_policy] epoch #851 | Optimizing policy...
2021-06-04 13:59:11 | [train_policy] epoch #851 | Computing loss before
2021-06-04 13:59:11 | [train_policy] epoch #851 | Computing KL before
2021-06-04 13:59:11 | [train_policy] epoch #851 | Optimizing
2021-06-04 13:59:11 | [train_policy] epoch #851 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:11 | [train_policy] epoch #851 | computing loss before
2021-06-04 13:59:11 | [train_policy] epoch #851 | computing gradient
2021-06-04 13:59:11 | [train_policy] epoch #851 | gradient computed
2021-06-04 13:59:11 | [train_policy] epoch #851 | computing descent direction
2021-06-04 13:59:11 | [train_policy] epoch #851 | descent direction computed
2021-06-04 13:59:11 | [train_policy] epoch #851 | backtrack iters: 0
2021-06-04 13:59:11 | [train_policy] epoch #851 | optimization finished
2021-06-04 13:59:11 | [train_policy] epoch #851 | Computing KL after
2021-06-04 13:59:11 | [train_policy] epoch #851 | Computing loss after
2021-06-04 13:59:11 | [train_policy] epoch #851 | Fitting baseline...
2021-06-04 13:59:11 | [train_policy] epoch #851 | Saving snapshot...
2021-06-04 13:59:11 | [train_policy] epoch #851 | Saved
2021-06-04 13:59:11 | [train_policy] epoch #851 | Time 683.08 s
2021-06-04 13:59:11 | [train_policy] epoch #851 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284981
Evaluation/AverageDiscountedReturn          -41.1589
Evaluation/AverageReturn                    -41.1589
Evaluation/CompletionRate                     0
Evaluation/Iteration                        851
Evaluation/MaxReturn                        -31.5413
Evaluation/MinReturn                        -89.0544
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1849
Extras/EpisodeRewardMean                    -41.5567
LinearFeatureBaseline/ExplainedVariance     -21.4943
PolicyExecTime                                0.216454
ProcessExecTime                               0.0312464
TotalEnvSteps                            862224
policy/Entropy                               -1.76308
policy/KL                                     0.00973641
policy/KLBefore                               0
policy/LossAfter                             -0.0265688
policy/LossBefore                            -3.06269e-08
policy/Perplexity                             0.171516
policy/dLoss                                  0.0265688
---------------------------------------  ----------------
2021-06-04 13:59:11 | [train_policy] epoch #852 | Obtaining samples for iteration 852...
2021-06-04 13:59:11 | [train_policy] epoch #852 | Logging diagnostics...
2021-06-04 13:59:11 | [train_policy] epoch #852 | Optimizing policy...
2021-06-04 13:59:11 | [train_policy] epoch #852 | Computing loss before
2021-06-04 13:59:12 | [train_policy] epoch #852 | Computing KL before
2021-06-04 13:59:12 | [train_policy] epoch #852 | Optimizing
2021-06-04 13:59:12 | [train_policy] epoch #852 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:12 | [train_policy] epoch #852 | computing loss before
2021-06-04 13:59:12 | [train_policy] epoch #852 | computing gradient
2021-06-04 13:59:12 | [train_policy] epoch #852 | gradient computed
2021-06-04 13:59:12 | [train_policy] epoch #852 | computing descent direction
2021-06-04 13:59:12 | [train_policy] epoch #852 | descent direction computed
2021-06-04 13:59:12 | [train_policy] epoch #852 | backtrack iters: 0
2021-06-04 13:59:12 | [train_policy] epoch #852 | optimization finished
2021-06-04 13:59:12 | [train_policy] epoch #852 | Computing KL after
2021-06-04 13:59:12 | [train_policy] epoch #852 | Computing loss after
2021-06-04 13:59:12 | [train_policy] epoch #852 | Fitting baseline...
2021-06-04 13:59:12 | [train_policy] epoch #852 | Saving snapshot...
2021-06-04 13:59:12 | [train_policy] epoch #852 | Saved
2021-06-04 13:59:12 | [train_policy] epoch #852 | Time 683.87 s
2021-06-04 13:59:12 | [train_policy] epoch #852 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286156
Evaluation/AverageDiscountedReturn          -41.1508
Evaluation/AverageReturn                    -41.1508
Evaluation/CompletionRate                     0
Evaluation/Iteration                        852
Evaluation/MaxReturn                        -29.633
Evaluation/MinReturn                        -86.5585
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.63372
Extras/EpisodeRewardMean                    -40.658
LinearFeatureBaseline/ExplainedVariance       0.85329
PolicyExecTime                                0.211852
ProcessExecTime                               0.0313704
TotalEnvSteps                            863236
policy/Entropy                               -1.76416
policy/KL                                     0.0094421
policy/KLBefore                               0
policy/LossAfter                             -0.0229993
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.171331
policy/dLoss                                  0.0229993
---------------------------------------  ----------------
2021-06-04 13:59:12 | [train_policy] epoch #853 | Obtaining samples for iteration 853...
2021-06-04 13:59:12 | [train_policy] epoch #853 | Logging diagnostics...
2021-06-04 13:59:12 | [train_policy] epoch #853 | Optimizing policy...
2021-06-04 13:59:12 | [train_policy] epoch #853 | Computing loss before
2021-06-04 13:59:12 | [train_policy] epoch #853 | Computing KL before
2021-06-04 13:59:12 | [train_policy] epoch #853 | Optimizing
2021-06-04 13:59:12 | [train_policy] epoch #853 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:12 | [train_policy] epoch #853 | computing loss before
2021-06-04 13:59:12 | [train_policy] epoch #853 | computing gradient
2021-06-04 13:59:12 | [train_policy] epoch #853 | gradient computed
2021-06-04 13:59:12 | [train_policy] epoch #853 | computing descent direction
2021-06-04 13:59:12 | [train_policy] epoch #853 | descent direction computed
2021-06-04 13:59:12 | [train_policy] epoch #853 | backtrack iters: 1
2021-06-04 13:59:12 | [train_policy] epoch #853 | optimization finished
2021-06-04 13:59:12 | [train_policy] epoch #853 | Computing KL after
2021-06-04 13:59:12 | [train_policy] epoch #853 | Computing loss after
2021-06-04 13:59:12 | [train_policy] epoch #853 | Fitting baseline...
2021-06-04 13:59:12 | [train_policy] epoch #853 | Saving snapshot...
2021-06-04 13:59:12 | [train_policy] epoch #853 | Saved
2021-06-04 13:59:12 | [train_policy] epoch #853 | Time 684.68 s
2021-06-04 13:59:12 | [train_policy] epoch #853 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287551
Evaluation/AverageDiscountedReturn          -41.9322
Evaluation/AverageReturn                    -41.9322
Evaluation/CompletionRate                     0
Evaluation/Iteration                        853
Evaluation/MaxReturn                        -29.7181
Evaluation/MinReturn                        -63.7927
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.46234
Extras/EpisodeRewardMean                    -42.1047
LinearFeatureBaseline/ExplainedVariance       0.861586
PolicyExecTime                                0.233184
ProcessExecTime                               0.0314398
TotalEnvSteps                            864248
policy/Entropy                               -1.77384
policy/KL                                     0.00645257
policy/KLBefore                               0
policy/LossAfter                             -0.0139457
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.16968
policy/dLoss                                  0.0139457
---------------------------------------  ----------------
2021-06-04 13:59:12 | [train_policy] epoch #854 | Obtaining samples for iteration 854...
2021-06-04 13:59:13 | [train_policy] epoch #854 | Logging diagnostics...
2021-06-04 13:59:13 | [train_policy] epoch #854 | Optimizing policy...
2021-06-04 13:59:13 | [train_policy] epoch #854 | Computing loss before
2021-06-04 13:59:13 | [train_policy] epoch #854 | Computing KL before
2021-06-04 13:59:13 | [train_policy] epoch #854 | Optimizing
2021-06-04 13:59:13 | [train_policy] epoch #854 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:13 | [train_policy] epoch #854 | computing loss before
2021-06-04 13:59:13 | [train_policy] epoch #854 | computing gradient
2021-06-04 13:59:13 | [train_policy] epoch #854 | gradient computed
2021-06-04 13:59:13 | [train_policy] epoch #854 | computing descent direction
2021-06-04 13:59:13 | [train_policy] epoch #854 | descent direction computed
2021-06-04 13:59:13 | [train_policy] epoch #854 | backtrack iters: 0
2021-06-04 13:59:13 | [train_policy] epoch #854 | optimization finished
2021-06-04 13:59:13 | [train_policy] epoch #854 | Computing KL after
2021-06-04 13:59:13 | [train_policy] epoch #854 | Computing loss after
2021-06-04 13:59:13 | [train_policy] epoch #854 | Fitting baseline...
2021-06-04 13:59:13 | [train_policy] epoch #854 | Saving snapshot...
2021-06-04 13:59:13 | [train_policy] epoch #854 | Saved
2021-06-04 13:59:13 | [train_policy] epoch #854 | Time 685.46 s
2021-06-04 13:59:13 | [train_policy] epoch #854 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28482
Evaluation/AverageDiscountedReturn          -41.2909
Evaluation/AverageReturn                    -41.2909
Evaluation/CompletionRate                     0
Evaluation/Iteration                        854
Evaluation/MaxReturn                        -30.9426
Evaluation/MinReturn                        -97.1735
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.92836
Extras/EpisodeRewardMean                    -41.678
LinearFeatureBaseline/ExplainedVariance       0.813629
PolicyExecTime                                0.214614
ProcessExecTime                               0.031137
TotalEnvSteps                            865260
policy/Entropy                               -1.74629
policy/KL                                     0.00986948
policy/KLBefore                               0
policy/LossAfter                             -0.0233385
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.174419
policy/dLoss                                  0.0233384
---------------------------------------  ----------------
2021-06-04 13:59:13 | [train_policy] epoch #855 | Obtaining samples for iteration 855...
2021-06-04 13:59:14 | [train_policy] epoch #855 | Logging diagnostics...
2021-06-04 13:59:14 | [train_policy] epoch #855 | Optimizing policy...
2021-06-04 13:59:14 | [train_policy] epoch #855 | Computing loss before
2021-06-04 13:59:14 | [train_policy] epoch #855 | Computing KL before
2021-06-04 13:59:14 | [train_policy] epoch #855 | Optimizing
2021-06-04 13:59:14 | [train_policy] epoch #855 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:14 | [train_policy] epoch #855 | computing loss before
2021-06-04 13:59:14 | [train_policy] epoch #855 | computing gradient
2021-06-04 13:59:14 | [train_policy] epoch #855 | gradient computed
2021-06-04 13:59:14 | [train_policy] epoch #855 | computing descent direction
2021-06-04 13:59:14 | [train_policy] epoch #855 | descent direction computed
2021-06-04 13:59:14 | [train_policy] epoch #855 | backtrack iters: 0
2021-06-04 13:59:14 | [train_policy] epoch #855 | optimization finished
2021-06-04 13:59:14 | [train_policy] epoch #855 | Computing KL after
2021-06-04 13:59:14 | [train_policy] epoch #855 | Computing loss after
2021-06-04 13:59:14 | [train_policy] epoch #855 | Fitting baseline...
2021-06-04 13:59:14 | [train_policy] epoch #855 | Saving snapshot...
2021-06-04 13:59:14 | [train_policy] epoch #855 | Saved
2021-06-04 13:59:14 | [train_policy] epoch #855 | Time 686.27 s
2021-06-04 13:59:14 | [train_policy] epoch #855 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.287139
Evaluation/AverageDiscountedReturn          -48.9238
Evaluation/AverageReturn                    -48.9238
Evaluation/CompletionRate                     0
Evaluation/Iteration                        855
Evaluation/MaxReturn                        -28.2447
Evaluation/MinReturn                       -707.118
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         69.5064
Extras/EpisodeRewardMean                    -48.1082
LinearFeatureBaseline/ExplainedVariance       0.0730681
PolicyExecTime                                0.23916
ProcessExecTime                               0.0313942
TotalEnvSteps                            866272
policy/Entropy                               -1.74657
policy/KL                                     0.00996836
policy/KLBefore                               0
policy/LossAfter                             -0.0243387
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.174371
policy/dLoss                                  0.0243387
---------------------------------------  ---------------
2021-06-04 13:59:14 | [train_policy] epoch #856 | Obtaining samples for iteration 856...
2021-06-04 13:59:15 | [train_policy] epoch #856 | Logging diagnostics...
2021-06-04 13:59:15 | [train_policy] epoch #856 | Optimizing policy...
2021-06-04 13:59:15 | [train_policy] epoch #856 | Computing loss before
2021-06-04 13:59:15 | [train_policy] epoch #856 | Computing KL before
2021-06-04 13:59:15 | [train_policy] epoch #856 | Optimizing
2021-06-04 13:59:15 | [train_policy] epoch #856 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:15 | [train_policy] epoch #856 | computing loss before
2021-06-04 13:59:15 | [train_policy] epoch #856 | computing gradient
2021-06-04 13:59:15 | [train_policy] epoch #856 | gradient computed
2021-06-04 13:59:15 | [train_policy] epoch #856 | computing descent direction
2021-06-04 13:59:15 | [train_policy] epoch #856 | descent direction computed
2021-06-04 13:59:15 | [train_policy] epoch #856 | backtrack iters: 0
2021-06-04 13:59:15 | [train_policy] epoch #856 | optimization finished
2021-06-04 13:59:15 | [train_policy] epoch #856 | Computing KL after
2021-06-04 13:59:15 | [train_policy] epoch #856 | Computing loss after
2021-06-04 13:59:15 | [train_policy] epoch #856 | Fitting baseline...
2021-06-04 13:59:15 | [train_policy] epoch #856 | Saving snapshot...
2021-06-04 13:59:15 | [train_policy] epoch #856 | Saved
2021-06-04 13:59:15 | [train_policy] epoch #856 | Time 687.06 s
2021-06-04 13:59:15 | [train_policy] epoch #856 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.28597
Evaluation/AverageDiscountedReturn          -45.1897
Evaluation/AverageReturn                    -45.1897
Evaluation/CompletionRate                     0
Evaluation/Iteration                        856
Evaluation/MaxReturn                        -28.7562
Evaluation/MinReturn                       -315.053
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         29.9207
Extras/EpisodeRewardMean                    -44.6574
LinearFeatureBaseline/ExplainedVariance      -0.487478
PolicyExecTime                                0.227005
ProcessExecTime                               0.0313435
TotalEnvSteps                            867284
policy/Entropy                               -1.73925
policy/KL                                     0.0098663
policy/KLBefore                               0
policy/LossAfter                             -0.0389844
policy/LossBefore                            -1.7905e-08
policy/Perplexity                             0.175652
policy/dLoss                                  0.0389844
---------------------------------------  ---------------
2021-06-04 13:59:15 | [train_policy] epoch #857 | Obtaining samples for iteration 857...
2021-06-04 13:59:15 | [train_policy] epoch #857 | Logging diagnostics...
2021-06-04 13:59:15 | [train_policy] epoch #857 | Optimizing policy...
2021-06-04 13:59:15 | [train_policy] epoch #857 | Computing loss before
2021-06-04 13:59:15 | [train_policy] epoch #857 | Computing KL before
2021-06-04 13:59:15 | [train_policy] epoch #857 | Optimizing
2021-06-04 13:59:15 | [train_policy] epoch #857 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:15 | [train_policy] epoch #857 | computing loss before
2021-06-04 13:59:15 | [train_policy] epoch #857 | computing gradient
2021-06-04 13:59:16 | [train_policy] epoch #857 | gradient computed
2021-06-04 13:59:16 | [train_policy] epoch #857 | computing descent direction
2021-06-04 13:59:16 | [train_policy] epoch #857 | descent direction computed
2021-06-04 13:59:16 | [train_policy] epoch #857 | backtrack iters: 1
2021-06-04 13:59:16 | [train_policy] epoch #857 | optimization finished
2021-06-04 13:59:16 | [train_policy] epoch #857 | Computing KL after
2021-06-04 13:59:16 | [train_policy] epoch #857 | Computing loss after
2021-06-04 13:59:16 | [train_policy] epoch #857 | Fitting baseline...
2021-06-04 13:59:16 | [train_policy] epoch #857 | Saving snapshot...
2021-06-04 13:59:16 | [train_policy] epoch #857 | Saved
2021-06-04 13:59:16 | [train_policy] epoch #857 | Time 687.86 s
2021-06-04 13:59:16 | [train_policy] epoch #857 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28545
Evaluation/AverageDiscountedReturn          -41.3713
Evaluation/AverageReturn                    -41.3713
Evaluation/CompletionRate                     0
Evaluation/Iteration                        857
Evaluation/MaxReturn                        -29.9975
Evaluation/MinReturn                       -136.589
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.4392
Extras/EpisodeRewardMean                    -41.5643
LinearFeatureBaseline/ExplainedVariance       0.585276
PolicyExecTime                                0.22391
ProcessExecTime                               0.0312142
TotalEnvSteps                            868296
policy/Entropy                               -1.77058
policy/KL                                     0.00670135
policy/KLBefore                               0
policy/LossAfter                             -0.013879
policy/LossBefore                            -1.86117e-08
policy/Perplexity                             0.170235
policy/dLoss                                  0.013879
---------------------------------------  ----------------
2021-06-04 13:59:16 | [train_policy] epoch #858 | Obtaining samples for iteration 858...
2021-06-04 13:59:16 | [train_policy] epoch #858 | Logging diagnostics...
2021-06-04 13:59:16 | [train_policy] epoch #858 | Optimizing policy...
2021-06-04 13:59:16 | [train_policy] epoch #858 | Computing loss before
2021-06-04 13:59:16 | [train_policy] epoch #858 | Computing KL before
2021-06-04 13:59:16 | [train_policy] epoch #858 | Optimizing
2021-06-04 13:59:16 | [train_policy] epoch #858 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:16 | [train_policy] epoch #858 | computing loss before
2021-06-04 13:59:16 | [train_policy] epoch #858 | computing gradient
2021-06-04 13:59:16 | [train_policy] epoch #858 | gradient computed
2021-06-04 13:59:16 | [train_policy] epoch #858 | computing descent direction
2021-06-04 13:59:16 | [train_policy] epoch #858 | descent direction computed
2021-06-04 13:59:16 | [train_policy] epoch #858 | backtrack iters: 1
2021-06-04 13:59:16 | [train_policy] epoch #858 | optimization finished
2021-06-04 13:59:16 | [train_policy] epoch #858 | Computing KL after
2021-06-04 13:59:16 | [train_policy] epoch #858 | Computing loss after
2021-06-04 13:59:16 | [train_policy] epoch #858 | Fitting baseline...
2021-06-04 13:59:16 | [train_policy] epoch #858 | Saving snapshot...
2021-06-04 13:59:16 | [train_policy] epoch #858 | Saved
2021-06-04 13:59:16 | [train_policy] epoch #858 | Time 688.65 s
2021-06-04 13:59:16 | [train_policy] epoch #858 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.288079
Evaluation/AverageDiscountedReturn          -42.3616
Evaluation/AverageReturn                    -42.3616
Evaluation/CompletionRate                     0
Evaluation/Iteration                        858
Evaluation/MaxReturn                        -30.0868
Evaluation/MinReturn                        -90.7391
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.51487
Extras/EpisodeRewardMean                    -41.9499
LinearFeatureBaseline/ExplainedVariance       0.758774
PolicyExecTime                                0.212679
ProcessExecTime                               0.0315943
TotalEnvSteps                            869308
policy/Entropy                               -1.77544
policy/KL                                     0.00642676
policy/KLBefore                               0
policy/LossAfter                             -0.0190234
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.16941
policy/dLoss                                  0.0190234
---------------------------------------  ----------------
2021-06-04 13:59:16 | [train_policy] epoch #859 | Obtaining samples for iteration 859...
2021-06-04 13:59:17 | [train_policy] epoch #859 | Logging diagnostics...
2021-06-04 13:59:17 | [train_policy] epoch #859 | Optimizing policy...
2021-06-04 13:59:17 | [train_policy] epoch #859 | Computing loss before
2021-06-04 13:59:17 | [train_policy] epoch #859 | Computing KL before
2021-06-04 13:59:17 | [train_policy] epoch #859 | Optimizing
2021-06-04 13:59:17 | [train_policy] epoch #859 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:17 | [train_policy] epoch #859 | computing loss before
2021-06-04 13:59:17 | [train_policy] epoch #859 | computing gradient
2021-06-04 13:59:17 | [train_policy] epoch #859 | gradient computed
2021-06-04 13:59:17 | [train_policy] epoch #859 | computing descent direction
2021-06-04 13:59:17 | [train_policy] epoch #859 | descent direction computed
2021-06-04 13:59:17 | [train_policy] epoch #859 | backtrack iters: 0
2021-06-04 13:59:17 | [train_policy] epoch #859 | optimization finished
2021-06-04 13:59:17 | [train_policy] epoch #859 | Computing KL after
2021-06-04 13:59:17 | [train_policy] epoch #859 | Computing loss after
2021-06-04 13:59:17 | [train_policy] epoch #859 | Fitting baseline...
2021-06-04 13:59:17 | [train_policy] epoch #859 | Saving snapshot...
2021-06-04 13:59:17 | [train_policy] epoch #859 | Saved
2021-06-04 13:59:17 | [train_policy] epoch #859 | Time 689.45 s
2021-06-04 13:59:17 | [train_policy] epoch #859 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284995
Evaluation/AverageDiscountedReturn          -61.842
Evaluation/AverageReturn                    -61.842
Evaluation/CompletionRate                     0
Evaluation/Iteration                        859
Evaluation/MaxReturn                        -28.683
Evaluation/MinReturn                      -2062.15
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.791
Extras/EpisodeRewardMean                    -60.0044
LinearFeatureBaseline/ExplainedVariance       0.0165138
PolicyExecTime                                0.22473
ProcessExecTime                               0.0312021
TotalEnvSteps                            870320
policy/Entropy                               -1.75689
policy/KL                                     0.00968995
policy/KLBefore                               0
policy/LossAfter                             -0.0241556
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.172581
policy/dLoss                                  0.0241556
---------------------------------------  ----------------
2021-06-04 13:59:17 | [train_policy] epoch #860 | Obtaining samples for iteration 860...
2021-06-04 13:59:18 | [train_policy] epoch #860 | Logging diagnostics...
2021-06-04 13:59:18 | [train_policy] epoch #860 | Optimizing policy...
2021-06-04 13:59:18 | [train_policy] epoch #860 | Computing loss before
2021-06-04 13:59:18 | [train_policy] epoch #860 | Computing KL before
2021-06-04 13:59:18 | [train_policy] epoch #860 | Optimizing
2021-06-04 13:59:18 | [train_policy] epoch #860 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:18 | [train_policy] epoch #860 | computing loss before
2021-06-04 13:59:18 | [train_policy] epoch #860 | computing gradient
2021-06-04 13:59:18 | [train_policy] epoch #860 | gradient computed
2021-06-04 13:59:18 | [train_policy] epoch #860 | computing descent direction
2021-06-04 13:59:18 | [train_policy] epoch #860 | descent direction computed
2021-06-04 13:59:18 | [train_policy] epoch #860 | backtrack iters: 1
2021-06-04 13:59:18 | [train_policy] epoch #860 | optimization finished
2021-06-04 13:59:18 | [train_policy] epoch #860 | Computing KL after
2021-06-04 13:59:18 | [train_policy] epoch #860 | Computing loss after
2021-06-04 13:59:18 | [train_policy] epoch #860 | Fitting baseline...
2021-06-04 13:59:18 | [train_policy] epoch #860 | Saving snapshot...
2021-06-04 13:59:18 | [train_policy] epoch #860 | Saved
2021-06-04 13:59:18 | [train_policy] epoch #860 | Time 690.25 s
2021-06-04 13:59:18 | [train_policy] epoch #860 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285574
Evaluation/AverageDiscountedReturn          -39.5414
Evaluation/AverageReturn                    -39.5414
Evaluation/CompletionRate                     0
Evaluation/Iteration                        860
Evaluation/MaxReturn                        -28.5168
Evaluation/MinReturn                        -57.0723
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.26649
Extras/EpisodeRewardMean                    -59.8043
LinearFeatureBaseline/ExplainedVariance     -30.9652
PolicyExecTime                                0.223082
ProcessExecTime                               0.0312951
TotalEnvSteps                            871332
policy/Entropy                               -1.79092
policy/KL                                     0.00649522
policy/KLBefore                               0
policy/LossAfter                             -0.0149764
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.166807
policy/dLoss                                  0.0149764
---------------------------------------  ----------------
2021-06-04 13:59:18 | [train_policy] epoch #861 | Obtaining samples for iteration 861...
2021-06-04 13:59:19 | [train_policy] epoch #861 | Logging diagnostics...
2021-06-04 13:59:19 | [train_policy] epoch #861 | Optimizing policy...
2021-06-04 13:59:19 | [train_policy] epoch #861 | Computing loss before
2021-06-04 13:59:19 | [train_policy] epoch #861 | Computing KL before
2021-06-04 13:59:19 | [train_policy] epoch #861 | Optimizing
2021-06-04 13:59:19 | [train_policy] epoch #861 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:19 | [train_policy] epoch #861 | computing loss before
2021-06-04 13:59:19 | [train_policy] epoch #861 | computing gradient
2021-06-04 13:59:19 | [train_policy] epoch #861 | gradient computed
2021-06-04 13:59:19 | [train_policy] epoch #861 | computing descent direction
2021-06-04 13:59:19 | [train_policy] epoch #861 | descent direction computed
2021-06-04 13:59:19 | [train_policy] epoch #861 | backtrack iters: 1
2021-06-04 13:59:19 | [train_policy] epoch #861 | optimization finished
2021-06-04 13:59:19 | [train_policy] epoch #861 | Computing KL after
2021-06-04 13:59:19 | [train_policy] epoch #861 | Computing loss after
2021-06-04 13:59:19 | [train_policy] epoch #861 | Fitting baseline...
2021-06-04 13:59:19 | [train_policy] epoch #861 | Saving snapshot...
2021-06-04 13:59:19 | [train_policy] epoch #861 | Saved
2021-06-04 13:59:19 | [train_policy] epoch #861 | Time 691.04 s
2021-06-04 13:59:19 | [train_policy] epoch #861 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284749
Evaluation/AverageDiscountedReturn          -40.7045
Evaluation/AverageReturn                    -40.7045
Evaluation/CompletionRate                     0
Evaluation/Iteration                        861
Evaluation/MaxReturn                        -29.47
Evaluation/MinReturn                        -63.7646
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.77238
Extras/EpisodeRewardMean                    -40.9863
LinearFeatureBaseline/ExplainedVariance       0.899062
PolicyExecTime                                0.221863
ProcessExecTime                               0.0312212
TotalEnvSteps                            872344
policy/Entropy                               -1.80611
policy/KL                                     0.006526
policy/KLBefore                               0
policy/LossAfter                             -0.0149335
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.164292
policy/dLoss                                  0.0149335
---------------------------------------  ----------------
2021-06-04 13:59:19 | [train_policy] epoch #862 | Obtaining samples for iteration 862...
2021-06-04 13:59:19 | [train_policy] epoch #862 | Logging diagnostics...
2021-06-04 13:59:19 | [train_policy] epoch #862 | Optimizing policy...
2021-06-04 13:59:19 | [train_policy] epoch #862 | Computing loss before
2021-06-04 13:59:19 | [train_policy] epoch #862 | Computing KL before
2021-06-04 13:59:19 | [train_policy] epoch #862 | Optimizing
2021-06-04 13:59:19 | [train_policy] epoch #862 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:19 | [train_policy] epoch #862 | computing loss before
2021-06-04 13:59:19 | [train_policy] epoch #862 | computing gradient
2021-06-04 13:59:19 | [train_policy] epoch #862 | gradient computed
2021-06-04 13:59:19 | [train_policy] epoch #862 | computing descent direction
2021-06-04 13:59:20 | [train_policy] epoch #862 | descent direction computed
2021-06-04 13:59:20 | [train_policy] epoch #862 | backtrack iters: 1
2021-06-04 13:59:20 | [train_policy] epoch #862 | optimization finished
2021-06-04 13:59:20 | [train_policy] epoch #862 | Computing KL after
2021-06-04 13:59:20 | [train_policy] epoch #862 | Computing loss after
2021-06-04 13:59:20 | [train_policy] epoch #862 | Fitting baseline...
2021-06-04 13:59:20 | [train_policy] epoch #862 | Saving snapshot...
2021-06-04 13:59:20 | [train_policy] epoch #862 | Saved
2021-06-04 13:59:20 | [train_policy] epoch #862 | Time 691.83 s
2021-06-04 13:59:20 | [train_policy] epoch #862 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28604
Evaluation/AverageDiscountedReturn          -40.5921
Evaluation/AverageReturn                    -40.5921
Evaluation/CompletionRate                     0
Evaluation/Iteration                        862
Evaluation/MaxReturn                        -28.7718
Evaluation/MinReturn                        -63.3472
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.54243
Extras/EpisodeRewardMean                    -41.0722
LinearFeatureBaseline/ExplainedVariance       0.868117
PolicyExecTime                                0.212177
ProcessExecTime                               0.0312793
TotalEnvSteps                            873356
policy/Entropy                               -1.79997
policy/KL                                     0.00640563
policy/KLBefore                               0
policy/LossAfter                             -0.0412232
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.165304
policy/dLoss                                  0.0412232
---------------------------------------  ----------------
2021-06-04 13:59:20 | [train_policy] epoch #863 | Obtaining samples for iteration 863...
2021-06-04 13:59:20 | [train_policy] epoch #863 | Logging diagnostics...
2021-06-04 13:59:20 | [train_policy] epoch #863 | Optimizing policy...
2021-06-04 13:59:20 | [train_policy] epoch #863 | Computing loss before
2021-06-04 13:59:20 | [train_policy] epoch #863 | Computing KL before
2021-06-04 13:59:20 | [train_policy] epoch #863 | Optimizing
2021-06-04 13:59:20 | [train_policy] epoch #863 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:20 | [train_policy] epoch #863 | computing loss before
2021-06-04 13:59:20 | [train_policy] epoch #863 | computing gradient
2021-06-04 13:59:20 | [train_policy] epoch #863 | gradient computed
2021-06-04 13:59:20 | [train_policy] epoch #863 | computing descent direction
2021-06-04 13:59:20 | [train_policy] epoch #863 | descent direction computed
2021-06-04 13:59:20 | [train_policy] epoch #863 | backtrack iters: 1
2021-06-04 13:59:20 | [train_policy] epoch #863 | optimization finished
2021-06-04 13:59:20 | [train_policy] epoch #863 | Computing KL after
2021-06-04 13:59:20 | [train_policy] epoch #863 | Computing loss after
2021-06-04 13:59:20 | [train_policy] epoch #863 | Fitting baseline...
2021-06-04 13:59:20 | [train_policy] epoch #863 | Saving snapshot...
2021-06-04 13:59:20 | [train_policy] epoch #863 | Saved
2021-06-04 13:59:20 | [train_policy] epoch #863 | Time 692.63 s
2021-06-04 13:59:20 | [train_policy] epoch #863 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288365
Evaluation/AverageDiscountedReturn          -40.8487
Evaluation/AverageReturn                    -40.8487
Evaluation/CompletionRate                     0
Evaluation/Iteration                        863
Evaluation/MaxReturn                        -29.4685
Evaluation/MinReturn                        -64.011
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.19438
Extras/EpisodeRewardMean                    -40.8379
LinearFeatureBaseline/ExplainedVariance       0.888847
PolicyExecTime                                0.227033
ProcessExecTime                               0.0315375
TotalEnvSteps                            874368
policy/Entropy                               -1.81619
policy/KL                                     0.00672661
policy/KLBefore                               0
policy/LossAfter                             -0.0192356
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.162644
policy/dLoss                                  0.0192356
---------------------------------------  ----------------
2021-06-04 13:59:20 | [train_policy] epoch #864 | Obtaining samples for iteration 864...
2021-06-04 13:59:21 | [train_policy] epoch #864 | Logging diagnostics...
2021-06-04 13:59:21 | [train_policy] epoch #864 | Optimizing policy...
2021-06-04 13:59:21 | [train_policy] epoch #864 | Computing loss before
2021-06-04 13:59:21 | [train_policy] epoch #864 | Computing KL before
2021-06-04 13:59:21 | [train_policy] epoch #864 | Optimizing
2021-06-04 13:59:21 | [train_policy] epoch #864 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:21 | [train_policy] epoch #864 | computing loss before
2021-06-04 13:59:21 | [train_policy] epoch #864 | computing gradient
2021-06-04 13:59:21 | [train_policy] epoch #864 | gradient computed
2021-06-04 13:59:21 | [train_policy] epoch #864 | computing descent direction
2021-06-04 13:59:21 | [train_policy] epoch #864 | descent direction computed
2021-06-04 13:59:21 | [train_policy] epoch #864 | backtrack iters: 1
2021-06-04 13:59:21 | [train_policy] epoch #864 | optimization finished
2021-06-04 13:59:21 | [train_policy] epoch #864 | Computing KL after
2021-06-04 13:59:21 | [train_policy] epoch #864 | Computing loss after
2021-06-04 13:59:21 | [train_policy] epoch #864 | Fitting baseline...
2021-06-04 13:59:21 | [train_policy] epoch #864 | Saving snapshot...
2021-06-04 13:59:21 | [train_policy] epoch #864 | Saved
2021-06-04 13:59:21 | [train_policy] epoch #864 | Time 693.43 s
2021-06-04 13:59:21 | [train_policy] epoch #864 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286433
Evaluation/AverageDiscountedReturn          -39.4231
Evaluation/AverageReturn                    -39.4231
Evaluation/CompletionRate                     0
Evaluation/Iteration                        864
Evaluation/MaxReturn                        -31.2637
Evaluation/MinReturn                        -64.0186
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.1539
Extras/EpisodeRewardMean                    -39.4567
LinearFeatureBaseline/ExplainedVariance       0.893272
PolicyExecTime                                0.229584
ProcessExecTime                               0.0313785
TotalEnvSteps                            875380
policy/Entropy                               -1.81296
policy/KL                                     0.00663052
policy/KLBefore                               0
policy/LossAfter                             -0.0209239
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.16317
policy/dLoss                                  0.0209239
---------------------------------------  ----------------
2021-06-04 13:59:21 | [train_policy] epoch #865 | Obtaining samples for iteration 865...
2021-06-04 13:59:22 | [train_policy] epoch #865 | Logging diagnostics...
2021-06-04 13:59:22 | [train_policy] epoch #865 | Optimizing policy...
2021-06-04 13:59:22 | [train_policy] epoch #865 | Computing loss before
2021-06-04 13:59:22 | [train_policy] epoch #865 | Computing KL before
2021-06-04 13:59:22 | [train_policy] epoch #865 | Optimizing
2021-06-04 13:59:22 | [train_policy] epoch #865 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:22 | [train_policy] epoch #865 | computing loss before
2021-06-04 13:59:22 | [train_policy] epoch #865 | computing gradient
2021-06-04 13:59:22 | [train_policy] epoch #865 | gradient computed
2021-06-04 13:59:22 | [train_policy] epoch #865 | computing descent direction
2021-06-04 13:59:22 | [train_policy] epoch #865 | descent direction computed
2021-06-04 13:59:22 | [train_policy] epoch #865 | backtrack iters: 1
2021-06-04 13:59:22 | [train_policy] epoch #865 | optimization finished
2021-06-04 13:59:22 | [train_policy] epoch #865 | Computing KL after
2021-06-04 13:59:22 | [train_policy] epoch #865 | Computing loss after
2021-06-04 13:59:22 | [train_policy] epoch #865 | Fitting baseline...
2021-06-04 13:59:22 | [train_policy] epoch #865 | Saving snapshot...
2021-06-04 13:59:22 | [train_policy] epoch #865 | Saved
2021-06-04 13:59:22 | [train_policy] epoch #865 | Time 694.25 s
2021-06-04 13:59:22 | [train_policy] epoch #865 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286526
Evaluation/AverageDiscountedReturn          -40.6609
Evaluation/AverageReturn                    -40.6609
Evaluation/CompletionRate                     0
Evaluation/Iteration                        865
Evaluation/MaxReturn                        -29.3756
Evaluation/MinReturn                        -64.3389
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.32012
Extras/EpisodeRewardMean                    -40.5574
LinearFeatureBaseline/ExplainedVariance       0.877924
PolicyExecTime                                0.235292
ProcessExecTime                               0.0313816
TotalEnvSteps                            876392
policy/Entropy                               -1.83368
policy/KL                                     0.00650289
policy/KLBefore                               0
policy/LossAfter                             -0.0153806
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.159824
policy/dLoss                                  0.0153806
---------------------------------------  ----------------
2021-06-04 13:59:22 | [train_policy] epoch #866 | Obtaining samples for iteration 866...
2021-06-04 13:59:23 | [train_policy] epoch #866 | Logging diagnostics...
2021-06-04 13:59:23 | [train_policy] epoch #866 | Optimizing policy...
2021-06-04 13:59:23 | [train_policy] epoch #866 | Computing loss before
2021-06-04 13:59:23 | [train_policy] epoch #866 | Computing KL before
2021-06-04 13:59:23 | [train_policy] epoch #866 | Optimizing
2021-06-04 13:59:23 | [train_policy] epoch #866 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:23 | [train_policy] epoch #866 | computing loss before
2021-06-04 13:59:23 | [train_policy] epoch #866 | computing gradient
2021-06-04 13:59:23 | [train_policy] epoch #866 | gradient computed
2021-06-04 13:59:23 | [train_policy] epoch #866 | computing descent direction
2021-06-04 13:59:23 | [train_policy] epoch #866 | descent direction computed
2021-06-04 13:59:23 | [train_policy] epoch #866 | backtrack iters: 0
2021-06-04 13:59:23 | [train_policy] epoch #866 | optimization finished
2021-06-04 13:59:23 | [train_policy] epoch #866 | Computing KL after
2021-06-04 13:59:23 | [train_policy] epoch #866 | Computing loss after
2021-06-04 13:59:23 | [train_policy] epoch #866 | Fitting baseline...
2021-06-04 13:59:23 | [train_policy] epoch #866 | Saving snapshot...
2021-06-04 13:59:23 | [train_policy] epoch #866 | Saved
2021-06-04 13:59:23 | [train_policy] epoch #866 | Time 695.04 s
2021-06-04 13:59:23 | [train_policy] epoch #866 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284322
Evaluation/AverageDiscountedReturn          -40.5737
Evaluation/AverageReturn                    -40.5737
Evaluation/CompletionRate                     0
Evaluation/Iteration                        866
Evaluation/MaxReturn                        -28.0049
Evaluation/MinReturn                        -63.7758
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.31688
Extras/EpisodeRewardMean                    -40.7458
LinearFeatureBaseline/ExplainedVariance       0.883976
PolicyExecTime                                0.213799
ProcessExecTime                               0.0311596
TotalEnvSteps                            877404
policy/Entropy                               -1.81898
policy/KL                                     0.0096745
policy/KLBefore                               0
policy/LossAfter                             -0.0181769
policy/LossBefore                             6.12538e-09
policy/Perplexity                             0.162191
policy/dLoss                                  0.0181769
---------------------------------------  ----------------
2021-06-04 13:59:23 | [train_policy] epoch #867 | Obtaining samples for iteration 867...
2021-06-04 13:59:23 | [train_policy] epoch #867 | Logging diagnostics...
2021-06-04 13:59:23 | [train_policy] epoch #867 | Optimizing policy...
2021-06-04 13:59:23 | [train_policy] epoch #867 | Computing loss before
2021-06-04 13:59:23 | [train_policy] epoch #867 | Computing KL before
2021-06-04 13:59:23 | [train_policy] epoch #867 | Optimizing
2021-06-04 13:59:23 | [train_policy] epoch #867 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:23 | [train_policy] epoch #867 | computing loss before
2021-06-04 13:59:23 | [train_policy] epoch #867 | computing gradient
2021-06-04 13:59:23 | [train_policy] epoch #867 | gradient computed
2021-06-04 13:59:23 | [train_policy] epoch #867 | computing descent direction
2021-06-04 13:59:24 | [train_policy] epoch #867 | descent direction computed
2021-06-04 13:59:24 | [train_policy] epoch #867 | backtrack iters: 1
2021-06-04 13:59:24 | [train_policy] epoch #867 | optimization finished
2021-06-04 13:59:24 | [train_policy] epoch #867 | Computing KL after
2021-06-04 13:59:24 | [train_policy] epoch #867 | Computing loss after
2021-06-04 13:59:24 | [train_policy] epoch #867 | Fitting baseline...
2021-06-04 13:59:24 | [train_policy] epoch #867 | Saving snapshot...
2021-06-04 13:59:24 | [train_policy] epoch #867 | Saved
2021-06-04 13:59:24 | [train_policy] epoch #867 | Time 695.84 s
2021-06-04 13:59:24 | [train_policy] epoch #867 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286381
Evaluation/AverageDiscountedReturn          -39.6644
Evaluation/AverageReturn                    -39.6644
Evaluation/CompletionRate                     0
Evaluation/Iteration                        867
Evaluation/MaxReturn                        -28.163
Evaluation/MinReturn                        -62.9031
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.06071
Extras/EpisodeRewardMean                    -39.6476
LinearFeatureBaseline/ExplainedVariance       0.894659
PolicyExecTime                                0.2276
ProcessExecTime                               0.0313239
TotalEnvSteps                            878416
policy/Entropy                               -1.86471
policy/KL                                     0.00670445
policy/KLBefore                               0
policy/LossAfter                             -0.0135191
policy/LossBefore                             1.11906e-08
policy/Perplexity                             0.154941
policy/dLoss                                  0.0135191
---------------------------------------  ----------------
2021-06-04 13:59:24 | [train_policy] epoch #868 | Obtaining samples for iteration 868...
2021-06-04 13:59:24 | [train_policy] epoch #868 | Logging diagnostics...
2021-06-04 13:59:24 | [train_policy] epoch #868 | Optimizing policy...
2021-06-04 13:59:24 | [train_policy] epoch #868 | Computing loss before
2021-06-04 13:59:24 | [train_policy] epoch #868 | Computing KL before
2021-06-04 13:59:24 | [train_policy] epoch #868 | Optimizing
2021-06-04 13:59:24 | [train_policy] epoch #868 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:24 | [train_policy] epoch #868 | computing loss before
2021-06-04 13:59:24 | [train_policy] epoch #868 | computing gradient
2021-06-04 13:59:24 | [train_policy] epoch #868 | gradient computed
2021-06-04 13:59:24 | [train_policy] epoch #868 | computing descent direction
2021-06-04 13:59:24 | [train_policy] epoch #868 | descent direction computed
2021-06-04 13:59:24 | [train_policy] epoch #868 | backtrack iters: 1
2021-06-04 13:59:24 | [train_policy] epoch #868 | optimization finished
2021-06-04 13:59:24 | [train_policy] epoch #868 | Computing KL after
2021-06-04 13:59:24 | [train_policy] epoch #868 | Computing loss after
2021-06-04 13:59:24 | [train_policy] epoch #868 | Fitting baseline...
2021-06-04 13:59:24 | [train_policy] epoch #868 | Saving snapshot...
2021-06-04 13:59:24 | [train_policy] epoch #868 | Saved
2021-06-04 13:59:24 | [train_policy] epoch #868 | Time 696.65 s
2021-06-04 13:59:24 | [train_policy] epoch #868 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284457
Evaluation/AverageDiscountedReturn          -40.5227
Evaluation/AverageReturn                    -40.5227
Evaluation/CompletionRate                     0
Evaluation/Iteration                        868
Evaluation/MaxReturn                        -28.8673
Evaluation/MinReturn                        -56.4103
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.03742
Extras/EpisodeRewardMean                    -40.6499
LinearFeatureBaseline/ExplainedVariance       0.893074
PolicyExecTime                                0.22735
ProcessExecTime                               0.031116
TotalEnvSteps                            879428
policy/Entropy                               -1.92262
policy/KL                                     0.00669159
policy/KLBefore                               0
policy/LossAfter                             -0.014229
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.146223
policy/dLoss                                  0.014229
---------------------------------------  ----------------
2021-06-04 13:59:24 | [train_policy] epoch #869 | Obtaining samples for iteration 869...
2021-06-04 13:59:25 | [train_policy] epoch #869 | Logging diagnostics...
2021-06-04 13:59:25 | [train_policy] epoch #869 | Optimizing policy...
2021-06-04 13:59:25 | [train_policy] epoch #869 | Computing loss before
2021-06-04 13:59:25 | [train_policy] epoch #869 | Computing KL before
2021-06-04 13:59:25 | [train_policy] epoch #869 | Optimizing
2021-06-04 13:59:25 | [train_policy] epoch #869 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:25 | [train_policy] epoch #869 | computing loss before
2021-06-04 13:59:25 | [train_policy] epoch #869 | computing gradient
2021-06-04 13:59:25 | [train_policy] epoch #869 | gradient computed
2021-06-04 13:59:25 | [train_policy] epoch #869 | computing descent direction
2021-06-04 13:59:25 | [train_policy] epoch #869 | descent direction computed
2021-06-04 13:59:25 | [train_policy] epoch #869 | backtrack iters: 1
2021-06-04 13:59:25 | [train_policy] epoch #869 | optimization finished
2021-06-04 13:59:25 | [train_policy] epoch #869 | Computing KL after
2021-06-04 13:59:25 | [train_policy] epoch #869 | Computing loss after
2021-06-04 13:59:25 | [train_policy] epoch #869 | Fitting baseline...
2021-06-04 13:59:25 | [train_policy] epoch #869 | Saving snapshot...
2021-06-04 13:59:25 | [train_policy] epoch #869 | Saved
2021-06-04 13:59:25 | [train_policy] epoch #869 | Time 697.46 s
2021-06-04 13:59:25 | [train_policy] epoch #869 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285004
Evaluation/AverageDiscountedReturn          -41.1337
Evaluation/AverageReturn                    -41.1337
Evaluation/CompletionRate                     0
Evaluation/Iteration                        869
Evaluation/MaxReturn                        -28.4877
Evaluation/MinReturn                        -63.9335
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12828
Extras/EpisodeRewardMean                    -41.177
LinearFeatureBaseline/ExplainedVariance       0.901921
PolicyExecTime                                0.229941
ProcessExecTime                               0.0312436
TotalEnvSteps                            880440
policy/Entropy                               -1.93408
policy/KL                                     0.00681098
policy/KLBefore                               0
policy/LossAfter                             -0.0173711
policy/LossBefore                            -1.06016e-08
policy/Perplexity                             0.144557
policy/dLoss                                  0.0173711
---------------------------------------  ----------------
2021-06-04 13:59:25 | [train_policy] epoch #870 | Obtaining samples for iteration 870...
2021-06-04 13:59:26 | [train_policy] epoch #870 | Logging diagnostics...
2021-06-04 13:59:26 | [train_policy] epoch #870 | Optimizing policy...
2021-06-04 13:59:26 | [train_policy] epoch #870 | Computing loss before
2021-06-04 13:59:26 | [train_policy] epoch #870 | Computing KL before
2021-06-04 13:59:26 | [train_policy] epoch #870 | Optimizing
2021-06-04 13:59:26 | [train_policy] epoch #870 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:26 | [train_policy] epoch #870 | computing loss before
2021-06-04 13:59:26 | [train_policy] epoch #870 | computing gradient
2021-06-04 13:59:26 | [train_policy] epoch #870 | gradient computed
2021-06-04 13:59:26 | [train_policy] epoch #870 | computing descent direction
2021-06-04 13:59:26 | [train_policy] epoch #870 | descent direction computed
2021-06-04 13:59:26 | [train_policy] epoch #870 | backtrack iters: 1
2021-06-04 13:59:26 | [train_policy] epoch #870 | optimization finished
2021-06-04 13:59:26 | [train_policy] epoch #870 | Computing KL after
2021-06-04 13:59:26 | [train_policy] epoch #870 | Computing loss after
2021-06-04 13:59:26 | [train_policy] epoch #870 | Fitting baseline...
2021-06-04 13:59:26 | [train_policy] epoch #870 | Saving snapshot...
2021-06-04 13:59:26 | [train_policy] epoch #870 | Saved
2021-06-04 13:59:26 | [train_policy] epoch #870 | Time 698.25 s
2021-06-04 13:59:26 | [train_policy] epoch #870 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285059
Evaluation/AverageDiscountedReturn          -42.1967
Evaluation/AverageReturn                    -42.1967
Evaluation/CompletionRate                     0
Evaluation/Iteration                        870
Evaluation/MaxReturn                        -28.7351
Evaluation/MinReturn                        -82.6569
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.19701
Extras/EpisodeRewardMean                    -41.8425
LinearFeatureBaseline/ExplainedVariance       0.869162
PolicyExecTime                                0.218071
ProcessExecTime                               0.0311623
TotalEnvSteps                            881452
policy/Entropy                               -1.96465
policy/KL                                     0.00660122
policy/KLBefore                               0
policy/LossAfter                             -0.0179981
policy/LossBefore                            -9.65925e-09
policy/Perplexity                             0.140205
policy/dLoss                                  0.0179981
---------------------------------------  ----------------
2021-06-04 13:59:26 | [train_policy] epoch #871 | Obtaining samples for iteration 871...
2021-06-04 13:59:27 | [train_policy] epoch #871 | Logging diagnostics...
2021-06-04 13:59:27 | [train_policy] epoch #871 | Optimizing policy...
2021-06-04 13:59:27 | [train_policy] epoch #871 | Computing loss before
2021-06-04 13:59:27 | [train_policy] epoch #871 | Computing KL before
2021-06-04 13:59:27 | [train_policy] epoch #871 | Optimizing
2021-06-04 13:59:27 | [train_policy] epoch #871 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:27 | [train_policy] epoch #871 | computing loss before
2021-06-04 13:59:27 | [train_policy] epoch #871 | computing gradient
2021-06-04 13:59:27 | [train_policy] epoch #871 | gradient computed
2021-06-04 13:59:27 | [train_policy] epoch #871 | computing descent direction
2021-06-04 13:59:27 | [train_policy] epoch #871 | descent direction computed
2021-06-04 13:59:27 | [train_policy] epoch #871 | backtrack iters: 1
2021-06-04 13:59:27 | [train_policy] epoch #871 | optimization finished
2021-06-04 13:59:27 | [train_policy] epoch #871 | Computing KL after
2021-06-04 13:59:27 | [train_policy] epoch #871 | Computing loss after
2021-06-04 13:59:27 | [train_policy] epoch #871 | Fitting baseline...
2021-06-04 13:59:27 | [train_policy] epoch #871 | Saving snapshot...
2021-06-04 13:59:27 | [train_policy] epoch #871 | Saved
2021-06-04 13:59:27 | [train_policy] epoch #871 | Time 699.05 s
2021-06-04 13:59:27 | [train_policy] epoch #871 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285208
Evaluation/AverageDiscountedReturn          -42.1882
Evaluation/AverageReturn                    -42.1882
Evaluation/CompletionRate                     0
Evaluation/Iteration                        871
Evaluation/MaxReturn                        -29.0197
Evaluation/MinReturn                        -90.4278
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.7466
Extras/EpisodeRewardMean                    -41.9842
LinearFeatureBaseline/ExplainedVariance       0.806804
PolicyExecTime                                0.222379
ProcessExecTime                               0.0311396
TotalEnvSteps                            882464
policy/Entropy                               -1.97448
policy/KL                                     0.00654563
policy/KLBefore                               0
policy/LossAfter                             -0.0237534
policy/LossBefore                             4.59403e-09
policy/Perplexity                             0.138834
policy/dLoss                                  0.0237534
---------------------------------------  ----------------
2021-06-04 13:59:27 | [train_policy] epoch #872 | Obtaining samples for iteration 872...
2021-06-04 13:59:27 | [train_policy] epoch #872 | Logging diagnostics...
2021-06-04 13:59:27 | [train_policy] epoch #872 | Optimizing policy...
2021-06-04 13:59:27 | [train_policy] epoch #872 | Computing loss before
2021-06-04 13:59:27 | [train_policy] epoch #872 | Computing KL before
2021-06-04 13:59:27 | [train_policy] epoch #872 | Optimizing
2021-06-04 13:59:27 | [train_policy] epoch #872 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:27 | [train_policy] epoch #872 | computing loss before
2021-06-04 13:59:27 | [train_policy] epoch #872 | computing gradient
2021-06-04 13:59:27 | [train_policy] epoch #872 | gradient computed
2021-06-04 13:59:28 | [train_policy] epoch #872 | computing descent direction
2021-06-04 13:59:28 | [train_policy] epoch #872 | descent direction computed
2021-06-04 13:59:28 | [train_policy] epoch #872 | backtrack iters: 1
2021-06-04 13:59:28 | [train_policy] epoch #872 | optimization finished
2021-06-04 13:59:28 | [train_policy] epoch #872 | Computing KL after
2021-06-04 13:59:28 | [train_policy] epoch #872 | Computing loss after
2021-06-04 13:59:28 | [train_policy] epoch #872 | Fitting baseline...
2021-06-04 13:59:28 | [train_policy] epoch #872 | Saving snapshot...
2021-06-04 13:59:28 | [train_policy] epoch #872 | Saved
2021-06-04 13:59:28 | [train_policy] epoch #872 | Time 699.86 s
2021-06-04 13:59:28 | [train_policy] epoch #872 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286639
Evaluation/AverageDiscountedReturn          -41.6087
Evaluation/AverageReturn                    -41.6087
Evaluation/CompletionRate                     0
Evaluation/Iteration                        872
Evaluation/MaxReturn                        -30.0545
Evaluation/MinReturn                        -91.2622
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.07452
Extras/EpisodeRewardMean                    -41.3387
LinearFeatureBaseline/ExplainedVariance       0.843433
PolicyExecTime                                0.228981
ProcessExecTime                               0.0314157
TotalEnvSteps                            883476
policy/Entropy                               -2.00649
policy/KL                                     0.0066539
policy/KLBefore                               0
policy/LossAfter                             -0.0146881
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.134459
policy/dLoss                                  0.0146881
---------------------------------------  ----------------
2021-06-04 13:59:28 | [train_policy] epoch #873 | Obtaining samples for iteration 873...
2021-06-04 13:59:28 | [train_policy] epoch #873 | Logging diagnostics...
2021-06-04 13:59:28 | [train_policy] epoch #873 | Optimizing policy...
2021-06-04 13:59:28 | [train_policy] epoch #873 | Computing loss before
2021-06-04 13:59:28 | [train_policy] epoch #873 | Computing KL before
2021-06-04 13:59:28 | [train_policy] epoch #873 | Optimizing
2021-06-04 13:59:28 | [train_policy] epoch #873 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:28 | [train_policy] epoch #873 | computing loss before
2021-06-04 13:59:28 | [train_policy] epoch #873 | computing gradient
2021-06-04 13:59:28 | [train_policy] epoch #873 | gradient computed
2021-06-04 13:59:28 | [train_policy] epoch #873 | computing descent direction
2021-06-04 13:59:28 | [train_policy] epoch #873 | descent direction computed
2021-06-04 13:59:28 | [train_policy] epoch #873 | backtrack iters: 0
2021-06-04 13:59:28 | [train_policy] epoch #873 | optimization finished
2021-06-04 13:59:28 | [train_policy] epoch #873 | Computing KL after
2021-06-04 13:59:28 | [train_policy] epoch #873 | Computing loss after
2021-06-04 13:59:28 | [train_policy] epoch #873 | Fitting baseline...
2021-06-04 13:59:28 | [train_policy] epoch #873 | Saving snapshot...
2021-06-04 13:59:28 | [train_policy] epoch #873 | Saved
2021-06-04 13:59:28 | [train_policy] epoch #873 | Time 700.65 s
2021-06-04 13:59:28 | [train_policy] epoch #873 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284643
Evaluation/AverageDiscountedReturn          -41.8096
Evaluation/AverageReturn                    -41.8096
Evaluation/CompletionRate                     0
Evaluation/Iteration                        873
Evaluation/MaxReturn                        -30.3116
Evaluation/MinReturn                        -80.2999
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.98659
Extras/EpisodeRewardMean                    -41.8972
LinearFeatureBaseline/ExplainedVariance       0.852101
PolicyExecTime                                0.214829
ProcessExecTime                               0.0311687
TotalEnvSteps                            884488
policy/Entropy                               -2.0108
policy/KL                                     0.00993232
policy/KLBefore                               0
policy/LossAfter                             -0.0224912
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.133882
policy/dLoss                                  0.0224913
---------------------------------------  ----------------
2021-06-04 13:59:28 | [train_policy] epoch #874 | Obtaining samples for iteration 874...
2021-06-04 13:59:29 | [train_policy] epoch #874 | Logging diagnostics...
2021-06-04 13:59:29 | [train_policy] epoch #874 | Optimizing policy...
2021-06-04 13:59:29 | [train_policy] epoch #874 | Computing loss before
2021-06-04 13:59:29 | [train_policy] epoch #874 | Computing KL before
2021-06-04 13:59:29 | [train_policy] epoch #874 | Optimizing
2021-06-04 13:59:29 | [train_policy] epoch #874 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:29 | [train_policy] epoch #874 | computing loss before
2021-06-04 13:59:29 | [train_policy] epoch #874 | computing gradient
2021-06-04 13:59:29 | [train_policy] epoch #874 | gradient computed
2021-06-04 13:59:29 | [train_policy] epoch #874 | computing descent direction
2021-06-04 13:59:29 | [train_policy] epoch #874 | descent direction computed
2021-06-04 13:59:29 | [train_policy] epoch #874 | backtrack iters: 1
2021-06-04 13:59:29 | [train_policy] epoch #874 | optimization finished
2021-06-04 13:59:29 | [train_policy] epoch #874 | Computing KL after
2021-06-04 13:59:29 | [train_policy] epoch #874 | Computing loss after
2021-06-04 13:59:29 | [train_policy] epoch #874 | Fitting baseline...
2021-06-04 13:59:29 | [train_policy] epoch #874 | Saving snapshot...
2021-06-04 13:59:29 | [train_policy] epoch #874 | Saved
2021-06-04 13:59:29 | [train_policy] epoch #874 | Time 701.47 s
2021-06-04 13:59:29 | [train_policy] epoch #874 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.285517
Evaluation/AverageDiscountedReturn          -40.7242
Evaluation/AverageReturn                    -40.7242
Evaluation/CompletionRate                     0
Evaluation/Iteration                        874
Evaluation/MaxReturn                        -29.1091
Evaluation/MinReturn                        -60.4121
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83893
Extras/EpisodeRewardMean                    -40.7455
LinearFeatureBaseline/ExplainedVariance       0.882915
PolicyExecTime                                0.238238
ProcessExecTime                               0.0311799
TotalEnvSteps                            885500
policy/Entropy                               -2.04271
policy/KL                                     0.00666965
policy/KLBefore                               0
policy/LossAfter                             -0.0136551
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.129677
policy/dLoss                                  0.0136551
---------------------------------------  ----------------
2021-06-04 13:59:29 | [train_policy] epoch #875 | Obtaining samples for iteration 875...
2021-06-04 13:59:30 | [train_policy] epoch #875 | Logging diagnostics...
2021-06-04 13:59:30 | [train_policy] epoch #875 | Optimizing policy...
2021-06-04 13:59:30 | [train_policy] epoch #875 | Computing loss before
2021-06-04 13:59:30 | [train_policy] epoch #875 | Computing KL before
2021-06-04 13:59:30 | [train_policy] epoch #875 | Optimizing
2021-06-04 13:59:30 | [train_policy] epoch #875 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:30 | [train_policy] epoch #875 | computing loss before
2021-06-04 13:59:30 | [train_policy] epoch #875 | computing gradient
2021-06-04 13:59:30 | [train_policy] epoch #875 | gradient computed
2021-06-04 13:59:30 | [train_policy] epoch #875 | computing descent direction
2021-06-04 13:59:30 | [train_policy] epoch #875 | descent direction computed
2021-06-04 13:59:30 | [train_policy] epoch #875 | backtrack iters: 1
2021-06-04 13:59:30 | [train_policy] epoch #875 | optimization finished
2021-06-04 13:59:30 | [train_policy] epoch #875 | Computing KL after
2021-06-04 13:59:30 | [train_policy] epoch #875 | Computing loss after
2021-06-04 13:59:30 | [train_policy] epoch #875 | Fitting baseline...
2021-06-04 13:59:30 | [train_policy] epoch #875 | Saving snapshot...
2021-06-04 13:59:30 | [train_policy] epoch #875 | Saved
2021-06-04 13:59:30 | [train_policy] epoch #875 | Time 702.28 s
2021-06-04 13:59:30 | [train_policy] epoch #875 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28424
Evaluation/AverageDiscountedReturn          -41.2218
Evaluation/AverageReturn                    -41.2218
Evaluation/CompletionRate                     0
Evaluation/Iteration                        875
Evaluation/MaxReturn                        -28.103
Evaluation/MinReturn                        -63.9151
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.19399
Extras/EpisodeRewardMean                    -40.9041
LinearFeatureBaseline/ExplainedVariance       0.790307
PolicyExecTime                                0.221318
ProcessExecTime                               0.031261
TotalEnvSteps                            886512
policy/Entropy                               -2.04485
policy/KL                                     0.0064008
policy/KLBefore                               0
policy/LossAfter                             -0.0192816
policy/LossBefore                            -3.18048e-09
policy/Perplexity                             0.1294
policy/dLoss                                  0.0192816
---------------------------------------  ----------------
2021-06-04 13:59:30 | [train_policy] epoch #876 | Obtaining samples for iteration 876...
2021-06-04 13:59:31 | [train_policy] epoch #876 | Logging diagnostics...
2021-06-04 13:59:31 | [train_policy] epoch #876 | Optimizing policy...
2021-06-04 13:59:31 | [train_policy] epoch #876 | Computing loss before
2021-06-04 13:59:31 | [train_policy] epoch #876 | Computing KL before
2021-06-04 13:59:31 | [train_policy] epoch #876 | Optimizing
2021-06-04 13:59:31 | [train_policy] epoch #876 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:31 | [train_policy] epoch #876 | computing loss before
2021-06-04 13:59:31 | [train_policy] epoch #876 | computing gradient
2021-06-04 13:59:31 | [train_policy] epoch #876 | gradient computed
2021-06-04 13:59:31 | [train_policy] epoch #876 | computing descent direction
2021-06-04 13:59:31 | [train_policy] epoch #876 | descent direction computed
2021-06-04 13:59:31 | [train_policy] epoch #876 | backtrack iters: 1
2021-06-04 13:59:31 | [train_policy] epoch #876 | optimization finished
2021-06-04 13:59:31 | [train_policy] epoch #876 | Computing KL after
2021-06-04 13:59:31 | [train_policy] epoch #876 | Computing loss after
2021-06-04 13:59:31 | [train_policy] epoch #876 | Fitting baseline...
2021-06-04 13:59:31 | [train_policy] epoch #876 | Saving snapshot...
2021-06-04 13:59:31 | [train_policy] epoch #876 | Saved
2021-06-04 13:59:31 | [train_policy] epoch #876 | Time 703.07 s
2021-06-04 13:59:31 | [train_policy] epoch #876 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.28416
Evaluation/AverageDiscountedReturn          -41.6577
Evaluation/AverageReturn                    -41.6577
Evaluation/CompletionRate                     0
Evaluation/Iteration                        876
Evaluation/MaxReturn                        -29.584
Evaluation/MinReturn                        -79.7336
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.87621
Extras/EpisodeRewardMean                    -41.5923
LinearFeatureBaseline/ExplainedVariance       0.858961
PolicyExecTime                                0.213175
ProcessExecTime                               0.031122
TotalEnvSteps                            887524
policy/Entropy                               -2.06178
policy/KL                                     0.00656667
policy/KLBefore                               0
policy/LossAfter                             -0.0120081
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.127227
policy/dLoss                                  0.0120081
---------------------------------------  ---------------
2021-06-04 13:59:31 | [train_policy] epoch #877 | Obtaining samples for iteration 877...
2021-06-04 13:59:32 | [train_policy] epoch #877 | Logging diagnostics...
2021-06-04 13:59:32 | [train_policy] epoch #877 | Optimizing policy...
2021-06-04 13:59:32 | [train_policy] epoch #877 | Computing loss before
2021-06-04 13:59:32 | [train_policy] epoch #877 | Computing KL before
2021-06-04 13:59:32 | [train_policy] epoch #877 | Optimizing
2021-06-04 13:59:32 | [train_policy] epoch #877 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:32 | [train_policy] epoch #877 | computing loss before
2021-06-04 13:59:32 | [train_policy] epoch #877 | computing gradient
2021-06-04 13:59:32 | [train_policy] epoch #877 | gradient computed
2021-06-04 13:59:32 | [train_policy] epoch #877 | computing descent direction
2021-06-04 13:59:32 | [train_policy] epoch #877 | descent direction computed
2021-06-04 13:59:32 | [train_policy] epoch #877 | backtrack iters: 0
2021-06-04 13:59:32 | [train_policy] epoch #877 | optimization finished
2021-06-04 13:59:32 | [train_policy] epoch #877 | Computing KL after
2021-06-04 13:59:32 | [train_policy] epoch #877 | Computing loss after
2021-06-04 13:59:32 | [train_policy] epoch #877 | Fitting baseline...
2021-06-04 13:59:32 | [train_policy] epoch #877 | Saving snapshot...
2021-06-04 13:59:32 | [train_policy] epoch #877 | Saved
2021-06-04 13:59:32 | [train_policy] epoch #877 | Time 703.88 s
2021-06-04 13:59:32 | [train_policy] epoch #877 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.287466
Evaluation/AverageDiscountedReturn          -40.9395
Evaluation/AverageReturn                    -40.9395
Evaluation/CompletionRate                     0
Evaluation/Iteration                        877
Evaluation/MaxReturn                        -31.3847
Evaluation/MinReturn                        -79.051
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.48548
Extras/EpisodeRewardMean                    -40.7783
LinearFeatureBaseline/ExplainedVariance       0.866816
PolicyExecTime                                0.229961
ProcessExecTime                               0.0315201
TotalEnvSteps                            888536
policy/Entropy                               -2.06034
policy/KL                                     0.00994443
policy/KLBefore                               0
policy/LossAfter                             -0.0241047
policy/LossBefore                             1.77872e-08
policy/Perplexity                             0.127411
policy/dLoss                                  0.0241048
---------------------------------------  ----------------
2021-06-04 13:59:32 | [train_policy] epoch #878 | Obtaining samples for iteration 878...
2021-06-04 13:59:32 | [train_policy] epoch #878 | Logging diagnostics...
2021-06-04 13:59:32 | [train_policy] epoch #878 | Optimizing policy...
2021-06-04 13:59:32 | [train_policy] epoch #878 | Computing loss before
2021-06-04 13:59:32 | [train_policy] epoch #878 | Computing KL before
2021-06-04 13:59:32 | [train_policy] epoch #878 | Optimizing
2021-06-04 13:59:32 | [train_policy] epoch #878 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:32 | [train_policy] epoch #878 | computing loss before
2021-06-04 13:59:32 | [train_policy] epoch #878 | computing gradient
2021-06-04 13:59:32 | [train_policy] epoch #878 | gradient computed
2021-06-04 13:59:32 | [train_policy] epoch #878 | computing descent direction
2021-06-04 13:59:32 | [train_policy] epoch #878 | descent direction computed
2021-06-04 13:59:32 | [train_policy] epoch #878 | backtrack iters: 1
2021-06-04 13:59:32 | [train_policy] epoch #878 | optimization finished
2021-06-04 13:59:32 | [train_policy] epoch #878 | Computing KL after
2021-06-04 13:59:32 | [train_policy] epoch #878 | Computing loss after
2021-06-04 13:59:32 | [train_policy] epoch #878 | Fitting baseline...
2021-06-04 13:59:32 | [train_policy] epoch #878 | Saving snapshot...
2021-06-04 13:59:32 | [train_policy] epoch #878 | Saved
2021-06-04 13:59:32 | [train_policy] epoch #878 | Time 704.67 s
2021-06-04 13:59:32 | [train_policy] epoch #878 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284552
Evaluation/AverageDiscountedReturn          -40.1581
Evaluation/AverageReturn                    -40.1581
Evaluation/CompletionRate                     0
Evaluation/Iteration                        878
Evaluation/MaxReturn                        -28.6111
Evaluation/MinReturn                        -57.1089
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.0135
Extras/EpisodeRewardMean                    -40.3678
LinearFeatureBaseline/ExplainedVariance       0.870455
PolicyExecTime                                0.223709
ProcessExecTime                               0.0311568
TotalEnvSteps                            889548
policy/Entropy                               -2.08906
policy/KL                                     0.00661551
policy/KLBefore                               0
policy/LossAfter                             -0.0115111
policy/LossBefore                            -1.18974e-08
policy/Perplexity                             0.123803
policy/dLoss                                  0.0115111
---------------------------------------  ----------------
2021-06-04 13:59:32 | [train_policy] epoch #879 | Obtaining samples for iteration 879...
2021-06-04 13:59:33 | [train_policy] epoch #879 | Logging diagnostics...
2021-06-04 13:59:33 | [train_policy] epoch #879 | Optimizing policy...
2021-06-04 13:59:33 | [train_policy] epoch #879 | Computing loss before
2021-06-04 13:59:33 | [train_policy] epoch #879 | Computing KL before
2021-06-04 13:59:33 | [train_policy] epoch #879 | Optimizing
2021-06-04 13:59:33 | [train_policy] epoch #879 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:33 | [train_policy] epoch #879 | computing loss before
2021-06-04 13:59:33 | [train_policy] epoch #879 | computing gradient
2021-06-04 13:59:33 | [train_policy] epoch #879 | gradient computed
2021-06-04 13:59:33 | [train_policy] epoch #879 | computing descent direction
2021-06-04 13:59:33 | [train_policy] epoch #879 | descent direction computed
2021-06-04 13:59:33 | [train_policy] epoch #879 | backtrack iters: 1
2021-06-04 13:59:33 | [train_policy] epoch #879 | optimization finished
2021-06-04 13:59:33 | [train_policy] epoch #879 | Computing KL after
2021-06-04 13:59:33 | [train_policy] epoch #879 | Computing loss after
2021-06-04 13:59:33 | [train_policy] epoch #879 | Fitting baseline...
2021-06-04 13:59:33 | [train_policy] epoch #879 | Saving snapshot...
2021-06-04 13:59:33 | [train_policy] epoch #879 | Saved
2021-06-04 13:59:33 | [train_policy] epoch #879 | Time 705.48 s
2021-06-04 13:59:33 | [train_policy] epoch #879 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284611
Evaluation/AverageDiscountedReturn          -40.9097
Evaluation/AverageReturn                    -40.9097
Evaluation/CompletionRate                     0
Evaluation/Iteration                        879
Evaluation/MaxReturn                        -28.4306
Evaluation/MinReturn                        -85.5512
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.05609
Extras/EpisodeRewardMean                    -40.8356
LinearFeatureBaseline/ExplainedVariance       0.845178
PolicyExecTime                                0.230991
ProcessExecTime                               0.0311213
TotalEnvSteps                            890560
policy/Entropy                               -2.09088
policy/KL                                     0.00652249
policy/KLBefore                               0
policy/LossAfter                             -0.0111684
policy/LossBefore                             8.83468e-09
policy/Perplexity                             0.123578
policy/dLoss                                  0.0111684
---------------------------------------  ----------------
2021-06-04 13:59:33 | [train_policy] epoch #880 | Obtaining samples for iteration 880...
2021-06-04 13:59:34 | [train_policy] epoch #880 | Logging diagnostics...
2021-06-04 13:59:34 | [train_policy] epoch #880 | Optimizing policy...
2021-06-04 13:59:34 | [train_policy] epoch #880 | Computing loss before
2021-06-04 13:59:34 | [train_policy] epoch #880 | Computing KL before
2021-06-04 13:59:34 | [train_policy] epoch #880 | Optimizing
2021-06-04 13:59:34 | [train_policy] epoch #880 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:34 | [train_policy] epoch #880 | computing loss before
2021-06-04 13:59:34 | [train_policy] epoch #880 | computing gradient
2021-06-04 13:59:34 | [train_policy] epoch #880 | gradient computed
2021-06-04 13:59:34 | [train_policy] epoch #880 | computing descent direction
2021-06-04 13:59:34 | [train_policy] epoch #880 | descent direction computed
2021-06-04 13:59:34 | [train_policy] epoch #880 | backtrack iters: 1
2021-06-04 13:59:34 | [train_policy] epoch #880 | optimization finished
2021-06-04 13:59:34 | [train_policy] epoch #880 | Computing KL after
2021-06-04 13:59:34 | [train_policy] epoch #880 | Computing loss after
2021-06-04 13:59:34 | [train_policy] epoch #880 | Fitting baseline...
2021-06-04 13:59:34 | [train_policy] epoch #880 | Saving snapshot...
2021-06-04 13:59:34 | [train_policy] epoch #880 | Saved
2021-06-04 13:59:34 | [train_policy] epoch #880 | Time 706.28 s
2021-06-04 13:59:34 | [train_policy] epoch #880 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284555
Evaluation/AverageDiscountedReturn          -39.7357
Evaluation/AverageReturn                    -39.7357
Evaluation/CompletionRate                     0
Evaluation/Iteration                        880
Evaluation/MaxReturn                        -28.6089
Evaluation/MinReturn                        -79.0788
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05327
Extras/EpisodeRewardMean                    -39.6418
LinearFeatureBaseline/ExplainedVariance       0.873751
PolicyExecTime                                0.22145
ProcessExecTime                               0.0312161
TotalEnvSteps                            891572
policy/Entropy                               -2.10727
policy/KL                                     0.00646624
policy/KLBefore                               0
policy/LossAfter                             -0.0216862
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.12157
policy/dLoss                                  0.0216862
---------------------------------------  ----------------
2021-06-04 13:59:34 | [train_policy] epoch #881 | Obtaining samples for iteration 881...
2021-06-04 13:59:35 | [train_policy] epoch #881 | Logging diagnostics...
2021-06-04 13:59:35 | [train_policy] epoch #881 | Optimizing policy...
2021-06-04 13:59:35 | [train_policy] epoch #881 | Computing loss before
2021-06-04 13:59:35 | [train_policy] epoch #881 | Computing KL before
2021-06-04 13:59:35 | [train_policy] epoch #881 | Optimizing
2021-06-04 13:59:35 | [train_policy] epoch #881 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:35 | [train_policy] epoch #881 | computing loss before
2021-06-04 13:59:35 | [train_policy] epoch #881 | computing gradient
2021-06-04 13:59:35 | [train_policy] epoch #881 | gradient computed
2021-06-04 13:59:35 | [train_policy] epoch #881 | computing descent direction
2021-06-04 13:59:35 | [train_policy] epoch #881 | descent direction computed
2021-06-04 13:59:35 | [train_policy] epoch #881 | backtrack iters: 1
2021-06-04 13:59:35 | [train_policy] epoch #881 | optimization finished
2021-06-04 13:59:35 | [train_policy] epoch #881 | Computing KL after
2021-06-04 13:59:35 | [train_policy] epoch #881 | Computing loss after
2021-06-04 13:59:35 | [train_policy] epoch #881 | Fitting baseline...
2021-06-04 13:59:35 | [train_policy] epoch #881 | Saving snapshot...
2021-06-04 13:59:35 | [train_policy] epoch #881 | Saved
2021-06-04 13:59:35 | [train_policy] epoch #881 | Time 707.10 s
2021-06-04 13:59:35 | [train_policy] epoch #881 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.284623
Evaluation/AverageDiscountedReturn          -40.497
Evaluation/AverageReturn                    -40.497
Evaluation/CompletionRate                     0
Evaluation/Iteration                        881
Evaluation/MaxReturn                        -29.3196
Evaluation/MinReturn                        -96.4183
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.05222
Extras/EpisodeRewardMean                    -40.7143
LinearFeatureBaseline/ExplainedVariance       0.827243
PolicyExecTime                                0.236243
ProcessExecTime                               0.0312371
TotalEnvSteps                            892584
policy/Entropy                               -2.10917
policy/KL                                     0.00644572
policy/KLBefore                               0
policy/LossAfter                             -0.0230835
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.121338
policy/dLoss                                  0.0230835
---------------------------------------  ----------------
2021-06-04 13:59:35 | [train_policy] epoch #882 | Obtaining samples for iteration 882...
2021-06-04 13:59:36 | [train_policy] epoch #882 | Logging diagnostics...
2021-06-04 13:59:36 | [train_policy] epoch #882 | Optimizing policy...
2021-06-04 13:59:36 | [train_policy] epoch #882 | Computing loss before
2021-06-04 13:59:36 | [train_policy] epoch #882 | Computing KL before
2021-06-04 13:59:36 | [train_policy] epoch #882 | Optimizing
2021-06-04 13:59:36 | [train_policy] epoch #882 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:36 | [train_policy] epoch #882 | computing loss before
2021-06-04 13:59:36 | [train_policy] epoch #882 | computing gradient
2021-06-04 13:59:36 | [train_policy] epoch #882 | gradient computed
2021-06-04 13:59:36 | [train_policy] epoch #882 | computing descent direction
2021-06-04 13:59:36 | [train_policy] epoch #882 | descent direction computed
2021-06-04 13:59:36 | [train_policy] epoch #882 | backtrack iters: 1
2021-06-04 13:59:36 | [train_policy] epoch #882 | optimization finished
2021-06-04 13:59:36 | [train_policy] epoch #882 | Computing KL after
2021-06-04 13:59:36 | [train_policy] epoch #882 | Computing loss after
2021-06-04 13:59:36 | [train_policy] epoch #882 | Fitting baseline...
2021-06-04 13:59:36 | [train_policy] epoch #882 | Saving snapshot...
2021-06-04 13:59:36 | [train_policy] epoch #882 | Saved
2021-06-04 13:59:36 | [train_policy] epoch #882 | Time 707.91 s
2021-06-04 13:59:36 | [train_policy] epoch #882 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.285875
Evaluation/AverageDiscountedReturn          -39.9396
Evaluation/AverageReturn                    -39.9396
Evaluation/CompletionRate                     0
Evaluation/Iteration                        882
Evaluation/MaxReturn                        -29.3234
Evaluation/MinReturn                        -93.4645
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.55547
Extras/EpisodeRewardMean                    -39.8942
LinearFeatureBaseline/ExplainedVariance       0.853196
PolicyExecTime                                0.226825
ProcessExecTime                               0.031373
TotalEnvSteps                            893596
policy/Entropy                               -2.12123
policy/KL                                     0.00650188
policy/KLBefore                               0
policy/LossAfter                             -0.0199223
policy/LossBefore                            -8.2457e-10
policy/Perplexity                             0.119884
policy/dLoss                                  0.0199223
---------------------------------------  ---------------
2021-06-04 13:59:36 | [train_policy] epoch #883 | Obtaining samples for iteration 883...
2021-06-04 13:59:36 | [train_policy] epoch #883 | Logging diagnostics...
2021-06-04 13:59:36 | [train_policy] epoch #883 | Optimizing policy...
2021-06-04 13:59:36 | [train_policy] epoch #883 | Computing loss before
2021-06-04 13:59:36 | [train_policy] epoch #883 | Computing KL before
2021-06-04 13:59:36 | [train_policy] epoch #883 | Optimizing
2021-06-04 13:59:36 | [train_policy] epoch #883 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:36 | [train_policy] epoch #883 | computing loss before
2021-06-04 13:59:36 | [train_policy] epoch #883 | computing gradient
2021-06-04 13:59:36 | [train_policy] epoch #883 | gradient computed
2021-06-04 13:59:36 | [train_policy] epoch #883 | computing descent direction
2021-06-04 13:59:36 | [train_policy] epoch #883 | descent direction computed
2021-06-04 13:59:36 | [train_policy] epoch #883 | backtrack iters: 1
2021-06-04 13:59:36 | [train_policy] epoch #883 | optimization finished
2021-06-04 13:59:36 | [train_policy] epoch #883 | Computing KL after
2021-06-04 13:59:36 | [train_policy] epoch #883 | Computing loss after
2021-06-04 13:59:36 | [train_policy] epoch #883 | Fitting baseline...
2021-06-04 13:59:36 | [train_policy] epoch #883 | Saving snapshot...
2021-06-04 13:59:36 | [train_policy] epoch #883 | Saved
2021-06-04 13:59:36 | [train_policy] epoch #883 | Time 708.69 s
2021-06-04 13:59:36 | [train_policy] epoch #883 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285222
Evaluation/AverageDiscountedReturn          -42.3491
Evaluation/AverageReturn                    -42.3491
Evaluation/CompletionRate                     0
Evaluation/Iteration                        883
Evaluation/MaxReturn                        -28.5434
Evaluation/MinReturn                        -89.6784
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.88956
Extras/EpisodeRewardMean                    -41.8783
LinearFeatureBaseline/ExplainedVariance       0.818659
PolicyExecTime                                0.210015
ProcessExecTime                               0.0312362
TotalEnvSteps                            894608
policy/Entropy                               -2.12616
policy/KL                                     0.00640929
policy/KLBefore                               0
policy/LossAfter                             -0.0192233
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             0.119295
policy/dLoss                                  0.0192233
---------------------------------------  ----------------
2021-06-04 13:59:36 | [train_policy] epoch #884 | Obtaining samples for iteration 884...
2021-06-04 13:59:37 | [train_policy] epoch #884 | Logging diagnostics...
2021-06-04 13:59:37 | [train_policy] epoch #884 | Optimizing policy...
2021-06-04 13:59:37 | [train_policy] epoch #884 | Computing loss before
2021-06-04 13:59:37 | [train_policy] epoch #884 | Computing KL before
2021-06-04 13:59:37 | [train_policy] epoch #884 | Optimizing
2021-06-04 13:59:37 | [train_policy] epoch #884 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:37 | [train_policy] epoch #884 | computing loss before
2021-06-04 13:59:37 | [train_policy] epoch #884 | computing gradient
2021-06-04 13:59:37 | [train_policy] epoch #884 | gradient computed
2021-06-04 13:59:37 | [train_policy] epoch #884 | computing descent direction
2021-06-04 13:59:37 | [train_policy] epoch #884 | descent direction computed
2021-06-04 13:59:37 | [train_policy] epoch #884 | backtrack iters: 0
2021-06-04 13:59:37 | [train_policy] epoch #884 | optimization finished
2021-06-04 13:59:37 | [train_policy] epoch #884 | Computing KL after
2021-06-04 13:59:37 | [train_policy] epoch #884 | Computing loss after
2021-06-04 13:59:37 | [train_policy] epoch #884 | Fitting baseline...
2021-06-04 13:59:37 | [train_policy] epoch #884 | Saving snapshot...
2021-06-04 13:59:37 | [train_policy] epoch #884 | Saved
2021-06-04 13:59:37 | [train_policy] epoch #884 | Time 709.49 s
2021-06-04 13:59:37 | [train_policy] epoch #884 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285058
Evaluation/AverageDiscountedReturn          -40.3328
Evaluation/AverageReturn                    -40.3328
Evaluation/CompletionRate                     0
Evaluation/Iteration                        884
Evaluation/MaxReturn                        -30.1418
Evaluation/MinReturn                        -81.3537
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.90714
Extras/EpisodeRewardMean                    -40.2398
LinearFeatureBaseline/ExplainedVariance       0.872519
PolicyExecTime                                0.228191
ProcessExecTime                               0.0312667
TotalEnvSteps                            895620
policy/Entropy                               -2.09972
policy/KL                                     0.00992037
policy/KLBefore                               0
policy/LossAfter                             -0.014391
policy/LossBefore                            -1.46067e-08
policy/Perplexity                             0.122491
policy/dLoss                                  0.014391
---------------------------------------  ----------------
2021-06-04 13:59:37 | [train_policy] epoch #885 | Obtaining samples for iteration 885...
2021-06-04 13:59:38 | [train_policy] epoch #885 | Logging diagnostics...
2021-06-04 13:59:38 | [train_policy] epoch #885 | Optimizing policy...
2021-06-04 13:59:38 | [train_policy] epoch #885 | Computing loss before
2021-06-04 13:59:38 | [train_policy] epoch #885 | Computing KL before
2021-06-04 13:59:38 | [train_policy] epoch #885 | Optimizing
2021-06-04 13:59:38 | [train_policy] epoch #885 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:38 | [train_policy] epoch #885 | computing loss before
2021-06-04 13:59:38 | [train_policy] epoch #885 | computing gradient
2021-06-04 13:59:38 | [train_policy] epoch #885 | gradient computed
2021-06-04 13:59:38 | [train_policy] epoch #885 | computing descent direction
2021-06-04 13:59:38 | [train_policy] epoch #885 | descent direction computed
2021-06-04 13:59:38 | [train_policy] epoch #885 | backtrack iters: 1
2021-06-04 13:59:38 | [train_policy] epoch #885 | optimization finished
2021-06-04 13:59:38 | [train_policy] epoch #885 | Computing KL after
2021-06-04 13:59:38 | [train_policy] epoch #885 | Computing loss after
2021-06-04 13:59:38 | [train_policy] epoch #885 | Fitting baseline...
2021-06-04 13:59:38 | [train_policy] epoch #885 | Saving snapshot...
2021-06-04 13:59:38 | [train_policy] epoch #885 | Saved
2021-06-04 13:59:38 | [train_policy] epoch #885 | Time 710.28 s
2021-06-04 13:59:38 | [train_policy] epoch #885 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284608
Evaluation/AverageDiscountedReturn          -41.461
Evaluation/AverageReturn                    -41.461
Evaluation/CompletionRate                     0
Evaluation/Iteration                        885
Evaluation/MaxReturn                        -30.0159
Evaluation/MinReturn                        -77.9218
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.33528
Extras/EpisodeRewardMean                    -41.3932
LinearFeatureBaseline/ExplainedVariance       0.878593
PolicyExecTime                                0.223355
ProcessExecTime                               0.0312772
TotalEnvSteps                            896632
policy/Entropy                               -2.09747
policy/KL                                     0.00653446
policy/KLBefore                               0
policy/LossAfter                             -0.0217299
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.122767
policy/dLoss                                  0.0217299
---------------------------------------  ----------------
2021-06-04 13:59:38 | [train_policy] epoch #886 | Obtaining samples for iteration 886...
2021-06-04 13:59:39 | [train_policy] epoch #886 | Logging diagnostics...
2021-06-04 13:59:39 | [train_policy] epoch #886 | Optimizing policy...
2021-06-04 13:59:39 | [train_policy] epoch #886 | Computing loss before
2021-06-04 13:59:39 | [train_policy] epoch #886 | Computing KL before
2021-06-04 13:59:39 | [train_policy] epoch #886 | Optimizing
2021-06-04 13:59:39 | [train_policy] epoch #886 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:39 | [train_policy] epoch #886 | computing loss before
2021-06-04 13:59:39 | [train_policy] epoch #886 | computing gradient
2021-06-04 13:59:39 | [train_policy] epoch #886 | gradient computed
2021-06-04 13:59:39 | [train_policy] epoch #886 | computing descent direction
2021-06-04 13:59:39 | [train_policy] epoch #886 | descent direction computed
2021-06-04 13:59:39 | [train_policy] epoch #886 | backtrack iters: 1
2021-06-04 13:59:39 | [train_policy] epoch #886 | optimization finished
2021-06-04 13:59:39 | [train_policy] epoch #886 | Computing KL after
2021-06-04 13:59:39 | [train_policy] epoch #886 | Computing loss after
2021-06-04 13:59:39 | [train_policy] epoch #886 | Fitting baseline...
2021-06-04 13:59:39 | [train_policy] epoch #886 | Saving snapshot...
2021-06-04 13:59:39 | [train_policy] epoch #886 | Saved
2021-06-04 13:59:39 | [train_policy] epoch #886 | Time 711.10 s
2021-06-04 13:59:39 | [train_policy] epoch #886 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.285384
Evaluation/AverageDiscountedReturn          -41.4514
Evaluation/AverageReturn                    -41.4514
Evaluation/CompletionRate                     0
Evaluation/Iteration                        886
Evaluation/MaxReturn                        -29.918
Evaluation/MinReturn                        -79.7495
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.69489
Extras/EpisodeRewardMean                    -41.455
LinearFeatureBaseline/ExplainedVariance       0.823715
PolicyExecTime                                0.2322
ProcessExecTime                               0.0311415
TotalEnvSteps                            897644
policy/Entropy                               -2.13285
policy/KL                                     0.00657029
policy/KLBefore                               0
policy/LossAfter                             -0.01752
policy/LossBefore                             1.7905e-08
policy/Perplexity                             0.118499
policy/dLoss                                  0.01752
---------------------------------------  ---------------
2021-06-04 13:59:39 | [train_policy] epoch #887 | Obtaining samples for iteration 887...
2021-06-04 13:59:40 | [train_policy] epoch #887 | Logging diagnostics...
2021-06-04 13:59:40 | [train_policy] epoch #887 | Optimizing policy...
2021-06-04 13:59:40 | [train_policy] epoch #887 | Computing loss before
2021-06-04 13:59:40 | [train_policy] epoch #887 | Computing KL before
2021-06-04 13:59:40 | [train_policy] epoch #887 | Optimizing
2021-06-04 13:59:40 | [train_policy] epoch #887 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:40 | [train_policy] epoch #887 | computing loss before
2021-06-04 13:59:40 | [train_policy] epoch #887 | computing gradient
2021-06-04 13:59:40 | [train_policy] epoch #887 | gradient computed
2021-06-04 13:59:40 | [train_policy] epoch #887 | computing descent direction
2021-06-04 13:59:40 | [train_policy] epoch #887 | descent direction computed
2021-06-04 13:59:40 | [train_policy] epoch #887 | backtrack iters: 1
2021-06-04 13:59:40 | [train_policy] epoch #887 | optimization finished
2021-06-04 13:59:40 | [train_policy] epoch #887 | Computing KL after
2021-06-04 13:59:40 | [train_policy] epoch #887 | Computing loss after
2021-06-04 13:59:40 | [train_policy] epoch #887 | Fitting baseline...
2021-06-04 13:59:40 | [train_policy] epoch #887 | Saving snapshot...
2021-06-04 13:59:40 | [train_policy] epoch #887 | Saved
2021-06-04 13:59:40 | [train_policy] epoch #887 | Time 711.89 s
2021-06-04 13:59:40 | [train_policy] epoch #887 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.287165
Evaluation/AverageDiscountedReturn          -40.709
Evaluation/AverageReturn                    -40.709
Evaluation/CompletionRate                     0
Evaluation/Iteration                        887
Evaluation/MaxReturn                        -29.4478
Evaluation/MinReturn                        -80.867
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.97939
Extras/EpisodeRewardMean                    -40.4446
LinearFeatureBaseline/ExplainedVariance       0.857825
PolicyExecTime                                0.224508
ProcessExecTime                               0.0314863
TotalEnvSteps                            898656
policy/Entropy                               -2.16142
policy/KL                                     0.00656346
policy/KLBefore                               0
policy/LossAfter                             -0.0152099
policy/LossBefore                            -7.30334e-09
policy/Perplexity                             0.115161
policy/dLoss                                  0.0152099
---------------------------------------  ----------------
2021-06-04 13:59:40 | [train_policy] epoch #888 | Obtaining samples for iteration 888...
2021-06-04 13:59:40 | [train_policy] epoch #888 | Logging diagnostics...
2021-06-04 13:59:40 | [train_policy] epoch #888 | Optimizing policy...
2021-06-04 13:59:40 | [train_policy] epoch #888 | Computing loss before
2021-06-04 13:59:40 | [train_policy] epoch #888 | Computing KL before
2021-06-04 13:59:40 | [train_policy] epoch #888 | Optimizing
2021-06-04 13:59:40 | [train_policy] epoch #888 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:40 | [train_policy] epoch #888 | computing loss before
2021-06-04 13:59:40 | [train_policy] epoch #888 | computing gradient
2021-06-04 13:59:40 | [train_policy] epoch #888 | gradient computed
2021-06-04 13:59:40 | [train_policy] epoch #888 | computing descent direction
2021-06-04 13:59:40 | [train_policy] epoch #888 | descent direction computed
2021-06-04 13:59:40 | [train_policy] epoch #888 | backtrack iters: 1
2021-06-04 13:59:40 | [train_policy] epoch #888 | optimization finished
2021-06-04 13:59:40 | [train_policy] epoch #888 | Computing KL after
2021-06-04 13:59:40 | [train_policy] epoch #888 | Computing loss after
2021-06-04 13:59:40 | [train_policy] epoch #888 | Fitting baseline...
2021-06-04 13:59:40 | [train_policy] epoch #888 | Saving snapshot...
2021-06-04 13:59:40 | [train_policy] epoch #888 | Saved
2021-06-04 13:59:40 | [train_policy] epoch #888 | Time 712.71 s
2021-06-04 13:59:40 | [train_policy] epoch #888 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.290666
Evaluation/AverageDiscountedReturn          -41.2411
Evaluation/AverageReturn                    -41.2411
Evaluation/CompletionRate                     0
Evaluation/Iteration                        888
Evaluation/MaxReturn                        -30.6295
Evaluation/MinReturn                        -63.9582
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.5424
Extras/EpisodeRewardMean                    -41.457
LinearFeatureBaseline/ExplainedVariance       0.897677
PolicyExecTime                                0.232736
ProcessExecTime                               0.0319176
TotalEnvSteps                            899668
policy/Entropy                               -2.15717
policy/KL                                     0.00640375
policy/KLBefore                               0
policy/LossAfter                             -0.0155166
policy/LossBefore                            -5.88979e-11
policy/Perplexity                             0.115651
policy/dLoss                                  0.0155166
---------------------------------------  ----------------
2021-06-04 13:59:41 | [train_policy] epoch #889 | Obtaining samples for iteration 889...
2021-06-04 13:59:41 | [train_policy] epoch #889 | Logging diagnostics...
2021-06-04 13:59:41 | [train_policy] epoch #889 | Optimizing policy...
2021-06-04 13:59:41 | [train_policy] epoch #889 | Computing loss before
2021-06-04 13:59:41 | [train_policy] epoch #889 | Computing KL before
2021-06-04 13:59:41 | [train_policy] epoch #889 | Optimizing
2021-06-04 13:59:41 | [train_policy] epoch #889 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:41 | [train_policy] epoch #889 | computing loss before
2021-06-04 13:59:41 | [train_policy] epoch #889 | computing gradient
2021-06-04 13:59:41 | [train_policy] epoch #889 | gradient computed
2021-06-04 13:59:41 | [train_policy] epoch #889 | computing descent direction
2021-06-04 13:59:41 | [train_policy] epoch #889 | descent direction computed
2021-06-04 13:59:41 | [train_policy] epoch #889 | backtrack iters: 1
2021-06-04 13:59:41 | [train_policy] epoch #889 | optimization finished
2021-06-04 13:59:41 | [train_policy] epoch #889 | Computing KL after
2021-06-04 13:59:41 | [train_policy] epoch #889 | Computing loss after
2021-06-04 13:59:41 | [train_policy] epoch #889 | Fitting baseline...
2021-06-04 13:59:41 | [train_policy] epoch #889 | Saving snapshot...
2021-06-04 13:59:41 | [train_policy] epoch #889 | Saved
2021-06-04 13:59:41 | [train_policy] epoch #889 | Time 713.51 s
2021-06-04 13:59:41 | [train_policy] epoch #889 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283569
Evaluation/AverageDiscountedReturn          -42.0597
Evaluation/AverageReturn                    -42.0597
Evaluation/CompletionRate                     0
Evaluation/Iteration                        889
Evaluation/MaxReturn                        -29.7398
Evaluation/MinReturn                        -83.0792
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.6373
Extras/EpisodeRewardMean                    -42.2464
LinearFeatureBaseline/ExplainedVariance       0.805447
PolicyExecTime                                0.216339
ProcessExecTime                               0.0310674
TotalEnvSteps                            900680
policy/Entropy                               -2.14586
policy/KL                                     0.0064216
policy/KLBefore                               0
policy/LossAfter                             -0.0232743
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             0.116968
policy/dLoss                                  0.0232743
---------------------------------------  ----------------
2021-06-04 13:59:41 | [train_policy] epoch #890 | Obtaining samples for iteration 890...
2021-06-04 13:59:42 | [train_policy] epoch #890 | Logging diagnostics...
2021-06-04 13:59:42 | [train_policy] epoch #890 | Optimizing policy...
2021-06-04 13:59:42 | [train_policy] epoch #890 | Computing loss before
2021-06-04 13:59:42 | [train_policy] epoch #890 | Computing KL before
2021-06-04 13:59:42 | [train_policy] epoch #890 | Optimizing
2021-06-04 13:59:42 | [train_policy] epoch #890 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:42 | [train_policy] epoch #890 | computing loss before
2021-06-04 13:59:42 | [train_policy] epoch #890 | computing gradient
2021-06-04 13:59:42 | [train_policy] epoch #890 | gradient computed
2021-06-04 13:59:42 | [train_policy] epoch #890 | computing descent direction
2021-06-04 13:59:42 | [train_policy] epoch #890 | descent direction computed
2021-06-04 13:59:42 | [train_policy] epoch #890 | backtrack iters: 1
2021-06-04 13:59:42 | [train_policy] epoch #890 | optimization finished
2021-06-04 13:59:42 | [train_policy] epoch #890 | Computing KL after
2021-06-04 13:59:42 | [train_policy] epoch #890 | Computing loss after
2021-06-04 13:59:42 | [train_policy] epoch #890 | Fitting baseline...
2021-06-04 13:59:42 | [train_policy] epoch #890 | Saving snapshot...
2021-06-04 13:59:42 | [train_policy] epoch #890 | Saved
2021-06-04 13:59:42 | [train_policy] epoch #890 | Time 714.31 s
2021-06-04 13:59:42 | [train_policy] epoch #890 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.284024
Evaluation/AverageDiscountedReturn          -41.5101
Evaluation/AverageReturn                    -41.5101
Evaluation/CompletionRate                     0
Evaluation/Iteration                        890
Evaluation/MaxReturn                        -28.2534
Evaluation/MinReturn                        -78.502
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.49153
Extras/EpisodeRewardMean                    -41.7818
LinearFeatureBaseline/ExplainedVariance       0.844963
PolicyExecTime                                0.22929
ProcessExecTime                               0.0312295
TotalEnvSteps                            901692
policy/Entropy                               -2.15368
policy/KL                                     0.00713946
policy/KLBefore                               0
policy/LossAfter                             -0.0163828
policy/LossBefore                            -8.3635e-09
policy/Perplexity                             0.116056
policy/dLoss                                  0.0163828
---------------------------------------  ---------------
2021-06-04 13:59:42 | [train_policy] epoch #891 | Obtaining samples for iteration 891...
2021-06-04 13:59:43 | [train_policy] epoch #891 | Logging diagnostics...
2021-06-04 13:59:43 | [train_policy] epoch #891 | Optimizing policy...
2021-06-04 13:59:43 | [train_policy] epoch #891 | Computing loss before
2021-06-04 13:59:43 | [train_policy] epoch #891 | Computing KL before
2021-06-04 13:59:43 | [train_policy] epoch #891 | Optimizing
2021-06-04 13:59:43 | [train_policy] epoch #891 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:43 | [train_policy] epoch #891 | computing loss before
2021-06-04 13:59:43 | [train_policy] epoch #891 | computing gradient
2021-06-04 13:59:43 | [train_policy] epoch #891 | gradient computed
2021-06-04 13:59:43 | [train_policy] epoch #891 | computing descent direction
2021-06-04 13:59:43 | [train_policy] epoch #891 | descent direction computed
2021-06-04 13:59:43 | [train_policy] epoch #891 | backtrack iters: 0
2021-06-04 13:59:43 | [train_policy] epoch #891 | optimization finished
2021-06-04 13:59:43 | [train_policy] epoch #891 | Computing KL after
2021-06-04 13:59:43 | [train_policy] epoch #891 | Computing loss after
2021-06-04 13:59:43 | [train_policy] epoch #891 | Fitting baseline...
2021-06-04 13:59:43 | [train_policy] epoch #891 | Saving snapshot...
2021-06-04 13:59:43 | [train_policy] epoch #891 | Saved
2021-06-04 13:59:43 | [train_policy] epoch #891 | Time 715.11 s
2021-06-04 13:59:43 | [train_policy] epoch #891 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.286297
Evaluation/AverageDiscountedReturn          -40.6224
Evaluation/AverageReturn                    -40.6224
Evaluation/CompletionRate                     0
Evaluation/Iteration                        891
Evaluation/MaxReturn                        -28.7344
Evaluation/MinReturn                        -63.7979
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.11829
Extras/EpisodeRewardMean                    -40.7614
LinearFeatureBaseline/ExplainedVariance       0.889513
PolicyExecTime                                0.224836
ProcessExecTime                               0.0313716
TotalEnvSteps                            902704
policy/Entropy                               -2.17374
policy/KL                                     0.00985998
policy/KLBefore                               0
policy/LossAfter                             -0.0175062
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.113752
policy/dLoss                                  0.0175062
---------------------------------------  ----------------
2021-06-04 13:59:43 | [train_policy] epoch #892 | Obtaining samples for iteration 892...
2021-06-04 13:59:44 | [train_policy] epoch #892 | Logging diagnostics...
2021-06-04 13:59:44 | [train_policy] epoch #892 | Optimizing policy...
2021-06-04 13:59:44 | [train_policy] epoch #892 | Computing loss before
2021-06-04 13:59:44 | [train_policy] epoch #892 | Computing KL before
2021-06-04 13:59:44 | [train_policy] epoch #892 | Optimizing
2021-06-04 13:59:44 | [train_policy] epoch #892 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:44 | [train_policy] epoch #892 | computing loss before
2021-06-04 13:59:44 | [train_policy] epoch #892 | computing gradient
2021-06-04 13:59:44 | [train_policy] epoch #892 | gradient computed
2021-06-04 13:59:44 | [train_policy] epoch #892 | computing descent direction
2021-06-04 13:59:44 | [train_policy] epoch #892 | descent direction computed
2021-06-04 13:59:44 | [train_policy] epoch #892 | backtrack iters: 0
2021-06-04 13:59:44 | [train_policy] epoch #892 | optimization finished
2021-06-04 13:59:44 | [train_policy] epoch #892 | Computing KL after
2021-06-04 13:59:44 | [train_policy] epoch #892 | Computing loss after
2021-06-04 13:59:44 | [train_policy] epoch #892 | Fitting baseline...
2021-06-04 13:59:44 | [train_policy] epoch #892 | Saving snapshot...
2021-06-04 13:59:44 | [train_policy] epoch #892 | Saved
2021-06-04 13:59:44 | [train_policy] epoch #892 | Time 715.92 s
2021-06-04 13:59:44 | [train_policy] epoch #892 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286478
Evaluation/AverageDiscountedReturn          -39.8669
Evaluation/AverageReturn                    -39.8669
Evaluation/CompletionRate                     0
Evaluation/Iteration                        892
Evaluation/MaxReturn                        -28.8893
Evaluation/MinReturn                        -63.8236
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.45146
Extras/EpisodeRewardMean                    -40.0971
LinearFeatureBaseline/ExplainedVariance       0.885726
PolicyExecTime                                0.236641
ProcessExecTime                               0.0313418
TotalEnvSteps                            903716
policy/Entropy                               -2.14959
policy/KL                                     0.00984126
policy/KLBefore                               0
policy/LossAfter                             -0.0162314
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.116532
policy/dLoss                                  0.0162314
---------------------------------------  ----------------
2021-06-04 13:59:44 | [train_policy] epoch #893 | Obtaining samples for iteration 893...
2021-06-04 13:59:44 | [train_policy] epoch #893 | Logging diagnostics...
2021-06-04 13:59:44 | [train_policy] epoch #893 | Optimizing policy...
2021-06-04 13:59:44 | [train_policy] epoch #893 | Computing loss before
2021-06-04 13:59:44 | [train_policy] epoch #893 | Computing KL before
2021-06-04 13:59:44 | [train_policy] epoch #893 | Optimizing
2021-06-04 13:59:44 | [train_policy] epoch #893 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:44 | [train_policy] epoch #893 | computing loss before
2021-06-04 13:59:44 | [train_policy] epoch #893 | computing gradient
2021-06-04 13:59:44 | [train_policy] epoch #893 | gradient computed
2021-06-04 13:59:44 | [train_policy] epoch #893 | computing descent direction
2021-06-04 13:59:44 | [train_policy] epoch #893 | descent direction computed
2021-06-04 13:59:44 | [train_policy] epoch #893 | backtrack iters: 1
2021-06-04 13:59:44 | [train_policy] epoch #893 | optimization finished
2021-06-04 13:59:44 | [train_policy] epoch #893 | Computing KL after
2021-06-04 13:59:44 | [train_policy] epoch #893 | Computing loss after
2021-06-04 13:59:44 | [train_policy] epoch #893 | Fitting baseline...
2021-06-04 13:59:44 | [train_policy] epoch #893 | Saving snapshot...
2021-06-04 13:59:45 | [train_policy] epoch #893 | Saved
2021-06-04 13:59:45 | [train_policy] epoch #893 | Time 716.72 s
2021-06-04 13:59:45 | [train_policy] epoch #893 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.288315
Evaluation/AverageDiscountedReturn          -41.5243
Evaluation/AverageReturn                    -41.5243
Evaluation/CompletionRate                     0
Evaluation/Iteration                        893
Evaluation/MaxReturn                        -28.6881
Evaluation/MinReturn                        -80.218
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.51927
Extras/EpisodeRewardMean                    -41.4075
LinearFeatureBaseline/ExplainedVariance       0.865479
PolicyExecTime                                0.224399
ProcessExecTime                               0.0315874
TotalEnvSteps                            904728
policy/Entropy                               -2.19427
policy/KL                                     0.00656809
policy/KLBefore                               0
policy/LossAfter                             -0.0207464
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.11144
policy/dLoss                                  0.0207464
---------------------------------------  ----------------
2021-06-04 13:59:45 | [train_policy] epoch #894 | Obtaining samples for iteration 894...
2021-06-04 13:59:45 | [train_policy] epoch #894 | Logging diagnostics...
2021-06-04 13:59:45 | [train_policy] epoch #894 | Optimizing policy...
2021-06-04 13:59:45 | [train_policy] epoch #894 | Computing loss before
2021-06-04 13:59:45 | [train_policy] epoch #894 | Computing KL before
2021-06-04 13:59:45 | [train_policy] epoch #894 | Optimizing
2021-06-04 13:59:45 | [train_policy] epoch #894 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:45 | [train_policy] epoch #894 | computing loss before
2021-06-04 13:59:45 | [train_policy] epoch #894 | computing gradient
2021-06-04 13:59:45 | [train_policy] epoch #894 | gradient computed
2021-06-04 13:59:45 | [train_policy] epoch #894 | computing descent direction
2021-06-04 13:59:45 | [train_policy] epoch #894 | descent direction computed
2021-06-04 13:59:45 | [train_policy] epoch #894 | backtrack iters: 1
2021-06-04 13:59:45 | [train_policy] epoch #894 | optimization finished
2021-06-04 13:59:45 | [train_policy] epoch #894 | Computing KL after
2021-06-04 13:59:45 | [train_policy] epoch #894 | Computing loss after
2021-06-04 13:59:45 | [train_policy] epoch #894 | Fitting baseline...
2021-06-04 13:59:45 | [train_policy] epoch #894 | Saving snapshot...
2021-06-04 13:59:45 | [train_policy] epoch #894 | Saved
2021-06-04 13:59:45 | [train_policy] epoch #894 | Time 717.53 s
2021-06-04 13:59:45 | [train_policy] epoch #894 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284915
Evaluation/AverageDiscountedReturn          -40.8985
Evaluation/AverageReturn                    -40.8985
Evaluation/CompletionRate                     0
Evaluation/Iteration                        894
Evaluation/MaxReturn                        -29.0313
Evaluation/MinReturn                        -79.3169
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.10538
Extras/EpisodeRewardMean                    -40.8356
LinearFeatureBaseline/ExplainedVariance       0.858993
PolicyExecTime                                0.227606
ProcessExecTime                               0.0311782
TotalEnvSteps                            905740
policy/Entropy                               -2.19843
policy/KL                                     0.0064863
policy/KLBefore                               0
policy/LossAfter                             -0.0144449
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.110977
policy/dLoss                                  0.0144449
---------------------------------------  ----------------
2021-06-04 13:59:45 | [train_policy] epoch #895 | Obtaining samples for iteration 895...
2021-06-04 13:59:46 | [train_policy] epoch #895 | Logging diagnostics...
2021-06-04 13:59:46 | [train_policy] epoch #895 | Optimizing policy...
2021-06-04 13:59:46 | [train_policy] epoch #895 | Computing loss before
2021-06-04 13:59:46 | [train_policy] epoch #895 | Computing KL before
2021-06-04 13:59:46 | [train_policy] epoch #895 | Optimizing
2021-06-04 13:59:46 | [train_policy] epoch #895 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:46 | [train_policy] epoch #895 | computing loss before
2021-06-04 13:59:46 | [train_policy] epoch #895 | computing gradient
2021-06-04 13:59:46 | [train_policy] epoch #895 | gradient computed
2021-06-04 13:59:46 | [train_policy] epoch #895 | computing descent direction
2021-06-04 13:59:46 | [train_policy] epoch #895 | descent direction computed
2021-06-04 13:59:46 | [train_policy] epoch #895 | backtrack iters: 1
2021-06-04 13:59:46 | [train_policy] epoch #895 | optimization finished
2021-06-04 13:59:46 | [train_policy] epoch #895 | Computing KL after
2021-06-04 13:59:46 | [train_policy] epoch #895 | Computing loss after
2021-06-04 13:59:46 | [train_policy] epoch #895 | Fitting baseline...
2021-06-04 13:59:46 | [train_policy] epoch #895 | Saving snapshot...
2021-06-04 13:59:46 | [train_policy] epoch #895 | Saved
2021-06-04 13:59:46 | [train_policy] epoch #895 | Time 718.33 s
2021-06-04 13:59:46 | [train_policy] epoch #895 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284224
Evaluation/AverageDiscountedReturn          -40.7509
Evaluation/AverageReturn                    -40.7509
Evaluation/CompletionRate                     0
Evaluation/Iteration                        895
Evaluation/MaxReturn                        -29.2826
Evaluation/MinReturn                        -78.5017
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.43619
Extras/EpisodeRewardMean                    -41.0522
LinearFeatureBaseline/ExplainedVariance       0.873269
PolicyExecTime                                0.228489
ProcessExecTime                               0.0310528
TotalEnvSteps                            906752
policy/Entropy                               -2.20065
policy/KL                                     0.0064374
policy/KLBefore                               0
policy/LossAfter                             -0.0182771
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.110731
policy/dLoss                                  0.0182771
---------------------------------------  ----------------
2021-06-04 13:59:46 | [train_policy] epoch #896 | Obtaining samples for iteration 896...
2021-06-04 13:59:47 | [train_policy] epoch #896 | Logging diagnostics...
2021-06-04 13:59:47 | [train_policy] epoch #896 | Optimizing policy...
2021-06-04 13:59:47 | [train_policy] epoch #896 | Computing loss before
2021-06-04 13:59:47 | [train_policy] epoch #896 | Computing KL before
2021-06-04 13:59:47 | [train_policy] epoch #896 | Optimizing
2021-06-04 13:59:47 | [train_policy] epoch #896 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:47 | [train_policy] epoch #896 | computing loss before
2021-06-04 13:59:47 | [train_policy] epoch #896 | computing gradient
2021-06-04 13:59:47 | [train_policy] epoch #896 | gradient computed
2021-06-04 13:59:47 | [train_policy] epoch #896 | computing descent direction
2021-06-04 13:59:47 | [train_policy] epoch #896 | descent direction computed
2021-06-04 13:59:47 | [train_policy] epoch #896 | backtrack iters: 0
2021-06-04 13:59:47 | [train_policy] epoch #896 | optimization finished
2021-06-04 13:59:47 | [train_policy] epoch #896 | Computing KL after
2021-06-04 13:59:47 | [train_policy] epoch #896 | Computing loss after
2021-06-04 13:59:47 | [train_policy] epoch #896 | Fitting baseline...
2021-06-04 13:59:47 | [train_policy] epoch #896 | Saving snapshot...
2021-06-04 13:59:47 | [train_policy] epoch #896 | Saved
2021-06-04 13:59:47 | [train_policy] epoch #896 | Time 719.12 s
2021-06-04 13:59:47 | [train_policy] epoch #896 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285173
Evaluation/AverageDiscountedReturn          -40.7829
Evaluation/AverageReturn                    -40.7829
Evaluation/CompletionRate                     0
Evaluation/Iteration                        896
Evaluation/MaxReturn                        -29.1574
Evaluation/MinReturn                        -77.1655
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.94961
Extras/EpisodeRewardMean                    -40.6364
LinearFeatureBaseline/ExplainedVariance       0.876195
PolicyExecTime                                0.225086
ProcessExecTime                               0.031188
TotalEnvSteps                            907764
policy/Entropy                               -2.19745
policy/KL                                     0.00996573
policy/KLBefore                               0
policy/LossAfter                             -0.0168298
policy/LossBefore                             3.18048e-09
policy/Perplexity                             0.111086
policy/dLoss                                  0.0168298
---------------------------------------  ----------------
2021-06-04 13:59:47 | [train_policy] epoch #897 | Obtaining samples for iteration 897...
2021-06-04 13:59:48 | [train_policy] epoch #897 | Logging diagnostics...
2021-06-04 13:59:48 | [train_policy] epoch #897 | Optimizing policy...
2021-06-04 13:59:48 | [train_policy] epoch #897 | Computing loss before
2021-06-04 13:59:48 | [train_policy] epoch #897 | Computing KL before
2021-06-04 13:59:48 | [train_policy] epoch #897 | Optimizing
2021-06-04 13:59:48 | [train_policy] epoch #897 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:48 | [train_policy] epoch #897 | computing loss before
2021-06-04 13:59:48 | [train_policy] epoch #897 | computing gradient
2021-06-04 13:59:48 | [train_policy] epoch #897 | gradient computed
2021-06-04 13:59:48 | [train_policy] epoch #897 | computing descent direction
2021-06-04 13:59:48 | [train_policy] epoch #897 | descent direction computed
2021-06-04 13:59:48 | [train_policy] epoch #897 | backtrack iters: 1
2021-06-04 13:59:48 | [train_policy] epoch #897 | optimization finished
2021-06-04 13:59:48 | [train_policy] epoch #897 | Computing KL after
2021-06-04 13:59:48 | [train_policy] epoch #897 | Computing loss after
2021-06-04 13:59:48 | [train_policy] epoch #897 | Fitting baseline...
2021-06-04 13:59:48 | [train_policy] epoch #897 | Saving snapshot...
2021-06-04 13:59:48 | [train_policy] epoch #897 | Saved
2021-06-04 13:59:48 | [train_policy] epoch #897 | Time 719.92 s
2021-06-04 13:59:48 | [train_policy] epoch #897 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285448
Evaluation/AverageDiscountedReturn          -40.6781
Evaluation/AverageReturn                    -40.6781
Evaluation/CompletionRate                     0
Evaluation/Iteration                        897
Evaluation/MaxReturn                        -29.51
Evaluation/MinReturn                        -60.4346
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43903
Extras/EpisodeRewardMean                    -40.903
LinearFeatureBaseline/ExplainedVariance       0.888757
PolicyExecTime                                0.227418
ProcessExecTime                               0.0312197
TotalEnvSteps                            908776
policy/Entropy                               -2.19812
policy/KL                                     0.00650418
policy/KLBefore                               0
policy/LossAfter                             -0.0154036
policy/LossBefore                             8.83468e-09
policy/Perplexity                             0.111012
policy/dLoss                                  0.0154036
---------------------------------------  ----------------
2021-06-04 13:59:48 | [train_policy] epoch #898 | Obtaining samples for iteration 898...
2021-06-04 13:59:48 | [train_policy] epoch #898 | Logging diagnostics...
2021-06-04 13:59:48 | [train_policy] epoch #898 | Optimizing policy...
2021-06-04 13:59:48 | [train_policy] epoch #898 | Computing loss before
2021-06-04 13:59:48 | [train_policy] epoch #898 | Computing KL before
2021-06-04 13:59:48 | [train_policy] epoch #898 | Optimizing
2021-06-04 13:59:48 | [train_policy] epoch #898 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:48 | [train_policy] epoch #898 | computing loss before
2021-06-04 13:59:48 | [train_policy] epoch #898 | computing gradient
2021-06-04 13:59:48 | [train_policy] epoch #898 | gradient computed
2021-06-04 13:59:48 | [train_policy] epoch #898 | computing descent direction
2021-06-04 13:59:48 | [train_policy] epoch #898 | descent direction computed
2021-06-04 13:59:48 | [train_policy] epoch #898 | backtrack iters: 1
2021-06-04 13:59:48 | [train_policy] epoch #898 | optimization finished
2021-06-04 13:59:48 | [train_policy] epoch #898 | Computing KL after
2021-06-04 13:59:48 | [train_policy] epoch #898 | Computing loss after
2021-06-04 13:59:48 | [train_policy] epoch #898 | Fitting baseline...
2021-06-04 13:59:48 | [train_policy] epoch #898 | Saving snapshot...
2021-06-04 13:59:48 | [train_policy] epoch #898 | Saved
2021-06-04 13:59:48 | [train_policy] epoch #898 | Time 720.71 s
2021-06-04 13:59:48 | [train_policy] epoch #898 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284505
Evaluation/AverageDiscountedReturn          -40.5585
Evaluation/AverageReturn                    -40.5585
Evaluation/CompletionRate                     0
Evaluation/Iteration                        898
Evaluation/MaxReturn                        -28.8103
Evaluation/MinReturn                        -63.2672
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.12834
Extras/EpisodeRewardMean                    -40.6207
LinearFeatureBaseline/ExplainedVariance       0.892254
PolicyExecTime                                0.222179
ProcessExecTime                               0.0311761
TotalEnvSteps                            909788
policy/Entropy                               -2.19472
policy/KL                                     0.00641014
policy/KLBefore                               0
policy/LossAfter                             -0.0123901
policy/LossBefore                             1.13084e-08
policy/Perplexity                             0.11139
policy/dLoss                                  0.0123901
---------------------------------------  ----------------
2021-06-04 13:59:49 | [train_policy] epoch #899 | Obtaining samples for iteration 899...
2021-06-04 13:59:49 | [train_policy] epoch #899 | Logging diagnostics...
2021-06-04 13:59:49 | [train_policy] epoch #899 | Optimizing policy...
2021-06-04 13:59:49 | [train_policy] epoch #899 | Computing loss before
2021-06-04 13:59:49 | [train_policy] epoch #899 | Computing KL before
2021-06-04 13:59:49 | [train_policy] epoch #899 | Optimizing
2021-06-04 13:59:49 | [train_policy] epoch #899 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:49 | [train_policy] epoch #899 | computing loss before
2021-06-04 13:59:49 | [train_policy] epoch #899 | computing gradient
2021-06-04 13:59:49 | [train_policy] epoch #899 | gradient computed
2021-06-04 13:59:49 | [train_policy] epoch #899 | computing descent direction
2021-06-04 13:59:49 | [train_policy] epoch #899 | descent direction computed
2021-06-04 13:59:49 | [train_policy] epoch #899 | backtrack iters: 1
2021-06-04 13:59:49 | [train_policy] epoch #899 | optimization finished
2021-06-04 13:59:49 | [train_policy] epoch #899 | Computing KL after
2021-06-04 13:59:49 | [train_policy] epoch #899 | Computing loss after
2021-06-04 13:59:49 | [train_policy] epoch #899 | Fitting baseline...
2021-06-04 13:59:49 | [train_policy] epoch #899 | Saving snapshot...
2021-06-04 13:59:49 | [train_policy] epoch #899 | Saved
2021-06-04 13:59:49 | [train_policy] epoch #899 | Time 721.53 s
2021-06-04 13:59:49 | [train_policy] epoch #899 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                   0.286911
Evaluation/AverageDiscountedReturn          -41.9086
Evaluation/AverageReturn                    -41.9086
Evaluation/CompletionRate                     0
Evaluation/Iteration                        899
Evaluation/MaxReturn                        -28.0065
Evaluation/MinReturn                        -78.0093
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.73607
Extras/EpisodeRewardMean                    -42.0418
LinearFeatureBaseline/ExplainedVariance       0.860192
PolicyExecTime                                0.241825
ProcessExecTime                               0.0314732
TotalEnvSteps                            910800
policy/Entropy                               -2.21091
policy/KL                                     0.00650428
policy/KLBefore                               0
policy/LossAfter                             -0.0160572
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             0.109601
policy/dLoss                                  0.0160571
---------------------------------------  ---------------
2021-06-04 13:59:49 | [train_policy] epoch #900 | Obtaining samples for iteration 900...
2021-06-04 13:59:50 | [train_policy] epoch #900 | Logging diagnostics...
2021-06-04 13:59:50 | [train_policy] epoch #900 | Optimizing policy...
2021-06-04 13:59:50 | [train_policy] epoch #900 | Computing loss before
2021-06-04 13:59:50 | [train_policy] epoch #900 | Computing KL before
2021-06-04 13:59:50 | [train_policy] epoch #900 | Optimizing
2021-06-04 13:59:50 | [train_policy] epoch #900 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:50 | [train_policy] epoch #900 | computing loss before
2021-06-04 13:59:50 | [train_policy] epoch #900 | computing gradient
2021-06-04 13:59:50 | [train_policy] epoch #900 | gradient computed
2021-06-04 13:59:50 | [train_policy] epoch #900 | computing descent direction
2021-06-04 13:59:50 | [train_policy] epoch #900 | descent direction computed
2021-06-04 13:59:50 | [train_policy] epoch #900 | backtrack iters: 1
2021-06-04 13:59:50 | [train_policy] epoch #900 | optimization finished
2021-06-04 13:59:50 | [train_policy] epoch #900 | Computing KL after
2021-06-04 13:59:50 | [train_policy] epoch #900 | Computing loss after
2021-06-04 13:59:50 | [train_policy] epoch #900 | Fitting baseline...
2021-06-04 13:59:50 | [train_policy] epoch #900 | Saving snapshot...
2021-06-04 13:59:50 | [train_policy] epoch #900 | Saved
2021-06-04 13:59:50 | [train_policy] epoch #900 | Time 722.33 s
2021-06-04 13:59:50 | [train_policy] epoch #900 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284381
Evaluation/AverageDiscountedReturn          -41.1009
Evaluation/AverageReturn                    -41.1009
Evaluation/CompletionRate                     0
Evaluation/Iteration                        900
Evaluation/MaxReturn                        -28.7682
Evaluation/MinReturn                        -77.2322
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.95003
Extras/EpisodeRewardMean                    -40.768
LinearFeatureBaseline/ExplainedVariance       0.864323
PolicyExecTime                                0.229779
ProcessExecTime                               0.0311944
TotalEnvSteps                            911812
policy/Entropy                               -2.22271
policy/KL                                     0.00654082
policy/KLBefore                               0
policy/LossAfter                             -0.0182361
policy/LossBefore                            -1.71982e-08
policy/Perplexity                             0.108315
policy/dLoss                                  0.0182361
---------------------------------------  ----------------
2021-06-04 13:59:50 | [train_policy] epoch #901 | Obtaining samples for iteration 901...
2021-06-04 13:59:51 | [train_policy] epoch #901 | Logging diagnostics...
2021-06-04 13:59:51 | [train_policy] epoch #901 | Optimizing policy...
2021-06-04 13:59:51 | [train_policy] epoch #901 | Computing loss before
2021-06-04 13:59:51 | [train_policy] epoch #901 | Computing KL before
2021-06-04 13:59:51 | [train_policy] epoch #901 | Optimizing
2021-06-04 13:59:51 | [train_policy] epoch #901 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:51 | [train_policy] epoch #901 | computing loss before
2021-06-04 13:59:51 | [train_policy] epoch #901 | computing gradient
2021-06-04 13:59:51 | [train_policy] epoch #901 | gradient computed
2021-06-04 13:59:51 | [train_policy] epoch #901 | computing descent direction
2021-06-04 13:59:51 | [train_policy] epoch #901 | descent direction computed
2021-06-04 13:59:51 | [train_policy] epoch #901 | backtrack iters: 1
2021-06-04 13:59:51 | [train_policy] epoch #901 | optimization finished
2021-06-04 13:59:51 | [train_policy] epoch #901 | Computing KL after
2021-06-04 13:59:51 | [train_policy] epoch #901 | Computing loss after
2021-06-04 13:59:51 | [train_policy] epoch #901 | Fitting baseline...
2021-06-04 13:59:51 | [train_policy] epoch #901 | Saving snapshot...
2021-06-04 13:59:51 | [train_policy] epoch #901 | Saved
2021-06-04 13:59:51 | [train_policy] epoch #901 | Time 723.14 s
2021-06-04 13:59:51 | [train_policy] epoch #901 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286298
Evaluation/AverageDiscountedReturn          -39.7213
Evaluation/AverageReturn                    -39.7213
Evaluation/CompletionRate                     0
Evaluation/Iteration                        901
Evaluation/MaxReturn                        -29.4163
Evaluation/MinReturn                        -63.8497
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.77392
Extras/EpisodeRewardMean                    -39.5906
LinearFeatureBaseline/ExplainedVariance       0.890956
PolicyExecTime                                0.228373
ProcessExecTime                               0.0313311
TotalEnvSteps                            912824
policy/Entropy                               -2.22724
policy/KL                                     0.00649809
policy/KLBefore                               0
policy/LossAfter                             -0.018548
policy/LossBefore                             2.14388e-08
policy/Perplexity                             0.107826
policy/dLoss                                  0.018548
---------------------------------------  ----------------
2021-06-04 13:59:51 | [train_policy] epoch #902 | Obtaining samples for iteration 902...
2021-06-04 13:59:52 | [train_policy] epoch #902 | Logging diagnostics...
2021-06-04 13:59:52 | [train_policy] epoch #902 | Optimizing policy...
2021-06-04 13:59:52 | [train_policy] epoch #902 | Computing loss before
2021-06-04 13:59:52 | [train_policy] epoch #902 | Computing KL before
2021-06-04 13:59:52 | [train_policy] epoch #902 | Optimizing
2021-06-04 13:59:52 | [train_policy] epoch #902 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:52 | [train_policy] epoch #902 | computing loss before
2021-06-04 13:59:52 | [train_policy] epoch #902 | computing gradient
2021-06-04 13:59:52 | [train_policy] epoch #902 | gradient computed
2021-06-04 13:59:52 | [train_policy] epoch #902 | computing descent direction
2021-06-04 13:59:52 | [train_policy] epoch #902 | descent direction computed
2021-06-04 13:59:52 | [train_policy] epoch #902 | backtrack iters: 0
2021-06-04 13:59:52 | [train_policy] epoch #902 | optimization finished
2021-06-04 13:59:52 | [train_policy] epoch #902 | Computing KL after
2021-06-04 13:59:52 | [train_policy] epoch #902 | Computing loss after
2021-06-04 13:59:52 | [train_policy] epoch #902 | Fitting baseline...
2021-06-04 13:59:52 | [train_policy] epoch #902 | Saving snapshot...
2021-06-04 13:59:52 | [train_policy] epoch #902 | Saved
2021-06-04 13:59:52 | [train_policy] epoch #902 | Time 723.95 s
2021-06-04 13:59:52 | [train_policy] epoch #902 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28632
Evaluation/AverageDiscountedReturn          -39.1887
Evaluation/AverageReturn                    -39.1887
Evaluation/CompletionRate                     0
Evaluation/Iteration                        902
Evaluation/MaxReturn                        -28.3412
Evaluation/MinReturn                        -57.1431
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.74865
Extras/EpisodeRewardMean                    -39.4253
LinearFeatureBaseline/ExplainedVariance       0.877967
PolicyExecTime                                0.2326
ProcessExecTime                               0.0313504
TotalEnvSteps                            913836
policy/Entropy                               -2.22921
policy/KL                                     0.00982154
policy/KLBefore                               0
policy/LossAfter                             -0.0191123
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.107613
policy/dLoss                                  0.0191123
---------------------------------------  ----------------
2021-06-04 13:59:52 | [train_policy] epoch #903 | Obtaining samples for iteration 903...
2021-06-04 13:59:52 | [train_policy] epoch #903 | Logging diagnostics...
2021-06-04 13:59:52 | [train_policy] epoch #903 | Optimizing policy...
2021-06-04 13:59:52 | [train_policy] epoch #903 | Computing loss before
2021-06-04 13:59:52 | [train_policy] epoch #903 | Computing KL before
2021-06-04 13:59:52 | [train_policy] epoch #903 | Optimizing
2021-06-04 13:59:52 | [train_policy] epoch #903 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:52 | [train_policy] epoch #903 | computing loss before
2021-06-04 13:59:52 | [train_policy] epoch #903 | computing gradient
2021-06-04 13:59:52 | [train_policy] epoch #903 | gradient computed
2021-06-04 13:59:52 | [train_policy] epoch #903 | computing descent direction
2021-06-04 13:59:52 | [train_policy] epoch #903 | descent direction computed
2021-06-04 13:59:52 | [train_policy] epoch #903 | backtrack iters: 1
2021-06-04 13:59:52 | [train_policy] epoch #903 | optimization finished
2021-06-04 13:59:52 | [train_policy] epoch #903 | Computing KL after
2021-06-04 13:59:52 | [train_policy] epoch #903 | Computing loss after
2021-06-04 13:59:52 | [train_policy] epoch #903 | Fitting baseline...
2021-06-04 13:59:53 | [train_policy] epoch #903 | Saving snapshot...
2021-06-04 13:59:53 | [train_policy] epoch #903 | Saved
2021-06-04 13:59:53 | [train_policy] epoch #903 | Time 724.76 s
2021-06-04 13:59:53 | [train_policy] epoch #903 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283999
Evaluation/AverageDiscountedReturn          -40.2233
Evaluation/AverageReturn                    -40.2233
Evaluation/CompletionRate                     0
Evaluation/Iteration                        903
Evaluation/MaxReturn                        -28.1564
Evaluation/MinReturn                        -63.8633
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.584
Extras/EpisodeRewardMean                    -39.8914
LinearFeatureBaseline/ExplainedVariance       0.884729
PolicyExecTime                                0.236892
ProcessExecTime                               0.0311015
TotalEnvSteps                            914848
policy/Entropy                               -2.2489
policy/KL                                     0.00659505
policy/KLBefore                               0
policy/LossAfter                             -0.0131314
policy/LossBefore                            -2.49727e-08
policy/Perplexity                             0.105515
policy/dLoss                                  0.0131314
---------------------------------------  ----------------
2021-06-04 13:59:53 | [train_policy] epoch #904 | Obtaining samples for iteration 904...
2021-06-04 13:59:53 | [train_policy] epoch #904 | Logging diagnostics...
2021-06-04 13:59:53 | [train_policy] epoch #904 | Optimizing policy...
2021-06-04 13:59:53 | [train_policy] epoch #904 | Computing loss before
2021-06-04 13:59:53 | [train_policy] epoch #904 | Computing KL before
2021-06-04 13:59:53 | [train_policy] epoch #904 | Optimizing
2021-06-04 13:59:53 | [train_policy] epoch #904 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:53 | [train_policy] epoch #904 | computing loss before
2021-06-04 13:59:53 | [train_policy] epoch #904 | computing gradient
2021-06-04 13:59:53 | [train_policy] epoch #904 | gradient computed
2021-06-04 13:59:53 | [train_policy] epoch #904 | computing descent direction
2021-06-04 13:59:53 | [train_policy] epoch #904 | descent direction computed
2021-06-04 13:59:53 | [train_policy] epoch #904 | backtrack iters: 1
2021-06-04 13:59:53 | [train_policy] epoch #904 | optimization finished
2021-06-04 13:59:53 | [train_policy] epoch #904 | Computing KL after
2021-06-04 13:59:53 | [train_policy] epoch #904 | Computing loss after
2021-06-04 13:59:53 | [train_policy] epoch #904 | Fitting baseline...
2021-06-04 13:59:53 | [train_policy] epoch #904 | Saving snapshot...
2021-06-04 13:59:53 | [train_policy] epoch #904 | Saved
2021-06-04 13:59:53 | [train_policy] epoch #904 | Time 725.57 s
2021-06-04 13:59:53 | [train_policy] epoch #904 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283056
Evaluation/AverageDiscountedReturn          -39.7465
Evaluation/AverageReturn                    -39.7465
Evaluation/CompletionRate                     0
Evaluation/Iteration                        904
Evaluation/MaxReturn                        -29.067
Evaluation/MinReturn                        -63.8298
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.98669
Extras/EpisodeRewardMean                    -39.5371
LinearFeatureBaseline/ExplainedVariance       0.886501
PolicyExecTime                                0.230437
ProcessExecTime                               0.03104
TotalEnvSteps                            915860
policy/Entropy                               -2.27863
policy/KL                                     0.00653789
policy/KLBefore                               0
policy/LossAfter                             -0.0157648
policy/LossBefore                            -6.89105e-09
policy/Perplexity                             0.102425
policy/dLoss                                  0.0157648
---------------------------------------  ----------------
2021-06-04 13:59:53 | [train_policy] epoch #905 | Obtaining samples for iteration 905...
2021-06-04 13:59:54 | [train_policy] epoch #905 | Logging diagnostics...
2021-06-04 13:59:54 | [train_policy] epoch #905 | Optimizing policy...
2021-06-04 13:59:54 | [train_policy] epoch #905 | Computing loss before
2021-06-04 13:59:54 | [train_policy] epoch #905 | Computing KL before
2021-06-04 13:59:54 | [train_policy] epoch #905 | Optimizing
2021-06-04 13:59:54 | [train_policy] epoch #905 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:54 | [train_policy] epoch #905 | computing loss before
2021-06-04 13:59:54 | [train_policy] epoch #905 | computing gradient
2021-06-04 13:59:54 | [train_policy] epoch #905 | gradient computed
2021-06-04 13:59:54 | [train_policy] epoch #905 | computing descent direction
2021-06-04 13:59:54 | [train_policy] epoch #905 | descent direction computed
2021-06-04 13:59:54 | [train_policy] epoch #905 | backtrack iters: 0
2021-06-04 13:59:54 | [train_policy] epoch #905 | optimization finished
2021-06-04 13:59:54 | [train_policy] epoch #905 | Computing KL after
2021-06-04 13:59:54 | [train_policy] epoch #905 | Computing loss after
2021-06-04 13:59:54 | [train_policy] epoch #905 | Fitting baseline...
2021-06-04 13:59:54 | [train_policy] epoch #905 | Saving snapshot...
2021-06-04 13:59:54 | [train_policy] epoch #905 | Saved
2021-06-04 13:59:54 | [train_policy] epoch #905 | Time 726.37 s
2021-06-04 13:59:54 | [train_policy] epoch #905 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283902
Evaluation/AverageDiscountedReturn          -40.4381
Evaluation/AverageReturn                    -40.4381
Evaluation/CompletionRate                     0
Evaluation/Iteration                        905
Evaluation/MaxReturn                        -28.2492
Evaluation/MinReturn                        -63.8242
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12912
Extras/EpisodeRewardMean                    -40.1037
LinearFeatureBaseline/ExplainedVariance       0.88169
PolicyExecTime                                0.215499
ProcessExecTime                               0.0311384
TotalEnvSteps                            916872
policy/Entropy                               -2.29633
policy/KL                                     0.00917528
policy/KLBefore                               0
policy/LossAfter                             -0.0182049
policy/LossBefore                             1.56668e-08
policy/Perplexity                             0.100627
policy/dLoss                                  0.0182049
---------------------------------------  ----------------
2021-06-04 13:59:54 | [train_policy] epoch #906 | Obtaining samples for iteration 906...
2021-06-04 13:59:55 | [train_policy] epoch #906 | Logging diagnostics...
2021-06-04 13:59:55 | [train_policy] epoch #906 | Optimizing policy...
2021-06-04 13:59:55 | [train_policy] epoch #906 | Computing loss before
2021-06-04 13:59:55 | [train_policy] epoch #906 | Computing KL before
2021-06-04 13:59:55 | [train_policy] epoch #906 | Optimizing
2021-06-04 13:59:55 | [train_policy] epoch #906 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:55 | [train_policy] epoch #906 | computing loss before
2021-06-04 13:59:55 | [train_policy] epoch #906 | computing gradient
2021-06-04 13:59:55 | [train_policy] epoch #906 | gradient computed
2021-06-04 13:59:55 | [train_policy] epoch #906 | computing descent direction
2021-06-04 13:59:55 | [train_policy] epoch #906 | descent direction computed
2021-06-04 13:59:55 | [train_policy] epoch #906 | backtrack iters: 1
2021-06-04 13:59:55 | [train_policy] epoch #906 | optimization finished
2021-06-04 13:59:55 | [train_policy] epoch #906 | Computing KL after
2021-06-04 13:59:55 | [train_policy] epoch #906 | Computing loss after
2021-06-04 13:59:55 | [train_policy] epoch #906 | Fitting baseline...
2021-06-04 13:59:55 | [train_policy] epoch #906 | Saving snapshot...
2021-06-04 13:59:55 | [train_policy] epoch #906 | Saved
2021-06-04 13:59:55 | [train_policy] epoch #906 | Time 727.19 s
2021-06-04 13:59:55 | [train_policy] epoch #906 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.288176
Evaluation/AverageDiscountedReturn          -40.1865
Evaluation/AverageReturn                    -40.1865
Evaluation/CompletionRate                     0
Evaluation/Iteration                        906
Evaluation/MaxReturn                        -28.9724
Evaluation/MinReturn                        -63.8013
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.1037
Extras/EpisodeRewardMean                    -40.2344
LinearFeatureBaseline/ExplainedVariance       0.891501
PolicyExecTime                                0.225859
ProcessExecTime                               0.0315931
TotalEnvSteps                            917884
policy/Entropy                               -2.30505
policy/KL                                     0.0064161
policy/KLBefore                               0
policy/LossAfter                             -0.0167459
policy/LossBefore                             5.35971e-09
policy/Perplexity                             0.0997538
policy/dLoss                                  0.0167459
---------------------------------------  ----------------
2021-06-04 13:59:55 | [train_policy] epoch #907 | Obtaining samples for iteration 907...
2021-06-04 13:59:56 | [train_policy] epoch #907 | Logging diagnostics...
2021-06-04 13:59:56 | [train_policy] epoch #907 | Optimizing policy...
2021-06-04 13:59:56 | [train_policy] epoch #907 | Computing loss before
2021-06-04 13:59:56 | [train_policy] epoch #907 | Computing KL before
2021-06-04 13:59:56 | [train_policy] epoch #907 | Optimizing
2021-06-04 13:59:56 | [train_policy] epoch #907 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:56 | [train_policy] epoch #907 | computing loss before
2021-06-04 13:59:56 | [train_policy] epoch #907 | computing gradient
2021-06-04 13:59:56 | [train_policy] epoch #907 | gradient computed
2021-06-04 13:59:56 | [train_policy] epoch #907 | computing descent direction
2021-06-04 13:59:56 | [train_policy] epoch #907 | descent direction computed
2021-06-04 13:59:56 | [train_policy] epoch #907 | backtrack iters: 0
2021-06-04 13:59:56 | [train_policy] epoch #907 | optimization finished
2021-06-04 13:59:56 | [train_policy] epoch #907 | Computing KL after
2021-06-04 13:59:56 | [train_policy] epoch #907 | Computing loss after
2021-06-04 13:59:56 | [train_policy] epoch #907 | Fitting baseline...
2021-06-04 13:59:56 | [train_policy] epoch #907 | Saving snapshot...
2021-06-04 13:59:56 | [train_policy] epoch #907 | Saved
2021-06-04 13:59:56 | [train_policy] epoch #907 | Time 727.98 s
2021-06-04 13:59:56 | [train_policy] epoch #907 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.284572
Evaluation/AverageDiscountedReturn          -40.5746
Evaluation/AverageReturn                    -40.5746
Evaluation/CompletionRate                     0
Evaluation/Iteration                        907
Evaluation/MaxReturn                        -29.0528
Evaluation/MinReturn                        -63.8725
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.63247
Extras/EpisodeRewardMean                    -40.4102
LinearFeatureBaseline/ExplainedVariance       0.884529
PolicyExecTime                                0.223235
ProcessExecTime                               0.0313399
TotalEnvSteps                            918896
policy/Entropy                               -2.27492
policy/KL                                     0.00986374
policy/KLBefore                               0
policy/LossAfter                             -0.0175003
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.102805
policy/dLoss                                  0.0175003
---------------------------------------  ---------------
2021-06-04 13:59:56 | [train_policy] epoch #908 | Obtaining samples for iteration 908...
2021-06-04 13:59:56 | [train_policy] epoch #908 | Logging diagnostics...
2021-06-04 13:59:56 | [train_policy] epoch #908 | Optimizing policy...
2021-06-04 13:59:56 | [train_policy] epoch #908 | Computing loss before
2021-06-04 13:59:56 | [train_policy] epoch #908 | Computing KL before
2021-06-04 13:59:56 | [train_policy] epoch #908 | Optimizing
2021-06-04 13:59:56 | [train_policy] epoch #908 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:56 | [train_policy] epoch #908 | computing loss before
2021-06-04 13:59:56 | [train_policy] epoch #908 | computing gradient
2021-06-04 13:59:56 | [train_policy] epoch #908 | gradient computed
2021-06-04 13:59:56 | [train_policy] epoch #908 | computing descent direction
2021-06-04 13:59:57 | [train_policy] epoch #908 | descent direction computed
2021-06-04 13:59:57 | [train_policy] epoch #908 | backtrack iters: 1
2021-06-04 13:59:57 | [train_policy] epoch #908 | optimization finished
2021-06-04 13:59:57 | [train_policy] epoch #908 | Computing KL after
2021-06-04 13:59:57 | [train_policy] epoch #908 | Computing loss after
2021-06-04 13:59:57 | [train_policy] epoch #908 | Fitting baseline...
2021-06-04 13:59:57 | [train_policy] epoch #908 | Saving snapshot...
2021-06-04 13:59:57 | [train_policy] epoch #908 | Saved
2021-06-04 13:59:57 | [train_policy] epoch #908 | Time 728.79 s
2021-06-04 13:59:57 | [train_policy] epoch #908 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285235
Evaluation/AverageDiscountedReturn          -39.4912
Evaluation/AverageReturn                    -39.4912
Evaluation/CompletionRate                     0
Evaluation/Iteration                        908
Evaluation/MaxReturn                        -28.3984
Evaluation/MinReturn                        -78.3132
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.70576
Extras/EpisodeRewardMean                    -39.6148
LinearFeatureBaseline/ExplainedVariance       0.8347
PolicyExecTime                                0.228302
ProcessExecTime                               0.0312316
TotalEnvSteps                            919908
policy/Entropy                               -2.29448
policy/KL                                     0.00657564
policy/KLBefore                               0
policy/LossAfter                             -0.0176834
policy/LossBefore                             3.88726e-09
policy/Perplexity                             0.100814
policy/dLoss                                  0.0176834
---------------------------------------  ----------------
2021-06-04 13:59:57 | [train_policy] epoch #909 | Obtaining samples for iteration 909...
2021-06-04 13:59:57 | [train_policy] epoch #909 | Logging diagnostics...
2021-06-04 13:59:57 | [train_policy] epoch #909 | Optimizing policy...
2021-06-04 13:59:57 | [train_policy] epoch #909 | Computing loss before
2021-06-04 13:59:57 | [train_policy] epoch #909 | Computing KL before
2021-06-04 13:59:57 | [train_policy] epoch #909 | Optimizing
2021-06-04 13:59:57 | [train_policy] epoch #909 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:57 | [train_policy] epoch #909 | computing loss before
2021-06-04 13:59:57 | [train_policy] epoch #909 | computing gradient
2021-06-04 13:59:57 | [train_policy] epoch #909 | gradient computed
2021-06-04 13:59:57 | [train_policy] epoch #909 | computing descent direction
2021-06-04 13:59:57 | [train_policy] epoch #909 | descent direction computed
2021-06-04 13:59:57 | [train_policy] epoch #909 | backtrack iters: 1
2021-06-04 13:59:57 | [train_policy] epoch #909 | optimization finished
2021-06-04 13:59:57 | [train_policy] epoch #909 | Computing KL after
2021-06-04 13:59:57 | [train_policy] epoch #909 | Computing loss after
2021-06-04 13:59:57 | [train_policy] epoch #909 | Fitting baseline...
2021-06-04 13:59:57 | [train_policy] epoch #909 | Saving snapshot...
2021-06-04 13:59:57 | [train_policy] epoch #909 | Saved
2021-06-04 13:59:57 | [train_policy] epoch #909 | Time 729.60 s
2021-06-04 13:59:57 | [train_policy] epoch #909 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284581
Evaluation/AverageDiscountedReturn          -38.8167
Evaluation/AverageReturn                    -38.8167
Evaluation/CompletionRate                     0
Evaluation/Iteration                        909
Evaluation/MaxReturn                        -28.2207
Evaluation/MinReturn                        -63.8953
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43917
Extras/EpisodeRewardMean                    -38.8693
LinearFeatureBaseline/ExplainedVariance       0.891815
PolicyExecTime                                0.236027
ProcessExecTime                               0.0313172
TotalEnvSteps                            920920
policy/Entropy                               -2.28134
policy/KL                                     0.00641514
policy/KLBefore                               0
policy/LossAfter                             -0.0151405
policy/LossBefore                            -9.42366e-09
policy/Perplexity                             0.102147
policy/dLoss                                  0.0151405
---------------------------------------  ----------------
2021-06-04 13:59:57 | [train_policy] epoch #910 | Obtaining samples for iteration 910...
2021-06-04 13:59:58 | [train_policy] epoch #910 | Logging diagnostics...
2021-06-04 13:59:58 | [train_policy] epoch #910 | Optimizing policy...
2021-06-04 13:59:58 | [train_policy] epoch #910 | Computing loss before
2021-06-04 13:59:58 | [train_policy] epoch #910 | Computing KL before
2021-06-04 13:59:58 | [train_policy] epoch #910 | Optimizing
2021-06-04 13:59:58 | [train_policy] epoch #910 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:58 | [train_policy] epoch #910 | computing loss before
2021-06-04 13:59:58 | [train_policy] epoch #910 | computing gradient
2021-06-04 13:59:58 | [train_policy] epoch #910 | gradient computed
2021-06-04 13:59:58 | [train_policy] epoch #910 | computing descent direction
2021-06-04 13:59:58 | [train_policy] epoch #910 | descent direction computed
2021-06-04 13:59:58 | [train_policy] epoch #910 | backtrack iters: 1
2021-06-04 13:59:58 | [train_policy] epoch #910 | optimization finished
2021-06-04 13:59:58 | [train_policy] epoch #910 | Computing KL after
2021-06-04 13:59:58 | [train_policy] epoch #910 | Computing loss after
2021-06-04 13:59:58 | [train_policy] epoch #910 | Fitting baseline...
2021-06-04 13:59:58 | [train_policy] epoch #910 | Saving snapshot...
2021-06-04 13:59:58 | [train_policy] epoch #910 | Saved
2021-06-04 13:59:58 | [train_policy] epoch #910 | Time 730.40 s
2021-06-04 13:59:58 | [train_policy] epoch #910 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284571
Evaluation/AverageDiscountedReturn          -41.6177
Evaluation/AverageReturn                    -41.6177
Evaluation/CompletionRate                     0
Evaluation/Iteration                        910
Evaluation/MaxReturn                        -27.9781
Evaluation/MinReturn                        -78.4109
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3645
Extras/EpisodeRewardMean                    -41.5197
LinearFeatureBaseline/ExplainedVariance       0.836418
PolicyExecTime                                0.218927
ProcessExecTime                               0.03107
TotalEnvSteps                            921932
policy/Entropy                               -2.28513
policy/KL                                     0.00645236
policy/KLBefore                               0
policy/LossAfter                             -0.013395
policy/LossBefore                             7.18554e-09
policy/Perplexity                             0.10176
policy/dLoss                                  0.013395
---------------------------------------  ----------------
2021-06-04 13:59:58 | [train_policy] epoch #911 | Obtaining samples for iteration 911...
2021-06-04 13:59:59 | [train_policy] epoch #911 | Logging diagnostics...
2021-06-04 13:59:59 | [train_policy] epoch #911 | Optimizing policy...
2021-06-04 13:59:59 | [train_policy] epoch #911 | Computing loss before
2021-06-04 13:59:59 | [train_policy] epoch #911 | Computing KL before
2021-06-04 13:59:59 | [train_policy] epoch #911 | Optimizing
2021-06-04 13:59:59 | [train_policy] epoch #911 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 13:59:59 | [train_policy] epoch #911 | computing loss before
2021-06-04 13:59:59 | [train_policy] epoch #911 | computing gradient
2021-06-04 13:59:59 | [train_policy] epoch #911 | gradient computed
2021-06-04 13:59:59 | [train_policy] epoch #911 | computing descent direction
2021-06-04 13:59:59 | [train_policy] epoch #911 | descent direction computed
2021-06-04 13:59:59 | [train_policy] epoch #911 | backtrack iters: 1
2021-06-04 13:59:59 | [train_policy] epoch #911 | optimization finished
2021-06-04 13:59:59 | [train_policy] epoch #911 | Computing KL after
2021-06-04 13:59:59 | [train_policy] epoch #911 | Computing loss after
2021-06-04 13:59:59 | [train_policy] epoch #911 | Fitting baseline...
2021-06-04 13:59:59 | [train_policy] epoch #911 | Saving snapshot...
2021-06-04 13:59:59 | [train_policy] epoch #911 | Saved
2021-06-04 13:59:59 | [train_policy] epoch #911 | Time 731.24 s
2021-06-04 13:59:59 | [train_policy] epoch #911 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.285527
Evaluation/AverageDiscountedReturn          -39.7519
Evaluation/AverageReturn                    -39.7519
Evaluation/CompletionRate                     0
Evaluation/Iteration                        911
Evaluation/MaxReturn                        -28.2395
Evaluation/MinReturn                        -76.7281
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.65312
Extras/EpisodeRewardMean                    -40.0497
LinearFeatureBaseline/ExplainedVariance       0.847355
PolicyExecTime                                0.232871
ProcessExecTime                               0.0314016
TotalEnvSteps                            922944
policy/Entropy                               -2.2756
policy/KL                                     0.00642349
policy/KLBefore                               0
policy/LossAfter                             -0.0159698
policy/LossBefore                             2.39125e-08
policy/Perplexity                             0.102735
policy/dLoss                                  0.0159698
---------------------------------------  ----------------
2021-06-04 13:59:59 | [train_policy] epoch #912 | Obtaining samples for iteration 912...
2021-06-04 14:00:00 | [train_policy] epoch #912 | Logging diagnostics...
2021-06-04 14:00:00 | [train_policy] epoch #912 | Optimizing policy...
2021-06-04 14:00:00 | [train_policy] epoch #912 | Computing loss before
2021-06-04 14:00:00 | [train_policy] epoch #912 | Computing KL before
2021-06-04 14:00:00 | [train_policy] epoch #912 | Optimizing
2021-06-04 14:00:00 | [train_policy] epoch #912 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:00 | [train_policy] epoch #912 | computing loss before
2021-06-04 14:00:00 | [train_policy] epoch #912 | computing gradient
2021-06-04 14:00:00 | [train_policy] epoch #912 | gradient computed
2021-06-04 14:00:00 | [train_policy] epoch #912 | computing descent direction
2021-06-04 14:00:00 | [train_policy] epoch #912 | descent direction computed
2021-06-04 14:00:00 | [train_policy] epoch #912 | backtrack iters: 1
2021-06-04 14:00:00 | [train_policy] epoch #912 | optimization finished
2021-06-04 14:00:00 | [train_policy] epoch #912 | Computing KL after
2021-06-04 14:00:00 | [train_policy] epoch #912 | Computing loss after
2021-06-04 14:00:00 | [train_policy] epoch #912 | Fitting baseline...
2021-06-04 14:00:00 | [train_policy] epoch #912 | Saving snapshot...
2021-06-04 14:00:00 | [train_policy] epoch #912 | Saved
2021-06-04 14:00:00 | [train_policy] epoch #912 | Time 732.05 s
2021-06-04 14:00:00 | [train_policy] epoch #912 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283931
Evaluation/AverageDiscountedReturn          -40.0854
Evaluation/AverageReturn                    -40.0854
Evaluation/CompletionRate                     0
Evaluation/Iteration                        912
Evaluation/MaxReturn                        -28.3026
Evaluation/MinReturn                        -63.8109
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.60709
Extras/EpisodeRewardMean                    -39.8437
LinearFeatureBaseline/ExplainedVariance       0.898726
PolicyExecTime                                0.232768
ProcessExecTime                               0.0312672
TotalEnvSteps                            923956
policy/Entropy                               -2.27478
policy/KL                                     0.00645549
policy/KLBefore                               0
policy/LossAfter                             -0.0114531
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.102819
policy/dLoss                                  0.0114531
---------------------------------------  ----------------
2021-06-04 14:00:00 | [train_policy] epoch #913 | Obtaining samples for iteration 913...
2021-06-04 14:00:00 | [train_policy] epoch #913 | Logging diagnostics...
2021-06-04 14:00:00 | [train_policy] epoch #913 | Optimizing policy...
2021-06-04 14:00:00 | [train_policy] epoch #913 | Computing loss before
2021-06-04 14:00:00 | [train_policy] epoch #913 | Computing KL before
2021-06-04 14:00:00 | [train_policy] epoch #913 | Optimizing
2021-06-04 14:00:00 | [train_policy] epoch #913 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:00 | [train_policy] epoch #913 | computing loss before
2021-06-04 14:00:00 | [train_policy] epoch #913 | computing gradient
2021-06-04 14:00:01 | [train_policy] epoch #913 | gradient computed
2021-06-04 14:00:01 | [train_policy] epoch #913 | computing descent direction
2021-06-04 14:00:01 | [train_policy] epoch #913 | descent direction computed
2021-06-04 14:00:01 | [train_policy] epoch #913 | backtrack iters: 1
2021-06-04 14:00:01 | [train_policy] epoch #913 | optimization finished
2021-06-04 14:00:01 | [train_policy] epoch #913 | Computing KL after
2021-06-04 14:00:01 | [train_policy] epoch #913 | Computing loss after
2021-06-04 14:00:01 | [train_policy] epoch #913 | Fitting baseline...
2021-06-04 14:00:01 | [train_policy] epoch #913 | Saving snapshot...
2021-06-04 14:00:01 | [train_policy] epoch #913 | Saved
2021-06-04 14:00:01 | [train_policy] epoch #913 | Time 732.86 s
2021-06-04 14:00:01 | [train_policy] epoch #913 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.290939
Evaluation/AverageDiscountedReturn          -41.9054
Evaluation/AverageReturn                    -41.9054
Evaluation/CompletionRate                     0
Evaluation/Iteration                        913
Evaluation/MaxReturn                        -31.5464
Evaluation/MinReturn                        -80.7474
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2309
Extras/EpisodeRewardMean                    -41.6675
LinearFeatureBaseline/ExplainedVariance       0.828193
PolicyExecTime                                0.225874
ProcessExecTime                               0.0319328
TotalEnvSteps                            924968
policy/Entropy                               -2.29992
policy/KL                                     0.00650132
policy/KLBefore                               0
policy/LossAfter                             -0.0155079
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.100267
policy/dLoss                                  0.0155079
---------------------------------------  ----------------
2021-06-04 14:00:01 | [train_policy] epoch #914 | Obtaining samples for iteration 914...
2021-06-04 14:00:01 | [train_policy] epoch #914 | Logging diagnostics...
2021-06-04 14:00:01 | [train_policy] epoch #914 | Optimizing policy...
2021-06-04 14:00:01 | [train_policy] epoch #914 | Computing loss before
2021-06-04 14:00:01 | [train_policy] epoch #914 | Computing KL before
2021-06-04 14:00:01 | [train_policy] epoch #914 | Optimizing
2021-06-04 14:00:01 | [train_policy] epoch #914 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:01 | [train_policy] epoch #914 | computing loss before
2021-06-04 14:00:01 | [train_policy] epoch #914 | computing gradient
2021-06-04 14:00:01 | [train_policy] epoch #914 | gradient computed
2021-06-04 14:00:01 | [train_policy] epoch #914 | computing descent direction
2021-06-04 14:00:01 | [train_policy] epoch #914 | descent direction computed
2021-06-04 14:00:01 | [train_policy] epoch #914 | backtrack iters: 1
2021-06-04 14:00:01 | [train_policy] epoch #914 | optimization finished
2021-06-04 14:00:01 | [train_policy] epoch #914 | Computing KL after
2021-06-04 14:00:01 | [train_policy] epoch #914 | Computing loss after
2021-06-04 14:00:01 | [train_policy] epoch #914 | Fitting baseline...
2021-06-04 14:00:01 | [train_policy] epoch #914 | Saving snapshot...
2021-06-04 14:00:01 | [train_policy] epoch #914 | Saved
2021-06-04 14:00:01 | [train_policy] epoch #914 | Time 733.67 s
2021-06-04 14:00:01 | [train_policy] epoch #914 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.283383
Evaluation/AverageDiscountedReturn          -40.4091
Evaluation/AverageReturn                    -40.4091
Evaluation/CompletionRate                     0
Evaluation/Iteration                        914
Evaluation/MaxReturn                        -28.4524
Evaluation/MinReturn                        -62.9751
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.06866
Extras/EpisodeRewardMean                    -40.5703
LinearFeatureBaseline/ExplainedVariance       0.816887
PolicyExecTime                                0.22353
ProcessExecTime                               0.03125
TotalEnvSteps                            925980
policy/Entropy                               -2.29769
policy/KL                                     0.00645688
policy/KLBefore                               0
policy/LossAfter                             -0.0274332
policy/LossBefore                            -8.2457e-10
policy/Perplexity                             0.100491
policy/dLoss                                  0.0274332
---------------------------------------  ---------------
2021-06-04 14:00:01 | [train_policy] epoch #915 | Obtaining samples for iteration 915...
2021-06-04 14:00:02 | [train_policy] epoch #915 | Logging diagnostics...
2021-06-04 14:00:02 | [train_policy] epoch #915 | Optimizing policy...
2021-06-04 14:00:02 | [train_policy] epoch #915 | Computing loss before
2021-06-04 14:00:02 | [train_policy] epoch #915 | Computing KL before
2021-06-04 14:00:02 | [train_policy] epoch #915 | Optimizing
2021-06-04 14:00:02 | [train_policy] epoch #915 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:02 | [train_policy] epoch #915 | computing loss before
2021-06-04 14:00:02 | [train_policy] epoch #915 | computing gradient
2021-06-04 14:00:02 | [train_policy] epoch #915 | gradient computed
2021-06-04 14:00:02 | [train_policy] epoch #915 | computing descent direction
2021-06-04 14:00:02 | [train_policy] epoch #915 | descent direction computed
2021-06-04 14:00:02 | [train_policy] epoch #915 | backtrack iters: 1
2021-06-04 14:00:02 | [train_policy] epoch #915 | optimization finished
2021-06-04 14:00:02 | [train_policy] epoch #915 | Computing KL after
2021-06-04 14:00:02 | [train_policy] epoch #915 | Computing loss after
2021-06-04 14:00:02 | [train_policy] epoch #915 | Fitting baseline...
2021-06-04 14:00:02 | [train_policy] epoch #915 | Saving snapshot...
2021-06-04 14:00:02 | [train_policy] epoch #915 | Saved
2021-06-04 14:00:02 | [train_policy] epoch #915 | Time 734.49 s
2021-06-04 14:00:02 | [train_policy] epoch #915 | EpochTime 0.80 s
---------------------------------------  ----------------
EnvExecTime                                   0.288589
Evaluation/AverageDiscountedReturn          -41.3303
Evaluation/AverageReturn                    -41.3303
Evaluation/CompletionRate                     0
Evaluation/Iteration                        915
Evaluation/MaxReturn                        -28.0978
Evaluation/MinReturn                        -63.8497
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.41505
Extras/EpisodeRewardMean                    -41.4598
LinearFeatureBaseline/ExplainedVariance       0.887361
PolicyExecTime                                0.241361
ProcessExecTime                               0.0317852
TotalEnvSteps                            926992
policy/Entropy                               -2.32253
policy/KL                                     0.00668155
policy/KLBefore                               0
policy/LossAfter                             -0.0113777
policy/LossBefore                            -1.17796e-08
policy/Perplexity                             0.0980255
policy/dLoss                                  0.0113776
---------------------------------------  ----------------
2021-06-04 14:00:02 | [train_policy] epoch #916 | Obtaining samples for iteration 916...
2021-06-04 14:00:03 | [train_policy] epoch #916 | Logging diagnostics...
2021-06-04 14:00:03 | [train_policy] epoch #916 | Optimizing policy...
2021-06-04 14:00:03 | [train_policy] epoch #916 | Computing loss before
2021-06-04 14:00:03 | [train_policy] epoch #916 | Computing KL before
2021-06-04 14:00:03 | [train_policy] epoch #916 | Optimizing
2021-06-04 14:00:03 | [train_policy] epoch #916 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:03 | [train_policy] epoch #916 | computing loss before
2021-06-04 14:00:03 | [train_policy] epoch #916 | computing gradient
2021-06-04 14:00:03 | [train_policy] epoch #916 | gradient computed
2021-06-04 14:00:03 | [train_policy] epoch #916 | computing descent direction
2021-06-04 14:00:03 | [train_policy] epoch #916 | descent direction computed
2021-06-04 14:00:03 | [train_policy] epoch #916 | backtrack iters: 1
2021-06-04 14:00:03 | [train_policy] epoch #916 | optimization finished
2021-06-04 14:00:03 | [train_policy] epoch #916 | Computing KL after
2021-06-04 14:00:03 | [train_policy] epoch #916 | Computing loss after
2021-06-04 14:00:03 | [train_policy] epoch #916 | Fitting baseline...
2021-06-04 14:00:03 | [train_policy] epoch #916 | Saving snapshot...
2021-06-04 14:00:03 | [train_policy] epoch #916 | Saved
2021-06-04 14:00:03 | [train_policy] epoch #916 | Time 735.31 s
2021-06-04 14:00:03 | [train_policy] epoch #916 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285106
Evaluation/AverageDiscountedReturn          -40.3702
Evaluation/AverageReturn                    -40.3702
Evaluation/CompletionRate                     0
Evaluation/Iteration                        916
Evaluation/MaxReturn                        -28.239
Evaluation/MinReturn                        -63.8299
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.72017
Extras/EpisodeRewardMean                    -40.394
LinearFeatureBaseline/ExplainedVariance       0.866875
PolicyExecTime                                0.231867
ProcessExecTime                               0.0313179
TotalEnvSteps                            928004
policy/Entropy                               -2.32901
policy/KL                                     0.00648099
policy/KLBefore                               0
policy/LossAfter                             -0.0201991
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.0973917
policy/dLoss                                  0.0201991
---------------------------------------  ----------------
2021-06-04 14:00:03 | [train_policy] epoch #917 | Obtaining samples for iteration 917...
2021-06-04 14:00:04 | [train_policy] epoch #917 | Logging diagnostics...
2021-06-04 14:00:04 | [train_policy] epoch #917 | Optimizing policy...
2021-06-04 14:00:04 | [train_policy] epoch #917 | Computing loss before
2021-06-04 14:00:04 | [train_policy] epoch #917 | Computing KL before
2021-06-04 14:00:04 | [train_policy] epoch #917 | Optimizing
2021-06-04 14:00:04 | [train_policy] epoch #917 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:04 | [train_policy] epoch #917 | computing loss before
2021-06-04 14:00:04 | [train_policy] epoch #917 | computing gradient
2021-06-04 14:00:04 | [train_policy] epoch #917 | gradient computed
2021-06-04 14:00:04 | [train_policy] epoch #917 | computing descent direction
2021-06-04 14:00:04 | [train_policy] epoch #917 | descent direction computed
2021-06-04 14:00:04 | [train_policy] epoch #917 | backtrack iters: 0
2021-06-04 14:00:04 | [train_policy] epoch #917 | optimization finished
2021-06-04 14:00:04 | [train_policy] epoch #917 | Computing KL after
2021-06-04 14:00:04 | [train_policy] epoch #917 | Computing loss after
2021-06-04 14:00:04 | [train_policy] epoch #917 | Fitting baseline...
2021-06-04 14:00:04 | [train_policy] epoch #917 | Saving snapshot...
2021-06-04 14:00:04 | [train_policy] epoch #917 | Saved
2021-06-04 14:00:04 | [train_policy] epoch #917 | Time 736.11 s
2021-06-04 14:00:04 | [train_policy] epoch #917 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284149
Evaluation/AverageDiscountedReturn          -41.2466
Evaluation/AverageReturn                    -41.2466
Evaluation/CompletionRate                     0
Evaluation/Iteration                        917
Evaluation/MaxReturn                        -27.821
Evaluation/MinReturn                        -82.3518
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.81244
Extras/EpisodeRewardMean                    -40.7894
LinearFeatureBaseline/ExplainedVariance       0.835595
PolicyExecTime                                0.22566
ProcessExecTime                               0.0311973
TotalEnvSteps                            929016
policy/Entropy                               -2.33787
policy/KL                                     0.00999135
policy/KLBefore                               0
policy/LossAfter                             -0.0243417
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.0965329
policy/dLoss                                  0.0243417
---------------------------------------  ----------------
2021-06-04 14:00:04 | [train_policy] epoch #918 | Obtaining samples for iteration 918...
2021-06-04 14:00:05 | [train_policy] epoch #918 | Logging diagnostics...
2021-06-04 14:00:05 | [train_policy] epoch #918 | Optimizing policy...
2021-06-04 14:00:05 | [train_policy] epoch #918 | Computing loss before
2021-06-04 14:00:05 | [train_policy] epoch #918 | Computing KL before
2021-06-04 14:00:05 | [train_policy] epoch #918 | Optimizing
2021-06-04 14:00:05 | [train_policy] epoch #918 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:05 | [train_policy] epoch #918 | computing loss before
2021-06-04 14:00:05 | [train_policy] epoch #918 | computing gradient
2021-06-04 14:00:05 | [train_policy] epoch #918 | gradient computed
2021-06-04 14:00:05 | [train_policy] epoch #918 | computing descent direction
2021-06-04 14:00:05 | [train_policy] epoch #918 | descent direction computed
2021-06-04 14:00:05 | [train_policy] epoch #918 | backtrack iters: 1
2021-06-04 14:00:05 | [train_policy] epoch #918 | optimization finished
2021-06-04 14:00:05 | [train_policy] epoch #918 | Computing KL after
2021-06-04 14:00:05 | [train_policy] epoch #918 | Computing loss after
2021-06-04 14:00:05 | [train_policy] epoch #918 | Fitting baseline...
2021-06-04 14:00:05 | [train_policy] epoch #918 | Saving snapshot...
2021-06-04 14:00:05 | [train_policy] epoch #918 | Saved
2021-06-04 14:00:05 | [train_policy] epoch #918 | Time 736.91 s
2021-06-04 14:00:05 | [train_policy] epoch #918 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283917
Evaluation/AverageDiscountedReturn          -39.9776
Evaluation/AverageReturn                    -39.9776
Evaluation/CompletionRate                     0
Evaluation/Iteration                        918
Evaluation/MaxReturn                        -28.3644
Evaluation/MinReturn                        -63.8286
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.22474
Extras/EpisodeRewardMean                    -40.3468
LinearFeatureBaseline/ExplainedVariance       0.816399
PolicyExecTime                                0.221158
ProcessExecTime                               0.0312335
TotalEnvSteps                            930028
policy/Entropy                               -2.33063
policy/KL                                     0.00642332
policy/KLBefore                               0
policy/LossAfter                             -0.0314655
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             0.0972341
policy/dLoss                                  0.0314655
---------------------------------------  ----------------
2021-06-04 14:00:05 | [train_policy] epoch #919 | Obtaining samples for iteration 919...
2021-06-04 14:00:05 | [train_policy] epoch #919 | Logging diagnostics...
2021-06-04 14:00:05 | [train_policy] epoch #919 | Optimizing policy...
2021-06-04 14:00:05 | [train_policy] epoch #919 | Computing loss before
2021-06-04 14:00:05 | [train_policy] epoch #919 | Computing KL before
2021-06-04 14:00:05 | [train_policy] epoch #919 | Optimizing
2021-06-04 14:00:05 | [train_policy] epoch #919 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:05 | [train_policy] epoch #919 | computing loss before
2021-06-04 14:00:05 | [train_policy] epoch #919 | computing gradient
2021-06-04 14:00:05 | [train_policy] epoch #919 | gradient computed
2021-06-04 14:00:05 | [train_policy] epoch #919 | computing descent direction
2021-06-04 14:00:05 | [train_policy] epoch #919 | descent direction computed
2021-06-04 14:00:05 | [train_policy] epoch #919 | backtrack iters: 0
2021-06-04 14:00:05 | [train_policy] epoch #919 | optimization finished
2021-06-04 14:00:05 | [train_policy] epoch #919 | Computing KL after
2021-06-04 14:00:05 | [train_policy] epoch #919 | Computing loss after
2021-06-04 14:00:05 | [train_policy] epoch #919 | Fitting baseline...
2021-06-04 14:00:05 | [train_policy] epoch #919 | Saving snapshot...
2021-06-04 14:00:05 | [train_policy] epoch #919 | Saved
2021-06-04 14:00:05 | [train_policy] epoch #919 | Time 737.70 s
2021-06-04 14:00:05 | [train_policy] epoch #919 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28272
Evaluation/AverageDiscountedReturn          -39.5999
Evaluation/AverageReturn                    -39.5999
Evaluation/CompletionRate                     0
Evaluation/Iteration                        919
Evaluation/MaxReturn                        -28.0698
Evaluation/MinReturn                        -81.4085
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.45511
Extras/EpisodeRewardMean                    -39.506
LinearFeatureBaseline/ExplainedVariance       0.854047
PolicyExecTime                                0.229142
ProcessExecTime                               0.0309765
TotalEnvSteps                            931040
policy/Entropy                               -2.32492
policy/KL                                     0.009964
policy/KLBefore                               0
policy/LossAfter                             -0.0215557
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.0977917
policy/dLoss                                  0.0215557
---------------------------------------  ----------------
2021-06-04 14:00:06 | [train_policy] epoch #920 | Obtaining samples for iteration 920...
2021-06-04 14:00:06 | [train_policy] epoch #920 | Logging diagnostics...
2021-06-04 14:00:06 | [train_policy] epoch #920 | Optimizing policy...
2021-06-04 14:00:06 | [train_policy] epoch #920 | Computing loss before
2021-06-04 14:00:06 | [train_policy] epoch #920 | Computing KL before
2021-06-04 14:00:06 | [train_policy] epoch #920 | Optimizing
2021-06-04 14:00:06 | [train_policy] epoch #920 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:06 | [train_policy] epoch #920 | computing loss before
2021-06-04 14:00:06 | [train_policy] epoch #920 | computing gradient
2021-06-04 14:00:06 | [train_policy] epoch #920 | gradient computed
2021-06-04 14:00:06 | [train_policy] epoch #920 | computing descent direction
2021-06-04 14:00:06 | [train_policy] epoch #920 | descent direction computed
2021-06-04 14:00:06 | [train_policy] epoch #920 | backtrack iters: 1
2021-06-04 14:00:06 | [train_policy] epoch #920 | optimization finished
2021-06-04 14:00:06 | [train_policy] epoch #920 | Computing KL after
2021-06-04 14:00:06 | [train_policy] epoch #920 | Computing loss after
2021-06-04 14:00:06 | [train_policy] epoch #920 | Fitting baseline...
2021-06-04 14:00:06 | [train_policy] epoch #920 | Saving snapshot...
2021-06-04 14:00:06 | [train_policy] epoch #920 | Saved
2021-06-04 14:00:06 | [train_policy] epoch #920 | Time 738.50 s
2021-06-04 14:00:06 | [train_policy] epoch #920 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.285192
Evaluation/AverageDiscountedReturn          -40.7052
Evaluation/AverageReturn                    -40.7052
Evaluation/CompletionRate                     0
Evaluation/Iteration                        920
Evaluation/MaxReturn                        -27.4985
Evaluation/MinReturn                        -63.7984
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.00287
Extras/EpisodeRewardMean                    -40.4001
LinearFeatureBaseline/ExplainedVariance       0.834912
PolicyExecTime                                0.213348
ProcessExecTime                               0.0312886
TotalEnvSteps                            932052
policy/Entropy                               -2.32429
policy/KL                                     0.0064117
policy/KLBefore                               0
policy/LossAfter                             -0.025556
policy/LossBefore                            -1.7316e-08
policy/Perplexity                             0.0978528
policy/dLoss                                  0.025556
---------------------------------------  ---------------
2021-06-04 14:00:06 | [train_policy] epoch #921 | Obtaining samples for iteration 921...
2021-06-04 14:00:07 | [train_policy] epoch #921 | Logging diagnostics...
2021-06-04 14:00:07 | [train_policy] epoch #921 | Optimizing policy...
2021-06-04 14:00:07 | [train_policy] epoch #921 | Computing loss before
2021-06-04 14:00:07 | [train_policy] epoch #921 | Computing KL before
2021-06-04 14:00:07 | [train_policy] epoch #921 | Optimizing
2021-06-04 14:00:07 | [train_policy] epoch #921 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:07 | [train_policy] epoch #921 | computing loss before
2021-06-04 14:00:07 | [train_policy] epoch #921 | computing gradient
2021-06-04 14:00:07 | [train_policy] epoch #921 | gradient computed
2021-06-04 14:00:07 | [train_policy] epoch #921 | computing descent direction
2021-06-04 14:00:07 | [train_policy] epoch #921 | descent direction computed
2021-06-04 14:00:07 | [train_policy] epoch #921 | backtrack iters: 1
2021-06-04 14:00:07 | [train_policy] epoch #921 | optimization finished
2021-06-04 14:00:07 | [train_policy] epoch #921 | Computing KL after
2021-06-04 14:00:07 | [train_policy] epoch #921 | Computing loss after
2021-06-04 14:00:07 | [train_policy] epoch #921 | Fitting baseline...
2021-06-04 14:00:07 | [train_policy] epoch #921 | Saving snapshot...
2021-06-04 14:00:07 | [train_policy] epoch #921 | Saved
2021-06-04 14:00:07 | [train_policy] epoch #921 | Time 739.32 s
2021-06-04 14:00:07 | [train_policy] epoch #921 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286193
Evaluation/AverageDiscountedReturn          -40.2622
Evaluation/AverageReturn                    -40.2622
Evaluation/CompletionRate                     0
Evaluation/Iteration                        921
Evaluation/MaxReturn                        -27.9467
Evaluation/MinReturn                        -63.0195
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.21305
Extras/EpisodeRewardMean                    -40.221
LinearFeatureBaseline/ExplainedVariance       0.875719
PolicyExecTime                                0.223709
ProcessExecTime                               0.0313566
TotalEnvSteps                            933064
policy/Entropy                               -2.32388
policy/KL                                     0.00651524
policy/KLBefore                               0
policy/LossAfter                             -0.0120993
policy/LossBefore                             1.20152e-08
policy/Perplexity                             0.0978931
policy/dLoss                                  0.0120993
---------------------------------------  ----------------
2021-06-04 14:00:07 | [train_policy] epoch #922 | Obtaining samples for iteration 922...
2021-06-04 14:00:08 | [train_policy] epoch #922 | Logging diagnostics...
2021-06-04 14:00:08 | [train_policy] epoch #922 | Optimizing policy...
2021-06-04 14:00:08 | [train_policy] epoch #922 | Computing loss before
2021-06-04 14:00:08 | [train_policy] epoch #922 | Computing KL before
2021-06-04 14:00:08 | [train_policy] epoch #922 | Optimizing
2021-06-04 14:00:08 | [train_policy] epoch #922 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:08 | [train_policy] epoch #922 | computing loss before
2021-06-04 14:00:08 | [train_policy] epoch #922 | computing gradient
2021-06-04 14:00:08 | [train_policy] epoch #922 | gradient computed
2021-06-04 14:00:08 | [train_policy] epoch #922 | computing descent direction
2021-06-04 14:00:08 | [train_policy] epoch #922 | descent direction computed
2021-06-04 14:00:08 | [train_policy] epoch #922 | backtrack iters: 0
2021-06-04 14:00:08 | [train_policy] epoch #922 | optimization finished
2021-06-04 14:00:08 | [train_policy] epoch #922 | Computing KL after
2021-06-04 14:00:08 | [train_policy] epoch #922 | Computing loss after
2021-06-04 14:00:08 | [train_policy] epoch #922 | Fitting baseline...
2021-06-04 14:00:08 | [train_policy] epoch #922 | Saving snapshot...
2021-06-04 14:00:08 | [train_policy] epoch #922 | Saved
2021-06-04 14:00:08 | [train_policy] epoch #922 | Time 740.12 s
2021-06-04 14:00:08 | [train_policy] epoch #922 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285432
Evaluation/AverageDiscountedReturn          -41.2746
Evaluation/AverageReturn                    -41.2746
Evaluation/CompletionRate                     0
Evaluation/Iteration                        922
Evaluation/MaxReturn                        -27.6396
Evaluation/MinReturn                        -77.9441
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.23843
Extras/EpisodeRewardMean                    -41.0014
LinearFeatureBaseline/ExplainedVariance       0.868235
PolicyExecTime                                0.231041
ProcessExecTime                               0.0313609
TotalEnvSteps                            934076
policy/Entropy                               -2.32785
policy/KL                                     0.00975672
policy/KLBefore                               0
policy/LossAfter                             -0.016868
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.0975053
policy/dLoss                                  0.016868
---------------------------------------  ----------------
2021-06-04 14:00:08 | [train_policy] epoch #923 | Obtaining samples for iteration 923...
2021-06-04 14:00:09 | [train_policy] epoch #923 | Logging diagnostics...
2021-06-04 14:00:09 | [train_policy] epoch #923 | Optimizing policy...
2021-06-04 14:00:09 | [train_policy] epoch #923 | Computing loss before
2021-06-04 14:00:09 | [train_policy] epoch #923 | Computing KL before
2021-06-04 14:00:09 | [train_policy] epoch #923 | Optimizing
2021-06-04 14:00:09 | [train_policy] epoch #923 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:09 | [train_policy] epoch #923 | computing loss before
2021-06-04 14:00:09 | [train_policy] epoch #923 | computing gradient
2021-06-04 14:00:09 | [train_policy] epoch #923 | gradient computed
2021-06-04 14:00:09 | [train_policy] epoch #923 | computing descent direction
2021-06-04 14:00:09 | [train_policy] epoch #923 | descent direction computed
2021-06-04 14:00:09 | [train_policy] epoch #923 | backtrack iters: 1
2021-06-04 14:00:09 | [train_policy] epoch #923 | optimization finished
2021-06-04 14:00:09 | [train_policy] epoch #923 | Computing KL after
2021-06-04 14:00:09 | [train_policy] epoch #923 | Computing loss after
2021-06-04 14:00:09 | [train_policy] epoch #923 | Fitting baseline...
2021-06-04 14:00:09 | [train_policy] epoch #923 | Saving snapshot...
2021-06-04 14:00:09 | [train_policy] epoch #923 | Saved
2021-06-04 14:00:09 | [train_policy] epoch #923 | Time 740.92 s
2021-06-04 14:00:09 | [train_policy] epoch #923 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286836
Evaluation/AverageDiscountedReturn          -38.4599
Evaluation/AverageReturn                    -38.4599
Evaluation/CompletionRate                     0
Evaluation/Iteration                        923
Evaluation/MaxReturn                        -28.9663
Evaluation/MinReturn                        -62.6493
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.1793
Extras/EpisodeRewardMean                    -38.8808
LinearFeatureBaseline/ExplainedVariance       0.871027
PolicyExecTime                                0.225035
ProcessExecTime                               0.031431
TotalEnvSteps                            935088
policy/Entropy                               -2.34275
policy/KL                                     0.00659572
policy/KLBefore                               0
policy/LossAfter                             -0.0172416
policy/LossBefore                             3.10981e-08
policy/Perplexity                             0.0960628
policy/dLoss                                  0.0172417
---------------------------------------  ----------------
2021-06-04 14:00:09 | [train_policy] epoch #924 | Obtaining samples for iteration 924...
2021-06-04 14:00:09 | [train_policy] epoch #924 | Logging diagnostics...
2021-06-04 14:00:09 | [train_policy] epoch #924 | Optimizing policy...
2021-06-04 14:00:09 | [train_policy] epoch #924 | Computing loss before
2021-06-04 14:00:09 | [train_policy] epoch #924 | Computing KL before
2021-06-04 14:00:09 | [train_policy] epoch #924 | Optimizing
2021-06-04 14:00:09 | [train_policy] epoch #924 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:09 | [train_policy] epoch #924 | computing loss before
2021-06-04 14:00:09 | [train_policy] epoch #924 | computing gradient
2021-06-04 14:00:09 | [train_policy] epoch #924 | gradient computed
2021-06-04 14:00:09 | [train_policy] epoch #924 | computing descent direction
2021-06-04 14:00:09 | [train_policy] epoch #924 | descent direction computed
2021-06-04 14:00:09 | [train_policy] epoch #924 | backtrack iters: 1
2021-06-04 14:00:09 | [train_policy] epoch #924 | optimization finished
2021-06-04 14:00:09 | [train_policy] epoch #924 | Computing KL after
2021-06-04 14:00:09 | [train_policy] epoch #924 | Computing loss after
2021-06-04 14:00:09 | [train_policy] epoch #924 | Fitting baseline...
2021-06-04 14:00:09 | [train_policy] epoch #924 | Saving snapshot...
2021-06-04 14:00:09 | [train_policy] epoch #924 | Saved
2021-06-04 14:00:09 | [train_policy] epoch #924 | Time 741.72 s
2021-06-04 14:00:09 | [train_policy] epoch #924 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28451
Evaluation/AverageDiscountedReturn          -42.0527
Evaluation/AverageReturn                    -42.0527
Evaluation/CompletionRate                     0
Evaluation/Iteration                        924
Evaluation/MaxReturn                        -28.6126
Evaluation/MinReturn                        -86.7168
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.56238
Extras/EpisodeRewardMean                    -41.6586
LinearFeatureBaseline/ExplainedVariance       0.83655
PolicyExecTime                                0.230363
ProcessExecTime                               0.0312634
TotalEnvSteps                            936100
policy/Entropy                               -2.35669
policy/KL                                     0.00652686
policy/KLBefore                               0
policy/LossAfter                             -0.0248648
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.0947337
policy/dLoss                                  0.0248648
---------------------------------------  ----------------
2021-06-04 14:00:10 | [train_policy] epoch #925 | Obtaining samples for iteration 925...
2021-06-04 14:00:10 | [train_policy] epoch #925 | Logging diagnostics...
2021-06-04 14:00:10 | [train_policy] epoch #925 | Optimizing policy...
2021-06-04 14:00:10 | [train_policy] epoch #925 | Computing loss before
2021-06-04 14:00:10 | [train_policy] epoch #925 | Computing KL before
2021-06-04 14:00:10 | [train_policy] epoch #925 | Optimizing
2021-06-04 14:00:10 | [train_policy] epoch #925 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:10 | [train_policy] epoch #925 | computing loss before
2021-06-04 14:00:10 | [train_policy] epoch #925 | computing gradient
2021-06-04 14:00:10 | [train_policy] epoch #925 | gradient computed
2021-06-04 14:00:10 | [train_policy] epoch #925 | computing descent direction
2021-06-04 14:00:10 | [train_policy] epoch #925 | descent direction computed
2021-06-04 14:00:10 | [train_policy] epoch #925 | backtrack iters: 1
2021-06-04 14:00:10 | [train_policy] epoch #925 | optimization finished
2021-06-04 14:00:10 | [train_policy] epoch #925 | Computing KL after
2021-06-04 14:00:10 | [train_policy] epoch #925 | Computing loss after
2021-06-04 14:00:10 | [train_policy] epoch #925 | Fitting baseline...
2021-06-04 14:00:10 | [train_policy] epoch #925 | Saving snapshot...
2021-06-04 14:00:10 | [train_policy] epoch #925 | Saved
2021-06-04 14:00:10 | [train_policy] epoch #925 | Time 742.51 s
2021-06-04 14:00:10 | [train_policy] epoch #925 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284847
Evaluation/AverageDiscountedReturn          -42.3779
Evaluation/AverageReturn                    -42.3779
Evaluation/CompletionRate                     0
Evaluation/Iteration                        925
Evaluation/MaxReturn                        -28.2349
Evaluation/MinReturn                        -81.366
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.70846
Extras/EpisodeRewardMean                    -42.1297
LinearFeatureBaseline/ExplainedVariance       0.863155
PolicyExecTime                                0.222116
ProcessExecTime                               0.0311103
TotalEnvSteps                            937112
policy/Entropy                               -2.35669
policy/KL                                     0.00640328
policy/KLBefore                               0
policy/LossAfter                             -0.0159371
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.0947334
policy/dLoss                                  0.0159371
---------------------------------------  ----------------
2021-06-04 14:00:10 | [train_policy] epoch #926 | Obtaining samples for iteration 926...
2021-06-04 14:00:11 | [train_policy] epoch #926 | Logging diagnostics...
2021-06-04 14:00:11 | [train_policy] epoch #926 | Optimizing policy...
2021-06-04 14:00:11 | [train_policy] epoch #926 | Computing loss before
2021-06-04 14:00:11 | [train_policy] epoch #926 | Computing KL before
2021-06-04 14:00:11 | [train_policy] epoch #926 | Optimizing
2021-06-04 14:00:11 | [train_policy] epoch #926 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:11 | [train_policy] epoch #926 | computing loss before
2021-06-04 14:00:11 | [train_policy] epoch #926 | computing gradient
2021-06-04 14:00:11 | [train_policy] epoch #926 | gradient computed
2021-06-04 14:00:11 | [train_policy] epoch #926 | computing descent direction
2021-06-04 14:00:11 | [train_policy] epoch #926 | descent direction computed
2021-06-04 14:00:11 | [train_policy] epoch #926 | backtrack iters: 1
2021-06-04 14:00:11 | [train_policy] epoch #926 | optimization finished
2021-06-04 14:00:11 | [train_policy] epoch #926 | Computing KL after
2021-06-04 14:00:11 | [train_policy] epoch #926 | Computing loss after
2021-06-04 14:00:11 | [train_policy] epoch #926 | Fitting baseline...
2021-06-04 14:00:11 | [train_policy] epoch #926 | Saving snapshot...
2021-06-04 14:00:11 | [train_policy] epoch #926 | Saved
2021-06-04 14:00:11 | [train_policy] epoch #926 | Time 743.32 s
2021-06-04 14:00:11 | [train_policy] epoch #926 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286145
Evaluation/AverageDiscountedReturn          -40.8919
Evaluation/AverageReturn                    -40.8919
Evaluation/CompletionRate                     0
Evaluation/Iteration                        926
Evaluation/MaxReturn                        -28.2569
Evaluation/MinReturn                        -83.8305
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.59562
Extras/EpisodeRewardMean                    -40.7311
LinearFeatureBaseline/ExplainedVariance       0.853024
PolicyExecTime                                0.229203
ProcessExecTime                               0.0313056
TotalEnvSteps                            938124
policy/Entropy                               -2.3606
policy/KL                                     0.00650036
policy/KLBefore                               0
policy/LossAfter                             -0.0193094
policy/LossBefore                             8.12791e-09
policy/Perplexity                             0.094364
policy/dLoss                                  0.0193094
---------------------------------------  ----------------
2021-06-04 14:00:11 | [train_policy] epoch #927 | Obtaining samples for iteration 927...
2021-06-04 14:00:12 | [train_policy] epoch #927 | Logging diagnostics...
2021-06-04 14:00:12 | [train_policy] epoch #927 | Optimizing policy...
2021-06-04 14:00:12 | [train_policy] epoch #927 | Computing loss before
2021-06-04 14:00:12 | [train_policy] epoch #927 | Computing KL before
2021-06-04 14:00:12 | [train_policy] epoch #927 | Optimizing
2021-06-04 14:00:12 | [train_policy] epoch #927 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:12 | [train_policy] epoch #927 | computing loss before
2021-06-04 14:00:12 | [train_policy] epoch #927 | computing gradient
2021-06-04 14:00:12 | [train_policy] epoch #927 | gradient computed
2021-06-04 14:00:12 | [train_policy] epoch #927 | computing descent direction
2021-06-04 14:00:12 | [train_policy] epoch #927 | descent direction computed
2021-06-04 14:00:12 | [train_policy] epoch #927 | backtrack iters: 0
2021-06-04 14:00:12 | [train_policy] epoch #927 | optimization finished
2021-06-04 14:00:12 | [train_policy] epoch #927 | Computing KL after
2021-06-04 14:00:12 | [train_policy] epoch #927 | Computing loss after
2021-06-04 14:00:12 | [train_policy] epoch #927 | Fitting baseline...
2021-06-04 14:00:12 | [train_policy] epoch #927 | Saving snapshot...
2021-06-04 14:00:12 | [train_policy] epoch #927 | Saved
2021-06-04 14:00:12 | [train_policy] epoch #927 | Time 744.10 s
2021-06-04 14:00:12 | [train_policy] epoch #927 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285666
Evaluation/AverageDiscountedReturn          -41.4287
Evaluation/AverageReturn                    -41.4287
Evaluation/CompletionRate                     0
Evaluation/Iteration                        927
Evaluation/MaxReturn                        -31.283
Evaluation/MinReturn                        -78.6762
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.66113
Extras/EpisodeRewardMean                    -41.6158
LinearFeatureBaseline/ExplainedVariance       0.795191
PolicyExecTime                                0.208624
ProcessExecTime                               0.0313165
TotalEnvSteps                            939136
policy/Entropy                               -2.37245
policy/KL                                     0.00996019
policy/KLBefore                               0
policy/LossAfter                             -0.0220453
policy/LossBefore                            -3.19226e-08
policy/Perplexity                             0.093252
policy/dLoss                                  0.0220452
---------------------------------------  ----------------
2021-06-04 14:00:12 | [train_policy] epoch #928 | Obtaining samples for iteration 928...
2021-06-04 14:00:13 | [train_policy] epoch #928 | Logging diagnostics...
2021-06-04 14:00:13 | [train_policy] epoch #928 | Optimizing policy...
2021-06-04 14:00:13 | [train_policy] epoch #928 | Computing loss before
2021-06-04 14:00:13 | [train_policy] epoch #928 | Computing KL before
2021-06-04 14:00:13 | [train_policy] epoch #928 | Optimizing
2021-06-04 14:00:13 | [train_policy] epoch #928 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:13 | [train_policy] epoch #928 | computing loss before
2021-06-04 14:00:13 | [train_policy] epoch #928 | computing gradient
2021-06-04 14:00:13 | [train_policy] epoch #928 | gradient computed
2021-06-04 14:00:13 | [train_policy] epoch #928 | computing descent direction
2021-06-04 14:00:13 | [train_policy] epoch #928 | descent direction computed
2021-06-04 14:00:13 | [train_policy] epoch #928 | backtrack iters: 1
2021-06-04 14:00:13 | [train_policy] epoch #928 | optimization finished
2021-06-04 14:00:13 | [train_policy] epoch #928 | Computing KL after
2021-06-04 14:00:13 | [train_policy] epoch #928 | Computing loss after
2021-06-04 14:00:13 | [train_policy] epoch #928 | Fitting baseline...
2021-06-04 14:00:13 | [train_policy] epoch #928 | Saving snapshot...
2021-06-04 14:00:13 | [train_policy] epoch #928 | Saved
2021-06-04 14:00:13 | [train_policy] epoch #928 | Time 744.88 s
2021-06-04 14:00:13 | [train_policy] epoch #928 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284239
Evaluation/AverageDiscountedReturn          -41.3416
Evaluation/AverageReturn                    -41.3416
Evaluation/CompletionRate                     0
Evaluation/Iteration                        928
Evaluation/MaxReturn                        -28.4174
Evaluation/MinReturn                        -63.8568
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.08949
Extras/EpisodeRewardMean                    -41.2456
LinearFeatureBaseline/ExplainedVariance       0.868807
PolicyExecTime                                0.213308
ProcessExecTime                               0.0310769
TotalEnvSteps                            940148
policy/Entropy                               -2.37295
policy/KL                                     0.00643439
policy/KLBefore                               0
policy/LossAfter                             -0.0133504
policy/LossBefore                            -1.27219e-08
policy/Perplexity                             0.0932056
policy/dLoss                                  0.0133504
---------------------------------------  ----------------
2021-06-04 14:00:13 | [train_policy] epoch #929 | Obtaining samples for iteration 929...
2021-06-04 14:00:13 | [train_policy] epoch #929 | Logging diagnostics...
2021-06-04 14:00:13 | [train_policy] epoch #929 | Optimizing policy...
2021-06-04 14:00:13 | [train_policy] epoch #929 | Computing loss before
2021-06-04 14:00:13 | [train_policy] epoch #929 | Computing KL before
2021-06-04 14:00:13 | [train_policy] epoch #929 | Optimizing
2021-06-04 14:00:13 | [train_policy] epoch #929 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:13 | [train_policy] epoch #929 | computing loss before
2021-06-04 14:00:13 | [train_policy] epoch #929 | computing gradient
2021-06-04 14:00:13 | [train_policy] epoch #929 | gradient computed
2021-06-04 14:00:13 | [train_policy] epoch #929 | computing descent direction
2021-06-04 14:00:13 | [train_policy] epoch #929 | descent direction computed
2021-06-04 14:00:13 | [train_policy] epoch #929 | backtrack iters: 1
2021-06-04 14:00:13 | [train_policy] epoch #929 | optimization finished
2021-06-04 14:00:13 | [train_policy] epoch #929 | Computing KL after
2021-06-04 14:00:13 | [train_policy] epoch #929 | Computing loss after
2021-06-04 14:00:13 | [train_policy] epoch #929 | Fitting baseline...
2021-06-04 14:00:13 | [train_policy] epoch #929 | Saving snapshot...
2021-06-04 14:00:13 | [train_policy] epoch #929 | Saved
2021-06-04 14:00:13 | [train_policy] epoch #929 | Time 745.69 s
2021-06-04 14:00:13 | [train_policy] epoch #929 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284657
Evaluation/AverageDiscountedReturn          -40.7276
Evaluation/AverageReturn                    -40.7276
Evaluation/CompletionRate                     0
Evaluation/Iteration                        929
Evaluation/MaxReturn                        -28.5837
Evaluation/MinReturn                        -63.7957
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.9929
Extras/EpisodeRewardMean                    -40.8384
LinearFeatureBaseline/ExplainedVariance       0.884231
PolicyExecTime                                0.232228
ProcessExecTime                               0.0312245
TotalEnvSteps                            941160
policy/Entropy                               -2.36316
policy/KL                                     0.00643492
policy/KLBefore                               0
policy/LossAfter                             -0.011808
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.0941222
policy/dLoss                                  0.011808
---------------------------------------  ----------------
2021-06-04 14:00:13 | [train_policy] epoch #930 | Obtaining samples for iteration 930...
2021-06-04 14:00:14 | [train_policy] epoch #930 | Logging diagnostics...
2021-06-04 14:00:14 | [train_policy] epoch #930 | Optimizing policy...
2021-06-04 14:00:14 | [train_policy] epoch #930 | Computing loss before
2021-06-04 14:00:14 | [train_policy] epoch #930 | Computing KL before
2021-06-04 14:00:14 | [train_policy] epoch #930 | Optimizing
2021-06-04 14:00:14 | [train_policy] epoch #930 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:14 | [train_policy] epoch #930 | computing loss before
2021-06-04 14:00:14 | [train_policy] epoch #930 | computing gradient
2021-06-04 14:00:14 | [train_policy] epoch #930 | gradient computed
2021-06-04 14:00:14 | [train_policy] epoch #930 | computing descent direction
2021-06-04 14:00:14 | [train_policy] epoch #930 | descent direction computed
2021-06-04 14:00:14 | [train_policy] epoch #930 | backtrack iters: 0
2021-06-04 14:00:14 | [train_policy] epoch #930 | optimization finished
2021-06-04 14:00:14 | [train_policy] epoch #930 | Computing KL after
2021-06-04 14:00:14 | [train_policy] epoch #930 | Computing loss after
2021-06-04 14:00:14 | [train_policy] epoch #930 | Fitting baseline...
2021-06-04 14:00:14 | [train_policy] epoch #930 | Saving snapshot...
2021-06-04 14:00:14 | [train_policy] epoch #930 | Saved
2021-06-04 14:00:14 | [train_policy] epoch #930 | Time 746.48 s
2021-06-04 14:00:14 | [train_policy] epoch #930 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284726
Evaluation/AverageDiscountedReturn          -42.6973
Evaluation/AverageReturn                    -42.6973
Evaluation/CompletionRate                     0
Evaluation/Iteration                        930
Evaluation/MaxReturn                        -28.4141
Evaluation/MinReturn                        -81.0876
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.5822
Extras/EpisodeRewardMean                    -42.3356
LinearFeatureBaseline/ExplainedVariance       0.861588
PolicyExecTime                                0.217097
ProcessExecTime                               0.031163
TotalEnvSteps                            942172
policy/Entropy                               -2.34756
policy/KL                                     0.00982186
policy/KLBefore                               0
policy/LossAfter                             -0.0173537
policy/LossBefore                            -1.01304e-08
policy/Perplexity                             0.0956025
policy/dLoss                                  0.0173537
---------------------------------------  ----------------
2021-06-04 14:00:14 | [train_policy] epoch #931 | Obtaining samples for iteration 931...
2021-06-04 14:00:15 | [train_policy] epoch #931 | Logging diagnostics...
2021-06-04 14:00:15 | [train_policy] epoch #931 | Optimizing policy...
2021-06-04 14:00:15 | [train_policy] epoch #931 | Computing loss before
2021-06-04 14:00:15 | [train_policy] epoch #931 | Computing KL before
2021-06-04 14:00:15 | [train_policy] epoch #931 | Optimizing
2021-06-04 14:00:15 | [train_policy] epoch #931 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:15 | [train_policy] epoch #931 | computing loss before
2021-06-04 14:00:15 | [train_policy] epoch #931 | computing gradient
2021-06-04 14:00:15 | [train_policy] epoch #931 | gradient computed
2021-06-04 14:00:15 | [train_policy] epoch #931 | computing descent direction
2021-06-04 14:00:15 | [train_policy] epoch #931 | descent direction computed
2021-06-04 14:00:15 | [train_policy] epoch #931 | backtrack iters: 1
2021-06-04 14:00:15 | [train_policy] epoch #931 | optimization finished
2021-06-04 14:00:15 | [train_policy] epoch #931 | Computing KL after
2021-06-04 14:00:15 | [train_policy] epoch #931 | Computing loss after
2021-06-04 14:00:15 | [train_policy] epoch #931 | Fitting baseline...
2021-06-04 14:00:15 | [train_policy] epoch #931 | Saving snapshot...
2021-06-04 14:00:15 | [train_policy] epoch #931 | Saved
2021-06-04 14:00:15 | [train_policy] epoch #931 | Time 747.28 s
2021-06-04 14:00:15 | [train_policy] epoch #931 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.28478
Evaluation/AverageDiscountedReturn          -39.1556
Evaluation/AverageReturn                    -39.1556
Evaluation/CompletionRate                     0
Evaluation/Iteration                        931
Evaluation/MaxReturn                        -27.8351
Evaluation/MinReturn                        -62.9204
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.88609
Extras/EpisodeRewardMean                    -39.2778
LinearFeatureBaseline/ExplainedVariance       0.874987
PolicyExecTime                                0.222588
ProcessExecTime                               0.0311353
TotalEnvSteps                            943184
policy/Entropy                               -2.34939
policy/KL                                     0.00658142
policy/KLBefore                               0
policy/LossAfter                             -0.0174847
policy/LossBefore                             2.7093e-08
policy/Perplexity                             0.0954272
policy/dLoss                                  0.0174848
---------------------------------------  ---------------
2021-06-04 14:00:15 | [train_policy] epoch #932 | Obtaining samples for iteration 932...
2021-06-04 14:00:16 | [train_policy] epoch #932 | Logging diagnostics...
2021-06-04 14:00:16 | [train_policy] epoch #932 | Optimizing policy...
2021-06-04 14:00:16 | [train_policy] epoch #932 | Computing loss before
2021-06-04 14:00:16 | [train_policy] epoch #932 | Computing KL before
2021-06-04 14:00:16 | [train_policy] epoch #932 | Optimizing
2021-06-04 14:00:16 | [train_policy] epoch #932 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:16 | [train_policy] epoch #932 | computing loss before
2021-06-04 14:00:16 | [train_policy] epoch #932 | computing gradient
2021-06-04 14:00:16 | [train_policy] epoch #932 | gradient computed
2021-06-04 14:00:16 | [train_policy] epoch #932 | computing descent direction
2021-06-04 14:00:16 | [train_policy] epoch #932 | descent direction computed
2021-06-04 14:00:16 | [train_policy] epoch #932 | backtrack iters: 1
2021-06-04 14:00:16 | [train_policy] epoch #932 | optimization finished
2021-06-04 14:00:16 | [train_policy] epoch #932 | Computing KL after
2021-06-04 14:00:16 | [train_policy] epoch #932 | Computing loss after
2021-06-04 14:00:16 | [train_policy] epoch #932 | Fitting baseline...
2021-06-04 14:00:16 | [train_policy] epoch #932 | Saving snapshot...
2021-06-04 14:00:16 | [train_policy] epoch #932 | Saved
2021-06-04 14:00:16 | [train_policy] epoch #932 | Time 748.08 s
2021-06-04 14:00:16 | [train_policy] epoch #932 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284806
Evaluation/AverageDiscountedReturn          -41.9966
Evaluation/AverageReturn                    -41.9966
Evaluation/CompletionRate                     0
Evaluation/Iteration                        932
Evaluation/MaxReturn                        -27.9015
Evaluation/MinReturn                        -80.4231
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.80806
Extras/EpisodeRewardMean                    -42.223
LinearFeatureBaseline/ExplainedVariance       0.854888
PolicyExecTime                                0.232111
ProcessExecTime                               0.0312252
TotalEnvSteps                            944196
policy/Entropy                               -2.35489
policy/KL                                     0.00668035
policy/KLBefore                               0
policy/LossAfter                             -0.0198269
policy/LossBefore                             3.53387e-10
policy/Perplexity                             0.0949037
policy/dLoss                                  0.0198269
---------------------------------------  ----------------
2021-06-04 14:00:16 | [train_policy] epoch #933 | Obtaining samples for iteration 933...
2021-06-04 14:00:16 | [train_policy] epoch #933 | Logging diagnostics...
2021-06-04 14:00:16 | [train_policy] epoch #933 | Optimizing policy...
2021-06-04 14:00:16 | [train_policy] epoch #933 | Computing loss before
2021-06-04 14:00:16 | [train_policy] epoch #933 | Computing KL before
2021-06-04 14:00:16 | [train_policy] epoch #933 | Optimizing
2021-06-04 14:00:16 | [train_policy] epoch #933 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:16 | [train_policy] epoch #933 | computing loss before
2021-06-04 14:00:16 | [train_policy] epoch #933 | computing gradient
2021-06-04 14:00:17 | [train_policy] epoch #933 | gradient computed
2021-06-04 14:00:17 | [train_policy] epoch #933 | computing descent direction
2021-06-04 14:00:17 | [train_policy] epoch #933 | descent direction computed
2021-06-04 14:00:17 | [train_policy] epoch #933 | backtrack iters: 0
2021-06-04 14:00:17 | [train_policy] epoch #933 | optimization finished
2021-06-04 14:00:17 | [train_policy] epoch #933 | Computing KL after
2021-06-04 14:00:17 | [train_policy] epoch #933 | Computing loss after
2021-06-04 14:00:17 | [train_policy] epoch #933 | Fitting baseline...
2021-06-04 14:00:17 | [train_policy] epoch #933 | Saving snapshot...
2021-06-04 14:00:17 | [train_policy] epoch #933 | Saved
2021-06-04 14:00:17 | [train_policy] epoch #933 | Time 748.85 s
2021-06-04 14:00:17 | [train_policy] epoch #933 | EpochTime 0.75 s
---------------------------------------  ----------------
EnvExecTime                                   0.284578
Evaluation/AverageDiscountedReturn          -42.2465
Evaluation/AverageReturn                    -42.2465
Evaluation/CompletionRate                     0
Evaluation/Iteration                        933
Evaluation/MaxReturn                        -28.689
Evaluation/MinReturn                        -78.2703
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.6848
Extras/EpisodeRewardMean                    -41.9386
LinearFeatureBaseline/ExplainedVariance       0.865682
PolicyExecTime                                0.209886
ProcessExecTime                               0.0312142
TotalEnvSteps                            945208
policy/Entropy                               -2.34224
policy/KL                                     0.00979969
policy/KLBefore                               0
policy/LossAfter                             -0.0115748
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.0961121
policy/dLoss                                  0.0115748
---------------------------------------  ----------------
2021-06-04 14:00:17 | [train_policy] epoch #934 | Obtaining samples for iteration 934...
2021-06-04 14:00:17 | [train_policy] epoch #934 | Logging diagnostics...
2021-06-04 14:00:17 | [train_policy] epoch #934 | Optimizing policy...
2021-06-04 14:00:17 | [train_policy] epoch #934 | Computing loss before
2021-06-04 14:00:17 | [train_policy] epoch #934 | Computing KL before
2021-06-04 14:00:17 | [train_policy] epoch #934 | Optimizing
2021-06-04 14:00:17 | [train_policy] epoch #934 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:17 | [train_policy] epoch #934 | computing loss before
2021-06-04 14:00:17 | [train_policy] epoch #934 | computing gradient
2021-06-04 14:00:17 | [train_policy] epoch #934 | gradient computed
2021-06-04 14:00:17 | [train_policy] epoch #934 | computing descent direction
2021-06-04 14:00:17 | [train_policy] epoch #934 | descent direction computed
2021-06-04 14:00:17 | [train_policy] epoch #934 | backtrack iters: 0
2021-06-04 14:00:17 | [train_policy] epoch #934 | optimization finished
2021-06-04 14:00:17 | [train_policy] epoch #934 | Computing KL after
2021-06-04 14:00:17 | [train_policy] epoch #934 | Computing loss after
2021-06-04 14:00:17 | [train_policy] epoch #934 | Fitting baseline...
2021-06-04 14:00:17 | [train_policy] epoch #934 | Saving snapshot...
2021-06-04 14:00:17 | [train_policy] epoch #934 | Saved
2021-06-04 14:00:17 | [train_policy] epoch #934 | Time 749.66 s
2021-06-04 14:00:17 | [train_policy] epoch #934 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284092
Evaluation/AverageDiscountedReturn          -41.79
Evaluation/AverageReturn                    -41.79
Evaluation/CompletionRate                     0
Evaluation/Iteration                        934
Evaluation/MaxReturn                        -28.2648
Evaluation/MinReturn                        -78.1564
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.27549
Extras/EpisodeRewardMean                    -42.1906
LinearFeatureBaseline/ExplainedVariance       0.855903
PolicyExecTime                                0.233137
ProcessExecTime                               0.0310047
TotalEnvSteps                            946220
policy/Entropy                               -2.33821
policy/KL                                     0.00980811
policy/KLBefore                               0
policy/LossAfter                             -0.0165579
policy/LossBefore                            -1.88473e-08
policy/Perplexity                             0.0964999
policy/dLoss                                  0.0165579
---------------------------------------  ----------------
2021-06-04 14:00:17 | [train_policy] epoch #935 | Obtaining samples for iteration 935...
2021-06-04 14:00:18 | [train_policy] epoch #935 | Logging diagnostics...
2021-06-04 14:00:18 | [train_policy] epoch #935 | Optimizing policy...
2021-06-04 14:00:18 | [train_policy] epoch #935 | Computing loss before
2021-06-04 14:00:18 | [train_policy] epoch #935 | Computing KL before
2021-06-04 14:00:18 | [train_policy] epoch #935 | Optimizing
2021-06-04 14:00:18 | [train_policy] epoch #935 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:18 | [train_policy] epoch #935 | computing loss before
2021-06-04 14:00:18 | [train_policy] epoch #935 | computing gradient
2021-06-04 14:00:18 | [train_policy] epoch #935 | gradient computed
2021-06-04 14:00:18 | [train_policy] epoch #935 | computing descent direction
2021-06-04 14:00:18 | [train_policy] epoch #935 | descent direction computed
2021-06-04 14:00:18 | [train_policy] epoch #935 | backtrack iters: 1
2021-06-04 14:00:18 | [train_policy] epoch #935 | optimization finished
2021-06-04 14:00:18 | [train_policy] epoch #935 | Computing KL after
2021-06-04 14:00:18 | [train_policy] epoch #935 | Computing loss after
2021-06-04 14:00:18 | [train_policy] epoch #935 | Fitting baseline...
2021-06-04 14:00:18 | [train_policy] epoch #935 | Saving snapshot...
2021-06-04 14:00:18 | [train_policy] epoch #935 | Saved
2021-06-04 14:00:18 | [train_policy] epoch #935 | Time 750.46 s
2021-06-04 14:00:18 | [train_policy] epoch #935 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285099
Evaluation/AverageDiscountedReturn          -41.1451
Evaluation/AverageReturn                    -41.1451
Evaluation/CompletionRate                     0
Evaluation/Iteration                        935
Evaluation/MaxReturn                        -27.9327
Evaluation/MinReturn                        -76.7738
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.71672
Extras/EpisodeRewardMean                    -41.4645
LinearFeatureBaseline/ExplainedVariance       0.853984
PolicyExecTime                                0.220458
ProcessExecTime                               0.0313761
TotalEnvSteps                            947232
policy/Entropy                               -2.34088
policy/KL                                     0.00639983
policy/KLBefore                               0
policy/LossAfter                             -0.017338
policy/LossBefore                             1.24863e-08
policy/Perplexity                             0.0962429
policy/dLoss                                  0.017338
---------------------------------------  ----------------
2021-06-04 14:00:18 | [train_policy] epoch #936 | Obtaining samples for iteration 936...
2021-06-04 14:00:19 | [train_policy] epoch #936 | Logging diagnostics...
2021-06-04 14:00:19 | [train_policy] epoch #936 | Optimizing policy...
2021-06-04 14:00:19 | [train_policy] epoch #936 | Computing loss before
2021-06-04 14:00:19 | [train_policy] epoch #936 | Computing KL before
2021-06-04 14:00:19 | [train_policy] epoch #936 | Optimizing
2021-06-04 14:00:19 | [train_policy] epoch #936 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:19 | [train_policy] epoch #936 | computing loss before
2021-06-04 14:00:19 | [train_policy] epoch #936 | computing gradient
2021-06-04 14:00:19 | [train_policy] epoch #936 | gradient computed
2021-06-04 14:00:19 | [train_policy] epoch #936 | computing descent direction
2021-06-04 14:00:19 | [train_policy] epoch #936 | descent direction computed
2021-06-04 14:00:19 | [train_policy] epoch #936 | backtrack iters: 0
2021-06-04 14:00:19 | [train_policy] epoch #936 | optimization finished
2021-06-04 14:00:19 | [train_policy] epoch #936 | Computing KL after
2021-06-04 14:00:19 | [train_policy] epoch #936 | Computing loss after
2021-06-04 14:00:19 | [train_policy] epoch #936 | Fitting baseline...
2021-06-04 14:00:19 | [train_policy] epoch #936 | Saving snapshot...
2021-06-04 14:00:19 | [train_policy] epoch #936 | Saved
2021-06-04 14:00:19 | [train_policy] epoch #936 | Time 751.26 s
2021-06-04 14:00:19 | [train_policy] epoch #936 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.284029
Evaluation/AverageDiscountedReturn          -39.4445
Evaluation/AverageReturn                    -39.4445
Evaluation/CompletionRate                     0
Evaluation/Iteration                        936
Evaluation/MaxReturn                        -28.1797
Evaluation/MinReturn                        -60.423
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.69329
Extras/EpisodeRewardMean                    -39.2794
LinearFeatureBaseline/ExplainedVariance       0.890188
PolicyExecTime                                0.227904
ProcessExecTime                               0.031225
TotalEnvSteps                            948244
policy/Entropy                               -2.33177
policy/KL                                     0.00972325
policy/KLBefore                               0
policy/LossAfter                             -0.0154591
policy/LossBefore                            -1.6727e-08
policy/Perplexity                             0.0971241
policy/dLoss                                  0.0154591
---------------------------------------  ---------------
2021-06-04 14:00:19 | [train_policy] epoch #937 | Obtaining samples for iteration 937...
2021-06-04 14:00:20 | [train_policy] epoch #937 | Logging diagnostics...
2021-06-04 14:00:20 | [train_policy] epoch #937 | Optimizing policy...
2021-06-04 14:00:20 | [train_policy] epoch #937 | Computing loss before
2021-06-04 14:00:20 | [train_policy] epoch #937 | Computing KL before
2021-06-04 14:00:20 | [train_policy] epoch #937 | Optimizing
2021-06-04 14:00:20 | [train_policy] epoch #937 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:20 | [train_policy] epoch #937 | computing loss before
2021-06-04 14:00:20 | [train_policy] epoch #937 | computing gradient
2021-06-04 14:00:20 | [train_policy] epoch #937 | gradient computed
2021-06-04 14:00:20 | [train_policy] epoch #937 | computing descent direction
2021-06-04 14:00:20 | [train_policy] epoch #937 | descent direction computed
2021-06-04 14:00:20 | [train_policy] epoch #937 | backtrack iters: 0
2021-06-04 14:00:20 | [train_policy] epoch #937 | optimization finished
2021-06-04 14:00:20 | [train_policy] epoch #937 | Computing KL after
2021-06-04 14:00:20 | [train_policy] epoch #937 | Computing loss after
2021-06-04 14:00:20 | [train_policy] epoch #937 | Fitting baseline...
2021-06-04 14:00:20 | [train_policy] epoch #937 | Saving snapshot...
2021-06-04 14:00:20 | [train_policy] epoch #937 | Saved
2021-06-04 14:00:20 | [train_policy] epoch #937 | Time 752.06 s
2021-06-04 14:00:20 | [train_policy] epoch #937 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.283633
Evaluation/AverageDiscountedReturn          -39.1966
Evaluation/AverageReturn                    -39.1966
Evaluation/CompletionRate                     0
Evaluation/Iteration                        937
Evaluation/MaxReturn                        -28.0721
Evaluation/MinReturn                        -78.3092
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.70212
Extras/EpisodeRewardMean                    -39.4452
LinearFeatureBaseline/ExplainedVariance       0.857044
PolicyExecTime                                0.227538
ProcessExecTime                               0.0311015
TotalEnvSteps                            949256
policy/Entropy                               -2.33694
policy/KL                                     0.00995352
policy/KLBefore                               0
policy/LossAfter                             -0.0248684
policy/LossBefore                             8.2457e-09
policy/Perplexity                             0.0966224
policy/dLoss                                  0.0248684
---------------------------------------  ---------------
2021-06-04 14:00:20 | [train_policy] epoch #938 | Obtaining samples for iteration 938...
2021-06-04 14:00:20 | [train_policy] epoch #938 | Logging diagnostics...
2021-06-04 14:00:20 | [train_policy] epoch #938 | Optimizing policy...
2021-06-04 14:00:20 | [train_policy] epoch #938 | Computing loss before
2021-06-04 14:00:20 | [train_policy] epoch #938 | Computing KL before
2021-06-04 14:00:20 | [train_policy] epoch #938 | Optimizing
2021-06-04 14:00:20 | [train_policy] epoch #938 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:20 | [train_policy] epoch #938 | computing loss before
2021-06-04 14:00:20 | [train_policy] epoch #938 | computing gradient
2021-06-04 14:00:21 | [train_policy] epoch #938 | gradient computed
2021-06-04 14:00:21 | [train_policy] epoch #938 | computing descent direction
2021-06-04 14:00:21 | [train_policy] epoch #938 | descent direction computed
2021-06-04 14:00:21 | [train_policy] epoch #938 | backtrack iters: 1
2021-06-04 14:00:21 | [train_policy] epoch #938 | optimization finished
2021-06-04 14:00:21 | [train_policy] epoch #938 | Computing KL after
2021-06-04 14:00:21 | [train_policy] epoch #938 | Computing loss after
2021-06-04 14:00:21 | [train_policy] epoch #938 | Fitting baseline...
2021-06-04 14:00:21 | [train_policy] epoch #938 | Saving snapshot...
2021-06-04 14:00:21 | [train_policy] epoch #938 | Saved
2021-06-04 14:00:21 | [train_policy] epoch #938 | Time 752.86 s
2021-06-04 14:00:21 | [train_policy] epoch #938 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285711
Evaluation/AverageDiscountedReturn          -40.0278
Evaluation/AverageReturn                    -40.0278
Evaluation/CompletionRate                     0
Evaluation/Iteration                        938
Evaluation/MaxReturn                        -27.7945
Evaluation/MinReturn                        -60.9705
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.8129
Extras/EpisodeRewardMean                    -39.8239
LinearFeatureBaseline/ExplainedVariance       0.8995
PolicyExecTime                                0.224015
ProcessExecTime                               0.0313756
TotalEnvSteps                            950268
policy/Entropy                               -2.33572
policy/KL                                     0.00642748
policy/KLBefore                               0
policy/LossAfter                             -0.0127505
policy/LossBefore                            -1.29575e-09
policy/Perplexity                             0.0967409
policy/dLoss                                  0.0127504
---------------------------------------  ----------------
2021-06-04 14:00:21 | [train_policy] epoch #939 | Obtaining samples for iteration 939...
2021-06-04 14:00:21 | [train_policy] epoch #939 | Logging diagnostics...
2021-06-04 14:00:21 | [train_policy] epoch #939 | Optimizing policy...
2021-06-04 14:00:21 | [train_policy] epoch #939 | Computing loss before
2021-06-04 14:00:21 | [train_policy] epoch #939 | Computing KL before
2021-06-04 14:00:21 | [train_policy] epoch #939 | Optimizing
2021-06-04 14:00:21 | [train_policy] epoch #939 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:21 | [train_policy] epoch #939 | computing loss before
2021-06-04 14:00:21 | [train_policy] epoch #939 | computing gradient
2021-06-04 14:00:21 | [train_policy] epoch #939 | gradient computed
2021-06-04 14:00:21 | [train_policy] epoch #939 | computing descent direction
2021-06-04 14:00:21 | [train_policy] epoch #939 | descent direction computed
2021-06-04 14:00:21 | [train_policy] epoch #939 | backtrack iters: 1
2021-06-04 14:00:21 | [train_policy] epoch #939 | optimization finished
2021-06-04 14:00:21 | [train_policy] epoch #939 | Computing KL after
2021-06-04 14:00:21 | [train_policy] epoch #939 | Computing loss after
2021-06-04 14:00:21 | [train_policy] epoch #939 | Fitting baseline...
2021-06-04 14:00:21 | [train_policy] epoch #939 | Saving snapshot...
2021-06-04 14:00:21 | [train_policy] epoch #939 | Saved
2021-06-04 14:00:21 | [train_policy] epoch #939 | Time 753.69 s
2021-06-04 14:00:21 | [train_policy] epoch #939 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284035
Evaluation/AverageDiscountedReturn          -39.5989
Evaluation/AverageReturn                    -39.5989
Evaluation/CompletionRate                     0
Evaluation/Iteration                        939
Evaluation/MaxReturn                        -28.1188
Evaluation/MinReturn                        -64.1765
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.9486
Extras/EpisodeRewardMean                    -39.756
LinearFeatureBaseline/ExplainedVariance       0.889211
PolicyExecTime                                0.241555
ProcessExecTime                               0.0312026
TotalEnvSteps                            951280
policy/Entropy                               -2.3305
policy/KL                                     0.00651119
policy/KLBefore                               0
policy/LossAfter                             -0.00949402
policy/LossBefore                            -1.17796e-09
policy/Perplexity                             0.0972471
policy/dLoss                                  0.00949402
---------------------------------------  ----------------
2021-06-04 14:00:21 | [train_policy] epoch #940 | Obtaining samples for iteration 940...
2021-06-04 14:00:22 | [train_policy] epoch #940 | Logging diagnostics...
2021-06-04 14:00:22 | [train_policy] epoch #940 | Optimizing policy...
2021-06-04 14:00:22 | [train_policy] epoch #940 | Computing loss before
2021-06-04 14:00:22 | [train_policy] epoch #940 | Computing KL before
2021-06-04 14:00:22 | [train_policy] epoch #940 | Optimizing
2021-06-04 14:00:22 | [train_policy] epoch #940 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:22 | [train_policy] epoch #940 | computing loss before
2021-06-04 14:00:22 | [train_policy] epoch #940 | computing gradient
2021-06-04 14:00:22 | [train_policy] epoch #940 | gradient computed
2021-06-04 14:00:22 | [train_policy] epoch #940 | computing descent direction
2021-06-04 14:00:22 | [train_policy] epoch #940 | descent direction computed
2021-06-04 14:00:22 | [train_policy] epoch #940 | backtrack iters: 0
2021-06-04 14:00:22 | [train_policy] epoch #940 | optimization finished
2021-06-04 14:00:22 | [train_policy] epoch #940 | Computing KL after
2021-06-04 14:00:22 | [train_policy] epoch #940 | Computing loss after
2021-06-04 14:00:22 | [train_policy] epoch #940 | Fitting baseline...
2021-06-04 14:00:22 | [train_policy] epoch #940 | Saving snapshot...
2021-06-04 14:00:22 | [train_policy] epoch #940 | Saved
2021-06-04 14:00:22 | [train_policy] epoch #940 | Time 754.49 s
2021-06-04 14:00:22 | [train_policy] epoch #940 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284576
Evaluation/AverageDiscountedReturn          -39.8787
Evaluation/AverageReturn                    -39.8787
Evaluation/CompletionRate                     0
Evaluation/Iteration                        940
Evaluation/MaxReturn                        -28.8735
Evaluation/MinReturn                        -63.7802
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.54323
Extras/EpisodeRewardMean                    -39.8324
LinearFeatureBaseline/ExplainedVariance       0.895538
PolicyExecTime                                0.225398
ProcessExecTime                               0.0313051
TotalEnvSteps                            952292
policy/Entropy                               -2.32276
policy/KL                                     0.00988639
policy/KLBefore                               0
policy/LossAfter                             -0.0207461
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.0980032
policy/dLoss                                  0.0207462
---------------------------------------  ----------------
2021-06-04 14:00:22 | [train_policy] epoch #941 | Obtaining samples for iteration 941...
2021-06-04 14:00:23 | [train_policy] epoch #941 | Logging diagnostics...
2021-06-04 14:00:23 | [train_policy] epoch #941 | Optimizing policy...
2021-06-04 14:00:23 | [train_policy] epoch #941 | Computing loss before
2021-06-04 14:00:23 | [train_policy] epoch #941 | Computing KL before
2021-06-04 14:00:23 | [train_policy] epoch #941 | Optimizing
2021-06-04 14:00:23 | [train_policy] epoch #941 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:23 | [train_policy] epoch #941 | computing loss before
2021-06-04 14:00:23 | [train_policy] epoch #941 | computing gradient
2021-06-04 14:00:23 | [train_policy] epoch #941 | gradient computed
2021-06-04 14:00:23 | [train_policy] epoch #941 | computing descent direction
2021-06-04 14:00:23 | [train_policy] epoch #941 | descent direction computed
2021-06-04 14:00:23 | [train_policy] epoch #941 | backtrack iters: 1
2021-06-04 14:00:23 | [train_policy] epoch #941 | optimization finished
2021-06-04 14:00:23 | [train_policy] epoch #941 | Computing KL after
2021-06-04 14:00:23 | [train_policy] epoch #941 | Computing loss after
2021-06-04 14:00:23 | [train_policy] epoch #941 | Fitting baseline...
2021-06-04 14:00:23 | [train_policy] epoch #941 | Saving snapshot...
2021-06-04 14:00:23 | [train_policy] epoch #941 | Saved
2021-06-04 14:00:23 | [train_policy] epoch #941 | Time 755.30 s
2021-06-04 14:00:23 | [train_policy] epoch #941 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284917
Evaluation/AverageDiscountedReturn          -41.6311
Evaluation/AverageReturn                    -41.6311
Evaluation/CompletionRate                     0
Evaluation/Iteration                        941
Evaluation/MaxReturn                        -28.2271
Evaluation/MinReturn                        -77.3321
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1111
Extras/EpisodeRewardMean                    -41.3849
LinearFeatureBaseline/ExplainedVariance       0.850411
PolicyExecTime                                0.226707
ProcessExecTime                               0.0312886
TotalEnvSteps                            953304
policy/Entropy                               -2.32904
policy/KL                                     0.00650426
policy/KLBefore                               0
policy/LossAfter                             -0.016342
policy/LossBefore                             2.74464e-08
policy/Perplexity                             0.0973895
policy/dLoss                                  0.0163421
---------------------------------------  ----------------
2021-06-04 14:00:23 | [train_policy] epoch #942 | Obtaining samples for iteration 942...
2021-06-04 14:00:24 | [train_policy] epoch #942 | Logging diagnostics...
2021-06-04 14:00:24 | [train_policy] epoch #942 | Optimizing policy...
2021-06-04 14:00:24 | [train_policy] epoch #942 | Computing loss before
2021-06-04 14:00:24 | [train_policy] epoch #942 | Computing KL before
2021-06-04 14:00:24 | [train_policy] epoch #942 | Optimizing
2021-06-04 14:00:24 | [train_policy] epoch #942 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:24 | [train_policy] epoch #942 | computing loss before
2021-06-04 14:00:24 | [train_policy] epoch #942 | computing gradient
2021-06-04 14:00:24 | [train_policy] epoch #942 | gradient computed
2021-06-04 14:00:24 | [train_policy] epoch #942 | computing descent direction
2021-06-04 14:00:24 | [train_policy] epoch #942 | descent direction computed
2021-06-04 14:00:24 | [train_policy] epoch #942 | backtrack iters: 0
2021-06-04 14:00:24 | [train_policy] epoch #942 | optimization finished
2021-06-04 14:00:24 | [train_policy] epoch #942 | Computing KL after
2021-06-04 14:00:24 | [train_policy] epoch #942 | Computing loss after
2021-06-04 14:00:24 | [train_policy] epoch #942 | Fitting baseline...
2021-06-04 14:00:24 | [train_policy] epoch #942 | Saving snapshot...
2021-06-04 14:00:24 | [train_policy] epoch #942 | Saved
2021-06-04 14:00:24 | [train_policy] epoch #942 | Time 756.10 s
2021-06-04 14:00:24 | [train_policy] epoch #942 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283502
Evaluation/AverageDiscountedReturn          -40.3752
Evaluation/AverageReturn                    -40.3752
Evaluation/CompletionRate                     0
Evaluation/Iteration                        942
Evaluation/MaxReturn                        -28.2678
Evaluation/MinReturn                        -78.3182
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.52033
Extras/EpisodeRewardMean                    -39.9628
LinearFeatureBaseline/ExplainedVariance       0.870357
PolicyExecTime                                0.218579
ProcessExecTime                               0.0312433
TotalEnvSteps                            954316
policy/Entropy                               -2.28184
policy/KL                                     0.00942958
policy/KLBefore                               0
policy/LossAfter                             -0.0243848
policy/LossBefore                            -8.01011e-09
policy/Perplexity                             0.102096
policy/dLoss                                  0.0243848
---------------------------------------  ----------------
2021-06-04 14:00:24 | [train_policy] epoch #943 | Obtaining samples for iteration 943...
2021-06-04 14:00:25 | [train_policy] epoch #943 | Logging diagnostics...
2021-06-04 14:00:25 | [train_policy] epoch #943 | Optimizing policy...
2021-06-04 14:00:25 | [train_policy] epoch #943 | Computing loss before
2021-06-04 14:00:25 | [train_policy] epoch #943 | Computing KL before
2021-06-04 14:00:25 | [train_policy] epoch #943 | Optimizing
2021-06-04 14:00:25 | [train_policy] epoch #943 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:25 | [train_policy] epoch #943 | computing loss before
2021-06-04 14:00:25 | [train_policy] epoch #943 | computing gradient
2021-06-04 14:00:25 | [train_policy] epoch #943 | gradient computed
2021-06-04 14:00:25 | [train_policy] epoch #943 | computing descent direction
2021-06-04 14:00:25 | [train_policy] epoch #943 | descent direction computed
2021-06-04 14:00:25 | [train_policy] epoch #943 | backtrack iters: 0
2021-06-04 14:00:25 | [train_policy] epoch #943 | optimization finished
2021-06-04 14:00:25 | [train_policy] epoch #943 | Computing KL after
2021-06-04 14:00:25 | [train_policy] epoch #943 | Computing loss after
2021-06-04 14:00:25 | [train_policy] epoch #943 | Fitting baseline...
2021-06-04 14:00:25 | [train_policy] epoch #943 | Saving snapshot...
2021-06-04 14:00:25 | [train_policy] epoch #943 | Saved
2021-06-04 14:00:25 | [train_policy] epoch #943 | Time 756.89 s
2021-06-04 14:00:25 | [train_policy] epoch #943 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.28358
Evaluation/AverageDiscountedReturn          -41.3981
Evaluation/AverageReturn                    -41.3981
Evaluation/CompletionRate                     0
Evaluation/Iteration                        943
Evaluation/MaxReturn                        -27.8781
Evaluation/MinReturn                        -79.8632
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.09412
Extras/EpisodeRewardMean                    -41.3505
LinearFeatureBaseline/ExplainedVariance       0.878731
PolicyExecTime                                0.226925
ProcessExecTime                               0.0311072
TotalEnvSteps                            955328
policy/Entropy                               -2.25051
policy/KL                                     0.00982162
policy/KLBefore                               0
policy/LossAfter                             -0.0200722
policy/LossBefore                            -1.38999e-08
policy/Perplexity                             0.105346
policy/dLoss                                  0.0200722
---------------------------------------  ----------------
2021-06-04 14:00:25 | [train_policy] epoch #944 | Obtaining samples for iteration 944...
2021-06-04 14:00:25 | [train_policy] epoch #944 | Logging diagnostics...
2021-06-04 14:00:25 | [train_policy] epoch #944 | Optimizing policy...
2021-06-04 14:00:25 | [train_policy] epoch #944 | Computing loss before
2021-06-04 14:00:25 | [train_policy] epoch #944 | Computing KL before
2021-06-04 14:00:25 | [train_policy] epoch #944 | Optimizing
2021-06-04 14:00:25 | [train_policy] epoch #944 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:25 | [train_policy] epoch #944 | computing loss before
2021-06-04 14:00:25 | [train_policy] epoch #944 | computing gradient
2021-06-04 14:00:25 | [train_policy] epoch #944 | gradient computed
2021-06-04 14:00:25 | [train_policy] epoch #944 | computing descent direction
2021-06-04 14:00:25 | [train_policy] epoch #944 | descent direction computed
2021-06-04 14:00:25 | [train_policy] epoch #944 | backtrack iters: 0
2021-06-04 14:00:25 | [train_policy] epoch #944 | optimization finished
2021-06-04 14:00:25 | [train_policy] epoch #944 | Computing KL after
2021-06-04 14:00:25 | [train_policy] epoch #944 | Computing loss after
2021-06-04 14:00:25 | [train_policy] epoch #944 | Fitting baseline...
2021-06-04 14:00:25 | [train_policy] epoch #944 | Saving snapshot...
2021-06-04 14:00:25 | [train_policy] epoch #944 | Saved
2021-06-04 14:00:25 | [train_policy] epoch #944 | Time 757.71 s
2021-06-04 14:00:25 | [train_policy] epoch #944 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284447
Evaluation/AverageDiscountedReturn          -42.076
Evaluation/AverageReturn                    -42.076
Evaluation/CompletionRate                     0
Evaluation/Iteration                        944
Evaluation/MaxReturn                        -28.5392
Evaluation/MinReturn                        -80.754
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.3219
Extras/EpisodeRewardMean                    -41.5681
LinearFeatureBaseline/ExplainedVariance       0.829178
PolicyExecTime                                0.23544
ProcessExecTime                               0.03126
TotalEnvSteps                            956340
policy/Entropy                               -2.25658
policy/KL                                     0.00996406
policy/KLBefore                               0
policy/LossAfter                             -0.0186369
policy/LossBefore                             2.08498e-08
policy/Perplexity                             0.104708
policy/dLoss                                  0.0186369
---------------------------------------  ----------------
2021-06-04 14:00:26 | [train_policy] epoch #945 | Obtaining samples for iteration 945...
2021-06-04 14:00:26 | [train_policy] epoch #945 | Logging diagnostics...
2021-06-04 14:00:26 | [train_policy] epoch #945 | Optimizing policy...
2021-06-04 14:00:26 | [train_policy] epoch #945 | Computing loss before
2021-06-04 14:00:26 | [train_policy] epoch #945 | Computing KL before
2021-06-04 14:00:26 | [train_policy] epoch #945 | Optimizing
2021-06-04 14:00:26 | [train_policy] epoch #945 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:26 | [train_policy] epoch #945 | computing loss before
2021-06-04 14:00:26 | [train_policy] epoch #945 | computing gradient
2021-06-04 14:00:26 | [train_policy] epoch #945 | gradient computed
2021-06-04 14:00:26 | [train_policy] epoch #945 | computing descent direction
2021-06-04 14:00:26 | [train_policy] epoch #945 | descent direction computed
2021-06-04 14:00:26 | [train_policy] epoch #945 | backtrack iters: 1
2021-06-04 14:00:26 | [train_policy] epoch #945 | optimization finished
2021-06-04 14:00:26 | [train_policy] epoch #945 | Computing KL after
2021-06-04 14:00:26 | [train_policy] epoch #945 | Computing loss after
2021-06-04 14:00:26 | [train_policy] epoch #945 | Fitting baseline...
2021-06-04 14:00:26 | [train_policy] epoch #945 | Saving snapshot...
2021-06-04 14:00:26 | [train_policy] epoch #945 | Saved
2021-06-04 14:00:26 | [train_policy] epoch #945 | Time 758.51 s
2021-06-04 14:00:26 | [train_policy] epoch #945 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28475
Evaluation/AverageDiscountedReturn          -43.3384
Evaluation/AverageReturn                    -43.3384
Evaluation/CompletionRate                     0
Evaluation/Iteration                        945
Evaluation/MaxReturn                        -29.9095
Evaluation/MinReturn                        -78.7607
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.287
Extras/EpisodeRewardMean                    -43.3728
LinearFeatureBaseline/ExplainedVariance       0.872214
PolicyExecTime                                0.217779
ProcessExecTime                               0.0313034
TotalEnvSteps                            957352
policy/Entropy                               -2.26688
policy/KL                                     0.00651087
policy/KLBefore                               0
policy/LossAfter                             -0.0178804
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.103635
policy/dLoss                                  0.0178804
---------------------------------------  ----------------
2021-06-04 14:00:26 | [train_policy] epoch #946 | Obtaining samples for iteration 946...
2021-06-04 14:00:27 | [train_policy] epoch #946 | Logging diagnostics...
2021-06-04 14:00:27 | [train_policy] epoch #946 | Optimizing policy...
2021-06-04 14:00:27 | [train_policy] epoch #946 | Computing loss before
2021-06-04 14:00:27 | [train_policy] epoch #946 | Computing KL before
2021-06-04 14:00:27 | [train_policy] epoch #946 | Optimizing
2021-06-04 14:00:27 | [train_policy] epoch #946 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:27 | [train_policy] epoch #946 | computing loss before
2021-06-04 14:00:27 | [train_policy] epoch #946 | computing gradient
2021-06-04 14:00:27 | [train_policy] epoch #946 | gradient computed
2021-06-04 14:00:27 | [train_policy] epoch #946 | computing descent direction
2021-06-04 14:00:27 | [train_policy] epoch #946 | descent direction computed
2021-06-04 14:00:27 | [train_policy] epoch #946 | backtrack iters: 0
2021-06-04 14:00:27 | [train_policy] epoch #946 | optimization finished
2021-06-04 14:00:27 | [train_policy] epoch #946 | Computing KL after
2021-06-04 14:00:27 | [train_policy] epoch #946 | Computing loss after
2021-06-04 14:00:27 | [train_policy] epoch #946 | Fitting baseline...
2021-06-04 14:00:27 | [train_policy] epoch #946 | Saving snapshot...
2021-06-04 14:00:27 | [train_policy] epoch #946 | Saved
2021-06-04 14:00:27 | [train_policy] epoch #946 | Time 759.32 s
2021-06-04 14:00:27 | [train_policy] epoch #946 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283456
Evaluation/AverageDiscountedReturn          -40.8227
Evaluation/AverageReturn                    -40.8227
Evaluation/CompletionRate                     0
Evaluation/Iteration                        946
Evaluation/MaxReturn                        -28.2588
Evaluation/MinReturn                        -77.1785
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.90033
Extras/EpisodeRewardMean                    -41.0576
LinearFeatureBaseline/ExplainedVariance       0.711927
PolicyExecTime                                0.230056
ProcessExecTime                               0.0310936
TotalEnvSteps                            958364
policy/Entropy                               -2.26772
policy/KL                                     0.00952402
policy/KLBefore                               0
policy/LossAfter                             -0.034753
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.103548
policy/dLoss                                  0.034753
---------------------------------------  ----------------
2021-06-04 14:00:27 | [train_policy] epoch #947 | Obtaining samples for iteration 947...
2021-06-04 14:00:28 | [train_policy] epoch #947 | Logging diagnostics...
2021-06-04 14:00:28 | [train_policy] epoch #947 | Optimizing policy...
2021-06-04 14:00:28 | [train_policy] epoch #947 | Computing loss before
2021-06-04 14:00:28 | [train_policy] epoch #947 | Computing KL before
2021-06-04 14:00:28 | [train_policy] epoch #947 | Optimizing
2021-06-04 14:00:28 | [train_policy] epoch #947 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:28 | [train_policy] epoch #947 | computing loss before
2021-06-04 14:00:28 | [train_policy] epoch #947 | computing gradient
2021-06-04 14:00:28 | [train_policy] epoch #947 | gradient computed
2021-06-04 14:00:28 | [train_policy] epoch #947 | computing descent direction
2021-06-04 14:00:28 | [train_policy] epoch #947 | descent direction computed
2021-06-04 14:00:28 | [train_policy] epoch #947 | backtrack iters: 1
2021-06-04 14:00:28 | [train_policy] epoch #947 | optimization finished
2021-06-04 14:00:28 | [train_policy] epoch #947 | Computing KL after
2021-06-04 14:00:28 | [train_policy] epoch #947 | Computing loss after
2021-06-04 14:00:28 | [train_policy] epoch #947 | Fitting baseline...
2021-06-04 14:00:28 | [train_policy] epoch #947 | Saving snapshot...
2021-06-04 14:00:28 | [train_policy] epoch #947 | Saved
2021-06-04 14:00:28 | [train_policy] epoch #947 | Time 760.14 s
2021-06-04 14:00:28 | [train_policy] epoch #947 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                   0.285086
Evaluation/AverageDiscountedReturn          -41.8942
Evaluation/AverageReturn                    -41.8942
Evaluation/CompletionRate                     0
Evaluation/Iteration                        947
Evaluation/MaxReturn                        -28.5522
Evaluation/MinReturn                        -81.0245
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.4573
Extras/EpisodeRewardMean                    -41.4203
LinearFeatureBaseline/ExplainedVariance       0.792972
PolicyExecTime                                0.241351
ProcessExecTime                               0.0313013
TotalEnvSteps                            959376
policy/Entropy                               -2.28401
policy/KL                                     0.00671293
policy/KLBefore                               0
policy/LossAfter                             -0.0206703
policy/LossBefore                             1.443e-08
policy/Perplexity                             0.101874
policy/dLoss                                  0.0206703
---------------------------------------  ---------------
2021-06-04 14:00:28 | [train_policy] epoch #948 | Obtaining samples for iteration 948...
2021-06-04 14:00:29 | [train_policy] epoch #948 | Logging diagnostics...
2021-06-04 14:00:29 | [train_policy] epoch #948 | Optimizing policy...
2021-06-04 14:00:29 | [train_policy] epoch #948 | Computing loss before
2021-06-04 14:00:29 | [train_policy] epoch #948 | Computing KL before
2021-06-04 14:00:29 | [train_policy] epoch #948 | Optimizing
2021-06-04 14:00:29 | [train_policy] epoch #948 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:29 | [train_policy] epoch #948 | computing loss before
2021-06-04 14:00:29 | [train_policy] epoch #948 | computing gradient
2021-06-04 14:00:29 | [train_policy] epoch #948 | gradient computed
2021-06-04 14:00:29 | [train_policy] epoch #948 | computing descent direction
2021-06-04 14:00:29 | [train_policy] epoch #948 | descent direction computed
2021-06-04 14:00:29 | [train_policy] epoch #948 | backtrack iters: 1
2021-06-04 14:00:29 | [train_policy] epoch #948 | optimization finished
2021-06-04 14:00:29 | [train_policy] epoch #948 | Computing KL after
2021-06-04 14:00:29 | [train_policy] epoch #948 | Computing loss after
2021-06-04 14:00:29 | [train_policy] epoch #948 | Fitting baseline...
2021-06-04 14:00:29 | [train_policy] epoch #948 | Saving snapshot...
2021-06-04 14:00:29 | [train_policy] epoch #948 | Saved
2021-06-04 14:00:29 | [train_policy] epoch #948 | Time 760.95 s
2021-06-04 14:00:29 | [train_policy] epoch #948 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286041
Evaluation/AverageDiscountedReturn          -41.2043
Evaluation/AverageReturn                    -41.2043
Evaluation/CompletionRate                     0
Evaluation/Iteration                        948
Evaluation/MaxReturn                        -28.6528
Evaluation/MinReturn                        -63.1257
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.61972
Extras/EpisodeRewardMean                    -40.9718
LinearFeatureBaseline/ExplainedVariance       0.862697
PolicyExecTime                                0.231214
ProcessExecTime                               0.0314264
TotalEnvSteps                            960388
policy/Entropy                               -2.27696
policy/KL                                     0.00649693
policy/KLBefore                               0
policy/LossAfter                             -0.0148472
policy/LossBefore                            -1.04838e-08
policy/Perplexity                             0.102596
policy/dLoss                                  0.0148472
---------------------------------------  ----------------
2021-06-04 14:00:29 | [train_policy] epoch #949 | Obtaining samples for iteration 949...
2021-06-04 14:00:29 | [train_policy] epoch #949 | Logging diagnostics...
2021-06-04 14:00:29 | [train_policy] epoch #949 | Optimizing policy...
2021-06-04 14:00:29 | [train_policy] epoch #949 | Computing loss before
2021-06-04 14:00:29 | [train_policy] epoch #949 | Computing KL before
2021-06-04 14:00:29 | [train_policy] epoch #949 | Optimizing
2021-06-04 14:00:29 | [train_policy] epoch #949 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:29 | [train_policy] epoch #949 | computing loss before
2021-06-04 14:00:29 | [train_policy] epoch #949 | computing gradient
2021-06-04 14:00:29 | [train_policy] epoch #949 | gradient computed
2021-06-04 14:00:29 | [train_policy] epoch #949 | computing descent direction
2021-06-04 14:00:29 | [train_policy] epoch #949 | descent direction computed
2021-06-04 14:00:29 | [train_policy] epoch #949 | backtrack iters: 1
2021-06-04 14:00:29 | [train_policy] epoch #949 | optimization finished
2021-06-04 14:00:29 | [train_policy] epoch #949 | Computing KL after
2021-06-04 14:00:29 | [train_policy] epoch #949 | Computing loss after
2021-06-04 14:00:29 | [train_policy] epoch #949 | Fitting baseline...
2021-06-04 14:00:30 | [train_policy] epoch #949 | Saving snapshot...
2021-06-04 14:00:30 | [train_policy] epoch #949 | Saved
2021-06-04 14:00:30 | [train_policy] epoch #949 | Time 761.75 s
2021-06-04 14:00:30 | [train_policy] epoch #949 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284079
Evaluation/AverageDiscountedReturn          -40.6741
Evaluation/AverageReturn                    -40.6741
Evaluation/CompletionRate                     0
Evaluation/Iteration                        949
Evaluation/MaxReturn                        -28.2539
Evaluation/MinReturn                        -80.222
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.72854
Extras/EpisodeRewardMean                    -40.238
LinearFeatureBaseline/ExplainedVariance       0.874615
PolicyExecTime                                0.229078
ProcessExecTime                               0.0311263
TotalEnvSteps                            961400
policy/Entropy                               -2.28385
policy/KL                                     0.0064304
policy/KLBefore                               0
policy/LossAfter                             -0.0163013
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.101891
policy/dLoss                                  0.0163013
---------------------------------------  ----------------
2021-06-04 14:00:30 | [train_policy] epoch #950 | Obtaining samples for iteration 950...
2021-06-04 14:00:30 | [train_policy] epoch #950 | Logging diagnostics...
2021-06-04 14:00:30 | [train_policy] epoch #950 | Optimizing policy...
2021-06-04 14:00:30 | [train_policy] epoch #950 | Computing loss before
2021-06-04 14:00:30 | [train_policy] epoch #950 | Computing KL before
2021-06-04 14:00:30 | [train_policy] epoch #950 | Optimizing
2021-06-04 14:00:30 | [train_policy] epoch #950 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:30 | [train_policy] epoch #950 | computing loss before
2021-06-04 14:00:30 | [train_policy] epoch #950 | computing gradient
2021-06-04 14:00:30 | [train_policy] epoch #950 | gradient computed
2021-06-04 14:00:30 | [train_policy] epoch #950 | computing descent direction
2021-06-04 14:00:30 | [train_policy] epoch #950 | descent direction computed
2021-06-04 14:00:30 | [train_policy] epoch #950 | backtrack iters: 1
2021-06-04 14:00:30 | [train_policy] epoch #950 | optimization finished
2021-06-04 14:00:30 | [train_policy] epoch #950 | Computing KL after
2021-06-04 14:00:30 | [train_policy] epoch #950 | Computing loss after
2021-06-04 14:00:30 | [train_policy] epoch #950 | Fitting baseline...
2021-06-04 14:00:30 | [train_policy] epoch #950 | Saving snapshot...
2021-06-04 14:00:30 | [train_policy] epoch #950 | Saved
2021-06-04 14:00:30 | [train_policy] epoch #950 | Time 762.56 s
2021-06-04 14:00:30 | [train_policy] epoch #950 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.286995
Evaluation/AverageDiscountedReturn          -39.7416
Evaluation/AverageReturn                    -39.7416
Evaluation/CompletionRate                     0
Evaluation/Iteration                        950
Evaluation/MaxReturn                        -28.3778
Evaluation/MinReturn                        -56.5671
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.98715
Extras/EpisodeRewardMean                    -39.9909
LinearFeatureBaseline/ExplainedVariance       0.860369
PolicyExecTime                                0.22388
ProcessExecTime                               0.0315733
TotalEnvSteps                            962412
policy/Entropy                               -2.28629
policy/KL                                     0.00643842
policy/KLBefore                               0
policy/LossAfter                             -0.0196953
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.101643
policy/dLoss                                  0.0196953
---------------------------------------  ----------------
2021-06-04 14:00:30 | [train_policy] epoch #951 | Obtaining samples for iteration 951...
2021-06-04 14:00:31 | [train_policy] epoch #951 | Logging diagnostics...
2021-06-04 14:00:31 | [train_policy] epoch #951 | Optimizing policy...
2021-06-04 14:00:31 | [train_policy] epoch #951 | Computing loss before
2021-06-04 14:00:31 | [train_policy] epoch #951 | Computing KL before
2021-06-04 14:00:31 | [train_policy] epoch #951 | Optimizing
2021-06-04 14:00:31 | [train_policy] epoch #951 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:31 | [train_policy] epoch #951 | computing loss before
2021-06-04 14:00:31 | [train_policy] epoch #951 | computing gradient
2021-06-04 14:00:31 | [train_policy] epoch #951 | gradient computed
2021-06-04 14:00:31 | [train_policy] epoch #951 | computing descent direction
2021-06-04 14:00:31 | [train_policy] epoch #951 | descent direction computed
2021-06-04 14:00:31 | [train_policy] epoch #951 | backtrack iters: 0
2021-06-04 14:00:31 | [train_policy] epoch #951 | optimization finished
2021-06-04 14:00:31 | [train_policy] epoch #951 | Computing KL after
2021-06-04 14:00:31 | [train_policy] epoch #951 | Computing loss after
2021-06-04 14:00:31 | [train_policy] epoch #951 | Fitting baseline...
2021-06-04 14:00:31 | [train_policy] epoch #951 | Saving snapshot...
2021-06-04 14:00:31 | [train_policy] epoch #951 | Saved
2021-06-04 14:00:31 | [train_policy] epoch #951 | Time 763.34 s
2021-06-04 14:00:31 | [train_policy] epoch #951 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284594
Evaluation/AverageDiscountedReturn          -38.9849
Evaluation/AverageReturn                    -38.9849
Evaluation/CompletionRate                     0
Evaluation/Iteration                        951
Evaluation/MaxReturn                        -28.3182
Evaluation/MinReturn                        -63.1927
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.41729
Extras/EpisodeRewardMean                    -38.8524
LinearFeatureBaseline/ExplainedVariance       0.895948
PolicyExecTime                                0.212202
ProcessExecTime                               0.0312018
TotalEnvSteps                            963424
policy/Entropy                               -2.27545
policy/KL                                     0.00992102
policy/KLBefore                               0
policy/LossAfter                             -0.0160759
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.102751
policy/dLoss                                  0.0160759
---------------------------------------  ----------------
2021-06-04 14:00:31 | [train_policy] epoch #952 | Obtaining samples for iteration 952...
2021-06-04 14:00:32 | [train_policy] epoch #952 | Logging diagnostics...
2021-06-04 14:00:32 | [train_policy] epoch #952 | Optimizing policy...
2021-06-04 14:00:32 | [train_policy] epoch #952 | Computing loss before
2021-06-04 14:00:32 | [train_policy] epoch #952 | Computing KL before
2021-06-04 14:00:32 | [train_policy] epoch #952 | Optimizing
2021-06-04 14:00:32 | [train_policy] epoch #952 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:32 | [train_policy] epoch #952 | computing loss before
2021-06-04 14:00:32 | [train_policy] epoch #952 | computing gradient
2021-06-04 14:00:32 | [train_policy] epoch #952 | gradient computed
2021-06-04 14:00:32 | [train_policy] epoch #952 | computing descent direction
2021-06-04 14:00:32 | [train_policy] epoch #952 | descent direction computed
2021-06-04 14:00:32 | [train_policy] epoch #952 | backtrack iters: 1
2021-06-04 14:00:32 | [train_policy] epoch #952 | optimization finished
2021-06-04 14:00:32 | [train_policy] epoch #952 | Computing KL after
2021-06-04 14:00:32 | [train_policy] epoch #952 | Computing loss after
2021-06-04 14:00:32 | [train_policy] epoch #952 | Fitting baseline...
2021-06-04 14:00:32 | [train_policy] epoch #952 | Saving snapshot...
2021-06-04 14:00:32 | [train_policy] epoch #952 | Saved
2021-06-04 14:00:32 | [train_policy] epoch #952 | Time 764.14 s
2021-06-04 14:00:32 | [train_policy] epoch #952 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284613
Evaluation/AverageDiscountedReturn          -43.0928
Evaluation/AverageReturn                    -43.0928
Evaluation/CompletionRate                     0
Evaluation/Iteration                        952
Evaluation/MaxReturn                        -28.1548
Evaluation/MinReturn                        -80.8818
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.9477
Extras/EpisodeRewardMean                    -42.8711
LinearFeatureBaseline/ExplainedVariance       0.821124
PolicyExecTime                                0.227059
ProcessExecTime                               0.0311837
TotalEnvSteps                            964436
policy/Entropy                               -2.29616
policy/KL                                     0.006556
policy/KLBefore                               0
policy/LossAfter                             -0.0212982
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.100645
policy/dLoss                                  0.0212982
---------------------------------------  ----------------
2021-06-04 14:00:32 | [train_policy] epoch #953 | Obtaining samples for iteration 953...
2021-06-04 14:00:33 | [train_policy] epoch #953 | Logging diagnostics...
2021-06-04 14:00:33 | [train_policy] epoch #953 | Optimizing policy...
2021-06-04 14:00:33 | [train_policy] epoch #953 | Computing loss before
2021-06-04 14:00:33 | [train_policy] epoch #953 | Computing KL before
2021-06-04 14:00:33 | [train_policy] epoch #953 | Optimizing
2021-06-04 14:00:33 | [train_policy] epoch #953 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:33 | [train_policy] epoch #953 | computing loss before
2021-06-04 14:00:33 | [train_policy] epoch #953 | computing gradient
2021-06-04 14:00:33 | [train_policy] epoch #953 | gradient computed
2021-06-04 14:00:33 | [train_policy] epoch #953 | computing descent direction
2021-06-04 14:00:33 | [train_policy] epoch #953 | descent direction computed
2021-06-04 14:00:33 | [train_policy] epoch #953 | backtrack iters: 0
2021-06-04 14:00:33 | [train_policy] epoch #953 | optimization finished
2021-06-04 14:00:33 | [train_policy] epoch #953 | Computing KL after
2021-06-04 14:00:33 | [train_policy] epoch #953 | Computing loss after
2021-06-04 14:00:33 | [train_policy] epoch #953 | Fitting baseline...
2021-06-04 14:00:33 | [train_policy] epoch #953 | Saving snapshot...
2021-06-04 14:00:33 | [train_policy] epoch #953 | Saved
2021-06-04 14:00:33 | [train_policy] epoch #953 | Time 764.94 s
2021-06-04 14:00:33 | [train_policy] epoch #953 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283583
Evaluation/AverageDiscountedReturn          -42.0189
Evaluation/AverageReturn                    -42.0189
Evaluation/CompletionRate                     0
Evaluation/Iteration                        953
Evaluation/MaxReturn                        -30.3409
Evaluation/MinReturn                        -63.8246
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.83928
Extras/EpisodeRewardMean                    -41.8415
LinearFeatureBaseline/ExplainedVariance       0.871153
PolicyExecTime                                0.222618
ProcessExecTime                               0.0312154
TotalEnvSteps                            965448
policy/Entropy                               -2.32932
policy/KL                                     0.00996576
policy/KLBefore                               0
policy/LossAfter                             -0.0249427
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             0.0973615
policy/dLoss                                  0.0249427
---------------------------------------  ----------------
2021-06-04 14:00:33 | [train_policy] epoch #954 | Obtaining samples for iteration 954...
2021-06-04 14:00:33 | [train_policy] epoch #954 | Logging diagnostics...
2021-06-04 14:00:33 | [train_policy] epoch #954 | Optimizing policy...
2021-06-04 14:00:33 | [train_policy] epoch #954 | Computing loss before
2021-06-04 14:00:33 | [train_policy] epoch #954 | Computing KL before
2021-06-04 14:00:33 | [train_policy] epoch #954 | Optimizing
2021-06-04 14:00:33 | [train_policy] epoch #954 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:33 | [train_policy] epoch #954 | computing loss before
2021-06-04 14:00:33 | [train_policy] epoch #954 | computing gradient
2021-06-04 14:00:33 | [train_policy] epoch #954 | gradient computed
2021-06-04 14:00:33 | [train_policy] epoch #954 | computing descent direction
2021-06-04 14:00:33 | [train_policy] epoch #954 | descent direction computed
2021-06-04 14:00:33 | [train_policy] epoch #954 | backtrack iters: 1
2021-06-04 14:00:33 | [train_policy] epoch #954 | optimization finished
2021-06-04 14:00:33 | [train_policy] epoch #954 | Computing KL after
2021-06-04 14:00:33 | [train_policy] epoch #954 | Computing loss after
2021-06-04 14:00:33 | [train_policy] epoch #954 | Fitting baseline...
2021-06-04 14:00:34 | [train_policy] epoch #954 | Saving snapshot...
2021-06-04 14:00:34 | [train_policy] epoch #954 | Saved
2021-06-04 14:00:34 | [train_policy] epoch #954 | Time 765.76 s
2021-06-04 14:00:34 | [train_policy] epoch #954 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284853
Evaluation/AverageDiscountedReturn          -41.5628
Evaluation/AverageReturn                    -41.5628
Evaluation/CompletionRate                     0
Evaluation/Iteration                        954
Evaluation/MaxReturn                        -28.556
Evaluation/MinReturn                        -80.0155
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.89896
Extras/EpisodeRewardMean                    -41.457
LinearFeatureBaseline/ExplainedVariance       0.834642
PolicyExecTime                                0.231108
ProcessExecTime                               0.0312507
TotalEnvSteps                            966460
policy/Entropy                               -2.32861
policy/KL                                     0.00645958
policy/KLBefore                               0
policy/LossAfter                             -0.0177296
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.097431
policy/dLoss                                  0.0177296
---------------------------------------  ----------------
2021-06-04 14:00:34 | [train_policy] epoch #955 | Obtaining samples for iteration 955...
2021-06-04 14:00:34 | [train_policy] epoch #955 | Logging diagnostics...
2021-06-04 14:00:34 | [train_policy] epoch #955 | Optimizing policy...
2021-06-04 14:00:34 | [train_policy] epoch #955 | Computing loss before
2021-06-04 14:00:34 | [train_policy] epoch #955 | Computing KL before
2021-06-04 14:00:34 | [train_policy] epoch #955 | Optimizing
2021-06-04 14:00:34 | [train_policy] epoch #955 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:34 | [train_policy] epoch #955 | computing loss before
2021-06-04 14:00:34 | [train_policy] epoch #955 | computing gradient
2021-06-04 14:00:34 | [train_policy] epoch #955 | gradient computed
2021-06-04 14:00:34 | [train_policy] epoch #955 | computing descent direction
2021-06-04 14:00:34 | [train_policy] epoch #955 | descent direction computed
2021-06-04 14:00:34 | [train_policy] epoch #955 | backtrack iters: 1
2021-06-04 14:00:34 | [train_policy] epoch #955 | optimization finished
2021-06-04 14:00:34 | [train_policy] epoch #955 | Computing KL after
2021-06-04 14:00:34 | [train_policy] epoch #955 | Computing loss after
2021-06-04 14:00:34 | [train_policy] epoch #955 | Fitting baseline...
2021-06-04 14:00:34 | [train_policy] epoch #955 | Saving snapshot...
2021-06-04 14:00:34 | [train_policy] epoch #955 | Saved
2021-06-04 14:00:34 | [train_policy] epoch #955 | Time 766.56 s
2021-06-04 14:00:34 | [train_policy] epoch #955 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.287317
Evaluation/AverageDiscountedReturn          -40.6256
Evaluation/AverageReturn                    -40.6256
Evaluation/CompletionRate                     0
Evaluation/Iteration                        955
Evaluation/MaxReturn                        -28.4024
Evaluation/MinReturn                        -63.9659
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.51709
Extras/EpisodeRewardMean                    -40.5526
LinearFeatureBaseline/ExplainedVariance       0.894227
PolicyExecTime                                0.224675
ProcessExecTime                               0.0315208
TotalEnvSteps                            967472
policy/Entropy                               -2.3298
policy/KL                                     0.00660546
policy/KLBefore                               0
policy/LossAfter                             -0.0136821
policy/LossBefore                            -1.23097e-08
policy/Perplexity                             0.0973156
policy/dLoss                                  0.0136821
---------------------------------------  ----------------
2021-06-04 14:00:34 | [train_policy] epoch #956 | Obtaining samples for iteration 956...
2021-06-04 14:00:35 | [train_policy] epoch #956 | Logging diagnostics...
2021-06-04 14:00:35 | [train_policy] epoch #956 | Optimizing policy...
2021-06-04 14:00:35 | [train_policy] epoch #956 | Computing loss before
2021-06-04 14:00:35 | [train_policy] epoch #956 | Computing KL before
2021-06-04 14:00:35 | [train_policy] epoch #956 | Optimizing
2021-06-04 14:00:35 | [train_policy] epoch #956 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:35 | [train_policy] epoch #956 | computing loss before
2021-06-04 14:00:35 | [train_policy] epoch #956 | computing gradient
2021-06-04 14:00:35 | [train_policy] epoch #956 | gradient computed
2021-06-04 14:00:35 | [train_policy] epoch #956 | computing descent direction
2021-06-04 14:00:35 | [train_policy] epoch #956 | descent direction computed
2021-06-04 14:00:35 | [train_policy] epoch #956 | backtrack iters: 0
2021-06-04 14:00:35 | [train_policy] epoch #956 | optimization finished
2021-06-04 14:00:35 | [train_policy] epoch #956 | Computing KL after
2021-06-04 14:00:35 | [train_policy] epoch #956 | Computing loss after
2021-06-04 14:00:35 | [train_policy] epoch #956 | Fitting baseline...
2021-06-04 14:00:35 | [train_policy] epoch #956 | Saving snapshot...
2021-06-04 14:00:35 | [train_policy] epoch #956 | Saved
2021-06-04 14:00:35 | [train_policy] epoch #956 | Time 767.36 s
2021-06-04 14:00:35 | [train_policy] epoch #956 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.284425
Evaluation/AverageDiscountedReturn          -41.7438
Evaluation/AverageReturn                    -41.7438
Evaluation/CompletionRate                     0
Evaluation/Iteration                        956
Evaluation/MaxReturn                        -31.0833
Evaluation/MinReturn                        -81.6129
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.53205
Extras/EpisodeRewardMean                    -41.2622
LinearFeatureBaseline/ExplainedVariance       0.857999
PolicyExecTime                                0.219736
ProcessExecTime                               0.0311944
TotalEnvSteps                            968484
policy/Entropy                               -2.32495
policy/KL                                     0.00973007
policy/KLBefore                               0
policy/LossAfter                             -0.0214605
policy/LossBefore                            -1.60202e-08
policy/Perplexity                             0.0977882
policy/dLoss                                  0.0214605
---------------------------------------  ----------------
2021-06-04 14:00:35 | [train_policy] epoch #957 | Obtaining samples for iteration 957...
2021-06-04 14:00:36 | [train_policy] epoch #957 | Logging diagnostics...
2021-06-04 14:00:36 | [train_policy] epoch #957 | Optimizing policy...
2021-06-04 14:00:36 | [train_policy] epoch #957 | Computing loss before
2021-06-04 14:00:36 | [train_policy] epoch #957 | Computing KL before
2021-06-04 14:00:36 | [train_policy] epoch #957 | Optimizing
2021-06-04 14:00:36 | [train_policy] epoch #957 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:36 | [train_policy] epoch #957 | computing loss before
2021-06-04 14:00:36 | [train_policy] epoch #957 | computing gradient
2021-06-04 14:00:36 | [train_policy] epoch #957 | gradient computed
2021-06-04 14:00:36 | [train_policy] epoch #957 | computing descent direction
2021-06-04 14:00:36 | [train_policy] epoch #957 | descent direction computed
2021-06-04 14:00:36 | [train_policy] epoch #957 | backtrack iters: 0
2021-06-04 14:00:36 | [train_policy] epoch #957 | optimization finished
2021-06-04 14:00:36 | [train_policy] epoch #957 | Computing KL after
2021-06-04 14:00:36 | [train_policy] epoch #957 | Computing loss after
2021-06-04 14:00:36 | [train_policy] epoch #957 | Fitting baseline...
2021-06-04 14:00:36 | [train_policy] epoch #957 | Saving snapshot...
2021-06-04 14:00:36 | [train_policy] epoch #957 | Saved
2021-06-04 14:00:36 | [train_policy] epoch #957 | Time 768.14 s
2021-06-04 14:00:36 | [train_policy] epoch #957 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.28311
Evaluation/AverageDiscountedReturn          -40.6325
Evaluation/AverageReturn                    -40.6325
Evaluation/CompletionRate                     0
Evaluation/Iteration                        957
Evaluation/MaxReturn                        -28.5202
Evaluation/MinReturn                        -81.1223
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.60613
Extras/EpisodeRewardMean                    -40.6692
LinearFeatureBaseline/ExplainedVariance       0.845002
PolicyExecTime                                0.210686
ProcessExecTime                               0.0311735
TotalEnvSteps                            969496
policy/Entropy                               -2.32414
policy/KL                                     0.00987362
policy/KLBefore                               0
policy/LossAfter                             -0.0202824
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.0978678
policy/dLoss                                  0.0202824
---------------------------------------  ----------------
2021-06-04 14:00:36 | [train_policy] epoch #958 | Obtaining samples for iteration 958...
2021-06-04 14:00:37 | [train_policy] epoch #958 | Logging diagnostics...
2021-06-04 14:00:37 | [train_policy] epoch #958 | Optimizing policy...
2021-06-04 14:00:37 | [train_policy] epoch #958 | Computing loss before
2021-06-04 14:00:37 | [train_policy] epoch #958 | Computing KL before
2021-06-04 14:00:37 | [train_policy] epoch #958 | Optimizing
2021-06-04 14:00:37 | [train_policy] epoch #958 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:37 | [train_policy] epoch #958 | computing loss before
2021-06-04 14:00:37 | [train_policy] epoch #958 | computing gradient
2021-06-04 14:00:37 | [train_policy] epoch #958 | gradient computed
2021-06-04 14:00:37 | [train_policy] epoch #958 | computing descent direction
2021-06-04 14:00:37 | [train_policy] epoch #958 | descent direction computed
2021-06-04 14:00:37 | [train_policy] epoch #958 | backtrack iters: 1
2021-06-04 14:00:37 | [train_policy] epoch #958 | optimization finished
2021-06-04 14:00:37 | [train_policy] epoch #958 | Computing KL after
2021-06-04 14:00:37 | [train_policy] epoch #958 | Computing loss after
2021-06-04 14:00:37 | [train_policy] epoch #958 | Fitting baseline...
2021-06-04 14:00:37 | [train_policy] epoch #958 | Saving snapshot...
2021-06-04 14:00:37 | [train_policy] epoch #958 | Saved
2021-06-04 14:00:37 | [train_policy] epoch #958 | Time 768.95 s
2021-06-04 14:00:37 | [train_policy] epoch #958 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283251
Evaluation/AverageDiscountedReturn          -42.7838
Evaluation/AverageReturn                    -42.7838
Evaluation/CompletionRate                     0
Evaluation/Iteration                        958
Evaluation/MaxReturn                        -31.4243
Evaluation/MinReturn                        -89.5161
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.0066
Extras/EpisodeRewardMean                    -42.2748
LinearFeatureBaseline/ExplainedVariance       0.810731
PolicyExecTime                                0.222719
ProcessExecTime                               0.0310795
TotalEnvSteps                            970508
policy/Entropy                               -2.33206
policy/KL                                     0.00648477
policy/KLBefore                               0
policy/LossAfter                             -0.0170989
policy/LossBefore                             7.18554e-09
policy/Perplexity                             0.0970958
policy/dLoss                                  0.0170989
---------------------------------------  ----------------
2021-06-04 14:00:37 | [train_policy] epoch #959 | Obtaining samples for iteration 959...
2021-06-04 14:00:37 | [train_policy] epoch #959 | Logging diagnostics...
2021-06-04 14:00:37 | [train_policy] epoch #959 | Optimizing policy...
2021-06-04 14:00:37 | [train_policy] epoch #959 | Computing loss before
2021-06-04 14:00:37 | [train_policy] epoch #959 | Computing KL before
2021-06-04 14:00:37 | [train_policy] epoch #959 | Optimizing
2021-06-04 14:00:37 | [train_policy] epoch #959 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:37 | [train_policy] epoch #959 | computing loss before
2021-06-04 14:00:37 | [train_policy] epoch #959 | computing gradient
2021-06-04 14:00:37 | [train_policy] epoch #959 | gradient computed
2021-06-04 14:00:37 | [train_policy] epoch #959 | computing descent direction
2021-06-04 14:00:37 | [train_policy] epoch #959 | descent direction computed
2021-06-04 14:00:37 | [train_policy] epoch #959 | backtrack iters: 1
2021-06-04 14:00:37 | [train_policy] epoch #959 | optimization finished
2021-06-04 14:00:37 | [train_policy] epoch #959 | Computing KL after
2021-06-04 14:00:37 | [train_policy] epoch #959 | Computing loss after
2021-06-04 14:00:37 | [train_policy] epoch #959 | Fitting baseline...
2021-06-04 14:00:38 | [train_policy] epoch #959 | Saving snapshot...
2021-06-04 14:00:38 | [train_policy] epoch #959 | Saved
2021-06-04 14:00:38 | [train_policy] epoch #959 | Time 769.76 s
2021-06-04 14:00:38 | [train_policy] epoch #959 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284524
Evaluation/AverageDiscountedReturn          -40.2991
Evaluation/AverageReturn                    -40.2991
Evaluation/CompletionRate                     0
Evaluation/Iteration                        959
Evaluation/MaxReturn                        -28.401
Evaluation/MinReturn                        -63.2073
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.90031
Extras/EpisodeRewardMean                    -40.5353
LinearFeatureBaseline/ExplainedVariance       0.849454
PolicyExecTime                                0.224802
ProcessExecTime                               0.0312757
TotalEnvSteps                            971520
policy/Entropy                               -2.34804
policy/KL                                     0.00646046
policy/KLBefore                               0
policy/LossAfter                             -0.0134238
policy/LossBefore                            -1.29575e-08
policy/Perplexity                             0.0955561
policy/dLoss                                  0.0134238
---------------------------------------  ----------------
2021-06-04 14:00:38 | [train_policy] epoch #960 | Obtaining samples for iteration 960...
2021-06-04 14:00:38 | [train_policy] epoch #960 | Logging diagnostics...
2021-06-04 14:00:38 | [train_policy] epoch #960 | Optimizing policy...
2021-06-04 14:00:38 | [train_policy] epoch #960 | Computing loss before
2021-06-04 14:00:38 | [train_policy] epoch #960 | Computing KL before
2021-06-04 14:00:38 | [train_policy] epoch #960 | Optimizing
2021-06-04 14:00:38 | [train_policy] epoch #960 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:38 | [train_policy] epoch #960 | computing loss before
2021-06-04 14:00:38 | [train_policy] epoch #960 | computing gradient
2021-06-04 14:00:38 | [train_policy] epoch #960 | gradient computed
2021-06-04 14:00:38 | [train_policy] epoch #960 | computing descent direction
2021-06-04 14:00:38 | [train_policy] epoch #960 | descent direction computed
2021-06-04 14:00:38 | [train_policy] epoch #960 | backtrack iters: 1
2021-06-04 14:00:38 | [train_policy] epoch #960 | optimization finished
2021-06-04 14:00:38 | [train_policy] epoch #960 | Computing KL after
2021-06-04 14:00:38 | [train_policy] epoch #960 | Computing loss after
2021-06-04 14:00:38 | [train_policy] epoch #960 | Fitting baseline...
2021-06-04 14:00:38 | [train_policy] epoch #960 | Saving snapshot...
2021-06-04 14:00:38 | [train_policy] epoch #960 | Saved
2021-06-04 14:00:38 | [train_policy] epoch #960 | Time 770.57 s
2021-06-04 14:00:38 | [train_policy] epoch #960 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283489
Evaluation/AverageDiscountedReturn          -40.4853
Evaluation/AverageReturn                    -40.4853
Evaluation/CompletionRate                     0
Evaluation/Iteration                        960
Evaluation/MaxReturn                        -28.0549
Evaluation/MinReturn                        -63.0373
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.13077
Extras/EpisodeRewardMean                    -40.6141
LinearFeatureBaseline/ExplainedVariance       0.881189
PolicyExecTime                                0.230833
ProcessExecTime                               0.0312006
TotalEnvSteps                            972532
policy/Entropy                               -2.35575
policy/KL                                     0.00664778
policy/KLBefore                               0
policy/LossAfter                             -0.0134104
policy/LossBefore                             6.36097e-09
policy/Perplexity                             0.0948221
policy/dLoss                                  0.0134104
---------------------------------------  ----------------
2021-06-04 14:00:38 | [train_policy] epoch #961 | Obtaining samples for iteration 961...
2021-06-04 14:00:39 | [train_policy] epoch #961 | Logging diagnostics...
2021-06-04 14:00:39 | [train_policy] epoch #961 | Optimizing policy...
2021-06-04 14:00:39 | [train_policy] epoch #961 | Computing loss before
2021-06-04 14:00:39 | [train_policy] epoch #961 | Computing KL before
2021-06-04 14:00:39 | [train_policy] epoch #961 | Optimizing
2021-06-04 14:00:39 | [train_policy] epoch #961 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:39 | [train_policy] epoch #961 | computing loss before
2021-06-04 14:00:39 | [train_policy] epoch #961 | computing gradient
2021-06-04 14:00:39 | [train_policy] epoch #961 | gradient computed
2021-06-04 14:00:39 | [train_policy] epoch #961 | computing descent direction
2021-06-04 14:00:39 | [train_policy] epoch #961 | descent direction computed
2021-06-04 14:00:39 | [train_policy] epoch #961 | backtrack iters: 0
2021-06-04 14:00:39 | [train_policy] epoch #961 | optimization finished
2021-06-04 14:00:39 | [train_policy] epoch #961 | Computing KL after
2021-06-04 14:00:39 | [train_policy] epoch #961 | Computing loss after
2021-06-04 14:00:39 | [train_policy] epoch #961 | Fitting baseline...
2021-06-04 14:00:39 | [train_policy] epoch #961 | Saving snapshot...
2021-06-04 14:00:39 | [train_policy] epoch #961 | Saved
2021-06-04 14:00:39 | [train_policy] epoch #961 | Time 771.37 s
2021-06-04 14:00:39 | [train_policy] epoch #961 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.282774
Evaluation/AverageDiscountedReturn          -41.4803
Evaluation/AverageReturn                    -41.4803
Evaluation/CompletionRate                     0
Evaluation/Iteration                        961
Evaluation/MaxReturn                        -28.357
Evaluation/MinReturn                        -63.9224
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.15776
Extras/EpisodeRewardMean                    -41.0419
LinearFeatureBaseline/ExplainedVariance       0.855558
PolicyExecTime                                0.211286
ProcessExecTime                               0.0310698
TotalEnvSteps                            973544
policy/Entropy                               -2.35945
policy/KL                                     0.00997566
policy/KLBefore                               0
policy/LossAfter                             -0.024665
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.0944718
policy/dLoss                                  0.024665
---------------------------------------  ----------------
2021-06-04 14:00:39 | [train_policy] epoch #962 | Obtaining samples for iteration 962...
2021-06-04 14:00:40 | [train_policy] epoch #962 | Logging diagnostics...
2021-06-04 14:00:40 | [train_policy] epoch #962 | Optimizing policy...
2021-06-04 14:00:40 | [train_policy] epoch #962 | Computing loss before
2021-06-04 14:00:40 | [train_policy] epoch #962 | Computing KL before
2021-06-04 14:00:40 | [train_policy] epoch #962 | Optimizing
2021-06-04 14:00:40 | [train_policy] epoch #962 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:40 | [train_policy] epoch #962 | computing loss before
2021-06-04 14:00:40 | [train_policy] epoch #962 | computing gradient
2021-06-04 14:00:40 | [train_policy] epoch #962 | gradient computed
2021-06-04 14:00:40 | [train_policy] epoch #962 | computing descent direction
2021-06-04 14:00:40 | [train_policy] epoch #962 | descent direction computed
2021-06-04 14:00:40 | [train_policy] epoch #962 | backtrack iters: 1
2021-06-04 14:00:40 | [train_policy] epoch #962 | optimization finished
2021-06-04 14:00:40 | [train_policy] epoch #962 | Computing KL after
2021-06-04 14:00:40 | [train_policy] epoch #962 | Computing loss after
2021-06-04 14:00:40 | [train_policy] epoch #962 | Fitting baseline...
2021-06-04 14:00:40 | [train_policy] epoch #962 | Saving snapshot...
2021-06-04 14:00:40 | [train_policy] epoch #962 | Saved
2021-06-04 14:00:40 | [train_policy] epoch #962 | Time 772.17 s
2021-06-04 14:00:40 | [train_policy] epoch #962 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284057
Evaluation/AverageDiscountedReturn          -41.0061
Evaluation/AverageReturn                    -41.0061
Evaluation/CompletionRate                     0
Evaluation/Iteration                        962
Evaluation/MaxReturn                        -28.2276
Evaluation/MinReturn                        -63.4507
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.71882
Extras/EpisodeRewardMean                    -40.8999
LinearFeatureBaseline/ExplainedVariance       0.876319
PolicyExecTime                                0.226416
ProcessExecTime                               0.031204
TotalEnvSteps                            974556
policy/Entropy                               -2.37235
policy/KL                                     0.00654539
policy/KLBefore                               0
policy/LossAfter                             -0.0195439
policy/LossBefore                             7.77452e-09
policy/Perplexity                             0.0932613
policy/dLoss                                  0.0195439
---------------------------------------  ----------------
2021-06-04 14:00:40 | [train_policy] epoch #963 | Obtaining samples for iteration 963...
2021-06-04 14:00:41 | [train_policy] epoch #963 | Logging diagnostics...
2021-06-04 14:00:41 | [train_policy] epoch #963 | Optimizing policy...
2021-06-04 14:00:41 | [train_policy] epoch #963 | Computing loss before
2021-06-04 14:00:41 | [train_policy] epoch #963 | Computing KL before
2021-06-04 14:00:41 | [train_policy] epoch #963 | Optimizing
2021-06-04 14:00:41 | [train_policy] epoch #963 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:41 | [train_policy] epoch #963 | computing loss before
2021-06-04 14:00:41 | [train_policy] epoch #963 | computing gradient
2021-06-04 14:00:41 | [train_policy] epoch #963 | gradient computed
2021-06-04 14:00:41 | [train_policy] epoch #963 | computing descent direction
2021-06-04 14:00:41 | [train_policy] epoch #963 | descent direction computed
2021-06-04 14:00:41 | [train_policy] epoch #963 | backtrack iters: 1
2021-06-04 14:00:41 | [train_policy] epoch #963 | optimization finished
2021-06-04 14:00:41 | [train_policy] epoch #963 | Computing KL after
2021-06-04 14:00:41 | [train_policy] epoch #963 | Computing loss after
2021-06-04 14:00:41 | [train_policy] epoch #963 | Fitting baseline...
2021-06-04 14:00:41 | [train_policy] epoch #963 | Saving snapshot...
2021-06-04 14:00:41 | [train_policy] epoch #963 | Saved
2021-06-04 14:00:41 | [train_policy] epoch #963 | Time 773.00 s
2021-06-04 14:00:41 | [train_policy] epoch #963 | EpochTime 0.80 s
---------------------------------------  ---------------
EnvExecTime                                   0.284274
Evaluation/AverageDiscountedReturn          -41.3447
Evaluation/AverageReturn                    -41.3447
Evaluation/CompletionRate                     0
Evaluation/Iteration                        963
Evaluation/MaxReturn                        -28.3998
Evaluation/MinReturn                        -63.7836
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.54214
Extras/EpisodeRewardMean                    -41.408
LinearFeatureBaseline/ExplainedVariance       0.883744
PolicyExecTime                                0.215149
ProcessExecTime                               0.0312388
TotalEnvSteps                            975568
policy/Entropy                               -2.37734
policy/KL                                     0.00651353
policy/KLBefore                               0
policy/LossAfter                             -0.020511
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.0927968
policy/dLoss                                  0.020511
---------------------------------------  ---------------
2021-06-04 14:00:41 | [train_policy] epoch #964 | Obtaining samples for iteration 964...
2021-06-04 14:00:41 | [train_policy] epoch #964 | Logging diagnostics...
2021-06-04 14:00:41 | [train_policy] epoch #964 | Optimizing policy...
2021-06-04 14:00:41 | [train_policy] epoch #964 | Computing loss before
2021-06-04 14:00:41 | [train_policy] epoch #964 | Computing KL before
2021-06-04 14:00:41 | [train_policy] epoch #964 | Optimizing
2021-06-04 14:00:41 | [train_policy] epoch #964 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:41 | [train_policy] epoch #964 | computing loss before
2021-06-04 14:00:41 | [train_policy] epoch #964 | computing gradient
2021-06-04 14:00:41 | [train_policy] epoch #964 | gradient computed
2021-06-04 14:00:41 | [train_policy] epoch #964 | computing descent direction
2021-06-04 14:00:42 | [train_policy] epoch #964 | descent direction computed
2021-06-04 14:00:42 | [train_policy] epoch #964 | backtrack iters: 1
2021-06-04 14:00:42 | [train_policy] epoch #964 | optimization finished
2021-06-04 14:00:42 | [train_policy] epoch #964 | Computing KL after
2021-06-04 14:00:42 | [train_policy] epoch #964 | Computing loss after
2021-06-04 14:00:42 | [train_policy] epoch #964 | Fitting baseline...
2021-06-04 14:00:42 | [train_policy] epoch #964 | Saving snapshot...
2021-06-04 14:00:42 | [train_policy] epoch #964 | Saved
2021-06-04 14:00:42 | [train_policy] epoch #964 | Time 773.79 s
2021-06-04 14:00:42 | [train_policy] epoch #964 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.283164
Evaluation/AverageDiscountedReturn          -41.8803
Evaluation/AverageReturn                    -41.8803
Evaluation/CompletionRate                     0
Evaluation/Iteration                        964
Evaluation/MaxReturn                        -27.9331
Evaluation/MinReturn                        -80.2586
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.50393
Extras/EpisodeRewardMean                    -41.9864
LinearFeatureBaseline/ExplainedVariance       0.865189
PolicyExecTime                                0.208347
ProcessExecTime                               0.0310776
TotalEnvSteps                            976580
policy/Entropy                               -2.38966
policy/KL                                     0.00649154
policy/KLBefore                               0
policy/LossAfter                             -0.0121784
policy/LossBefore                            -1.17796e-09
policy/Perplexity                             0.0916607
policy/dLoss                                  0.0121784
---------------------------------------  ----------------
2021-06-04 14:00:42 | [train_policy] epoch #965 | Obtaining samples for iteration 965...
2021-06-04 14:00:42 | [train_policy] epoch #965 | Logging diagnostics...
2021-06-04 14:00:42 | [train_policy] epoch #965 | Optimizing policy...
2021-06-04 14:00:42 | [train_policy] epoch #965 | Computing loss before
2021-06-04 14:00:42 | [train_policy] epoch #965 | Computing KL before
2021-06-04 14:00:42 | [train_policy] epoch #965 | Optimizing
2021-06-04 14:00:42 | [train_policy] epoch #965 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:42 | [train_policy] epoch #965 | computing loss before
2021-06-04 14:00:42 | [train_policy] epoch #965 | computing gradient
2021-06-04 14:00:42 | [train_policy] epoch #965 | gradient computed
2021-06-04 14:00:42 | [train_policy] epoch #965 | computing descent direction
2021-06-04 14:00:42 | [train_policy] epoch #965 | descent direction computed
2021-06-04 14:00:42 | [train_policy] epoch #965 | backtrack iters: 1
2021-06-04 14:00:42 | [train_policy] epoch #965 | optimization finished
2021-06-04 14:00:42 | [train_policy] epoch #965 | Computing KL after
2021-06-04 14:00:42 | [train_policy] epoch #965 | Computing loss after
2021-06-04 14:00:42 | [train_policy] epoch #965 | Fitting baseline...
2021-06-04 14:00:42 | [train_policy] epoch #965 | Saving snapshot...
2021-06-04 14:00:42 | [train_policy] epoch #965 | Saved
2021-06-04 14:00:42 | [train_policy] epoch #965 | Time 774.63 s
2021-06-04 14:00:42 | [train_policy] epoch #965 | EpochTime 0.81 s
---------------------------------------  ----------------
EnvExecTime                                   0.28719
Evaluation/AverageDiscountedReturn          -41.274
Evaluation/AverageReturn                    -41.274
Evaluation/CompletionRate                     0
Evaluation/Iteration                        965
Evaluation/MaxReturn                        -28.4821
Evaluation/MinReturn                        -84.0181
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.52893
Extras/EpisodeRewardMean                    -41.4208
LinearFeatureBaseline/ExplainedVariance       0.83672
PolicyExecTime                                0.240684
ProcessExecTime                               0.0314119
TotalEnvSteps                            977592
policy/Entropy                               -2.39415
policy/KL                                     0.00649735
policy/KLBefore                               0
policy/LossAfter                             -0.0180215
policy/LossBefore                            -1.60202e-08
policy/Perplexity                             0.0912506
policy/dLoss                                  0.0180215
---------------------------------------  ----------------
2021-06-04 14:00:42 | [train_policy] epoch #966 | Obtaining samples for iteration 966...
2021-06-04 14:00:43 | [train_policy] epoch #966 | Logging diagnostics...
2021-06-04 14:00:43 | [train_policy] epoch #966 | Optimizing policy...
2021-06-04 14:00:43 | [train_policy] epoch #966 | Computing loss before
2021-06-04 14:00:43 | [train_policy] epoch #966 | Computing KL before
2021-06-04 14:00:43 | [train_policy] epoch #966 | Optimizing
2021-06-04 14:00:43 | [train_policy] epoch #966 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:43 | [train_policy] epoch #966 | computing loss before
2021-06-04 14:00:43 | [train_policy] epoch #966 | computing gradient
2021-06-04 14:00:43 | [train_policy] epoch #966 | gradient computed
2021-06-04 14:00:43 | [train_policy] epoch #966 | computing descent direction
2021-06-04 14:00:43 | [train_policy] epoch #966 | descent direction computed
2021-06-04 14:00:43 | [train_policy] epoch #966 | backtrack iters: 0
2021-06-04 14:00:43 | [train_policy] epoch #966 | optimization finished
2021-06-04 14:00:43 | [train_policy] epoch #966 | Computing KL after
2021-06-04 14:00:43 | [train_policy] epoch #966 | Computing loss after
2021-06-04 14:00:43 | [train_policy] epoch #966 | Fitting baseline...
2021-06-04 14:00:43 | [train_policy] epoch #966 | Saving snapshot...
2021-06-04 14:00:43 | [train_policy] epoch #966 | Saved
2021-06-04 14:00:43 | [train_policy] epoch #966 | Time 775.45 s
2021-06-04 14:00:43 | [train_policy] epoch #966 | EpochTime 0.79 s
---------------------------------------  ---------------
EnvExecTime                                   0.286878
Evaluation/AverageDiscountedReturn          -42.2643
Evaluation/AverageReturn                    -42.2643
Evaluation/CompletionRate                     0
Evaluation/Iteration                        966
Evaluation/MaxReturn                        -29.8984
Evaluation/MinReturn                        -82.6594
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.30395
Extras/EpisodeRewardMean                    -41.7843
LinearFeatureBaseline/ExplainedVariance       0.828987
PolicyExecTime                                0.241124
ProcessExecTime                               0.0314693
TotalEnvSteps                            978604
policy/Entropy                               -2.38382
policy/KL                                     0.00993957
policy/KLBefore                               0
policy/LossAfter                             -0.0195282
policy/LossBefore                            -8.2457e-10
policy/Perplexity                             0.0921977
policy/dLoss                                  0.0195282
---------------------------------------  ---------------
2021-06-04 14:00:43 | [train_policy] epoch #967 | Obtaining samples for iteration 967...
2021-06-04 14:00:44 | [train_policy] epoch #967 | Logging diagnostics...
2021-06-04 14:00:44 | [train_policy] epoch #967 | Optimizing policy...
2021-06-04 14:00:44 | [train_policy] epoch #967 | Computing loss before
2021-06-04 14:00:44 | [train_policy] epoch #967 | Computing KL before
2021-06-04 14:00:44 | [train_policy] epoch #967 | Optimizing
2021-06-04 14:00:44 | [train_policy] epoch #967 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:44 | [train_policy] epoch #967 | computing loss before
2021-06-04 14:00:44 | [train_policy] epoch #967 | computing gradient
2021-06-04 14:00:44 | [train_policy] epoch #967 | gradient computed
2021-06-04 14:00:44 | [train_policy] epoch #967 | computing descent direction
2021-06-04 14:00:44 | [train_policy] epoch #967 | descent direction computed
2021-06-04 14:00:44 | [train_policy] epoch #967 | backtrack iters: 1
2021-06-04 14:00:44 | [train_policy] epoch #967 | optimization finished
2021-06-04 14:00:44 | [train_policy] epoch #967 | Computing KL after
2021-06-04 14:00:44 | [train_policy] epoch #967 | Computing loss after
2021-06-04 14:00:44 | [train_policy] epoch #967 | Fitting baseline...
2021-06-04 14:00:44 | [train_policy] epoch #967 | Saving snapshot...
2021-06-04 14:00:44 | [train_policy] epoch #967 | Saved
2021-06-04 14:00:44 | [train_policy] epoch #967 | Time 776.25 s
2021-06-04 14:00:44 | [train_policy] epoch #967 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285649
Evaluation/AverageDiscountedReturn          -41.8438
Evaluation/AverageReturn                    -41.8438
Evaluation/CompletionRate                     0
Evaluation/Iteration                        967
Evaluation/MaxReturn                        -28.2047
Evaluation/MinReturn                        -82.5185
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.24946
Extras/EpisodeRewardMean                    -41.8345
LinearFeatureBaseline/ExplainedVariance       0.8502
PolicyExecTime                                0.226767
ProcessExecTime                               0.0313556
TotalEnvSteps                            979616
policy/Entropy                               -2.39411
policy/KL                                     0.00643562
policy/KLBefore                               0
policy/LossAfter                             -0.0108405
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.0912542
policy/dLoss                                  0.0108405
---------------------------------------  ----------------
2021-06-04 14:00:44 | [train_policy] epoch #968 | Obtaining samples for iteration 968...
2021-06-04 14:00:45 | [train_policy] epoch #968 | Logging diagnostics...
2021-06-04 14:00:45 | [train_policy] epoch #968 | Optimizing policy...
2021-06-04 14:00:45 | [train_policy] epoch #968 | Computing loss before
2021-06-04 14:00:45 | [train_policy] epoch #968 | Computing KL before
2021-06-04 14:00:45 | [train_policy] epoch #968 | Optimizing
2021-06-04 14:00:45 | [train_policy] epoch #968 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:45 | [train_policy] epoch #968 | computing loss before
2021-06-04 14:00:45 | [train_policy] epoch #968 | computing gradient
2021-06-04 14:00:45 | [train_policy] epoch #968 | gradient computed
2021-06-04 14:00:45 | [train_policy] epoch #968 | computing descent direction
2021-06-04 14:00:45 | [train_policy] epoch #968 | descent direction computed
2021-06-04 14:00:45 | [train_policy] epoch #968 | backtrack iters: 1
2021-06-04 14:00:45 | [train_policy] epoch #968 | optimization finished
2021-06-04 14:00:45 | [train_policy] epoch #968 | Computing KL after
2021-06-04 14:00:45 | [train_policy] epoch #968 | Computing loss after
2021-06-04 14:00:45 | [train_policy] epoch #968 | Fitting baseline...
2021-06-04 14:00:45 | [train_policy] epoch #968 | Saving snapshot...
2021-06-04 14:00:45 | [train_policy] epoch #968 | Saved
2021-06-04 14:00:45 | [train_policy] epoch #968 | Time 777.04 s
2021-06-04 14:00:45 | [train_policy] epoch #968 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.285182
Evaluation/AverageDiscountedReturn          -41.0395
Evaluation/AverageReturn                    -41.0395
Evaluation/CompletionRate                     0
Evaluation/Iteration                        968
Evaluation/MaxReturn                        -28.9608
Evaluation/MinReturn                        -63.3241
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.8343
Extras/EpisodeRewardMean                    -41.3678
LinearFeatureBaseline/ExplainedVariance       0.87679
PolicyExecTime                                0.211735
ProcessExecTime                               0.0313191
TotalEnvSteps                            980628
policy/Entropy                               -2.39282
policy/KL                                     0.0064921
policy/KLBefore                               0
policy/LossAfter                             -0.0115897
policy/LossBefore                             3.18048e-09
policy/Perplexity                             0.0913712
policy/dLoss                                  0.0115897
---------------------------------------  ----------------
2021-06-04 14:00:45 | [train_policy] epoch #969 | Obtaining samples for iteration 969...
2021-06-04 14:00:45 | [train_policy] epoch #969 | Logging diagnostics...
2021-06-04 14:00:45 | [train_policy] epoch #969 | Optimizing policy...
2021-06-04 14:00:45 | [train_policy] epoch #969 | Computing loss before
2021-06-04 14:00:45 | [train_policy] epoch #969 | Computing KL before
2021-06-04 14:00:45 | [train_policy] epoch #969 | Optimizing
2021-06-04 14:00:45 | [train_policy] epoch #969 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:45 | [train_policy] epoch #969 | computing loss before
2021-06-04 14:00:45 | [train_policy] epoch #969 | computing gradient
2021-06-04 14:00:45 | [train_policy] epoch #969 | gradient computed
2021-06-04 14:00:45 | [train_policy] epoch #969 | computing descent direction
2021-06-04 14:00:46 | [train_policy] epoch #969 | descent direction computed
2021-06-04 14:00:46 | [train_policy] epoch #969 | backtrack iters: 1
2021-06-04 14:00:46 | [train_policy] epoch #969 | optimization finished
2021-06-04 14:00:46 | [train_policy] epoch #969 | Computing KL after
2021-06-04 14:00:46 | [train_policy] epoch #969 | Computing loss after
2021-06-04 14:00:46 | [train_policy] epoch #969 | Fitting baseline...
2021-06-04 14:00:46 | [train_policy] epoch #969 | Saving snapshot...
2021-06-04 14:00:46 | [train_policy] epoch #969 | Saved
2021-06-04 14:00:46 | [train_policy] epoch #969 | Time 777.84 s
2021-06-04 14:00:46 | [train_policy] epoch #969 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28431
Evaluation/AverageDiscountedReturn          -39.0534
Evaluation/AverageReturn                    -39.0534
Evaluation/CompletionRate                     0
Evaluation/Iteration                        969
Evaluation/MaxReturn                        -29.245
Evaluation/MinReturn                        -63.931
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.00343
Extras/EpisodeRewardMean                    -38.9263
LinearFeatureBaseline/ExplainedVariance       0.89361
PolicyExecTime                                0.218979
ProcessExecTime                               0.0312052
TotalEnvSteps                            981640
policy/Entropy                               -2.38366
policy/KL                                     0.00641094
policy/KLBefore                               0
policy/LossAfter                             -0.0142435
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.0922122
policy/dLoss                                  0.0142435
---------------------------------------  ----------------
2021-06-04 14:00:46 | [train_policy] epoch #970 | Obtaining samples for iteration 970...
2021-06-04 14:00:46 | [train_policy] epoch #970 | Logging diagnostics...
2021-06-04 14:00:46 | [train_policy] epoch #970 | Optimizing policy...
2021-06-04 14:00:46 | [train_policy] epoch #970 | Computing loss before
2021-06-04 14:00:46 | [train_policy] epoch #970 | Computing KL before
2021-06-04 14:00:46 | [train_policy] epoch #970 | Optimizing
2021-06-04 14:00:46 | [train_policy] epoch #970 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:46 | [train_policy] epoch #970 | computing loss before
2021-06-04 14:00:46 | [train_policy] epoch #970 | computing gradient
2021-06-04 14:00:46 | [train_policy] epoch #970 | gradient computed
2021-06-04 14:00:46 | [train_policy] epoch #970 | computing descent direction
2021-06-04 14:00:46 | [train_policy] epoch #970 | descent direction computed
2021-06-04 14:00:46 | [train_policy] epoch #970 | backtrack iters: 1
2021-06-04 14:00:46 | [train_policy] epoch #970 | optimization finished
2021-06-04 14:00:46 | [train_policy] epoch #970 | Computing KL after
2021-06-04 14:00:46 | [train_policy] epoch #970 | Computing loss after
2021-06-04 14:00:46 | [train_policy] epoch #970 | Fitting baseline...
2021-06-04 14:00:46 | [train_policy] epoch #970 | Saving snapshot...
2021-06-04 14:00:46 | [train_policy] epoch #970 | Saved
2021-06-04 14:00:46 | [train_policy] epoch #970 | Time 778.65 s
2021-06-04 14:00:46 | [train_policy] epoch #970 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.28468
Evaluation/AverageDiscountedReturn          -41.302
Evaluation/AverageReturn                    -41.302
Evaluation/CompletionRate                     0
Evaluation/Iteration                        970
Evaluation/MaxReturn                        -28.3194
Evaluation/MinReturn                        -79.8866
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.52233
Extras/EpisodeRewardMean                    -41.4664
LinearFeatureBaseline/ExplainedVariance       0.857295
PolicyExecTime                                0.226601
ProcessExecTime                               0.0312397
TotalEnvSteps                            982652
policy/Entropy                               -2.37384
policy/KL                                     0.00644274
policy/KLBefore                               0
policy/LossAfter                             -0.0173529
policy/LossBefore                             4.82963e-09
policy/Perplexity                             0.0931222
policy/dLoss                                  0.0173529
---------------------------------------  ----------------
2021-06-04 14:00:46 | [train_policy] epoch #971 | Obtaining samples for iteration 971...
2021-06-04 14:00:47 | [train_policy] epoch #971 | Logging diagnostics...
2021-06-04 14:00:47 | [train_policy] epoch #971 | Optimizing policy...
2021-06-04 14:00:47 | [train_policy] epoch #971 | Computing loss before
2021-06-04 14:00:47 | [train_policy] epoch #971 | Computing KL before
2021-06-04 14:00:47 | [train_policy] epoch #971 | Optimizing
2021-06-04 14:00:47 | [train_policy] epoch #971 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:47 | [train_policy] epoch #971 | computing loss before
2021-06-04 14:00:47 | [train_policy] epoch #971 | computing gradient
2021-06-04 14:00:47 | [train_policy] epoch #971 | gradient computed
2021-06-04 14:00:47 | [train_policy] epoch #971 | computing descent direction
2021-06-04 14:00:47 | [train_policy] epoch #971 | descent direction computed
2021-06-04 14:00:47 | [train_policy] epoch #971 | backtrack iters: 0
2021-06-04 14:00:47 | [train_policy] epoch #971 | optimization finished
2021-06-04 14:00:47 | [train_policy] epoch #971 | Computing KL after
2021-06-04 14:00:47 | [train_policy] epoch #971 | Computing loss after
2021-06-04 14:00:47 | [train_policy] epoch #971 | Fitting baseline...
2021-06-04 14:00:47 | [train_policy] epoch #971 | Saving snapshot...
2021-06-04 14:00:47 | [train_policy] epoch #971 | Saved
2021-06-04 14:00:47 | [train_policy] epoch #971 | Time 779.46 s
2021-06-04 14:00:47 | [train_policy] epoch #971 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.283831
Evaluation/AverageDiscountedReturn          -41.5183
Evaluation/AverageReturn                    -41.5183
Evaluation/CompletionRate                     0
Evaluation/Iteration                        971
Evaluation/MaxReturn                        -31.382
Evaluation/MinReturn                        -78.8572
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.72864
Extras/EpisodeRewardMean                    -41.4495
LinearFeatureBaseline/ExplainedVariance       0.854759
PolicyExecTime                                0.229641
ProcessExecTime                               0.0311606
TotalEnvSteps                            983664
policy/Entropy                               -2.32866
policy/KL                                     0.00931731
policy/KLBefore                               0
policy/LossAfter                             -0.0258255
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.0974261
policy/dLoss                                  0.0258255
---------------------------------------  ----------------
2021-06-04 14:00:47 | [train_policy] epoch #972 | Obtaining samples for iteration 972...
2021-06-04 14:00:48 | [train_policy] epoch #972 | Logging diagnostics...
2021-06-04 14:00:48 | [train_policy] epoch #972 | Optimizing policy...
2021-06-04 14:00:48 | [train_policy] epoch #972 | Computing loss before
2021-06-04 14:00:48 | [train_policy] epoch #972 | Computing KL before
2021-06-04 14:00:48 | [train_policy] epoch #972 | Optimizing
2021-06-04 14:00:48 | [train_policy] epoch #972 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:48 | [train_policy] epoch #972 | computing loss before
2021-06-04 14:00:48 | [train_policy] epoch #972 | computing gradient
2021-06-04 14:00:48 | [train_policy] epoch #972 | gradient computed
2021-06-04 14:00:48 | [train_policy] epoch #972 | computing descent direction
2021-06-04 14:00:48 | [train_policy] epoch #972 | descent direction computed
2021-06-04 14:00:48 | [train_policy] epoch #972 | backtrack iters: 1
2021-06-04 14:00:48 | [train_policy] epoch #972 | optimization finished
2021-06-04 14:00:48 | [train_policy] epoch #972 | Computing KL after
2021-06-04 14:00:48 | [train_policy] epoch #972 | Computing loss after
2021-06-04 14:00:48 | [train_policy] epoch #972 | Fitting baseline...
2021-06-04 14:00:48 | [train_policy] epoch #972 | Saving snapshot...
2021-06-04 14:00:48 | [train_policy] epoch #972 | Saved
2021-06-04 14:00:48 | [train_policy] epoch #972 | Time 780.25 s
2021-06-04 14:00:48 | [train_policy] epoch #972 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284261
Evaluation/AverageDiscountedReturn          -40.9633
Evaluation/AverageReturn                    -40.9633
Evaluation/CompletionRate                     0
Evaluation/Iteration                        972
Evaluation/MaxReturn                        -28.7477
Evaluation/MinReturn                        -61.1202
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.82192
Extras/EpisodeRewardMean                    -41.1441
LinearFeatureBaseline/ExplainedVariance       0.874817
PolicyExecTime                                0.219661
ProcessExecTime                               0.0312216
TotalEnvSteps                            984676
policy/Entropy                               -2.32692
policy/KL                                     0.00645256
policy/KLBefore                               0
policy/LossAfter                             -0.0143189
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.0975955
policy/dLoss                                  0.0143189
---------------------------------------  ----------------
2021-06-04 14:00:48 | [train_policy] epoch #973 | Obtaining samples for iteration 973...
2021-06-04 14:00:49 | [train_policy] epoch #973 | Logging diagnostics...
2021-06-04 14:00:49 | [train_policy] epoch #973 | Optimizing policy...
2021-06-04 14:00:49 | [train_policy] epoch #973 | Computing loss before
2021-06-04 14:00:49 | [train_policy] epoch #973 | Computing KL before
2021-06-04 14:00:49 | [train_policy] epoch #973 | Optimizing
2021-06-04 14:00:49 | [train_policy] epoch #973 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:49 | [train_policy] epoch #973 | computing loss before
2021-06-04 14:00:49 | [train_policy] epoch #973 | computing gradient
2021-06-04 14:00:49 | [train_policy] epoch #973 | gradient computed
2021-06-04 14:00:49 | [train_policy] epoch #973 | computing descent direction
2021-06-04 14:00:49 | [train_policy] epoch #973 | descent direction computed
2021-06-04 14:00:49 | [train_policy] epoch #973 | backtrack iters: 1
2021-06-04 14:00:49 | [train_policy] epoch #973 | optimization finished
2021-06-04 14:00:49 | [train_policy] epoch #973 | Computing KL after
2021-06-04 14:00:49 | [train_policy] epoch #973 | Computing loss after
2021-06-04 14:00:49 | [train_policy] epoch #973 | Fitting baseline...
2021-06-04 14:00:49 | [train_policy] epoch #973 | Saving snapshot...
2021-06-04 14:00:49 | [train_policy] epoch #973 | Saved
2021-06-04 14:00:49 | [train_policy] epoch #973 | Time 781.06 s
2021-06-04 14:00:49 | [train_policy] epoch #973 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286026
Evaluation/AverageDiscountedReturn          -41.959
Evaluation/AverageReturn                    -41.959
Evaluation/CompletionRate                     0
Evaluation/Iteration                        973
Evaluation/MaxReturn                        -28.2194
Evaluation/MinReturn                        -81.6624
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.5232
Extras/EpisodeRewardMean                    -41.8367
LinearFeatureBaseline/ExplainedVariance       0.808485
PolicyExecTime                                0.232234
ProcessExecTime                               0.0312819
TotalEnvSteps                            985688
policy/Entropy                               -2.33628
policy/KL                                     0.0065108
policy/KLBefore                               0
policy/LossAfter                             -0.0193985
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.0966863
policy/dLoss                                  0.0193985
---------------------------------------  ----------------
2021-06-04 14:00:49 | [train_policy] epoch #974 | Obtaining samples for iteration 974...
2021-06-04 14:00:49 | [train_policy] epoch #974 | Logging diagnostics...
2021-06-04 14:00:49 | [train_policy] epoch #974 | Optimizing policy...
2021-06-04 14:00:49 | [train_policy] epoch #974 | Computing loss before
2021-06-04 14:00:49 | [train_policy] epoch #974 | Computing KL before
2021-06-04 14:00:50 | [train_policy] epoch #974 | Optimizing
2021-06-04 14:00:50 | [train_policy] epoch #974 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:50 | [train_policy] epoch #974 | computing loss before
2021-06-04 14:00:50 | [train_policy] epoch #974 | computing gradient
2021-06-04 14:00:50 | [train_policy] epoch #974 | gradient computed
2021-06-04 14:00:50 | [train_policy] epoch #974 | computing descent direction
2021-06-04 14:00:50 | [train_policy] epoch #974 | descent direction computed
2021-06-04 14:00:50 | [train_policy] epoch #974 | backtrack iters: 1
2021-06-04 14:00:50 | [train_policy] epoch #974 | optimization finished
2021-06-04 14:00:50 | [train_policy] epoch #974 | Computing KL after
2021-06-04 14:00:50 | [train_policy] epoch #974 | Computing loss after
2021-06-04 14:00:50 | [train_policy] epoch #974 | Fitting baseline...
2021-06-04 14:00:50 | [train_policy] epoch #974 | Saving snapshot...
2021-06-04 14:00:50 | [train_policy] epoch #974 | Saved
2021-06-04 14:00:50 | [train_policy] epoch #974 | Time 781.87 s
2021-06-04 14:00:50 | [train_policy] epoch #974 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.28623
Evaluation/AverageDiscountedReturn          -41.9097
Evaluation/AverageReturn                    -41.9097
Evaluation/CompletionRate                     0
Evaluation/Iteration                        974
Evaluation/MaxReturn                        -28.1616
Evaluation/MinReturn                        -80.9367
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.62186
Extras/EpisodeRewardMean                    -42.14
LinearFeatureBaseline/ExplainedVariance       0.856819
PolicyExecTime                                0.227683
ProcessExecTime                               0.0313191
TotalEnvSteps                            986700
policy/Entropy                               -2.34199
policy/KL                                     0.00640272
policy/KLBefore                               0
policy/LossAfter                             -0.0131808
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.096136
policy/dLoss                                  0.0131808
---------------------------------------  ----------------
2021-06-04 14:00:50 | [train_policy] epoch #975 | Obtaining samples for iteration 975...
2021-06-04 14:00:50 | [train_policy] epoch #975 | Logging diagnostics...
2021-06-04 14:00:50 | [train_policy] epoch #975 | Optimizing policy...
2021-06-04 14:00:50 | [train_policy] epoch #975 | Computing loss before
2021-06-04 14:00:50 | [train_policy] epoch #975 | Computing KL before
2021-06-04 14:00:50 | [train_policy] epoch #975 | Optimizing
2021-06-04 14:00:50 | [train_policy] epoch #975 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:50 | [train_policy] epoch #975 | computing loss before
2021-06-04 14:00:50 | [train_policy] epoch #975 | computing gradient
2021-06-04 14:00:50 | [train_policy] epoch #975 | gradient computed
2021-06-04 14:00:50 | [train_policy] epoch #975 | computing descent direction
2021-06-04 14:00:50 | [train_policy] epoch #975 | descent direction computed
2021-06-04 14:00:50 | [train_policy] epoch #975 | backtrack iters: 1
2021-06-04 14:00:50 | [train_policy] epoch #975 | optimization finished
2021-06-04 14:00:50 | [train_policy] epoch #975 | Computing KL after
2021-06-04 14:00:50 | [train_policy] epoch #975 | Computing loss after
2021-06-04 14:00:50 | [train_policy] epoch #975 | Fitting baseline...
2021-06-04 14:00:50 | [train_policy] epoch #975 | Saving snapshot...
2021-06-04 14:00:50 | [train_policy] epoch #975 | Saved
2021-06-04 14:00:50 | [train_policy] epoch #975 | Time 782.67 s
2021-06-04 14:00:50 | [train_policy] epoch #975 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285225
Evaluation/AverageDiscountedReturn          -41.5045
Evaluation/AverageReturn                    -41.5045
Evaluation/CompletionRate                     0
Evaluation/Iteration                        975
Evaluation/MaxReturn                        -31.244
Evaluation/MinReturn                        -91.7606
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.94699
Extras/EpisodeRewardMean                    -41.7135
LinearFeatureBaseline/ExplainedVariance       0.764779
PolicyExecTime                                0.222665
ProcessExecTime                               0.0312934
TotalEnvSteps                            987712
policy/Entropy                               -2.343
policy/KL                                     0.00640594
policy/KLBefore                               0
policy/LossAfter                             -0.0211231
policy/LossBefore                            -6.94995e-09
policy/Perplexity                             0.0960389
policy/dLoss                                  0.0211231
---------------------------------------  ----------------
2021-06-04 14:00:50 | [train_policy] epoch #976 | Obtaining samples for iteration 976...
2021-06-04 14:00:51 | [train_policy] epoch #976 | Logging diagnostics...
2021-06-04 14:00:51 | [train_policy] epoch #976 | Optimizing policy...
2021-06-04 14:00:51 | [train_policy] epoch #976 | Computing loss before
2021-06-04 14:00:51 | [train_policy] epoch #976 | Computing KL before
2021-06-04 14:00:51 | [train_policy] epoch #976 | Optimizing
2021-06-04 14:00:51 | [train_policy] epoch #976 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:51 | [train_policy] epoch #976 | computing loss before
2021-06-04 14:00:51 | [train_policy] epoch #976 | computing gradient
2021-06-04 14:00:51 | [train_policy] epoch #976 | gradient computed
2021-06-04 14:00:51 | [train_policy] epoch #976 | computing descent direction
2021-06-04 14:00:51 | [train_policy] epoch #976 | descent direction computed
2021-06-04 14:00:51 | [train_policy] epoch #976 | backtrack iters: 0
2021-06-04 14:00:51 | [train_policy] epoch #976 | optimization finished
2021-06-04 14:00:51 | [train_policy] epoch #976 | Computing KL after
2021-06-04 14:00:51 | [train_policy] epoch #976 | Computing loss after
2021-06-04 14:00:51 | [train_policy] epoch #976 | Fitting baseline...
2021-06-04 14:00:51 | [train_policy] epoch #976 | Saving snapshot...
2021-06-04 14:00:51 | [train_policy] epoch #976 | Saved
2021-06-04 14:00:51 | [train_policy] epoch #976 | Time 783.46 s
2021-06-04 14:00:51 | [train_policy] epoch #976 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.284402
Evaluation/AverageDiscountedReturn          -41.0182
Evaluation/AverageReturn                    -41.0182
Evaluation/CompletionRate                     0
Evaluation/Iteration                        976
Evaluation/MaxReturn                        -28.6082
Evaluation/MinReturn                        -64.0093
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.17661
Extras/EpisodeRewardMean                    -40.6897
LinearFeatureBaseline/ExplainedVariance       0.839252
PolicyExecTime                                0.22027
ProcessExecTime                               0.0311031
TotalEnvSteps                            988724
policy/Entropy                               -2.34449
policy/KL                                     0.00991203
policy/KLBefore                               0
policy/LossAfter                             -0.0240639
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.0958959
policy/dLoss                                  0.0240639
---------------------------------------  ---------------
2021-06-04 14:00:51 | [train_policy] epoch #977 | Obtaining samples for iteration 977...
2021-06-04 14:00:52 | [train_policy] epoch #977 | Logging diagnostics...
2021-06-04 14:00:52 | [train_policy] epoch #977 | Optimizing policy...
2021-06-04 14:00:52 | [train_policy] epoch #977 | Computing loss before
2021-06-04 14:00:52 | [train_policy] epoch #977 | Computing KL before
2021-06-04 14:00:52 | [train_policy] epoch #977 | Optimizing
2021-06-04 14:00:52 | [train_policy] epoch #977 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:52 | [train_policy] epoch #977 | computing loss before
2021-06-04 14:00:52 | [train_policy] epoch #977 | computing gradient
2021-06-04 14:00:52 | [train_policy] epoch #977 | gradient computed
2021-06-04 14:00:52 | [train_policy] epoch #977 | computing descent direction
2021-06-04 14:00:52 | [train_policy] epoch #977 | descent direction computed
2021-06-04 14:00:52 | [train_policy] epoch #977 | backtrack iters: 1
2021-06-04 14:00:52 | [train_policy] epoch #977 | optimization finished
2021-06-04 14:00:52 | [train_policy] epoch #977 | Computing KL after
2021-06-04 14:00:52 | [train_policy] epoch #977 | Computing loss after
2021-06-04 14:00:52 | [train_policy] epoch #977 | Fitting baseline...
2021-06-04 14:00:52 | [train_policy] epoch #977 | Saving snapshot...
2021-06-04 14:00:52 | [train_policy] epoch #977 | Saved
2021-06-04 14:00:52 | [train_policy] epoch #977 | Time 784.26 s
2021-06-04 14:00:52 | [train_policy] epoch #977 | EpochTime 0.78 s
---------------------------------------  ---------------
EnvExecTime                                   0.285035
Evaluation/AverageDiscountedReturn          -40.6614
Evaluation/AverageReturn                    -40.6614
Evaluation/CompletionRate                     0
Evaluation/Iteration                        977
Evaluation/MaxReturn                        -28.7067
Evaluation/MinReturn                        -83.8051
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.38135
Extras/EpisodeRewardMean                    -40.4026
LinearFeatureBaseline/ExplainedVariance       0.852528
PolicyExecTime                                0.218759
ProcessExecTime                               0.0311854
TotalEnvSteps                            989736
policy/Entropy                               -2.37264
policy/KL                                     0.00661845
policy/KLBefore                               0
policy/LossAfter                             -0.0145353
policy/LossBefore                            -5.4775e-09
policy/Perplexity                             0.0932343
policy/dLoss                                  0.0145353
---------------------------------------  ---------------
2021-06-04 14:00:52 | [train_policy] epoch #978 | Obtaining samples for iteration 978...
2021-06-04 14:00:53 | [train_policy] epoch #978 | Logging diagnostics...
2021-06-04 14:00:53 | [train_policy] epoch #978 | Optimizing policy...
2021-06-04 14:00:53 | [train_policy] epoch #978 | Computing loss before
2021-06-04 14:00:53 | [train_policy] epoch #978 | Computing KL before
2021-06-04 14:00:53 | [train_policy] epoch #978 | Optimizing
2021-06-04 14:00:53 | [train_policy] epoch #978 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:53 | [train_policy] epoch #978 | computing loss before
2021-06-04 14:00:53 | [train_policy] epoch #978 | computing gradient
2021-06-04 14:00:53 | [train_policy] epoch #978 | gradient computed
2021-06-04 14:00:53 | [train_policy] epoch #978 | computing descent direction
2021-06-04 14:00:53 | [train_policy] epoch #978 | descent direction computed
2021-06-04 14:00:53 | [train_policy] epoch #978 | backtrack iters: 0
2021-06-04 14:00:53 | [train_policy] epoch #978 | optimization finished
2021-06-04 14:00:53 | [train_policy] epoch #978 | Computing KL after
2021-06-04 14:00:53 | [train_policy] epoch #978 | Computing loss after
2021-06-04 14:00:53 | [train_policy] epoch #978 | Fitting baseline...
2021-06-04 14:00:53 | [train_policy] epoch #978 | Saving snapshot...
2021-06-04 14:00:53 | [train_policy] epoch #978 | Saved
2021-06-04 14:00:53 | [train_policy] epoch #978 | Time 785.07 s
2021-06-04 14:00:53 | [train_policy] epoch #978 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.284046
Evaluation/AverageDiscountedReturn          -38.9126
Evaluation/AverageReturn                    -38.9126
Evaluation/CompletionRate                     0
Evaluation/Iteration                        978
Evaluation/MaxReturn                        -28.7802
Evaluation/MinReturn                        -63.5445
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.20424
Extras/EpisodeRewardMean                    -39.0739
LinearFeatureBaseline/ExplainedVariance       0.871771
PolicyExecTime                                0.21823
ProcessExecTime                               0.0312302
TotalEnvSteps                            990748
policy/Entropy                               -2.36482
policy/KL                                     0.00979036
policy/KLBefore                               0
policy/LossAfter                             -0.0200233
policy/LossBefore                            -2.35591e-10
policy/Perplexity                             0.0939663
policy/dLoss                                  0.0200233
---------------------------------------  ----------------
2021-06-04 14:00:53 | [train_policy] epoch #979 | Obtaining samples for iteration 979...
2021-06-04 14:00:53 | [train_policy] epoch #979 | Logging diagnostics...
2021-06-04 14:00:53 | [train_policy] epoch #979 | Optimizing policy...
2021-06-04 14:00:53 | [train_policy] epoch #979 | Computing loss before
2021-06-04 14:00:53 | [train_policy] epoch #979 | Computing KL before
2021-06-04 14:00:53 | [train_policy] epoch #979 | Optimizing
2021-06-04 14:00:54 | [train_policy] epoch #979 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:54 | [train_policy] epoch #979 | computing loss before
2021-06-04 14:00:54 | [train_policy] epoch #979 | computing gradient
2021-06-04 14:00:54 | [train_policy] epoch #979 | gradient computed
2021-06-04 14:00:54 | [train_policy] epoch #979 | computing descent direction
2021-06-04 14:00:54 | [train_policy] epoch #979 | descent direction computed
2021-06-04 14:00:54 | [train_policy] epoch #979 | backtrack iters: 0
2021-06-04 14:00:54 | [train_policy] epoch #979 | optimization finished
2021-06-04 14:00:54 | [train_policy] epoch #979 | Computing KL after
2021-06-04 14:00:54 | [train_policy] epoch #979 | Computing loss after
2021-06-04 14:00:54 | [train_policy] epoch #979 | Fitting baseline...
2021-06-04 14:00:54 | [train_policy] epoch #979 | Saving snapshot...
2021-06-04 14:00:54 | [train_policy] epoch #979 | Saved
2021-06-04 14:00:54 | [train_policy] epoch #979 | Time 785.87 s
2021-06-04 14:00:54 | [train_policy] epoch #979 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.285227
Evaluation/AverageDiscountedReturn          -39.3261
Evaluation/AverageReturn                    -39.3261
Evaluation/CompletionRate                     0
Evaluation/Iteration                        979
Evaluation/MaxReturn                        -28.2226
Evaluation/MinReturn                        -63.7696
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.53549
Extras/EpisodeRewardMean                    -39.3262
LinearFeatureBaseline/ExplainedVariance       0.900788
PolicyExecTime                                0.22374
ProcessExecTime                               0.0312192
TotalEnvSteps                            991760
policy/Entropy                               -2.37894
policy/KL                                     0.00991451
policy/KLBefore                               0
policy/LossAfter                             -0.0121307
policy/LossBefore                             3.53387e-09
policy/Perplexity                             0.0926491
policy/dLoss                                  0.0121307
---------------------------------------  ----------------
2021-06-04 14:00:54 | [train_policy] epoch #980 | Obtaining samples for iteration 980...
2021-06-04 14:00:54 | [train_policy] epoch #980 | Logging diagnostics...
2021-06-04 14:00:54 | [train_policy] epoch #980 | Optimizing policy...
2021-06-04 14:00:54 | [train_policy] epoch #980 | Computing loss before
2021-06-04 14:00:54 | [train_policy] epoch #980 | Computing KL before
2021-06-04 14:00:54 | [train_policy] epoch #980 | Optimizing
2021-06-04 14:00:54 | [train_policy] epoch #980 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:54 | [train_policy] epoch #980 | computing loss before
2021-06-04 14:00:54 | [train_policy] epoch #980 | computing gradient
2021-06-04 14:00:54 | [train_policy] epoch #980 | gradient computed
2021-06-04 14:00:54 | [train_policy] epoch #980 | computing descent direction
2021-06-04 14:00:54 | [train_policy] epoch #980 | descent direction computed
2021-06-04 14:00:54 | [train_policy] epoch #980 | backtrack iters: 0
2021-06-04 14:00:54 | [train_policy] epoch #980 | optimization finished
2021-06-04 14:00:54 | [train_policy] epoch #980 | Computing KL after
2021-06-04 14:00:54 | [train_policy] epoch #980 | Computing loss after
2021-06-04 14:00:54 | [train_policy] epoch #980 | Fitting baseline...
2021-06-04 14:00:54 | [train_policy] epoch #980 | Saving snapshot...
2021-06-04 14:00:54 | [train_policy] epoch #980 | Saved
2021-06-04 14:00:54 | [train_policy] epoch #980 | Time 786.69 s
2021-06-04 14:00:54 | [train_policy] epoch #980 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.284188
Evaluation/AverageDiscountedReturn          -41.2058
Evaluation/AverageReturn                    -41.2058
Evaluation/CompletionRate                     0
Evaluation/Iteration                        980
Evaluation/MaxReturn                        -29.9556
Evaluation/MinReturn                        -83.1352
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.92955
Extras/EpisodeRewardMean                    -40.9102
LinearFeatureBaseline/ExplainedVariance       0.836182
PolicyExecTime                                0.220963
ProcessExecTime                               0.0312469
TotalEnvSteps                            992772
policy/Entropy                               -2.37642
policy/KL                                     0.0095907
policy/KLBefore                               0
policy/LossAfter                             -0.018854
policy/LossBefore                            -1.10728e-08
policy/Perplexity                             0.0928829
policy/dLoss                                  0.0188539
---------------------------------------  ----------------
2021-06-04 14:00:54 | [train_policy] epoch #981 | Obtaining samples for iteration 981...
2021-06-04 14:00:55 | [train_policy] epoch #981 | Logging diagnostics...
2021-06-04 14:00:55 | [train_policy] epoch #981 | Optimizing policy...
2021-06-04 14:00:55 | [train_policy] epoch #981 | Computing loss before
2021-06-04 14:00:55 | [train_policy] epoch #981 | Computing KL before
2021-06-04 14:00:55 | [train_policy] epoch #981 | Optimizing
2021-06-04 14:00:55 | [train_policy] epoch #981 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:55 | [train_policy] epoch #981 | computing loss before
2021-06-04 14:00:55 | [train_policy] epoch #981 | computing gradient
2021-06-04 14:00:55 | [train_policy] epoch #981 | gradient computed
2021-06-04 14:00:55 | [train_policy] epoch #981 | computing descent direction
2021-06-04 14:00:55 | [train_policy] epoch #981 | descent direction computed
2021-06-04 14:00:55 | [train_policy] epoch #981 | backtrack iters: 0
2021-06-04 14:00:55 | [train_policy] epoch #981 | optimization finished
2021-06-04 14:00:55 | [train_policy] epoch #981 | Computing KL after
2021-06-04 14:00:55 | [train_policy] epoch #981 | Computing loss after
2021-06-04 14:00:55 | [train_policy] epoch #981 | Fitting baseline...
2021-06-04 14:00:55 | [train_policy] epoch #981 | Saving snapshot...
2021-06-04 14:00:55 | [train_policy] epoch #981 | Saved
2021-06-04 14:00:55 | [train_policy] epoch #981 | Time 787.48 s
2021-06-04 14:00:55 | [train_policy] epoch #981 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.284233
Evaluation/AverageDiscountedReturn          -40.9256
Evaluation/AverageReturn                    -40.9256
Evaluation/CompletionRate                     0
Evaluation/Iteration                        981
Evaluation/MaxReturn                        -30.13
Evaluation/MinReturn                        -81.3586
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.50837
Extras/EpisodeRewardMean                    -40.888
LinearFeatureBaseline/ExplainedVariance       0.820174
PolicyExecTime                                0.221539
ProcessExecTime                               0.0311515
TotalEnvSteps                            993784
policy/Entropy                               -2.38566
policy/KL                                     0.00990519
policy/KLBefore                               0
policy/LossAfter                             -0.0144984
policy/LossBefore                            -1.46067e-08
policy/Perplexity                             0.0920282
policy/dLoss                                  0.0144984
---------------------------------------  ----------------
2021-06-04 14:00:55 | [train_policy] epoch #982 | Obtaining samples for iteration 982...
2021-06-04 14:00:56 | [train_policy] epoch #982 | Logging diagnostics...
2021-06-04 14:00:56 | [train_policy] epoch #982 | Optimizing policy...
2021-06-04 14:00:56 | [train_policy] epoch #982 | Computing loss before
2021-06-04 14:00:56 | [train_policy] epoch #982 | Computing KL before
2021-06-04 14:00:56 | [train_policy] epoch #982 | Optimizing
2021-06-04 14:00:56 | [train_policy] epoch #982 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:56 | [train_policy] epoch #982 | computing loss before
2021-06-04 14:00:56 | [train_policy] epoch #982 | computing gradient
2021-06-04 14:00:56 | [train_policy] epoch #982 | gradient computed
2021-06-04 14:00:56 | [train_policy] epoch #982 | computing descent direction
2021-06-04 14:00:56 | [train_policy] epoch #982 | descent direction computed
2021-06-04 14:00:56 | [train_policy] epoch #982 | backtrack iters: 1
2021-06-04 14:00:56 | [train_policy] epoch #982 | optimization finished
2021-06-04 14:00:56 | [train_policy] epoch #982 | Computing KL after
2021-06-04 14:00:56 | [train_policy] epoch #982 | Computing loss after
2021-06-04 14:00:56 | [train_policy] epoch #982 | Fitting baseline...
2021-06-04 14:00:56 | [train_policy] epoch #982 | Saving snapshot...
2021-06-04 14:00:56 | [train_policy] epoch #982 | Saved
2021-06-04 14:00:56 | [train_policy] epoch #982 | Time 788.27 s
2021-06-04 14:00:56 | [train_policy] epoch #982 | EpochTime 0.77 s
---------------------------------------  ----------------
EnvExecTime                                   0.285826
Evaluation/AverageDiscountedReturn          -41.5837
Evaluation/AverageReturn                    -41.5837
Evaluation/CompletionRate                     0
Evaluation/Iteration                        982
Evaluation/MaxReturn                        -28.9046
Evaluation/MinReturn                        -82.4873
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.1497
Extras/EpisodeRewardMean                    -41.2295
LinearFeatureBaseline/ExplainedVariance       0.863033
PolicyExecTime                                0.217114
ProcessExecTime                               0.0313265
TotalEnvSteps                            994796
policy/Entropy                               -2.39353
policy/KL                                     0.0064288
policy/KLBefore                               0
policy/LossAfter                             -0.0157458
policy/LossBefore                             1.60202e-08
policy/Perplexity                             0.0913066
policy/dLoss                                  0.0157458
---------------------------------------  ----------------
2021-06-04 14:00:56 | [train_policy] epoch #983 | Obtaining samples for iteration 983...
2021-06-04 14:00:57 | [train_policy] epoch #983 | Logging diagnostics...
2021-06-04 14:00:57 | [train_policy] epoch #983 | Optimizing policy...
2021-06-04 14:00:57 | [train_policy] epoch #983 | Computing loss before
2021-06-04 14:00:57 | [train_policy] epoch #983 | Computing KL before
2021-06-04 14:00:57 | [train_policy] epoch #983 | Optimizing
2021-06-04 14:00:57 | [train_policy] epoch #983 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:57 | [train_policy] epoch #983 | computing loss before
2021-06-04 14:00:57 | [train_policy] epoch #983 | computing gradient
2021-06-04 14:00:57 | [train_policy] epoch #983 | gradient computed
2021-06-04 14:00:57 | [train_policy] epoch #983 | computing descent direction
2021-06-04 14:00:57 | [train_policy] epoch #983 | descent direction computed
2021-06-04 14:00:57 | [train_policy] epoch #983 | backtrack iters: 0
2021-06-04 14:00:57 | [train_policy] epoch #983 | optimization finished
2021-06-04 14:00:57 | [train_policy] epoch #983 | Computing KL after
2021-06-04 14:00:57 | [train_policy] epoch #983 | Computing loss after
2021-06-04 14:00:57 | [train_policy] epoch #983 | Fitting baseline...
2021-06-04 14:00:57 | [train_policy] epoch #983 | Saving snapshot...
2021-06-04 14:00:57 | [train_policy] epoch #983 | Saved
2021-06-04 14:00:57 | [train_policy] epoch #983 | Time 789.08 s
2021-06-04 14:00:57 | [train_policy] epoch #983 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.285298
Evaluation/AverageDiscountedReturn          -41.4347
Evaluation/AverageReturn                    -41.4347
Evaluation/CompletionRate                     0
Evaluation/Iteration                        983
Evaluation/MaxReturn                        -28.8295
Evaluation/MinReturn                        -64.1276
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.84452
Extras/EpisodeRewardMean                    -41.1584
LinearFeatureBaseline/ExplainedVariance       0.868332
PolicyExecTime                                0.232359
ProcessExecTime                               0.0313382
TotalEnvSteps                            995808
policy/Entropy                               -2.4057
policy/KL                                     0.00972077
policy/KLBefore                               0
policy/LossAfter                             -0.0167479
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.0902025
policy/dLoss                                  0.0167479
---------------------------------------  ----------------
2021-06-04 14:00:57 | [train_policy] epoch #984 | Obtaining samples for iteration 984...
2021-06-04 14:00:58 | [train_policy] epoch #984 | Logging diagnostics...
2021-06-04 14:00:58 | [train_policy] epoch #984 | Optimizing policy...
2021-06-04 14:00:58 | [train_policy] epoch #984 | Computing loss before
2021-06-04 14:00:58 | [train_policy] epoch #984 | Computing KL before
2021-06-04 14:00:58 | [train_policy] epoch #984 | Optimizing
2021-06-04 14:00:58 | [train_policy] epoch #984 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:58 | [train_policy] epoch #984 | computing loss before
2021-06-04 14:00:58 | [train_policy] epoch #984 | computing gradient
2021-06-04 14:00:58 | [train_policy] epoch #984 | gradient computed
2021-06-04 14:00:58 | [train_policy] epoch #984 | computing descent direction
2021-06-04 14:00:58 | [train_policy] epoch #984 | descent direction computed
2021-06-04 14:00:58 | [train_policy] epoch #984 | backtrack iters: 0
2021-06-04 14:00:58 | [train_policy] epoch #984 | optimization finished
2021-06-04 14:00:58 | [train_policy] epoch #984 | Computing KL after
2021-06-04 14:00:58 | [train_policy] epoch #984 | Computing loss after
2021-06-04 14:00:58 | [train_policy] epoch #984 | Fitting baseline...
2021-06-04 14:00:58 | [train_policy] epoch #984 | Saving snapshot...
2021-06-04 14:00:58 | [train_policy] epoch #984 | Saved
2021-06-04 14:00:58 | [train_policy] epoch #984 | Time 789.89 s
2021-06-04 14:00:58 | [train_policy] epoch #984 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283854
Evaluation/AverageDiscountedReturn          -39.9529
Evaluation/AverageReturn                    -39.9529
Evaluation/CompletionRate                     0
Evaluation/Iteration                        984
Evaluation/MaxReturn                        -28.4344
Evaluation/MinReturn                        -64.2601
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.18538
Extras/EpisodeRewardMean                    -40.0954
LinearFeatureBaseline/ExplainedVariance       0.876329
PolicyExecTime                                0.222328
ProcessExecTime                               0.0312202
TotalEnvSteps                            996820
policy/Entropy                               -2.3964
policy/KL                                     0.00994266
policy/KLBefore                               0
policy/LossAfter                             -0.0161645
policy/LossBefore                            -4.47624e-09
policy/Perplexity                             0.0910447
policy/dLoss                                  0.0161645
---------------------------------------  ----------------
2021-06-04 14:00:58 | [train_policy] epoch #985 | Obtaining samples for iteration 985...
2021-06-04 14:00:58 | [train_policy] epoch #985 | Logging diagnostics...
2021-06-04 14:00:58 | [train_policy] epoch #985 | Optimizing policy...
2021-06-04 14:00:58 | [train_policy] epoch #985 | Computing loss before
2021-06-04 14:00:58 | [train_policy] epoch #985 | Computing KL before
2021-06-04 14:00:58 | [train_policy] epoch #985 | Optimizing
2021-06-04 14:00:58 | [train_policy] epoch #985 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:58 | [train_policy] epoch #985 | computing loss before
2021-06-04 14:00:58 | [train_policy] epoch #985 | computing gradient
2021-06-04 14:00:58 | [train_policy] epoch #985 | gradient computed
2021-06-04 14:00:58 | [train_policy] epoch #985 | computing descent direction
2021-06-04 14:00:58 | [train_policy] epoch #985 | descent direction computed
2021-06-04 14:00:58 | [train_policy] epoch #985 | backtrack iters: 1
2021-06-04 14:00:58 | [train_policy] epoch #985 | optimization finished
2021-06-04 14:00:58 | [train_policy] epoch #985 | Computing KL after
2021-06-04 14:00:58 | [train_policy] epoch #985 | Computing loss after
2021-06-04 14:00:58 | [train_policy] epoch #985 | Fitting baseline...
2021-06-04 14:00:58 | [train_policy] epoch #985 | Saving snapshot...
2021-06-04 14:00:58 | [train_policy] epoch #985 | Saved
2021-06-04 14:00:58 | [train_policy] epoch #985 | Time 790.69 s
2021-06-04 14:00:58 | [train_policy] epoch #985 | EpochTime 0.78 s
---------------------------------------  ----------------
EnvExecTime                                   0.283055
Evaluation/AverageDiscountedReturn          -41.3077
Evaluation/AverageReturn                    -41.3077
Evaluation/CompletionRate                     0
Evaluation/Iteration                        985
Evaluation/MaxReturn                        -31.3031
Evaluation/MinReturn                        -63.8585
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85761
Extras/EpisodeRewardMean                    -41.0823
LinearFeatureBaseline/ExplainedVariance       0.888343
PolicyExecTime                                0.227725
ProcessExecTime                               0.031029
TotalEnvSteps                            997832
policy/Entropy                               -2.38574
policy/KL                                     0.00649905
policy/KLBefore                               0
policy/LossAfter                             -0.016258
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             0.092021
policy/dLoss                                  0.016258
---------------------------------------  ----------------
2021-06-04 14:00:58 | [train_policy] epoch #986 | Obtaining samples for iteration 986...
2021-06-04 14:00:59 | [train_policy] epoch #986 | Logging diagnostics...
2021-06-04 14:00:59 | [train_policy] epoch #986 | Optimizing policy...
2021-06-04 14:00:59 | [train_policy] epoch #986 | Computing loss before
2021-06-04 14:00:59 | [train_policy] epoch #986 | Computing KL before
2021-06-04 14:00:59 | [train_policy] epoch #986 | Optimizing
2021-06-04 14:00:59 | [train_policy] epoch #986 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:00:59 | [train_policy] epoch #986 | computing loss before
2021-06-04 14:00:59 | [train_policy] epoch #986 | computing gradient
2021-06-04 14:00:59 | [train_policy] epoch #986 | gradient computed
2021-06-04 14:00:59 | [train_policy] epoch #986 | computing descent direction
2021-06-04 14:00:59 | [train_policy] epoch #986 | descent direction computed
2021-06-04 14:00:59 | [train_policy] epoch #986 | backtrack iters: 0
2021-06-04 14:00:59 | [train_policy] epoch #986 | optimization finished
2021-06-04 14:00:59 | [train_policy] epoch #986 | Computing KL after
2021-06-04 14:00:59 | [train_policy] epoch #986 | Computing loss after
2021-06-04 14:00:59 | [train_policy] epoch #986 | Fitting baseline...
2021-06-04 14:00:59 | [train_policy] epoch #986 | Saving snapshot...
2021-06-04 14:00:59 | [train_policy] epoch #986 | Saved
2021-06-04 14:00:59 | [train_policy] epoch #986 | Time 791.49 s
2021-06-04 14:00:59 | [train_policy] epoch #986 | EpochTime 0.77 s
---------------------------------------  ---------------
EnvExecTime                                   0.284365
Evaluation/AverageDiscountedReturn          -40.6335
Evaluation/AverageReturn                    -40.6335
Evaluation/CompletionRate                     0
Evaluation/Iteration                        986
Evaluation/MaxReturn                        -28.2861
Evaluation/MinReturn                        -81.9213
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.62816
Extras/EpisodeRewardMean                    -40.4149
LinearFeatureBaseline/ExplainedVariance       0.827869
PolicyExecTime                                0.231328
ProcessExecTime                               0.0312119
TotalEnvSteps                            998844
policy/Entropy                               -2.36722
policy/KL                                     0.00975093
policy/KLBefore                               0
policy/LossAfter                             -0.0270875
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             0.0937413
policy/dLoss                                  0.0270875
---------------------------------------  ---------------
2021-06-04 14:00:59 | [train_policy] epoch #987 | Obtaining samples for iteration 987...
2021-06-04 14:01:00 | [train_policy] epoch #987 | Logging diagnostics...
2021-06-04 14:01:00 | [train_policy] epoch #987 | Optimizing policy...
2021-06-04 14:01:00 | [train_policy] epoch #987 | Computing loss before
2021-06-04 14:01:00 | [train_policy] epoch #987 | Computing KL before
2021-06-04 14:01:00 | [train_policy] epoch #987 | Optimizing
2021-06-04 14:01:00 | [train_policy] epoch #987 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:00 | [train_policy] epoch #987 | computing loss before
2021-06-04 14:01:00 | [train_policy] epoch #987 | computing gradient
2021-06-04 14:01:00 | [train_policy] epoch #987 | gradient computed
2021-06-04 14:01:00 | [train_policy] epoch #987 | computing descent direction
2021-06-04 14:01:00 | [train_policy] epoch #987 | descent direction computed
2021-06-04 14:01:00 | [train_policy] epoch #987 | backtrack iters: 1
2021-06-04 14:01:00 | [train_policy] epoch #987 | optimization finished
2021-06-04 14:01:00 | [train_policy] epoch #987 | Computing KL after
2021-06-04 14:01:00 | [train_policy] epoch #987 | Computing loss after
2021-06-04 14:01:00 | [train_policy] epoch #987 | Fitting baseline...
2021-06-04 14:01:00 | [train_policy] epoch #987 | Saving snapshot...
2021-06-04 14:01:00 | [train_policy] epoch #987 | Saved
2021-06-04 14:01:00 | [train_policy] epoch #987 | Time 792.30 s
2021-06-04 14:01:00 | [train_policy] epoch #987 | EpochTime 0.79 s
---------------------------------------  ----------------
EnvExecTime                                   0.286106
Evaluation/AverageDiscountedReturn          -41.3697
Evaluation/AverageReturn                    -41.3697
Evaluation/CompletionRate                     0
Evaluation/Iteration                        987
Evaluation/MaxReturn                        -28.1928
Evaluation/MinReturn                        -79.5059
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.25798
Extras/EpisodeRewardMean                    -41.3197
LinearFeatureBaseline/ExplainedVariance       0.859076
PolicyExecTime                                0.226505
ProcessExecTime                               0.0315804
TotalEnvSteps                            999856
policy/Entropy                               -2.3885
policy/KL                                     0.00662509
policy/KLBefore                               0
policy/LossAfter                             -0.023114
policy/LossBefore                            -4.47624e-09
policy/Perplexity                             0.0917668
policy/dLoss                                  0.023114
---------------------------------------  ----------------
2021-06-04 14:01:00 | [train_policy] epoch #988 | Obtaining samples for iteration 988...
2021-06-04 14:01:01 | [train_policy] epoch #988 | Logging diagnostics...
2021-06-04 14:01:01 | [train_policy] epoch #988 | Optimizing policy...
2021-06-04 14:01:01 | [train_policy] epoch #988 | Computing loss before
2021-06-04 14:01:01 | [train_policy] epoch #988 | Computing KL before
2021-06-04 14:01:01 | [train_policy] epoch #988 | Optimizing
2021-06-04 14:01:01 | [train_policy] epoch #988 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:01 | [train_policy] epoch #988 | computing loss before
2021-06-04 14:01:01 | [train_policy] epoch #988 | computing gradient
2021-06-04 14:01:01 | [train_policy] epoch #988 | gradient computed
2021-06-04 14:01:01 | [train_policy] epoch #988 | computing descent direction
2021-06-04 14:01:01 | [train_policy] epoch #988 | descent direction computed
2021-06-04 14:01:01 | [train_policy] epoch #988 | backtrack iters: 1
2021-06-04 14:01:01 | [train_policy] epoch #988 | optimization finished
2021-06-04 14:01:01 | [train_policy] epoch #988 | Computing KL after
2021-06-04 14:01:01 | [train_policy] epoch #988 | Computing loss after
2021-06-04 14:01:01 | [train_policy] epoch #988 | Fitting baseline...
2021-06-04 14:01:01 | [train_policy] epoch #988 | Saving snapshot...
2021-06-04 14:01:01 | [train_policy] epoch #988 | Saved
2021-06-04 14:01:01 | [train_policy] epoch #988 | Time 793.11 s
2021-06-04 14:01:01 | [train_policy] epoch #988 | EpochTime 0.79 s
---------------------------------------  -------------
EnvExecTime                                0.285474
Evaluation/AverageDiscountedReturn       -40.2695
Evaluation/AverageReturn                 -40.2695
Evaluation/CompletionRate                  0
Evaluation/Iteration                     988
Evaluation/MaxReturn                     -28.5734
Evaluation/MinReturn                     -64.0322
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.8342
Extras/EpisodeRewardMean                 -40.2313
LinearFeatureBaseline/ExplainedVariance    0.896306
PolicyExecTime                             0.233438
ProcessExecTime                            0.0311623
TotalEnvSteps                              1.00087e+06
policy/Entropy                            -2.43023
policy/KL                                  0.00661207
policy/KLBefore                            0
policy/LossAfter                          -0.0154534
policy/LossBefore                         -1.88473e-09
policy/Perplexity                          0.0880162
policy/dLoss                               0.0154534
---------------------------------------  -------------
2021-06-04 14:01:01 | [train_policy] epoch #989 | Obtaining samples for iteration 989...
2021-06-04 14:01:02 | [train_policy] epoch #989 | Logging diagnostics...
2021-06-04 14:01:02 | [train_policy] epoch #989 | Optimizing policy...
2021-06-04 14:01:02 | [train_policy] epoch #989 | Computing loss before
2021-06-04 14:01:02 | [train_policy] epoch #989 | Computing KL before
2021-06-04 14:01:02 | [train_policy] epoch #989 | Optimizing
2021-06-04 14:01:02 | [train_policy] epoch #989 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:02 | [train_policy] epoch #989 | computing loss before
2021-06-04 14:01:02 | [train_policy] epoch #989 | computing gradient
2021-06-04 14:01:02 | [train_policy] epoch #989 | gradient computed
2021-06-04 14:01:02 | [train_policy] epoch #989 | computing descent direction
2021-06-04 14:01:02 | [train_policy] epoch #989 | descent direction computed
2021-06-04 14:01:02 | [train_policy] epoch #989 | backtrack iters: 1
2021-06-04 14:01:02 | [train_policy] epoch #989 | optimization finished
2021-06-04 14:01:02 | [train_policy] epoch #989 | Computing KL after
2021-06-04 14:01:02 | [train_policy] epoch #989 | Computing loss after
2021-06-04 14:01:02 | [train_policy] epoch #989 | Fitting baseline...
2021-06-04 14:01:02 | [train_policy] epoch #989 | Saving snapshot...
2021-06-04 14:01:02 | [train_policy] epoch #989 | Saved
2021-06-04 14:01:02 | [train_policy] epoch #989 | Time 793.93 s
2021-06-04 14:01:02 | [train_policy] epoch #989 | EpochTime 0.80 s
---------------------------------------  -------------
EnvExecTime                                0.290583
Evaluation/AverageDiscountedReturn       -41.2661
Evaluation/AverageReturn                 -41.2661
Evaluation/CompletionRate                  0
Evaluation/Iteration                     989
Evaluation/MaxReturn                     -28.6713
Evaluation/MinReturn                     -63.9658
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.04317
Extras/EpisodeRewardMean                 -41.5791
LinearFeatureBaseline/ExplainedVariance    0.889597
PolicyExecTime                             0.228534
ProcessExecTime                            0.031899
TotalEnvSteps                              1.00188e+06
policy/Entropy                            -2.4422
policy/KL                                  0.00641147
policy/KLBefore                            0
policy/LossAfter                          -0.0119168
policy/LossBefore                          1.0366e-08
policy/Perplexity                          0.0869693
policy/dLoss                               0.0119168
---------------------------------------  -------------
2021-06-04 14:01:02 | [train_policy] epoch #990 | Obtaining samples for iteration 990...
2021-06-04 14:01:02 | [train_policy] epoch #990 | Logging diagnostics...
2021-06-04 14:01:02 | [train_policy] epoch #990 | Optimizing policy...
2021-06-04 14:01:02 | [train_policy] epoch #990 | Computing loss before
2021-06-04 14:01:02 | [train_policy] epoch #990 | Computing KL before
2021-06-04 14:01:02 | [train_policy] epoch #990 | Optimizing
2021-06-04 14:01:02 | [train_policy] epoch #990 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:02 | [train_policy] epoch #990 | computing loss before
2021-06-04 14:01:02 | [train_policy] epoch #990 | computing gradient
2021-06-04 14:01:02 | [train_policy] epoch #990 | gradient computed
2021-06-04 14:01:02 | [train_policy] epoch #990 | computing descent direction
2021-06-04 14:01:02 | [train_policy] epoch #990 | descent direction computed
2021-06-04 14:01:02 | [train_policy] epoch #990 | backtrack iters: 1
2021-06-04 14:01:02 | [train_policy] epoch #990 | optimization finished
2021-06-04 14:01:02 | [train_policy] epoch #990 | Computing KL after
2021-06-04 14:01:02 | [train_policy] epoch #990 | Computing loss after
2021-06-04 14:01:02 | [train_policy] epoch #990 | Fitting baseline...
2021-06-04 14:01:02 | [train_policy] epoch #990 | Saving snapshot...
2021-06-04 14:01:03 | [train_policy] epoch #990 | Saved
2021-06-04 14:01:03 | [train_policy] epoch #990 | Time 794.74 s
2021-06-04 14:01:03 | [train_policy] epoch #990 | EpochTime 0.78 s
---------------------------------------  -------------
EnvExecTime                                0.28311
Evaluation/AverageDiscountedReturn       -40.7487
Evaluation/AverageReturn                 -40.7487
Evaluation/CompletionRate                  0
Evaluation/Iteration                     990
Evaluation/MaxReturn                     -28.6293
Evaluation/MinReturn                     -79.9988
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.92332
Extras/EpisodeRewardMean                 -40.9748
LinearFeatureBaseline/ExplainedVariance    0.849563
PolicyExecTime                             0.225398
ProcessExecTime                            0.0311806
TotalEnvSteps                              1.00289e+06
policy/Entropy                            -2.45536
policy/KL                                  0.0064315
policy/KLBefore                            0
policy/LossAfter                          -0.0106899
policy/LossBefore                          8.01011e-09
policy/Perplexity                          0.0858319
policy/dLoss                               0.0106899
---------------------------------------  -------------
2021-06-04 14:01:03 | [train_policy] epoch #991 | Obtaining samples for iteration 991...
2021-06-04 14:01:03 | [train_policy] epoch #991 | Logging diagnostics...
2021-06-04 14:01:03 | [train_policy] epoch #991 | Optimizing policy...
2021-06-04 14:01:03 | [train_policy] epoch #991 | Computing loss before
2021-06-04 14:01:03 | [train_policy] epoch #991 | Computing KL before
2021-06-04 14:01:03 | [train_policy] epoch #991 | Optimizing
2021-06-04 14:01:03 | [train_policy] epoch #991 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:03 | [train_policy] epoch #991 | computing loss before
2021-06-04 14:01:03 | [train_policy] epoch #991 | computing gradient
2021-06-04 14:01:03 | [train_policy] epoch #991 | gradient computed
2021-06-04 14:01:03 | [train_policy] epoch #991 | computing descent direction
2021-06-04 14:01:03 | [train_policy] epoch #991 | descent direction computed
2021-06-04 14:01:03 | [train_policy] epoch #991 | backtrack iters: 1
2021-06-04 14:01:03 | [train_policy] epoch #991 | optimization finished
2021-06-04 14:01:03 | [train_policy] epoch #991 | Computing KL after
2021-06-04 14:01:03 | [train_policy] epoch #991 | Computing loss after
2021-06-04 14:01:03 | [train_policy] epoch #991 | Fitting baseline...
2021-06-04 14:01:03 | [train_policy] epoch #991 | Saving snapshot...
2021-06-04 14:01:03 | [train_policy] epoch #991 | Saved
2021-06-04 14:01:03 | [train_policy] epoch #991 | Time 795.55 s
2021-06-04 14:01:03 | [train_policy] epoch #991 | EpochTime 0.79 s
---------------------------------------  -------------
EnvExecTime                                0.282722
Evaluation/AverageDiscountedReturn       -40.6264
Evaluation/AverageReturn                 -40.6264
Evaluation/CompletionRate                  0
Evaluation/Iteration                     991
Evaluation/MaxReturn                     -28.4668
Evaluation/MinReturn                     -63.8953
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       8.46467
Extras/EpisodeRewardMean                 -40.6866
LinearFeatureBaseline/ExplainedVariance    0.866476
PolicyExecTime                             0.229502
ProcessExecTime                            0.0310252
TotalEnvSteps                              1.0039e+06
policy/Entropy                            -2.45635
policy/KL                                  0.00642137
policy/KLBefore                            0
policy/LossAfter                          -0.0262175
policy/LossBefore                         -5.30081e-09
policy/Perplexity                          0.0857475
policy/dLoss                               0.0262175
---------------------------------------  -------------
2021-06-04 14:01:03 | [train_policy] epoch #992 | Obtaining samples for iteration 992...
2021-06-04 14:01:04 | [train_policy] epoch #992 | Logging diagnostics...
2021-06-04 14:01:04 | [train_policy] epoch #992 | Optimizing policy...
2021-06-04 14:01:04 | [train_policy] epoch #992 | Computing loss before
2021-06-04 14:01:04 | [train_policy] epoch #992 | Computing KL before
2021-06-04 14:01:04 | [train_policy] epoch #992 | Optimizing
2021-06-04 14:01:04 | [train_policy] epoch #992 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:04 | [train_policy] epoch #992 | computing loss before
2021-06-04 14:01:04 | [train_policy] epoch #992 | computing gradient
2021-06-04 14:01:04 | [train_policy] epoch #992 | gradient computed
2021-06-04 14:01:04 | [train_policy] epoch #992 | computing descent direction
2021-06-04 14:01:04 | [train_policy] epoch #992 | descent direction computed
2021-06-04 14:01:04 | [train_policy] epoch #992 | backtrack iters: 1
2021-06-04 14:01:04 | [train_policy] epoch #992 | optimization finished
2021-06-04 14:01:04 | [train_policy] epoch #992 | Computing KL after
2021-06-04 14:01:04 | [train_policy] epoch #992 | Computing loss after
2021-06-04 14:01:04 | [train_policy] epoch #992 | Fitting baseline...
2021-06-04 14:01:04 | [train_policy] epoch #992 | Saving snapshot...
2021-06-04 14:01:04 | [train_policy] epoch #992 | Saved
2021-06-04 14:01:04 | [train_policy] epoch #992 | Time 796.35 s
2021-06-04 14:01:04 | [train_policy] epoch #992 | EpochTime 0.77 s
---------------------------------------  -------------
EnvExecTime                                0.283288
Evaluation/AverageDiscountedReturn       -41.1281
Evaluation/AverageReturn                 -41.1281
Evaluation/CompletionRate                  0
Evaluation/Iteration                     992
Evaluation/MaxReturn                     -28.7897
Evaluation/MinReturn                     -63.8289
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.3349
Extras/EpisodeRewardMean                 -40.9695
LinearFeatureBaseline/ExplainedVariance    0.894462
PolicyExecTime                             0.219715
ProcessExecTime                            0.0311141
TotalEnvSteps                              1.00492e+06
policy/Entropy                            -2.47943
policy/KL                                  0.00649932
policy/KLBefore                            0
policy/LossAfter                          -0.0156453
policy/LossBefore                         -9.42366e-09
policy/Perplexity                          0.0837911
policy/dLoss                               0.0156453
---------------------------------------  -------------
2021-06-04 14:01:04 | [train_policy] epoch #993 | Obtaining samples for iteration 993...
2021-06-04 14:01:05 | [train_policy] epoch #993 | Logging diagnostics...
2021-06-04 14:01:05 | [train_policy] epoch #993 | Optimizing policy...
2021-06-04 14:01:05 | [train_policy] epoch #993 | Computing loss before
2021-06-04 14:01:05 | [train_policy] epoch #993 | Computing KL before
2021-06-04 14:01:05 | [train_policy] epoch #993 | Optimizing
2021-06-04 14:01:05 | [train_policy] epoch #993 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:05 | [train_policy] epoch #993 | computing loss before
2021-06-04 14:01:05 | [train_policy] epoch #993 | computing gradient
2021-06-04 14:01:05 | [train_policy] epoch #993 | gradient computed
2021-06-04 14:01:05 | [train_policy] epoch #993 | computing descent direction
2021-06-04 14:01:05 | [train_policy] epoch #993 | descent direction computed
2021-06-04 14:01:05 | [train_policy] epoch #993 | backtrack iters: 0
2021-06-04 14:01:05 | [train_policy] epoch #993 | optimization finished
2021-06-04 14:01:05 | [train_policy] epoch #993 | Computing KL after
2021-06-04 14:01:05 | [train_policy] epoch #993 | Computing loss after
2021-06-04 14:01:05 | [train_policy] epoch #993 | Fitting baseline...
2021-06-04 14:01:05 | [train_policy] epoch #993 | Saving snapshot...
2021-06-04 14:01:05 | [train_policy] epoch #993 | Saved
2021-06-04 14:01:05 | [train_policy] epoch #993 | Time 797.16 s
2021-06-04 14:01:05 | [train_policy] epoch #993 | EpochTime 0.79 s
---------------------------------------  -------------
EnvExecTime                                0.285643
Evaluation/AverageDiscountedReturn       -39.5147
Evaluation/AverageReturn                 -39.5147
Evaluation/CompletionRate                  0
Evaluation/Iteration                     993
Evaluation/MaxReturn                     -28.4592
Evaluation/MinReturn                     -63.8998
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.9701
Extras/EpisodeRewardMean                 -39.7299
LinearFeatureBaseline/ExplainedVariance    0.889448
PolicyExecTime                             0.237876
ProcessExecTime                            0.0313067
TotalEnvSteps                              1.00593e+06
policy/Entropy                            -2.4743
policy/KL                                  0.00998397
policy/KLBefore                            0
policy/LossAfter                          -0.0139381
policy/LossBefore                         -8.01011e-09
policy/Perplexity                          0.0842218
policy/dLoss                               0.0139381
---------------------------------------  -------------
2021-06-04 14:01:05 | [train_policy] epoch #994 | Obtaining samples for iteration 994...
2021-06-04 14:01:06 | [train_policy] epoch #994 | Logging diagnostics...
2021-06-04 14:01:06 | [train_policy] epoch #994 | Optimizing policy...
2021-06-04 14:01:06 | [train_policy] epoch #994 | Computing loss before
2021-06-04 14:01:06 | [train_policy] epoch #994 | Computing KL before
2021-06-04 14:01:06 | [train_policy] epoch #994 | Optimizing
2021-06-04 14:01:06 | [train_policy] epoch #994 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:06 | [train_policy] epoch #994 | computing loss before
2021-06-04 14:01:06 | [train_policy] epoch #994 | computing gradient
2021-06-04 14:01:06 | [train_policy] epoch #994 | gradient computed
2021-06-04 14:01:06 | [train_policy] epoch #994 | computing descent direction
2021-06-04 14:01:06 | [train_policy] epoch #994 | descent direction computed
2021-06-04 14:01:06 | [train_policy] epoch #994 | backtrack iters: 1
2021-06-04 14:01:06 | [train_policy] epoch #994 | optimization finished
2021-06-04 14:01:06 | [train_policy] epoch #994 | Computing KL after
2021-06-04 14:01:06 | [train_policy] epoch #994 | Computing loss after
2021-06-04 14:01:06 | [train_policy] epoch #994 | Fitting baseline...
2021-06-04 14:01:06 | [train_policy] epoch #994 | Saving snapshot...
2021-06-04 14:01:06 | [train_policy] epoch #994 | Saved
2021-06-04 14:01:06 | [train_policy] epoch #994 | Time 797.97 s
2021-06-04 14:01:06 | [train_policy] epoch #994 | EpochTime 0.78 s
---------------------------------------  -------------
EnvExecTime                                0.28418
Evaluation/AverageDiscountedReturn       -40.7127
Evaluation/AverageReturn                 -40.7127
Evaluation/CompletionRate                  0
Evaluation/Iteration                     994
Evaluation/MaxReturn                     -29.2865
Evaluation/MinReturn                     -62.927
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.06684
Extras/EpisodeRewardMean                 -40.879
LinearFeatureBaseline/ExplainedVariance    0.86274
PolicyExecTime                             0.220102
ProcessExecTime                            0.031137
TotalEnvSteps                              1.00694e+06
policy/Entropy                            -2.49455
policy/KL                                  0.00656342
policy/KLBefore                            0
policy/LossAfter                          -0.0165323
policy/LossBefore                          2.68574e-08
policy/Perplexity                          0.0825337
policy/dLoss                               0.0165324
---------------------------------------  -------------
2021-06-04 14:01:06 | [train_policy] epoch #995 | Obtaining samples for iteration 995...
2021-06-04 14:01:06 | [train_policy] epoch #995 | Logging diagnostics...
2021-06-04 14:01:06 | [train_policy] epoch #995 | Optimizing policy...
2021-06-04 14:01:06 | [train_policy] epoch #995 | Computing loss before
2021-06-04 14:01:06 | [train_policy] epoch #995 | Computing KL before
2021-06-04 14:01:06 | [train_policy] epoch #995 | Optimizing
2021-06-04 14:01:06 | [train_policy] epoch #995 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:06 | [train_policy] epoch #995 | computing loss before
2021-06-04 14:01:06 | [train_policy] epoch #995 | computing gradient
2021-06-04 14:01:06 | [train_policy] epoch #995 | gradient computed
2021-06-04 14:01:06 | [train_policy] epoch #995 | computing descent direction
2021-06-04 14:01:06 | [train_policy] epoch #995 | descent direction computed
2021-06-04 14:01:07 | [train_policy] epoch #995 | backtrack iters: 1
2021-06-04 14:01:07 | [train_policy] epoch #995 | optimization finished
2021-06-04 14:01:07 | [train_policy] epoch #995 | Computing KL after
2021-06-04 14:01:07 | [train_policy] epoch #995 | Computing loss after
2021-06-04 14:01:07 | [train_policy] epoch #995 | Fitting baseline...
2021-06-04 14:01:07 | [train_policy] epoch #995 | Saving snapshot...
2021-06-04 14:01:07 | [train_policy] epoch #995 | Saved
2021-06-04 14:01:07 | [train_policy] epoch #995 | Time 798.78 s
2021-06-04 14:01:07 | [train_policy] epoch #995 | EpochTime 0.79 s
---------------------------------------  -------------
EnvExecTime                                0.285355
Evaluation/AverageDiscountedReturn       -41.6389
Evaluation/AverageReturn                 -41.6389
Evaluation/CompletionRate                  0
Evaluation/Iteration                     995
Evaluation/MaxReturn                     -28.6628
Evaluation/MinReturn                     -82.5265
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.87178
Extras/EpisodeRewardMean                 -41.6494
LinearFeatureBaseline/ExplainedVariance    0.834628
PolicyExecTime                             0.230114
ProcessExecTime                            0.0313058
TotalEnvSteps                              1.00795e+06
policy/Entropy                            -2.5151
policy/KL                                  0.00653407
policy/KLBefore                            0
policy/LossAfter                          -0.0121854
policy/LossBefore                          1.41355e-08
policy/Perplexity                          0.0808552
policy/dLoss                               0.0121855
---------------------------------------  -------------
2021-06-04 14:01:07 | [train_policy] epoch #996 | Obtaining samples for iteration 996...
2021-06-04 14:01:07 | [train_policy] epoch #996 | Logging diagnostics...
2021-06-04 14:01:07 | [train_policy] epoch #996 | Optimizing policy...
2021-06-04 14:01:07 | [train_policy] epoch #996 | Computing loss before
2021-06-04 14:01:07 | [train_policy] epoch #996 | Computing KL before
2021-06-04 14:01:07 | [train_policy] epoch #996 | Optimizing
2021-06-04 14:01:07 | [train_policy] epoch #996 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:07 | [train_policy] epoch #996 | computing loss before
2021-06-04 14:01:07 | [train_policy] epoch #996 | computing gradient
2021-06-04 14:01:07 | [train_policy] epoch #996 | gradient computed
2021-06-04 14:01:07 | [train_policy] epoch #996 | computing descent direction
2021-06-04 14:01:07 | [train_policy] epoch #996 | descent direction computed
2021-06-04 14:01:07 | [train_policy] epoch #996 | backtrack iters: 1
2021-06-04 14:01:07 | [train_policy] epoch #996 | optimization finished
2021-06-04 14:01:07 | [train_policy] epoch #996 | Computing KL after
2021-06-04 14:01:07 | [train_policy] epoch #996 | Computing loss after
2021-06-04 14:01:07 | [train_policy] epoch #996 | Fitting baseline...
2021-06-04 14:01:07 | [train_policy] epoch #996 | Saving snapshot...
2021-06-04 14:01:07 | [train_policy] epoch #996 | Saved
2021-06-04 14:01:07 | [train_policy] epoch #996 | Time 799.57 s
2021-06-04 14:01:07 | [train_policy] epoch #996 | EpochTime 0.77 s
---------------------------------------  -------------
EnvExecTime                                0.284427
Evaluation/AverageDiscountedReturn       -39.9125
Evaluation/AverageReturn                 -39.9125
Evaluation/CompletionRate                  0
Evaluation/Iteration                     996
Evaluation/MaxReturn                     -28.2176
Evaluation/MinReturn                     -63.8818
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.8913
Extras/EpisodeRewardMean                 -40.2962
LinearFeatureBaseline/ExplainedVariance    0.873178
PolicyExecTime                             0.214613
ProcessExecTime                            0.0311971
TotalEnvSteps                              1.00896e+06
policy/Entropy                            -2.5153
policy/KL                                  0.00641137
policy/KLBefore                            0
policy/LossAfter                          -0.0210355
policy/LossBefore                          5.88979e-09
policy/Perplexity                          0.0808388
policy/dLoss                               0.0210355
---------------------------------------  -------------
2021-06-04 14:01:07 | [train_policy] epoch #997 | Obtaining samples for iteration 997...
2021-06-04 14:01:08 | [train_policy] epoch #997 | Logging diagnostics...
2021-06-04 14:01:08 | [train_policy] epoch #997 | Optimizing policy...
2021-06-04 14:01:08 | [train_policy] epoch #997 | Computing loss before
2021-06-04 14:01:08 | [train_policy] epoch #997 | Computing KL before
2021-06-04 14:01:08 | [train_policy] epoch #997 | Optimizing
2021-06-04 14:01:08 | [train_policy] epoch #997 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:08 | [train_policy] epoch #997 | computing loss before
2021-06-04 14:01:08 | [train_policy] epoch #997 | computing gradient
2021-06-04 14:01:08 | [train_policy] epoch #997 | gradient computed
2021-06-04 14:01:08 | [train_policy] epoch #997 | computing descent direction
2021-06-04 14:01:08 | [train_policy] epoch #997 | descent direction computed
2021-06-04 14:01:08 | [train_policy] epoch #997 | backtrack iters: 1
2021-06-04 14:01:08 | [train_policy] epoch #997 | optimization finished
2021-06-04 14:01:08 | [train_policy] epoch #997 | Computing KL after
2021-06-04 14:01:08 | [train_policy] epoch #997 | Computing loss after
2021-06-04 14:01:08 | [train_policy] epoch #997 | Fitting baseline...
2021-06-04 14:01:08 | [train_policy] epoch #997 | Saving snapshot...
2021-06-04 14:01:08 | [train_policy] epoch #997 | Saved
2021-06-04 14:01:08 | [train_policy] epoch #997 | Time 800.40 s
2021-06-04 14:01:08 | [train_policy] epoch #997 | EpochTime 0.80 s
---------------------------------------  -------------
EnvExecTime                                0.284539
Evaluation/AverageDiscountedReturn       -40.3372
Evaluation/AverageReturn                 -40.3372
Evaluation/CompletionRate                  0
Evaluation/Iteration                     997
Evaluation/MaxReturn                     -28.3586
Evaluation/MinReturn                     -79.7018
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.27048
Extras/EpisodeRewardMean                 -40.6208
LinearFeatureBaseline/ExplainedVariance    0.824146
PolicyExecTime                             0.234758
ProcessExecTime                            0.0311868
TotalEnvSteps                              1.00998e+06
policy/Entropy                            -2.50935
policy/KL                                  0.00642467
policy/KLBefore                            0
policy/LossAfter                          -0.0236601
policy/LossBefore                         -9.89484e-09
policy/Perplexity                          0.0813211
policy/dLoss                               0.0236601
---------------------------------------  -------------
2021-06-04 14:01:08 | [train_policy] epoch #998 | Obtaining samples for iteration 998...
2021-06-04 14:01:09 | [train_policy] epoch #998 | Logging diagnostics...
2021-06-04 14:01:09 | [train_policy] epoch #998 | Optimizing policy...
2021-06-04 14:01:09 | [train_policy] epoch #998 | Computing loss before
2021-06-04 14:01:09 | [train_policy] epoch #998 | Computing KL before
2021-06-04 14:01:09 | [train_policy] epoch #998 | Optimizing
2021-06-04 14:01:09 | [train_policy] epoch #998 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:09 | [train_policy] epoch #998 | computing loss before
2021-06-04 14:01:09 | [train_policy] epoch #998 | computing gradient
2021-06-04 14:01:09 | [train_policy] epoch #998 | gradient computed
2021-06-04 14:01:09 | [train_policy] epoch #998 | computing descent direction
2021-06-04 14:01:09 | [train_policy] epoch #998 | descent direction computed
2021-06-04 14:01:09 | [train_policy] epoch #998 | backtrack iters: 0
2021-06-04 14:01:09 | [train_policy] epoch #998 | optimization finished
2021-06-04 14:01:09 | [train_policy] epoch #998 | Computing KL after
2021-06-04 14:01:09 | [train_policy] epoch #998 | Computing loss after
2021-06-04 14:01:09 | [train_policy] epoch #998 | Fitting baseline...
2021-06-04 14:01:09 | [train_policy] epoch #998 | Saving snapshot...
2021-06-04 14:01:09 | [train_policy] epoch #998 | Saved
2021-06-04 14:01:09 | [train_policy] epoch #998 | Time 801.24 s
2021-06-04 14:01:09 | [train_policy] epoch #998 | EpochTime 0.81 s
---------------------------------------  -------------
EnvExecTime                                0.287146
Evaluation/AverageDiscountedReturn       -42.329
Evaluation/AverageReturn                 -42.329
Evaluation/CompletionRate                  0
Evaluation/Iteration                     998
Evaluation/MaxReturn                     -28.8299
Evaluation/MinReturn                     -79.7583
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.19148
Extras/EpisodeRewardMean                 -41.8682
LinearFeatureBaseline/ExplainedVariance    0.854108
PolicyExecTime                             0.236738
ProcessExecTime                            0.0315473
TotalEnvSteps                              1.01099e+06
policy/Entropy                            -2.50835
policy/KL                                  0.00993667
policy/KLBefore                            0
policy/LossAfter                          -0.0254789
policy/LossBefore                         -1.88473e-09
policy/Perplexity                          0.0814027
policy/dLoss                               0.0254789
---------------------------------------  -------------
2021-06-04 14:01:09 | [train_policy] epoch #999 | Obtaining samples for iteration 999...
2021-06-04 14:01:10 | [train_policy] epoch #999 | Logging diagnostics...
2021-06-04 14:01:10 | [train_policy] epoch #999 | Optimizing policy...
2021-06-04 14:01:10 | [train_policy] epoch #999 | Computing loss before
2021-06-04 14:01:10 | [train_policy] epoch #999 | Computing KL before
2021-06-04 14:01:10 | [train_policy] epoch #999 | Optimizing
2021-06-04 14:01:10 | [train_policy] epoch #999 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2021-06-04 14:01:10 | [train_policy] epoch #999 | computing loss before
2021-06-04 14:01:10 | [train_policy] epoch #999 | computing gradient
2021-06-04 14:01:10 | [train_policy] epoch #999 | gradient computed
2021-06-04 14:01:10 | [train_policy] epoch #999 | computing descent direction
2021-06-04 14:01:10 | [train_policy] epoch #999 | descent direction computed
2021-06-04 14:01:10 | [train_policy] epoch #999 | backtrack iters: 1
2021-06-04 14:01:10 | [train_policy] epoch #999 | optimization finished
2021-06-04 14:01:10 | [train_policy] epoch #999 | Computing KL after
2021-06-04 14:01:10 | [train_policy] epoch #999 | Computing loss after
2021-06-04 14:01:10 | [train_policy] epoch #999 | Fitting baseline...
2021-06-04 14:01:10 | [train_policy] epoch #999 | Saving snapshot...
2021-06-04 14:01:10 | [train_policy] epoch #999 | Saved
2021-06-04 14:01:10 | [train_policy] epoch #999 | Time 802.07 s
2021-06-04 14:01:10 | [train_policy] epoch #999 | EpochTime 0.80 s
---------------------------------------  ------------
EnvExecTime                                0.285699
Evaluation/AverageDiscountedReturn       -41.2878
Evaluation/AverageReturn                 -41.2878
Evaluation/CompletionRate                  0
Evaluation/Iteration                     999
Evaluation/MaxReturn                     -28.8599
Evaluation/MinReturn                     -85.5073
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.77557
Extras/EpisodeRewardMean                 -41.7869
LinearFeatureBaseline/ExplainedVariance    0.799708
PolicyExecTime                             0.236827
ProcessExecTime                            0.0314243
TotalEnvSteps                              1.012e+06
policy/Entropy                            -2.51872
policy/KL                                  0.00653473
policy/KLBefore                            0
policy/LossAfter                          -0.0235426
policy/LossBefore                         -0
policy/Perplexity                          0.0805628
policy/dLoss                               0.0235426
---------------------------------------  ------------

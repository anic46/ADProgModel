2022-04-23 13:49:29 | [train_policy] Logging to ../models/adni_split0_TRPO_11_7.0_delta_fixed_1.0_1000_1000_2.0_fixed_full_1.0_1.0_11_inverse_MMSE_32
2022-04-23 13:49:29 | [train_policy] Setting seed to 1
2022-04-23 13:49:31 | [train_policy] Obtaining samples...
2022-04-23 13:49:31 | [train_policy] epoch #0 | Obtaining samples for iteration 0...
2022-04-23 13:49:32 | [train_policy] epoch #0 | Logging diagnostics...
2022-04-23 13:49:32 | [train_policy] epoch #0 | Optimizing policy...
2022-04-23 13:49:32 | [train_policy] epoch #0 | Computing loss before
2022-04-23 13:49:32 | [train_policy] epoch #0 | Computing KL before
2022-04-23 13:49:32 | [train_policy] epoch #0 | Optimizing
2022-04-23 13:49:32 | [train_policy] epoch #0 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 13:49:32 | [train_policy] epoch #0 | computing loss before
2022-04-23 13:49:32 | [train_policy] epoch #0 | computing gradient
2022-04-23 13:49:33 | [train_policy] epoch #0 | gradient computed
2022-04-23 13:49:33 | [train_policy] epoch #0 | computing descent direction
2022-04-23 13:49:34 | [train_policy] epoch #0 | descent direction computed
2022-04-23 13:49:34 | [train_policy] epoch #0 | backtrack iters: 0
2022-04-23 13:49:34 | [train_policy] epoch #0 | optimization finished
2022-04-23 13:49:34 | [train_policy] epoch #0 | Computing KL after
2022-04-23 13:49:34 | [train_policy] epoch #0 | Computing loss after
2022-04-23 13:49:34 | [train_policy] epoch #0 | Fitting baseline...
2022-04-23 13:49:34 | [train_policy] epoch #0 | Saving snapshot...
2022-04-23 13:49:34 | [train_policy] epoch #0 | Saved
2022-04-23 13:49:34 | [train_policy] epoch #0 | Time 3.63 s
2022-04-23 13:49:34 | [train_policy] epoch #0 | EpochTime 3.63 s
---------------------------------------  ----------------
EnvExecTime                                   0.694121
Evaluation/AverageDiscountedReturn        -5122.66
Evaluation/AverageReturn                  -5122.66
Evaluation/CompletionRate                     0
Evaluation/Iteration                          0
Evaluation/MaxReturn                        -46.1096
Evaluation/MinReturn                     -22000
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       6669.7
Extras/EpisodeRewardMean                  -5122.66
LinearFeatureBaseline/ExplainedVariance      -7.34936e-10
PolicyExecTime                                0.480272
ProcessExecTime                               0.0291502
TotalEnvSteps                              1012
policy/Entropy                                2.83582
policy/KL                                     0.00934735
policy/KLBefore                               0
policy/LossAfter                             -0.0566963
policy/LossBefore                            -8.71688e-09
policy/Perplexity                            17.0444
policy/dLoss                                  0.0566963
---------------------------------------  ----------------
2022-04-23 13:49:34 | [train_policy] epoch #1 | Obtaining samples for iteration 1...
2022-04-23 13:49:35 | [train_policy] epoch #1 | Logging diagnostics...
2022-04-23 13:49:35 | [train_policy] epoch #1 | Optimizing policy...
2022-04-23 13:49:35 | [train_policy] epoch #1 | Computing loss before
2022-04-23 13:49:35 | [train_policy] epoch #1 | Computing KL before
2022-04-23 13:49:35 | [train_policy] epoch #1 | Optimizing
2022-04-23 13:49:35 | [train_policy] epoch #1 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 13:49:35 | [train_policy] epoch #1 | computing loss before
2022-04-23 13:49:35 | [train_policy] epoch #1 | computing gradient
2022-04-23 13:49:35 | [train_policy] epoch #1 | gradient computed
2022-04-23 13:49:35 | [train_policy] epoch #1 | computing descent direction
2022-04-23 13:49:35 | [train_policy] epoch #1 | descent direction computed
2022-04-23 13:49:35 | [train_policy] epoch #1 | backtrack iters: 1
2022-04-23 13:49:35 | [train_policy] epoch #1 | optimization finished
2022-04-23 13:49:35 | [train_policy] epoch #1 | Computing KL after
2022-04-23 13:49:35 | [train_policy] epoch #1 | Computing loss after
2022-04-23 13:49:35 | [train_policy] epoch #1 | Fitting baseline...
2022-04-23 13:49:35 | [train_policy] epoch #1 | Saving snapshot...
2022-04-23 13:49:35 | [train_policy] epoch #1 | Saved
2022-04-23 13:49:35 | [train_policy] epoch #1 | Time 4.38 s
2022-04-23 13:49:35 | [train_policy] epoch #1 | EpochTime 0.73 s
---------------------------------------  ----------------
EnvExecTime                                   0.279692
Evaluation/AverageDiscountedReturn        -3683.64
Evaluation/AverageReturn                  -3683.64
Evaluation/CompletionRate                     0
Evaluation/Iteration                          1
Evaluation/MaxReturn                        -46.6611
Evaluation/MinReturn                     -21881.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       5980.06
Extras/EpisodeRewardMean                  -3905.85
LinearFeatureBaseline/ExplainedVariance       0.207279
PolicyExecTime                                0.197752
ProcessExecTime                               0.0292647
TotalEnvSteps                              2024
policy/Entropy                                2.81486
policy/KL                                     0.00670421
policy/KLBefore                               0
policy/LossAfter                             -0.0500698
policy/LossBefore                             2.40303e-08
policy/Perplexity                            16.6908
policy/dLoss                                  0.0500698
---------------------------------------  ----------------
2022-04-23 13:49:35 | [train_policy] epoch #2 | Obtaining samples for iteration 2...
2022-04-23 13:49:36 | [train_policy] epoch #2 | Logging diagnostics...
2022-04-23 13:49:36 | [train_policy] epoch #2 | Optimizing policy...
2022-04-23 13:49:36 | [train_policy] epoch #2 | Computing loss before
2022-04-23 13:49:36 | [train_policy] epoch #2 | Computing KL before
2022-04-23 13:49:36 | [train_policy] epoch #2 | Optimizing
2022-04-23 13:49:36 | [train_policy] epoch #2 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 13:49:36 | [train_policy] epoch #2 | computing loss before
2022-04-23 13:49:36 | [train_policy] epoch #2 | computing gradient
2022-04-23 13:49:36 | [train_policy] epoch #2 | gradient computed
2022-04-23 13:49:36 | [train_policy] epoch #2 | computing descent direction
2022-04-23 13:49:36 | [train_policy] epoch #2 | descent direction computed
2022-04-23 13:49:36 | [train_policy] epoch #2 | backtrack iters: 1
2022-04-23 13:49:36 | [train_policy] epoch #2 | optimization finished
2022-04-23 13:49:36 | [train_policy] epoch #2 | Computing KL after
2022-04-23 13:49:36 | [train_policy] epoch #2 | Computing loss after
2022-04-23 13:49:36 | [train_policy] epoch #2 | Fitting baseline...
2022-04-23 13:49:36 | [train_policy] epoch #2 | Saving snapshot...
2022-04-23 13:49:36 | [train_policy] epoch #2 | Saved
2022-04-23 13:49:36 | [train_policy] epoch #2 | Time 5.22 s
2022-04-23 13:49:36 | [train_policy] epoch #2 | EpochTime 0.82 s
---------------------------------------  ----------------
EnvExecTime                                   0.30709
Evaluation/AverageDiscountedReturn        -1298.59
Evaluation/AverageReturn                  -1298.59
Evaluation/CompletionRate                     0
Evaluation/Iteration                          2
Evaluation/MaxReturn                        -44.9154
Evaluation/MinReturn                     -14300.1
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       2631.54
Extras/EpisodeRewardMean                  -1568.13
LinearFeatureBaseline/ExplainedVariance      -0.24769
PolicyExecTime                                0.235576
ProcessExecTime                               0.0318432
TotalEnvSteps                              3036
policy/Entropy                                2.80928
policy/KL                                     0.00686679
policy/KLBefore                               0
policy/LossAfter                             -0.038921
policy/LossBefore                             9.65925e-09
policy/Perplexity                            16.5979
policy/dLoss                                  0.038921
---------------------------------------  ----------------
2022-04-23 13:49:36 | [train_policy] epoch #3 | Obtaining samples for iteration 3...
2022-04-23 13:49:37 | [train_policy] epoch #3 | Logging diagnostics...
2022-04-23 13:49:37 | [train_policy] epoch #3 | Optimizing policy...
2022-04-23 13:49:37 | [train_policy] epoch #3 | Computing loss before
2022-04-23 13:49:37 | [train_policy] epoch #3 | Computing KL before
2022-04-23 13:49:37 | [train_policy] epoch #3 | Optimizing
2022-04-23 13:49:37 | [train_policy] epoch #3 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 13:49:37 | [train_policy] epoch #3 | computing loss before
2022-04-23 13:49:37 | [train_policy] epoch #3 | computing gradient
2022-04-23 13:49:37 | [train_policy] epoch #3 | gradient computed
2022-04-23 13:49:37 | [train_policy] epoch #3 | computing descent direction
2022-04-23 13:49:37 | [train_policy] epoch #3 | descent direction computed
2022-04-23 13:49:37 | [train_policy] epoch #3 | backtrack iters: 0
2022-04-23 13:49:37 | [train_policy] epoch #3 | optimization finished
2022-04-23 13:49:37 | [train_policy] epoch #3 | Computing KL after
2022-04-23 13:49:37 | [train_policy] epoch #3 | Computing loss after
2022-04-23 13:49:37 | [train_policy] epoch #3 | Fitting baseline...
2022-04-23 13:49:37 | [train_policy] epoch #3 | Saving snapshot...
2022-04-23 13:49:37 | [train_policy] epoch #3 | Saved
2022-04-23 13:49:37 | [train_policy] epoch #3 | Time 6.00 s
2022-04-23 13:49:37 | [train_policy] epoch #3 | EpochTime 0.76 s
---------------------------------------  ----------------
EnvExecTime                                   0.289768
Evaluation/AverageDiscountedReturn         -745.527
Evaluation/AverageReturn                   -745.527
Evaluation/CompletionRate                     0
Evaluation/Iteration                          3
Evaluation/MaxReturn                        -39.7793
Evaluation/MinReturn                     -12075.3
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       1985.08
Extras/EpisodeRewardMean                   -771.245
LinearFeatureBaseline/ExplainedVariance       0.147975
PolicyExecTime                                0.211863
ProcessExecTime                               0.0304742
TotalEnvSteps                              4048
policy/Entropy                                2.78334
policy/KL                                     0.00992237
policy/KLBefore                               0
policy/LossAfter                             -0.0422623
policy/LossBefore                            -9.42366e-10
policy/Perplexity                            16.1729
policy/dLoss                                  0.0422623
---------------------------------------  ----------------
2022-04-23 14:19:11 | [train_policy] Logging to ../models/adni_split0_TRPO_11_7.0_delta_fixed_1.0_1000_1000_2.0_fixed_full_1.0_1.0_11_inverse_MMSE_32
2022-04-23 14:19:11 | [train_policy] Setting seed to 1
2022-04-23 14:19:12 | [train_policy] Obtaining samples...
2022-04-23 14:19:12 | [train_policy] epoch #0 | Obtaining samples for iteration 0...
2022-04-23 14:19:12 | [train_policy] epoch #0 | Logging diagnostics...
2022-04-23 14:19:12 | [train_policy] epoch #0 | Optimizing policy...
2022-04-23 14:19:12 | [train_policy] epoch #0 | Computing loss before
2022-04-23 14:19:12 | [train_policy] epoch #0 | Computing KL before
2022-04-23 14:19:13 | [train_policy] epoch #0 | Optimizing
2022-04-23 14:19:13 | [train_policy] epoch #0 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:13 | [train_policy] epoch #0 | computing loss before
2022-04-23 14:19:13 | [train_policy] epoch #0 | computing gradient
2022-04-23 14:19:13 | [train_policy] epoch #0 | gradient computed
2022-04-23 14:19:13 | [train_policy] epoch #0 | computing descent direction
2022-04-23 14:19:14 | [train_policy] epoch #0 | descent direction computed
2022-04-23 14:19:15 | [train_policy] epoch #0 | backtrack iters: 0
2022-04-23 14:19:15 | [train_policy] epoch #0 | optimization finished
2022-04-23 14:19:15 | [train_policy] epoch #0 | Computing KL after
2022-04-23 14:19:15 | [train_policy] epoch #0 | Computing loss after
2022-04-23 14:19:15 | [train_policy] epoch #0 | Fitting baseline...
2022-04-23 14:19:15 | [train_policy] epoch #0 | Saving snapshot...
2022-04-23 14:19:15 | [train_policy] epoch #0 | Saved
2022-04-23 14:19:15 | [train_policy] epoch #0 | Time 3.35 s
2022-04-23 14:19:15 | [train_policy] epoch #0 | EpochTime 3.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.123404
Evaluation/AverageDiscountedReturn        -5122.66
Evaluation/AverageReturn                  -5122.66
Evaluation/CompletionRate                     0
Evaluation/Iteration                          0
Evaluation/MaxReturn                        -46.1096
Evaluation/MinReturn                     -22000
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       6669.7
Extras/EpisodeRewardMean                  -5122.66
LinearFeatureBaseline/ExplainedVariance       1.24773e-07
PolicyExecTime                                0.259949
ProcessExecTime                               0.0114927
TotalEnvSteps                              1012
policy/Entropy                                2.83542
policy/KL                                     0.00935038
policy/KLBefore                               0
policy/LossAfter                             -0.0566965
policy/LossBefore                             1.53134e-08
policy/Perplexity                            17.0376
policy/dLoss                                  0.0566966
---------------------------------------  ----------------
2022-04-23 14:19:15 | [train_policy] epoch #1 | Obtaining samples for iteration 1...
2022-04-23 14:19:15 | [train_policy] epoch #1 | Logging diagnostics...
2022-04-23 14:19:15 | [train_policy] epoch #1 | Optimizing policy...
2022-04-23 14:19:15 | [train_policy] epoch #1 | Computing loss before
2022-04-23 14:19:15 | [train_policy] epoch #1 | Computing KL before
2022-04-23 14:19:15 | [train_policy] epoch #1 | Optimizing
2022-04-23 14:19:15 | [train_policy] epoch #1 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:15 | [train_policy] epoch #1 | computing loss before
2022-04-23 14:19:15 | [train_policy] epoch #1 | computing gradient
2022-04-23 14:19:15 | [train_policy] epoch #1 | gradient computed
2022-04-23 14:19:15 | [train_policy] epoch #1 | computing descent direction
2022-04-23 14:19:15 | [train_policy] epoch #1 | descent direction computed
2022-04-23 14:19:15 | [train_policy] epoch #1 | backtrack iters: 1
2022-04-23 14:19:15 | [train_policy] epoch #1 | optimization finished
2022-04-23 14:19:15 | [train_policy] epoch #1 | Computing KL after
2022-04-23 14:19:15 | [train_policy] epoch #1 | Computing loss after
2022-04-23 14:19:16 | [train_policy] epoch #1 | Fitting baseline...
2022-04-23 14:19:16 | [train_policy] epoch #1 | Saving snapshot...
2022-04-23 14:19:16 | [train_policy] epoch #1 | Saved
2022-04-23 14:19:16 | [train_policy] epoch #1 | Time 3.73 s
2022-04-23 14:19:16 | [train_policy] epoch #1 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                   0.126395
Evaluation/AverageDiscountedReturn        -3682.57
Evaluation/AverageReturn                  -3682.57
Evaluation/CompletionRate                     0
Evaluation/Iteration                          1
Evaluation/MaxReturn                        -46.6595
Evaluation/MinReturn                     -21876.6
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       5978.8
Extras/EpisodeRewardMean                  -3904.87
LinearFeatureBaseline/ExplainedVariance       0.207204
PolicyExecTime                                0.106839
ProcessExecTime                               0.0124552
TotalEnvSteps                              2024
policy/Entropy                                2.81185
policy/KL                                     0.00676302
policy/KLBefore                               0
policy/LossAfter                             -0.0502074
policy/LossBefore                             5.6542e-09
policy/Perplexity                            16.6407
policy/dLoss                                  0.0502074
---------------------------------------  ---------------
2022-04-23 14:19:16 | [train_policy] epoch #2 | Obtaining samples for iteration 2...
2022-04-23 14:19:16 | [train_policy] epoch #2 | Logging diagnostics...
2022-04-23 14:19:16 | [train_policy] epoch #2 | Optimizing policy...
2022-04-23 14:19:16 | [train_policy] epoch #2 | Computing loss before
2022-04-23 14:19:16 | [train_policy] epoch #2 | Computing KL before
2022-04-23 14:19:16 | [train_policy] epoch #2 | Optimizing
2022-04-23 14:19:16 | [train_policy] epoch #2 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:16 | [train_policy] epoch #2 | computing loss before
2022-04-23 14:19:16 | [train_policy] epoch #2 | computing gradient
2022-04-23 14:19:16 | [train_policy] epoch #2 | gradient computed
2022-04-23 14:19:16 | [train_policy] epoch #2 | computing descent direction
2022-04-23 14:19:16 | [train_policy] epoch #2 | descent direction computed
2022-04-23 14:19:16 | [train_policy] epoch #2 | backtrack iters: 1
2022-04-23 14:19:16 | [train_policy] epoch #2 | optimization finished
2022-04-23 14:19:16 | [train_policy] epoch #2 | Computing KL after
2022-04-23 14:19:16 | [train_policy] epoch #2 | Computing loss after
2022-04-23 14:19:16 | [train_policy] epoch #2 | Fitting baseline...
2022-04-23 14:19:16 | [train_policy] epoch #2 | Saving snapshot...
2022-04-23 14:19:16 | [train_policy] epoch #2 | Saved
2022-04-23 14:19:16 | [train_policy] epoch #2 | Time 4.09 s
2022-04-23 14:19:16 | [train_policy] epoch #2 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116861
Evaluation/AverageDiscountedReturn        -1294.46
Evaluation/AverageReturn                  -1294.46
Evaluation/CompletionRate                     0
Evaluation/Iteration                          2
Evaluation/MaxReturn                        -44.8661
Evaluation/MinReturn                     -14286.9
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       2627.89
Extras/EpisodeRewardMean                  -1564.2
LinearFeatureBaseline/ExplainedVariance      -0.24852
PolicyExecTime                                0.106726
ProcessExecTime                               0.0110557
TotalEnvSteps                              3036
policy/Entropy                                2.81245
policy/KL                                     0.00679391
policy/KLBefore                               0
policy/LossAfter                             -0.0387496
policy/LossBefore                            -1.77872e-08
policy/Perplexity                            16.6507
policy/dLoss                                  0.0387496
---------------------------------------  ----------------
2022-04-23 14:19:16 | [train_policy] epoch #3 | Obtaining samples for iteration 3...
2022-04-23 14:19:16 | [train_policy] epoch #3 | Logging diagnostics...
2022-04-23 14:19:16 | [train_policy] epoch #3 | Optimizing policy...
2022-04-23 14:19:16 | [train_policy] epoch #3 | Computing loss before
2022-04-23 14:19:16 | [train_policy] epoch #3 | Computing KL before
2022-04-23 14:19:16 | [train_policy] epoch #3 | Optimizing
2022-04-23 14:19:16 | [train_policy] epoch #3 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:16 | [train_policy] epoch #3 | computing loss before
2022-04-23 14:19:16 | [train_policy] epoch #3 | computing gradient
2022-04-23 14:19:16 | [train_policy] epoch #3 | gradient computed
2022-04-23 14:19:16 | [train_policy] epoch #3 | computing descent direction
2022-04-23 14:19:16 | [train_policy] epoch #3 | descent direction computed
2022-04-23 14:19:16 | [train_policy] epoch #3 | backtrack iters: 1
2022-04-23 14:19:16 | [train_policy] epoch #3 | optimization finished
2022-04-23 14:19:16 | [train_policy] epoch #3 | Computing KL after
2022-04-23 14:19:16 | [train_policy] epoch #3 | Computing loss after
2022-04-23 14:19:16 | [train_policy] epoch #3 | Fitting baseline...
2022-04-23 14:19:16 | [train_policy] epoch #3 | Saving snapshot...
2022-04-23 14:19:16 | [train_policy] epoch #3 | Saved
2022-04-23 14:19:16 | [train_policy] epoch #3 | Time 4.47 s
2022-04-23 14:19:16 | [train_policy] epoch #3 | EpochTime 0.38 s
---------------------------------------  ---------------
EnvExecTime                                   0.12229
Evaluation/AverageDiscountedReturn         -746.024
Evaluation/AverageReturn                   -746.024
Evaluation/CompletionRate                     0
Evaluation/Iteration                          3
Evaluation/MaxReturn                        -39.803
Evaluation/MinReturn                     -12080.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       1986.59
Extras/EpisodeRewardMean                   -771.709
LinearFeatureBaseline/ExplainedVariance       0.148903
PolicyExecTime                                0.108493
ProcessExecTime                               0.0116131
TotalEnvSteps                              4048
policy/Entropy                                2.78213
policy/KL                                     0.00652626
policy/KLBefore                               0
policy/LossAfter                             -0.0350965
policy/LossBefore                            -5.6542e-09
policy/Perplexity                            16.1534
policy/dLoss                                  0.0350965
---------------------------------------  ---------------
2022-04-23 14:19:16 | [train_policy] epoch #4 | Obtaining samples for iteration 4...
2022-04-23 14:19:17 | [train_policy] epoch #4 | Logging diagnostics...
2022-04-23 14:19:17 | [train_policy] epoch #4 | Optimizing policy...
2022-04-23 14:19:17 | [train_policy] epoch #4 | Computing loss before
2022-04-23 14:19:17 | [train_policy] epoch #4 | Computing KL before
2022-04-23 14:19:17 | [train_policy] epoch #4 | Optimizing
2022-04-23 14:19:17 | [train_policy] epoch #4 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:17 | [train_policy] epoch #4 | computing loss before
2022-04-23 14:19:17 | [train_policy] epoch #4 | computing gradient
2022-04-23 14:19:17 | [train_policy] epoch #4 | gradient computed
2022-04-23 14:19:17 | [train_policy] epoch #4 | computing descent direction
2022-04-23 14:19:17 | [train_policy] epoch #4 | descent direction computed
2022-04-23 14:19:17 | [train_policy] epoch #4 | backtrack iters: 0
2022-04-23 14:19:17 | [train_policy] epoch #4 | optimization finished
2022-04-23 14:19:17 | [train_policy] epoch #4 | Computing KL after
2022-04-23 14:19:17 | [train_policy] epoch #4 | Computing loss after
2022-04-23 14:19:17 | [train_policy] epoch #4 | Fitting baseline...
2022-04-23 14:19:17 | [train_policy] epoch #4 | Saving snapshot...
2022-04-23 14:19:17 | [train_policy] epoch #4 | Saved
2022-04-23 14:19:17 | [train_policy] epoch #4 | Time 4.83 s
2022-04-23 14:19:17 | [train_policy] epoch #4 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.121512
Evaluation/AverageDiscountedReturn        -302.473
Evaluation/AverageReturn                  -302.473
Evaluation/CompletionRate                    0
Evaluation/Iteration                         4
Evaluation/MaxReturn                       -37.014
Evaluation/MinReturn                     -6052.39
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       908.651
Extras/EpisodeRewardMean                  -303.177
LinearFeatureBaseline/ExplainedVariance     -0.219263
PolicyExecTime                               0.111483
ProcessExecTime                              0.0123897
TotalEnvSteps                             5060
policy/Entropy                               2.76088
policy/KL                                    0.00928298
policy/KLBefore                              0
policy/LossAfter                            -0.0183224
policy/LossBefore                            6.36097e-09
policy/Perplexity                           15.8137
policy/dLoss                                 0.0183224
---------------------------------------  ---------------
2022-04-23 14:19:17 | [train_policy] epoch #5 | Obtaining samples for iteration 5...
2022-04-23 14:19:17 | [train_policy] epoch #5 | Logging diagnostics...
2022-04-23 14:19:17 | [train_policy] epoch #5 | Optimizing policy...
2022-04-23 14:19:17 | [train_policy] epoch #5 | Computing loss before
2022-04-23 14:19:17 | [train_policy] epoch #5 | Computing KL before
2022-04-23 14:19:17 | [train_policy] epoch #5 | Optimizing
2022-04-23 14:19:17 | [train_policy] epoch #5 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:17 | [train_policy] epoch #5 | computing loss before
2022-04-23 14:19:17 | [train_policy] epoch #5 | computing gradient
2022-04-23 14:19:17 | [train_policy] epoch #5 | gradient computed
2022-04-23 14:19:17 | [train_policy] epoch #5 | computing descent direction
2022-04-23 14:19:17 | [train_policy] epoch #5 | descent direction computed
2022-04-23 14:19:17 | [train_policy] epoch #5 | backtrack iters: 1
2022-04-23 14:19:17 | [train_policy] epoch #5 | optimization finished
2022-04-23 14:19:17 | [train_policy] epoch #5 | Computing KL after
2022-04-23 14:19:17 | [train_policy] epoch #5 | Computing loss after
2022-04-23 14:19:17 | [train_policy] epoch #5 | Fitting baseline...
2022-04-23 14:19:17 | [train_policy] epoch #5 | Saving snapshot...
2022-04-23 14:19:17 | [train_policy] epoch #5 | Saved
2022-04-23 14:19:17 | [train_policy] epoch #5 | Time 5.20 s
2022-04-23 14:19:17 | [train_policy] epoch #5 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.124347
Evaluation/AverageDiscountedReturn         -551.092
Evaluation/AverageReturn                   -551.092
Evaluation/CompletionRate                     0
Evaluation/Iteration                          5
Evaluation/MaxReturn                        -44.8455
Evaluation/MinReturn                     -12578
Evaluation/NumTrajs                          92
Evaluation/StdReturn                       1833.5
Extras/EpisodeRewardMean                   -512.338
LinearFeatureBaseline/ExplainedVariance       0.0388633
PolicyExecTime                                0.109858
ProcessExecTime                               0.0119843
TotalEnvSteps                              6072
policy/Entropy                                2.70994
policy/KL                                     0.00700693
policy/KLBefore                               0
policy/LossAfter                             -0.0371574
policy/LossBefore                            -1.41355e-09
policy/Perplexity                            15.0283
policy/dLoss                                  0.0371574
---------------------------------------  ----------------
2022-04-23 14:19:17 | [train_policy] epoch #6 | Obtaining samples for iteration 6...
2022-04-23 14:19:17 | [train_policy] epoch #6 | Logging diagnostics...
2022-04-23 14:19:17 | [train_policy] epoch #6 | Optimizing policy...
2022-04-23 14:19:17 | [train_policy] epoch #6 | Computing loss before
2022-04-23 14:19:17 | [train_policy] epoch #6 | Computing KL before
2022-04-23 14:19:17 | [train_policy] epoch #6 | Optimizing
2022-04-23 14:19:17 | [train_policy] epoch #6 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:17 | [train_policy] epoch #6 | computing loss before
2022-04-23 14:19:17 | [train_policy] epoch #6 | computing gradient
2022-04-23 14:19:17 | [train_policy] epoch #6 | gradient computed
2022-04-23 14:19:17 | [train_policy] epoch #6 | computing descent direction
2022-04-23 14:19:17 | [train_policy] epoch #6 | descent direction computed
2022-04-23 14:19:17 | [train_policy] epoch #6 | backtrack iters: 1
2022-04-23 14:19:17 | [train_policy] epoch #6 | optimization finished
2022-04-23 14:19:17 | [train_policy] epoch #6 | Computing KL after
2022-04-23 14:19:17 | [train_policy] epoch #6 | Computing loss after
2022-04-23 14:19:17 | [train_policy] epoch #6 | Fitting baseline...
2022-04-23 14:19:17 | [train_policy] epoch #6 | Saving snapshot...
2022-04-23 14:19:17 | [train_policy] epoch #6 | Saved
2022-04-23 14:19:17 | [train_policy] epoch #6 | Time 5.57 s
2022-04-23 14:19:17 | [train_policy] epoch #6 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.1215
Evaluation/AverageDiscountedReturn        -144.919
Evaluation/AverageReturn                  -144.919
Evaluation/CompletionRate                    0
Evaluation/Iteration                         6
Evaluation/MaxReturn                       -45.2055
Evaluation/MinReturn                     -4044.81
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       468.307
Extras/EpisodeRewardMean                  -159.964
LinearFeatureBaseline/ExplainedVariance     -1.64151
PolicyExecTime                               0.110482
ProcessExecTime                              0.0116603
TotalEnvSteps                             7084
policy/Entropy                               2.7065
policy/KL                                    0.00601196
policy/KLBefore                              0
policy/LossAfter                            -0.0236897
policy/LossBefore                            1.41355e-09
policy/Perplexity                           14.9767
policy/dLoss                                 0.0236897
---------------------------------------  ---------------
2022-04-23 14:19:17 | [train_policy] epoch #7 | Obtaining samples for iteration 7...
2022-04-23 14:19:18 | [train_policy] epoch #7 | Logging diagnostics...
2022-04-23 14:19:18 | [train_policy] epoch #7 | Optimizing policy...
2022-04-23 14:19:18 | [train_policy] epoch #7 | Computing loss before
2022-04-23 14:19:18 | [train_policy] epoch #7 | Computing KL before
2022-04-23 14:19:18 | [train_policy] epoch #7 | Optimizing
2022-04-23 14:19:18 | [train_policy] epoch #7 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:18 | [train_policy] epoch #7 | computing loss before
2022-04-23 14:19:18 | [train_policy] epoch #7 | computing gradient
2022-04-23 14:19:18 | [train_policy] epoch #7 | gradient computed
2022-04-23 14:19:18 | [train_policy] epoch #7 | computing descent direction
2022-04-23 14:19:18 | [train_policy] epoch #7 | descent direction computed
2022-04-23 14:19:18 | [train_policy] epoch #7 | backtrack iters: 1
2022-04-23 14:19:18 | [train_policy] epoch #7 | optimization finished
2022-04-23 14:19:18 | [train_policy] epoch #7 | Computing KL after
2022-04-23 14:19:18 | [train_policy] epoch #7 | Computing loss after
2022-04-23 14:19:18 | [train_policy] epoch #7 | Fitting baseline...
2022-04-23 14:19:18 | [train_policy] epoch #7 | Saving snapshot...
2022-04-23 14:19:18 | [train_policy] epoch #7 | Saved
2022-04-23 14:19:18 | [train_policy] epoch #7 | Time 5.93 s
2022-04-23 14:19:18 | [train_policy] epoch #7 | EpochTime 0.36 s
---------------------------------------  --------------
EnvExecTime                                  0.120571
Evaluation/AverageDiscountedReturn        -177.142
Evaluation/AverageReturn                  -177.142
Evaluation/CompletionRate                    0
Evaluation/Iteration                         7
Evaluation/MaxReturn                       -49.7442
Evaluation/MinReturn                     -2944.65
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       427.417
Extras/EpisodeRewardMean                  -168.113
LinearFeatureBaseline/ExplainedVariance     -0.117228
PolicyExecTime                               0.108462
ProcessExecTime                              0.0115187
TotalEnvSteps                             8096
policy/Entropy                               2.68986
policy/KL                                    0.00735664
policy/KLBefore                              0
policy/LossAfter                            -0.0361676
policy/LossBefore                           -0
policy/Perplexity                           14.7297
policy/dLoss                                 0.0361676
---------------------------------------  --------------
2022-04-23 14:19:18 | [train_policy] epoch #8 | Obtaining samples for iteration 8...
2022-04-23 14:19:18 | [train_policy] epoch #8 | Logging diagnostics...
2022-04-23 14:19:18 | [train_policy] epoch #8 | Optimizing policy...
2022-04-23 14:19:18 | [train_policy] epoch #8 | Computing loss before
2022-04-23 14:19:18 | [train_policy] epoch #8 | Computing KL before
2022-04-23 14:19:18 | [train_policy] epoch #8 | Optimizing
2022-04-23 14:19:18 | [train_policy] epoch #8 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:18 | [train_policy] epoch #8 | computing loss before
2022-04-23 14:19:18 | [train_policy] epoch #8 | computing gradient
2022-04-23 14:19:18 | [train_policy] epoch #8 | gradient computed
2022-04-23 14:19:18 | [train_policy] epoch #8 | computing descent direction
2022-04-23 14:19:18 | [train_policy] epoch #8 | descent direction computed
2022-04-23 14:19:18 | [train_policy] epoch #8 | backtrack iters: 3
2022-04-23 14:19:18 | [train_policy] epoch #8 | optimization finished
2022-04-23 14:19:18 | [train_policy] epoch #8 | Computing KL after
2022-04-23 14:19:18 | [train_policy] epoch #8 | Computing loss after
2022-04-23 14:19:18 | [train_policy] epoch #8 | Fitting baseline...
2022-04-23 14:19:18 | [train_policy] epoch #8 | Saving snapshot...
2022-04-23 14:19:18 | [train_policy] epoch #8 | Saved
2022-04-23 14:19:18 | [train_policy] epoch #8 | Time 6.31 s
2022-04-23 14:19:18 | [train_policy] epoch #8 | EpochTime 0.38 s
---------------------------------------  ---------------
EnvExecTime                                  0.128068
Evaluation/AverageDiscountedReturn        -141.097
Evaluation/AverageReturn                  -141.097
Evaluation/CompletionRate                    0
Evaluation/Iteration                         8
Evaluation/MaxReturn                       -47.4745
Evaluation/MinReturn                     -3906.34
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       448.004
Extras/EpisodeRewardMean                  -135.475
LinearFeatureBaseline/ExplainedVariance      0.00458149
PolicyExecTime                               0.115023
ProcessExecTime                              0.0125558
TotalEnvSteps                             9108
policy/Entropy                               2.66307
policy/KL                                    0.00486797
policy/KLBefore                              0
policy/LossAfter                            -0.0135953
policy/LossBefore                            1.41355e-09
policy/Perplexity                           14.3403
policy/dLoss                                 0.0135953
---------------------------------------  ---------------
2022-04-23 14:19:18 | [train_policy] epoch #9 | Obtaining samples for iteration 9...
2022-04-23 14:19:18 | [train_policy] epoch #9 | Logging diagnostics...
2022-04-23 14:19:18 | [train_policy] epoch #9 | Optimizing policy...
2022-04-23 14:19:18 | [train_policy] epoch #9 | Computing loss before
2022-04-23 14:19:18 | [train_policy] epoch #9 | Computing KL before
2022-04-23 14:19:18 | [train_policy] epoch #9 | Optimizing
2022-04-23 14:19:18 | [train_policy] epoch #9 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:18 | [train_policy] epoch #9 | computing loss before
2022-04-23 14:19:18 | [train_policy] epoch #9 | computing gradient
2022-04-23 14:19:18 | [train_policy] epoch #9 | gradient computed
2022-04-23 14:19:18 | [train_policy] epoch #9 | computing descent direction
2022-04-23 14:19:18 | [train_policy] epoch #9 | descent direction computed
2022-04-23 14:19:18 | [train_policy] epoch #9 | backtrack iters: 2
2022-04-23 14:19:18 | [train_policy] epoch #9 | optimization finished
2022-04-23 14:19:18 | [train_policy] epoch #9 | Computing KL after
2022-04-23 14:19:18 | [train_policy] epoch #9 | Computing loss after
2022-04-23 14:19:18 | [train_policy] epoch #9 | Fitting baseline...
2022-04-23 14:19:18 | [train_policy] epoch #9 | Saving snapshot...
2022-04-23 14:19:18 | [train_policy] epoch #9 | Saved
2022-04-23 14:19:18 | [train_policy] epoch #9 | Time 6.70 s
2022-04-23 14:19:18 | [train_policy] epoch #9 | EpochTime 0.39 s
---------------------------------------  ---------------
EnvExecTime                                  0.125739
Evaluation/AverageDiscountedReturn         -67.8201
Evaluation/AverageReturn                   -67.8201
Evaluation/CompletionRate                    0
Evaluation/Iteration                         9
Evaluation/MaxReturn                       -39.3275
Evaluation/MinReturn                      -109.067
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.85495
Extras/EpisodeRewardMean                   -67.6496
LinearFeatureBaseline/ExplainedVariance     -5.71946
PolicyExecTime                               0.121664
ProcessExecTime                              0.0123794
TotalEnvSteps                            10120
policy/Entropy                               2.66583
policy/KL                                    0.00722354
policy/KLBefore                              0
policy/LossAfter                            -0.0195312
policy/LossBefore                            4.52336e-08
policy/Perplexity                           14.3799
policy/dLoss                                 0.0195313
---------------------------------------  ---------------
2022-04-23 14:19:18 | [train_policy] epoch #10 | Obtaining samples for iteration 10...
2022-04-23 14:19:19 | [train_policy] epoch #10 | Logging diagnostics...
2022-04-23 14:19:19 | [train_policy] epoch #10 | Optimizing policy...
2022-04-23 14:19:19 | [train_policy] epoch #10 | Computing loss before
2022-04-23 14:19:19 | [train_policy] epoch #10 | Computing KL before
2022-04-23 14:19:19 | [train_policy] epoch #10 | Optimizing
2022-04-23 14:19:19 | [train_policy] epoch #10 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:19 | [train_policy] epoch #10 | computing loss before
2022-04-23 14:19:19 | [train_policy] epoch #10 | computing gradient
2022-04-23 14:19:19 | [train_policy] epoch #10 | gradient computed
2022-04-23 14:19:19 | [train_policy] epoch #10 | computing descent direction
2022-04-23 14:19:19 | [train_policy] epoch #10 | descent direction computed
2022-04-23 14:19:19 | [train_policy] epoch #10 | backtrack iters: 1
2022-04-23 14:19:19 | [train_policy] epoch #10 | optimization finished
2022-04-23 14:19:19 | [train_policy] epoch #10 | Computing KL after
2022-04-23 14:19:19 | [train_policy] epoch #10 | Computing loss after
2022-04-23 14:19:19 | [train_policy] epoch #10 | Fitting baseline...
2022-04-23 14:19:19 | [train_policy] epoch #10 | Saving snapshot...
2022-04-23 14:19:19 | [train_policy] epoch #10 | Saved
2022-04-23 14:19:19 | [train_policy] epoch #10 | Time 7.07 s
2022-04-23 14:19:19 | [train_policy] epoch #10 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                  0.120122
Evaluation/AverageDiscountedReturn        -107.093
Evaluation/AverageReturn                  -107.093
Evaluation/CompletionRate                    0
Evaluation/Iteration                        10
Evaluation/MaxReturn                       -51.4011
Evaluation/MinReturn                     -2060.58
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       233.71
Extras/EpisodeRewardMean                  -104.262
LinearFeatureBaseline/ExplainedVariance      0.0693702
PolicyExecTime                               0.115339
ProcessExecTime                              0.0118122
TotalEnvSteps                            11132
policy/Entropy                               2.63968
policy/KL                                    0.00716371
policy/KLBefore                              0
policy/LossAfter                            -0.0425603
policy/LossBefore                           -2.59151e-09
policy/Perplexity                           14.0088
policy/dLoss                                 0.0425603
---------------------------------------  ---------------
2022-04-23 14:19:19 | [train_policy] epoch #11 | Obtaining samples for iteration 11...
2022-04-23 14:19:19 | [train_policy] epoch #11 | Logging diagnostics...
2022-04-23 14:19:19 | [train_policy] epoch #11 | Optimizing policy...
2022-04-23 14:19:19 | [train_policy] epoch #11 | Computing loss before
2022-04-23 14:19:19 | [train_policy] epoch #11 | Computing KL before
2022-04-23 14:19:19 | [train_policy] epoch #11 | Optimizing
2022-04-23 14:19:19 | [train_policy] epoch #11 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:19 | [train_policy] epoch #11 | computing loss before
2022-04-23 14:19:19 | [train_policy] epoch #11 | computing gradient
2022-04-23 14:19:19 | [train_policy] epoch #11 | gradient computed
2022-04-23 14:19:19 | [train_policy] epoch #11 | computing descent direction
2022-04-23 14:19:19 | [train_policy] epoch #11 | descent direction computed
2022-04-23 14:19:19 | [train_policy] epoch #11 | backtrack iters: 1
2022-04-23 14:19:19 | [train_policy] epoch #11 | optimization finished
2022-04-23 14:19:19 | [train_policy] epoch #11 | Computing KL after
2022-04-23 14:19:19 | [train_policy] epoch #11 | Computing loss after
2022-04-23 14:19:19 | [train_policy] epoch #11 | Fitting baseline...
2022-04-23 14:19:19 | [train_policy] epoch #11 | Saving snapshot...
2022-04-23 14:19:19 | [train_policy] epoch #11 | Saved
2022-04-23 14:19:19 | [train_policy] epoch #11 | Time 7.44 s
2022-04-23 14:19:19 | [train_policy] epoch #11 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                  0.120235
Evaluation/AverageDiscountedReturn         -71.5585
Evaluation/AverageReturn                   -71.5585
Evaluation/CompletionRate                    0
Evaluation/Iteration                        11
Evaluation/MaxReturn                       -48.2035
Evaluation/MinReturn                      -423.089
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        37.6106
Extras/EpisodeRewardMean                   -71.2001
LinearFeatureBaseline/ExplainedVariance      0.350025
PolicyExecTime                               0.120391
ProcessExecTime                              0.0118973
TotalEnvSteps                            12144
policy/Entropy                               2.64346
policy/KL                                    0.00658433
policy/KLBefore                              0
policy/LossAfter                            -0.0268405
policy/LossBefore                           -1.31931e-08
policy/Perplexity                           14.0618
policy/dLoss                                 0.0268405
---------------------------------------  ---------------
2022-04-23 14:19:19 | [train_policy] epoch #12 | Obtaining samples for iteration 12...
2022-04-23 14:19:20 | [train_policy] epoch #12 | Logging diagnostics...
2022-04-23 14:19:20 | [train_policy] epoch #12 | Optimizing policy...
2022-04-23 14:19:20 | [train_policy] epoch #12 | Computing loss before
2022-04-23 14:19:20 | [train_policy] epoch #12 | Computing KL before
2022-04-23 14:19:20 | [train_policy] epoch #12 | Optimizing
2022-04-23 14:19:20 | [train_policy] epoch #12 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:20 | [train_policy] epoch #12 | computing loss before
2022-04-23 14:19:20 | [train_policy] epoch #12 | computing gradient
2022-04-23 14:19:20 | [train_policy] epoch #12 | gradient computed
2022-04-23 14:19:20 | [train_policy] epoch #12 | computing descent direction
2022-04-23 14:19:20 | [train_policy] epoch #12 | descent direction computed
2022-04-23 14:19:20 | [train_policy] epoch #12 | backtrack iters: 1
2022-04-23 14:19:20 | [train_policy] epoch #12 | optimization finished
2022-04-23 14:19:20 | [train_policy] epoch #12 | Computing KL after
2022-04-23 14:19:20 | [train_policy] epoch #12 | Computing loss after
2022-04-23 14:19:20 | [train_policy] epoch #12 | Fitting baseline...
2022-04-23 14:19:20 | [train_policy] epoch #12 | Saving snapshot...
2022-04-23 14:19:20 | [train_policy] epoch #12 | Saved
2022-04-23 14:19:20 | [train_policy] epoch #12 | Time 7.82 s
2022-04-23 14:19:20 | [train_policy] epoch #12 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                  0.125531
Evaluation/AverageDiscountedReturn         -95.3754
Evaluation/AverageReturn                   -95.3754
Evaluation/CompletionRate                    0
Evaluation/Iteration                        12
Evaluation/MaxReturn                       -49.6722
Evaluation/MinReturn                     -2063.37
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       214.692
Extras/EpisodeRewardMean                   -93.142
LinearFeatureBaseline/ExplainedVariance      0.109713
PolicyExecTime                               0.11358
ProcessExecTime                              0.0121844
TotalEnvSteps                            13156
policy/Entropy                               2.63362
policy/KL                                    0.00679593
policy/KLBefore                              0
policy/LossAfter                            -0.0456162
policy/LossBefore                            2.35591e-08
policy/Perplexity                           13.924
policy/dLoss                                 0.0456163
---------------------------------------  ---------------
2022-04-23 14:19:20 | [train_policy] epoch #13 | Obtaining samples for iteration 13...
2022-04-23 14:19:20 | [train_policy] epoch #13 | Logging diagnostics...
2022-04-23 14:19:20 | [train_policy] epoch #13 | Optimizing policy...
2022-04-23 14:19:20 | [train_policy] epoch #13 | Computing loss before
2022-04-23 14:19:20 | [train_policy] epoch #13 | Computing KL before
2022-04-23 14:19:20 | [train_policy] epoch #13 | Optimizing
2022-04-23 14:19:20 | [train_policy] epoch #13 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:20 | [train_policy] epoch #13 | computing loss before
2022-04-23 14:19:20 | [train_policy] epoch #13 | computing gradient
2022-04-23 14:19:20 | [train_policy] epoch #13 | gradient computed
2022-04-23 14:19:20 | [train_policy] epoch #13 | computing descent direction
2022-04-23 14:19:20 | [train_policy] epoch #13 | descent direction computed
2022-04-23 14:19:20 | [train_policy] epoch #13 | backtrack iters: 0
2022-04-23 14:19:20 | [train_policy] epoch #13 | optimization finished
2022-04-23 14:19:20 | [train_policy] epoch #13 | Computing KL after
2022-04-23 14:19:20 | [train_policy] epoch #13 | Computing loss after
2022-04-23 14:19:20 | [train_policy] epoch #13 | Fitting baseline...
2022-04-23 14:19:20 | [train_policy] epoch #13 | Saving snapshot...
2022-04-23 14:19:20 | [train_policy] epoch #13 | Saved
2022-04-23 14:19:20 | [train_policy] epoch #13 | Time 8.19 s
2022-04-23 14:19:20 | [train_policy] epoch #13 | EpochTime 0.36 s
---------------------------------------  --------------
EnvExecTime                                  0.125028
Evaluation/AverageDiscountedReturn         -84.9787
Evaluation/AverageReturn                   -84.9787
Evaluation/CompletionRate                    0
Evaluation/Iteration                        13
Evaluation/MaxReturn                       -49.6901
Evaluation/MinReturn                     -2043.44
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       205.461
Extras/EpisodeRewardMean                   -83.3205
LinearFeatureBaseline/ExplainedVariance      0.0540739
PolicyExecTime                               0.113556
ProcessExecTime                              0.0125427
TotalEnvSteps                            14168
policy/Entropy                               2.60764
policy/KL                                    0.00865677
policy/KLBefore                              0
policy/LossAfter                            -0.038475
policy/LossBefore                           -3.3454e-08
policy/Perplexity                           13.567
policy/dLoss                                 0.038475
---------------------------------------  --------------
2022-04-23 14:19:20 | [train_policy] epoch #14 | Obtaining samples for iteration 14...
2022-04-23 14:19:20 | [train_policy] epoch #14 | Logging diagnostics...
2022-04-23 14:19:20 | [train_policy] epoch #14 | Optimizing policy...
2022-04-23 14:19:20 | [train_policy] epoch #14 | Computing loss before
2022-04-23 14:19:20 | [train_policy] epoch #14 | Computing KL before
2022-04-23 14:19:20 | [train_policy] epoch #14 | Optimizing
2022-04-23 14:19:20 | [train_policy] epoch #14 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:20 | [train_policy] epoch #14 | computing loss before
2022-04-23 14:19:20 | [train_policy] epoch #14 | computing gradient
2022-04-23 14:19:20 | [train_policy] epoch #14 | gradient computed
2022-04-23 14:19:20 | [train_policy] epoch #14 | computing descent direction
2022-04-23 14:19:20 | [train_policy] epoch #14 | descent direction computed
2022-04-23 14:19:20 | [train_policy] epoch #14 | backtrack iters: 0
2022-04-23 14:19:20 | [train_policy] epoch #14 | optimization finished
2022-04-23 14:19:20 | [train_policy] epoch #14 | Computing KL after
2022-04-23 14:19:20 | [train_policy] epoch #14 | Computing loss after
2022-04-23 14:19:20 | [train_policy] epoch #14 | Fitting baseline...
2022-04-23 14:19:20 | [train_policy] epoch #14 | Saving snapshot...
2022-04-23 14:19:20 | [train_policy] epoch #14 | Saved
2022-04-23 14:19:20 | [train_policy] epoch #14 | Time 8.55 s
2022-04-23 14:19:20 | [train_policy] epoch #14 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.119497
Evaluation/AverageDiscountedReturn         -68.4475
Evaluation/AverageReturn                   -68.4475
Evaluation/CompletionRate                    0
Evaluation/Iteration                        14
Evaluation/MaxReturn                       -47.9232
Evaluation/MinReturn                      -405.138
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        36.2511
Extras/EpisodeRewardMean                   -68.1199
LinearFeatureBaseline/ExplainedVariance     -1.06155
PolicyExecTime                               0.108436
ProcessExecTime                              0.0116923
TotalEnvSteps                            15180
policy/Entropy                               2.55916
policy/KL                                    0.0097999
policy/KLBefore                              0
policy/LossAfter                            -0.0194039
policy/LossBefore                           -1.57846e-08
policy/Perplexity                           12.925
policy/dLoss                                 0.0194039
---------------------------------------  ---------------
2022-04-23 14:19:20 | [train_policy] epoch #15 | Obtaining samples for iteration 15...
2022-04-23 14:19:21 | [train_policy] epoch #15 | Logging diagnostics...
2022-04-23 14:19:21 | [train_policy] epoch #15 | Optimizing policy...
2022-04-23 14:19:21 | [train_policy] epoch #15 | Computing loss before
2022-04-23 14:19:21 | [train_policy] epoch #15 | Computing KL before
2022-04-23 14:19:21 | [train_policy] epoch #15 | Optimizing
2022-04-23 14:19:21 | [train_policy] epoch #15 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:21 | [train_policy] epoch #15 | computing loss before
2022-04-23 14:19:21 | [train_policy] epoch #15 | computing gradient
2022-04-23 14:19:21 | [train_policy] epoch #15 | gradient computed
2022-04-23 14:19:21 | [train_policy] epoch #15 | computing descent direction
2022-04-23 14:19:21 | [train_policy] epoch #15 | descent direction computed
2022-04-23 14:19:21 | [train_policy] epoch #15 | backtrack iters: 1
2022-04-23 14:19:21 | [train_policy] epoch #15 | optimization finished
2022-04-23 14:19:21 | [train_policy] epoch #15 | Computing KL after
2022-04-23 14:19:21 | [train_policy] epoch #15 | Computing loss after
2022-04-23 14:19:21 | [train_policy] epoch #15 | Fitting baseline...
2022-04-23 14:19:21 | [train_policy] epoch #15 | Saving snapshot...
2022-04-23 14:19:21 | [train_policy] epoch #15 | Saved
2022-04-23 14:19:21 | [train_policy] epoch #15 | Time 8.90 s
2022-04-23 14:19:21 | [train_policy] epoch #15 | EpochTime 0.35 s
---------------------------------------  --------------
EnvExecTime                                  0.115957
Evaluation/AverageDiscountedReturn         -61.3808
Evaluation/AverageReturn                   -61.3808
Evaluation/CompletionRate                    0
Evaluation/Iteration                        15
Evaluation/MaxReturn                       -47.0359
Evaluation/MinReturn                       -79.7949
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.90483
Extras/EpisodeRewardMean                   -64.7419
LinearFeatureBaseline/ExplainedVariance      0.94358
PolicyExecTime                               0.101066
ProcessExecTime                              0.0108798
TotalEnvSteps                            16192
policy/Entropy                               2.55519
policy/KL                                    0.00671354
policy/KLBefore                              0
policy/LossAfter                            -0.0467897
policy/LossBefore                            2.8271e-09
policy/Perplexity                           12.8738
policy/dLoss                                 0.0467897
---------------------------------------  --------------
2022-04-23 14:19:21 | [train_policy] epoch #16 | Obtaining samples for iteration 16...
2022-04-23 14:19:21 | [train_policy] epoch #16 | Logging diagnostics...
2022-04-23 14:19:21 | [train_policy] epoch #16 | Optimizing policy...
2022-04-23 14:19:21 | [train_policy] epoch #16 | Computing loss before
2022-04-23 14:19:21 | [train_policy] epoch #16 | Computing KL before
2022-04-23 14:19:21 | [train_policy] epoch #16 | Optimizing
2022-04-23 14:19:21 | [train_policy] epoch #16 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:21 | [train_policy] epoch #16 | computing loss before
2022-04-23 14:19:21 | [train_policy] epoch #16 | computing gradient
2022-04-23 14:19:21 | [train_policy] epoch #16 | gradient computed
2022-04-23 14:19:21 | [train_policy] epoch #16 | computing descent direction
2022-04-23 14:19:21 | [train_policy] epoch #16 | descent direction computed
2022-04-23 14:19:21 | [train_policy] epoch #16 | backtrack iters: 1
2022-04-23 14:19:21 | [train_policy] epoch #16 | optimization finished
2022-04-23 14:19:21 | [train_policy] epoch #16 | Computing KL after
2022-04-23 14:19:21 | [train_policy] epoch #16 | Computing loss after
2022-04-23 14:19:21 | [train_policy] epoch #16 | Fitting baseline...
2022-04-23 14:19:21 | [train_policy] epoch #16 | Saving snapshot...
2022-04-23 14:19:21 | [train_policy] epoch #16 | Saved
2022-04-23 14:19:21 | [train_policy] epoch #16 | Time 9.26 s
2022-04-23 14:19:21 | [train_policy] epoch #16 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.116864
Evaluation/AverageDiscountedReturn         -61.0449
Evaluation/AverageReturn                   -61.0449
Evaluation/CompletionRate                    0
Evaluation/Iteration                        16
Evaluation/MaxReturn                       -44.993
Evaluation/MinReturn                       -76.0805
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.06716
Extras/EpisodeRewardMean                   -61.0218
LinearFeatureBaseline/ExplainedVariance      0.958833
PolicyExecTime                               0.107282
ProcessExecTime                              0.0114412
TotalEnvSteps                            17204
policy/Entropy                               2.54771
policy/KL                                    0.00714709
policy/KLBefore                              0
policy/LossAfter                            -0.0418546
policy/LossBefore                            1.31931e-08
policy/Perplexity                           12.7778
policy/dLoss                                 0.0418546
---------------------------------------  ---------------
2022-04-23 14:19:21 | [train_policy] epoch #17 | Obtaining samples for iteration 17...
2022-04-23 14:19:21 | [train_policy] epoch #17 | Logging diagnostics...
2022-04-23 14:19:21 | [train_policy] epoch #17 | Optimizing policy...
2022-04-23 14:19:21 | [train_policy] epoch #17 | Computing loss before
2022-04-23 14:19:21 | [train_policy] epoch #17 | Computing KL before
2022-04-23 14:19:21 | [train_policy] epoch #17 | Optimizing
2022-04-23 14:19:21 | [train_policy] epoch #17 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:21 | [train_policy] epoch #17 | computing loss before
2022-04-23 14:19:21 | [train_policy] epoch #17 | computing gradient
2022-04-23 14:19:21 | [train_policy] epoch #17 | gradient computed
2022-04-23 14:19:21 | [train_policy] epoch #17 | computing descent direction
2022-04-23 14:19:21 | [train_policy] epoch #17 | descent direction computed
2022-04-23 14:19:21 | [train_policy] epoch #17 | backtrack iters: 1
2022-04-23 14:19:21 | [train_policy] epoch #17 | optimization finished
2022-04-23 14:19:21 | [train_policy] epoch #17 | Computing KL after
2022-04-23 14:19:21 | [train_policy] epoch #17 | Computing loss after
2022-04-23 14:19:21 | [train_policy] epoch #17 | Fitting baseline...
2022-04-23 14:19:21 | [train_policy] epoch #17 | Saving snapshot...
2022-04-23 14:19:21 | [train_policy] epoch #17 | Saved
2022-04-23 14:19:21 | [train_policy] epoch #17 | Time 9.63 s
2022-04-23 14:19:21 | [train_policy] epoch #17 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.126966
Evaluation/AverageDiscountedReturn         -81.4208
Evaluation/AverageReturn                   -81.4208
Evaluation/CompletionRate                    0
Evaluation/Iteration                        17
Evaluation/MaxReturn                       -47.8958
Evaluation/MinReturn                     -2055.41
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       207.038
Extras/EpisodeRewardMean                   -79.5411
LinearFeatureBaseline/ExplainedVariance      0.0958421
PolicyExecTime                               0.108982
ProcessExecTime                              0.0125082
TotalEnvSteps                            18216
policy/Entropy                               2.54104
policy/KL                                    0.0068542
policy/KLBefore                              0
policy/LossAfter                            -0.0404812
policy/LossBefore                           -7.53893e-09
policy/Perplexity                           12.6929
policy/dLoss                                 0.0404812
---------------------------------------  ---------------
2022-04-23 14:19:21 | [train_policy] epoch #18 | Obtaining samples for iteration 18...
2022-04-23 14:19:22 | [train_policy] epoch #18 | Logging diagnostics...
2022-04-23 14:19:22 | [train_policy] epoch #18 | Optimizing policy...
2022-04-23 14:19:22 | [train_policy] epoch #18 | Computing loss before
2022-04-23 14:19:22 | [train_policy] epoch #18 | Computing KL before
2022-04-23 14:19:22 | [train_policy] epoch #18 | Optimizing
2022-04-23 14:19:22 | [train_policy] epoch #18 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:22 | [train_policy] epoch #18 | computing loss before
2022-04-23 14:19:22 | [train_policy] epoch #18 | computing gradient
2022-04-23 14:19:22 | [train_policy] epoch #18 | gradient computed
2022-04-23 14:19:22 | [train_policy] epoch #18 | computing descent direction
2022-04-23 14:19:22 | [train_policy] epoch #18 | descent direction computed
2022-04-23 14:19:22 | [train_policy] epoch #18 | backtrack iters: 1
2022-04-23 14:19:22 | [train_policy] epoch #18 | optimization finished
2022-04-23 14:19:22 | [train_policy] epoch #18 | Computing KL after
2022-04-23 14:19:22 | [train_policy] epoch #18 | Computing loss after
2022-04-23 14:19:22 | [train_policy] epoch #18 | Fitting baseline...
2022-04-23 14:19:22 | [train_policy] epoch #18 | Saving snapshot...
2022-04-23 14:19:22 | [train_policy] epoch #18 | Saved
2022-04-23 14:19:22 | [train_policy] epoch #18 | Time 9.99 s
2022-04-23 14:19:22 | [train_policy] epoch #18 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.120444
Evaluation/AverageDiscountedReturn         -81.3374
Evaluation/AverageReturn                   -81.3374
Evaluation/CompletionRate                    0
Evaluation/Iteration                        18
Evaluation/MaxReturn                       -44.1689
Evaluation/MinReturn                     -2047.89
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       206.397
Extras/EpisodeRewardMean                   -79.3431
LinearFeatureBaseline/ExplainedVariance      0.0538619
PolicyExecTime                               0.101612
ProcessExecTime                              0.0115373
TotalEnvSteps                            19228
policy/Entropy                               2.49798
policy/KL                                    0.00705943
policy/KLBefore                              0
policy/LossAfter                            -0.024718
policy/LossBefore                            1.11906e-08
policy/Perplexity                           12.1579
policy/dLoss                                 0.024718
---------------------------------------  ---------------
2022-04-23 14:19:22 | [train_policy] epoch #19 | Obtaining samples for iteration 19...
2022-04-23 14:19:22 | [train_policy] epoch #19 | Logging diagnostics...
2022-04-23 14:19:22 | [train_policy] epoch #19 | Optimizing policy...
2022-04-23 14:19:22 | [train_policy] epoch #19 | Computing loss before
2022-04-23 14:19:22 | [train_policy] epoch #19 | Computing KL before
2022-04-23 14:19:22 | [train_policy] epoch #19 | Optimizing
2022-04-23 14:19:22 | [train_policy] epoch #19 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:22 | [train_policy] epoch #19 | computing loss before
2022-04-23 14:19:22 | [train_policy] epoch #19 | computing gradient
2022-04-23 14:19:22 | [train_policy] epoch #19 | gradient computed
2022-04-23 14:19:22 | [train_policy] epoch #19 | computing descent direction
2022-04-23 14:19:22 | [train_policy] epoch #19 | descent direction computed
2022-04-23 14:19:22 | [train_policy] epoch #19 | backtrack iters: 1
2022-04-23 14:19:22 | [train_policy] epoch #19 | optimization finished
2022-04-23 14:19:22 | [train_policy] epoch #19 | Computing KL after
2022-04-23 14:19:22 | [train_policy] epoch #19 | Computing loss after
2022-04-23 14:19:22 | [train_policy] epoch #19 | Fitting baseline...
2022-04-23 14:19:22 | [train_policy] epoch #19 | Saving snapshot...
2022-04-23 14:19:22 | [train_policy] epoch #19 | Saved
2022-04-23 14:19:22 | [train_policy] epoch #19 | Time 10.34 s
2022-04-23 14:19:22 | [train_policy] epoch #19 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.119758
Evaluation/AverageDiscountedReturn         -60.078
Evaluation/AverageReturn                   -60.078
Evaluation/CompletionRate                    0
Evaluation/Iteration                        19
Evaluation/MaxReturn                       -40.8774
Evaluation/MinReturn                      -303.142
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        26.2837
Extras/EpisodeRewardMean                   -59.7582
LinearFeatureBaseline/ExplainedVariance      0.371887
PolicyExecTime                               0.100986
ProcessExecTime                              0.0115469
TotalEnvSteps                            20240
policy/Entropy                               2.46942
policy/KL                                    0.00646184
policy/KLBefore                              0
policy/LossAfter                            -0.0190757
policy/LossBefore                            3.62811e-08
policy/Perplexity                           11.8156
policy/dLoss                                 0.0190757
---------------------------------------  ---------------
2022-04-23 14:19:22 | [train_policy] epoch #20 | Obtaining samples for iteration 20...
2022-04-23 14:19:22 | [train_policy] epoch #20 | Logging diagnostics...
2022-04-23 14:19:22 | [train_policy] epoch #20 | Optimizing policy...
2022-04-23 14:19:22 | [train_policy] epoch #20 | Computing loss before
2022-04-23 14:19:22 | [train_policy] epoch #20 | Computing KL before
2022-04-23 14:19:22 | [train_policy] epoch #20 | Optimizing
2022-04-23 14:19:22 | [train_policy] epoch #20 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:22 | [train_policy] epoch #20 | computing loss before
2022-04-23 14:19:22 | [train_policy] epoch #20 | computing gradient
2022-04-23 14:19:22 | [train_policy] epoch #20 | gradient computed
2022-04-23 14:19:22 | [train_policy] epoch #20 | computing descent direction
2022-04-23 14:19:22 | [train_policy] epoch #20 | descent direction computed
2022-04-23 14:19:22 | [train_policy] epoch #20 | backtrack iters: 1
2022-04-23 14:19:22 | [train_policy] epoch #20 | optimization finished
2022-04-23 14:19:22 | [train_policy] epoch #20 | Computing KL after
2022-04-23 14:19:22 | [train_policy] epoch #20 | Computing loss after
2022-04-23 14:19:22 | [train_policy] epoch #20 | Fitting baseline...
2022-04-23 14:19:22 | [train_policy] epoch #20 | Saving snapshot...
2022-04-23 14:19:22 | [train_policy] epoch #20 | Saved
2022-04-23 14:19:22 | [train_policy] epoch #20 | Time 10.68 s
2022-04-23 14:19:22 | [train_policy] epoch #20 | EpochTime 0.34 s
---------------------------------------  -------------
EnvExecTime                                  0.11909
Evaluation/AverageDiscountedReturn         -81.5137
Evaluation/AverageReturn                   -81.5137
Evaluation/CompletionRate                    0
Evaluation/Iteration                        20
Evaluation/MaxReturn                       -42.6694
Evaluation/MinReturn                     -2062.52
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.387
Extras/EpisodeRewardMean                   -82.2004
LinearFeatureBaseline/ExplainedVariance      0.0108191
PolicyExecTime                               0.101193
ProcessExecTime                              0.0116277
TotalEnvSteps                            21252
policy/Entropy                               2.48556
policy/KL                                    0.0064102
policy/KLBefore                              0
policy/LossAfter                            -0.0167374
policy/LossBefore                           -0
policy/Perplexity                           12.0078
policy/dLoss                                 0.0167374
---------------------------------------  -------------
2022-04-23 14:19:22 | [train_policy] epoch #21 | Obtaining samples for iteration 21...
2022-04-23 14:19:23 | [train_policy] epoch #21 | Logging diagnostics...
2022-04-23 14:19:23 | [train_policy] epoch #21 | Optimizing policy...
2022-04-23 14:19:23 | [train_policy] epoch #21 | Computing loss before
2022-04-23 14:19:23 | [train_policy] epoch #21 | Computing KL before
2022-04-23 14:19:23 | [train_policy] epoch #21 | Optimizing
2022-04-23 14:19:23 | [train_policy] epoch #21 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:23 | [train_policy] epoch #21 | computing loss before
2022-04-23 14:19:23 | [train_policy] epoch #21 | computing gradient
2022-04-23 14:19:23 | [train_policy] epoch #21 | gradient computed
2022-04-23 14:19:23 | [train_policy] epoch #21 | computing descent direction
2022-04-23 14:19:23 | [train_policy] epoch #21 | descent direction computed
2022-04-23 14:19:23 | [train_policy] epoch #21 | backtrack iters: 1
2022-04-23 14:19:23 | [train_policy] epoch #21 | optimization finished
2022-04-23 14:19:23 | [train_policy] epoch #21 | Computing KL after
2022-04-23 14:19:23 | [train_policy] epoch #21 | Computing loss after
2022-04-23 14:19:23 | [train_policy] epoch #21 | Fitting baseline...
2022-04-23 14:19:23 | [train_policy] epoch #21 | Saving snapshot...
2022-04-23 14:19:23 | [train_policy] epoch #21 | Saved
2022-04-23 14:19:23 | [train_policy] epoch #21 | Time 11.03 s
2022-04-23 14:19:23 | [train_policy] epoch #21 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.120313
Evaluation/AverageDiscountedReturn         -69.7262
Evaluation/AverageReturn                   -69.7262
Evaluation/CompletionRate                    0
Evaluation/Iteration                        21
Evaluation/MaxReturn                       -45.6009
Evaluation/MinReturn                     -1241.7
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       123.235
Extras/EpisodeRewardMean                   -68.5233
LinearFeatureBaseline/ExplainedVariance     -0.25841
PolicyExecTime                               0.0968804
ProcessExecTime                              0.0112522
TotalEnvSteps                            22264
policy/Entropy                               2.44257
policy/KL                                    0.00709903
policy/KLBefore                              0
policy/LossAfter                            -0.0186113
policy/LossBefore                           -2.35591e-09
policy/Perplexity                           11.5025
policy/dLoss                                 0.0186113
---------------------------------------  ---------------
2022-04-23 14:19:23 | [train_policy] epoch #22 | Obtaining samples for iteration 22...
2022-04-23 14:19:23 | [train_policy] epoch #22 | Logging diagnostics...
2022-04-23 14:19:23 | [train_policy] epoch #22 | Optimizing policy...
2022-04-23 14:19:23 | [train_policy] epoch #22 | Computing loss before
2022-04-23 14:19:23 | [train_policy] epoch #22 | Computing KL before
2022-04-23 14:19:23 | [train_policy] epoch #22 | Optimizing
2022-04-23 14:19:23 | [train_policy] epoch #22 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:23 | [train_policy] epoch #22 | computing loss before
2022-04-23 14:19:23 | [train_policy] epoch #22 | computing gradient
2022-04-23 14:19:23 | [train_policy] epoch #22 | gradient computed
2022-04-23 14:19:23 | [train_policy] epoch #22 | computing descent direction
2022-04-23 14:19:23 | [train_policy] epoch #22 | descent direction computed
2022-04-23 14:19:23 | [train_policy] epoch #22 | backtrack iters: 1
2022-04-23 14:19:23 | [train_policy] epoch #22 | optimization finished
2022-04-23 14:19:23 | [train_policy] epoch #22 | Computing KL after
2022-04-23 14:19:23 | [train_policy] epoch #22 | Computing loss after
2022-04-23 14:19:23 | [train_policy] epoch #22 | Fitting baseline...
2022-04-23 14:19:23 | [train_policy] epoch #22 | Saving snapshot...
2022-04-23 14:19:23 | [train_policy] epoch #22 | Saved
2022-04-23 14:19:23 | [train_policy] epoch #22 | Time 11.37 s
2022-04-23 14:19:23 | [train_policy] epoch #22 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.117496
Evaluation/AverageDiscountedReturn         -59.2704
Evaluation/AverageReturn                   -59.2704
Evaluation/CompletionRate                    0
Evaluation/Iteration                        22
Evaluation/MaxReturn                       -44.2907
Evaluation/MinReturn                      -196.921
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        16.0831
Extras/EpisodeRewardMean                   -59.8613
LinearFeatureBaseline/ExplainedVariance     -0.838237
PolicyExecTime                               0.0984399
ProcessExecTime                              0.0112236
TotalEnvSteps                            23276
policy/Entropy                               2.42669
policy/KL                                    0.00994834
policy/KLBefore                              0
policy/LossAfter                            -0.0166655
policy/LossBefore                            8.95248e-09
policy/Perplexity                           11.3214
policy/dLoss                                 0.0166655
---------------------------------------  ---------------
2022-04-23 14:19:23 | [train_policy] epoch #23 | Obtaining samples for iteration 23...
2022-04-23 14:19:23 | [train_policy] epoch #23 | Logging diagnostics...
2022-04-23 14:19:23 | [train_policy] epoch #23 | Optimizing policy...
2022-04-23 14:19:23 | [train_policy] epoch #23 | Computing loss before
2022-04-23 14:19:23 | [train_policy] epoch #23 | Computing KL before
2022-04-23 14:19:23 | [train_policy] epoch #23 | Optimizing
2022-04-23 14:19:23 | [train_policy] epoch #23 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:23 | [train_policy] epoch #23 | computing loss before
2022-04-23 14:19:23 | [train_policy] epoch #23 | computing gradient
2022-04-23 14:19:23 | [train_policy] epoch #23 | gradient computed
2022-04-23 14:19:23 | [train_policy] epoch #23 | computing descent direction
2022-04-23 14:19:23 | [train_policy] epoch #23 | descent direction computed
2022-04-23 14:19:23 | [train_policy] epoch #23 | backtrack iters: 0
2022-04-23 14:19:23 | [train_policy] epoch #23 | optimization finished
2022-04-23 14:19:23 | [train_policy] epoch #23 | Computing KL after
2022-04-23 14:19:23 | [train_policy] epoch #23 | Computing loss after
2022-04-23 14:19:23 | [train_policy] epoch #23 | Fitting baseline...
2022-04-23 14:19:23 | [train_policy] epoch #23 | Saving snapshot...
2022-04-23 14:19:23 | [train_policy] epoch #23 | Saved
2022-04-23 14:19:23 | [train_policy] epoch #23 | Time 11.71 s
2022-04-23 14:19:23 | [train_policy] epoch #23 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.119684
Evaluation/AverageDiscountedReturn         -58.8077
Evaluation/AverageReturn                   -58.8077
Evaluation/CompletionRate                    0
Evaluation/Iteration                        23
Evaluation/MaxReturn                       -43.2811
Evaluation/MinReturn                       -81.5143
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.17774
Extras/EpisodeRewardMean                   -58.684
LinearFeatureBaseline/ExplainedVariance      0.948127
PolicyExecTime                               0.101121
ProcessExecTime                              0.0114238
TotalEnvSteps                            24288
policy/Entropy                               2.42457
policy/KL                                    0.00996295
policy/KLBefore                              0
policy/LossAfter                            -0.0393422
policy/LossBefore                            1.62558e-08
policy/Perplexity                           11.2974
policy/dLoss                                 0.0393422
---------------------------------------  ---------------
2022-04-23 14:19:24 | [train_policy] epoch #24 | Obtaining samples for iteration 24...
2022-04-23 14:19:24 | [train_policy] epoch #24 | Logging diagnostics...
2022-04-23 14:19:24 | [train_policy] epoch #24 | Optimizing policy...
2022-04-23 14:19:24 | [train_policy] epoch #24 | Computing loss before
2022-04-23 14:19:24 | [train_policy] epoch #24 | Computing KL before
2022-04-23 14:19:24 | [train_policy] epoch #24 | Optimizing
2022-04-23 14:19:24 | [train_policy] epoch #24 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:24 | [train_policy] epoch #24 | computing loss before
2022-04-23 14:19:24 | [train_policy] epoch #24 | computing gradient
2022-04-23 14:19:24 | [train_policy] epoch #24 | gradient computed
2022-04-23 14:19:24 | [train_policy] epoch #24 | computing descent direction
2022-04-23 14:19:24 | [train_policy] epoch #24 | descent direction computed
2022-04-23 14:19:24 | [train_policy] epoch #24 | backtrack iters: 0
2022-04-23 14:19:24 | [train_policy] epoch #24 | optimization finished
2022-04-23 14:19:24 | [train_policy] epoch #24 | Computing KL after
2022-04-23 14:19:24 | [train_policy] epoch #24 | Computing loss after
2022-04-23 14:19:24 | [train_policy] epoch #24 | Fitting baseline...
2022-04-23 14:19:24 | [train_policy] epoch #24 | Saving snapshot...
2022-04-23 14:19:24 | [train_policy] epoch #24 | Saved
2022-04-23 14:19:24 | [train_policy] epoch #24 | Time 12.06 s
2022-04-23 14:19:24 | [train_policy] epoch #24 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.120107
Evaluation/AverageDiscountedReturn         -60.6037
Evaluation/AverageReturn                   -60.6037
Evaluation/CompletionRate                    0
Evaluation/Iteration                        24
Evaluation/MaxReturn                       -41.7525
Evaluation/MinReturn                      -306.567
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        27.0629
Extras/EpisodeRewardMean                   -60.4529
LinearFeatureBaseline/ExplainedVariance      0.304878
PolicyExecTime                               0.102272
ProcessExecTime                              0.0114231
TotalEnvSteps                            25300
policy/Entropy                               2.37242
policy/KL                                    0.00856956
policy/KLBefore                              0
policy/LossAfter                            -0.0257278
policy/LossBefore                            1.31931e-08
policy/Perplexity                           10.7233
policy/dLoss                                 0.0257278
---------------------------------------  ---------------
2022-04-23 14:19:24 | [train_policy] epoch #25 | Obtaining samples for iteration 25...
2022-04-23 14:19:24 | [train_policy] epoch #25 | Logging diagnostics...
2022-04-23 14:19:24 | [train_policy] epoch #25 | Optimizing policy...
2022-04-23 14:19:24 | [train_policy] epoch #25 | Computing loss before
2022-04-23 14:19:24 | [train_policy] epoch #25 | Computing KL before
2022-04-23 14:19:24 | [train_policy] epoch #25 | Optimizing
2022-04-23 14:19:24 | [train_policy] epoch #25 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:24 | [train_policy] epoch #25 | computing loss before
2022-04-23 14:19:24 | [train_policy] epoch #25 | computing gradient
2022-04-23 14:19:24 | [train_policy] epoch #25 | gradient computed
2022-04-23 14:19:24 | [train_policy] epoch #25 | computing descent direction
2022-04-23 14:19:24 | [train_policy] epoch #25 | descent direction computed
2022-04-23 14:19:24 | [train_policy] epoch #25 | backtrack iters: 1
2022-04-23 14:19:24 | [train_policy] epoch #25 | optimization finished
2022-04-23 14:19:24 | [train_policy] epoch #25 | Computing KL after
2022-04-23 14:19:24 | [train_policy] epoch #25 | Computing loss after
2022-04-23 14:19:24 | [train_policy] epoch #25 | Fitting baseline...
2022-04-23 14:19:24 | [train_policy] epoch #25 | Saving snapshot...
2022-04-23 14:19:24 | [train_policy] epoch #25 | Saved
2022-04-23 14:19:24 | [train_policy] epoch #25 | Time 12.40 s
2022-04-23 14:19:24 | [train_policy] epoch #25 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.118881
Evaluation/AverageDiscountedReturn         -56.7908
Evaluation/AverageReturn                   -56.7908
Evaluation/CompletionRate                    0
Evaluation/Iteration                        25
Evaluation/MaxReturn                       -44.0254
Evaluation/MinReturn                       -73.5106
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.15237
Extras/EpisodeRewardMean                   -56.434
LinearFeatureBaseline/ExplainedVariance      0.800001
PolicyExecTime                               0.100926
ProcessExecTime                              0.0112634
TotalEnvSteps                            26312
policy/Entropy                               2.3815
policy/KL                                    0.00669236
policy/KLBefore                              0
policy/LossAfter                            -0.014001
policy/LossBefore                            1.97897e-08
policy/Perplexity                           10.8211
policy/dLoss                                 0.014001
---------------------------------------  ---------------
2022-04-23 14:19:24 | [train_policy] epoch #26 | Obtaining samples for iteration 26...
2022-04-23 14:19:24 | [train_policy] epoch #26 | Logging diagnostics...
2022-04-23 14:19:24 | [train_policy] epoch #26 | Optimizing policy...
2022-04-23 14:19:24 | [train_policy] epoch #26 | Computing loss before
2022-04-23 14:19:24 | [train_policy] epoch #26 | Computing KL before
2022-04-23 14:19:24 | [train_policy] epoch #26 | Optimizing
2022-04-23 14:19:24 | [train_policy] epoch #26 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:24 | [train_policy] epoch #26 | computing loss before
2022-04-23 14:19:24 | [train_policy] epoch #26 | computing gradient
2022-04-23 14:19:24 | [train_policy] epoch #26 | gradient computed
2022-04-23 14:19:24 | [train_policy] epoch #26 | computing descent direction
2022-04-23 14:19:24 | [train_policy] epoch #26 | descent direction computed
2022-04-23 14:19:25 | [train_policy] epoch #26 | backtrack iters: 1
2022-04-23 14:19:25 | [train_policy] epoch #26 | optimization finished
2022-04-23 14:19:25 | [train_policy] epoch #26 | Computing KL after
2022-04-23 14:19:25 | [train_policy] epoch #26 | Computing loss after
2022-04-23 14:19:25 | [train_policy] epoch #26 | Fitting baseline...
2022-04-23 14:19:25 | [train_policy] epoch #26 | Saving snapshot...
2022-04-23 14:19:25 | [train_policy] epoch #26 | Saved
2022-04-23 14:19:25 | [train_policy] epoch #26 | Time 12.73 s
2022-04-23 14:19:25 | [train_policy] epoch #26 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.118802
Evaluation/AverageDiscountedReturn         -57.0068
Evaluation/AverageReturn                   -57.0068
Evaluation/CompletionRate                    0
Evaluation/Iteration                        26
Evaluation/MaxReturn                       -42.6
Evaluation/MinReturn                       -94.4771
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.80916
Extras/EpisodeRewardMean                   -57.0712
LinearFeatureBaseline/ExplainedVariance      0.945018
PolicyExecTime                               0.0949204
ProcessExecTime                              0.0111787
TotalEnvSteps                            27324
policy/Entropy                               2.38214
policy/KL                                    0.00657311
policy/KLBefore                              0
policy/LossAfter                            -0.0285586
policy/LossBefore                            2.02609e-08
policy/Perplexity                           10.8281
policy/dLoss                                 0.0285587
---------------------------------------  ---------------
2022-04-23 14:19:25 | [train_policy] epoch #27 | Obtaining samples for iteration 27...
2022-04-23 14:19:25 | [train_policy] epoch #27 | Logging diagnostics...
2022-04-23 14:19:25 | [train_policy] epoch #27 | Optimizing policy...
2022-04-23 14:19:25 | [train_policy] epoch #27 | Computing loss before
2022-04-23 14:19:25 | [train_policy] epoch #27 | Computing KL before
2022-04-23 14:19:25 | [train_policy] epoch #27 | Optimizing
2022-04-23 14:19:25 | [train_policy] epoch #27 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:25 | [train_policy] epoch #27 | computing loss before
2022-04-23 14:19:25 | [train_policy] epoch #27 | computing gradient
2022-04-23 14:19:25 | [train_policy] epoch #27 | gradient computed
2022-04-23 14:19:25 | [train_policy] epoch #27 | computing descent direction
2022-04-23 14:19:25 | [train_policy] epoch #27 | descent direction computed
2022-04-23 14:19:25 | [train_policy] epoch #27 | backtrack iters: 1
2022-04-23 14:19:25 | [train_policy] epoch #27 | optimization finished
2022-04-23 14:19:25 | [train_policy] epoch #27 | Computing KL after
2022-04-23 14:19:25 | [train_policy] epoch #27 | Computing loss after
2022-04-23 14:19:25 | [train_policy] epoch #27 | Fitting baseline...
2022-04-23 14:19:25 | [train_policy] epoch #27 | Saving snapshot...
2022-04-23 14:19:25 | [train_policy] epoch #27 | Saved
2022-04-23 14:19:25 | [train_policy] epoch #27 | Time 13.08 s
2022-04-23 14:19:25 | [train_policy] epoch #27 | EpochTime 0.34 s
---------------------------------------  --------------
EnvExecTime                                  0.118044
Evaluation/AverageDiscountedReturn         -77.2119
Evaluation/AverageReturn                   -77.2119
Evaluation/CompletionRate                    0
Evaluation/Iteration                        27
Evaluation/MaxReturn                       -39.0631
Evaluation/MinReturn                     -2041.81
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       206.027
Extras/EpisodeRewardMean                   -75.599
LinearFeatureBaseline/ExplainedVariance      0.0451695
PolicyExecTime                               0.0972581
ProcessExecTime                              0.0112901
TotalEnvSteps                            28336
policy/Entropy                               2.35097
policy/KL                                    0.00645315
policy/KLBefore                              0
policy/LossAfter                            -0.0297373
policy/LossBefore                           -8.2457e-09
policy/Perplexity                           10.4958
policy/dLoss                                 0.0297373
---------------------------------------  --------------
2022-04-23 14:19:25 | [train_policy] epoch #28 | Obtaining samples for iteration 28...
2022-04-23 14:19:25 | [train_policy] epoch #28 | Logging diagnostics...
2022-04-23 14:19:25 | [train_policy] epoch #28 | Optimizing policy...
2022-04-23 14:19:25 | [train_policy] epoch #28 | Computing loss before
2022-04-23 14:19:25 | [train_policy] epoch #28 | Computing KL before
2022-04-23 14:19:25 | [train_policy] epoch #28 | Optimizing
2022-04-23 14:19:25 | [train_policy] epoch #28 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:25 | [train_policy] epoch #28 | computing loss before
2022-04-23 14:19:25 | [train_policy] epoch #28 | computing gradient
2022-04-23 14:19:25 | [train_policy] epoch #28 | gradient computed
2022-04-23 14:19:25 | [train_policy] epoch #28 | computing descent direction
2022-04-23 14:19:25 | [train_policy] epoch #28 | descent direction computed
2022-04-23 14:19:25 | [train_policy] epoch #28 | backtrack iters: 1
2022-04-23 14:19:25 | [train_policy] epoch #28 | optimization finished
2022-04-23 14:19:25 | [train_policy] epoch #28 | Computing KL after
2022-04-23 14:19:25 | [train_policy] epoch #28 | Computing loss after
2022-04-23 14:19:25 | [train_policy] epoch #28 | Fitting baseline...
2022-04-23 14:19:25 | [train_policy] epoch #28 | Saving snapshot...
2022-04-23 14:19:25 | [train_policy] epoch #28 | Saved
2022-04-23 14:19:25 | [train_policy] epoch #28 | Time 13.41 s
2022-04-23 14:19:25 | [train_policy] epoch #28 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.118556
Evaluation/AverageDiscountedReturn        -121.87
Evaluation/AverageReturn                  -121.87
Evaluation/CompletionRate                    0
Evaluation/Iteration                        28
Evaluation/MaxReturn                       -42.9868
Evaluation/MinReturn                     -4057.86
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       462.273
Extras/EpisodeRewardMean                  -116.642
LinearFeatureBaseline/ExplainedVariance      0.0137596
PolicyExecTime                               0.0929289
ProcessExecTime                              0.0109525
TotalEnvSteps                            29348
policy/Entropy                               2.27787
policy/KL                                    0.00646738
policy/KLBefore                              0
policy/LossAfter                            -0.0132274
policy/LossBefore                           -9.42366e-10
policy/Perplexity                            9.75588
policy/dLoss                                 0.0132274
---------------------------------------  ---------------
2022-04-23 14:19:25 | [train_policy] epoch #29 | Obtaining samples for iteration 29...
2022-04-23 14:19:25 | [train_policy] epoch #29 | Logging diagnostics...
2022-04-23 14:19:25 | [train_policy] epoch #29 | Optimizing policy...
2022-04-23 14:19:25 | [train_policy] epoch #29 | Computing loss before
2022-04-23 14:19:25 | [train_policy] epoch #29 | Computing KL before
2022-04-23 14:19:25 | [train_policy] epoch #29 | Optimizing
2022-04-23 14:19:25 | [train_policy] epoch #29 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:25 | [train_policy] epoch #29 | computing loss before
2022-04-23 14:19:25 | [train_policy] epoch #29 | computing gradient
2022-04-23 14:19:25 | [train_policy] epoch #29 | gradient computed
2022-04-23 14:19:25 | [train_policy] epoch #29 | computing descent direction
2022-04-23 14:19:26 | [train_policy] epoch #29 | descent direction computed
2022-04-23 14:19:26 | [train_policy] epoch #29 | backtrack iters: 0
2022-04-23 14:19:26 | [train_policy] epoch #29 | optimization finished
2022-04-23 14:19:26 | [train_policy] epoch #29 | Computing KL after
2022-04-23 14:19:26 | [train_policy] epoch #29 | Computing loss after
2022-04-23 14:19:26 | [train_policy] epoch #29 | Fitting baseline...
2022-04-23 14:19:26 | [train_policy] epoch #29 | Saving snapshot...
2022-04-23 14:19:26 | [train_policy] epoch #29 | Saved
2022-04-23 14:19:26 | [train_policy] epoch #29 | Time 13.75 s
2022-04-23 14:19:26 | [train_policy] epoch #29 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.118634
Evaluation/AverageDiscountedReturn         -55.5633
Evaluation/AverageReturn                   -55.5633
Evaluation/CompletionRate                    0
Evaluation/Iteration                        29
Evaluation/MaxReturn                       -40.7756
Evaluation/MinReturn                       -72.8725
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.22618
Extras/EpisodeRewardMean                   -75.7478
LinearFeatureBaseline/ExplainedVariance   -122.116
PolicyExecTime                               0.0958941
ProcessExecTime                              0.0111442
TotalEnvSteps                            30360
policy/Entropy                               2.23106
policy/KL                                    0.00984544
policy/KLBefore                              0
policy/LossAfter                            -0.0211911
policy/LossBefore                           -9.42366e-10
policy/Perplexity                            9.30969
policy/dLoss                                 0.0211911
---------------------------------------  ---------------
2022-04-23 14:19:26 | [train_policy] epoch #30 | Obtaining samples for iteration 30...
2022-04-23 14:19:26 | [train_policy] epoch #30 | Logging diagnostics...
2022-04-23 14:19:26 | [train_policy] epoch #30 | Optimizing policy...
2022-04-23 14:19:26 | [train_policy] epoch #30 | Computing loss before
2022-04-23 14:19:26 | [train_policy] epoch #30 | Computing KL before
2022-04-23 14:19:26 | [train_policy] epoch #30 | Optimizing
2022-04-23 14:19:26 | [train_policy] epoch #30 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:26 | [train_policy] epoch #30 | computing loss before
2022-04-23 14:19:26 | [train_policy] epoch #30 | computing gradient
2022-04-23 14:19:26 | [train_policy] epoch #30 | gradient computed
2022-04-23 14:19:26 | [train_policy] epoch #30 | computing descent direction
2022-04-23 14:19:26 | [train_policy] epoch #30 | descent direction computed
2022-04-23 14:19:26 | [train_policy] epoch #30 | backtrack iters: 1
2022-04-23 14:19:26 | [train_policy] epoch #30 | optimization finished
2022-04-23 14:19:26 | [train_policy] epoch #30 | Computing KL after
2022-04-23 14:19:26 | [train_policy] epoch #30 | Computing loss after
2022-04-23 14:19:26 | [train_policy] epoch #30 | Fitting baseline...
2022-04-23 14:19:26 | [train_policy] epoch #30 | Saving snapshot...
2022-04-23 14:19:26 | [train_policy] epoch #30 | Saved
2022-04-23 14:19:26 | [train_policy] epoch #30 | Time 14.09 s
2022-04-23 14:19:26 | [train_policy] epoch #30 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.118516
Evaluation/AverageDiscountedReturn         -54.725
Evaluation/AverageReturn                   -54.725
Evaluation/CompletionRate                    0
Evaluation/Iteration                        30
Evaluation/MaxReturn                       -37.1858
Evaluation/MinReturn                       -73.9934
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.37596
Extras/EpisodeRewardMean                   -55.0734
LinearFeatureBaseline/ExplainedVariance      0.962511
PolicyExecTime                               0.0945849
ProcessExecTime                              0.011209
TotalEnvSteps                            31372
policy/Entropy                               2.21563
policy/KL                                    0.00654959
policy/KLBefore                              0
policy/LossAfter                            -0.0387823
policy/LossBefore                            4.41734e-09
policy/Perplexity                            9.16722
policy/dLoss                                 0.0387823
---------------------------------------  ---------------
2022-04-23 14:19:26 | [train_policy] epoch #31 | Obtaining samples for iteration 31...
2022-04-23 14:19:26 | [train_policy] epoch #31 | Logging diagnostics...
2022-04-23 14:19:26 | [train_policy] epoch #31 | Optimizing policy...
2022-04-23 14:19:26 | [train_policy] epoch #31 | Computing loss before
2022-04-23 14:19:26 | [train_policy] epoch #31 | Computing KL before
2022-04-23 14:19:26 | [train_policy] epoch #31 | Optimizing
2022-04-23 14:19:26 | [train_policy] epoch #31 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:26 | [train_policy] epoch #31 | computing loss before
2022-04-23 14:19:26 | [train_policy] epoch #31 | computing gradient
2022-04-23 14:19:26 | [train_policy] epoch #31 | gradient computed
2022-04-23 14:19:26 | [train_policy] epoch #31 | computing descent direction
2022-04-23 14:19:26 | [train_policy] epoch #31 | descent direction computed
2022-04-23 14:19:26 | [train_policy] epoch #31 | backtrack iters: 1
2022-04-23 14:19:26 | [train_policy] epoch #31 | optimization finished
2022-04-23 14:19:26 | [train_policy] epoch #31 | Computing KL after
2022-04-23 14:19:26 | [train_policy] epoch #31 | Computing loss after
2022-04-23 14:19:26 | [train_policy] epoch #31 | Fitting baseline...
2022-04-23 14:19:26 | [train_policy] epoch #31 | Saving snapshot...
2022-04-23 14:19:26 | [train_policy] epoch #31 | Saved
2022-04-23 14:19:26 | [train_policy] epoch #31 | Time 14.45 s
2022-04-23 14:19:26 | [train_policy] epoch #31 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.117928
Evaluation/AverageDiscountedReturn         -74.7908
Evaluation/AverageReturn                   -74.7908
Evaluation/CompletionRate                    0
Evaluation/Iteration                        31
Evaluation/MaxReturn                       -37.4501
Evaluation/MinReturn                     -2055.83
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       207.775
Extras/EpisodeRewardMean                   -72.9618
LinearFeatureBaseline/ExplainedVariance      0.00380088
PolicyExecTime                               0.105297
ProcessExecTime                              0.0121374
TotalEnvSteps                            32384
policy/Entropy                               2.21412
policy/KL                                    0.00650236
policy/KLBefore                              0
policy/LossAfter                            -0.0167698
policy/LossBefore                           -9.42366e-10
policy/Perplexity                            9.15337
policy/dLoss                                 0.0167698
---------------------------------------  ---------------
2022-04-23 14:19:26 | [train_policy] epoch #32 | Obtaining samples for iteration 32...
2022-04-23 14:19:27 | [train_policy] epoch #32 | Logging diagnostics...
2022-04-23 14:19:27 | [train_policy] epoch #32 | Optimizing policy...
2022-04-23 14:19:27 | [train_policy] epoch #32 | Computing loss before
2022-04-23 14:19:27 | [train_policy] epoch #32 | Computing KL before
2022-04-23 14:19:27 | [train_policy] epoch #32 | Optimizing
2022-04-23 14:19:27 | [train_policy] epoch #32 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:27 | [train_policy] epoch #32 | computing loss before
2022-04-23 14:19:27 | [train_policy] epoch #32 | computing gradient
2022-04-23 14:19:27 | [train_policy] epoch #32 | gradient computed
2022-04-23 14:19:27 | [train_policy] epoch #32 | computing descent direction
2022-04-23 14:19:27 | [train_policy] epoch #32 | descent direction computed
2022-04-23 14:19:27 | [train_policy] epoch #32 | backtrack iters: 1
2022-04-23 14:19:27 | [train_policy] epoch #32 | optimization finished
2022-04-23 14:19:27 | [train_policy] epoch #32 | Computing KL after
2022-04-23 14:19:27 | [train_policy] epoch #32 | Computing loss after
2022-04-23 14:19:27 | [train_policy] epoch #32 | Fitting baseline...
2022-04-23 14:19:27 | [train_policy] epoch #32 | Saving snapshot...
2022-04-23 14:19:27 | [train_policy] epoch #32 | Saved
2022-04-23 14:19:27 | [train_policy] epoch #32 | Time 14.81 s
2022-04-23 14:19:27 | [train_policy] epoch #32 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.119601
Evaluation/AverageDiscountedReturn         -59.3284
Evaluation/AverageReturn                   -59.3284
Evaluation/CompletionRate                    0
Evaluation/Iteration                        32
Evaluation/MaxReturn                       -38.6602
Evaluation/MinReturn                      -521.778
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        49.2068
Extras/EpisodeRewardMean                   -58.7932
LinearFeatureBaseline/ExplainedVariance     -2.29996
PolicyExecTime                               0.105999
ProcessExecTime                              0.0119555
TotalEnvSteps                            33396
policy/Entropy                               2.21865
policy/KL                                    0.00651352
policy/KLBefore                              0
policy/LossAfter                            -0.0238112
policy/LossBefore                           -4.71183e-09
policy/Perplexity                            9.1949
policy/dLoss                                 0.0238112
---------------------------------------  ---------------
2022-04-23 14:19:27 | [train_policy] epoch #33 | Obtaining samples for iteration 33...
2022-04-23 14:19:27 | [train_policy] epoch #33 | Logging diagnostics...
2022-04-23 14:19:27 | [train_policy] epoch #33 | Optimizing policy...
2022-04-23 14:19:27 | [train_policy] epoch #33 | Computing loss before
2022-04-23 14:19:27 | [train_policy] epoch #33 | Computing KL before
2022-04-23 14:19:27 | [train_policy] epoch #33 | Optimizing
2022-04-23 14:19:27 | [train_policy] epoch #33 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:27 | [train_policy] epoch #33 | computing loss before
2022-04-23 14:19:27 | [train_policy] epoch #33 | computing gradient
2022-04-23 14:19:27 | [train_policy] epoch #33 | gradient computed
2022-04-23 14:19:27 | [train_policy] epoch #33 | computing descent direction
2022-04-23 14:19:27 | [train_policy] epoch #33 | descent direction computed
2022-04-23 14:19:27 | [train_policy] epoch #33 | backtrack iters: 0
2022-04-23 14:19:27 | [train_policy] epoch #33 | optimization finished
2022-04-23 14:19:27 | [train_policy] epoch #33 | Computing KL after
2022-04-23 14:19:27 | [train_policy] epoch #33 | Computing loss after
2022-04-23 14:19:27 | [train_policy] epoch #33 | Fitting baseline...
2022-04-23 14:19:27 | [train_policy] epoch #33 | Saving snapshot...
2022-04-23 14:19:27 | [train_policy] epoch #33 | Saved
2022-04-23 14:19:27 | [train_policy] epoch #33 | Time 15.15 s
2022-04-23 14:19:27 | [train_policy] epoch #33 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.118284
Evaluation/AverageDiscountedReturn         -55.2284
Evaluation/AverageReturn                   -55.2284
Evaluation/CompletionRate                    0
Evaluation/Iteration                        33
Evaluation/MaxReturn                       -39.6086
Evaluation/MinReturn                       -68.9699
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.91383
Extras/EpisodeRewardMean                   -54.7032
LinearFeatureBaseline/ExplainedVariance      0.632373
PolicyExecTime                               0.097682
ProcessExecTime                              0.0111761
TotalEnvSteps                            34408
policy/Entropy                               2.22407
policy/KL                                    0.0090793
policy/KLBefore                              0
policy/LossAfter                            -0.0230937
policy/LossBefore                           -3.15693e-08
policy/Perplexity                            9.24491
policy/dLoss                                 0.0230937
---------------------------------------  ---------------
2022-04-23 14:19:27 | [train_policy] epoch #34 | Obtaining samples for iteration 34...
2022-04-23 14:19:27 | [train_policy] epoch #34 | Logging diagnostics...
2022-04-23 14:19:27 | [train_policy] epoch #34 | Optimizing policy...
2022-04-23 14:19:27 | [train_policy] epoch #34 | Computing loss before
2022-04-23 14:19:27 | [train_policy] epoch #34 | Computing KL before
2022-04-23 14:19:27 | [train_policy] epoch #34 | Optimizing
2022-04-23 14:19:27 | [train_policy] epoch #34 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:27 | [train_policy] epoch #34 | computing loss before
2022-04-23 14:19:27 | [train_policy] epoch #34 | computing gradient
2022-04-23 14:19:27 | [train_policy] epoch #34 | gradient computed
2022-04-23 14:19:27 | [train_policy] epoch #34 | computing descent direction
2022-04-23 14:19:27 | [train_policy] epoch #34 | descent direction computed
2022-04-23 14:19:27 | [train_policy] epoch #34 | backtrack iters: 1
2022-04-23 14:19:27 | [train_policy] epoch #34 | optimization finished
2022-04-23 14:19:27 | [train_policy] epoch #34 | Computing KL after
2022-04-23 14:19:27 | [train_policy] epoch #34 | Computing loss after
2022-04-23 14:19:27 | [train_policy] epoch #34 | Fitting baseline...
2022-04-23 14:19:27 | [train_policy] epoch #34 | Saving snapshot...
2022-04-23 14:19:27 | [train_policy] epoch #34 | Saved
2022-04-23 14:19:27 | [train_policy] epoch #34 | Time 15.50 s
2022-04-23 14:19:27 | [train_policy] epoch #34 | EpochTime 0.34 s
---------------------------------------  --------------
EnvExecTime                                  0.118347
Evaluation/AverageDiscountedReturn         -55.0164
Evaluation/AverageReturn                   -55.0164
Evaluation/CompletionRate                    0
Evaluation/Iteration                        34
Evaluation/MaxReturn                       -37.2207
Evaluation/MinReturn                       -99.9795
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.87889
Extras/EpisodeRewardMean                   -55.1684
LinearFeatureBaseline/ExplainedVariance      0.954383
PolicyExecTime                               0.0998139
ProcessExecTime                              0.0113513
TotalEnvSteps                            35420
policy/Entropy                               2.24071
policy/KL                                    0.0065615
policy/KLBefore                              0
policy/LossAfter                            -0.0362985
policy/LossBefore                           -2.8271e-09
policy/Perplexity                            9.40003
policy/dLoss                                 0.0362985
---------------------------------------  --------------
2022-04-23 14:19:27 | [train_policy] epoch #35 | Obtaining samples for iteration 35...
2022-04-23 14:19:28 | [train_policy] epoch #35 | Logging diagnostics...
2022-04-23 14:19:28 | [train_policy] epoch #35 | Optimizing policy...
2022-04-23 14:19:28 | [train_policy] epoch #35 | Computing loss before
2022-04-23 14:19:28 | [train_policy] epoch #35 | Computing KL before
2022-04-23 14:19:28 | [train_policy] epoch #35 | Optimizing
2022-04-23 14:19:28 | [train_policy] epoch #35 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:28 | [train_policy] epoch #35 | computing loss before
2022-04-23 14:19:28 | [train_policy] epoch #35 | computing gradient
2022-04-23 14:19:28 | [train_policy] epoch #35 | gradient computed
2022-04-23 14:19:28 | [train_policy] epoch #35 | computing descent direction
2022-04-23 14:19:28 | [train_policy] epoch #35 | descent direction computed
2022-04-23 14:19:28 | [train_policy] epoch #35 | backtrack iters: 1
2022-04-23 14:19:28 | [train_policy] epoch #35 | optimization finished
2022-04-23 14:19:28 | [train_policy] epoch #35 | Computing KL after
2022-04-23 14:19:28 | [train_policy] epoch #35 | Computing loss after
2022-04-23 14:19:28 | [train_policy] epoch #35 | Fitting baseline...
2022-04-23 14:19:28 | [train_policy] epoch #35 | Saving snapshot...
2022-04-23 14:19:28 | [train_policy] epoch #35 | Saved
2022-04-23 14:19:28 | [train_policy] epoch #35 | Time 15.85 s
2022-04-23 14:19:28 | [train_policy] epoch #35 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.118667
Evaluation/AverageDiscountedReturn         -53.2942
Evaluation/AverageReturn                   -53.2942
Evaluation/CompletionRate                    0
Evaluation/Iteration                        35
Evaluation/MaxReturn                       -42.4906
Evaluation/MinReturn                       -71.396
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.29394
Extras/EpisodeRewardMean                   -53.4987
LinearFeatureBaseline/ExplainedVariance      0.955971
PolicyExecTime                               0.104163
ProcessExecTime                              0.0114715
TotalEnvSteps                            36432
policy/Entropy                               2.23391
policy/KL                                    0.00664755
policy/KLBefore                              0
policy/LossAfter                            -0.0363225
policy/LossBefore                            3.06269e-09
policy/Perplexity                            9.33626
policy/dLoss                                 0.0363225
---------------------------------------  ---------------
2022-04-23 14:19:28 | [train_policy] epoch #36 | Obtaining samples for iteration 36...
2022-04-23 14:19:28 | [train_policy] epoch #36 | Logging diagnostics...
2022-04-23 14:19:28 | [train_policy] epoch #36 | Optimizing policy...
2022-04-23 14:19:28 | [train_policy] epoch #36 | Computing loss before
2022-04-23 14:19:28 | [train_policy] epoch #36 | Computing KL before
2022-04-23 14:19:28 | [train_policy] epoch #36 | Optimizing
2022-04-23 14:19:28 | [train_policy] epoch #36 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:28 | [train_policy] epoch #36 | computing loss before
2022-04-23 14:19:28 | [train_policy] epoch #36 | computing gradient
2022-04-23 14:19:28 | [train_policy] epoch #36 | gradient computed
2022-04-23 14:19:28 | [train_policy] epoch #36 | computing descent direction
2022-04-23 14:19:28 | [train_policy] epoch #36 | descent direction computed
2022-04-23 14:19:28 | [train_policy] epoch #36 | backtrack iters: 1
2022-04-23 14:19:28 | [train_policy] epoch #36 | optimization finished
2022-04-23 14:19:28 | [train_policy] epoch #36 | Computing KL after
2022-04-23 14:19:28 | [train_policy] epoch #36 | Computing loss after
2022-04-23 14:19:28 | [train_policy] epoch #36 | Fitting baseline...
2022-04-23 14:19:28 | [train_policy] epoch #36 | Saving snapshot...
2022-04-23 14:19:28 | [train_policy] epoch #36 | Saved
2022-04-23 14:19:28 | [train_policy] epoch #36 | Time 16.21 s
2022-04-23 14:19:28 | [train_policy] epoch #36 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.121028
Evaluation/AverageDiscountedReturn         -53.4467
Evaluation/AverageReturn                   -53.4467
Evaluation/CompletionRate                    0
Evaluation/Iteration                        36
Evaluation/MaxReturn                       -39.8422
Evaluation/MinReturn                       -84.3301
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.72944
Extras/EpisodeRewardMean                   -53.221
LinearFeatureBaseline/ExplainedVariance      0.935923
PolicyExecTime                               0.102702
ProcessExecTime                              0.0118892
TotalEnvSteps                            37444
policy/Entropy                               2.22831
policy/KL                                    0.00732526
policy/KLBefore                              0
policy/LossAfter                            -0.0213183
policy/LossBefore                           -4.24065e-09
policy/Perplexity                            9.28413
policy/dLoss                                 0.0213183
---------------------------------------  ---------------
2022-04-23 14:19:28 | [train_policy] epoch #37 | Obtaining samples for iteration 37...
2022-04-23 14:19:28 | [train_policy] epoch #37 | Logging diagnostics...
2022-04-23 14:19:28 | [train_policy] epoch #37 | Optimizing policy...
2022-04-23 14:19:28 | [train_policy] epoch #37 | Computing loss before
2022-04-23 14:19:28 | [train_policy] epoch #37 | Computing KL before
2022-04-23 14:19:28 | [train_policy] epoch #37 | Optimizing
2022-04-23 14:19:28 | [train_policy] epoch #37 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:28 | [train_policy] epoch #37 | computing loss before
2022-04-23 14:19:28 | [train_policy] epoch #37 | computing gradient
2022-04-23 14:19:28 | [train_policy] epoch #37 | gradient computed
2022-04-23 14:19:28 | [train_policy] epoch #37 | computing descent direction
2022-04-23 14:19:28 | [train_policy] epoch #37 | descent direction computed
2022-04-23 14:19:28 | [train_policy] epoch #37 | backtrack iters: 1
2022-04-23 14:19:28 | [train_policy] epoch #37 | optimization finished
2022-04-23 14:19:28 | [train_policy] epoch #37 | Computing KL after
2022-04-23 14:19:28 | [train_policy] epoch #37 | Computing loss after
2022-04-23 14:19:28 | [train_policy] epoch #37 | Fitting baseline...
2022-04-23 14:19:28 | [train_policy] epoch #37 | Saving snapshot...
2022-04-23 14:19:28 | [train_policy] epoch #37 | Saved
2022-04-23 14:19:28 | [train_policy] epoch #37 | Time 16.57 s
2022-04-23 14:19:28 | [train_policy] epoch #37 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.125562
Evaluation/AverageDiscountedReturn         -55.6371
Evaluation/AverageReturn                   -55.6371
Evaluation/CompletionRate                    0
Evaluation/Iteration                        37
Evaluation/MaxReturn                       -38.6836
Evaluation/MinReturn                      -317.712
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        28.2974
Extras/EpisodeRewardMean                   -55.3492
LinearFeatureBaseline/ExplainedVariance      0.740572
PolicyExecTime                               0.101607
ProcessExecTime                              0.012691
TotalEnvSteps                            38456
policy/Entropy                               2.19219
policy/KL                                    0.00681182
policy/KLBefore                              0
policy/LossAfter                            -0.024445
policy/LossBefore                           -1.88473e-09
policy/Perplexity                            8.95484
policy/dLoss                                 0.024445
---------------------------------------  ---------------
2022-04-23 14:19:28 | [train_policy] epoch #38 | Obtaining samples for iteration 38...
2022-04-23 14:19:29 | [train_policy] epoch #38 | Logging diagnostics...
2022-04-23 14:19:29 | [train_policy] epoch #38 | Optimizing policy...
2022-04-23 14:19:29 | [train_policy] epoch #38 | Computing loss before
2022-04-23 14:19:29 | [train_policy] epoch #38 | Computing KL before
2022-04-23 14:19:29 | [train_policy] epoch #38 | Optimizing
2022-04-23 14:19:29 | [train_policy] epoch #38 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:29 | [train_policy] epoch #38 | computing loss before
2022-04-23 14:19:29 | [train_policy] epoch #38 | computing gradient
2022-04-23 14:19:29 | [train_policy] epoch #38 | gradient computed
2022-04-23 14:19:29 | [train_policy] epoch #38 | computing descent direction
2022-04-23 14:19:29 | [train_policy] epoch #38 | descent direction computed
2022-04-23 14:19:29 | [train_policy] epoch #38 | backtrack iters: 1
2022-04-23 14:19:29 | [train_policy] epoch #38 | optimization finished
2022-04-23 14:19:29 | [train_policy] epoch #38 | Computing KL after
2022-04-23 14:19:29 | [train_policy] epoch #38 | Computing loss after
2022-04-23 14:19:29 | [train_policy] epoch #38 | Fitting baseline...
2022-04-23 14:19:29 | [train_policy] epoch #38 | Saving snapshot...
2022-04-23 14:19:29 | [train_policy] epoch #38 | Saved
2022-04-23 14:19:29 | [train_policy] epoch #38 | Time 16.93 s
2022-04-23 14:19:29 | [train_policy] epoch #38 | EpochTime 0.35 s
---------------------------------------  -------------
EnvExecTime                                  0.118218
Evaluation/AverageDiscountedReturn         -74.116
Evaluation/AverageReturn                   -74.116
Evaluation/CompletionRate                    0
Evaluation/Iteration                        38
Evaluation/MaxReturn                       -36.8002
Evaluation/MinReturn                     -2118.86
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       214.427
Extras/EpisodeRewardMean                   -72.0555
LinearFeatureBaseline/ExplainedVariance      0.0101497
PolicyExecTime                               0.10062
ProcessExecTime                              0.0119283
TotalEnvSteps                            39468
policy/Entropy                               2.21249
policy/KL                                    0.0066429
policy/KLBefore                              0
policy/LossAfter                            -0.0240935
policy/LossBefore                           -0
policy/Perplexity                            9.13845
policy/dLoss                                 0.0240935
---------------------------------------  -------------
2022-04-23 14:19:29 | [train_policy] epoch #39 | Obtaining samples for iteration 39...
2022-04-23 14:19:29 | [train_policy] epoch #39 | Logging diagnostics...
2022-04-23 14:19:29 | [train_policy] epoch #39 | Optimizing policy...
2022-04-23 14:19:29 | [train_policy] epoch #39 | Computing loss before
2022-04-23 14:19:29 | [train_policy] epoch #39 | Computing KL before
2022-04-23 14:19:29 | [train_policy] epoch #39 | Optimizing
2022-04-23 14:19:29 | [train_policy] epoch #39 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:29 | [train_policy] epoch #39 | computing loss before
2022-04-23 14:19:29 | [train_policy] epoch #39 | computing gradient
2022-04-23 14:19:29 | [train_policy] epoch #39 | gradient computed
2022-04-23 14:19:29 | [train_policy] epoch #39 | computing descent direction
2022-04-23 14:19:29 | [train_policy] epoch #39 | descent direction computed
2022-04-23 14:19:29 | [train_policy] epoch #39 | backtrack iters: 1
2022-04-23 14:19:29 | [train_policy] epoch #39 | optimization finished
2022-04-23 14:19:29 | [train_policy] epoch #39 | Computing KL after
2022-04-23 14:19:29 | [train_policy] epoch #39 | Computing loss after
2022-04-23 14:19:29 | [train_policy] epoch #39 | Fitting baseline...
2022-04-23 14:19:29 | [train_policy] epoch #39 | Saving snapshot...
2022-04-23 14:19:29 | [train_policy] epoch #39 | Saved
2022-04-23 14:19:29 | [train_policy] epoch #39 | Time 17.29 s
2022-04-23 14:19:29 | [train_policy] epoch #39 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.123305
Evaluation/AverageDiscountedReturn         -81.1458
Evaluation/AverageReturn                   -81.1458
Evaluation/CompletionRate                    0
Evaluation/Iteration                        39
Evaluation/MaxReturn                       -37.4619
Evaluation/MinReturn                     -2039.21
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.365
Extras/EpisodeRewardMean                   -99.2829
LinearFeatureBaseline/ExplainedVariance     -0.306508
PolicyExecTime                               0.106099
ProcessExecTime                              0.0122707
TotalEnvSteps                            40480
policy/Entropy                               2.18618
policy/KL                                    0.00653617
policy/KLBefore                              0
policy/LossAfter                            -0.0250398
policy/LossBefore                           -3.29828e-09
policy/Perplexity                            8.90114
policy/dLoss                                 0.0250398
---------------------------------------  ---------------
2022-04-23 14:19:29 | [train_policy] epoch #40 | Obtaining samples for iteration 40...
2022-04-23 14:19:29 | [train_policy] epoch #40 | Logging diagnostics...
2022-04-23 14:19:29 | [train_policy] epoch #40 | Optimizing policy...
2022-04-23 14:19:29 | [train_policy] epoch #40 | Computing loss before
2022-04-23 14:19:29 | [train_policy] epoch #40 | Computing KL before
2022-04-23 14:19:29 | [train_policy] epoch #40 | Optimizing
2022-04-23 14:19:29 | [train_policy] epoch #40 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:29 | [train_policy] epoch #40 | computing loss before
2022-04-23 14:19:29 | [train_policy] epoch #40 | computing gradient
2022-04-23 14:19:29 | [train_policy] epoch #40 | gradient computed
2022-04-23 14:19:29 | [train_policy] epoch #40 | computing descent direction
2022-04-23 14:19:29 | [train_policy] epoch #40 | descent direction computed
2022-04-23 14:19:29 | [train_policy] epoch #40 | backtrack iters: 0
2022-04-23 14:19:29 | [train_policy] epoch #40 | optimization finished
2022-04-23 14:19:29 | [train_policy] epoch #40 | Computing KL after
2022-04-23 14:19:29 | [train_policy] epoch #40 | Computing loss after
2022-04-23 14:19:29 | [train_policy] epoch #40 | Fitting baseline...
2022-04-23 14:19:29 | [train_policy] epoch #40 | Saving snapshot...
2022-04-23 14:19:29 | [train_policy] epoch #40 | Saved
2022-04-23 14:19:29 | [train_policy] epoch #40 | Time 17.63 s
2022-04-23 14:19:29 | [train_policy] epoch #40 | EpochTime 0.34 s
---------------------------------------  --------------
EnvExecTime                                  0.118929
Evaluation/AverageDiscountedReturn         -91.4567
Evaluation/AverageReturn                   -91.4567
Evaluation/CompletionRate                    0
Evaluation/Iteration                        40
Evaluation/MaxReturn                       -40.0893
Evaluation/MinReturn                     -2059.33
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       229.207
Extras/EpisodeRewardMean                   -88.6552
LinearFeatureBaseline/ExplainedVariance     -0.543739
PolicyExecTime                               0.0998857
ProcessExecTime                              0.0112395
TotalEnvSteps                            41492
policy/Entropy                               2.15725
policy/KL                                    0.00950283
policy/KLBefore                              0
policy/LossAfter                            -0.031117
policy/LossBefore                            5.6542e-09
policy/Perplexity                            8.64731
policy/dLoss                                 0.031117
---------------------------------------  --------------
2022-04-23 14:19:29 | [train_policy] epoch #41 | Obtaining samples for iteration 41...
2022-04-23 14:19:30 | [train_policy] epoch #41 | Logging diagnostics...
2022-04-23 14:19:30 | [train_policy] epoch #41 | Optimizing policy...
2022-04-23 14:19:30 | [train_policy] epoch #41 | Computing loss before
2022-04-23 14:19:30 | [train_policy] epoch #41 | Computing KL before
2022-04-23 14:19:30 | [train_policy] epoch #41 | Optimizing
2022-04-23 14:19:30 | [train_policy] epoch #41 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:30 | [train_policy] epoch #41 | computing loss before
2022-04-23 14:19:30 | [train_policy] epoch #41 | computing gradient
2022-04-23 14:19:30 | [train_policy] epoch #41 | gradient computed
2022-04-23 14:19:30 | [train_policy] epoch #41 | computing descent direction
2022-04-23 14:19:30 | [train_policy] epoch #41 | descent direction computed
2022-04-23 14:19:30 | [train_policy] epoch #41 | backtrack iters: 1
2022-04-23 14:19:30 | [train_policy] epoch #41 | optimization finished
2022-04-23 14:19:30 | [train_policy] epoch #41 | Computing KL after
2022-04-23 14:19:30 | [train_policy] epoch #41 | Computing loss after
2022-04-23 14:19:30 | [train_policy] epoch #41 | Fitting baseline...
2022-04-23 14:19:30 | [train_policy] epoch #41 | Saving snapshot...
2022-04-23 14:19:30 | [train_policy] epoch #41 | Saved
2022-04-23 14:19:30 | [train_policy] epoch #41 | Time 17.98 s
2022-04-23 14:19:30 | [train_policy] epoch #41 | EpochTime 0.35 s
---------------------------------------  --------------
EnvExecTime                                  0.120432
Evaluation/AverageDiscountedReturn         -55.1543
Evaluation/AverageReturn                   -55.1543
Evaluation/CompletionRate                    0
Evaluation/Iteration                        41
Evaluation/MaxReturn                       -41.5143
Evaluation/MinReturn                      -241.556
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        20.5591
Extras/EpisodeRewardMean                   -54.706
LinearFeatureBaseline/ExplainedVariance    -14.9102
PolicyExecTime                               0.0982504
ProcessExecTime                              0.0117421
TotalEnvSteps                            42504
policy/Entropy                               2.13908
policy/KL                                    0.0068157
policy/KLBefore                              0
policy/LossAfter                            -0.0166894
policy/LossBefore                           -1.1544e-08
policy/Perplexity                            8.49162
policy/dLoss                                 0.0166894
---------------------------------------  --------------
2022-04-23 14:19:30 | [train_policy] epoch #42 | Obtaining samples for iteration 42...
2022-04-23 14:19:30 | [train_policy] epoch #42 | Logging diagnostics...
2022-04-23 14:19:30 | [train_policy] epoch #42 | Optimizing policy...
2022-04-23 14:19:30 | [train_policy] epoch #42 | Computing loss before
2022-04-23 14:19:30 | [train_policy] epoch #42 | Computing KL before
2022-04-23 14:19:30 | [train_policy] epoch #42 | Optimizing
2022-04-23 14:19:30 | [train_policy] epoch #42 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:30 | [train_policy] epoch #42 | computing loss before
2022-04-23 14:19:30 | [train_policy] epoch #42 | computing gradient
2022-04-23 14:19:30 | [train_policy] epoch #42 | gradient computed
2022-04-23 14:19:30 | [train_policy] epoch #42 | computing descent direction
2022-04-23 14:19:30 | [train_policy] epoch #42 | descent direction computed
2022-04-23 14:19:30 | [train_policy] epoch #42 | backtrack iters: 1
2022-04-23 14:19:30 | [train_policy] epoch #42 | optimization finished
2022-04-23 14:19:30 | [train_policy] epoch #42 | Computing KL after
2022-04-23 14:19:30 | [train_policy] epoch #42 | Computing loss after
2022-04-23 14:19:30 | [train_policy] epoch #42 | Fitting baseline...
2022-04-23 14:19:30 | [train_policy] epoch #42 | Saving snapshot...
2022-04-23 14:19:30 | [train_policy] epoch #42 | Saved
2022-04-23 14:19:30 | [train_policy] epoch #42 | Time 18.33 s
2022-04-23 14:19:30 | [train_policy] epoch #42 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.118479
Evaluation/AverageDiscountedReturn         -52.9558
Evaluation/AverageReturn                   -52.9558
Evaluation/CompletionRate                    0
Evaluation/Iteration                        42
Evaluation/MaxReturn                       -39.3198
Evaluation/MinReturn                       -67.143
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.38702
Extras/EpisodeRewardMean                   -53.0933
LinearFeatureBaseline/ExplainedVariance      0.926927
PolicyExecTime                               0.0972221
ProcessExecTime                              0.0113821
TotalEnvSteps                            43516
policy/Entropy                               2.08832
policy/KL                                    0.00692028
policy/KLBefore                              0
policy/LossAfter                            -0.0214459
policy/LossBefore                           -4.71183e-10
policy/Perplexity                            8.07131
policy/dLoss                                 0.0214459
---------------------------------------  ---------------
2022-04-23 14:19:30 | [train_policy] epoch #43 | Obtaining samples for iteration 43...
2022-04-23 14:19:30 | [train_policy] epoch #43 | Logging diagnostics...
2022-04-23 14:19:30 | [train_policy] epoch #43 | Optimizing policy...
2022-04-23 14:19:30 | [train_policy] epoch #43 | Computing loss before
2022-04-23 14:19:30 | [train_policy] epoch #43 | Computing KL before
2022-04-23 14:19:30 | [train_policy] epoch #43 | Optimizing
2022-04-23 14:19:30 | [train_policy] epoch #43 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:30 | [train_policy] epoch #43 | computing loss before
2022-04-23 14:19:30 | [train_policy] epoch #43 | computing gradient
2022-04-23 14:19:30 | [train_policy] epoch #43 | gradient computed
2022-04-23 14:19:30 | [train_policy] epoch #43 | computing descent direction
2022-04-23 14:19:30 | [train_policy] epoch #43 | descent direction computed
2022-04-23 14:19:30 | [train_policy] epoch #43 | backtrack iters: 1
2022-04-23 14:19:30 | [train_policy] epoch #43 | optimization finished
2022-04-23 14:19:30 | [train_policy] epoch #43 | Computing KL after
2022-04-23 14:19:30 | [train_policy] epoch #43 | Computing loss after
2022-04-23 14:19:30 | [train_policy] epoch #43 | Fitting baseline...
2022-04-23 14:19:30 | [train_policy] epoch #43 | Saving snapshot...
2022-04-23 14:19:30 | [train_policy] epoch #43 | Saved
2022-04-23 14:19:30 | [train_policy] epoch #43 | Time 18.66 s
2022-04-23 14:19:30 | [train_policy] epoch #43 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.117932
Evaluation/AverageDiscountedReturn         -51.5906
Evaluation/AverageReturn                   -51.5906
Evaluation/CompletionRate                    0
Evaluation/Iteration                        43
Evaluation/MaxReturn                       -38.785
Evaluation/MinReturn                       -88.2604
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.10119
Extras/EpisodeRewardMean                   -51.7017
LinearFeatureBaseline/ExplainedVariance      0.903963
PolicyExecTime                               0.0957558
ProcessExecTime                              0.0113752
TotalEnvSteps                            44528
policy/Entropy                               2.04929
policy/KL                                    0.00672463
policy/KLBefore                              0
policy/LossAfter                            -0.0228322
policy/LossBefore                            1.13084e-08
policy/Perplexity                            7.76235
policy/dLoss                                 0.0228322
---------------------------------------  ---------------
2022-04-23 14:19:30 | [train_policy] epoch #44 | Obtaining samples for iteration 44...
2022-04-23 14:19:31 | [train_policy] epoch #44 | Logging diagnostics...
2022-04-23 14:19:31 | [train_policy] epoch #44 | Optimizing policy...
2022-04-23 14:19:31 | [train_policy] epoch #44 | Computing loss before
2022-04-23 14:19:31 | [train_policy] epoch #44 | Computing KL before
2022-04-23 14:19:31 | [train_policy] epoch #44 | Optimizing
2022-04-23 14:19:31 | [train_policy] epoch #44 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:31 | [train_policy] epoch #44 | computing loss before
2022-04-23 14:19:31 | [train_policy] epoch #44 | computing gradient
2022-04-23 14:19:31 | [train_policy] epoch #44 | gradient computed
2022-04-23 14:19:31 | [train_policy] epoch #44 | computing descent direction
2022-04-23 14:19:31 | [train_policy] epoch #44 | descent direction computed
2022-04-23 14:19:31 | [train_policy] epoch #44 | backtrack iters: 0
2022-04-23 14:19:31 | [train_policy] epoch #44 | optimization finished
2022-04-23 14:19:31 | [train_policy] epoch #44 | Computing KL after
2022-04-23 14:19:31 | [train_policy] epoch #44 | Computing loss after
2022-04-23 14:19:31 | [train_policy] epoch #44 | Fitting baseline...
2022-04-23 14:19:31 | [train_policy] epoch #44 | Saving snapshot...
2022-04-23 14:19:31 | [train_policy] epoch #44 | Saved
2022-04-23 14:19:31 | [train_policy] epoch #44 | Time 19.00 s
2022-04-23 14:19:31 | [train_policy] epoch #44 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.118528
Evaluation/AverageDiscountedReturn         -52.5827
Evaluation/AverageReturn                   -52.5827
Evaluation/CompletionRate                    0
Evaluation/Iteration                        44
Evaluation/MaxReturn                       -37.7049
Evaluation/MinReturn                       -76.3608
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.60193
Extras/EpisodeRewardMean                   -52.6035
LinearFeatureBaseline/ExplainedVariance      0.878466
PolicyExecTime                               0.0992241
ProcessExecTime                              0.0112629
TotalEnvSteps                            45540
policy/Entropy                               2.02456
policy/KL                                    0.00986631
policy/KLBefore                              0
policy/LossAfter                            -0.0199811
policy/LossBefore                            8.95248e-09
policy/Perplexity                            7.57279
policy/dLoss                                 0.0199811
---------------------------------------  ---------------
2022-04-23 14:19:31 | [train_policy] epoch #45 | Obtaining samples for iteration 45...
2022-04-23 14:19:31 | [train_policy] epoch #45 | Logging diagnostics...
2022-04-23 14:19:31 | [train_policy] epoch #45 | Optimizing policy...
2022-04-23 14:19:31 | [train_policy] epoch #45 | Computing loss before
2022-04-23 14:19:31 | [train_policy] epoch #45 | Computing KL before
2022-04-23 14:19:31 | [train_policy] epoch #45 | Optimizing
2022-04-23 14:19:31 | [train_policy] epoch #45 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:31 | [train_policy] epoch #45 | computing loss before
2022-04-23 14:19:31 | [train_policy] epoch #45 | computing gradient
2022-04-23 14:19:31 | [train_policy] epoch #45 | gradient computed
2022-04-23 14:19:31 | [train_policy] epoch #45 | computing descent direction
2022-04-23 14:19:31 | [train_policy] epoch #45 | descent direction computed
2022-04-23 14:19:31 | [train_policy] epoch #45 | backtrack iters: 0
2022-04-23 14:19:31 | [train_policy] epoch #45 | optimization finished
2022-04-23 14:19:31 | [train_policy] epoch #45 | Computing KL after
2022-04-23 14:19:31 | [train_policy] epoch #45 | Computing loss after
2022-04-23 14:19:31 | [train_policy] epoch #45 | Fitting baseline...
2022-04-23 14:19:31 | [train_policy] epoch #45 | Saving snapshot...
2022-04-23 14:19:31 | [train_policy] epoch #45 | Saved
2022-04-23 14:19:31 | [train_policy] epoch #45 | Time 19.34 s
2022-04-23 14:19:31 | [train_policy] epoch #45 | EpochTime 0.34 s
---------------------------------------  --------------
EnvExecTime                                  0.117995
Evaluation/AverageDiscountedReturn         -52.4856
Evaluation/AverageReturn                   -52.4856
Evaluation/CompletionRate                    0
Evaluation/Iteration                        45
Evaluation/MaxReturn                       -38.8598
Evaluation/MinReturn                      -111.939
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        10.7299
Extras/EpisodeRewardMean                   -52.6424
LinearFeatureBaseline/ExplainedVariance      0.888935
PolicyExecTime                               0.0966854
ProcessExecTime                              0.011205
TotalEnvSteps                            46552
policy/Entropy                               1.96432
policy/KL                                    0.00927744
policy/KLBefore                              0
policy/LossAfter                            -0.0239796
policy/LossBefore                           -5.6542e-09
policy/Perplexity                            7.13004
policy/dLoss                                 0.0239796
---------------------------------------  --------------
2022-04-23 14:19:31 | [train_policy] epoch #46 | Obtaining samples for iteration 46...
2022-04-23 14:19:31 | [train_policy] epoch #46 | Logging diagnostics...
2022-04-23 14:19:31 | [train_policy] epoch #46 | Optimizing policy...
2022-04-23 14:19:31 | [train_policy] epoch #46 | Computing loss before
2022-04-23 14:19:31 | [train_policy] epoch #46 | Computing KL before
2022-04-23 14:19:31 | [train_policy] epoch #46 | Optimizing
2022-04-23 14:19:31 | [train_policy] epoch #46 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:31 | [train_policy] epoch #46 | computing loss before
2022-04-23 14:19:31 | [train_policy] epoch #46 | computing gradient
2022-04-23 14:19:31 | [train_policy] epoch #46 | gradient computed
2022-04-23 14:19:31 | [train_policy] epoch #46 | computing descent direction
2022-04-23 14:19:31 | [train_policy] epoch #46 | descent direction computed
2022-04-23 14:19:31 | [train_policy] epoch #46 | backtrack iters: 1
2022-04-23 14:19:31 | [train_policy] epoch #46 | optimization finished
2022-04-23 14:19:31 | [train_policy] epoch #46 | Computing KL after
2022-04-23 14:19:31 | [train_policy] epoch #46 | Computing loss after
2022-04-23 14:19:31 | [train_policy] epoch #46 | Fitting baseline...
2022-04-23 14:19:31 | [train_policy] epoch #46 | Saving snapshot...
2022-04-23 14:19:31 | [train_policy] epoch #46 | Saved
2022-04-23 14:19:31 | [train_policy] epoch #46 | Time 19.70 s
2022-04-23 14:19:31 | [train_policy] epoch #46 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.119365
Evaluation/AverageDiscountedReturn         -76.9385
Evaluation/AverageReturn                   -76.9385
Evaluation/CompletionRate                    0
Evaluation/Iteration                        46
Evaluation/MaxReturn                       -39.5359
Evaluation/MinReturn                     -2059.13
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.721
Extras/EpisodeRewardMean                   -75.6461
LinearFeatureBaseline/ExplainedVariance      0.00955338
PolicyExecTime                               0.106
ProcessExecTime                              0.0117133
TotalEnvSteps                            47564
policy/Entropy                               1.94219
policy/KL                                    0.00702101
policy/KLBefore                              0
policy/LossAfter                            -0.0125095
policy/LossBefore                            8.48129e-09
policy/Perplexity                            6.97399
policy/dLoss                                 0.0125095
---------------------------------------  ---------------
2022-04-23 14:19:31 | [train_policy] epoch #47 | Obtaining samples for iteration 47...
2022-04-23 14:19:32 | [train_policy] epoch #47 | Logging diagnostics...
2022-04-23 14:19:32 | [train_policy] epoch #47 | Optimizing policy...
2022-04-23 14:19:32 | [train_policy] epoch #47 | Computing loss before
2022-04-23 14:19:32 | [train_policy] epoch #47 | Computing KL before
2022-04-23 14:19:32 | [train_policy] epoch #47 | Optimizing
2022-04-23 14:19:32 | [train_policy] epoch #47 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:32 | [train_policy] epoch #47 | computing loss before
2022-04-23 14:19:32 | [train_policy] epoch #47 | computing gradient
2022-04-23 14:19:32 | [train_policy] epoch #47 | gradient computed
2022-04-23 14:19:32 | [train_policy] epoch #47 | computing descent direction
2022-04-23 14:19:32 | [train_policy] epoch #47 | descent direction computed
2022-04-23 14:19:32 | [train_policy] epoch #47 | backtrack iters: 0
2022-04-23 14:19:32 | [train_policy] epoch #47 | optimization finished
2022-04-23 14:19:32 | [train_policy] epoch #47 | Computing KL after
2022-04-23 14:19:32 | [train_policy] epoch #47 | Computing loss after
2022-04-23 14:19:32 | [train_policy] epoch #47 | Fitting baseline...
2022-04-23 14:19:32 | [train_policy] epoch #47 | Saving snapshot...
2022-04-23 14:19:32 | [train_policy] epoch #47 | Saved
2022-04-23 14:19:32 | [train_policy] epoch #47 | Time 20.05 s
2022-04-23 14:19:32 | [train_policy] epoch #47 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.118454
Evaluation/AverageDiscountedReturn         -52.7728
Evaluation/AverageReturn                   -52.7728
Evaluation/CompletionRate                    0
Evaluation/Iteration                        47
Evaluation/MaxReturn                       -40.3473
Evaluation/MinReturn                      -141.091
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        11.2147
Extras/EpisodeRewardMean                   -52.6502
LinearFeatureBaseline/ExplainedVariance    -13.3047
PolicyExecTime                               0.102831
ProcessExecTime                              0.0115201
TotalEnvSteps                            48576
policy/Entropy                               1.93929
policy/KL                                    0.00978379
policy/KLBefore                              0
policy/LossAfter                            -0.0204702
policy/LossBefore                           -9.18807e-09
policy/Perplexity                            6.95383
policy/dLoss                                 0.0204701
---------------------------------------  ---------------
2022-04-23 14:19:32 | [train_policy] epoch #48 | Obtaining samples for iteration 48...
2022-04-23 14:19:32 | [train_policy] epoch #48 | Logging diagnostics...
2022-04-23 14:19:32 | [train_policy] epoch #48 | Optimizing policy...
2022-04-23 14:19:32 | [train_policy] epoch #48 | Computing loss before
2022-04-23 14:19:32 | [train_policy] epoch #48 | Computing KL before
2022-04-23 14:19:32 | [train_policy] epoch #48 | Optimizing
2022-04-23 14:19:32 | [train_policy] epoch #48 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:32 | [train_policy] epoch #48 | computing loss before
2022-04-23 14:19:32 | [train_policy] epoch #48 | computing gradient
2022-04-23 14:19:32 | [train_policy] epoch #48 | gradient computed
2022-04-23 14:19:32 | [train_policy] epoch #48 | computing descent direction
2022-04-23 14:19:32 | [train_policy] epoch #48 | descent direction computed
2022-04-23 14:19:32 | [train_policy] epoch #48 | backtrack iters: 0
2022-04-23 14:19:32 | [train_policy] epoch #48 | optimization finished
2022-04-23 14:19:32 | [train_policy] epoch #48 | Computing KL after
2022-04-23 14:19:32 | [train_policy] epoch #48 | Computing loss after
2022-04-23 14:19:32 | [train_policy] epoch #48 | Fitting baseline...
2022-04-23 14:19:32 | [train_policy] epoch #48 | Saving snapshot...
2022-04-23 14:19:32 | [train_policy] epoch #48 | Saved
2022-04-23 14:19:32 | [train_policy] epoch #48 | Time 20.39 s
2022-04-23 14:19:32 | [train_policy] epoch #48 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.117288
Evaluation/AverageDiscountedReturn         -52.2919
Evaluation/AverageReturn                   -52.2919
Evaluation/CompletionRate                    0
Evaluation/Iteration                        48
Evaluation/MaxReturn                       -37.35
Evaluation/MinReturn                       -80.8256
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.22223
Extras/EpisodeRewardMean                   -51.8175
LinearFeatureBaseline/ExplainedVariance      0.924957
PolicyExecTime                               0.098079
ProcessExecTime                              0.0112085
TotalEnvSteps                            49588
policy/Entropy                               1.9359
policy/KL                                    0.00948919
policy/KLBefore                              0
policy/LossAfter                            -0.0305596
policy/LossBefore                            1.41355e-09
policy/Perplexity                            6.93026
policy/dLoss                                 0.0305596
---------------------------------------  ---------------
2022-04-23 14:19:32 | [train_policy] epoch #49 | Obtaining samples for iteration 49...
2022-04-23 14:19:32 | [train_policy] epoch #49 | Logging diagnostics...
2022-04-23 14:19:32 | [train_policy] epoch #49 | Optimizing policy...
2022-04-23 14:19:32 | [train_policy] epoch #49 | Computing loss before
2022-04-23 14:19:32 | [train_policy] epoch #49 | Computing KL before
2022-04-23 14:19:32 | [train_policy] epoch #49 | Optimizing
2022-04-23 14:19:32 | [train_policy] epoch #49 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:32 | [train_policy] epoch #49 | computing loss before
2022-04-23 14:19:32 | [train_policy] epoch #49 | computing gradient
2022-04-23 14:19:32 | [train_policy] epoch #49 | gradient computed
2022-04-23 14:19:32 | [train_policy] epoch #49 | computing descent direction
2022-04-23 14:19:32 | [train_policy] epoch #49 | descent direction computed
2022-04-23 14:19:32 | [train_policy] epoch #49 | backtrack iters: 1
2022-04-23 14:19:32 | [train_policy] epoch #49 | optimization finished
2022-04-23 14:19:32 | [train_policy] epoch #49 | Computing KL after
2022-04-23 14:19:32 | [train_policy] epoch #49 | Computing loss after
2022-04-23 14:19:33 | [train_policy] epoch #49 | Fitting baseline...
2022-04-23 14:19:33 | [train_policy] epoch #49 | Saving snapshot...
2022-04-23 14:19:33 | [train_policy] epoch #49 | Saved
2022-04-23 14:19:33 | [train_policy] epoch #49 | Time 20.73 s
2022-04-23 14:19:33 | [train_policy] epoch #49 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.11681
Evaluation/AverageDiscountedReturn         -93.8137
Evaluation/AverageReturn                   -93.8137
Evaluation/CompletionRate                    0
Evaluation/Iteration                        49
Evaluation/MaxReturn                       -39.4048
Evaluation/MinReturn                     -4056.01
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       415.4
Extras/EpisodeRewardMean                   -90.6985
LinearFeatureBaseline/ExplainedVariance      0.00638001
PolicyExecTime                               0.0994632
ProcessExecTime                              0.0112674
TotalEnvSteps                            50600
policy/Entropy                               1.93988
policy/KL                                    0.00674009
policy/KLBefore                              0
policy/LossAfter                            -0.0178198
policy/LossBefore                            1.17796e-08
policy/Perplexity                            6.95792
policy/dLoss                                 0.0178198
---------------------------------------  ---------------
2022-04-23 14:19:33 | [train_policy] epoch #50 | Obtaining samples for iteration 50...
2022-04-23 14:19:33 | [train_policy] epoch #50 | Logging diagnostics...
2022-04-23 14:19:33 | [train_policy] epoch #50 | Optimizing policy...
2022-04-23 14:19:33 | [train_policy] epoch #50 | Computing loss before
2022-04-23 14:19:33 | [train_policy] epoch #50 | Computing KL before
2022-04-23 14:19:33 | [train_policy] epoch #50 | Optimizing
2022-04-23 14:19:33 | [train_policy] epoch #50 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:33 | [train_policy] epoch #50 | computing loss before
2022-04-23 14:19:33 | [train_policy] epoch #50 | computing gradient
2022-04-23 14:19:33 | [train_policy] epoch #50 | gradient computed
2022-04-23 14:19:33 | [train_policy] epoch #50 | computing descent direction
2022-04-23 14:19:33 | [train_policy] epoch #50 | descent direction computed
2022-04-23 14:19:33 | [train_policy] epoch #50 | backtrack iters: 1
2022-04-23 14:19:33 | [train_policy] epoch #50 | optimization finished
2022-04-23 14:19:33 | [train_policy] epoch #50 | Computing KL after
2022-04-23 14:19:33 | [train_policy] epoch #50 | Computing loss after
2022-04-23 14:19:33 | [train_policy] epoch #50 | Fitting baseline...
2022-04-23 14:19:33 | [train_policy] epoch #50 | Saving snapshot...
2022-04-23 14:19:33 | [train_policy] epoch #50 | Saved
2022-04-23 14:19:33 | [train_policy] epoch #50 | Time 21.07 s
2022-04-23 14:19:33 | [train_policy] epoch #50 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.117366
Evaluation/AverageDiscountedReturn         -50.6258
Evaluation/AverageReturn                   -50.6258
Evaluation/CompletionRate                    0
Evaluation/Iteration                        50
Evaluation/MaxReturn                       -38.595
Evaluation/MinReturn                       -85.8405
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.23748
Extras/EpisodeRewardMean                   -90.5561
LinearFeatureBaseline/ExplainedVariance    -58.468
PolicyExecTime                               0.0991576
ProcessExecTime                              0.0112126
TotalEnvSteps                            51612
policy/Entropy                               1.93304
policy/KL                                    0.00735123
policy/KLBefore                              0
policy/LossAfter                            -0.0161527
policy/LossBefore                            3.81658e-08
policy/Perplexity                            6.91052
policy/dLoss                                 0.0161527
---------------------------------------  ---------------
2022-04-23 14:19:33 | [train_policy] epoch #51 | Obtaining samples for iteration 51...
2022-04-23 14:19:33 | [train_policy] epoch #51 | Logging diagnostics...
2022-04-23 14:19:33 | [train_policy] epoch #51 | Optimizing policy...
2022-04-23 14:19:33 | [train_policy] epoch #51 | Computing loss before
2022-04-23 14:19:33 | [train_policy] epoch #51 | Computing KL before
2022-04-23 14:19:33 | [train_policy] epoch #51 | Optimizing
2022-04-23 14:19:33 | [train_policy] epoch #51 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:33 | [train_policy] epoch #51 | computing loss before
2022-04-23 14:19:33 | [train_policy] epoch #51 | computing gradient
2022-04-23 14:19:33 | [train_policy] epoch #51 | gradient computed
2022-04-23 14:19:33 | [train_policy] epoch #51 | computing descent direction
2022-04-23 14:19:33 | [train_policy] epoch #51 | descent direction computed
2022-04-23 14:19:33 | [train_policy] epoch #51 | backtrack iters: 0
2022-04-23 14:19:33 | [train_policy] epoch #51 | optimization finished
2022-04-23 14:19:33 | [train_policy] epoch #51 | Computing KL after
2022-04-23 14:19:33 | [train_policy] epoch #51 | Computing loss after
2022-04-23 14:19:33 | [train_policy] epoch #51 | Fitting baseline...
2022-04-23 14:19:33 | [train_policy] epoch #51 | Saving snapshot...
2022-04-23 14:19:33 | [train_policy] epoch #51 | Saved
2022-04-23 14:19:33 | [train_policy] epoch #51 | Time 21.42 s
2022-04-23 14:19:33 | [train_policy] epoch #51 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.12007
Evaluation/AverageDiscountedReturn         -52.3484
Evaluation/AverageReturn                   -52.3484
Evaluation/CompletionRate                    0
Evaluation/Iteration                        51
Evaluation/MaxReturn                       -36.8474
Evaluation/MinReturn                      -197.285
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        17.1144
Extras/EpisodeRewardMean                   -52.6275
LinearFeatureBaseline/ExplainedVariance      0.626459
PolicyExecTime                               0.104785
ProcessExecTime                              0.0118477
TotalEnvSteps                            52624
policy/Entropy                               1.89668
policy/KL                                    0.00911823
policy/KLBefore                              0
policy/LossAfter                            -0.0199131
policy/LossBefore                           -8.95248e-09
policy/Perplexity                            6.66373
policy/dLoss                                 0.0199131
---------------------------------------  ---------------
2022-04-23 14:19:33 | [train_policy] epoch #52 | Obtaining samples for iteration 52...
2022-04-23 14:19:33 | [train_policy] epoch #52 | Logging diagnostics...
2022-04-23 14:19:33 | [train_policy] epoch #52 | Optimizing policy...
2022-04-23 14:19:33 | [train_policy] epoch #52 | Computing loss before
2022-04-23 14:19:33 | [train_policy] epoch #52 | Computing KL before
2022-04-23 14:19:33 | [train_policy] epoch #52 | Optimizing
2022-04-23 14:19:33 | [train_policy] epoch #52 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:33 | [train_policy] epoch #52 | computing loss before
2022-04-23 14:19:33 | [train_policy] epoch #52 | computing gradient
2022-04-23 14:19:33 | [train_policy] epoch #52 | gradient computed
2022-04-23 14:19:33 | [train_policy] epoch #52 | computing descent direction
2022-04-23 14:19:34 | [train_policy] epoch #52 | descent direction computed
2022-04-23 14:19:34 | [train_policy] epoch #52 | backtrack iters: 1
2022-04-23 14:19:34 | [train_policy] epoch #52 | optimization finished
2022-04-23 14:19:34 | [train_policy] epoch #52 | Computing KL after
2022-04-23 14:19:34 | [train_policy] epoch #52 | Computing loss after
2022-04-23 14:19:34 | [train_policy] epoch #52 | Fitting baseline...
2022-04-23 14:19:34 | [train_policy] epoch #52 | Saving snapshot...
2022-04-23 14:19:34 | [train_policy] epoch #52 | Saved
2022-04-23 14:19:34 | [train_policy] epoch #52 | Time 21.77 s
2022-04-23 14:19:34 | [train_policy] epoch #52 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.115744
Evaluation/AverageDiscountedReturn         -55.9644
Evaluation/AverageReturn                   -55.9644
Evaluation/CompletionRate                    0
Evaluation/Iteration                        52
Evaluation/MaxReturn                       -37.3715
Evaluation/MinReturn                      -527.732
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        49.9209
Extras/EpisodeRewardMean                   -55.4177
LinearFeatureBaseline/ExplainedVariance      0.317856
PolicyExecTime                               0.103324
ProcessExecTime                              0.0111911
TotalEnvSteps                            53636
policy/Entropy                               1.88254
policy/KL                                    0.00691413
policy/KLBefore                              0
policy/LossAfter                            -0.0125923
policy/LossBefore                           -1.22508e-08
policy/Perplexity                            6.57016
policy/dLoss                                 0.0125922
---------------------------------------  ---------------
2022-04-23 14:19:34 | [train_policy] epoch #53 | Obtaining samples for iteration 53...
2022-04-23 14:19:34 | [train_policy] epoch #53 | Logging diagnostics...
2022-04-23 14:19:34 | [train_policy] epoch #53 | Optimizing policy...
2022-04-23 14:19:34 | [train_policy] epoch #53 | Computing loss before
2022-04-23 14:19:34 | [train_policy] epoch #53 | Computing KL before
2022-04-23 14:19:34 | [train_policy] epoch #53 | Optimizing
2022-04-23 14:19:34 | [train_policy] epoch #53 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:34 | [train_policy] epoch #53 | computing loss before
2022-04-23 14:19:34 | [train_policy] epoch #53 | computing gradient
2022-04-23 14:19:34 | [train_policy] epoch #53 | gradient computed
2022-04-23 14:19:34 | [train_policy] epoch #53 | computing descent direction
2022-04-23 14:19:34 | [train_policy] epoch #53 | descent direction computed
2022-04-23 14:19:34 | [train_policy] epoch #53 | backtrack iters: 1
2022-04-23 14:19:34 | [train_policy] epoch #53 | optimization finished
2022-04-23 14:19:34 | [train_policy] epoch #53 | Computing KL after
2022-04-23 14:19:34 | [train_policy] epoch #53 | Computing loss after
2022-04-23 14:19:34 | [train_policy] epoch #53 | Fitting baseline...
2022-04-23 14:19:34 | [train_policy] epoch #53 | Saving snapshot...
2022-04-23 14:19:34 | [train_policy] epoch #53 | Saved
2022-04-23 14:19:34 | [train_policy] epoch #53 | Time 22.12 s
2022-04-23 14:19:34 | [train_policy] epoch #53 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.11741
Evaluation/AverageDiscountedReturn         -94.4531
Evaluation/AverageReturn                   -94.4531
Evaluation/CompletionRate                    0
Evaluation/Iteration                        53
Evaluation/MaxReturn                       -37.3832
Evaluation/MinReturn                     -2070.24
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       293.715
Extras/EpisodeRewardMean                   -91.2795
LinearFeatureBaseline/ExplainedVariance      0.0121899
PolicyExecTime                               0.103519
ProcessExecTime                              0.0114572
TotalEnvSteps                            54648
policy/Entropy                               1.90121
policy/KL                                    0.00738462
policy/KLBefore                              0
policy/LossAfter                            -0.0157104
policy/LossBefore                           -6.59656e-09
policy/Perplexity                            6.69397
policy/dLoss                                 0.0157104
---------------------------------------  ---------------
2022-04-23 14:19:34 | [train_policy] epoch #54 | Obtaining samples for iteration 54...
2022-04-23 14:19:34 | [train_policy] epoch #54 | Logging diagnostics...
2022-04-23 14:19:34 | [train_policy] epoch #54 | Optimizing policy...
2022-04-23 14:19:34 | [train_policy] epoch #54 | Computing loss before
2022-04-23 14:19:34 | [train_policy] epoch #54 | Computing KL before
2022-04-23 14:19:34 | [train_policy] epoch #54 | Optimizing
2022-04-23 14:19:34 | [train_policy] epoch #54 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:34 | [train_policy] epoch #54 | computing loss before
2022-04-23 14:19:34 | [train_policy] epoch #54 | computing gradient
2022-04-23 14:19:34 | [train_policy] epoch #54 | gradient computed
2022-04-23 14:19:34 | [train_policy] epoch #54 | computing descent direction
2022-04-23 14:19:34 | [train_policy] epoch #54 | descent direction computed
2022-04-23 14:19:34 | [train_policy] epoch #54 | backtrack iters: 1
2022-04-23 14:19:34 | [train_policy] epoch #54 | optimization finished
2022-04-23 14:19:34 | [train_policy] epoch #54 | Computing KL after
2022-04-23 14:19:34 | [train_policy] epoch #54 | Computing loss after
2022-04-23 14:19:34 | [train_policy] epoch #54 | Fitting baseline...
2022-04-23 14:19:34 | [train_policy] epoch #54 | Saving snapshot...
2022-04-23 14:19:34 | [train_policy] epoch #54 | Saved
2022-04-23 14:19:34 | [train_policy] epoch #54 | Time 22.48 s
2022-04-23 14:19:34 | [train_policy] epoch #54 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.116257
Evaluation/AverageDiscountedReturn         -52.8913
Evaluation/AverageReturn                   -52.8913
Evaluation/CompletionRate                    0
Evaluation/Iteration                        54
Evaluation/MaxReturn                       -41.5265
Evaluation/MinReturn                      -108.208
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.87221
Extras/EpisodeRewardMean                   -72.7536
LinearFeatureBaseline/ExplainedVariance    -34.2835
PolicyExecTime                               0.109603
ProcessExecTime                              0.0113976
TotalEnvSteps                            55660
policy/Entropy                               1.9114
policy/KL                                    0.00813244
policy/KLBefore                              0
policy/LossAfter                            -0.0143107
policy/LossBefore                            4.00506e-09
policy/Perplexity                            6.76257
policy/dLoss                                 0.0143107
---------------------------------------  ---------------
2022-04-23 14:19:34 | [train_policy] epoch #55 | Obtaining samples for iteration 55...
2022-04-23 14:19:35 | [train_policy] epoch #55 | Logging diagnostics...
2022-04-23 14:19:35 | [train_policy] epoch #55 | Optimizing policy...
2022-04-23 14:19:35 | [train_policy] epoch #55 | Computing loss before
2022-04-23 14:19:35 | [train_policy] epoch #55 | Computing KL before
2022-04-23 14:19:35 | [train_policy] epoch #55 | Optimizing
2022-04-23 14:19:35 | [train_policy] epoch #55 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:35 | [train_policy] epoch #55 | computing loss before
2022-04-23 14:19:35 | [train_policy] epoch #55 | computing gradient
2022-04-23 14:19:35 | [train_policy] epoch #55 | gradient computed
2022-04-23 14:19:35 | [train_policy] epoch #55 | computing descent direction
2022-04-23 14:19:35 | [train_policy] epoch #55 | descent direction computed
2022-04-23 14:19:35 | [train_policy] epoch #55 | backtrack iters: 1
2022-04-23 14:19:35 | [train_policy] epoch #55 | optimization finished
2022-04-23 14:19:35 | [train_policy] epoch #55 | Computing KL after
2022-04-23 14:19:35 | [train_policy] epoch #55 | Computing loss after
2022-04-23 14:19:35 | [train_policy] epoch #55 | Fitting baseline...
2022-04-23 14:19:35 | [train_policy] epoch #55 | Saving snapshot...
2022-04-23 14:19:35 | [train_policy] epoch #55 | Saved
2022-04-23 14:19:35 | [train_policy] epoch #55 | Time 22.82 s
2022-04-23 14:19:35 | [train_policy] epoch #55 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.115983
Evaluation/AverageDiscountedReturn         -50.1236
Evaluation/AverageReturn                   -50.1236
Evaluation/CompletionRate                    0
Evaluation/Iteration                        55
Evaluation/MaxReturn                       -39.2989
Evaluation/MinReturn                       -86.6412
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.58437
Extras/EpisodeRewardMean                   -50.479
LinearFeatureBaseline/ExplainedVariance      0.920316
PolicyExecTime                               0.0967705
ProcessExecTime                              0.0109689
TotalEnvSteps                            56672
policy/Entropy                               1.88513
policy/KL                                    0.00686865
policy/KLBefore                              0
policy/LossAfter                            -0.0227527
policy/LossBefore                           -3.25116e-08
policy/Perplexity                            6.58723
policy/dLoss                                 0.0227527
---------------------------------------  ---------------
2022-04-23 14:19:35 | [train_policy] epoch #56 | Obtaining samples for iteration 56...
2022-04-23 14:19:35 | [train_policy] epoch #56 | Logging diagnostics...
2022-04-23 14:19:35 | [train_policy] epoch #56 | Optimizing policy...
2022-04-23 14:19:35 | [train_policy] epoch #56 | Computing loss before
2022-04-23 14:19:35 | [train_policy] epoch #56 | Computing KL before
2022-04-23 14:19:35 | [train_policy] epoch #56 | Optimizing
2022-04-23 14:19:35 | [train_policy] epoch #56 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:35 | [train_policy] epoch #56 | computing loss before
2022-04-23 14:19:35 | [train_policy] epoch #56 | computing gradient
2022-04-23 14:19:35 | [train_policy] epoch #56 | gradient computed
2022-04-23 14:19:35 | [train_policy] epoch #56 | computing descent direction
2022-04-23 14:19:35 | [train_policy] epoch #56 | descent direction computed
2022-04-23 14:19:35 | [train_policy] epoch #56 | backtrack iters: 1
2022-04-23 14:19:35 | [train_policy] epoch #56 | optimization finished
2022-04-23 14:19:35 | [train_policy] epoch #56 | Computing KL after
2022-04-23 14:19:35 | [train_policy] epoch #56 | Computing loss after
2022-04-23 14:19:35 | [train_policy] epoch #56 | Fitting baseline...
2022-04-23 14:19:35 | [train_policy] epoch #56 | Saving snapshot...
2022-04-23 14:19:35 | [train_policy] epoch #56 | Saved
2022-04-23 14:19:35 | [train_policy] epoch #56 | Time 23.16 s
2022-04-23 14:19:35 | [train_policy] epoch #56 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.117044
Evaluation/AverageDiscountedReturn         -50.0482
Evaluation/AverageReturn                   -50.0482
Evaluation/CompletionRate                    0
Evaluation/Iteration                        56
Evaluation/MaxReturn                       -36.6035
Evaluation/MinReturn                       -73.2159
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.73592
Extras/EpisodeRewardMean                   -50.1705
LinearFeatureBaseline/ExplainedVariance      0.94556
PolicyExecTime                               0.0950427
ProcessExecTime                              0.0112114
TotalEnvSteps                            57684
policy/Entropy                               1.85655
policy/KL                                    0.00663355
policy/KLBefore                              0
policy/LossAfter                            -0.0215996
policy/LossBefore                            9.42366e-10
policy/Perplexity                            6.40163
policy/dLoss                                 0.0215996
---------------------------------------  ---------------
2022-04-23 14:19:35 | [train_policy] epoch #57 | Obtaining samples for iteration 57...
2022-04-23 14:19:35 | [train_policy] epoch #57 | Logging diagnostics...
2022-04-23 14:19:35 | [train_policy] epoch #57 | Optimizing policy...
2022-04-23 14:19:35 | [train_policy] epoch #57 | Computing loss before
2022-04-23 14:19:35 | [train_policy] epoch #57 | Computing KL before
2022-04-23 14:19:35 | [train_policy] epoch #57 | Optimizing
2022-04-23 14:19:35 | [train_policy] epoch #57 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:35 | [train_policy] epoch #57 | computing loss before
2022-04-23 14:19:35 | [train_policy] epoch #57 | computing gradient
2022-04-23 14:19:35 | [train_policy] epoch #57 | gradient computed
2022-04-23 14:19:35 | [train_policy] epoch #57 | computing descent direction
2022-04-23 14:19:35 | [train_policy] epoch #57 | descent direction computed
2022-04-23 14:19:35 | [train_policy] epoch #57 | backtrack iters: 1
2022-04-23 14:19:35 | [train_policy] epoch #57 | optimization finished
2022-04-23 14:19:35 | [train_policy] epoch #57 | Computing KL after
2022-04-23 14:19:35 | [train_policy] epoch #57 | Computing loss after
2022-04-23 14:19:35 | [train_policy] epoch #57 | Fitting baseline...
2022-04-23 14:19:35 | [train_policy] epoch #57 | Saving snapshot...
2022-04-23 14:19:35 | [train_policy] epoch #57 | Saved
2022-04-23 14:19:35 | [train_policy] epoch #57 | Time 23.51 s
2022-04-23 14:19:35 | [train_policy] epoch #57 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.118049
Evaluation/AverageDiscountedReturn         -51.2217
Evaluation/AverageReturn                   -51.2217
Evaluation/CompletionRate                    0
Evaluation/Iteration                        57
Evaluation/MaxReturn                       -38.5097
Evaluation/MinReturn                      -179.977
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        14.9607
Extras/EpisodeRewardMean                   -51.024
LinearFeatureBaseline/ExplainedVariance      0.717722
PolicyExecTime                               0.0992601
ProcessExecTime                              0.0113294
TotalEnvSteps                            58696
policy/Entropy                               1.7816
policy/KL                                    0.0071933
policy/KLBefore                              0
policy/LossAfter                            -0.0266462
policy/LossBefore                            1.29575e-08
policy/Perplexity                            5.93935
policy/dLoss                                 0.0266462
---------------------------------------  ---------------
2022-04-23 14:19:35 | [train_policy] epoch #58 | Obtaining samples for iteration 58...
2022-04-23 14:19:36 | [train_policy] epoch #58 | Logging diagnostics...
2022-04-23 14:19:36 | [train_policy] epoch #58 | Optimizing policy...
2022-04-23 14:19:36 | [train_policy] epoch #58 | Computing loss before
2022-04-23 14:19:36 | [train_policy] epoch #58 | Computing KL before
2022-04-23 14:19:36 | [train_policy] epoch #58 | Optimizing
2022-04-23 14:19:36 | [train_policy] epoch #58 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:36 | [train_policy] epoch #58 | computing loss before
2022-04-23 14:19:36 | [train_policy] epoch #58 | computing gradient
2022-04-23 14:19:36 | [train_policy] epoch #58 | gradient computed
2022-04-23 14:19:36 | [train_policy] epoch #58 | computing descent direction
2022-04-23 14:19:36 | [train_policy] epoch #58 | descent direction computed
2022-04-23 14:19:36 | [train_policy] epoch #58 | backtrack iters: 1
2022-04-23 14:19:36 | [train_policy] epoch #58 | optimization finished
2022-04-23 14:19:36 | [train_policy] epoch #58 | Computing KL after
2022-04-23 14:19:36 | [train_policy] epoch #58 | Computing loss after
2022-04-23 14:19:36 | [train_policy] epoch #58 | Fitting baseline...
2022-04-23 14:19:36 | [train_policy] epoch #58 | Saving snapshot...
2022-04-23 14:19:36 | [train_policy] epoch #58 | Saved
2022-04-23 14:19:36 | [train_policy] epoch #58 | Time 23.85 s
2022-04-23 14:19:36 | [train_policy] epoch #58 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.117068
Evaluation/AverageDiscountedReturn         -51.0246
Evaluation/AverageReturn                   -51.0246
Evaluation/CompletionRate                    0
Evaluation/Iteration                        58
Evaluation/MaxReturn                       -38.7892
Evaluation/MinReturn                       -97.5687
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         9.02261
Extras/EpisodeRewardMean                   -51.1138
LinearFeatureBaseline/ExplainedVariance      0.857488
PolicyExecTime                               0.0939207
ProcessExecTime                              0.0110424
TotalEnvSteps                            59708
policy/Entropy                               1.72945
policy/KL                                    0.00762428
policy/KLBefore                              0
policy/LossAfter                            -0.0209083
policy/LossBefore                            3.29828e-09
policy/Perplexity                            5.63758
policy/dLoss                                 0.0209083
---------------------------------------  ---------------
2022-04-23 14:19:36 | [train_policy] epoch #59 | Obtaining samples for iteration 59...
2022-04-23 14:19:36 | [train_policy] epoch #59 | Logging diagnostics...
2022-04-23 14:19:36 | [train_policy] epoch #59 | Optimizing policy...
2022-04-23 14:19:36 | [train_policy] epoch #59 | Computing loss before
2022-04-23 14:19:36 | [train_policy] epoch #59 | Computing KL before
2022-04-23 14:19:36 | [train_policy] epoch #59 | Optimizing
2022-04-23 14:19:36 | [train_policy] epoch #59 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:36 | [train_policy] epoch #59 | computing loss before
2022-04-23 14:19:36 | [train_policy] epoch #59 | computing gradient
2022-04-23 14:19:36 | [train_policy] epoch #59 | gradient computed
2022-04-23 14:19:36 | [train_policy] epoch #59 | computing descent direction
2022-04-23 14:19:36 | [train_policy] epoch #59 | descent direction computed
2022-04-23 14:19:36 | [train_policy] epoch #59 | backtrack iters: 2
2022-04-23 14:19:36 | [train_policy] epoch #59 | optimization finished
2022-04-23 14:19:36 | [train_policy] epoch #59 | Computing KL after
2022-04-23 14:19:36 | [train_policy] epoch #59 | Computing loss after
2022-04-23 14:19:36 | [train_policy] epoch #59 | Fitting baseline...
2022-04-23 14:19:36 | [train_policy] epoch #59 | Saving snapshot...
2022-04-23 14:19:36 | [train_policy] epoch #59 | Saved
2022-04-23 14:19:36 | [train_policy] epoch #59 | Time 24.19 s
2022-04-23 14:19:36 | [train_policy] epoch #59 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.116874
Evaluation/AverageDiscountedReturn         -70.5363
Evaluation/AverageReturn                   -70.5363
Evaluation/CompletionRate                    0
Evaluation/Iteration                        59
Evaluation/MaxReturn                       -36.4517
Evaluation/MinReturn                     -2054.62
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.066
Extras/EpisodeRewardMean                   -68.8366
LinearFeatureBaseline/ExplainedVariance      0.0114806
PolicyExecTime                               0.093339
ProcessExecTime                              0.011174
TotalEnvSteps                            60720
policy/Entropy                               1.73148
policy/KL                                    0.00895481
policy/KLBefore                              0
policy/LossAfter                            -0.0234697
policy/LossBefore                           -1.17796e-08
policy/Perplexity                            5.649
policy/dLoss                                 0.0234697
---------------------------------------  ---------------
2022-04-23 14:19:36 | [train_policy] epoch #60 | Obtaining samples for iteration 60...
2022-04-23 14:19:36 | [train_policy] epoch #60 | Logging diagnostics...
2022-04-23 14:19:36 | [train_policy] epoch #60 | Optimizing policy...
2022-04-23 14:19:36 | [train_policy] epoch #60 | Computing loss before
2022-04-23 14:19:36 | [train_policy] epoch #60 | Computing KL before
2022-04-23 14:19:36 | [train_policy] epoch #60 | Optimizing
2022-04-23 14:19:36 | [train_policy] epoch #60 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:36 | [train_policy] epoch #60 | computing loss before
2022-04-23 14:19:36 | [train_policy] epoch #60 | computing gradient
2022-04-23 14:19:36 | [train_policy] epoch #60 | gradient computed
2022-04-23 14:19:36 | [train_policy] epoch #60 | computing descent direction
2022-04-23 14:19:36 | [train_policy] epoch #60 | descent direction computed
2022-04-23 14:19:36 | [train_policy] epoch #60 | backtrack iters: 1
2022-04-23 14:19:36 | [train_policy] epoch #60 | optimization finished
2022-04-23 14:19:36 | [train_policy] epoch #60 | Computing KL after
2022-04-23 14:19:36 | [train_policy] epoch #60 | Computing loss after
2022-04-23 14:19:36 | [train_policy] epoch #60 | Fitting baseline...
2022-04-23 14:19:36 | [train_policy] epoch #60 | Saving snapshot...
2022-04-23 14:19:36 | [train_policy] epoch #60 | Saved
2022-04-23 14:19:36 | [train_policy] epoch #60 | Time 24.52 s
2022-04-23 14:19:36 | [train_policy] epoch #60 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.116474
Evaluation/AverageDiscountedReturn         -77.405
Evaluation/AverageReturn                   -77.405
Evaluation/CompletionRate                    0
Evaluation/Iteration                        60
Evaluation/MaxReturn                       -37.2177
Evaluation/MinReturn                     -2062.01
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       211.258
Extras/EpisodeRewardMean                   -75.1123
LinearFeatureBaseline/ExplainedVariance      0.0500534
PolicyExecTime                               0.0955083
ProcessExecTime                              0.0109296
TotalEnvSteps                            61732
policy/Entropy                               1.73516
policy/KL                                    0.00739621
policy/KLBefore                              0
policy/LossAfter                            -0.0196034
policy/LossBefore                           -4.71183e-10
policy/Perplexity                            5.66986
policy/dLoss                                 0.0196034
---------------------------------------  ---------------
2022-04-23 14:19:36 | [train_policy] epoch #61 | Obtaining samples for iteration 61...
2022-04-23 14:19:37 | [train_policy] epoch #61 | Logging diagnostics...
2022-04-23 14:19:37 | [train_policy] epoch #61 | Optimizing policy...
2022-04-23 14:19:37 | [train_policy] epoch #61 | Computing loss before
2022-04-23 14:19:37 | [train_policy] epoch #61 | Computing KL before
2022-04-23 14:19:37 | [train_policy] epoch #61 | Optimizing
2022-04-23 14:19:37 | [train_policy] epoch #61 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:37 | [train_policy] epoch #61 | computing loss before
2022-04-23 14:19:37 | [train_policy] epoch #61 | computing gradient
2022-04-23 14:19:37 | [train_policy] epoch #61 | gradient computed
2022-04-23 14:19:37 | [train_policy] epoch #61 | computing descent direction
2022-04-23 14:19:37 | [train_policy] epoch #61 | descent direction computed
2022-04-23 14:19:37 | [train_policy] epoch #61 | backtrack iters: 0
2022-04-23 14:19:37 | [train_policy] epoch #61 | optimization finished
2022-04-23 14:19:37 | [train_policy] epoch #61 | Computing KL after
2022-04-23 14:19:37 | [train_policy] epoch #61 | Computing loss after
2022-04-23 14:19:37 | [train_policy] epoch #61 | Fitting baseline...
2022-04-23 14:19:37 | [train_policy] epoch #61 | Saving snapshot...
2022-04-23 14:19:37 | [train_policy] epoch #61 | Saved
2022-04-23 14:19:37 | [train_policy] epoch #61 | Time 24.86 s
2022-04-23 14:19:37 | [train_policy] epoch #61 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.117547
Evaluation/AverageDiscountedReturn         -71.7079
Evaluation/AverageReturn                   -71.7079
Evaluation/CompletionRate                    0
Evaluation/Iteration                        61
Evaluation/MaxReturn                       -35.8592
Evaluation/MinReturn                     -2065.44
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.348
Extras/EpisodeRewardMean                   -69.9091
LinearFeatureBaseline/ExplainedVariance      0.162431
PolicyExecTime                               0.097306
ProcessExecTime                              0.0111585
TotalEnvSteps                            62744
policy/Entropy                               1.71571
policy/KL                                    0.00946527
policy/KLBefore                              0
policy/LossAfter                            -0.0179966
policy/LossBefore                           -2.45015e-08
policy/Perplexity                            5.56063
policy/dLoss                                 0.0179966
---------------------------------------  ---------------
2022-04-23 14:19:37 | [train_policy] epoch #62 | Obtaining samples for iteration 62...
2022-04-23 14:19:37 | [train_policy] epoch #62 | Logging diagnostics...
2022-04-23 14:19:37 | [train_policy] epoch #62 | Optimizing policy...
2022-04-23 14:19:37 | [train_policy] epoch #62 | Computing loss before
2022-04-23 14:19:37 | [train_policy] epoch #62 | Computing KL before
2022-04-23 14:19:37 | [train_policy] epoch #62 | Optimizing
2022-04-23 14:19:37 | [train_policy] epoch #62 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:37 | [train_policy] epoch #62 | computing loss before
2022-04-23 14:19:37 | [train_policy] epoch #62 | computing gradient
2022-04-23 14:19:37 | [train_policy] epoch #62 | gradient computed
2022-04-23 14:19:37 | [train_policy] epoch #62 | computing descent direction
2022-04-23 14:19:37 | [train_policy] epoch #62 | descent direction computed
2022-04-23 14:19:37 | [train_policy] epoch #62 | backtrack iters: 0
2022-04-23 14:19:37 | [train_policy] epoch #62 | optimization finished
2022-04-23 14:19:37 | [train_policy] epoch #62 | Computing KL after
2022-04-23 14:19:37 | [train_policy] epoch #62 | Computing loss after
2022-04-23 14:19:37 | [train_policy] epoch #62 | Fitting baseline...
2022-04-23 14:19:37 | [train_policy] epoch #62 | Saving snapshot...
2022-04-23 14:19:37 | [train_policy] epoch #62 | Saved
2022-04-23 14:19:37 | [train_policy] epoch #62 | Time 25.20 s
2022-04-23 14:19:37 | [train_policy] epoch #62 | EpochTime 0.34 s
---------------------------------------  --------------
EnvExecTime                                  0.1175
Evaluation/AverageDiscountedReturn         -49.8469
Evaluation/AverageReturn                   -49.8469
Evaluation/CompletionRate                    0
Evaluation/Iteration                        62
Evaluation/MaxReturn                       -37.0247
Evaluation/MinReturn                      -150.231
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        11.9774
Extras/EpisodeRewardMean                   -49.2451
LinearFeatureBaseline/ExplainedVariance    -20.9922
PolicyExecTime                               0.0955427
ProcessExecTime                              0.0110583
TotalEnvSteps                            63756
policy/Entropy                               1.71112
policy/KL                                    0.0096136
policy/KLBefore                              0
policy/LossAfter                            -0.0232387
policy/LossBefore                            1.1544e-08
policy/Perplexity                            5.53514
policy/dLoss                                 0.0232387
---------------------------------------  --------------
2022-04-23 14:19:37 | [train_policy] epoch #63 | Obtaining samples for iteration 63...
2022-04-23 14:19:37 | [train_policy] epoch #63 | Logging diagnostics...
2022-04-23 14:19:37 | [train_policy] epoch #63 | Optimizing policy...
2022-04-23 14:19:37 | [train_policy] epoch #63 | Computing loss before
2022-04-23 14:19:37 | [train_policy] epoch #63 | Computing KL before
2022-04-23 14:19:37 | [train_policy] epoch #63 | Optimizing
2022-04-23 14:19:37 | [train_policy] epoch #63 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:37 | [train_policy] epoch #63 | computing loss before
2022-04-23 14:19:37 | [train_policy] epoch #63 | computing gradient
2022-04-23 14:19:37 | [train_policy] epoch #63 | gradient computed
2022-04-23 14:19:37 | [train_policy] epoch #63 | computing descent direction
2022-04-23 14:19:37 | [train_policy] epoch #63 | descent direction computed
2022-04-23 14:19:37 | [train_policy] epoch #63 | backtrack iters: 1
2022-04-23 14:19:37 | [train_policy] epoch #63 | optimization finished
2022-04-23 14:19:37 | [train_policy] epoch #63 | Computing KL after
2022-04-23 14:19:37 | [train_policy] epoch #63 | Computing loss after
2022-04-23 14:19:37 | [train_policy] epoch #63 | Fitting baseline...
2022-04-23 14:19:37 | [train_policy] epoch #63 | Saving snapshot...
2022-04-23 14:19:37 | [train_policy] epoch #63 | Saved
2022-04-23 14:19:37 | [train_policy] epoch #63 | Time 25.54 s
2022-04-23 14:19:37 | [train_policy] epoch #63 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.117821
Evaluation/AverageDiscountedReturn         -50.4408
Evaluation/AverageReturn                   -50.4408
Evaluation/CompletionRate                    0
Evaluation/Iteration                        63
Evaluation/MaxReturn                       -38.2105
Evaluation/MinReturn                       -93.3883
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.43159
Extras/EpisodeRewardMean                   -50.2173
LinearFeatureBaseline/ExplainedVariance      0.924963
PolicyExecTime                               0.0985711
ProcessExecTime                              0.0115957
TotalEnvSteps                            64768
policy/Entropy                               1.67008
policy/KL                                    0.00650148
policy/KLBefore                              0
policy/LossAfter                            -0.0170583
policy/LossBefore                            7.30334e-09
policy/Perplexity                            5.31257
policy/dLoss                                 0.0170583
---------------------------------------  ---------------
2022-04-23 14:19:37 | [train_policy] epoch #64 | Obtaining samples for iteration 64...
2022-04-23 14:19:38 | [train_policy] epoch #64 | Logging diagnostics...
2022-04-23 14:19:38 | [train_policy] epoch #64 | Optimizing policy...
2022-04-23 14:19:38 | [train_policy] epoch #64 | Computing loss before
2022-04-23 14:19:38 | [train_policy] epoch #64 | Computing KL before
2022-04-23 14:19:38 | [train_policy] epoch #64 | Optimizing
2022-04-23 14:19:38 | [train_policy] epoch #64 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:38 | [train_policy] epoch #64 | computing loss before
2022-04-23 14:19:38 | [train_policy] epoch #64 | computing gradient
2022-04-23 14:19:38 | [train_policy] epoch #64 | gradient computed
2022-04-23 14:19:38 | [train_policy] epoch #64 | computing descent direction
2022-04-23 14:19:38 | [train_policy] epoch #64 | descent direction computed
2022-04-23 14:19:38 | [train_policy] epoch #64 | backtrack iters: 1
2022-04-23 14:19:38 | [train_policy] epoch #64 | optimization finished
2022-04-23 14:19:38 | [train_policy] epoch #64 | Computing KL after
2022-04-23 14:19:38 | [train_policy] epoch #64 | Computing loss after
2022-04-23 14:19:38 | [train_policy] epoch #64 | Fitting baseline...
2022-04-23 14:19:38 | [train_policy] epoch #64 | Saving snapshot...
2022-04-23 14:19:38 | [train_policy] epoch #64 | Saved
2022-04-23 14:19:38 | [train_policy] epoch #64 | Time 25.87 s
2022-04-23 14:19:38 | [train_policy] epoch #64 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                  0.116423
Evaluation/AverageDiscountedReturn         -73.6508
Evaluation/AverageReturn                   -73.6508
Evaluation/CompletionRate                    0
Evaluation/Iteration                        64
Evaluation/MaxReturn                       -36.7382
Evaluation/MinReturn                     -2039.73
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       207.319
Extras/EpisodeRewardMean                   -71.9449
LinearFeatureBaseline/ExplainedVariance      0.013642
PolicyExecTime                               0.09113
ProcessExecTime                              0.0109656
TotalEnvSteps                            65780
policy/Entropy                               1.60531
policy/KL                                    0.00647723
policy/KLBefore                              0
policy/LossAfter                            -0.0162123
policy/LossBefore                            8.95248e-09
policy/Perplexity                            4.9794
policy/dLoss                                 0.0162123
---------------------------------------  ---------------
2022-04-23 14:19:38 | [train_policy] epoch #65 | Obtaining samples for iteration 65...
2022-04-23 14:19:38 | [train_policy] epoch #65 | Logging diagnostics...
2022-04-23 14:19:38 | [train_policy] epoch #65 | Optimizing policy...
2022-04-23 14:19:38 | [train_policy] epoch #65 | Computing loss before
2022-04-23 14:19:38 | [train_policy] epoch #65 | Computing KL before
2022-04-23 14:19:38 | [train_policy] epoch #65 | Optimizing
2022-04-23 14:19:38 | [train_policy] epoch #65 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:38 | [train_policy] epoch #65 | computing loss before
2022-04-23 14:19:38 | [train_policy] epoch #65 | computing gradient
2022-04-23 14:19:38 | [train_policy] epoch #65 | gradient computed
2022-04-23 14:19:38 | [train_policy] epoch #65 | computing descent direction
2022-04-23 14:19:38 | [train_policy] epoch #65 | descent direction computed
2022-04-23 14:19:38 | [train_policy] epoch #65 | backtrack iters: 1
2022-04-23 14:19:38 | [train_policy] epoch #65 | optimization finished
2022-04-23 14:19:38 | [train_policy] epoch #65 | Computing KL after
2022-04-23 14:19:38 | [train_policy] epoch #65 | Computing loss after
2022-04-23 14:19:38 | [train_policy] epoch #65 | Fitting baseline...
2022-04-23 14:19:38 | [train_policy] epoch #65 | Saving snapshot...
2022-04-23 14:19:38 | [train_policy] epoch #65 | Saved
2022-04-23 14:19:38 | [train_policy] epoch #65 | Time 26.19 s
2022-04-23 14:19:38 | [train_policy] epoch #65 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                  0.115845
Evaluation/AverageDiscountedReturn         -73.8555
Evaluation/AverageReturn                   -73.8555
Evaluation/CompletionRate                    0
Evaluation/Iteration                        65
Evaluation/MaxReturn                       -35.7358
Evaluation/MinReturn                     -2067.05
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       210.971
Extras/EpisodeRewardMean                   -91.9547
LinearFeatureBaseline/ExplainedVariance     -0.065836
PolicyExecTime                               0.0877442
ProcessExecTime                              0.0108247
TotalEnvSteps                            66792
policy/Entropy                               1.61974
policy/KL                                    0.00669691
policy/KLBefore                              0
policy/LossAfter                            -0.0211542
policy/LossBefore                           -3.76946e-09
policy/Perplexity                            5.05175
policy/dLoss                                 0.0211542
---------------------------------------  ---------------
2022-04-23 14:19:38 | [train_policy] epoch #66 | Obtaining samples for iteration 66...
2022-04-23 14:19:38 | [train_policy] epoch #66 | Logging diagnostics...
2022-04-23 14:19:38 | [train_policy] epoch #66 | Optimizing policy...
2022-04-23 14:19:38 | [train_policy] epoch #66 | Computing loss before
2022-04-23 14:19:38 | [train_policy] epoch #66 | Computing KL before
2022-04-23 14:19:38 | [train_policy] epoch #66 | Optimizing
2022-04-23 14:19:38 | [train_policy] epoch #66 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:38 | [train_policy] epoch #66 | computing loss before
2022-04-23 14:19:38 | [train_policy] epoch #66 | computing gradient
2022-04-23 14:19:38 | [train_policy] epoch #66 | gradient computed
2022-04-23 14:19:38 | [train_policy] epoch #66 | computing descent direction
2022-04-23 14:19:38 | [train_policy] epoch #66 | descent direction computed
2022-04-23 14:19:38 | [train_policy] epoch #66 | backtrack iters: 1
2022-04-23 14:19:38 | [train_policy] epoch #66 | optimization finished
2022-04-23 14:19:38 | [train_policy] epoch #66 | Computing KL after
2022-04-23 14:19:38 | [train_policy] epoch #66 | Computing loss after
2022-04-23 14:19:38 | [train_policy] epoch #66 | Fitting baseline...
2022-04-23 14:19:38 | [train_policy] epoch #66 | Saving snapshot...
2022-04-23 14:19:38 | [train_policy] epoch #66 | Saved
2022-04-23 14:19:38 | [train_policy] epoch #66 | Time 26.52 s
2022-04-23 14:19:38 | [train_policy] epoch #66 | EpochTime 0.32 s
---------------------------------------  --------------
EnvExecTime                                  0.116418
Evaluation/AverageDiscountedReturn         -53.669
Evaluation/AverageReturn                   -53.669
Evaluation/CompletionRate                    0
Evaluation/Iteration                        66
Evaluation/MaxReturn                       -38.4234
Evaluation/MinReturn                      -208.308
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        23.1841
Extras/EpisodeRewardMean                   -53.3087
LinearFeatureBaseline/ExplainedVariance     -7.61921
PolicyExecTime                               0.0911219
ProcessExecTime                              0.0108976
TotalEnvSteps                            67804
policy/Entropy                               1.61944
policy/KL                                    0.0074926
policy/KLBefore                              0
policy/LossAfter                            -0.0437981
policy/LossBefore                            1.5549e-08
policy/Perplexity                            5.05028
policy/dLoss                                 0.0437981
---------------------------------------  --------------
2022-04-23 14:19:38 | [train_policy] epoch #67 | Obtaining samples for iteration 67...
2022-04-23 14:19:39 | [train_policy] epoch #67 | Logging diagnostics...
2022-04-23 14:19:39 | [train_policy] epoch #67 | Optimizing policy...
2022-04-23 14:19:39 | [train_policy] epoch #67 | Computing loss before
2022-04-23 14:19:39 | [train_policy] epoch #67 | Computing KL before
2022-04-23 14:19:39 | [train_policy] epoch #67 | Optimizing
2022-04-23 14:19:39 | [train_policy] epoch #67 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:39 | [train_policy] epoch #67 | computing loss before
2022-04-23 14:19:39 | [train_policy] epoch #67 | computing gradient
2022-04-23 14:19:39 | [train_policy] epoch #67 | gradient computed
2022-04-23 14:19:39 | [train_policy] epoch #67 | computing descent direction
2022-04-23 14:19:39 | [train_policy] epoch #67 | descent direction computed
2022-04-23 14:19:39 | [train_policy] epoch #67 | backtrack iters: 1
2022-04-23 14:19:39 | [train_policy] epoch #67 | optimization finished
2022-04-23 14:19:39 | [train_policy] epoch #67 | Computing KL after
2022-04-23 14:19:39 | [train_policy] epoch #67 | Computing loss after
2022-04-23 14:19:39 | [train_policy] epoch #67 | Fitting baseline...
2022-04-23 14:19:39 | [train_policy] epoch #67 | Saving snapshot...
2022-04-23 14:19:39 | [train_policy] epoch #67 | Saved
2022-04-23 14:19:39 | [train_policy] epoch #67 | Time 26.85 s
2022-04-23 14:19:39 | [train_policy] epoch #67 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                  0.117171
Evaluation/AverageDiscountedReturn         -50.0366
Evaluation/AverageReturn                   -50.0366
Evaluation/CompletionRate                    0
Evaluation/Iteration                        67
Evaluation/MaxReturn                       -37.9509
Evaluation/MinReturn                      -102.755
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.29148
Extras/EpisodeRewardMean                   -50.1013
LinearFeatureBaseline/ExplainedVariance      0.639819
PolicyExecTime                               0.087395
ProcessExecTime                              0.0107601
TotalEnvSteps                            68816
policy/Entropy                               1.59253
policy/KL                                    0.00659087
policy/KLBefore                              0
policy/LossAfter                            -0.0126516
policy/LossBefore                           -2.26168e-08
policy/Perplexity                            4.91619
policy/dLoss                                 0.0126516
---------------------------------------  ---------------
2022-04-23 14:19:39 | [train_policy] epoch #68 | Obtaining samples for iteration 68...
2022-04-23 14:19:39 | [train_policy] epoch #68 | Logging diagnostics...
2022-04-23 14:19:39 | [train_policy] epoch #68 | Optimizing policy...
2022-04-23 14:19:39 | [train_policy] epoch #68 | Computing loss before
2022-04-23 14:19:39 | [train_policy] epoch #68 | Computing KL before
2022-04-23 14:19:39 | [train_policy] epoch #68 | Optimizing
2022-04-23 14:19:39 | [train_policy] epoch #68 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:39 | [train_policy] epoch #68 | computing loss before
2022-04-23 14:19:39 | [train_policy] epoch #68 | computing gradient
2022-04-23 14:19:39 | [train_policy] epoch #68 | gradient computed
2022-04-23 14:19:39 | [train_policy] epoch #68 | computing descent direction
2022-04-23 14:19:39 | [train_policy] epoch #68 | descent direction computed
2022-04-23 14:19:39 | [train_policy] epoch #68 | backtrack iters: 0
2022-04-23 14:19:39 | [train_policy] epoch #68 | optimization finished
2022-04-23 14:19:39 | [train_policy] epoch #68 | Computing KL after
2022-04-23 14:19:39 | [train_policy] epoch #68 | Computing loss after
2022-04-23 14:19:39 | [train_policy] epoch #68 | Fitting baseline...
2022-04-23 14:19:39 | [train_policy] epoch #68 | Saving snapshot...
2022-04-23 14:19:39 | [train_policy] epoch #68 | Saved
2022-04-23 14:19:39 | [train_policy] epoch #68 | Time 27.18 s
2022-04-23 14:19:39 | [train_policy] epoch #68 | EpochTime 0.32 s
---------------------------------------  --------------
EnvExecTime                                  0.116041
Evaluation/AverageDiscountedReturn         -54.2585
Evaluation/AverageReturn                   -54.2585
Evaluation/CompletionRate                    0
Evaluation/Iteration                        68
Evaluation/MaxReturn                       -35.9566
Evaluation/MinReturn                      -360.098
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        35.1992
Extras/EpisodeRewardMean                   -53.8867
LinearFeatureBaseline/ExplainedVariance      0.427991
PolicyExecTime                               0.0890079
ProcessExecTime                              0.0109179
TotalEnvSteps                            69828
policy/Entropy                               1.52646
policy/KL                                    0.00951907
policy/KLBefore                              0
policy/LossAfter                            -0.0251671
policy/LossBefore                           -5.4186e-09
policy/Perplexity                            4.60188
policy/dLoss                                 0.0251671
---------------------------------------  --------------
2022-04-23 14:19:39 | [train_policy] epoch #69 | Obtaining samples for iteration 69...
2022-04-23 14:19:39 | [train_policy] epoch #69 | Logging diagnostics...
2022-04-23 14:19:39 | [train_policy] epoch #69 | Optimizing policy...
2022-04-23 14:19:39 | [train_policy] epoch #69 | Computing loss before
2022-04-23 14:19:39 | [train_policy] epoch #69 | Computing KL before
2022-04-23 14:19:39 | [train_policy] epoch #69 | Optimizing
2022-04-23 14:19:39 | [train_policy] epoch #69 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:39 | [train_policy] epoch #69 | computing loss before
2022-04-23 14:19:39 | [train_policy] epoch #69 | computing gradient
2022-04-23 14:19:39 | [train_policy] epoch #69 | gradient computed
2022-04-23 14:19:39 | [train_policy] epoch #69 | computing descent direction
2022-04-23 14:19:39 | [train_policy] epoch #69 | descent direction computed
2022-04-23 14:19:39 | [train_policy] epoch #69 | backtrack iters: 1
2022-04-23 14:19:39 | [train_policy] epoch #69 | optimization finished
2022-04-23 14:19:39 | [train_policy] epoch #69 | Computing KL after
2022-04-23 14:19:39 | [train_policy] epoch #69 | Computing loss after
2022-04-23 14:19:39 | [train_policy] epoch #69 | Fitting baseline...
2022-04-23 14:19:39 | [train_policy] epoch #69 | Saving snapshot...
2022-04-23 14:19:39 | [train_policy] epoch #69 | Saved
2022-04-23 14:19:39 | [train_policy] epoch #69 | Time 27.52 s
2022-04-23 14:19:39 | [train_policy] epoch #69 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.118151
Evaluation/AverageDiscountedReturn         -48.7626
Evaluation/AverageReturn                   -48.7626
Evaluation/CompletionRate                    0
Evaluation/Iteration                        69
Evaluation/MaxReturn                       -36.801
Evaluation/MinReturn                       -81.1941
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.72912
Extras/EpisodeRewardMean                   -50.014
LinearFeatureBaseline/ExplainedVariance      0.879068
PolicyExecTime                               0.0976553
ProcessExecTime                              0.0112748
TotalEnvSteps                            70840
policy/Entropy                               1.51734
policy/KL                                    0.00982385
policy/KLBefore                              0
policy/LossAfter                            -0.0190024
policy/LossBefore                           -1.71982e-08
policy/Perplexity                            4.56009
policy/dLoss                                 0.0190024
---------------------------------------  ---------------
2022-04-23 14:19:39 | [train_policy] epoch #70 | Obtaining samples for iteration 70...
2022-04-23 14:19:40 | [train_policy] epoch #70 | Logging diagnostics...
2022-04-23 14:19:40 | [train_policy] epoch #70 | Optimizing policy...
2022-04-23 14:19:40 | [train_policy] epoch #70 | Computing loss before
2022-04-23 14:19:40 | [train_policy] epoch #70 | Computing KL before
2022-04-23 14:19:40 | [train_policy] epoch #70 | Optimizing
2022-04-23 14:19:40 | [train_policy] epoch #70 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:40 | [train_policy] epoch #70 | computing loss before
2022-04-23 14:19:40 | [train_policy] epoch #70 | computing gradient
2022-04-23 14:19:40 | [train_policy] epoch #70 | gradient computed
2022-04-23 14:19:40 | [train_policy] epoch #70 | computing descent direction
2022-04-23 14:19:40 | [train_policy] epoch #70 | descent direction computed
2022-04-23 14:19:40 | [train_policy] epoch #70 | backtrack iters: 0
2022-04-23 14:19:40 | [train_policy] epoch #70 | optimization finished
2022-04-23 14:19:40 | [train_policy] epoch #70 | Computing KL after
2022-04-23 14:19:40 | [train_policy] epoch #70 | Computing loss after
2022-04-23 14:19:40 | [train_policy] epoch #70 | Fitting baseline...
2022-04-23 14:19:40 | [train_policy] epoch #70 | Saving snapshot...
2022-04-23 14:19:40 | [train_policy] epoch #70 | Saved
2022-04-23 14:19:40 | [train_policy] epoch #70 | Time 27.86 s
2022-04-23 14:19:40 | [train_policy] epoch #70 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.116801
Evaluation/AverageDiscountedReturn         -60.4071
Evaluation/AverageReturn                   -60.4071
Evaluation/CompletionRate                    0
Evaluation/Iteration                        70
Evaluation/MaxReturn                       -38.4213
Evaluation/MinReturn                     -1108.5
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       110.024
Extras/EpisodeRewardMean                   -59.4936
LinearFeatureBaseline/ExplainedVariance      0.0217686
PolicyExecTime                               0.101182
ProcessExecTime                              0.0112143
TotalEnvSteps                            71852
policy/Entropy                               1.462
policy/KL                                    0.00967837
policy/KLBefore                              0
policy/LossAfter                            -0.0265818
policy/LossBefore                            3.76946e-09
policy/Perplexity                            4.31458
policy/dLoss                                 0.0265818
---------------------------------------  ---------------
2022-04-23 14:19:40 | [train_policy] epoch #71 | Obtaining samples for iteration 71...
2022-04-23 14:19:40 | [train_policy] epoch #71 | Logging diagnostics...
2022-04-23 14:19:40 | [train_policy] epoch #71 | Optimizing policy...
2022-04-23 14:19:40 | [train_policy] epoch #71 | Computing loss before
2022-04-23 14:19:40 | [train_policy] epoch #71 | Computing KL before
2022-04-23 14:19:40 | [train_policy] epoch #71 | Optimizing
2022-04-23 14:19:40 | [train_policy] epoch #71 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:40 | [train_policy] epoch #71 | computing loss before
2022-04-23 14:19:40 | [train_policy] epoch #71 | computing gradient
2022-04-23 14:19:40 | [train_policy] epoch #71 | gradient computed
2022-04-23 14:19:40 | [train_policy] epoch #71 | computing descent direction
2022-04-23 14:19:40 | [train_policy] epoch #71 | descent direction computed
2022-04-23 14:19:40 | [train_policy] epoch #71 | backtrack iters: 0
2022-04-23 14:19:40 | [train_policy] epoch #71 | optimization finished
2022-04-23 14:19:40 | [train_policy] epoch #71 | Computing KL after
2022-04-23 14:19:40 | [train_policy] epoch #71 | Computing loss after
2022-04-23 14:19:40 | [train_policy] epoch #71 | Fitting baseline...
2022-04-23 14:19:40 | [train_policy] epoch #71 | Saving snapshot...
2022-04-23 14:19:40 | [train_policy] epoch #71 | Saved
2022-04-23 14:19:40 | [train_policy] epoch #71 | Time 28.19 s
2022-04-23 14:19:40 | [train_policy] epoch #71 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.117996
Evaluation/AverageDiscountedReturn         -49.7558
Evaluation/AverageReturn                   -49.7558
Evaluation/CompletionRate                    0
Evaluation/Iteration                        71
Evaluation/MaxReturn                       -38.7267
Evaluation/MinReturn                       -83.0804
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.13863
Extras/EpisodeRewardMean                   -49.3212
LinearFeatureBaseline/ExplainedVariance     -1.98664
PolicyExecTime                               0.0917854
ProcessExecTime                              0.0110166
TotalEnvSteps                            72864
policy/Entropy                               1.4436
policy/KL                                    0.00935479
policy/KLBefore                              0
policy/LossAfter                            -0.0174196
policy/LossBefore                           -7.06774e-09
policy/Perplexity                            4.23591
policy/dLoss                                 0.0174196
---------------------------------------  ---------------
2022-04-23 14:19:40 | [train_policy] epoch #72 | Obtaining samples for iteration 72...
2022-04-23 14:19:40 | [train_policy] epoch #72 | Logging diagnostics...
2022-04-23 14:19:40 | [train_policy] epoch #72 | Optimizing policy...
2022-04-23 14:19:40 | [train_policy] epoch #72 | Computing loss before
2022-04-23 14:19:40 | [train_policy] epoch #72 | Computing KL before
2022-04-23 14:19:40 | [train_policy] epoch #72 | Optimizing
2022-04-23 14:19:40 | [train_policy] epoch #72 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:40 | [train_policy] epoch #72 | computing loss before
2022-04-23 14:19:40 | [train_policy] epoch #72 | computing gradient
2022-04-23 14:19:40 | [train_policy] epoch #72 | gradient computed
2022-04-23 14:19:40 | [train_policy] epoch #72 | computing descent direction
2022-04-23 14:19:40 | [train_policy] epoch #72 | descent direction computed
2022-04-23 14:19:40 | [train_policy] epoch #72 | backtrack iters: 1
2022-04-23 14:19:40 | [train_policy] epoch #72 | optimization finished
2022-04-23 14:19:40 | [train_policy] epoch #72 | Computing KL after
2022-04-23 14:19:40 | [train_policy] epoch #72 | Computing loss after
2022-04-23 14:19:40 | [train_policy] epoch #72 | Fitting baseline...
2022-04-23 14:19:40 | [train_policy] epoch #72 | Saving snapshot...
2022-04-23 14:19:40 | [train_policy] epoch #72 | Saved
2022-04-23 14:19:40 | [train_policy] epoch #72 | Time 28.52 s
2022-04-23 14:19:40 | [train_policy] epoch #72 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                  0.115862
Evaluation/AverageDiscountedReturn         -50.3454
Evaluation/AverageReturn                   -50.3454
Evaluation/CompletionRate                    0
Evaluation/Iteration                        72
Evaluation/MaxReturn                       -38.0272
Evaluation/MinReturn                       -82.9816
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.44192
Extras/EpisodeRewardMean                   -50.0791
LinearFeatureBaseline/ExplainedVariance      0.92549
PolicyExecTime                               0.0923648
ProcessExecTime                              0.0109677
TotalEnvSteps                            73876
policy/Entropy                               1.40521
policy/KL                                    0.00687472
policy/KLBefore                              0
policy/LossAfter                            -0.0222379
policy/LossBefore                            1.48423e-08
policy/Perplexity                            4.07637
policy/dLoss                                 0.0222379
---------------------------------------  ---------------
2022-04-23 14:19:40 | [train_policy] epoch #73 | Obtaining samples for iteration 73...
2022-04-23 14:19:41 | [train_policy] epoch #73 | Logging diagnostics...
2022-04-23 14:19:41 | [train_policy] epoch #73 | Optimizing policy...
2022-04-23 14:19:41 | [train_policy] epoch #73 | Computing loss before
2022-04-23 14:19:41 | [train_policy] epoch #73 | Computing KL before
2022-04-23 14:19:41 | [train_policy] epoch #73 | Optimizing
2022-04-23 14:19:41 | [train_policy] epoch #73 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:41 | [train_policy] epoch #73 | computing loss before
2022-04-23 14:19:41 | [train_policy] epoch #73 | computing gradient
2022-04-23 14:19:41 | [train_policy] epoch #73 | gradient computed
2022-04-23 14:19:41 | [train_policy] epoch #73 | computing descent direction
2022-04-23 14:19:41 | [train_policy] epoch #73 | descent direction computed
2022-04-23 14:19:41 | [train_policy] epoch #73 | backtrack iters: 1
2022-04-23 14:19:41 | [train_policy] epoch #73 | optimization finished
2022-04-23 14:19:41 | [train_policy] epoch #73 | Computing KL after
2022-04-23 14:19:41 | [train_policy] epoch #73 | Computing loss after
2022-04-23 14:19:41 | [train_policy] epoch #73 | Fitting baseline...
2022-04-23 14:19:41 | [train_policy] epoch #73 | Saving snapshot...
2022-04-23 14:19:41 | [train_policy] epoch #73 | Saved
2022-04-23 14:19:41 | [train_policy] epoch #73 | Time 28.85 s
2022-04-23 14:19:41 | [train_policy] epoch #73 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.11583
Evaluation/AverageDiscountedReturn         -61.8677
Evaluation/AverageReturn                   -61.8677
Evaluation/CompletionRate                    0
Evaluation/Iteration                        73
Evaluation/MaxReturn                       -38.9161
Evaluation/MinReturn                     -1227.08
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       122.427
Extras/EpisodeRewardMean                   -60.9123
LinearFeatureBaseline/ExplainedVariance      0.0269787
PolicyExecTime                               0.0918517
ProcessExecTime                              0.01089
TotalEnvSteps                            74888
policy/Entropy                               1.37478
policy/KL                                    0.00757071
policy/KLBefore                              0
policy/LossAfter                            -0.0142248
policy/LossBefore                           -1.46067e-08
policy/Perplexity                            3.95422
policy/dLoss                                 0.0142248
---------------------------------------  ---------------
2022-04-23 14:19:41 | [train_policy] epoch #74 | Obtaining samples for iteration 74...
2022-04-23 14:19:41 | [train_policy] epoch #74 | Logging diagnostics...
2022-04-23 14:19:41 | [train_policy] epoch #74 | Optimizing policy...
2022-04-23 14:19:41 | [train_policy] epoch #74 | Computing loss before
2022-04-23 14:19:41 | [train_policy] epoch #74 | Computing KL before
2022-04-23 14:19:41 | [train_policy] epoch #74 | Optimizing
2022-04-23 14:19:41 | [train_policy] epoch #74 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:41 | [train_policy] epoch #74 | computing loss before
2022-04-23 14:19:41 | [train_policy] epoch #74 | computing gradient
2022-04-23 14:19:41 | [train_policy] epoch #74 | gradient computed
2022-04-23 14:19:41 | [train_policy] epoch #74 | computing descent direction
2022-04-23 14:19:41 | [train_policy] epoch #74 | descent direction computed
2022-04-23 14:19:41 | [train_policy] epoch #74 | backtrack iters: 1
2022-04-23 14:19:41 | [train_policy] epoch #74 | optimization finished
2022-04-23 14:19:41 | [train_policy] epoch #74 | Computing KL after
2022-04-23 14:19:41 | [train_policy] epoch #74 | Computing loss after
2022-04-23 14:19:41 | [train_policy] epoch #74 | Fitting baseline...
2022-04-23 14:19:41 | [train_policy] epoch #74 | Saving snapshot...
2022-04-23 14:19:41 | [train_policy] epoch #74 | Saved
2022-04-23 14:19:41 | [train_policy] epoch #74 | Time 29.19 s
2022-04-23 14:19:41 | [train_policy] epoch #74 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.11782
Evaluation/AverageDiscountedReturn         -48.2746
Evaluation/AverageReturn                   -48.2746
Evaluation/CompletionRate                    0
Evaluation/Iteration                        74
Evaluation/MaxReturn                       -37.0307
Evaluation/MinReturn                       -79.6828
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.58159
Extras/EpisodeRewardMean                   -48.1016
LinearFeatureBaseline/ExplainedVariance    -14.0083
PolicyExecTime                               0.0935881
ProcessExecTime                              0.0117331
TotalEnvSteps                            75900
policy/Entropy                               1.35626
policy/KL                                    0.00711192
policy/KLBefore                              0
policy/LossAfter                            -0.0236869
policy/LossBefore                            3.08625e-08
policy/Perplexity                            3.88163
policy/dLoss                                 0.023687
---------------------------------------  ---------------
2022-04-23 14:19:41 | [train_policy] epoch #75 | Obtaining samples for iteration 75...
2022-04-23 14:19:41 | [train_policy] epoch #75 | Logging diagnostics...
2022-04-23 14:19:41 | [train_policy] epoch #75 | Optimizing policy...
2022-04-23 14:19:41 | [train_policy] epoch #75 | Computing loss before
2022-04-23 14:19:41 | [train_policy] epoch #75 | Computing KL before
2022-04-23 14:19:41 | [train_policy] epoch #75 | Optimizing
2022-04-23 14:19:41 | [train_policy] epoch #75 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:41 | [train_policy] epoch #75 | computing loss before
2022-04-23 14:19:41 | [train_policy] epoch #75 | computing gradient
2022-04-23 14:19:41 | [train_policy] epoch #75 | gradient computed
2022-04-23 14:19:41 | [train_policy] epoch #75 | computing descent direction
2022-04-23 14:19:41 | [train_policy] epoch #75 | descent direction computed
2022-04-23 14:19:41 | [train_policy] epoch #75 | backtrack iters: 1
2022-04-23 14:19:41 | [train_policy] epoch #75 | optimization finished
2022-04-23 14:19:41 | [train_policy] epoch #75 | Computing KL after
2022-04-23 14:19:41 | [train_policy] epoch #75 | Computing loss after
2022-04-23 14:19:41 | [train_policy] epoch #75 | Fitting baseline...
2022-04-23 14:19:41 | [train_policy] epoch #75 | Saving snapshot...
2022-04-23 14:19:41 | [train_policy] epoch #75 | Saved
2022-04-23 14:19:41 | [train_policy] epoch #75 | Time 29.53 s
2022-04-23 14:19:41 | [train_policy] epoch #75 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.116513
Evaluation/AverageDiscountedReturn         -50.4233
Evaluation/AverageReturn                   -50.4233
Evaluation/CompletionRate                    0
Evaluation/Iteration                        75
Evaluation/MaxReturn                       -38.6884
Evaluation/MinReturn                       -80.4017
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.05882
Extras/EpisodeRewardMean                   -50.3735
LinearFeatureBaseline/ExplainedVariance      0.922133
PolicyExecTime                               0.0929158
ProcessExecTime                              0.0109227
TotalEnvSteps                            76912
policy/Entropy                               1.34848
policy/KL                                    0.0065809
policy/KLBefore                              0
policy/LossAfter                            -0.0184682
policy/LossBefore                           -8.48129e-09
policy/Perplexity                            3.85157
policy/dLoss                                 0.0184682
---------------------------------------  ---------------
2022-04-23 14:19:41 | [train_policy] epoch #76 | Obtaining samples for iteration 76...
2022-04-23 14:19:42 | [train_policy] epoch #76 | Logging diagnostics...
2022-04-23 14:19:42 | [train_policy] epoch #76 | Optimizing policy...
2022-04-23 14:19:42 | [train_policy] epoch #76 | Computing loss before
2022-04-23 14:19:42 | [train_policy] epoch #76 | Computing KL before
2022-04-23 14:19:42 | [train_policy] epoch #76 | Optimizing
2022-04-23 14:19:42 | [train_policy] epoch #76 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:42 | [train_policy] epoch #76 | computing loss before
2022-04-23 14:19:42 | [train_policy] epoch #76 | computing gradient
2022-04-23 14:19:42 | [train_policy] epoch #76 | gradient computed
2022-04-23 14:19:42 | [train_policy] epoch #76 | computing descent direction
2022-04-23 14:19:42 | [train_policy] epoch #76 | descent direction computed
2022-04-23 14:19:42 | [train_policy] epoch #76 | backtrack iters: 1
2022-04-23 14:19:42 | [train_policy] epoch #76 | optimization finished
2022-04-23 14:19:42 | [train_policy] epoch #76 | Computing KL after
2022-04-23 14:19:42 | [train_policy] epoch #76 | Computing loss after
2022-04-23 14:19:42 | [train_policy] epoch #76 | Fitting baseline...
2022-04-23 14:19:42 | [train_policy] epoch #76 | Saving snapshot...
2022-04-23 14:19:42 | [train_policy] epoch #76 | Saved
2022-04-23 14:19:42 | [train_policy] epoch #76 | Time 29.86 s
2022-04-23 14:19:42 | [train_policy] epoch #76 | EpochTime 0.33 s
---------------------------------------  --------------
EnvExecTime                                  0.116472
Evaluation/AverageDiscountedReturn         -48.0592
Evaluation/AverageReturn                   -48.0592
Evaluation/CompletionRate                    0
Evaluation/Iteration                        76
Evaluation/MaxReturn                       -37.4936
Evaluation/MinReturn                       -65.6148
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.15936
Extras/EpisodeRewardMean                   -48.1351
LinearFeatureBaseline/ExplainedVariance      0.936682
PolicyExecTime                               0.0944953
ProcessExecTime                              0.0112581
TotalEnvSteps                            77924
policy/Entropy                               1.29749
policy/KL                                    0.00675858
policy/KLBefore                              0
policy/LossAfter                            -0.0194742
policy/LossBefore                           -5.6542e-09
policy/Perplexity                            3.6601
policy/dLoss                                 0.0194742
---------------------------------------  --------------
2022-04-23 14:19:42 | [train_policy] epoch #77 | Obtaining samples for iteration 77...
2022-04-23 14:19:42 | [train_policy] epoch #77 | Logging diagnostics...
2022-04-23 14:19:42 | [train_policy] epoch #77 | Optimizing policy...
2022-04-23 14:19:42 | [train_policy] epoch #77 | Computing loss before
2022-04-23 14:19:42 | [train_policy] epoch #77 | Computing KL before
2022-04-23 14:19:42 | [train_policy] epoch #77 | Optimizing
2022-04-23 14:19:42 | [train_policy] epoch #77 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:42 | [train_policy] epoch #77 | computing loss before
2022-04-23 14:19:42 | [train_policy] epoch #77 | computing gradient
2022-04-23 14:19:42 | [train_policy] epoch #77 | gradient computed
2022-04-23 14:19:42 | [train_policy] epoch #77 | computing descent direction
2022-04-23 14:19:42 | [train_policy] epoch #77 | descent direction computed
2022-04-23 14:19:42 | [train_policy] epoch #77 | backtrack iters: 1
2022-04-23 14:19:42 | [train_policy] epoch #77 | optimization finished
2022-04-23 14:19:42 | [train_policy] epoch #77 | Computing KL after
2022-04-23 14:19:42 | [train_policy] epoch #77 | Computing loss after
2022-04-23 14:19:42 | [train_policy] epoch #77 | Fitting baseline...
2022-04-23 14:19:42 | [train_policy] epoch #77 | Saving snapshot...
2022-04-23 14:19:42 | [train_policy] epoch #77 | Saved
2022-04-23 14:19:42 | [train_policy] epoch #77 | Time 30.21 s
2022-04-23 14:19:42 | [train_policy] epoch #77 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.115767
Evaluation/AverageDiscountedReturn         -48.6858
Evaluation/AverageReturn                   -48.6858
Evaluation/CompletionRate                    0
Evaluation/Iteration                        77
Evaluation/MaxReturn                       -38.1163
Evaluation/MinReturn                       -84.6913
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.05572
Extras/EpisodeRewardMean                   -48.6198
LinearFeatureBaseline/ExplainedVariance      0.922757
PolicyExecTime                               0.0991647
ProcessExecTime                              0.0112832
TotalEnvSteps                            78936
policy/Entropy                               1.28802
policy/KL                                    0.00683152
policy/KLBefore                              0
policy/LossAfter                            -0.0267162
policy/LossBefore                           -3.53387e-09
policy/Perplexity                            3.62561
policy/dLoss                                 0.0267162
---------------------------------------  ---------------
2022-04-23 14:19:42 | [train_policy] epoch #78 | Obtaining samples for iteration 78...
2022-04-23 14:19:42 | [train_policy] epoch #78 | Logging diagnostics...
2022-04-23 14:19:42 | [train_policy] epoch #78 | Optimizing policy...
2022-04-23 14:19:42 | [train_policy] epoch #78 | Computing loss before
2022-04-23 14:19:42 | [train_policy] epoch #78 | Computing KL before
2022-04-23 14:19:42 | [train_policy] epoch #78 | Optimizing
2022-04-23 14:19:42 | [train_policy] epoch #78 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:42 | [train_policy] epoch #78 | computing loss before
2022-04-23 14:19:42 | [train_policy] epoch #78 | computing gradient
2022-04-23 14:19:42 | [train_policy] epoch #78 | gradient computed
2022-04-23 14:19:42 | [train_policy] epoch #78 | computing descent direction
2022-04-23 14:19:42 | [train_policy] epoch #78 | descent direction computed
2022-04-23 14:19:42 | [train_policy] epoch #78 | backtrack iters: 1
2022-04-23 14:19:42 | [train_policy] epoch #78 | optimization finished
2022-04-23 14:19:42 | [train_policy] epoch #78 | Computing KL after
2022-04-23 14:19:42 | [train_policy] epoch #78 | Computing loss after
2022-04-23 14:19:42 | [train_policy] epoch #78 | Fitting baseline...
2022-04-23 14:19:42 | [train_policy] epoch #78 | Saving snapshot...
2022-04-23 14:19:42 | [train_policy] epoch #78 | Saved
2022-04-23 14:19:42 | [train_policy] epoch #78 | Time 30.54 s
2022-04-23 14:19:42 | [train_policy] epoch #78 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.117668
Evaluation/AverageDiscountedReturn         -70.5272
Evaluation/AverageReturn                   -70.5272
Evaluation/CompletionRate                    0
Evaluation/Iteration                        78
Evaluation/MaxReturn                       -36.8906
Evaluation/MinReturn                     -2063.11
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.071
Extras/EpisodeRewardMean                   -68.7487
LinearFeatureBaseline/ExplainedVariance      0.0117414
PolicyExecTime                               0.0940454
ProcessExecTime                              0.0110536
TotalEnvSteps                            79948
policy/Entropy                               1.29106
policy/KL                                    0.00767098
policy/KLBefore                              0
policy/LossAfter                            -0.0184258
policy/LossBefore                           -1.31931e-08
policy/Perplexity                            3.63665
policy/dLoss                                 0.0184258
---------------------------------------  ---------------
2022-04-23 14:19:42 | [train_policy] epoch #79 | Obtaining samples for iteration 79...
2022-04-23 14:19:43 | [train_policy] epoch #79 | Logging diagnostics...
2022-04-23 14:19:43 | [train_policy] epoch #79 | Optimizing policy...
2022-04-23 14:19:43 | [train_policy] epoch #79 | Computing loss before
2022-04-23 14:19:43 | [train_policy] epoch #79 | Computing KL before
2022-04-23 14:19:43 | [train_policy] epoch #79 | Optimizing
2022-04-23 14:19:43 | [train_policy] epoch #79 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:43 | [train_policy] epoch #79 | computing loss before
2022-04-23 14:19:43 | [train_policy] epoch #79 | computing gradient
2022-04-23 14:19:43 | [train_policy] epoch #79 | gradient computed
2022-04-23 14:19:43 | [train_policy] epoch #79 | computing descent direction
2022-04-23 14:19:43 | [train_policy] epoch #79 | descent direction computed
2022-04-23 14:19:43 | [train_policy] epoch #79 | backtrack iters: 1
2022-04-23 14:19:43 | [train_policy] epoch #79 | optimization finished
2022-04-23 14:19:43 | [train_policy] epoch #79 | Computing KL after
2022-04-23 14:19:43 | [train_policy] epoch #79 | Computing loss after
2022-04-23 14:19:43 | [train_policy] epoch #79 | Fitting baseline...
2022-04-23 14:19:43 | [train_policy] epoch #79 | Saving snapshot...
2022-04-23 14:19:43 | [train_policy] epoch #79 | Saved
2022-04-23 14:19:43 | [train_policy] epoch #79 | Time 30.87 s
2022-04-23 14:19:43 | [train_policy] epoch #79 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.115633
Evaluation/AverageDiscountedReturn         -68.6561
Evaluation/AverageReturn                   -68.6561
Evaluation/CompletionRate                    0
Evaluation/Iteration                        79
Evaluation/MaxReturn                       -37.8055
Evaluation/MinReturn                     -2064.88
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.324
Extras/EpisodeRewardMean                   -67.2112
LinearFeatureBaseline/ExplainedVariance      0.150585
PolicyExecTime                               0.0951941
ProcessExecTime                              0.0109065
TotalEnvSteps                            80960
policy/Entropy                               1.29339
policy/KL                                    0.00643089
policy/KLBefore                              0
policy/LossAfter                            -0.018332
policy/LossBefore                            1.08372e-08
policy/Perplexity                            3.64511
policy/dLoss                                 0.018332
---------------------------------------  ---------------
2022-04-23 14:19:43 | [train_policy] epoch #80 | Obtaining samples for iteration 80...
2022-04-23 14:19:43 | [train_policy] epoch #80 | Logging diagnostics...
2022-04-23 14:19:43 | [train_policy] epoch #80 | Optimizing policy...
2022-04-23 14:19:43 | [train_policy] epoch #80 | Computing loss before
2022-04-23 14:19:43 | [train_policy] epoch #80 | Computing KL before
2022-04-23 14:19:43 | [train_policy] epoch #80 | Optimizing
2022-04-23 14:19:43 | [train_policy] epoch #80 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:43 | [train_policy] epoch #80 | computing loss before
2022-04-23 14:19:43 | [train_policy] epoch #80 | computing gradient
2022-04-23 14:19:43 | [train_policy] epoch #80 | gradient computed
2022-04-23 14:19:43 | [train_policy] epoch #80 | computing descent direction
2022-04-23 14:19:43 | [train_policy] epoch #80 | descent direction computed
2022-04-23 14:19:43 | [train_policy] epoch #80 | backtrack iters: 1
2022-04-23 14:19:43 | [train_policy] epoch #80 | optimization finished
2022-04-23 14:19:43 | [train_policy] epoch #80 | Computing KL after
2022-04-23 14:19:43 | [train_policy] epoch #80 | Computing loss after
2022-04-23 14:19:43 | [train_policy] epoch #80 | Fitting baseline...
2022-04-23 14:19:43 | [train_policy] epoch #80 | Saving snapshot...
2022-04-23 14:19:43 | [train_policy] epoch #80 | Saved
2022-04-23 14:19:43 | [train_policy] epoch #80 | Time 31.22 s
2022-04-23 14:19:43 | [train_policy] epoch #80 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.117848
Evaluation/AverageDiscountedReturn         -49.6432
Evaluation/AverageReturn                   -49.6432
Evaluation/CompletionRate                    0
Evaluation/Iteration                        80
Evaluation/MaxReturn                       -38.2004
Evaluation/MinReturn                      -107.947
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         9.03031
Extras/EpisodeRewardMean                   -49.4325
LinearFeatureBaseline/ExplainedVariance    -38.7466
PolicyExecTime                               0.0999298
ProcessExecTime                              0.0117793
TotalEnvSteps                            81972
policy/Entropy                               1.27015
policy/KL                                    0.00717122
policy/KLBefore                              0
policy/LossAfter                            -0.0203165
policy/LossBefore                            3.01557e-08
policy/Perplexity                            3.56137
policy/dLoss                                 0.0203165
---------------------------------------  ---------------
2022-04-23 14:19:43 | [train_policy] epoch #81 | Obtaining samples for iteration 81...
2022-04-23 14:19:43 | [train_policy] epoch #81 | Logging diagnostics...
2022-04-23 14:19:43 | [train_policy] epoch #81 | Optimizing policy...
2022-04-23 14:19:43 | [train_policy] epoch #81 | Computing loss before
2022-04-23 14:19:43 | [train_policy] epoch #81 | Computing KL before
2022-04-23 14:19:43 | [train_policy] epoch #81 | Optimizing
2022-04-23 14:19:43 | [train_policy] epoch #81 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:43 | [train_policy] epoch #81 | computing loss before
2022-04-23 14:19:43 | [train_policy] epoch #81 | computing gradient
2022-04-23 14:19:43 | [train_policy] epoch #81 | gradient computed
2022-04-23 14:19:43 | [train_policy] epoch #81 | computing descent direction
2022-04-23 14:19:43 | [train_policy] epoch #81 | descent direction computed
2022-04-23 14:19:43 | [train_policy] epoch #81 | backtrack iters: 1
2022-04-23 14:19:43 | [train_policy] epoch #81 | optimization finished
2022-04-23 14:19:43 | [train_policy] epoch #81 | Computing KL after
2022-04-23 14:19:43 | [train_policy] epoch #81 | Computing loss after
2022-04-23 14:19:43 | [train_policy] epoch #81 | Fitting baseline...
2022-04-23 14:19:43 | [train_policy] epoch #81 | Saving snapshot...
2022-04-23 14:19:43 | [train_policy] epoch #81 | Saved
2022-04-23 14:19:43 | [train_policy] epoch #81 | Time 31.56 s
2022-04-23 14:19:43 | [train_policy] epoch #81 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                  0.115783
Evaluation/AverageDiscountedReturn         -47.4613
Evaluation/AverageReturn                   -47.4613
Evaluation/CompletionRate                    0
Evaluation/Iteration                        81
Evaluation/MaxReturn                       -35.3685
Evaluation/MinReturn                       -95.2827
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         7.07304
Extras/EpisodeRewardMean                   -47.5884
LinearFeatureBaseline/ExplainedVariance      0.894126
PolicyExecTime                               0.0959551
ProcessExecTime                              0.0109355
TotalEnvSteps                            82984
policy/Entropy                               1.25125
policy/KL                                    0.00770638
policy/KLBefore                              0
policy/LossAfter                            -0.0231478
policy/LossBefore                           -6.59656e-09
policy/Perplexity                            3.4947
policy/dLoss                                 0.0231478
---------------------------------------  ---------------
2022-04-23 14:19:43 | [train_policy] epoch #82 | Obtaining samples for iteration 82...
2022-04-23 14:19:44 | [train_policy] epoch #82 | Logging diagnostics...
2022-04-23 14:19:44 | [train_policy] epoch #82 | Optimizing policy...
2022-04-23 14:19:44 | [train_policy] epoch #82 | Computing loss before
2022-04-23 14:19:44 | [train_policy] epoch #82 | Computing KL before
2022-04-23 14:19:44 | [train_policy] epoch #82 | Optimizing
2022-04-23 14:19:44 | [train_policy] epoch #82 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:44 | [train_policy] epoch #82 | computing loss before
2022-04-23 14:19:44 | [train_policy] epoch #82 | computing gradient
2022-04-23 14:19:44 | [train_policy] epoch #82 | gradient computed
2022-04-23 14:19:44 | [train_policy] epoch #82 | computing descent direction
2022-04-23 14:19:44 | [train_policy] epoch #82 | descent direction computed
2022-04-23 14:19:44 | [train_policy] epoch #82 | backtrack iters: 1
2022-04-23 14:19:44 | [train_policy] epoch #82 | optimization finished
2022-04-23 14:19:44 | [train_policy] epoch #82 | Computing KL after
2022-04-23 14:19:44 | [train_policy] epoch #82 | Computing loss after
2022-04-23 14:19:44 | [train_policy] epoch #82 | Fitting baseline...
2022-04-23 14:19:44 | [train_policy] epoch #82 | Saving snapshot...
2022-04-23 14:19:44 | [train_policy] epoch #82 | Saved
2022-04-23 14:19:44 | [train_policy] epoch #82 | Time 31.90 s
2022-04-23 14:19:44 | [train_policy] epoch #82 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.115729
Evaluation/AverageDiscountedReturn         -70.1872
Evaluation/AverageReturn                   -70.1872
Evaluation/CompletionRate                    0
Evaluation/Iteration                        82
Evaluation/MaxReturn                       -38.3478
Evaluation/MinReturn                     -2072.09
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.97
Extras/EpisodeRewardMean                   -68.231
LinearFeatureBaseline/ExplainedVariance      0.0137626
PolicyExecTime                               0.0975008
ProcessExecTime                              0.0110953
TotalEnvSteps                            83996
policy/Entropy                               1.27102
policy/KL                                    0.00679277
policy/KLBefore                              0
policy/LossAfter                            -0.0151771
policy/LossBefore                           -1.74338e-08
policy/Perplexity                            3.56448
policy/dLoss                                 0.0151771
---------------------------------------  ---------------
2022-04-23 14:19:44 | [train_policy] epoch #83 | Obtaining samples for iteration 83...
2022-04-23 14:19:44 | [train_policy] epoch #83 | Logging diagnostics...
2022-04-23 14:19:44 | [train_policy] epoch #83 | Optimizing policy...
2022-04-23 14:19:44 | [train_policy] epoch #83 | Computing loss before
2022-04-23 14:19:44 | [train_policy] epoch #83 | Computing KL before
2022-04-23 14:19:44 | [train_policy] epoch #83 | Optimizing
2022-04-23 14:19:44 | [train_policy] epoch #83 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:44 | [train_policy] epoch #83 | computing loss before
2022-04-23 14:19:44 | [train_policy] epoch #83 | computing gradient
2022-04-23 14:19:44 | [train_policy] epoch #83 | gradient computed
2022-04-23 14:19:44 | [train_policy] epoch #83 | computing descent direction
2022-04-23 14:19:44 | [train_policy] epoch #83 | descent direction computed
2022-04-23 14:19:44 | [train_policy] epoch #83 | backtrack iters: 0
2022-04-23 14:19:44 | [train_policy] epoch #83 | optimization finished
2022-04-23 14:19:44 | [train_policy] epoch #83 | Computing KL after
2022-04-23 14:19:44 | [train_policy] epoch #83 | Computing loss after
2022-04-23 14:19:44 | [train_policy] epoch #83 | Fitting baseline...
2022-04-23 14:19:44 | [train_policy] epoch #83 | Saving snapshot...
2022-04-23 14:19:44 | [train_policy] epoch #83 | Saved
2022-04-23 14:19:44 | [train_policy] epoch #83 | Time 32.24 s
2022-04-23 14:19:44 | [train_policy] epoch #83 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.117833
Evaluation/AverageDiscountedReturn         -47.6971
Evaluation/AverageReturn                   -47.6971
Evaluation/CompletionRate                    0
Evaluation/Iteration                        83
Evaluation/MaxReturn                       -36.0108
Evaluation/MinReturn                       -66.4656
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         6.10477
Extras/EpisodeRewardMean                   -48.1752
LinearFeatureBaseline/ExplainedVariance    -26.0698
PolicyExecTime                               0.0970745
ProcessExecTime                              0.0109611
TotalEnvSteps                            85008
policy/Entropy                               1.22864
policy/KL                                    0.0092762
policy/KLBefore                              0
policy/LossAfter                            -0.0195037
policy/LossBefore                           -1.38999e-08
policy/Perplexity                            3.41659
policy/dLoss                                 0.0195037
---------------------------------------  ---------------
2022-04-23 14:19:44 | [train_policy] epoch #84 | Obtaining samples for iteration 84...
2022-04-23 14:19:44 | [train_policy] epoch #84 | Logging diagnostics...
2022-04-23 14:19:44 | [train_policy] epoch #84 | Optimizing policy...
2022-04-23 14:19:44 | [train_policy] epoch #84 | Computing loss before
2022-04-23 14:19:44 | [train_policy] epoch #84 | Computing KL before
2022-04-23 14:19:44 | [train_policy] epoch #84 | Optimizing
2022-04-23 14:19:44 | [train_policy] epoch #84 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:44 | [train_policy] epoch #84 | computing loss before
2022-04-23 14:19:44 | [train_policy] epoch #84 | computing gradient
2022-04-23 14:19:44 | [train_policy] epoch #84 | gradient computed
2022-04-23 14:19:44 | [train_policy] epoch #84 | computing descent direction
2022-04-23 14:19:44 | [train_policy] epoch #84 | descent direction computed
2022-04-23 14:19:44 | [train_policy] epoch #84 | backtrack iters: 1
2022-04-23 14:19:44 | [train_policy] epoch #84 | optimization finished
2022-04-23 14:19:44 | [train_policy] epoch #84 | Computing KL after
2022-04-23 14:19:44 | [train_policy] epoch #84 | Computing loss after
2022-04-23 14:19:44 | [train_policy] epoch #84 | Fitting baseline...
2022-04-23 14:19:44 | [train_policy] epoch #84 | Saving snapshot...
2022-04-23 14:19:44 | [train_policy] epoch #84 | Saved
2022-04-23 14:19:44 | [train_policy] epoch #84 | Time 32.58 s
2022-04-23 14:19:44 | [train_policy] epoch #84 | EpochTime 0.34 s
---------------------------------------  --------------
EnvExecTime                                  0.115846
Evaluation/AverageDiscountedReturn         -70.8145
Evaluation/AverageReturn                   -70.8145
Evaluation/CompletionRate                    0
Evaluation/Iteration                        84
Evaluation/MaxReturn                       -37.3452
Evaluation/MinReturn                     -2063.52
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.591
Extras/EpisodeRewardMean                   -69.201
LinearFeatureBaseline/ExplainedVariance      0.010996
PolicyExecTime                               0.0989513
ProcessExecTime                              0.0109572
TotalEnvSteps                            86020
policy/Entropy                               1.21074
policy/KL                                    0.00783033
policy/KLBefore                              0
policy/LossAfter                            -0.0162898
policy/LossBefore                            5.6542e-09
policy/Perplexity                            3.35597
policy/dLoss                                 0.0162898
---------------------------------------  --------------
2022-04-23 14:19:44 | [train_policy] epoch #85 | Obtaining samples for iteration 85...
2022-04-23 14:19:45 | [train_policy] epoch #85 | Logging diagnostics...
2022-04-23 14:19:45 | [train_policy] epoch #85 | Optimizing policy...
2022-04-23 14:19:45 | [train_policy] epoch #85 | Computing loss before
2022-04-23 14:19:45 | [train_policy] epoch #85 | Computing KL before
2022-04-23 14:19:45 | [train_policy] epoch #85 | Optimizing
2022-04-23 14:19:45 | [train_policy] epoch #85 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:45 | [train_policy] epoch #85 | computing loss before
2022-04-23 14:19:45 | [train_policy] epoch #85 | computing gradient
2022-04-23 14:19:45 | [train_policy] epoch #85 | gradient computed
2022-04-23 14:19:45 | [train_policy] epoch #85 | computing descent direction
2022-04-23 14:19:45 | [train_policy] epoch #85 | descent direction computed
2022-04-23 14:19:45 | [train_policy] epoch #85 | backtrack iters: 1
2022-04-23 14:19:45 | [train_policy] epoch #85 | optimization finished
2022-04-23 14:19:45 | [train_policy] epoch #85 | Computing KL after
2022-04-23 14:19:45 | [train_policy] epoch #85 | Computing loss after
2022-04-23 14:19:45 | [train_policy] epoch #85 | Fitting baseline...
2022-04-23 14:19:45 | [train_policy] epoch #85 | Saving snapshot...
2022-04-23 14:19:45 | [train_policy] epoch #85 | Saved
2022-04-23 14:19:45 | [train_policy] epoch #85 | Time 32.92 s
2022-04-23 14:19:45 | [train_policy] epoch #85 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.115684
Evaluation/AverageDiscountedReturn         -47.9088
Evaluation/AverageReturn                   -47.9088
Evaluation/CompletionRate                    0
Evaluation/Iteration                        85
Evaluation/MaxReturn                       -37.1977
Evaluation/MinReturn                      -115.693
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         8.85486
Extras/EpisodeRewardMean                   -47.7584
LinearFeatureBaseline/ExplainedVariance    -16.0579
PolicyExecTime                               0.101314
ProcessExecTime                              0.011018
TotalEnvSteps                            87032
policy/Entropy                               1.2194
policy/KL                                    0.00672544
policy/KLBefore                              0
policy/LossAfter                            -0.0201305
policy/LossBefore                           -9.89484e-09
policy/Perplexity                            3.38515
policy/dLoss                                 0.0201305
---------------------------------------  ---------------
2022-04-23 14:19:45 | [train_policy] epoch #86 | Obtaining samples for iteration 86...
2022-04-23 14:19:45 | [train_policy] epoch #86 | Logging diagnostics...
2022-04-23 14:19:45 | [train_policy] epoch #86 | Optimizing policy...
2022-04-23 14:19:45 | [train_policy] epoch #86 | Computing loss before
2022-04-23 14:19:45 | [train_policy] epoch #86 | Computing KL before
2022-04-23 14:19:45 | [train_policy] epoch #86 | Optimizing
2022-04-23 14:19:45 | [train_policy] epoch #86 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:45 | [train_policy] epoch #86 | computing loss before
2022-04-23 14:19:45 | [train_policy] epoch #86 | computing gradient
2022-04-23 14:19:45 | [train_policy] epoch #86 | gradient computed
2022-04-23 14:19:45 | [train_policy] epoch #86 | computing descent direction
2022-04-23 14:19:45 | [train_policy] epoch #86 | descent direction computed
2022-04-23 14:19:45 | [train_policy] epoch #86 | backtrack iters: 1
2022-04-23 14:19:45 | [train_policy] epoch #86 | optimization finished
2022-04-23 14:19:45 | [train_policy] epoch #86 | Computing KL after
2022-04-23 14:19:45 | [train_policy] epoch #86 | Computing loss after
2022-04-23 14:19:45 | [train_policy] epoch #86 | Fitting baseline...
2022-04-23 14:19:45 | [train_policy] epoch #86 | Saving snapshot...
2022-04-23 14:19:45 | [train_policy] epoch #86 | Saved
2022-04-23 14:19:45 | [train_policy] epoch #86 | Time 33.27 s
2022-04-23 14:19:45 | [train_policy] epoch #86 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.118344
Evaluation/AverageDiscountedReturn         -90.9602
Evaluation/AverageReturn                   -90.9602
Evaluation/CompletionRate                    0
Evaluation/Iteration                        86
Evaluation/MaxReturn                       -37.9055
Evaluation/MinReturn                     -2063.25
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       293.973
Extras/EpisodeRewardMean                   -87.6368
LinearFeatureBaseline/ExplainedVariance      0.0148818
PolicyExecTime                               0.100168
ProcessExecTime                              0.0114639
TotalEnvSteps                            88044
policy/Entropy                               1.22959
policy/KL                                    0.00719125
policy/KLBefore                              0
policy/LossAfter                            -0.012673
policy/LossBefore                            6.12538e-09
policy/Perplexity                            3.41984
policy/dLoss                                 0.012673
---------------------------------------  ---------------
2022-04-23 14:19:45 | [train_policy] epoch #87 | Obtaining samples for iteration 87...
2022-04-23 14:19:45 | [train_policy] epoch #87 | Logging diagnostics...
2022-04-23 14:19:45 | [train_policy] epoch #87 | Optimizing policy...
2022-04-23 14:19:45 | [train_policy] epoch #87 | Computing loss before
2022-04-23 14:19:45 | [train_policy] epoch #87 | Computing KL before
2022-04-23 14:19:45 | [train_policy] epoch #87 | Optimizing
2022-04-23 14:19:45 | [train_policy] epoch #87 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:45 | [train_policy] epoch #87 | computing loss before
2022-04-23 14:19:45 | [train_policy] epoch #87 | computing gradient
2022-04-23 14:19:45 | [train_policy] epoch #87 | gradient computed
2022-04-23 14:19:45 | [train_policy] epoch #87 | computing descent direction
2022-04-23 14:19:45 | [train_policy] epoch #87 | descent direction computed
2022-04-23 14:19:45 | [train_policy] epoch #87 | backtrack iters: 1
2022-04-23 14:19:45 | [train_policy] epoch #87 | optimization finished
2022-04-23 14:19:45 | [train_policy] epoch #87 | Computing KL after
2022-04-23 14:19:45 | [train_policy] epoch #87 | Computing loss after
2022-04-23 14:19:45 | [train_policy] epoch #87 | Fitting baseline...
2022-04-23 14:19:45 | [train_policy] epoch #87 | Saving snapshot...
2022-04-23 14:19:45 | [train_policy] epoch #87 | Saved
2022-04-23 14:19:45 | [train_policy] epoch #87 | Time 33.63 s
2022-04-23 14:19:45 | [train_policy] epoch #87 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.119382
Evaluation/AverageDiscountedReturn         -47.4238
Evaluation/AverageReturn                   -47.4238
Evaluation/CompletionRate                    0
Evaluation/Iteration                        87
Evaluation/MaxReturn                       -38.6074
Evaluation/MinReturn                       -61.368
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         4.81356
Extras/EpisodeRewardMean                   -47.4495
LinearFeatureBaseline/ExplainedVariance    -61.2402
PolicyExecTime                               0.0980585
ProcessExecTime                              0.0114555
TotalEnvSteps                            89056
policy/Entropy                               1.22753
policy/KL                                    0.00680669
policy/KLBefore                              0
policy/LossAfter                            -0.0179116
policy/LossBefore                            3.05091e-08
policy/Perplexity                            3.4128
policy/dLoss                                 0.0179117
---------------------------------------  ---------------
2022-04-23 14:19:45 | [train_policy] epoch #88 | Obtaining samples for iteration 88...
2022-04-23 14:19:46 | [train_policy] epoch #88 | Logging diagnostics...
2022-04-23 14:19:46 | [train_policy] epoch #88 | Optimizing policy...
2022-04-23 14:19:46 | [train_policy] epoch #88 | Computing loss before
2022-04-23 14:19:46 | [train_policy] epoch #88 | Computing KL before
2022-04-23 14:19:46 | [train_policy] epoch #88 | Optimizing
2022-04-23 14:19:46 | [train_policy] epoch #88 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:46 | [train_policy] epoch #88 | computing loss before
2022-04-23 14:19:46 | [train_policy] epoch #88 | computing gradient
2022-04-23 14:19:46 | [train_policy] epoch #88 | gradient computed
2022-04-23 14:19:46 | [train_policy] epoch #88 | computing descent direction
2022-04-23 14:19:46 | [train_policy] epoch #88 | descent direction computed
2022-04-23 14:19:46 | [train_policy] epoch #88 | backtrack iters: 1
2022-04-23 14:19:46 | [train_policy] epoch #88 | optimization finished
2022-04-23 14:19:46 | [train_policy] epoch #88 | Computing KL after
2022-04-23 14:19:46 | [train_policy] epoch #88 | Computing loss after
2022-04-23 14:19:46 | [train_policy] epoch #88 | Fitting baseline...
2022-04-23 14:19:46 | [train_policy] epoch #88 | Saving snapshot...
2022-04-23 14:19:46 | [train_policy] epoch #88 | Saved
2022-04-23 14:19:46 | [train_policy] epoch #88 | Time 33.98 s
2022-04-23 14:19:46 | [train_policy] epoch #88 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.118279
Evaluation/AverageDiscountedReturn         -49.5543
Evaluation/AverageReturn                   -49.5543
Evaluation/CompletionRate                    0
Evaluation/Iteration                        88
Evaluation/MaxReturn                       -38.6007
Evaluation/MinReturn                      -174.009
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        15.442
Extras/EpisodeRewardMean                   -49.7179
LinearFeatureBaseline/ExplainedVariance      0.525142
PolicyExecTime                               0.102775
ProcessExecTime                              0.0124996
TotalEnvSteps                            90068
policy/Entropy                               1.19521
policy/KL                                    0.00665262
policy/KLBefore                              0
policy/LossAfter                            -0.0183655
policy/LossBefore                           -5.88979e-09
policy/Perplexity                            3.30424
policy/dLoss                                 0.0183655
---------------------------------------  ---------------
2022-04-23 14:19:46 | [train_policy] epoch #89 | Obtaining samples for iteration 89...
2022-04-23 14:19:46 | [train_policy] epoch #89 | Logging diagnostics...
2022-04-23 14:19:46 | [train_policy] epoch #89 | Optimizing policy...
2022-04-23 14:19:46 | [train_policy] epoch #89 | Computing loss before
2022-04-23 14:19:46 | [train_policy] epoch #89 | Computing KL before
2022-04-23 14:19:46 | [train_policy] epoch #89 | Optimizing
2022-04-23 14:19:46 | [train_policy] epoch #89 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:46 | [train_policy] epoch #89 | computing loss before
2022-04-23 14:19:46 | [train_policy] epoch #89 | computing gradient
2022-04-23 14:19:46 | [train_policy] epoch #89 | gradient computed
2022-04-23 14:19:46 | [train_policy] epoch #89 | computing descent direction
2022-04-23 14:19:46 | [train_policy] epoch #89 | descent direction computed
2022-04-23 14:19:46 | [train_policy] epoch #89 | backtrack iters: 1
2022-04-23 14:19:46 | [train_policy] epoch #89 | optimization finished
2022-04-23 14:19:46 | [train_policy] epoch #89 | Computing KL after
2022-04-23 14:19:46 | [train_policy] epoch #89 | Computing loss after
2022-04-23 14:19:46 | [train_policy] epoch #89 | Fitting baseline...
2022-04-23 14:19:46 | [train_policy] epoch #89 | Saving snapshot...
2022-04-23 14:19:46 | [train_policy] epoch #89 | Saved
2022-04-23 14:19:46 | [train_policy] epoch #89 | Time 34.33 s
2022-04-23 14:19:46 | [train_policy] epoch #89 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.11988
Evaluation/AverageDiscountedReturn         -70.4192
Evaluation/AverageReturn                   -70.4192
Evaluation/CompletionRate                    0
Evaluation/Iteration                        89
Evaluation/MaxReturn                       -36.6974
Evaluation/MinReturn                     -2054.17
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.427
Extras/EpisodeRewardMean                   -68.4193
LinearFeatureBaseline/ExplainedVariance      0.0116053
PolicyExecTime                               0.0954235
ProcessExecTime                              0.0113981
TotalEnvSteps                            91080
policy/Entropy                               1.18972
policy/KL                                    0.00949164
policy/KLBefore                              0
policy/LossAfter                            -0.0124034
policy/LossBefore                            1.88473e-09
policy/Perplexity                            3.28617
policy/dLoss                                 0.0124034
---------------------------------------  ---------------
2022-04-23 14:19:46 | [train_policy] epoch #90 | Obtaining samples for iteration 90...
2022-04-23 14:19:46 | [train_policy] epoch #90 | Logging diagnostics...
2022-04-23 14:19:46 | [train_policy] epoch #90 | Optimizing policy...
2022-04-23 14:19:46 | [train_policy] epoch #90 | Computing loss before
2022-04-23 14:19:46 | [train_policy] epoch #90 | Computing KL before
2022-04-23 14:19:46 | [train_policy] epoch #90 | Optimizing
2022-04-23 14:19:46 | [train_policy] epoch #90 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:46 | [train_policy] epoch #90 | computing loss before
2022-04-23 14:19:46 | [train_policy] epoch #90 | computing gradient
2022-04-23 14:19:46 | [train_policy] epoch #90 | gradient computed
2022-04-23 14:19:46 | [train_policy] epoch #90 | computing descent direction
2022-04-23 14:19:46 | [train_policy] epoch #90 | descent direction computed
2022-04-23 14:19:46 | [train_policy] epoch #90 | backtrack iters: 1
2022-04-23 14:19:46 | [train_policy] epoch #90 | optimization finished
2022-04-23 14:19:46 | [train_policy] epoch #90 | Computing KL after
2022-04-23 14:19:46 | [train_policy] epoch #90 | Computing loss after
2022-04-23 14:19:46 | [train_policy] epoch #90 | Fitting baseline...
2022-04-23 14:19:46 | [train_policy] epoch #90 | Saving snapshot...
2022-04-23 14:19:46 | [train_policy] epoch #90 | Saved
2022-04-23 14:19:46 | [train_policy] epoch #90 | Time 34.67 s
2022-04-23 14:19:46 | [train_policy] epoch #90 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.116743
Evaluation/AverageDiscountedReturn         -47.8949
Evaluation/AverageReturn                   -47.8949
Evaluation/CompletionRate                    0
Evaluation/Iteration                        90
Evaluation/MaxReturn                       -38.0175
Evaluation/MinReturn                       -64.3544
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.57419
Extras/EpisodeRewardMean                   -49.1656
LinearFeatureBaseline/ExplainedVariance    -31.6982
PolicyExecTime                               0.0951936
ProcessExecTime                              0.0119026
TotalEnvSteps                            92092
policy/Entropy                               1.18988
policy/KL                                    0.0068762
policy/KLBefore                              0
policy/LossAfter                            -0.058367
policy/LossBefore                           -1.22508e-08
policy/Perplexity                            3.28669
policy/dLoss                                 0.058367
---------------------------------------  ---------------
2022-04-23 14:19:46 | [train_policy] epoch #91 | Obtaining samples for iteration 91...
2022-04-23 14:19:47 | [train_policy] epoch #91 | Logging diagnostics...
2022-04-23 14:19:47 | [train_policy] epoch #91 | Optimizing policy...
2022-04-23 14:19:47 | [train_policy] epoch #91 | Computing loss before
2022-04-23 14:19:47 | [train_policy] epoch #91 | Computing KL before
2022-04-23 14:19:47 | [train_policy] epoch #91 | Optimizing
2022-04-23 14:19:47 | [train_policy] epoch #91 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:47 | [train_policy] epoch #91 | computing loss before
2022-04-23 14:19:47 | [train_policy] epoch #91 | computing gradient
2022-04-23 14:19:47 | [train_policy] epoch #91 | gradient computed
2022-04-23 14:19:47 | [train_policy] epoch #91 | computing descent direction
2022-04-23 14:19:47 | [train_policy] epoch #91 | descent direction computed
2022-04-23 14:19:47 | [train_policy] epoch #91 | backtrack iters: 1
2022-04-23 14:19:47 | [train_policy] epoch #91 | optimization finished
2022-04-23 14:19:47 | [train_policy] epoch #91 | Computing KL after
2022-04-23 14:19:47 | [train_policy] epoch #91 | Computing loss after
2022-04-23 14:19:47 | [train_policy] epoch #91 | Fitting baseline...
2022-04-23 14:19:47 | [train_policy] epoch #91 | Saving snapshot...
2022-04-23 14:19:47 | [train_policy] epoch #91 | Saved
2022-04-23 14:19:47 | [train_policy] epoch #91 | Time 35.02 s
2022-04-23 14:19:47 | [train_policy] epoch #91 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.116798
Evaluation/AverageDiscountedReturn         -71.1182
Evaluation/AverageReturn                   -71.1182
Evaluation/CompletionRate                    0
Evaluation/Iteration                        91
Evaluation/MaxReturn                       -35.6154
Evaluation/MinReturn                     -2063.83
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       209.571
Extras/EpisodeRewardMean                   -69.2316
LinearFeatureBaseline/ExplainedVariance      0.0107568
PolicyExecTime                               0.0995891
ProcessExecTime                              0.0112948
TotalEnvSteps                            93104
policy/Entropy                               1.19562
policy/KL                                    0.00737413
policy/KLBefore                              0
policy/LossAfter                            -0.0193568
policy/LossBefore                            1.93185e-08
policy/Perplexity                            3.30561
policy/dLoss                                 0.0193568
---------------------------------------  ---------------
2022-04-23 14:19:47 | [train_policy] epoch #92 | Obtaining samples for iteration 92...
2022-04-23 14:19:47 | [train_policy] epoch #92 | Logging diagnostics...
2022-04-23 14:19:47 | [train_policy] epoch #92 | Optimizing policy...
2022-04-23 14:19:47 | [train_policy] epoch #92 | Computing loss before
2022-04-23 14:19:47 | [train_policy] epoch #92 | Computing KL before
2022-04-23 14:19:47 | [train_policy] epoch #92 | Optimizing
2022-04-23 14:19:47 | [train_policy] epoch #92 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:47 | [train_policy] epoch #92 | computing loss before
2022-04-23 14:19:47 | [train_policy] epoch #92 | computing gradient
2022-04-23 14:19:47 | [train_policy] epoch #92 | gradient computed
2022-04-23 14:19:47 | [train_policy] epoch #92 | computing descent direction
2022-04-23 14:19:47 | [train_policy] epoch #92 | descent direction computed
2022-04-23 14:19:47 | [train_policy] epoch #92 | backtrack iters: 0
2022-04-23 14:19:47 | [train_policy] epoch #92 | optimization finished
2022-04-23 14:19:47 | [train_policy] epoch #92 | Computing KL after
2022-04-23 14:19:47 | [train_policy] epoch #92 | Computing loss after
2022-04-23 14:19:47 | [train_policy] epoch #92 | Fitting baseline...
2022-04-23 14:19:47 | [train_policy] epoch #92 | Saving snapshot...
2022-04-23 14:19:47 | [train_policy] epoch #92 | Saved
2022-04-23 14:19:47 | [train_policy] epoch #92 | Time 35.39 s
2022-04-23 14:19:47 | [train_policy] epoch #92 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.121711
Evaluation/AverageDiscountedReturn        -136.179
Evaluation/AverageReturn                  -136.179
Evaluation/CompletionRate                    0
Evaluation/Iteration                        92
Evaluation/MaxReturn                       -36.6112
Evaluation/MinReturn                     -2062.85
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       410.746
Extras/EpisodeRewardMean                  -129.396
LinearFeatureBaseline/ExplainedVariance      0.166008
PolicyExecTime                               0.107698
ProcessExecTime                              0.0119281
TotalEnvSteps                            94116
policy/Entropy                               1.28304
policy/KL                                    0.00946938
policy/KLBefore                              0
policy/LossAfter                            -0.0128161
policy/LossBefore                            6.71436e-09
policy/Perplexity                            3.6076
policy/dLoss                                 0.0128161
---------------------------------------  ---------------
2022-04-23 14:19:47 | [train_policy] epoch #93 | Obtaining samples for iteration 93...
2022-04-23 14:19:47 | [train_policy] epoch #93 | Logging diagnostics...
2022-04-23 14:19:47 | [train_policy] epoch #93 | Optimizing policy...
2022-04-23 14:19:47 | [train_policy] epoch #93 | Computing loss before
2022-04-23 14:19:47 | [train_policy] epoch #93 | Computing KL before
2022-04-23 14:19:47 | [train_policy] epoch #93 | Optimizing
2022-04-23 14:19:47 | [train_policy] epoch #93 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:47 | [train_policy] epoch #93 | computing loss before
2022-04-23 14:19:47 | [train_policy] epoch #93 | computing gradient
2022-04-23 14:19:47 | [train_policy] epoch #93 | gradient computed
2022-04-23 14:19:47 | [train_policy] epoch #93 | computing descent direction
2022-04-23 14:19:48 | [train_policy] epoch #93 | descent direction computed
2022-04-23 14:19:48 | [train_policy] epoch #93 | backtrack iters: 1
2022-04-23 14:19:48 | [train_policy] epoch #93 | optimization finished
2022-04-23 14:19:48 | [train_policy] epoch #93 | Computing KL after
2022-04-23 14:19:48 | [train_policy] epoch #93 | Computing loss after
2022-04-23 14:19:48 | [train_policy] epoch #93 | Fitting baseline...
2022-04-23 14:19:48 | [train_policy] epoch #93 | Saving snapshot...
2022-04-23 14:19:48 | [train_policy] epoch #93 | Saved
2022-04-23 14:19:48 | [train_policy] epoch #93 | Time 35.75 s
2022-04-23 14:19:48 | [train_policy] epoch #93 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                  0.120026
Evaluation/AverageDiscountedReturn         -52.2802
Evaluation/AverageReturn                   -52.2802
Evaluation/CompletionRate                    0
Evaluation/Iteration                        93
Evaluation/MaxReturn                       -36.9237
Evaluation/MinReturn                      -394.155
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        36.4761
Extras/EpisodeRewardMean                   -92.4364
LinearFeatureBaseline/ExplainedVariance   -129.888
PolicyExecTime                               0.108019
ProcessExecTime                              0.0117593
TotalEnvSteps                            95128
policy/Entropy                               1.27329
policy/KL                                    0.00781044
policy/KLBefore                              0
policy/LossAfter                            -0.0175691
policy/LossBefore                           -4.24065e-09
policy/Perplexity                            3.57257
policy/dLoss                                 0.0175691
---------------------------------------  ---------------
2022-04-23 14:19:48 | [train_policy] epoch #94 | Obtaining samples for iteration 94...
2022-04-23 14:19:48 | [train_policy] epoch #94 | Logging diagnostics...
2022-04-23 14:19:48 | [train_policy] epoch #94 | Optimizing policy...
2022-04-23 14:19:48 | [train_policy] epoch #94 | Computing loss before
2022-04-23 14:19:48 | [train_policy] epoch #94 | Computing KL before
2022-04-23 14:19:48 | [train_policy] epoch #94 | Optimizing
2022-04-23 14:19:48 | [train_policy] epoch #94 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:48 | [train_policy] epoch #94 | computing loss before
2022-04-23 14:19:48 | [train_policy] epoch #94 | computing gradient
2022-04-23 14:19:48 | [train_policy] epoch #94 | gradient computed
2022-04-23 14:19:48 | [train_policy] epoch #94 | computing descent direction
2022-04-23 14:19:48 | [train_policy] epoch #94 | descent direction computed
2022-04-23 14:19:48 | [train_policy] epoch #94 | backtrack iters: 1
2022-04-23 14:19:48 | [train_policy] epoch #94 | optimization finished
2022-04-23 14:19:48 | [train_policy] epoch #94 | Computing KL after
2022-04-23 14:19:48 | [train_policy] epoch #94 | Computing loss after
2022-04-23 14:19:48 | [train_policy] epoch #94 | Fitting baseline...
2022-04-23 14:19:48 | [train_policy] epoch #94 | Saving snapshot...
2022-04-23 14:19:48 | [train_policy] epoch #94 | Saved
2022-04-23 14:19:48 | [train_policy] epoch #94 | Time 36.10 s
2022-04-23 14:19:48 | [train_policy] epoch #94 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                  0.118358
Evaluation/AverageDiscountedReturn         -47.527
Evaluation/AverageReturn                   -47.527
Evaluation/CompletionRate                    0
Evaluation/Iteration                        94
Evaluation/MaxReturn                       -33.9899
Evaluation/MinReturn                       -65.1645
Evaluation/NumTrajs                         92
Evaluation/StdReturn                         5.96985
Extras/EpisodeRewardMean                   -47.5919
LinearFeatureBaseline/ExplainedVariance      0.941395
PolicyExecTime                               0.105921
ProcessExecTime                              0.0120037
TotalEnvSteps                            96140
policy/Entropy                               1.27068
policy/KL                                    0.00670841
policy/KLBefore                              0
policy/LossAfter                            -0.0243954
policy/LossBefore                            6.36097e-09
policy/Perplexity                            3.56326
policy/dLoss                                 0.0243954
---------------------------------------  ---------------
2022-04-23 14:19:48 | [train_policy] epoch #95 | Obtaining samples for iteration 95...
2022-04-23 14:19:48 | [train_policy] epoch #95 | Logging diagnostics...
2022-04-23 14:19:48 | [train_policy] epoch #95 | Optimizing policy...
2022-04-23 14:19:48 | [train_policy] epoch #95 | Computing loss before
2022-04-23 14:19:48 | [train_policy] epoch #95 | Computing KL before
2022-04-23 14:19:48 | [train_policy] epoch #95 | Optimizing
2022-04-23 14:19:48 | [train_policy] epoch #95 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:48 | [train_policy] epoch #95 | computing loss before
2022-04-23 14:19:48 | [train_policy] epoch #95 | computing gradient
2022-04-23 14:19:48 | [train_policy] epoch #95 | gradient computed
2022-04-23 14:19:48 | [train_policy] epoch #95 | computing descent direction
2022-04-23 14:19:48 | [train_policy] epoch #95 | descent direction computed
2022-04-23 14:19:48 | [train_policy] epoch #95 | backtrack iters: 1
2022-04-23 14:19:48 | [train_policy] epoch #95 | optimization finished
2022-04-23 14:19:48 | [train_policy] epoch #95 | Computing KL after
2022-04-23 14:19:48 | [train_policy] epoch #95 | Computing loss after
2022-04-23 14:19:48 | [train_policy] epoch #95 | Fitting baseline...
2022-04-23 14:19:48 | [train_policy] epoch #95 | Saving snapshot...
2022-04-23 14:19:48 | [train_policy] epoch #95 | Saved
2022-04-23 14:19:48 | [train_policy] epoch #95 | Time 36.45 s
2022-04-23 14:19:48 | [train_policy] epoch #95 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.117389
Evaluation/AverageDiscountedReturn         -68.7301
Evaluation/AverageReturn                   -68.7301
Evaluation/CompletionRate                    0
Evaluation/Iteration                        95
Evaluation/MaxReturn                       -35.9998
Evaluation/MinReturn                     -2061.66
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       208.99
Extras/EpisodeRewardMean                   -67.0865
LinearFeatureBaseline/ExplainedVariance      0.0107436
PolicyExecTime                               0.100735
ProcessExecTime                              0.0118208
TotalEnvSteps                            97152
policy/Entropy                               1.24374
policy/KL                                    0.00651142
policy/KLBefore                              0
policy/LossAfter                            -0.0115012
policy/LossBefore                           -1.08372e-08
policy/Perplexity                            3.46855
policy/dLoss                                 0.0115012
---------------------------------------  ---------------
2022-04-23 14:19:48 | [train_policy] epoch #96 | Obtaining samples for iteration 96...
2022-04-23 14:19:49 | [train_policy] epoch #96 | Logging diagnostics...
2022-04-23 14:19:49 | [train_policy] epoch #96 | Optimizing policy...
2022-04-23 14:19:49 | [train_policy] epoch #96 | Computing loss before
2022-04-23 14:19:49 | [train_policy] epoch #96 | Computing KL before
2022-04-23 14:19:49 | [train_policy] epoch #96 | Optimizing
2022-04-23 14:19:49 | [train_policy] epoch #96 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:49 | [train_policy] epoch #96 | computing loss before
2022-04-23 14:19:49 | [train_policy] epoch #96 | computing gradient
2022-04-23 14:19:49 | [train_policy] epoch #96 | gradient computed
2022-04-23 14:19:49 | [train_policy] epoch #96 | computing descent direction
2022-04-23 14:19:49 | [train_policy] epoch #96 | descent direction computed
2022-04-23 14:19:49 | [train_policy] epoch #96 | backtrack iters: 0
2022-04-23 14:19:49 | [train_policy] epoch #96 | optimization finished
2022-04-23 14:19:49 | [train_policy] epoch #96 | Computing KL after
2022-04-23 14:19:49 | [train_policy] epoch #96 | Computing loss after
2022-04-23 14:19:49 | [train_policy] epoch #96 | Fitting baseline...
2022-04-23 14:19:49 | [train_policy] epoch #96 | Saving snapshot...
2022-04-23 14:19:49 | [train_policy] epoch #96 | Saved
2022-04-23 14:19:49 | [train_policy] epoch #96 | Time 36.80 s
2022-04-23 14:19:49 | [train_policy] epoch #96 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.116683
Evaluation/AverageDiscountedReturn         -48.6244
Evaluation/AverageReturn                   -48.6244
Evaluation/CompletionRate                    0
Evaluation/Iteration                        96
Evaluation/MaxReturn                       -37.7343
Evaluation/MinReturn                      -133.4
Evaluation/NumTrajs                         92
Evaluation/StdReturn                        10.6046
Extras/EpisodeRewardMean                   -48.2051
LinearFeatureBaseline/ExplainedVariance    -18.0196
PolicyExecTime                               0.102415
ProcessExecTime                              0.0115867
TotalEnvSteps                            98164
policy/Entropy                               1.19453
policy/KL                                    0.00870441
policy/KLBefore                              0
policy/LossAfter                            -0.0183214
policy/LossBefore                            1.38999e-08
policy/Perplexity                            3.30202
policy/dLoss                                 0.0183214
---------------------------------------  ---------------
2022-04-23 14:19:49 | [train_policy] epoch #97 | Obtaining samples for iteration 97...
2022-04-23 14:19:49 | [train_policy] epoch #97 | Logging diagnostics...
2022-04-23 14:19:49 | [train_policy] epoch #97 | Optimizing policy...
2022-04-23 14:19:49 | [train_policy] epoch #97 | Computing loss before
2022-04-23 14:19:49 | [train_policy] epoch #97 | Computing KL before
2022-04-23 14:19:49 | [train_policy] epoch #97 | Optimizing
2022-04-23 14:19:49 | [train_policy] epoch #97 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:49 | [train_policy] epoch #97 | computing loss before
2022-04-23 14:19:49 | [train_policy] epoch #97 | computing gradient
2022-04-23 14:19:49 | [train_policy] epoch #97 | gradient computed
2022-04-23 14:19:49 | [train_policy] epoch #97 | computing descent direction
2022-04-23 14:19:49 | [train_policy] epoch #97 | descent direction computed
2022-04-23 14:19:49 | [train_policy] epoch #97 | backtrack iters: 0
2022-04-23 14:19:49 | [train_policy] epoch #97 | optimization finished
2022-04-23 14:19:49 | [train_policy] epoch #97 | Computing KL after
2022-04-23 14:19:49 | [train_policy] epoch #97 | Computing loss after
2022-04-23 14:19:49 | [train_policy] epoch #97 | Fitting baseline...
2022-04-23 14:19:49 | [train_policy] epoch #97 | Saving snapshot...
2022-04-23 14:19:49 | [train_policy] epoch #97 | Saved
2022-04-23 14:19:49 | [train_policy] epoch #97 | Time 37.14 s
2022-04-23 14:19:49 | [train_policy] epoch #97 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                  0.115392
Evaluation/AverageDiscountedReturn         -80.112
Evaluation/AverageReturn                   -80.112
Evaluation/CompletionRate                    0
Evaluation/Iteration                        97
Evaluation/MaxReturn                       -34.4754
Evaluation/MinReturn                     -2062.87
Evaluation/NumTrajs                         92
Evaluation/StdReturn                       224.499
Extras/EpisodeRewardMean                   -77.4787
LinearFeatureBaseline/ExplainedVariance      0.00948626
PolicyExecTime                               0.0962389
ProcessExecTime                              0.0110841
TotalEnvSteps                            99176
policy/Entropy                               1.17
policy/KL                                    0.00992045
policy/KLBefore                              0
policy/LossAfter                            -0.0242068
policy/LossBefore                            1.46067e-08
policy/Perplexity                            3.22198
policy/dLoss                                 0.0242068
---------------------------------------  ---------------
2022-04-23 14:19:49 | [train_policy] epoch #98 | Obtaining samples for iteration 98...
2022-04-23 14:19:49 | [train_policy] epoch #98 | Logging diagnostics...
2022-04-23 14:19:49 | [train_policy] epoch #98 | Optimizing policy...
2022-04-23 14:19:49 | [train_policy] epoch #98 | Computing loss before
2022-04-23 14:19:49 | [train_policy] epoch #98 | Computing KL before
2022-04-23 14:19:49 | [train_policy] epoch #98 | Optimizing
2022-04-23 14:19:49 | [train_policy] epoch #98 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:49 | [train_policy] epoch #98 | computing loss before
2022-04-23 14:19:49 | [train_policy] epoch #98 | computing gradient
2022-04-23 14:19:49 | [train_policy] epoch #98 | gradient computed
2022-04-23 14:19:49 | [train_policy] epoch #98 | computing descent direction
2022-04-23 14:19:49 | [train_policy] epoch #98 | descent direction computed
2022-04-23 14:19:49 | [train_policy] epoch #98 | backtrack iters: 0
2022-04-23 14:19:49 | [train_policy] epoch #98 | optimization finished
2022-04-23 14:19:49 | [train_policy] epoch #98 | Computing KL after
2022-04-23 14:19:49 | [train_policy] epoch #98 | Computing loss after
2022-04-23 14:19:49 | [train_policy] epoch #98 | Fitting baseline...
2022-04-23 14:19:49 | [train_policy] epoch #98 | Saving snapshot...
2022-04-23 14:19:49 | [train_policy] epoch #98 | Saved
2022-04-23 14:19:49 | [train_policy] epoch #98 | Time 37.49 s
2022-04-23 14:19:49 | [train_policy] epoch #98 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116622
Evaluation/AverageDiscountedReturn          -47.1082
Evaluation/AverageReturn                    -47.1082
Evaluation/CompletionRate                     0
Evaluation/Iteration                         98
Evaluation/MaxReturn                        -34.7802
Evaluation/MinReturn                        -80.7483
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.59535
Extras/EpisodeRewardMean                    -48.0987
LinearFeatureBaseline/ExplainedVariance     -13.3361
PolicyExecTime                                0.102975
ProcessExecTime                               0.0113673
TotalEnvSteps                            100188
policy/Entropy                                1.23827
policy/KL                                     0.00907293
policy/KLBefore                               0
policy/LossAfter                             -0.0211512
policy/LossBefore                            -2.83299e-08
policy/Perplexity                             3.44964
policy/dLoss                                  0.0211512
---------------------------------------  ----------------
2022-04-23 14:19:49 | [train_policy] epoch #99 | Obtaining samples for iteration 99...
2022-04-23 14:19:50 | [train_policy] epoch #99 | Logging diagnostics...
2022-04-23 14:19:50 | [train_policy] epoch #99 | Optimizing policy...
2022-04-23 14:19:50 | [train_policy] epoch #99 | Computing loss before
2022-04-23 14:19:50 | [train_policy] epoch #99 | Computing KL before
2022-04-23 14:19:50 | [train_policy] epoch #99 | Optimizing
2022-04-23 14:19:50 | [train_policy] epoch #99 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:50 | [train_policy] epoch #99 | computing loss before
2022-04-23 14:19:50 | [train_policy] epoch #99 | computing gradient
2022-04-23 14:19:50 | [train_policy] epoch #99 | gradient computed
2022-04-23 14:19:50 | [train_policy] epoch #99 | computing descent direction
2022-04-23 14:19:50 | [train_policy] epoch #99 | descent direction computed
2022-04-23 14:19:50 | [train_policy] epoch #99 | backtrack iters: 0
2022-04-23 14:19:50 | [train_policy] epoch #99 | optimization finished
2022-04-23 14:19:50 | [train_policy] epoch #99 | Computing KL after
2022-04-23 14:19:50 | [train_policy] epoch #99 | Computing loss after
2022-04-23 14:19:50 | [train_policy] epoch #99 | Fitting baseline...
2022-04-23 14:19:50 | [train_policy] epoch #99 | Saving snapshot...
2022-04-23 14:19:50 | [train_policy] epoch #99 | Saved
2022-04-23 14:19:50 | [train_policy] epoch #99 | Time 37.83 s
2022-04-23 14:19:50 | [train_policy] epoch #99 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115961
Evaluation/AverageDiscountedReturn          -47.2438
Evaluation/AverageReturn                    -47.2438
Evaluation/CompletionRate                     0
Evaluation/Iteration                         99
Evaluation/MaxReturn                        -34.4632
Evaluation/MinReturn                       -101.279
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.03622
Extras/EpisodeRewardMean                    -47.2594
LinearFeatureBaseline/ExplainedVariance       0.784264
PolicyExecTime                                0.10053
ProcessExecTime                               0.011023
TotalEnvSteps                            101200
policy/Entropy                                1.18677
policy/KL                                     0.00867946
policy/KLBefore                               0
policy/LossAfter                             -0.0226966
policy/LossBefore                             9.42366e-10
policy/Perplexity                             3.27648
policy/dLoss                                  0.0226966
---------------------------------------  ----------------
2022-04-23 14:19:50 | [train_policy] epoch #100 | Obtaining samples for iteration 100...
2022-04-23 14:19:50 | [train_policy] epoch #100 | Logging diagnostics...
2022-04-23 14:19:50 | [train_policy] epoch #100 | Optimizing policy...
2022-04-23 14:19:50 | [train_policy] epoch #100 | Computing loss before
2022-04-23 14:19:50 | [train_policy] epoch #100 | Computing KL before
2022-04-23 14:19:50 | [train_policy] epoch #100 | Optimizing
2022-04-23 14:19:50 | [train_policy] epoch #100 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:50 | [train_policy] epoch #100 | computing loss before
2022-04-23 14:19:50 | [train_policy] epoch #100 | computing gradient
2022-04-23 14:19:50 | [train_policy] epoch #100 | gradient computed
2022-04-23 14:19:50 | [train_policy] epoch #100 | computing descent direction
2022-04-23 14:19:50 | [train_policy] epoch #100 | descent direction computed
2022-04-23 14:19:50 | [train_policy] epoch #100 | backtrack iters: 1
2022-04-23 14:19:50 | [train_policy] epoch #100 | optimization finished
2022-04-23 14:19:50 | [train_policy] epoch #100 | Computing KL after
2022-04-23 14:19:50 | [train_policy] epoch #100 | Computing loss after
2022-04-23 14:19:50 | [train_policy] epoch #100 | Fitting baseline...
2022-04-23 14:19:50 | [train_policy] epoch #100 | Saving snapshot...
2022-04-23 14:19:50 | [train_policy] epoch #100 | Saved
2022-04-23 14:19:50 | [train_policy] epoch #100 | Time 38.18 s
2022-04-23 14:19:50 | [train_policy] epoch #100 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.116542
Evaluation/AverageDiscountedReturn          -69.4413
Evaluation/AverageReturn                    -69.4413
Evaluation/CompletionRate                     0
Evaluation/Iteration                        100
Evaluation/MaxReturn                        -33.7118
Evaluation/MinReturn                      -2056.39
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.364
Extras/EpisodeRewardMean                    -67.4021
LinearFeatureBaseline/ExplainedVariance       0.00927451
PolicyExecTime                                0.102129
ProcessExecTime                               0.0116501
TotalEnvSteps                            102212
policy/Entropy                                1.2178
policy/KL                                     0.00670558
policy/KLBefore                               0
policy/LossAfter                             -0.0149736
policy/LossBefore                             1.0366e-08
policy/Perplexity                             3.37976
policy/dLoss                                  0.0149736
---------------------------------------  ---------------
2022-04-23 14:19:50 | [train_policy] epoch #101 | Obtaining samples for iteration 101...
2022-04-23 14:19:50 | [train_policy] epoch #101 | Logging diagnostics...
2022-04-23 14:19:50 | [train_policy] epoch #101 | Optimizing policy...
2022-04-23 14:19:50 | [train_policy] epoch #101 | Computing loss before
2022-04-23 14:19:50 | [train_policy] epoch #101 | Computing KL before
2022-04-23 14:19:50 | [train_policy] epoch #101 | Optimizing
2022-04-23 14:19:50 | [train_policy] epoch #101 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:50 | [train_policy] epoch #101 | computing loss before
2022-04-23 14:19:50 | [train_policy] epoch #101 | computing gradient
2022-04-23 14:19:50 | [train_policy] epoch #101 | gradient computed
2022-04-23 14:19:50 | [train_policy] epoch #101 | computing descent direction
2022-04-23 14:19:50 | [train_policy] epoch #101 | descent direction computed
2022-04-23 14:19:50 | [train_policy] epoch #101 | backtrack iters: 0
2022-04-23 14:19:50 | [train_policy] epoch #101 | optimization finished
2022-04-23 14:19:50 | [train_policy] epoch #101 | Computing KL after
2022-04-23 14:19:50 | [train_policy] epoch #101 | Computing loss after
2022-04-23 14:19:50 | [train_policy] epoch #101 | Fitting baseline...
2022-04-23 14:19:50 | [train_policy] epoch #101 | Saving snapshot...
2022-04-23 14:19:50 | [train_policy] epoch #101 | Saved
2022-04-23 14:19:50 | [train_policy] epoch #101 | Time 38.54 s
2022-04-23 14:19:50 | [train_policy] epoch #101 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118383
Evaluation/AverageDiscountedReturn          -67.8673
Evaluation/AverageReturn                    -67.8673
Evaluation/CompletionRate                     0
Evaluation/Iteration                        101
Evaluation/MaxReturn                        -32.9566
Evaluation/MinReturn                      -2063.96
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.303
Extras/EpisodeRewardMean                    -86.6253
LinearFeatureBaseline/ExplainedVariance       0.1016
PolicyExecTime                                0.113823
ProcessExecTime                               0.0120926
TotalEnvSteps                            103224
policy/Entropy                                1.27933
policy/KL                                     0.00934736
policy/KLBefore                               0
policy/LossAfter                             -0.0203459
policy/LossBefore                             9.42366e-10
policy/Perplexity                             3.59423
policy/dLoss                                  0.0203459
---------------------------------------  ----------------
2022-04-23 14:19:50 | [train_policy] epoch #102 | Obtaining samples for iteration 102...
2022-04-23 14:19:51 | [train_policy] epoch #102 | Logging diagnostics...
2022-04-23 14:19:51 | [train_policy] epoch #102 | Optimizing policy...
2022-04-23 14:19:51 | [train_policy] epoch #102 | Computing loss before
2022-04-23 14:19:51 | [train_policy] epoch #102 | Computing KL before
2022-04-23 14:19:51 | [train_policy] epoch #102 | Optimizing
2022-04-23 14:19:51 | [train_policy] epoch #102 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:51 | [train_policy] epoch #102 | computing loss before
2022-04-23 14:19:51 | [train_policy] epoch #102 | computing gradient
2022-04-23 14:19:51 | [train_policy] epoch #102 | gradient computed
2022-04-23 14:19:51 | [train_policy] epoch #102 | computing descent direction
2022-04-23 14:19:51 | [train_policy] epoch #102 | descent direction computed
2022-04-23 14:19:51 | [train_policy] epoch #102 | backtrack iters: 1
2022-04-23 14:19:51 | [train_policy] epoch #102 | optimization finished
2022-04-23 14:19:51 | [train_policy] epoch #102 | Computing KL after
2022-04-23 14:19:51 | [train_policy] epoch #102 | Computing loss after
2022-04-23 14:19:51 | [train_policy] epoch #102 | Fitting baseline...
2022-04-23 14:19:51 | [train_policy] epoch #102 | Saving snapshot...
2022-04-23 14:19:51 | [train_policy] epoch #102 | Saved
2022-04-23 14:19:51 | [train_policy] epoch #102 | Time 38.88 s
2022-04-23 14:19:51 | [train_policy] epoch #102 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116536
Evaluation/AverageDiscountedReturn          -92.5518
Evaluation/AverageReturn                    -92.5518
Evaluation/CompletionRate                     0
Evaluation/Iteration                        102
Evaluation/MaxReturn                        -39.33
Evaluation/MinReturn                      -2064.62
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.068
Extras/EpisodeRewardMean                    -88.7628
LinearFeatureBaseline/ExplainedVariance       0.184642
PolicyExecTime                                0.103267
ProcessExecTime                               0.0113623
TotalEnvSteps                            104236
policy/Entropy                                1.31716
policy/KL                                     0.00758254
policy/KLBefore                               0
policy/LossAfter                             -0.0200462
policy/LossBefore                             7.77452e-09
policy/Perplexity                             3.7328
policy/dLoss                                  0.0200462
---------------------------------------  ----------------
2022-04-23 14:19:51 | [train_policy] epoch #103 | Obtaining samples for iteration 103...
2022-04-23 14:19:51 | [train_policy] epoch #103 | Logging diagnostics...
2022-04-23 14:19:51 | [train_policy] epoch #103 | Optimizing policy...
2022-04-23 14:19:51 | [train_policy] epoch #103 | Computing loss before
2022-04-23 14:19:51 | [train_policy] epoch #103 | Computing KL before
2022-04-23 14:19:51 | [train_policy] epoch #103 | Optimizing
2022-04-23 14:19:51 | [train_policy] epoch #103 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:51 | [train_policy] epoch #103 | computing loss before
2022-04-23 14:19:51 | [train_policy] epoch #103 | computing gradient
2022-04-23 14:19:51 | [train_policy] epoch #103 | gradient computed
2022-04-23 14:19:51 | [train_policy] epoch #103 | computing descent direction
2022-04-23 14:19:51 | [train_policy] epoch #103 | descent direction computed
2022-04-23 14:19:51 | [train_policy] epoch #103 | backtrack iters: 1
2022-04-23 14:19:51 | [train_policy] epoch #103 | optimization finished
2022-04-23 14:19:51 | [train_policy] epoch #103 | Computing KL after
2022-04-23 14:19:51 | [train_policy] epoch #103 | Computing loss after
2022-04-23 14:19:51 | [train_policy] epoch #103 | Fitting baseline...
2022-04-23 14:19:51 | [train_policy] epoch #103 | Saving snapshot...
2022-04-23 14:19:51 | [train_policy] epoch #103 | Saved
2022-04-23 14:19:51 | [train_policy] epoch #103 | Time 39.23 s
2022-04-23 14:19:51 | [train_policy] epoch #103 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116333
Evaluation/AverageDiscountedReturn         -201.775
Evaluation/AverageReturn                   -201.775
Evaluation/CompletionRate                     0
Evaluation/Iteration                        103
Evaluation/MaxReturn                        -36.0383
Evaluation/MinReturn                      -4052.51
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        612.176
Extras/EpisodeRewardMean                   -189.283
LinearFeatureBaseline/ExplainedVariance       0.229944
PolicyExecTime                                0.109614
ProcessExecTime                               0.011225
TotalEnvSteps                            105248
policy/Entropy                                1.35333
policy/KL                                     0.00731192
policy/KLBefore                               0
policy/LossAfter                             -0.0235973
policy/LossBefore                             7.18554e-09
policy/Perplexity                             3.87027
policy/dLoss                                  0.0235973
---------------------------------------  ----------------
2022-04-23 14:19:51 | [train_policy] epoch #104 | Obtaining samples for iteration 104...
2022-04-23 14:19:51 | [train_policy] epoch #104 | Logging diagnostics...
2022-04-23 14:19:51 | [train_policy] epoch #104 | Optimizing policy...
2022-04-23 14:19:51 | [train_policy] epoch #104 | Computing loss before
2022-04-23 14:19:51 | [train_policy] epoch #104 | Computing KL before
2022-04-23 14:19:51 | [train_policy] epoch #104 | Optimizing
2022-04-23 14:19:51 | [train_policy] epoch #104 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:51 | [train_policy] epoch #104 | computing loss before
2022-04-23 14:19:51 | [train_policy] epoch #104 | computing gradient
2022-04-23 14:19:51 | [train_policy] epoch #104 | gradient computed
2022-04-23 14:19:51 | [train_policy] epoch #104 | computing descent direction
2022-04-23 14:19:51 | [train_policy] epoch #104 | descent direction computed
2022-04-23 14:19:51 | [train_policy] epoch #104 | backtrack iters: 1
2022-04-23 14:19:51 | [train_policy] epoch #104 | optimization finished
2022-04-23 14:19:51 | [train_policy] epoch #104 | Computing KL after
2022-04-23 14:19:51 | [train_policy] epoch #104 | Computing loss after
2022-04-23 14:19:51 | [train_policy] epoch #104 | Fitting baseline...
2022-04-23 14:19:51 | [train_policy] epoch #104 | Saving snapshot...
2022-04-23 14:19:51 | [train_policy] epoch #104 | Saved
2022-04-23 14:19:51 | [train_policy] epoch #104 | Time 39.59 s
2022-04-23 14:19:51 | [train_policy] epoch #104 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.124102
Evaluation/AverageDiscountedReturn          -69.5737
Evaluation/AverageReturn                    -69.5737
Evaluation/CompletionRate                     0
Evaluation/Iteration                        104
Evaluation/MaxReturn                        -33.189
Evaluation/MinReturn                      -2061.53
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.906
Extras/EpisodeRewardMean                   -108.062
LinearFeatureBaseline/ExplainedVariance      -0.832523
PolicyExecTime                                0.107584
ProcessExecTime                               0.0120742
TotalEnvSteps                            106260
policy/Entropy                                1.35763
policy/KL                                     0.00779783
policy/KLBefore                               0
policy/LossAfter                             -0.0176856
policy/LossBefore                             1.13084e-08
policy/Perplexity                             3.88699
policy/dLoss                                  0.0176856
---------------------------------------  ----------------
2022-04-23 14:19:51 | [train_policy] epoch #105 | Obtaining samples for iteration 105...
2022-04-23 14:19:52 | [train_policy] epoch #105 | Logging diagnostics...
2022-04-23 14:19:52 | [train_policy] epoch #105 | Optimizing policy...
2022-04-23 14:19:52 | [train_policy] epoch #105 | Computing loss before
2022-04-23 14:19:52 | [train_policy] epoch #105 | Computing KL before
2022-04-23 14:19:52 | [train_policy] epoch #105 | Optimizing
2022-04-23 14:19:52 | [train_policy] epoch #105 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:52 | [train_policy] epoch #105 | computing loss before
2022-04-23 14:19:52 | [train_policy] epoch #105 | computing gradient
2022-04-23 14:19:52 | [train_policy] epoch #105 | gradient computed
2022-04-23 14:19:52 | [train_policy] epoch #105 | computing descent direction
2022-04-23 14:19:52 | [train_policy] epoch #105 | descent direction computed
2022-04-23 14:19:52 | [train_policy] epoch #105 | backtrack iters: 1
2022-04-23 14:19:52 | [train_policy] epoch #105 | optimization finished
2022-04-23 14:19:52 | [train_policy] epoch #105 | Computing KL after
2022-04-23 14:19:52 | [train_policy] epoch #105 | Computing loss after
2022-04-23 14:19:52 | [train_policy] epoch #105 | Fitting baseline...
2022-04-23 14:19:52 | [train_policy] epoch #105 | Saving snapshot...
2022-04-23 14:19:52 | [train_policy] epoch #105 | Saved
2022-04-23 14:19:52 | [train_policy] epoch #105 | Time 39.95 s
2022-04-23 14:19:52 | [train_policy] epoch #105 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.123564
Evaluation/AverageDiscountedReturn          -47.2862
Evaluation/AverageReturn                    -47.2862
Evaluation/CompletionRate                     0
Evaluation/Iteration                        105
Evaluation/MaxReturn                        -34.7788
Evaluation/MinReturn                        -66.0808
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.49301
Extras/EpisodeRewardMean                    -67.3185
LinearFeatureBaseline/ExplainedVariance     -21.9199
PolicyExecTime                                0.110882
ProcessExecTime                               0.0122831
TotalEnvSteps                            107272
policy/Entropy                                1.34093
policy/KL                                     0.00661823
policy/KLBefore                               0
policy/LossAfter                             -0.0179094
policy/LossBefore                            -1.1544e-08
policy/Perplexity                             3.82258
policy/dLoss                                  0.0179094
---------------------------------------  ---------------
2022-04-23 14:19:52 | [train_policy] epoch #106 | Obtaining samples for iteration 106...
2022-04-23 14:19:52 | [train_policy] epoch #106 | Logging diagnostics...
2022-04-23 14:19:52 | [train_policy] epoch #106 | Optimizing policy...
2022-04-23 14:19:52 | [train_policy] epoch #106 | Computing loss before
2022-04-23 14:19:52 | [train_policy] epoch #106 | Computing KL before
2022-04-23 14:19:52 | [train_policy] epoch #106 | Optimizing
2022-04-23 14:19:52 | [train_policy] epoch #106 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:52 | [train_policy] epoch #106 | computing loss before
2022-04-23 14:19:52 | [train_policy] epoch #106 | computing gradient
2022-04-23 14:19:52 | [train_policy] epoch #106 | gradient computed
2022-04-23 14:19:52 | [train_policy] epoch #106 | computing descent direction
2022-04-23 14:19:52 | [train_policy] epoch #106 | descent direction computed
2022-04-23 14:19:52 | [train_policy] epoch #106 | backtrack iters: 0
2022-04-23 14:19:52 | [train_policy] epoch #106 | optimization finished
2022-04-23 14:19:52 | [train_policy] epoch #106 | Computing KL after
2022-04-23 14:19:52 | [train_policy] epoch #106 | Computing loss after
2022-04-23 14:19:52 | [train_policy] epoch #106 | Fitting baseline...
2022-04-23 14:19:52 | [train_policy] epoch #106 | Saving snapshot...
2022-04-23 14:19:52 | [train_policy] epoch #106 | Saved
2022-04-23 14:19:52 | [train_policy] epoch #106 | Time 40.31 s
2022-04-23 14:19:52 | [train_policy] epoch #106 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.121875
Evaluation/AverageDiscountedReturn         -115.588
Evaluation/AverageReturn                   -115.588
Evaluation/CompletionRate                     0
Evaluation/Iteration                        106
Evaluation/MaxReturn                        -35.0806
Evaluation/MinReturn                      -2074.66
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        358.688
Extras/EpisodeRewardMean                   -109.95
LinearFeatureBaseline/ExplainedVariance       0.00771575
PolicyExecTime                                0.11232
ProcessExecTime                               0.0118647
TotalEnvSteps                            108284
policy/Entropy                                1.36173
policy/KL                                     0.00809534
policy/KLBefore                               0
policy/LossAfter                             -0.0177585
policy/LossBefore                             1.88473e-08
policy/Perplexity                             3.90293
policy/dLoss                                  0.0177585
---------------------------------------  ----------------
2022-04-23 14:19:52 | [train_policy] epoch #107 | Obtaining samples for iteration 107...
2022-04-23 14:19:52 | [train_policy] epoch #107 | Logging diagnostics...
2022-04-23 14:19:52 | [train_policy] epoch #107 | Optimizing policy...
2022-04-23 14:19:52 | [train_policy] epoch #107 | Computing loss before
2022-04-23 14:19:52 | [train_policy] epoch #107 | Computing KL before
2022-04-23 14:19:52 | [train_policy] epoch #107 | Optimizing
2022-04-23 14:19:52 | [train_policy] epoch #107 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:52 | [train_policy] epoch #107 | computing loss before
2022-04-23 14:19:52 | [train_policy] epoch #107 | computing gradient
2022-04-23 14:19:52 | [train_policy] epoch #107 | gradient computed
2022-04-23 14:19:52 | [train_policy] epoch #107 | computing descent direction
2022-04-23 14:19:52 | [train_policy] epoch #107 | descent direction computed
2022-04-23 14:19:52 | [train_policy] epoch #107 | backtrack iters: 1
2022-04-23 14:19:52 | [train_policy] epoch #107 | optimization finished
2022-04-23 14:19:52 | [train_policy] epoch #107 | Computing KL after
2022-04-23 14:19:52 | [train_policy] epoch #107 | Computing loss after
2022-04-23 14:19:52 | [train_policy] epoch #107 | Fitting baseline...
2022-04-23 14:19:52 | [train_policy] epoch #107 | Saving snapshot...
2022-04-23 14:19:52 | [train_policy] epoch #107 | Saved
2022-04-23 14:19:52 | [train_policy] epoch #107 | Time 40.67 s
2022-04-23 14:19:52 | [train_policy] epoch #107 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118414
Evaluation/AverageDiscountedReturn          -71.6842
Evaluation/AverageReturn                    -71.6842
Evaluation/CompletionRate                     0
Evaluation/Iteration                        107
Evaluation/MaxReturn                        -38.7003
Evaluation/MinReturn                      -2056.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.572
Extras/EpisodeRewardMean                    -69.7617
LinearFeatureBaseline/ExplainedVariance      -0.223432
PolicyExecTime                                0.113372
ProcessExecTime                               0.0115702
TotalEnvSteps                            109296
policy/Entropy                                1.33935
policy/KL                                     0.00945691
policy/KLBefore                               0
policy/LossAfter                             -0.0173837
policy/LossBefore                             8.01011e-09
policy/Perplexity                             3.81658
policy/dLoss                                  0.0173837
---------------------------------------  ----------------
2022-04-23 14:19:52 | [train_policy] epoch #108 | Obtaining samples for iteration 108...
2022-04-23 14:19:53 | [train_policy] epoch #108 | Logging diagnostics...
2022-04-23 14:19:53 | [train_policy] epoch #108 | Optimizing policy...
2022-04-23 14:19:53 | [train_policy] epoch #108 | Computing loss before
2022-04-23 14:19:53 | [train_policy] epoch #108 | Computing KL before
2022-04-23 14:19:53 | [train_policy] epoch #108 | Optimizing
2022-04-23 14:19:53 | [train_policy] epoch #108 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:53 | [train_policy] epoch #108 | computing loss before
2022-04-23 14:19:53 | [train_policy] epoch #108 | computing gradient
2022-04-23 14:19:53 | [train_policy] epoch #108 | gradient computed
2022-04-23 14:19:53 | [train_policy] epoch #108 | computing descent direction
2022-04-23 14:19:53 | [train_policy] epoch #108 | descent direction computed
2022-04-23 14:19:53 | [train_policy] epoch #108 | backtrack iters: 0
2022-04-23 14:19:53 | [train_policy] epoch #108 | optimization finished
2022-04-23 14:19:53 | [train_policy] epoch #108 | Computing KL after
2022-04-23 14:19:53 | [train_policy] epoch #108 | Computing loss after
2022-04-23 14:19:53 | [train_policy] epoch #108 | Fitting baseline...
2022-04-23 14:19:53 | [train_policy] epoch #108 | Saving snapshot...
2022-04-23 14:19:53 | [train_policy] epoch #108 | Saved
2022-04-23 14:19:53 | [train_policy] epoch #108 | Time 41.01 s
2022-04-23 14:19:53 | [train_policy] epoch #108 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116189
Evaluation/AverageDiscountedReturn          -69.5686
Evaluation/AverageReturn                    -69.5686
Evaluation/CompletionRate                     0
Evaluation/Iteration                        108
Evaluation/MaxReturn                        -39.2841
Evaluation/MinReturn                      -2052.86
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        207.987
Extras/EpisodeRewardMean                    -67.7087
LinearFeatureBaseline/ExplainedVariance       0.0819833
PolicyExecTime                                0.106125
ProcessExecTime                               0.0114279
TotalEnvSteps                            110308
policy/Entropy                                1.30255
policy/KL                                     0.00980263
policy/KLBefore                               0
policy/LossAfter                             -0.0183243
policy/LossBefore                             1.41355e-08
policy/Perplexity                             3.67867
policy/dLoss                                  0.0183243
---------------------------------------  ----------------
2022-04-23 14:19:53 | [train_policy] epoch #109 | Obtaining samples for iteration 109...
2022-04-23 14:19:53 | [train_policy] epoch #109 | Logging diagnostics...
2022-04-23 14:19:53 | [train_policy] epoch #109 | Optimizing policy...
2022-04-23 14:19:53 | [train_policy] epoch #109 | Computing loss before
2022-04-23 14:19:53 | [train_policy] epoch #109 | Computing KL before
2022-04-23 14:19:53 | [train_policy] epoch #109 | Optimizing
2022-04-23 14:19:53 | [train_policy] epoch #109 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:53 | [train_policy] epoch #109 | computing loss before
2022-04-23 14:19:53 | [train_policy] epoch #109 | computing gradient
2022-04-23 14:19:53 | [train_policy] epoch #109 | gradient computed
2022-04-23 14:19:53 | [train_policy] epoch #109 | computing descent direction
2022-04-23 14:19:53 | [train_policy] epoch #109 | descent direction computed
2022-04-23 14:19:53 | [train_policy] epoch #109 | backtrack iters: 0
2022-04-23 14:19:53 | [train_policy] epoch #109 | optimization finished
2022-04-23 14:19:53 | [train_policy] epoch #109 | Computing KL after
2022-04-23 14:19:53 | [train_policy] epoch #109 | Computing loss after
2022-04-23 14:19:53 | [train_policy] epoch #109 | Fitting baseline...
2022-04-23 14:19:53 | [train_policy] epoch #109 | Saving snapshot...
2022-04-23 14:19:53 | [train_policy] epoch #109 | Saved
2022-04-23 14:19:53 | [train_policy] epoch #109 | Time 41.36 s
2022-04-23 14:19:53 | [train_policy] epoch #109 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115821
Evaluation/AverageDiscountedReturn          -70.1557
Evaluation/AverageReturn                    -70.1557
Evaluation/CompletionRate                     0
Evaluation/Iteration                        109
Evaluation/MaxReturn                        -35.7872
Evaluation/MinReturn                      -2063.45
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.063
Extras/EpisodeRewardMean                    -88.393
LinearFeatureBaseline/ExplainedVariance       0.0622198
PolicyExecTime                                0.106569
ProcessExecTime                               0.0114083
TotalEnvSteps                            111320
policy/Entropy                                1.31734
policy/KL                                     0.00989918
policy/KLBefore                               0
policy/LossAfter                             -0.0139869
policy/LossBefore                             3.29828e-09
policy/Perplexity                             3.73347
policy/dLoss                                  0.0139869
---------------------------------------  ----------------
2022-04-23 14:19:53 | [train_policy] epoch #110 | Obtaining samples for iteration 110...
2022-04-23 14:19:53 | [train_policy] epoch #110 | Logging diagnostics...
2022-04-23 14:19:53 | [train_policy] epoch #110 | Optimizing policy...
2022-04-23 14:19:53 | [train_policy] epoch #110 | Computing loss before
2022-04-23 14:19:53 | [train_policy] epoch #110 | Computing KL before
2022-04-23 14:19:53 | [train_policy] epoch #110 | Optimizing
2022-04-23 14:19:53 | [train_policy] epoch #110 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:53 | [train_policy] epoch #110 | computing loss before
2022-04-23 14:19:53 | [train_policy] epoch #110 | computing gradient
2022-04-23 14:19:53 | [train_policy] epoch #110 | gradient computed
2022-04-23 14:19:53 | [train_policy] epoch #110 | computing descent direction
2022-04-23 14:19:53 | [train_policy] epoch #110 | descent direction computed
2022-04-23 14:19:53 | [train_policy] epoch #110 | backtrack iters: 0
2022-04-23 14:19:53 | [train_policy] epoch #110 | optimization finished
2022-04-23 14:19:53 | [train_policy] epoch #110 | Computing KL after
2022-04-23 14:19:53 | [train_policy] epoch #110 | Computing loss after
2022-04-23 14:19:53 | [train_policy] epoch #110 | Fitting baseline...
2022-04-23 14:19:53 | [train_policy] epoch #110 | Saving snapshot...
2022-04-23 14:19:53 | [train_policy] epoch #110 | Saved
2022-04-23 14:19:53 | [train_policy] epoch #110 | Time 41.70 s
2022-04-23 14:19:53 | [train_policy] epoch #110 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11591
Evaluation/AverageDiscountedReturn          -70.3815
Evaluation/AverageReturn                    -70.3815
Evaluation/CompletionRate                     0
Evaluation/Iteration                        110
Evaluation/MaxReturn                        -34.3145
Evaluation/MinReturn                      -2061.84
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.93
Extras/EpisodeRewardMean                    -68.5124
LinearFeatureBaseline/ExplainedVariance       0.108158
PolicyExecTime                                0.105807
ProcessExecTime                               0.0113454
TotalEnvSteps                            112332
policy/Entropy                                1.35156
policy/KL                                     0.00872033
policy/KLBefore                               0
policy/LossAfter                             -0.0224117
policy/LossBefore                             4.24065e-09
policy/Perplexity                             3.86344
policy/dLoss                                  0.0224117
---------------------------------------  ----------------
2022-04-23 14:19:53 | [train_policy] epoch #111 | Obtaining samples for iteration 111...
2022-04-23 14:19:54 | [train_policy] epoch #111 | Logging diagnostics...
2022-04-23 14:19:54 | [train_policy] epoch #111 | Optimizing policy...
2022-04-23 14:19:54 | [train_policy] epoch #111 | Computing loss before
2022-04-23 14:19:54 | [train_policy] epoch #111 | Computing KL before
2022-04-23 14:19:54 | [train_policy] epoch #111 | Optimizing
2022-04-23 14:19:54 | [train_policy] epoch #111 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:54 | [train_policy] epoch #111 | computing loss before
2022-04-23 14:19:54 | [train_policy] epoch #111 | computing gradient
2022-04-23 14:19:54 | [train_policy] epoch #111 | gradient computed
2022-04-23 14:19:54 | [train_policy] epoch #111 | computing descent direction
2022-04-23 14:19:54 | [train_policy] epoch #111 | descent direction computed
2022-04-23 14:19:54 | [train_policy] epoch #111 | backtrack iters: 1
2022-04-23 14:19:54 | [train_policy] epoch #111 | optimization finished
2022-04-23 14:19:54 | [train_policy] epoch #111 | Computing KL after
2022-04-23 14:19:54 | [train_policy] epoch #111 | Computing loss after
2022-04-23 14:19:54 | [train_policy] epoch #111 | Fitting baseline...
2022-04-23 14:19:54 | [train_policy] epoch #111 | Saving snapshot...
2022-04-23 14:19:54 | [train_policy] epoch #111 | Saved
2022-04-23 14:19:54 | [train_policy] epoch #111 | Time 42.05 s
2022-04-23 14:19:54 | [train_policy] epoch #111 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116696
Evaluation/AverageDiscountedReturn          -49.9876
Evaluation/AverageReturn                    -49.9876
Evaluation/CompletionRate                     0
Evaluation/Iteration                        111
Evaluation/MaxReturn                        -38.4325
Evaluation/MinReturn                        -86.8334
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.17072
Extras/EpisodeRewardMean                    -49.5968
LinearFeatureBaseline/ExplainedVariance     -14.7844
PolicyExecTime                                0.107205
ProcessExecTime                               0.0115275
TotalEnvSteps                            113344
policy/Entropy                                1.31669
policy/KL                                     0.00820334
policy/KLBefore                               0
policy/LossAfter                             -0.011502
policy/LossBefore                            -1.60202e-08
policy/Perplexity                             3.73105
policy/dLoss                                  0.011502
---------------------------------------  ----------------
2022-04-23 14:19:54 | [train_policy] epoch #112 | Obtaining samples for iteration 112...
2022-04-23 14:19:54 | [train_policy] epoch #112 | Logging diagnostics...
2022-04-23 14:19:54 | [train_policy] epoch #112 | Optimizing policy...
2022-04-23 14:19:54 | [train_policy] epoch #112 | Computing loss before
2022-04-23 14:19:54 | [train_policy] epoch #112 | Computing KL before
2022-04-23 14:19:54 | [train_policy] epoch #112 | Optimizing
2022-04-23 14:19:54 | [train_policy] epoch #112 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:54 | [train_policy] epoch #112 | computing loss before
2022-04-23 14:19:54 | [train_policy] epoch #112 | computing gradient
2022-04-23 14:19:54 | [train_policy] epoch #112 | gradient computed
2022-04-23 14:19:54 | [train_policy] epoch #112 | computing descent direction
2022-04-23 14:19:54 | [train_policy] epoch #112 | descent direction computed
2022-04-23 14:19:54 | [train_policy] epoch #112 | backtrack iters: 0
2022-04-23 14:19:54 | [train_policy] epoch #112 | optimization finished
2022-04-23 14:19:54 | [train_policy] epoch #112 | Computing KL after
2022-04-23 14:19:54 | [train_policy] epoch #112 | Computing loss after
2022-04-23 14:19:54 | [train_policy] epoch #112 | Fitting baseline...
2022-04-23 14:19:54 | [train_policy] epoch #112 | Saving snapshot...
2022-04-23 14:19:54 | [train_policy] epoch #112 | Saved
2022-04-23 14:19:54 | [train_policy] epoch #112 | Time 42.39 s
2022-04-23 14:19:54 | [train_policy] epoch #112 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115907
Evaluation/AverageDiscountedReturn          -59.8475
Evaluation/AverageReturn                    -59.8475
Evaluation/CompletionRate                     0
Evaluation/Iteration                        112
Evaluation/MaxReturn                        -31.5788
Evaluation/MinReturn                      -1003.6
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         99.7112
Extras/EpisodeRewardMean                    -59.0407
LinearFeatureBaseline/ExplainedVariance       0.0409176
PolicyExecTime                                0.106733
ProcessExecTime                               0.0113111
TotalEnvSteps                            114356
policy/Entropy                                1.27886
policy/KL                                     0.0098946
policy/KLBefore                               0
policy/LossAfter                             -0.0245567
policy/LossBefore                             9.42366e-10
policy/Perplexity                             3.59254
policy/dLoss                                  0.0245567
---------------------------------------  ----------------
2022-04-23 14:19:54 | [train_policy] epoch #113 | Obtaining samples for iteration 113...
2022-04-23 14:19:54 | [train_policy] epoch #113 | Logging diagnostics...
2022-04-23 14:19:54 | [train_policy] epoch #113 | Optimizing policy...
2022-04-23 14:19:54 | [train_policy] epoch #113 | Computing loss before
2022-04-23 14:19:54 | [train_policy] epoch #113 | Computing KL before
2022-04-23 14:19:54 | [train_policy] epoch #113 | Optimizing
2022-04-23 14:19:54 | [train_policy] epoch #113 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:54 | [train_policy] epoch #113 | computing loss before
2022-04-23 14:19:54 | [train_policy] epoch #113 | computing gradient
2022-04-23 14:19:54 | [train_policy] epoch #113 | gradient computed
2022-04-23 14:19:54 | [train_policy] epoch #113 | computing descent direction
2022-04-23 14:19:55 | [train_policy] epoch #113 | descent direction computed
2022-04-23 14:19:55 | [train_policy] epoch #113 | backtrack iters: 1
2022-04-23 14:19:55 | [train_policy] epoch #113 | optimization finished
2022-04-23 14:19:55 | [train_policy] epoch #113 | Computing KL after
2022-04-23 14:19:55 | [train_policy] epoch #113 | Computing loss after
2022-04-23 14:19:55 | [train_policy] epoch #113 | Fitting baseline...
2022-04-23 14:19:55 | [train_policy] epoch #113 | Saving snapshot...
2022-04-23 14:19:55 | [train_policy] epoch #113 | Saved
2022-04-23 14:19:55 | [train_policy] epoch #113 | Time 42.74 s
2022-04-23 14:19:55 | [train_policy] epoch #113 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.115862
Evaluation/AverageDiscountedReturn          -49.8557
Evaluation/AverageReturn                    -49.8557
Evaluation/CompletionRate                     0
Evaluation/Iteration                        113
Evaluation/MaxReturn                        -35.1847
Evaluation/MinReturn                       -131.029
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.279
Extras/EpisodeRewardMean                    -50.4238
LinearFeatureBaseline/ExplainedVariance      -3.16105
PolicyExecTime                                0.113768
ProcessExecTime                               0.0112233
TotalEnvSteps                            115368
policy/Entropy                                1.2701
policy/KL                                     0.00659285
policy/KLBefore                               0
policy/LossAfter                             -0.0211596
policy/LossBefore                            -2.3088e-08
policy/Perplexity                             3.56122
policy/dLoss                                  0.0211595
---------------------------------------  ---------------
2022-04-23 14:19:55 | [train_policy] epoch #114 | Obtaining samples for iteration 114...
2022-04-23 14:19:55 | [train_policy] epoch #114 | Logging diagnostics...
2022-04-23 14:19:55 | [train_policy] epoch #114 | Optimizing policy...
2022-04-23 14:19:55 | [train_policy] epoch #114 | Computing loss before
2022-04-23 14:19:55 | [train_policy] epoch #114 | Computing KL before
2022-04-23 14:19:55 | [train_policy] epoch #114 | Optimizing
2022-04-23 14:19:55 | [train_policy] epoch #114 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:55 | [train_policy] epoch #114 | computing loss before
2022-04-23 14:19:55 | [train_policy] epoch #114 | computing gradient
2022-04-23 14:19:55 | [train_policy] epoch #114 | gradient computed
2022-04-23 14:19:55 | [train_policy] epoch #114 | computing descent direction
2022-04-23 14:19:55 | [train_policy] epoch #114 | descent direction computed
2022-04-23 14:19:55 | [train_policy] epoch #114 | backtrack iters: 1
2022-04-23 14:19:55 | [train_policy] epoch #114 | optimization finished
2022-04-23 14:19:55 | [train_policy] epoch #114 | Computing KL after
2022-04-23 14:19:55 | [train_policy] epoch #114 | Computing loss after
2022-04-23 14:19:55 | [train_policy] epoch #114 | Fitting baseline...
2022-04-23 14:19:55 | [train_policy] epoch #114 | Saving snapshot...
2022-04-23 14:19:55 | [train_policy] epoch #114 | Saved
2022-04-23 14:19:55 | [train_policy] epoch #114 | Time 43.09 s
2022-04-23 14:19:55 | [train_policy] epoch #114 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.115301
Evaluation/AverageDiscountedReturn          -48.2181
Evaluation/AverageReturn                    -48.2181
Evaluation/CompletionRate                     0
Evaluation/Iteration                        114
Evaluation/MaxReturn                        -37.9943
Evaluation/MinReturn                        -68.5349
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.79249
Extras/EpisodeRewardMean                    -48.4905
LinearFeatureBaseline/ExplainedVariance       0.905266
PolicyExecTime                                0.111036
ProcessExecTime                               0.0112569
TotalEnvSteps                            116380
policy/Entropy                                1.25153
policy/KL                                     0.00655289
policy/KLBefore                               0
policy/LossAfter                             -0.0156785
policy/LossBefore                            -8.59909e-09
policy/Perplexity                             3.49568
policy/dLoss                                  0.0156785
---------------------------------------  ----------------
2022-04-23 14:19:55 | [train_policy] epoch #115 | Obtaining samples for iteration 115...
2022-04-23 14:19:55 | [train_policy] epoch #115 | Logging diagnostics...
2022-04-23 14:19:55 | [train_policy] epoch #115 | Optimizing policy...
2022-04-23 14:19:55 | [train_policy] epoch #115 | Computing loss before
2022-04-23 14:19:55 | [train_policy] epoch #115 | Computing KL before
2022-04-23 14:19:55 | [train_policy] epoch #115 | Optimizing
2022-04-23 14:19:55 | [train_policy] epoch #115 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:55 | [train_policy] epoch #115 | computing loss before
2022-04-23 14:19:55 | [train_policy] epoch #115 | computing gradient
2022-04-23 14:19:55 | [train_policy] epoch #115 | gradient computed
2022-04-23 14:19:55 | [train_policy] epoch #115 | computing descent direction
2022-04-23 14:19:55 | [train_policy] epoch #115 | descent direction computed
2022-04-23 14:19:55 | [train_policy] epoch #115 | backtrack iters: 1
2022-04-23 14:19:55 | [train_policy] epoch #115 | optimization finished
2022-04-23 14:19:55 | [train_policy] epoch #115 | Computing KL after
2022-04-23 14:19:55 | [train_policy] epoch #115 | Computing loss after
2022-04-23 14:19:55 | [train_policy] epoch #115 | Fitting baseline...
2022-04-23 14:19:55 | [train_policy] epoch #115 | Saving snapshot...
2022-04-23 14:19:55 | [train_policy] epoch #115 | Saved
2022-04-23 14:19:55 | [train_policy] epoch #115 | Time 43.44 s
2022-04-23 14:19:55 | [train_policy] epoch #115 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116175
Evaluation/AverageDiscountedReturn          -92.6326
Evaluation/AverageReturn                    -92.6326
Evaluation/CompletionRate                     0
Evaluation/Iteration                        115
Evaluation/MaxReturn                        -36.9298
Evaluation/MinReturn                      -2063.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        293.981
Extras/EpisodeRewardMean                    -89.1328
LinearFeatureBaseline/ExplainedVariance       0.008767
PolicyExecTime                                0.103954
ProcessExecTime                               0.0112922
TotalEnvSteps                            117392
policy/Entropy                                1.22051
policy/KL                                     0.00647499
policy/KLBefore                               0
policy/LossAfter                             -0.0196812
policy/LossBefore                             4.24065e-09
policy/Perplexity                             3.38891
policy/dLoss                                  0.0196812
---------------------------------------  ----------------
2022-04-23 14:19:55 | [train_policy] epoch #116 | Obtaining samples for iteration 116...
2022-04-23 14:19:55 | [train_policy] epoch #116 | Logging diagnostics...
2022-04-23 14:19:55 | [train_policy] epoch #116 | Optimizing policy...
2022-04-23 14:19:55 | [train_policy] epoch #116 | Computing loss before
2022-04-23 14:19:55 | [train_policy] epoch #116 | Computing KL before
2022-04-23 14:19:55 | [train_policy] epoch #116 | Optimizing
2022-04-23 14:19:55 | [train_policy] epoch #116 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:55 | [train_policy] epoch #116 | computing loss before
2022-04-23 14:19:56 | [train_policy] epoch #116 | computing gradient
2022-04-23 14:19:56 | [train_policy] epoch #116 | gradient computed
2022-04-23 14:19:56 | [train_policy] epoch #116 | computing descent direction
2022-04-23 14:19:56 | [train_policy] epoch #116 | descent direction computed
2022-04-23 14:19:56 | [train_policy] epoch #116 | backtrack iters: 0
2022-04-23 14:19:56 | [train_policy] epoch #116 | optimization finished
2022-04-23 14:19:56 | [train_policy] epoch #116 | Computing KL after
2022-04-23 14:19:56 | [train_policy] epoch #116 | Computing loss after
2022-04-23 14:19:56 | [train_policy] epoch #116 | Fitting baseline...
2022-04-23 14:19:56 | [train_policy] epoch #116 | Saving snapshot...
2022-04-23 14:19:56 | [train_policy] epoch #116 | Saved
2022-04-23 14:19:56 | [train_policy] epoch #116 | Time 43.79 s
2022-04-23 14:19:56 | [train_policy] epoch #116 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118621
Evaluation/AverageDiscountedReturn          -69.9095
Evaluation/AverageReturn                    -69.9095
Evaluation/CompletionRate                     0
Evaluation/Iteration                        116
Evaluation/MaxReturn                        -35.2494
Evaluation/MinReturn                      -2061.43
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.93
Extras/EpisodeRewardMean                    -68.1645
LinearFeatureBaseline/ExplainedVariance       0.00314301
PolicyExecTime                                0.101792
ProcessExecTime                               0.0115607
TotalEnvSteps                            118404
policy/Entropy                                1.25806
policy/KL                                     0.00956271
policy/KLBefore                               0
policy/LossAfter                             -0.0160728
policy/LossBefore                             5.18301e-09
policy/Perplexity                             3.51858
policy/dLoss                                  0.0160728
---------------------------------------  ----------------
2022-04-23 14:19:56 | [train_policy] epoch #117 | Obtaining samples for iteration 117...
2022-04-23 14:19:56 | [train_policy] epoch #117 | Logging diagnostics...
2022-04-23 14:19:56 | [train_policy] epoch #117 | Optimizing policy...
2022-04-23 14:19:56 | [train_policy] epoch #117 | Computing loss before
2022-04-23 14:19:56 | [train_policy] epoch #117 | Computing KL before
2022-04-23 14:19:56 | [train_policy] epoch #117 | Optimizing
2022-04-23 14:19:56 | [train_policy] epoch #117 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:56 | [train_policy] epoch #117 | computing loss before
2022-04-23 14:19:56 | [train_policy] epoch #117 | computing gradient
2022-04-23 14:19:56 | [train_policy] epoch #117 | gradient computed
2022-04-23 14:19:56 | [train_policy] epoch #117 | computing descent direction
2022-04-23 14:19:56 | [train_policy] epoch #117 | descent direction computed
2022-04-23 14:19:56 | [train_policy] epoch #117 | backtrack iters: 0
2022-04-23 14:19:56 | [train_policy] epoch #117 | optimization finished
2022-04-23 14:19:56 | [train_policy] epoch #117 | Computing KL after
2022-04-23 14:19:56 | [train_policy] epoch #117 | Computing loss after
2022-04-23 14:19:56 | [train_policy] epoch #117 | Fitting baseline...
2022-04-23 14:19:56 | [train_policy] epoch #117 | Saving snapshot...
2022-04-23 14:19:56 | [train_policy] epoch #117 | Saved
2022-04-23 14:19:56 | [train_policy] epoch #117 | Time 44.15 s
2022-04-23 14:19:56 | [train_policy] epoch #117 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119274
Evaluation/AverageDiscountedReturn          -48.2711
Evaluation/AverageReturn                    -48.2711
Evaluation/CompletionRate                     0
Evaluation/Iteration                        117
Evaluation/MaxReturn                        -37.0581
Evaluation/MinReturn                       -115.753
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.7365
Extras/EpisodeRewardMean                    -48.1891
LinearFeatureBaseline/ExplainedVariance     -21.7408
PolicyExecTime                                0.112752
ProcessExecTime                               0.0119679
TotalEnvSteps                            119416
policy/Entropy                                1.26727
policy/KL                                     0.00908156
policy/KLBefore                               0
policy/LossAfter                             -0.0228508
policy/LossBefore                            -3.76946e-08
policy/Perplexity                             3.55113
policy/dLoss                                  0.0228508
---------------------------------------  ----------------
2022-04-23 14:19:56 | [train_policy] epoch #118 | Obtaining samples for iteration 118...
2022-04-23 14:19:56 | [train_policy] epoch #118 | Logging diagnostics...
2022-04-23 14:19:56 | [train_policy] epoch #118 | Optimizing policy...
2022-04-23 14:19:56 | [train_policy] epoch #118 | Computing loss before
2022-04-23 14:19:56 | [train_policy] epoch #118 | Computing KL before
2022-04-23 14:19:56 | [train_policy] epoch #118 | Optimizing
2022-04-23 14:19:56 | [train_policy] epoch #118 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:56 | [train_policy] epoch #118 | computing loss before
2022-04-23 14:19:56 | [train_policy] epoch #118 | computing gradient
2022-04-23 14:19:56 | [train_policy] epoch #118 | gradient computed
2022-04-23 14:19:56 | [train_policy] epoch #118 | computing descent direction
2022-04-23 14:19:56 | [train_policy] epoch #118 | descent direction computed
2022-04-23 14:19:56 | [train_policy] epoch #118 | backtrack iters: 0
2022-04-23 14:19:56 | [train_policy] epoch #118 | optimization finished
2022-04-23 14:19:56 | [train_policy] epoch #118 | Computing KL after
2022-04-23 14:19:56 | [train_policy] epoch #118 | Computing loss after
2022-04-23 14:19:56 | [train_policy] epoch #118 | Fitting baseline...
2022-04-23 14:19:56 | [train_policy] epoch #118 | Saving snapshot...
2022-04-23 14:19:56 | [train_policy] epoch #118 | Saved
2022-04-23 14:19:56 | [train_policy] epoch #118 | Time 44.49 s
2022-04-23 14:19:56 | [train_policy] epoch #118 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116879
Evaluation/AverageDiscountedReturn          -47.97
Evaluation/AverageReturn                    -47.97
Evaluation/CompletionRate                     0
Evaluation/Iteration                        118
Evaluation/MaxReturn                        -37.8524
Evaluation/MinReturn                        -65.3294
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.31998
Extras/EpisodeRewardMean                    -47.7533
LinearFeatureBaseline/ExplainedVariance       0.960242
PolicyExecTime                                0.105254
ProcessExecTime                               0.0115385
TotalEnvSteps                            120428
policy/Entropy                                1.23783
policy/KL                                     0.00890452
policy/KLBefore                               0
policy/LossAfter                             -0.0202592
policy/LossBefore                             1.27219e-08
policy/Perplexity                             3.44813
policy/dLoss                                  0.0202592
---------------------------------------  ----------------
2022-04-23 14:19:56 | [train_policy] epoch #119 | Obtaining samples for iteration 119...
2022-04-23 14:19:57 | [train_policy] epoch #119 | Logging diagnostics...
2022-04-23 14:19:57 | [train_policy] epoch #119 | Optimizing policy...
2022-04-23 14:19:57 | [train_policy] epoch #119 | Computing loss before
2022-04-23 14:19:57 | [train_policy] epoch #119 | Computing KL before
2022-04-23 14:19:57 | [train_policy] epoch #119 | Optimizing
2022-04-23 14:19:57 | [train_policy] epoch #119 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:57 | [train_policy] epoch #119 | computing loss before
2022-04-23 14:19:57 | [train_policy] epoch #119 | computing gradient
2022-04-23 14:19:57 | [train_policy] epoch #119 | gradient computed
2022-04-23 14:19:57 | [train_policy] epoch #119 | computing descent direction
2022-04-23 14:19:57 | [train_policy] epoch #119 | descent direction computed
2022-04-23 14:19:57 | [train_policy] epoch #119 | backtrack iters: 1
2022-04-23 14:19:57 | [train_policy] epoch #119 | optimization finished
2022-04-23 14:19:57 | [train_policy] epoch #119 | Computing KL after
2022-04-23 14:19:57 | [train_policy] epoch #119 | Computing loss after
2022-04-23 14:19:57 | [train_policy] epoch #119 | Fitting baseline...
2022-04-23 14:19:57 | [train_policy] epoch #119 | Saving snapshot...
2022-04-23 14:19:57 | [train_policy] epoch #119 | Saved
2022-04-23 14:19:57 | [train_policy] epoch #119 | Time 44.84 s
2022-04-23 14:19:57 | [train_policy] epoch #119 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116317
Evaluation/AverageDiscountedReturn          -48.3162
Evaluation/AverageReturn                    -48.3162
Evaluation/CompletionRate                     0
Evaluation/Iteration                        119
Evaluation/MaxReturn                        -37.579
Evaluation/MinReturn                        -72.48
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.36046
Extras/EpisodeRewardMean                    -48.3069
LinearFeatureBaseline/ExplainedVariance       0.933452
PolicyExecTime                                0.105823
ProcessExecTime                               0.0112665
TotalEnvSteps                            121440
policy/Entropy                                1.22723
policy/KL                                     0.00666951
policy/KLBefore                               0
policy/LossAfter                             -0.021525
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             3.41177
policy/dLoss                                  0.0215249
---------------------------------------  ----------------
2022-04-23 14:19:57 | [train_policy] epoch #120 | Obtaining samples for iteration 120...
2022-04-23 14:19:57 | [train_policy] epoch #120 | Logging diagnostics...
2022-04-23 14:19:57 | [train_policy] epoch #120 | Optimizing policy...
2022-04-23 14:19:57 | [train_policy] epoch #120 | Computing loss before
2022-04-23 14:19:57 | [train_policy] epoch #120 | Computing KL before
2022-04-23 14:19:57 | [train_policy] epoch #120 | Optimizing
2022-04-23 14:19:57 | [train_policy] epoch #120 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:57 | [train_policy] epoch #120 | computing loss before
2022-04-23 14:19:57 | [train_policy] epoch #120 | computing gradient
2022-04-23 14:19:57 | [train_policy] epoch #120 | gradient computed
2022-04-23 14:19:57 | [train_policy] epoch #120 | computing descent direction
2022-04-23 14:19:57 | [train_policy] epoch #120 | descent direction computed
2022-04-23 14:19:57 | [train_policy] epoch #120 | backtrack iters: 0
2022-04-23 14:19:57 | [train_policy] epoch #120 | optimization finished
2022-04-23 14:19:57 | [train_policy] epoch #120 | Computing KL after
2022-04-23 14:19:57 | [train_policy] epoch #120 | Computing loss after
2022-04-23 14:19:57 | [train_policy] epoch #120 | Fitting baseline...
2022-04-23 14:19:57 | [train_policy] epoch #120 | Saving snapshot...
2022-04-23 14:19:57 | [train_policy] epoch #120 | Saved
2022-04-23 14:19:57 | [train_policy] epoch #120 | Time 45.20 s
2022-04-23 14:19:57 | [train_policy] epoch #120 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.116469
Evaluation/AverageDiscountedReturn          -49.4516
Evaluation/AverageReturn                    -49.4516
Evaluation/CompletionRate                     0
Evaluation/Iteration                        120
Evaluation/MaxReturn                        -37.9399
Evaluation/MinReturn                       -184.45
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         19.6473
Extras/EpisodeRewardMean                    -49.4144
LinearFeatureBaseline/ExplainedVariance       0.492729
PolicyExecTime                                0.103554
ProcessExecTime                               0.0112314
TotalEnvSteps                            122452
policy/Entropy                                1.2549
policy/KL                                     0.00926017
policy/KLBefore                               0
policy/LossAfter                             -0.0186168
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             3.50749
policy/dLoss                                  0.0186168
---------------------------------------  ----------------
2022-04-23 14:19:57 | [train_policy] epoch #121 | Obtaining samples for iteration 121...
2022-04-23 14:19:57 | [train_policy] epoch #121 | Logging diagnostics...
2022-04-23 14:19:57 | [train_policy] epoch #121 | Optimizing policy...
2022-04-23 14:19:57 | [train_policy] epoch #121 | Computing loss before
2022-04-23 14:19:57 | [train_policy] epoch #121 | Computing KL before
2022-04-23 14:19:57 | [train_policy] epoch #121 | Optimizing
2022-04-23 14:19:57 | [train_policy] epoch #121 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:57 | [train_policy] epoch #121 | computing loss before
2022-04-23 14:19:57 | [train_policy] epoch #121 | computing gradient
2022-04-23 14:19:57 | [train_policy] epoch #121 | gradient computed
2022-04-23 14:19:57 | [train_policy] epoch #121 | computing descent direction
2022-04-23 14:19:57 | [train_policy] epoch #121 | descent direction computed
2022-04-23 14:19:57 | [train_policy] epoch #121 | backtrack iters: 0
2022-04-23 14:19:57 | [train_policy] epoch #121 | optimization finished
2022-04-23 14:19:57 | [train_policy] epoch #121 | Computing KL after
2022-04-23 14:19:57 | [train_policy] epoch #121 | Computing loss after
2022-04-23 14:19:57 | [train_policy] epoch #121 | Fitting baseline...
2022-04-23 14:19:57 | [train_policy] epoch #121 | Saving snapshot...
2022-04-23 14:19:57 | [train_policy] epoch #121 | Saved
2022-04-23 14:19:57 | [train_policy] epoch #121 | Time 45.58 s
2022-04-23 14:19:57 | [train_policy] epoch #121 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.122926
Evaluation/AverageDiscountedReturn          -71.4659
Evaluation/AverageReturn                    -71.4659
Evaluation/CompletionRate                     0
Evaluation/Iteration                        121
Evaluation/MaxReturn                        -37.8517
Evaluation/MinReturn                      -2063.37
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.993
Extras/EpisodeRewardMean                    -70.5802
LinearFeatureBaseline/ExplainedVariance       0.027184
PolicyExecTime                                0.115407
ProcessExecTime                               0.0131483
TotalEnvSteps                            123464
policy/Entropy                                1.27784
policy/KL                                     0.0091961
policy/KLBefore                               0
policy/LossAfter                             -0.0166508
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             3.58889
policy/dLoss                                  0.0166508
---------------------------------------  ----------------
2022-04-23 14:19:57 | [train_policy] epoch #122 | Obtaining samples for iteration 122...
2022-04-23 14:19:58 | [train_policy] epoch #122 | Logging diagnostics...
2022-04-23 14:19:58 | [train_policy] epoch #122 | Optimizing policy...
2022-04-23 14:19:58 | [train_policy] epoch #122 | Computing loss before
2022-04-23 14:19:58 | [train_policy] epoch #122 | Computing KL before
2022-04-23 14:19:58 | [train_policy] epoch #122 | Optimizing
2022-04-23 14:19:58 | [train_policy] epoch #122 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:58 | [train_policy] epoch #122 | computing loss before
2022-04-23 14:19:58 | [train_policy] epoch #122 | computing gradient
2022-04-23 14:19:58 | [train_policy] epoch #122 | gradient computed
2022-04-23 14:19:58 | [train_policy] epoch #122 | computing descent direction
2022-04-23 14:19:58 | [train_policy] epoch #122 | descent direction computed
2022-04-23 14:19:58 | [train_policy] epoch #122 | backtrack iters: 0
2022-04-23 14:19:58 | [train_policy] epoch #122 | optimization finished
2022-04-23 14:19:58 | [train_policy] epoch #122 | Computing KL after
2022-04-23 14:19:58 | [train_policy] epoch #122 | Computing loss after
2022-04-23 14:19:58 | [train_policy] epoch #122 | Fitting baseline...
2022-04-23 14:19:58 | [train_policy] epoch #122 | Saving snapshot...
2022-04-23 14:19:58 | [train_policy] epoch #122 | Saved
2022-04-23 14:19:58 | [train_policy] epoch #122 | Time 45.94 s
2022-04-23 14:19:58 | [train_policy] epoch #122 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.128302
Evaluation/AverageDiscountedReturn          -46.6177
Evaluation/AverageReturn                    -46.6177
Evaluation/CompletionRate                     0
Evaluation/Iteration                        122
Evaluation/MaxReturn                        -34.7804
Evaluation/MinReturn                        -74.8096
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.3334
Extras/EpisodeRewardMean                    -47.6429
LinearFeatureBaseline/ExplainedVariance      -8.1649
PolicyExecTime                                0.111861
ProcessExecTime                               0.012511
TotalEnvSteps                            124476
policy/Entropy                                1.25741
policy/KL                                     0.00948256
policy/KLBefore                               0
policy/LossAfter                             -0.0291058
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             3.51631
policy/dLoss                                  0.0291058
---------------------------------------  ----------------
2022-04-23 14:19:58 | [train_policy] epoch #123 | Obtaining samples for iteration 123...
2022-04-23 14:19:58 | [train_policy] epoch #123 | Logging diagnostics...
2022-04-23 14:19:58 | [train_policy] epoch #123 | Optimizing policy...
2022-04-23 14:19:58 | [train_policy] epoch #123 | Computing loss before
2022-04-23 14:19:58 | [train_policy] epoch #123 | Computing KL before
2022-04-23 14:19:58 | [train_policy] epoch #123 | Optimizing
2022-04-23 14:19:58 | [train_policy] epoch #123 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:58 | [train_policy] epoch #123 | computing loss before
2022-04-23 14:19:58 | [train_policy] epoch #123 | computing gradient
2022-04-23 14:19:58 | [train_policy] epoch #123 | gradient computed
2022-04-23 14:19:58 | [train_policy] epoch #123 | computing descent direction
2022-04-23 14:19:58 | [train_policy] epoch #123 | descent direction computed
2022-04-23 14:19:58 | [train_policy] epoch #123 | backtrack iters: 1
2022-04-23 14:19:58 | [train_policy] epoch #123 | optimization finished
2022-04-23 14:19:58 | [train_policy] epoch #123 | Computing KL after
2022-04-23 14:19:58 | [train_policy] epoch #123 | Computing loss after
2022-04-23 14:19:58 | [train_policy] epoch #123 | Fitting baseline...
2022-04-23 14:19:58 | [train_policy] epoch #123 | Saving snapshot...
2022-04-23 14:19:58 | [train_policy] epoch #123 | Saved
2022-04-23 14:19:58 | [train_policy] epoch #123 | Time 46.29 s
2022-04-23 14:19:58 | [train_policy] epoch #123 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.122905
Evaluation/AverageDiscountedReturn          -71.5795
Evaluation/AverageReturn                    -71.5795
Evaluation/CompletionRate                     0
Evaluation/Iteration                        123
Evaluation/MaxReturn                        -37.4936
Evaluation/MinReturn                      -2065.18
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.142
Extras/EpisodeRewardMean                    -69.4674
LinearFeatureBaseline/ExplainedVariance       0.0101184
PolicyExecTime                                0.0998466
ProcessExecTime                               0.0116704
TotalEnvSteps                            125488
policy/Entropy                                1.24177
policy/KL                                     0.00687162
policy/KLBefore                               0
policy/LossAfter                             -0.0296718
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             3.46173
policy/dLoss                                  0.0296718
---------------------------------------  ----------------
2022-04-23 14:19:58 | [train_policy] epoch #124 | Obtaining samples for iteration 124...
2022-04-23 14:19:58 | [train_policy] epoch #124 | Logging diagnostics...
2022-04-23 14:19:58 | [train_policy] epoch #124 | Optimizing policy...
2022-04-23 14:19:58 | [train_policy] epoch #124 | Computing loss before
2022-04-23 14:19:58 | [train_policy] epoch #124 | Computing KL before
2022-04-23 14:19:58 | [train_policy] epoch #124 | Optimizing
2022-04-23 14:19:58 | [train_policy] epoch #124 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:58 | [train_policy] epoch #124 | computing loss before
2022-04-23 14:19:58 | [train_policy] epoch #124 | computing gradient
2022-04-23 14:19:58 | [train_policy] epoch #124 | gradient computed
2022-04-23 14:19:58 | [train_policy] epoch #124 | computing descent direction
2022-04-23 14:19:58 | [train_policy] epoch #124 | descent direction computed
2022-04-23 14:19:58 | [train_policy] epoch #124 | backtrack iters: 0
2022-04-23 14:19:58 | [train_policy] epoch #124 | optimization finished
2022-04-23 14:19:58 | [train_policy] epoch #124 | Computing KL after
2022-04-23 14:19:58 | [train_policy] epoch #124 | Computing loss after
2022-04-23 14:19:58 | [train_policy] epoch #124 | Fitting baseline...
2022-04-23 14:19:58 | [train_policy] epoch #124 | Saving snapshot...
2022-04-23 14:19:58 | [train_policy] epoch #124 | Saved
2022-04-23 14:19:58 | [train_policy] epoch #124 | Time 46.66 s
2022-04-23 14:19:58 | [train_policy] epoch #124 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.126567
Evaluation/AverageDiscountedReturn          -48.3507
Evaluation/AverageReturn                    -48.3507
Evaluation/CompletionRate                     0
Evaluation/Iteration                        124
Evaluation/MaxReturn                        -35.7252
Evaluation/MinReturn                        -93.9481
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.60049
Extras/EpisodeRewardMean                    -68.5025
LinearFeatureBaseline/ExplainedVariance     -20.2151
PolicyExecTime                                0.108957
ProcessExecTime                               0.0120208
TotalEnvSteps                            126500
policy/Entropy                                1.23867
policy/KL                                     0.00965085
policy/KLBefore                               0
policy/LossAfter                             -0.025277
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             3.45102
policy/dLoss                                  0.025277
---------------------------------------  ----------------
2022-04-23 14:19:58 | [train_policy] epoch #125 | Obtaining samples for iteration 125...
2022-04-23 14:19:59 | [train_policy] epoch #125 | Logging diagnostics...
2022-04-23 14:19:59 | [train_policy] epoch #125 | Optimizing policy...
2022-04-23 14:19:59 | [train_policy] epoch #125 | Computing loss before
2022-04-23 14:19:59 | [train_policy] epoch #125 | Computing KL before
2022-04-23 14:19:59 | [train_policy] epoch #125 | Optimizing
2022-04-23 14:19:59 | [train_policy] epoch #125 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:59 | [train_policy] epoch #125 | computing loss before
2022-04-23 14:19:59 | [train_policy] epoch #125 | computing gradient
2022-04-23 14:19:59 | [train_policy] epoch #125 | gradient computed
2022-04-23 14:19:59 | [train_policy] epoch #125 | computing descent direction
2022-04-23 14:19:59 | [train_policy] epoch #125 | descent direction computed
2022-04-23 14:19:59 | [train_policy] epoch #125 | backtrack iters: 1
2022-04-23 14:19:59 | [train_policy] epoch #125 | optimization finished
2022-04-23 14:19:59 | [train_policy] epoch #125 | Computing KL after
2022-04-23 14:19:59 | [train_policy] epoch #125 | Computing loss after
2022-04-23 14:19:59 | [train_policy] epoch #125 | Fitting baseline...
2022-04-23 14:19:59 | [train_policy] epoch #125 | Saving snapshot...
2022-04-23 14:19:59 | [train_policy] epoch #125 | Saved
2022-04-23 14:19:59 | [train_policy] epoch #125 | Time 47.01 s
2022-04-23 14:19:59 | [train_policy] epoch #125 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.124409
Evaluation/AverageDiscountedReturn          -47.6722
Evaluation/AverageReturn                    -47.6722
Evaluation/CompletionRate                     0
Evaluation/Iteration                        125
Evaluation/MaxReturn                        -35.2976
Evaluation/MinReturn                        -77.6251
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.44339
Extras/EpisodeRewardMean                    -47.5794
LinearFeatureBaseline/ExplainedVariance       0.929386
PolicyExecTime                                0.103727
ProcessExecTime                               0.0118804
TotalEnvSteps                            127512
policy/Entropy                                1.18251
policy/KL                                     0.00641358
policy/KLBefore                               0
policy/LossAfter                             -0.0180294
policy/LossBefore                             7.30334e-09
policy/Perplexity                             3.26255
policy/dLoss                                  0.0180294
---------------------------------------  ----------------
2022-04-23 14:19:59 | [train_policy] epoch #126 | Obtaining samples for iteration 126...
2022-04-23 14:19:59 | [train_policy] epoch #126 | Logging diagnostics...
2022-04-23 14:19:59 | [train_policy] epoch #126 | Optimizing policy...
2022-04-23 14:19:59 | [train_policy] epoch #126 | Computing loss before
2022-04-23 14:19:59 | [train_policy] epoch #126 | Computing KL before
2022-04-23 14:19:59 | [train_policy] epoch #126 | Optimizing
2022-04-23 14:19:59 | [train_policy] epoch #126 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:59 | [train_policy] epoch #126 | computing loss before
2022-04-23 14:19:59 | [train_policy] epoch #126 | computing gradient
2022-04-23 14:19:59 | [train_policy] epoch #126 | gradient computed
2022-04-23 14:19:59 | [train_policy] epoch #126 | computing descent direction
2022-04-23 14:19:59 | [train_policy] epoch #126 | descent direction computed
2022-04-23 14:19:59 | [train_policy] epoch #126 | backtrack iters: 1
2022-04-23 14:19:59 | [train_policy] epoch #126 | optimization finished
2022-04-23 14:19:59 | [train_policy] epoch #126 | Computing KL after
2022-04-23 14:19:59 | [train_policy] epoch #126 | Computing loss after
2022-04-23 14:19:59 | [train_policy] epoch #126 | Fitting baseline...
2022-04-23 14:19:59 | [train_policy] epoch #126 | Saving snapshot...
2022-04-23 14:19:59 | [train_policy] epoch #126 | Saved
2022-04-23 14:19:59 | [train_policy] epoch #126 | Time 47.35 s
2022-04-23 14:19:59 | [train_policy] epoch #126 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.121771
Evaluation/AverageDiscountedReturn          -48.5694
Evaluation/AverageReturn                    -48.5694
Evaluation/CompletionRate                     0
Evaluation/Iteration                        126
Evaluation/MaxReturn                        -33.2095
Evaluation/MinReturn                       -147.082
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.9146
Extras/EpisodeRewardMean                    -48.5336
LinearFeatureBaseline/ExplainedVariance       0.553854
PolicyExecTime                                0.0989683
ProcessExecTime                               0.0115323
TotalEnvSteps                            128524
policy/Entropy                                1.14563
policy/KL                                     0.00684045
policy/KLBefore                               0
policy/LossAfter                             -0.0173674
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             3.14441
policy/dLoss                                  0.0173674
---------------------------------------  ----------------
2022-04-23 14:19:59 | [train_policy] epoch #127 | Obtaining samples for iteration 127...
2022-04-23 14:19:59 | [train_policy] epoch #127 | Logging diagnostics...
2022-04-23 14:19:59 | [train_policy] epoch #127 | Optimizing policy...
2022-04-23 14:19:59 | [train_policy] epoch #127 | Computing loss before
2022-04-23 14:19:59 | [train_policy] epoch #127 | Computing KL before
2022-04-23 14:19:59 | [train_policy] epoch #127 | Optimizing
2022-04-23 14:19:59 | [train_policy] epoch #127 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:19:59 | [train_policy] epoch #127 | computing loss before
2022-04-23 14:19:59 | [train_policy] epoch #127 | computing gradient
2022-04-23 14:19:59 | [train_policy] epoch #127 | gradient computed
2022-04-23 14:19:59 | [train_policy] epoch #127 | computing descent direction
2022-04-23 14:19:59 | [train_policy] epoch #127 | descent direction computed
2022-04-23 14:19:59 | [train_policy] epoch #127 | backtrack iters: 1
2022-04-23 14:19:59 | [train_policy] epoch #127 | optimization finished
2022-04-23 14:19:59 | [train_policy] epoch #127 | Computing KL after
2022-04-23 14:19:59 | [train_policy] epoch #127 | Computing loss after
2022-04-23 14:20:00 | [train_policy] epoch #127 | Fitting baseline...
2022-04-23 14:20:00 | [train_policy] epoch #127 | Saving snapshot...
2022-04-23 14:20:00 | [train_policy] epoch #127 | Saved
2022-04-23 14:20:00 | [train_policy] epoch #127 | Time 47.73 s
2022-04-23 14:20:00 | [train_policy] epoch #127 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.132055
Evaluation/AverageDiscountedReturn          -46.2787
Evaluation/AverageReturn                    -46.2787
Evaluation/CompletionRate                     0
Evaluation/Iteration                        127
Evaluation/MaxReturn                        -34.5681
Evaluation/MinReturn                        -60.6876
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.43003
Extras/EpisodeRewardMean                    -46.3187
LinearFeatureBaseline/ExplainedVariance       0.914114
PolicyExecTime                                0.112425
ProcessExecTime                               0.0129645
TotalEnvSteps                            129536
policy/Entropy                                1.11194
policy/KL                                     0.00644135
policy/KLBefore                               0
policy/LossAfter                             -0.0167647
policy/LossBefore                             1.10728e-08
policy/Perplexity                             3.04025
policy/dLoss                                  0.0167647
---------------------------------------  ----------------
2022-04-23 14:20:00 | [train_policy] epoch #128 | Obtaining samples for iteration 128...
2022-04-23 14:20:00 | [train_policy] epoch #128 | Logging diagnostics...
2022-04-23 14:20:00 | [train_policy] epoch #128 | Optimizing policy...
2022-04-23 14:20:00 | [train_policy] epoch #128 | Computing loss before
2022-04-23 14:20:00 | [train_policy] epoch #128 | Computing KL before
2022-04-23 14:20:00 | [train_policy] epoch #128 | Optimizing
2022-04-23 14:20:00 | [train_policy] epoch #128 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:00 | [train_policy] epoch #128 | computing loss before
2022-04-23 14:20:00 | [train_policy] epoch #128 | computing gradient
2022-04-23 14:20:00 | [train_policy] epoch #128 | gradient computed
2022-04-23 14:20:00 | [train_policy] epoch #128 | computing descent direction
2022-04-23 14:20:00 | [train_policy] epoch #128 | descent direction computed
2022-04-23 14:20:00 | [train_policy] epoch #128 | backtrack iters: 1
2022-04-23 14:20:00 | [train_policy] epoch #128 | optimization finished
2022-04-23 14:20:00 | [train_policy] epoch #128 | Computing KL after
2022-04-23 14:20:00 | [train_policy] epoch #128 | Computing loss after
2022-04-23 14:20:00 | [train_policy] epoch #128 | Fitting baseline...
2022-04-23 14:20:00 | [train_policy] epoch #128 | Saving snapshot...
2022-04-23 14:20:00 | [train_policy] epoch #128 | Saved
2022-04-23 14:20:00 | [train_policy] epoch #128 | Time 48.07 s
2022-04-23 14:20:00 | [train_policy] epoch #128 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116352
Evaluation/AverageDiscountedReturn          -49.2203
Evaluation/AverageReturn                    -49.2203
Evaluation/CompletionRate                     0
Evaluation/Iteration                        128
Evaluation/MaxReturn                        -36.3558
Evaluation/MinReturn                       -290.492
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         26.2714
Extras/EpisodeRewardMean                    -48.8717
LinearFeatureBaseline/ExplainedVariance       0.345455
PolicyExecTime                                0.10454
ProcessExecTime                               0.0109975
TotalEnvSteps                            130548
policy/Entropy                                1.07014
policy/KL                                     0.00675228
policy/KLBefore                               0
policy/LossAfter                             -0.0149076
policy/LossBefore                             1.76694e-08
policy/Perplexity                             2.91579
policy/dLoss                                  0.0149076
---------------------------------------  ----------------
2022-04-23 14:20:00 | [train_policy] epoch #129 | Obtaining samples for iteration 129...
2022-04-23 14:20:00 | [train_policy] epoch #129 | Logging diagnostics...
2022-04-23 14:20:00 | [train_policy] epoch #129 | Optimizing policy...
2022-04-23 14:20:00 | [train_policy] epoch #129 | Computing loss before
2022-04-23 14:20:00 | [train_policy] epoch #129 | Computing KL before
2022-04-23 14:20:00 | [train_policy] epoch #129 | Optimizing
2022-04-23 14:20:00 | [train_policy] epoch #129 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:00 | [train_policy] epoch #129 | computing loss before
2022-04-23 14:20:00 | [train_policy] epoch #129 | computing gradient
2022-04-23 14:20:00 | [train_policy] epoch #129 | gradient computed
2022-04-23 14:20:00 | [train_policy] epoch #129 | computing descent direction
2022-04-23 14:20:00 | [train_policy] epoch #129 | descent direction computed
2022-04-23 14:20:00 | [train_policy] epoch #129 | backtrack iters: 1
2022-04-23 14:20:00 | [train_policy] epoch #129 | optimization finished
2022-04-23 14:20:00 | [train_policy] epoch #129 | Computing KL after
2022-04-23 14:20:00 | [train_policy] epoch #129 | Computing loss after
2022-04-23 14:20:00 | [train_policy] epoch #129 | Fitting baseline...
2022-04-23 14:20:00 | [train_policy] epoch #129 | Saving snapshot...
2022-04-23 14:20:00 | [train_policy] epoch #129 | Saved
2022-04-23 14:20:00 | [train_policy] epoch #129 | Time 48.42 s
2022-04-23 14:20:00 | [train_policy] epoch #129 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.11557
Evaluation/AverageDiscountedReturn          -48.2028
Evaluation/AverageReturn                    -48.2028
Evaluation/CompletionRate                     0
Evaluation/Iteration                        129
Evaluation/MaxReturn                        -36.3751
Evaluation/MinReturn                       -162.637
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         13.4248
Extras/EpisodeRewardMean                    -47.945
LinearFeatureBaseline/ExplainedVariance       0.566261
PolicyExecTime                                0.0996015
ProcessExecTime                               0.0109222
TotalEnvSteps                            131560
policy/Entropy                                1.08939
policy/KL                                     0.0066102
policy/KLBefore                               0
policy/LossAfter                             -0.0104789
policy/LossBefore                            -5.5364e-09
policy/Perplexity                             2.97246
policy/dLoss                                  0.0104789
---------------------------------------  ---------------
2022-04-23 14:20:00 | [train_policy] epoch #130 | Obtaining samples for iteration 130...
2022-04-23 14:20:00 | [train_policy] epoch #130 | Logging diagnostics...
2022-04-23 14:20:00 | [train_policy] epoch #130 | Optimizing policy...
2022-04-23 14:20:00 | [train_policy] epoch #130 | Computing loss before
2022-04-23 14:20:00 | [train_policy] epoch #130 | Computing KL before
2022-04-23 14:20:00 | [train_policy] epoch #130 | Optimizing
2022-04-23 14:20:00 | [train_policy] epoch #130 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:00 | [train_policy] epoch #130 | computing loss before
2022-04-23 14:20:00 | [train_policy] epoch #130 | computing gradient
2022-04-23 14:20:00 | [train_policy] epoch #130 | gradient computed
2022-04-23 14:20:00 | [train_policy] epoch #130 | computing descent direction
2022-04-23 14:20:01 | [train_policy] epoch #130 | descent direction computed
2022-04-23 14:20:01 | [train_policy] epoch #130 | backtrack iters: 1
2022-04-23 14:20:01 | [train_policy] epoch #130 | optimization finished
2022-04-23 14:20:01 | [train_policy] epoch #130 | Computing KL after
2022-04-23 14:20:01 | [train_policy] epoch #130 | Computing loss after
2022-04-23 14:20:01 | [train_policy] epoch #130 | Fitting baseline...
2022-04-23 14:20:01 | [train_policy] epoch #130 | Saving snapshot...
2022-04-23 14:20:01 | [train_policy] epoch #130 | Saved
2022-04-23 14:20:01 | [train_policy] epoch #130 | Time 48.77 s
2022-04-23 14:20:01 | [train_policy] epoch #130 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.115447
Evaluation/AverageDiscountedReturn          -46.7555
Evaluation/AverageReturn                    -46.7555
Evaluation/CompletionRate                     0
Evaluation/Iteration                        130
Evaluation/MaxReturn                        -35.5379
Evaluation/MinReturn                        -65.4852
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.91764
Extras/EpisodeRewardMean                    -46.6872
LinearFeatureBaseline/ExplainedVariance       0.907912
PolicyExecTime                                0.104816
ProcessExecTime                               0.0110557
TotalEnvSteps                            132572
policy/Entropy                                1.04712
policy/KL                                     0.00660103
policy/KLBefore                               0
policy/LossAfter                             -0.018386
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             2.84943
policy/dLoss                                  0.018386
---------------------------------------  ----------------
2022-04-23 14:20:01 | [train_policy] epoch #131 | Obtaining samples for iteration 131...
2022-04-23 14:20:01 | [train_policy] epoch #131 | Logging diagnostics...
2022-04-23 14:20:01 | [train_policy] epoch #131 | Optimizing policy...
2022-04-23 14:20:01 | [train_policy] epoch #131 | Computing loss before
2022-04-23 14:20:01 | [train_policy] epoch #131 | Computing KL before
2022-04-23 14:20:01 | [train_policy] epoch #131 | Optimizing
2022-04-23 14:20:01 | [train_policy] epoch #131 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:01 | [train_policy] epoch #131 | computing loss before
2022-04-23 14:20:01 | [train_policy] epoch #131 | computing gradient
2022-04-23 14:20:01 | [train_policy] epoch #131 | gradient computed
2022-04-23 14:20:01 | [train_policy] epoch #131 | computing descent direction
2022-04-23 14:20:01 | [train_policy] epoch #131 | descent direction computed
2022-04-23 14:20:01 | [train_policy] epoch #131 | backtrack iters: 1
2022-04-23 14:20:01 | [train_policy] epoch #131 | optimization finished
2022-04-23 14:20:01 | [train_policy] epoch #131 | Computing KL after
2022-04-23 14:20:01 | [train_policy] epoch #131 | Computing loss after
2022-04-23 14:20:01 | [train_policy] epoch #131 | Fitting baseline...
2022-04-23 14:20:01 | [train_policy] epoch #131 | Saving snapshot...
2022-04-23 14:20:01 | [train_policy] epoch #131 | Saved
2022-04-23 14:20:01 | [train_policy] epoch #131 | Time 49.11 s
2022-04-23 14:20:01 | [train_policy] epoch #131 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118731
Evaluation/AverageDiscountedReturn          -48.7982
Evaluation/AverageReturn                    -48.7982
Evaluation/CompletionRate                     0
Evaluation/Iteration                        131
Evaluation/MaxReturn                        -33.4223
Evaluation/MinReturn                       -172.947
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         16.1363
Extras/EpisodeRewardMean                    -48.6032
LinearFeatureBaseline/ExplainedVariance       0.530044
PolicyExecTime                                0.100831
ProcessExecTime                               0.0112302
TotalEnvSteps                            133584
policy/Entropy                                1.0056
policy/KL                                     0.00669519
policy/KLBefore                               0
policy/LossAfter                             -0.0192096
policy/LossBefore                             7.06774e-09
policy/Perplexity                             2.73354
policy/dLoss                                  0.0192096
---------------------------------------  ----------------
2022-04-23 14:20:01 | [train_policy] epoch #132 | Obtaining samples for iteration 132...
2022-04-23 14:20:01 | [train_policy] epoch #132 | Logging diagnostics...
2022-04-23 14:20:01 | [train_policy] epoch #132 | Optimizing policy...
2022-04-23 14:20:01 | [train_policy] epoch #132 | Computing loss before
2022-04-23 14:20:01 | [train_policy] epoch #132 | Computing KL before
2022-04-23 14:20:01 | [train_policy] epoch #132 | Optimizing
2022-04-23 14:20:01 | [train_policy] epoch #132 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:01 | [train_policy] epoch #132 | computing loss before
2022-04-23 14:20:01 | [train_policy] epoch #132 | computing gradient
2022-04-23 14:20:01 | [train_policy] epoch #132 | gradient computed
2022-04-23 14:20:01 | [train_policy] epoch #132 | computing descent direction
2022-04-23 14:20:01 | [train_policy] epoch #132 | descent direction computed
2022-04-23 14:20:01 | [train_policy] epoch #132 | backtrack iters: 1
2022-04-23 14:20:01 | [train_policy] epoch #132 | optimization finished
2022-04-23 14:20:01 | [train_policy] epoch #132 | Computing KL after
2022-04-23 14:20:01 | [train_policy] epoch #132 | Computing loss after
2022-04-23 14:20:01 | [train_policy] epoch #132 | Fitting baseline...
2022-04-23 14:20:01 | [train_policy] epoch #132 | Saving snapshot...
2022-04-23 14:20:01 | [train_policy] epoch #132 | Saved
2022-04-23 14:20:01 | [train_policy] epoch #132 | Time 49.46 s
2022-04-23 14:20:01 | [train_policy] epoch #132 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118504
Evaluation/AverageDiscountedReturn          -48.264
Evaluation/AverageReturn                    -48.264
Evaluation/CompletionRate                     0
Evaluation/Iteration                        132
Evaluation/MaxReturn                        -32.203
Evaluation/MinReturn                       -144.762
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         13.0389
Extras/EpisodeRewardMean                    -48.4307
LinearFeatureBaseline/ExplainedVariance       0.541723
PolicyExecTime                                0.100662
ProcessExecTime                               0.0112104
TotalEnvSteps                            134596
policy/Entropy                                0.940357
policy/KL                                     0.00674738
policy/KLBefore                               0
policy/LossAfter                             -0.0238007
policy/LossBefore                             5.18301e-09
policy/Perplexity                             2.5609
policy/dLoss                                  0.0238007
---------------------------------------  ----------------
2022-04-23 14:20:01 | [train_policy] epoch #133 | Obtaining samples for iteration 133...
2022-04-23 14:20:02 | [train_policy] epoch #133 | Logging diagnostics...
2022-04-23 14:20:02 | [train_policy] epoch #133 | Optimizing policy...
2022-04-23 14:20:02 | [train_policy] epoch #133 | Computing loss before
2022-04-23 14:20:02 | [train_policy] epoch #133 | Computing KL before
2022-04-23 14:20:02 | [train_policy] epoch #133 | Optimizing
2022-04-23 14:20:02 | [train_policy] epoch #133 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:02 | [train_policy] epoch #133 | computing loss before
2022-04-23 14:20:02 | [train_policy] epoch #133 | computing gradient
2022-04-23 14:20:02 | [train_policy] epoch #133 | gradient computed
2022-04-23 14:20:02 | [train_policy] epoch #133 | computing descent direction
2022-04-23 14:20:02 | [train_policy] epoch #133 | descent direction computed
2022-04-23 14:20:02 | [train_policy] epoch #133 | backtrack iters: 1
2022-04-23 14:20:02 | [train_policy] epoch #133 | optimization finished
2022-04-23 14:20:02 | [train_policy] epoch #133 | Computing KL after
2022-04-23 14:20:02 | [train_policy] epoch #133 | Computing loss after
2022-04-23 14:20:02 | [train_policy] epoch #133 | Fitting baseline...
2022-04-23 14:20:02 | [train_policy] epoch #133 | Saving snapshot...
2022-04-23 14:20:02 | [train_policy] epoch #133 | Saved
2022-04-23 14:20:02 | [train_policy] epoch #133 | Time 49.82 s
2022-04-23 14:20:02 | [train_policy] epoch #133 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.119472
Evaluation/AverageDiscountedReturn          -46.1653
Evaluation/AverageReturn                    -46.1653
Evaluation/CompletionRate                     0
Evaluation/Iteration                        133
Evaluation/MaxReturn                        -33.9726
Evaluation/MinReturn                        -67.1999
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.86266
Extras/EpisodeRewardMean                    -47.4211
LinearFeatureBaseline/ExplainedVariance       0.917961
PolicyExecTime                                0.107023
ProcessExecTime                               0.0117366
TotalEnvSteps                            135608
policy/Entropy                                0.91881
policy/KL                                     0.00689744
policy/KLBefore                               0
policy/LossAfter                             -0.0199695
policy/LossBefore                             4.71183e-09
policy/Perplexity                             2.50631
policy/dLoss                                  0.0199695
---------------------------------------  ----------------
2022-04-23 14:20:02 | [train_policy] epoch #134 | Obtaining samples for iteration 134...
2022-04-23 14:20:02 | [train_policy] epoch #134 | Logging diagnostics...
2022-04-23 14:20:02 | [train_policy] epoch #134 | Optimizing policy...
2022-04-23 14:20:02 | [train_policy] epoch #134 | Computing loss before
2022-04-23 14:20:02 | [train_policy] epoch #134 | Computing KL before
2022-04-23 14:20:02 | [train_policy] epoch #134 | Optimizing
2022-04-23 14:20:02 | [train_policy] epoch #134 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:02 | [train_policy] epoch #134 | computing loss before
2022-04-23 14:20:02 | [train_policy] epoch #134 | computing gradient
2022-04-23 14:20:02 | [train_policy] epoch #134 | gradient computed
2022-04-23 14:20:02 | [train_policy] epoch #134 | computing descent direction
2022-04-23 14:20:02 | [train_policy] epoch #134 | descent direction computed
2022-04-23 14:20:02 | [train_policy] epoch #134 | backtrack iters: 1
2022-04-23 14:20:02 | [train_policy] epoch #134 | optimization finished
2022-04-23 14:20:02 | [train_policy] epoch #134 | Computing KL after
2022-04-23 14:20:02 | [train_policy] epoch #134 | Computing loss after
2022-04-23 14:20:02 | [train_policy] epoch #134 | Fitting baseline...
2022-04-23 14:20:02 | [train_policy] epoch #134 | Saving snapshot...
2022-04-23 14:20:02 | [train_policy] epoch #134 | Saved
2022-04-23 14:20:02 | [train_policy] epoch #134 | Time 50.17 s
2022-04-23 14:20:02 | [train_policy] epoch #134 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.117975
Evaluation/AverageDiscountedReturn          -48.7216
Evaluation/AverageReturn                    -48.7216
Evaluation/CompletionRate                     0
Evaluation/Iteration                        134
Evaluation/MaxReturn                        -34.6398
Evaluation/MinReturn                        -99.2234
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.0181
Extras/EpisodeRewardMean                    -48.3341
LinearFeatureBaseline/ExplainedVariance       0.844383
PolicyExecTime                                0.10503
ProcessExecTime                               0.012064
TotalEnvSteps                            136620
policy/Entropy                                0.943009
policy/KL                                     0.00733953
policy/KLBefore                               0
policy/LossAfter                             -0.0131863
policy/LossBefore                             2.8271e-09
policy/Perplexity                             2.5677
policy/dLoss                                  0.0131863
---------------------------------------  ---------------
2022-04-23 14:20:02 | [train_policy] epoch #135 | Obtaining samples for iteration 135...
2022-04-23 14:20:02 | [train_policy] epoch #135 | Logging diagnostics...
2022-04-23 14:20:02 | [train_policy] epoch #135 | Optimizing policy...
2022-04-23 14:20:02 | [train_policy] epoch #135 | Computing loss before
2022-04-23 14:20:02 | [train_policy] epoch #135 | Computing KL before
2022-04-23 14:20:02 | [train_policy] epoch #135 | Optimizing
2022-04-23 14:20:02 | [train_policy] epoch #135 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:02 | [train_policy] epoch #135 | computing loss before
2022-04-23 14:20:02 | [train_policy] epoch #135 | computing gradient
2022-04-23 14:20:02 | [train_policy] epoch #135 | gradient computed
2022-04-23 14:20:02 | [train_policy] epoch #135 | computing descent direction
2022-04-23 14:20:02 | [train_policy] epoch #135 | descent direction computed
2022-04-23 14:20:02 | [train_policy] epoch #135 | backtrack iters: 0
2022-04-23 14:20:02 | [train_policy] epoch #135 | optimization finished
2022-04-23 14:20:02 | [train_policy] epoch #135 | Computing KL after
2022-04-23 14:20:02 | [train_policy] epoch #135 | Computing loss after
2022-04-23 14:20:02 | [train_policy] epoch #135 | Fitting baseline...
2022-04-23 14:20:02 | [train_policy] epoch #135 | Saving snapshot...
2022-04-23 14:20:02 | [train_policy] epoch #135 | Saved
2022-04-23 14:20:02 | [train_policy] epoch #135 | Time 50.53 s
2022-04-23 14:20:02 | [train_policy] epoch #135 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.115697
Evaluation/AverageDiscountedReturn          -47.0405
Evaluation/AverageReturn                    -47.0405
Evaluation/CompletionRate                     0
Evaluation/Iteration                        135
Evaluation/MaxReturn                        -33.6334
Evaluation/MinReturn                        -67.7128
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.31973
Extras/EpisodeRewardMean                    -47.3987
LinearFeatureBaseline/ExplainedVariance       0.939653
PolicyExecTime                                0.103045
ProcessExecTime                               0.0114977
TotalEnvSteps                            137632
policy/Entropy                                0.940662
policy/KL                                     0.00847453
policy/KLBefore                               0
policy/LossAfter                             -0.0180306
policy/LossBefore                            -7.89231e-09
policy/Perplexity                             2.56168
policy/dLoss                                  0.0180306
---------------------------------------  ----------------
2022-04-23 14:20:02 | [train_policy] epoch #136 | Obtaining samples for iteration 136...
2022-04-23 14:20:03 | [train_policy] epoch #136 | Logging diagnostics...
2022-04-23 14:20:03 | [train_policy] epoch #136 | Optimizing policy...
2022-04-23 14:20:03 | [train_policy] epoch #136 | Computing loss before
2022-04-23 14:20:03 | [train_policy] epoch #136 | Computing KL before
2022-04-23 14:20:03 | [train_policy] epoch #136 | Optimizing
2022-04-23 14:20:03 | [train_policy] epoch #136 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:03 | [train_policy] epoch #136 | computing loss before
2022-04-23 14:20:03 | [train_policy] epoch #136 | computing gradient
2022-04-23 14:20:03 | [train_policy] epoch #136 | gradient computed
2022-04-23 14:20:03 | [train_policy] epoch #136 | computing descent direction
2022-04-23 14:20:03 | [train_policy] epoch #136 | descent direction computed
2022-04-23 14:20:03 | [train_policy] epoch #136 | backtrack iters: 1
2022-04-23 14:20:03 | [train_policy] epoch #136 | optimization finished
2022-04-23 14:20:03 | [train_policy] epoch #136 | Computing KL after
2022-04-23 14:20:03 | [train_policy] epoch #136 | Computing loss after
2022-04-23 14:20:03 | [train_policy] epoch #136 | Fitting baseline...
2022-04-23 14:20:03 | [train_policy] epoch #136 | Saving snapshot...
2022-04-23 14:20:03 | [train_policy] epoch #136 | Saved
2022-04-23 14:20:03 | [train_policy] epoch #136 | Time 50.88 s
2022-04-23 14:20:03 | [train_policy] epoch #136 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116219
Evaluation/AverageDiscountedReturn          -46.1369
Evaluation/AverageReturn                    -46.1369
Evaluation/CompletionRate                     0
Evaluation/Iteration                        136
Evaluation/MaxReturn                        -35.213
Evaluation/MinReturn                        -64.6573
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.27449
Extras/EpisodeRewardMean                    -46.2813
LinearFeatureBaseline/ExplainedVariance       0.949361
PolicyExecTime                                0.101575
ProcessExecTime                               0.0113506
TotalEnvSteps                            138644
policy/Entropy                                0.947778
policy/KL                                     0.00734997
policy/KLBefore                               0
policy/LossAfter                             -0.020409
policy/LossBefore                             6.12538e-09
policy/Perplexity                             2.57997
policy/dLoss                                  0.020409
---------------------------------------  ----------------
2022-04-23 14:20:03 | [train_policy] epoch #137 | Obtaining samples for iteration 137...
2022-04-23 14:20:03 | [train_policy] epoch #137 | Logging diagnostics...
2022-04-23 14:20:03 | [train_policy] epoch #137 | Optimizing policy...
2022-04-23 14:20:03 | [train_policy] epoch #137 | Computing loss before
2022-04-23 14:20:03 | [train_policy] epoch #137 | Computing KL before
2022-04-23 14:20:03 | [train_policy] epoch #137 | Optimizing
2022-04-23 14:20:03 | [train_policy] epoch #137 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:03 | [train_policy] epoch #137 | computing loss before
2022-04-23 14:20:03 | [train_policy] epoch #137 | computing gradient
2022-04-23 14:20:03 | [train_policy] epoch #137 | gradient computed
2022-04-23 14:20:03 | [train_policy] epoch #137 | computing descent direction
2022-04-23 14:20:03 | [train_policy] epoch #137 | descent direction computed
2022-04-23 14:20:03 | [train_policy] epoch #137 | backtrack iters: 1
2022-04-23 14:20:03 | [train_policy] epoch #137 | optimization finished
2022-04-23 14:20:03 | [train_policy] epoch #137 | Computing KL after
2022-04-23 14:20:03 | [train_policy] epoch #137 | Computing loss after
2022-04-23 14:20:03 | [train_policy] epoch #137 | Fitting baseline...
2022-04-23 14:20:03 | [train_policy] epoch #137 | Saving snapshot...
2022-04-23 14:20:03 | [train_policy] epoch #137 | Saved
2022-04-23 14:20:03 | [train_policy] epoch #137 | Time 51.22 s
2022-04-23 14:20:03 | [train_policy] epoch #137 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117112
Evaluation/AverageDiscountedReturn          -45.9543
Evaluation/AverageReturn                    -45.9543
Evaluation/CompletionRate                     0
Evaluation/Iteration                        137
Evaluation/MaxReturn                        -32.6142
Evaluation/MinReturn                        -69.5774
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.36135
Extras/EpisodeRewardMean                    -45.7464
LinearFeatureBaseline/ExplainedVariance       0.953651
PolicyExecTime                                0.103457
ProcessExecTime                               0.0115683
TotalEnvSteps                            139656
policy/Entropy                                0.92069
policy/KL                                     0.00691162
policy/KLBefore                               0
policy/LossAfter                             -0.0246512
policy/LossBefore                             2.35591e-09
policy/Perplexity                             2.51102
policy/dLoss                                  0.0246512
---------------------------------------  ----------------
2022-04-23 14:20:03 | [train_policy] epoch #138 | Obtaining samples for iteration 138...
2022-04-23 14:20:03 | [train_policy] epoch #138 | Logging diagnostics...
2022-04-23 14:20:03 | [train_policy] epoch #138 | Optimizing policy...
2022-04-23 14:20:03 | [train_policy] epoch #138 | Computing loss before
2022-04-23 14:20:03 | [train_policy] epoch #138 | Computing KL before
2022-04-23 14:20:03 | [train_policy] epoch #138 | Optimizing
2022-04-23 14:20:03 | [train_policy] epoch #138 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:03 | [train_policy] epoch #138 | computing loss before
2022-04-23 14:20:03 | [train_policy] epoch #138 | computing gradient
2022-04-23 14:20:03 | [train_policy] epoch #138 | gradient computed
2022-04-23 14:20:03 | [train_policy] epoch #138 | computing descent direction
2022-04-23 14:20:03 | [train_policy] epoch #138 | descent direction computed
2022-04-23 14:20:03 | [train_policy] epoch #138 | backtrack iters: 1
2022-04-23 14:20:03 | [train_policy] epoch #138 | optimization finished
2022-04-23 14:20:03 | [train_policy] epoch #138 | Computing KL after
2022-04-23 14:20:03 | [train_policy] epoch #138 | Computing loss after
2022-04-23 14:20:03 | [train_policy] epoch #138 | Fitting baseline...
2022-04-23 14:20:03 | [train_policy] epoch #138 | Saving snapshot...
2022-04-23 14:20:03 | [train_policy] epoch #138 | Saved
2022-04-23 14:20:03 | [train_policy] epoch #138 | Time 51.59 s
2022-04-23 14:20:03 | [train_policy] epoch #138 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.123106
Evaluation/AverageDiscountedReturn          -48.1323
Evaluation/AverageReturn                    -48.1323
Evaluation/CompletionRate                     0
Evaluation/Iteration                        138
Evaluation/MaxReturn                        -34.133
Evaluation/MinReturn                       -263.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         23.7748
Extras/EpisodeRewardMean                    -47.7586
LinearFeatureBaseline/ExplainedVariance       0.272138
PolicyExecTime                                0.109438
ProcessExecTime                               0.012208
TotalEnvSteps                            140668
policy/Entropy                                0.892111
policy/KL                                     0.00693636
policy/KLBefore                               0
policy/LossAfter                             -0.0224318
policy/LossBefore                            -2.49727e-08
policy/Perplexity                             2.44027
policy/dLoss                                  0.0224318
---------------------------------------  ----------------
2022-04-23 14:20:03 | [train_policy] epoch #139 | Obtaining samples for iteration 139...
2022-04-23 14:20:04 | [train_policy] epoch #139 | Logging diagnostics...
2022-04-23 14:20:04 | [train_policy] epoch #139 | Optimizing policy...
2022-04-23 14:20:04 | [train_policy] epoch #139 | Computing loss before
2022-04-23 14:20:04 | [train_policy] epoch #139 | Computing KL before
2022-04-23 14:20:04 | [train_policy] epoch #139 | Optimizing
2022-04-23 14:20:04 | [train_policy] epoch #139 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:04 | [train_policy] epoch #139 | computing loss before
2022-04-23 14:20:04 | [train_policy] epoch #139 | computing gradient
2022-04-23 14:20:04 | [train_policy] epoch #139 | gradient computed
2022-04-23 14:20:04 | [train_policy] epoch #139 | computing descent direction
2022-04-23 14:20:04 | [train_policy] epoch #139 | descent direction computed
2022-04-23 14:20:04 | [train_policy] epoch #139 | backtrack iters: 1
2022-04-23 14:20:04 | [train_policy] epoch #139 | optimization finished
2022-04-23 14:20:04 | [train_policy] epoch #139 | Computing KL after
2022-04-23 14:20:04 | [train_policy] epoch #139 | Computing loss after
2022-04-23 14:20:04 | [train_policy] epoch #139 | Fitting baseline...
2022-04-23 14:20:04 | [train_policy] epoch #139 | Saving snapshot...
2022-04-23 14:20:04 | [train_policy] epoch #139 | Saved
2022-04-23 14:20:04 | [train_policy] epoch #139 | Time 51.95 s
2022-04-23 14:20:04 | [train_policy] epoch #139 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.11757
Evaluation/AverageDiscountedReturn          -47.0012
Evaluation/AverageReturn                    -47.0012
Evaluation/CompletionRate                     0
Evaluation/Iteration                        139
Evaluation/MaxReturn                        -35.6506
Evaluation/MinReturn                        -75.4766
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.30681
Extras/EpisodeRewardMean                    -46.9486
LinearFeatureBaseline/ExplainedVariance       0.800566
PolicyExecTime                                0.105674
ProcessExecTime                               0.0122147
TotalEnvSteps                            141680
policy/Entropy                                0.850084
policy/KL                                     0.00692765
policy/KLBefore                               0
policy/LossAfter                             -0.0140885
policy/LossBefore                            -9.42366e-09
policy/Perplexity                             2.33984
policy/dLoss                                  0.0140885
---------------------------------------  ----------------
2022-04-23 14:20:04 | [train_policy] epoch #140 | Obtaining samples for iteration 140...
2022-04-23 14:20:04 | [train_policy] epoch #140 | Logging diagnostics...
2022-04-23 14:20:04 | [train_policy] epoch #140 | Optimizing policy...
2022-04-23 14:20:04 | [train_policy] epoch #140 | Computing loss before
2022-04-23 14:20:04 | [train_policy] epoch #140 | Computing KL before
2022-04-23 14:20:04 | [train_policy] epoch #140 | Optimizing
2022-04-23 14:20:04 | [train_policy] epoch #140 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:04 | [train_policy] epoch #140 | computing loss before
2022-04-23 14:20:04 | [train_policy] epoch #140 | computing gradient
2022-04-23 14:20:04 | [train_policy] epoch #140 | gradient computed
2022-04-23 14:20:04 | [train_policy] epoch #140 | computing descent direction
2022-04-23 14:20:04 | [train_policy] epoch #140 | descent direction computed
2022-04-23 14:20:04 | [train_policy] epoch #140 | backtrack iters: 1
2022-04-23 14:20:04 | [train_policy] epoch #140 | optimization finished
2022-04-23 14:20:04 | [train_policy] epoch #140 | Computing KL after
2022-04-23 14:20:04 | [train_policy] epoch #140 | Computing loss after
2022-04-23 14:20:04 | [train_policy] epoch #140 | Fitting baseline...
2022-04-23 14:20:04 | [train_policy] epoch #140 | Saving snapshot...
2022-04-23 14:20:04 | [train_policy] epoch #140 | Saved
2022-04-23 14:20:04 | [train_policy] epoch #140 | Time 52.33 s
2022-04-23 14:20:04 | [train_policy] epoch #140 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.123958
Evaluation/AverageDiscountedReturn          -45.9562
Evaluation/AverageReturn                    -45.9562
Evaluation/CompletionRate                     0
Evaluation/Iteration                        140
Evaluation/MaxReturn                        -36.1481
Evaluation/MinReturn                        -71.977
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.88282
Extras/EpisodeRewardMean                    -46.1521
LinearFeatureBaseline/ExplainedVariance       0.922122
PolicyExecTime                                0.112875
ProcessExecTime                               0.0124495
TotalEnvSteps                            142692
policy/Entropy                                0.834072
policy/KL                                     0.00640757
policy/KLBefore                               0
policy/LossAfter                             -0.0164246
policy/LossBefore                            -4.41734e-09
policy/Perplexity                             2.30268
policy/dLoss                                  0.0164246
---------------------------------------  ----------------
2022-04-23 14:20:04 | [train_policy] epoch #141 | Obtaining samples for iteration 141...
2022-04-23 14:20:04 | [train_policy] epoch #141 | Logging diagnostics...
2022-04-23 14:20:04 | [train_policy] epoch #141 | Optimizing policy...
2022-04-23 14:20:04 | [train_policy] epoch #141 | Computing loss before
2022-04-23 14:20:04 | [train_policy] epoch #141 | Computing KL before
2022-04-23 14:20:04 | [train_policy] epoch #141 | Optimizing
2022-04-23 14:20:04 | [train_policy] epoch #141 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:04 | [train_policy] epoch #141 | computing loss before
2022-04-23 14:20:04 | [train_policy] epoch #141 | computing gradient
2022-04-23 14:20:04 | [train_policy] epoch #141 | gradient computed
2022-04-23 14:20:04 | [train_policy] epoch #141 | computing descent direction
2022-04-23 14:20:04 | [train_policy] epoch #141 | descent direction computed
2022-04-23 14:20:04 | [train_policy] epoch #141 | backtrack iters: 1
2022-04-23 14:20:04 | [train_policy] epoch #141 | optimization finished
2022-04-23 14:20:04 | [train_policy] epoch #141 | Computing KL after
2022-04-23 14:20:04 | [train_policy] epoch #141 | Computing loss after
2022-04-23 14:20:04 | [train_policy] epoch #141 | Fitting baseline...
2022-04-23 14:20:04 | [train_policy] epoch #141 | Saving snapshot...
2022-04-23 14:20:04 | [train_policy] epoch #141 | Saved
2022-04-23 14:20:04 | [train_policy] epoch #141 | Time 52.70 s
2022-04-23 14:20:04 | [train_policy] epoch #141 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.125803
Evaluation/AverageDiscountedReturn          -47.6951
Evaluation/AverageReturn                    -47.6951
Evaluation/CompletionRate                     0
Evaluation/Iteration                        141
Evaluation/MaxReturn                        -35.0457
Evaluation/MinReturn                       -178.572
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         15.9144
Extras/EpisodeRewardMean                    -47.3326
LinearFeatureBaseline/ExplainedVariance       0.549917
PolicyExecTime                                0.111415
ProcessExecTime                               0.0123546
TotalEnvSteps                            143704
policy/Entropy                                0.834635
policy/KL                                     0.00678981
policy/KLBefore                               0
policy/LossAfter                             -0.0280688
policy/LossBefore                            -6.71436e-09
policy/Perplexity                             2.30397
policy/dLoss                                  0.0280688
---------------------------------------  ----------------
2022-04-23 14:20:04 | [train_policy] epoch #142 | Obtaining samples for iteration 142...
2022-04-23 14:20:05 | [train_policy] epoch #142 | Logging diagnostics...
2022-04-23 14:20:05 | [train_policy] epoch #142 | Optimizing policy...
2022-04-23 14:20:05 | [train_policy] epoch #142 | Computing loss before
2022-04-23 14:20:05 | [train_policy] epoch #142 | Computing KL before
2022-04-23 14:20:05 | [train_policy] epoch #142 | Optimizing
2022-04-23 14:20:05 | [train_policy] epoch #142 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:05 | [train_policy] epoch #142 | computing loss before
2022-04-23 14:20:05 | [train_policy] epoch #142 | computing gradient
2022-04-23 14:20:05 | [train_policy] epoch #142 | gradient computed
2022-04-23 14:20:05 | [train_policy] epoch #142 | computing descent direction
2022-04-23 14:20:05 | [train_policy] epoch #142 | descent direction computed
2022-04-23 14:20:05 | [train_policy] epoch #142 | backtrack iters: 0
2022-04-23 14:20:05 | [train_policy] epoch #142 | optimization finished
2022-04-23 14:20:05 | [train_policy] epoch #142 | Computing KL after
2022-04-23 14:20:05 | [train_policy] epoch #142 | Computing loss after
2022-04-23 14:20:05 | [train_policy] epoch #142 | Fitting baseline...
2022-04-23 14:20:05 | [train_policy] epoch #142 | Saving snapshot...
2022-04-23 14:20:05 | [train_policy] epoch #142 | Saved
2022-04-23 14:20:05 | [train_policy] epoch #142 | Time 53.06 s
2022-04-23 14:20:05 | [train_policy] epoch #142 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.122333
Evaluation/AverageDiscountedReturn          -46.5808
Evaluation/AverageReturn                    -46.5808
Evaluation/CompletionRate                     0
Evaluation/Iteration                        142
Evaluation/MaxReturn                        -36.5453
Evaluation/MinReturn                        -67.7754
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.26593
Extras/EpisodeRewardMean                    -46.3455
LinearFeatureBaseline/ExplainedVariance       0.823474
PolicyExecTime                                0.108853
ProcessExecTime                               0.0118685
TotalEnvSteps                            144716
policy/Entropy                                0.805524
policy/KL                                     0.00953845
policy/KLBefore                               0
policy/LossAfter                             -0.0164125
policy/LossBefore                             3.76946e-09
policy/Perplexity                             2.23787
policy/dLoss                                  0.0164125
---------------------------------------  ----------------
2022-04-23 14:20:05 | [train_policy] epoch #143 | Obtaining samples for iteration 143...
2022-04-23 14:20:05 | [train_policy] epoch #143 | Logging diagnostics...
2022-04-23 14:20:05 | [train_policy] epoch #143 | Optimizing policy...
2022-04-23 14:20:05 | [train_policy] epoch #143 | Computing loss before
2022-04-23 14:20:05 | [train_policy] epoch #143 | Computing KL before
2022-04-23 14:20:05 | [train_policy] epoch #143 | Optimizing
2022-04-23 14:20:05 | [train_policy] epoch #143 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:05 | [train_policy] epoch #143 | computing loss before
2022-04-23 14:20:05 | [train_policy] epoch #143 | computing gradient
2022-04-23 14:20:05 | [train_policy] epoch #143 | gradient computed
2022-04-23 14:20:05 | [train_policy] epoch #143 | computing descent direction
2022-04-23 14:20:05 | [train_policy] epoch #143 | descent direction computed
2022-04-23 14:20:05 | [train_policy] epoch #143 | backtrack iters: 0
2022-04-23 14:20:05 | [train_policy] epoch #143 | optimization finished
2022-04-23 14:20:05 | [train_policy] epoch #143 | Computing KL after
2022-04-23 14:20:05 | [train_policy] epoch #143 | Computing loss after
2022-04-23 14:20:05 | [train_policy] epoch #143 | Fitting baseline...
2022-04-23 14:20:05 | [train_policy] epoch #143 | Saving snapshot...
2022-04-23 14:20:05 | [train_policy] epoch #143 | Saved
2022-04-23 14:20:05 | [train_policy] epoch #143 | Time 53.42 s
2022-04-23 14:20:05 | [train_policy] epoch #143 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.118602
Evaluation/AverageDiscountedReturn          -46.5688
Evaluation/AverageReturn                    -46.5688
Evaluation/CompletionRate                     0
Evaluation/Iteration                        143
Evaluation/MaxReturn                        -35.346
Evaluation/MinReturn                       -104.536
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.13604
Extras/EpisodeRewardMean                    -46.4898
LinearFeatureBaseline/ExplainedVariance       0.859769
PolicyExecTime                                0.109632
ProcessExecTime                               0.0118492
TotalEnvSteps                            145728
policy/Entropy                                0.808851
policy/KL                                     0.00958686
policy/KLBefore                               0
policy/LossAfter                             -0.0161056
policy/LossBefore                             3.3454e-08
policy/Perplexity                             2.24533
policy/dLoss                                  0.0161057
---------------------------------------  ---------------
2022-04-23 14:20:05 | [train_policy] epoch #144 | Obtaining samples for iteration 144...
2022-04-23 14:20:05 | [train_policy] epoch #144 | Logging diagnostics...
2022-04-23 14:20:05 | [train_policy] epoch #144 | Optimizing policy...
2022-04-23 14:20:05 | [train_policy] epoch #144 | Computing loss before
2022-04-23 14:20:05 | [train_policy] epoch #144 | Computing KL before
2022-04-23 14:20:05 | [train_policy] epoch #144 | Optimizing
2022-04-23 14:20:05 | [train_policy] epoch #144 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:05 | [train_policy] epoch #144 | computing loss before
2022-04-23 14:20:05 | [train_policy] epoch #144 | computing gradient
2022-04-23 14:20:05 | [train_policy] epoch #144 | gradient computed
2022-04-23 14:20:05 | [train_policy] epoch #144 | computing descent direction
2022-04-23 14:20:06 | [train_policy] epoch #144 | descent direction computed
2022-04-23 14:20:06 | [train_policy] epoch #144 | backtrack iters: 0
2022-04-23 14:20:06 | [train_policy] epoch #144 | optimization finished
2022-04-23 14:20:06 | [train_policy] epoch #144 | Computing KL after
2022-04-23 14:20:06 | [train_policy] epoch #144 | Computing loss after
2022-04-23 14:20:06 | [train_policy] epoch #144 | Fitting baseline...
2022-04-23 14:20:06 | [train_policy] epoch #144 | Saving snapshot...
2022-04-23 14:20:06 | [train_policy] epoch #144 | Saved
2022-04-23 14:20:06 | [train_policy] epoch #144 | Time 53.79 s
2022-04-23 14:20:06 | [train_policy] epoch #144 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.11957
Evaluation/AverageDiscountedReturn          -45.6407
Evaluation/AverageReturn                    -45.6407
Evaluation/CompletionRate                     0
Evaluation/Iteration                        144
Evaluation/MaxReturn                        -36.2885
Evaluation/MinReturn                        -60.1727
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.86412
Extras/EpisodeRewardMean                    -45.5178
LinearFeatureBaseline/ExplainedVariance       0.933917
PolicyExecTime                                0.113448
ProcessExecTime                               0.0122256
TotalEnvSteps                            146740
policy/Entropy                                0.807313
policy/KL                                     0.00942079
policy/KLBefore                               0
policy/LossAfter                             -0.0135001
policy/LossBefore                             5.18301e-09
policy/Perplexity                             2.24188
policy/dLoss                                  0.0135001
---------------------------------------  ----------------
2022-04-23 14:20:06 | [train_policy] epoch #145 | Obtaining samples for iteration 145...
2022-04-23 14:20:06 | [train_policy] epoch #145 | Logging diagnostics...
2022-04-23 14:20:06 | [train_policy] epoch #145 | Optimizing policy...
2022-04-23 14:20:06 | [train_policy] epoch #145 | Computing loss before
2022-04-23 14:20:06 | [train_policy] epoch #145 | Computing KL before
2022-04-23 14:20:06 | [train_policy] epoch #145 | Optimizing
2022-04-23 14:20:06 | [train_policy] epoch #145 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:06 | [train_policy] epoch #145 | computing loss before
2022-04-23 14:20:06 | [train_policy] epoch #145 | computing gradient
2022-04-23 14:20:06 | [train_policy] epoch #145 | gradient computed
2022-04-23 14:20:06 | [train_policy] epoch #145 | computing descent direction
2022-04-23 14:20:06 | [train_policy] epoch #145 | descent direction computed
2022-04-23 14:20:06 | [train_policy] epoch #145 | backtrack iters: 0
2022-04-23 14:20:06 | [train_policy] epoch #145 | optimization finished
2022-04-23 14:20:06 | [train_policy] epoch #145 | Computing KL after
2022-04-23 14:20:06 | [train_policy] epoch #145 | Computing loss after
2022-04-23 14:20:06 | [train_policy] epoch #145 | Fitting baseline...
2022-04-23 14:20:06 | [train_policy] epoch #145 | Saving snapshot...
2022-04-23 14:20:06 | [train_policy] epoch #145 | Saved
2022-04-23 14:20:06 | [train_policy] epoch #145 | Time 54.15 s
2022-04-23 14:20:06 | [train_policy] epoch #145 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.120367
Evaluation/AverageDiscountedReturn          -45.4438
Evaluation/AverageReturn                    -45.4438
Evaluation/CompletionRate                     0
Evaluation/Iteration                        145
Evaluation/MaxReturn                        -32.7532
Evaluation/MinReturn                        -74.7794
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.90983
Extras/EpisodeRewardMean                    -45.4347
LinearFeatureBaseline/ExplainedVariance       0.91524
PolicyExecTime                                0.109124
ProcessExecTime                               0.0118613
TotalEnvSteps                            147752
policy/Entropy                                0.795681
policy/KL                                     0.00957835
policy/KLBefore                               0
policy/LossAfter                             -0.0210036
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             2.21595
policy/dLoss                                  0.0210036
---------------------------------------  ----------------
2022-04-23 14:20:06 | [train_policy] epoch #146 | Obtaining samples for iteration 146...
2022-04-23 14:20:06 | [train_policy] epoch #146 | Logging diagnostics...
2022-04-23 14:20:06 | [train_policy] epoch #146 | Optimizing policy...
2022-04-23 14:20:06 | [train_policy] epoch #146 | Computing loss before
2022-04-23 14:20:06 | [train_policy] epoch #146 | Computing KL before
2022-04-23 14:20:06 | [train_policy] epoch #146 | Optimizing
2022-04-23 14:20:06 | [train_policy] epoch #146 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:06 | [train_policy] epoch #146 | computing loss before
2022-04-23 14:20:06 | [train_policy] epoch #146 | computing gradient
2022-04-23 14:20:06 | [train_policy] epoch #146 | gradient computed
2022-04-23 14:20:06 | [train_policy] epoch #146 | computing descent direction
2022-04-23 14:20:06 | [train_policy] epoch #146 | descent direction computed
2022-04-23 14:20:06 | [train_policy] epoch #146 | backtrack iters: 1
2022-04-23 14:20:06 | [train_policy] epoch #146 | optimization finished
2022-04-23 14:20:06 | [train_policy] epoch #146 | Computing KL after
2022-04-23 14:20:06 | [train_policy] epoch #146 | Computing loss after
2022-04-23 14:20:06 | [train_policy] epoch #146 | Fitting baseline...
2022-04-23 14:20:06 | [train_policy] epoch #146 | Saving snapshot...
2022-04-23 14:20:06 | [train_policy] epoch #146 | Saved
2022-04-23 14:20:06 | [train_policy] epoch #146 | Time 54.52 s
2022-04-23 14:20:06 | [train_policy] epoch #146 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.129777
Evaluation/AverageDiscountedReturn         -111.42
Evaluation/AverageReturn                   -111.42
Evaluation/CompletionRate                     0
Evaluation/Iteration                        146
Evaluation/MaxReturn                        -35.2172
Evaluation/MinReturn                      -2063.24
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        358.159
Extras/EpisodeRewardMean                   -105.979
LinearFeatureBaseline/ExplainedVariance       0.0107997
PolicyExecTime                                0.109138
ProcessExecTime                               0.0127542
TotalEnvSteps                            148764
policy/Entropy                                0.765023
policy/KL                                     0.00733356
policy/KLBefore                               0
policy/LossAfter                             -0.0229029
policy/LossBefore                            -2.37947e-08
policy/Perplexity                             2.14904
policy/dLoss                                  0.0229028
---------------------------------------  ----------------
2022-04-23 14:20:06 | [train_policy] epoch #147 | Obtaining samples for iteration 147...
2022-04-23 14:20:07 | [train_policy] epoch #147 | Logging diagnostics...
2022-04-23 14:20:07 | [train_policy] epoch #147 | Optimizing policy...
2022-04-23 14:20:07 | [train_policy] epoch #147 | Computing loss before
2022-04-23 14:20:07 | [train_policy] epoch #147 | Computing KL before
2022-04-23 14:20:07 | [train_policy] epoch #147 | Optimizing
2022-04-23 14:20:07 | [train_policy] epoch #147 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:07 | [train_policy] epoch #147 | computing loss before
2022-04-23 14:20:07 | [train_policy] epoch #147 | computing gradient
2022-04-23 14:20:07 | [train_policy] epoch #147 | gradient computed
2022-04-23 14:20:07 | [train_policy] epoch #147 | computing descent direction
2022-04-23 14:20:07 | [train_policy] epoch #147 | descent direction computed
2022-04-23 14:20:07 | [train_policy] epoch #147 | backtrack iters: 0
2022-04-23 14:20:07 | [train_policy] epoch #147 | optimization finished
2022-04-23 14:20:07 | [train_policy] epoch #147 | Computing KL after
2022-04-23 14:20:07 | [train_policy] epoch #147 | Computing loss after
2022-04-23 14:20:07 | [train_policy] epoch #147 | Fitting baseline...
2022-04-23 14:20:07 | [train_policy] epoch #147 | Saving snapshot...
2022-04-23 14:20:07 | [train_policy] epoch #147 | Saved
2022-04-23 14:20:07 | [train_policy] epoch #147 | Time 54.89 s
2022-04-23 14:20:07 | [train_policy] epoch #147 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.129811
Evaluation/AverageDiscountedReturn          -46.3658
Evaluation/AverageReturn                    -46.3658
Evaluation/CompletionRate                     0
Evaluation/Iteration                        147
Evaluation/MaxReturn                        -35.3784
Evaluation/MinReturn                       -107.16
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.4271
Extras/EpisodeRewardMean                    -46.3616
LinearFeatureBaseline/ExplainedVariance    -149.677
PolicyExecTime                                0.106105
ProcessExecTime                               0.0126078
TotalEnvSteps                            149776
policy/Entropy                                0.755897
policy/KL                                     0.00973703
policy/KLBefore                               0
policy/LossAfter                             -0.0287583
policy/LossBefore                            -2.56795e-08
policy/Perplexity                             2.12952
policy/dLoss                                  0.0287583
---------------------------------------  ----------------
2022-04-23 14:20:07 | [train_policy] epoch #148 | Obtaining samples for iteration 148...
2022-04-23 14:20:07 | [train_policy] epoch #148 | Logging diagnostics...
2022-04-23 14:20:07 | [train_policy] epoch #148 | Optimizing policy...
2022-04-23 14:20:07 | [train_policy] epoch #148 | Computing loss before
2022-04-23 14:20:07 | [train_policy] epoch #148 | Computing KL before
2022-04-23 14:20:07 | [train_policy] epoch #148 | Optimizing
2022-04-23 14:20:07 | [train_policy] epoch #148 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:07 | [train_policy] epoch #148 | computing loss before
2022-04-23 14:20:07 | [train_policy] epoch #148 | computing gradient
2022-04-23 14:20:07 | [train_policy] epoch #148 | gradient computed
2022-04-23 14:20:07 | [train_policy] epoch #148 | computing descent direction
2022-04-23 14:20:07 | [train_policy] epoch #148 | descent direction computed
2022-04-23 14:20:07 | [train_policy] epoch #148 | backtrack iters: 1
2022-04-23 14:20:07 | [train_policy] epoch #148 | optimization finished
2022-04-23 14:20:07 | [train_policy] epoch #148 | Computing KL after
2022-04-23 14:20:07 | [train_policy] epoch #148 | Computing loss after
2022-04-23 14:20:07 | [train_policy] epoch #148 | Fitting baseline...
2022-04-23 14:20:07 | [train_policy] epoch #148 | Saving snapshot...
2022-04-23 14:20:07 | [train_policy] epoch #148 | Saved
2022-04-23 14:20:07 | [train_policy] epoch #148 | Time 55.24 s
2022-04-23 14:20:07 | [train_policy] epoch #148 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.121569
Evaluation/AverageDiscountedReturn          -66.9775
Evaluation/AverageReturn                    -66.9775
Evaluation/CompletionRate                     0
Evaluation/Iteration                        148
Evaluation/MaxReturn                        -35.3361
Evaluation/MinReturn                      -2067.52
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.84
Extras/EpisodeRewardMean                    -65.3171
LinearFeatureBaseline/ExplainedVariance       0.0118861
PolicyExecTime                                0.101838
ProcessExecTime                               0.0115719
TotalEnvSteps                            150788
policy/Entropy                                0.75016
policy/KL                                     0.00687725
policy/KLBefore                               0
policy/LossAfter                             -0.0259022
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             2.11734
policy/dLoss                                  0.0259022
---------------------------------------  ----------------
2022-04-23 14:20:07 | [train_policy] epoch #149 | Obtaining samples for iteration 149...
2022-04-23 14:20:07 | [train_policy] epoch #149 | Logging diagnostics...
2022-04-23 14:20:07 | [train_policy] epoch #149 | Optimizing policy...
2022-04-23 14:20:07 | [train_policy] epoch #149 | Computing loss before
2022-04-23 14:20:07 | [train_policy] epoch #149 | Computing KL before
2022-04-23 14:20:07 | [train_policy] epoch #149 | Optimizing
2022-04-23 14:20:07 | [train_policy] epoch #149 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:07 | [train_policy] epoch #149 | computing loss before
2022-04-23 14:20:07 | [train_policy] epoch #149 | computing gradient
2022-04-23 14:20:07 | [train_policy] epoch #149 | gradient computed
2022-04-23 14:20:07 | [train_policy] epoch #149 | computing descent direction
2022-04-23 14:20:07 | [train_policy] epoch #149 | descent direction computed
2022-04-23 14:20:07 | [train_policy] epoch #149 | backtrack iters: 0
2022-04-23 14:20:07 | [train_policy] epoch #149 | optimization finished
2022-04-23 14:20:07 | [train_policy] epoch #149 | Computing KL after
2022-04-23 14:20:07 | [train_policy] epoch #149 | Computing loss after
2022-04-23 14:20:07 | [train_policy] epoch #149 | Fitting baseline...
2022-04-23 14:20:07 | [train_policy] epoch #149 | Saving snapshot...
2022-04-23 14:20:07 | [train_policy] epoch #149 | Saved
2022-04-23 14:20:07 | [train_policy] epoch #149 | Time 55.59 s
2022-04-23 14:20:07 | [train_policy] epoch #149 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116497
Evaluation/AverageDiscountedReturn          -47.206
Evaluation/AverageReturn                    -47.206
Evaluation/CompletionRate                     0
Evaluation/Iteration                        149
Evaluation/MaxReturn                        -33.8434
Evaluation/MinReturn                       -250.486
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         22.7973
Extras/EpisodeRewardMean                    -67.3301
LinearFeatureBaseline/ExplainedVariance      -9.93284
PolicyExecTime                                0.10248
ProcessExecTime                               0.0113664
TotalEnvSteps                            151800
policy/Entropy                                0.753945
policy/KL                                     0.00973524
policy/KLBefore                               0
policy/LossAfter                             -0.0272501
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             2.12537
policy/dLoss                                  0.0272501
---------------------------------------  ----------------
2022-04-23 14:20:07 | [train_policy] epoch #150 | Obtaining samples for iteration 150...
2022-04-23 14:20:08 | [train_policy] epoch #150 | Logging diagnostics...
2022-04-23 14:20:08 | [train_policy] epoch #150 | Optimizing policy...
2022-04-23 14:20:08 | [train_policy] epoch #150 | Computing loss before
2022-04-23 14:20:08 | [train_policy] epoch #150 | Computing KL before
2022-04-23 14:20:08 | [train_policy] epoch #150 | Optimizing
2022-04-23 14:20:08 | [train_policy] epoch #150 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:08 | [train_policy] epoch #150 | computing loss before
2022-04-23 14:20:08 | [train_policy] epoch #150 | computing gradient
2022-04-23 14:20:08 | [train_policy] epoch #150 | gradient computed
2022-04-23 14:20:08 | [train_policy] epoch #150 | computing descent direction
2022-04-23 14:20:08 | [train_policy] epoch #150 | descent direction computed
2022-04-23 14:20:08 | [train_policy] epoch #150 | backtrack iters: 1
2022-04-23 14:20:08 | [train_policy] epoch #150 | optimization finished
2022-04-23 14:20:08 | [train_policy] epoch #150 | Computing KL after
2022-04-23 14:20:08 | [train_policy] epoch #150 | Computing loss after
2022-04-23 14:20:08 | [train_policy] epoch #150 | Fitting baseline...
2022-04-23 14:20:08 | [train_policy] epoch #150 | Saving snapshot...
2022-04-23 14:20:08 | [train_policy] epoch #150 | Saved
2022-04-23 14:20:08 | [train_policy] epoch #150 | Time 55.93 s
2022-04-23 14:20:08 | [train_policy] epoch #150 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116338
Evaluation/AverageDiscountedReturn          -67.2567
Evaluation/AverageReturn                    -67.2567
Evaluation/CompletionRate                     0
Evaluation/Iteration                        150
Evaluation/MaxReturn                        -33.2821
Evaluation/MinReturn                      -2061.01
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.105
Extras/EpisodeRewardMean                    -65.228
LinearFeatureBaseline/ExplainedVariance       0.0290252
PolicyExecTime                                0.098613
ProcessExecTime                               0.0109625
TotalEnvSteps                            152812
policy/Entropy                                0.754034
policy/KL                                     0.0079884
policy/KLBefore                               0
policy/LossAfter                             -0.0226452
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             2.12556
policy/dLoss                                  0.0226452
---------------------------------------  ----------------
2022-04-23 14:20:08 | [train_policy] epoch #151 | Obtaining samples for iteration 151...
2022-04-23 14:20:08 | [train_policy] epoch #151 | Logging diagnostics...
2022-04-23 14:20:08 | [train_policy] epoch #151 | Optimizing policy...
2022-04-23 14:20:08 | [train_policy] epoch #151 | Computing loss before
2022-04-23 14:20:08 | [train_policy] epoch #151 | Computing KL before
2022-04-23 14:20:08 | [train_policy] epoch #151 | Optimizing
2022-04-23 14:20:08 | [train_policy] epoch #151 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:08 | [train_policy] epoch #151 | computing loss before
2022-04-23 14:20:08 | [train_policy] epoch #151 | computing gradient
2022-04-23 14:20:08 | [train_policy] epoch #151 | gradient computed
2022-04-23 14:20:08 | [train_policy] epoch #151 | computing descent direction
2022-04-23 14:20:08 | [train_policy] epoch #151 | descent direction computed
2022-04-23 14:20:08 | [train_policy] epoch #151 | backtrack iters: 1
2022-04-23 14:20:08 | [train_policy] epoch #151 | optimization finished
2022-04-23 14:20:08 | [train_policy] epoch #151 | Computing KL after
2022-04-23 14:20:08 | [train_policy] epoch #151 | Computing loss after
2022-04-23 14:20:08 | [train_policy] epoch #151 | Fitting baseline...
2022-04-23 14:20:08 | [train_policy] epoch #151 | Saving snapshot...
2022-04-23 14:20:08 | [train_policy] epoch #151 | Saved
2022-04-23 14:20:08 | [train_policy] epoch #151 | Time 56.31 s
2022-04-23 14:20:08 | [train_policy] epoch #151 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.126068
Evaluation/AverageDiscountedReturn          -46.0561
Evaluation/AverageReturn                    -46.0561
Evaluation/CompletionRate                     0
Evaluation/Iteration                        151
Evaluation/MaxReturn                        -35.0389
Evaluation/MinReturn                        -65.3217
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.77383
Extras/EpisodeRewardMean                    -66.4017
LinearFeatureBaseline/ExplainedVariance     -25.2235
PolicyExecTime                                0.10657
ProcessExecTime                               0.0124698
TotalEnvSteps                            153824
policy/Entropy                                0.749518
policy/KL                                     0.00654796
policy/KLBefore                               0
policy/LossAfter                             -0.0202539
policy/LossBefore                             2.92133e-08
policy/Perplexity                             2.11598
policy/dLoss                                  0.0202539
---------------------------------------  ----------------
2022-04-23 14:20:08 | [train_policy] epoch #152 | Obtaining samples for iteration 152...
2022-04-23 14:20:08 | [train_policy] epoch #152 | Logging diagnostics...
2022-04-23 14:20:08 | [train_policy] epoch #152 | Optimizing policy...
2022-04-23 14:20:08 | [train_policy] epoch #152 | Computing loss before
2022-04-23 14:20:08 | [train_policy] epoch #152 | Computing KL before
2022-04-23 14:20:08 | [train_policy] epoch #152 | Optimizing
2022-04-23 14:20:08 | [train_policy] epoch #152 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:08 | [train_policy] epoch #152 | computing loss before
2022-04-23 14:20:08 | [train_policy] epoch #152 | computing gradient
2022-04-23 14:20:08 | [train_policy] epoch #152 | gradient computed
2022-04-23 14:20:08 | [train_policy] epoch #152 | computing descent direction
2022-04-23 14:20:08 | [train_policy] epoch #152 | descent direction computed
2022-04-23 14:20:08 | [train_policy] epoch #152 | backtrack iters: 2
2022-04-23 14:20:08 | [train_policy] epoch #152 | optimization finished
2022-04-23 14:20:08 | [train_policy] epoch #152 | Computing KL after
2022-04-23 14:20:08 | [train_policy] epoch #152 | Computing loss after
2022-04-23 14:20:08 | [train_policy] epoch #152 | Fitting baseline...
2022-04-23 14:20:08 | [train_policy] epoch #152 | Saving snapshot...
2022-04-23 14:20:08 | [train_policy] epoch #152 | Saved
2022-04-23 14:20:08 | [train_policy] epoch #152 | Time 56.67 s
2022-04-23 14:20:08 | [train_policy] epoch #152 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.122903
Evaluation/AverageDiscountedReturn          -65.9439
Evaluation/AverageReturn                    -65.9439
Evaluation/CompletionRate                     0
Evaluation/Iteration                        152
Evaluation/MaxReturn                        -36.0817
Evaluation/MinReturn                      -2061.25
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.213
Extras/EpisodeRewardMean                    -64.6887
LinearFeatureBaseline/ExplainedVariance       0.010692
PolicyExecTime                                0.105622
ProcessExecTime                               0.0113621
TotalEnvSteps                            154836
policy/Entropy                                0.747863
policy/KL                                     0.0062258
policy/KLBefore                               0
policy/LossAfter                             -0.0272496
policy/LossBefore                             1.50779e-08
policy/Perplexity                             2.11248
policy/dLoss                                  0.0272496
---------------------------------------  ----------------
2022-04-23 14:20:08 | [train_policy] epoch #153 | Obtaining samples for iteration 153...
2022-04-23 14:20:09 | [train_policy] epoch #153 | Logging diagnostics...
2022-04-23 14:20:09 | [train_policy] epoch #153 | Optimizing policy...
2022-04-23 14:20:09 | [train_policy] epoch #153 | Computing loss before
2022-04-23 14:20:09 | [train_policy] epoch #153 | Computing KL before
2022-04-23 14:20:09 | [train_policy] epoch #153 | Optimizing
2022-04-23 14:20:09 | [train_policy] epoch #153 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:09 | [train_policy] epoch #153 | computing loss before
2022-04-23 14:20:09 | [train_policy] epoch #153 | computing gradient
2022-04-23 14:20:09 | [train_policy] epoch #153 | gradient computed
2022-04-23 14:20:09 | [train_policy] epoch #153 | computing descent direction
2022-04-23 14:20:09 | [train_policy] epoch #153 | descent direction computed
2022-04-23 14:20:09 | [train_policy] epoch #153 | backtrack iters: 0
2022-04-23 14:20:09 | [train_policy] epoch #153 | optimization finished
2022-04-23 14:20:09 | [train_policy] epoch #153 | Computing KL after
2022-04-23 14:20:09 | [train_policy] epoch #153 | Computing loss after
2022-04-23 14:20:09 | [train_policy] epoch #153 | Fitting baseline...
2022-04-23 14:20:09 | [train_policy] epoch #153 | Saving snapshot...
2022-04-23 14:20:09 | [train_policy] epoch #153 | Saved
2022-04-23 14:20:09 | [train_policy] epoch #153 | Time 57.02 s
2022-04-23 14:20:09 | [train_policy] epoch #153 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116602
Evaluation/AverageDiscountedReturn          -67.7968
Evaluation/AverageReturn                    -67.7968
Evaluation/CompletionRate                     0
Evaluation/Iteration                        153
Evaluation/MaxReturn                        -33.4615
Evaluation/MinReturn                      -2053.34
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.253
Extras/EpisodeRewardMean                    -65.882
LinearFeatureBaseline/ExplainedVariance      -0.0757194
PolicyExecTime                                0.106337
ProcessExecTime                               0.0113008
TotalEnvSteps                            155848
policy/Entropy                                0.732351
policy/KL                                     0.00963107
policy/KLBefore                               0
policy/LossAfter                             -0.0195286
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             2.07997
policy/dLoss                                  0.0195286
---------------------------------------  ----------------
2022-04-23 14:20:09 | [train_policy] epoch #154 | Obtaining samples for iteration 154...
2022-04-23 14:20:09 | [train_policy] epoch #154 | Logging diagnostics...
2022-04-23 14:20:09 | [train_policy] epoch #154 | Optimizing policy...
2022-04-23 14:20:09 | [train_policy] epoch #154 | Computing loss before
2022-04-23 14:20:09 | [train_policy] epoch #154 | Computing KL before
2022-04-23 14:20:09 | [train_policy] epoch #154 | Optimizing
2022-04-23 14:20:09 | [train_policy] epoch #154 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:09 | [train_policy] epoch #154 | computing loss before
2022-04-23 14:20:09 | [train_policy] epoch #154 | computing gradient
2022-04-23 14:20:09 | [train_policy] epoch #154 | gradient computed
2022-04-23 14:20:09 | [train_policy] epoch #154 | computing descent direction
2022-04-23 14:20:09 | [train_policy] epoch #154 | descent direction computed
2022-04-23 14:20:09 | [train_policy] epoch #154 | backtrack iters: 1
2022-04-23 14:20:09 | [train_policy] epoch #154 | optimization finished
2022-04-23 14:20:09 | [train_policy] epoch #154 | Computing KL after
2022-04-23 14:20:09 | [train_policy] epoch #154 | Computing loss after
2022-04-23 14:20:09 | [train_policy] epoch #154 | Fitting baseline...
2022-04-23 14:20:09 | [train_policy] epoch #154 | Saving snapshot...
2022-04-23 14:20:09 | [train_policy] epoch #154 | Saved
2022-04-23 14:20:09 | [train_policy] epoch #154 | Time 57.38 s
2022-04-23 14:20:09 | [train_policy] epoch #154 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.120313
Evaluation/AverageDiscountedReturn          -45.8878
Evaluation/AverageReturn                    -45.8878
Evaluation/CompletionRate                     0
Evaluation/Iteration                        154
Evaluation/MaxReturn                        -34.3426
Evaluation/MinReturn                        -64.3453
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.17047
Extras/EpisodeRewardMean                    -45.7235
LinearFeatureBaseline/ExplainedVariance     -11.2674
PolicyExecTime                                0.103789
ProcessExecTime                               0.0116258
TotalEnvSteps                            156860
policy/Entropy                                0.721348
policy/KL                                     0.00647895
policy/KLBefore                               0
policy/LossAfter                             -0.0131855
policy/LossBefore                             3.06269e-08
policy/Perplexity                             2.0572
policy/dLoss                                  0.0131855
---------------------------------------  ----------------
2022-04-23 14:20:09 | [train_policy] epoch #155 | Obtaining samples for iteration 155...
2022-04-23 14:20:09 | [train_policy] epoch #155 | Logging diagnostics...
2022-04-23 14:20:09 | [train_policy] epoch #155 | Optimizing policy...
2022-04-23 14:20:09 | [train_policy] epoch #155 | Computing loss before
2022-04-23 14:20:09 | [train_policy] epoch #155 | Computing KL before
2022-04-23 14:20:09 | [train_policy] epoch #155 | Optimizing
2022-04-23 14:20:09 | [train_policy] epoch #155 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:09 | [train_policy] epoch #155 | computing loss before
2022-04-23 14:20:09 | [train_policy] epoch #155 | computing gradient
2022-04-23 14:20:09 | [train_policy] epoch #155 | gradient computed
2022-04-23 14:20:09 | [train_policy] epoch #155 | computing descent direction
2022-04-23 14:20:09 | [train_policy] epoch #155 | descent direction computed
2022-04-23 14:20:09 | [train_policy] epoch #155 | backtrack iters: 1
2022-04-23 14:20:09 | [train_policy] epoch #155 | optimization finished
2022-04-23 14:20:09 | [train_policy] epoch #155 | Computing KL after
2022-04-23 14:20:09 | [train_policy] epoch #155 | Computing loss after
2022-04-23 14:20:09 | [train_policy] epoch #155 | Fitting baseline...
2022-04-23 14:20:09 | [train_policy] epoch #155 | Saving snapshot...
2022-04-23 14:20:10 | [train_policy] epoch #155 | Saved
2022-04-23 14:20:10 | [train_policy] epoch #155 | Time 57.72 s
2022-04-23 14:20:10 | [train_policy] epoch #155 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118384
Evaluation/AverageDiscountedReturn          -46.3392
Evaluation/AverageReturn                    -46.3392
Evaluation/CompletionRate                     0
Evaluation/Iteration                        155
Evaluation/MaxReturn                        -34.1626
Evaluation/MinReturn                        -64.878
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.03673
Extras/EpisodeRewardMean                    -46.053
LinearFeatureBaseline/ExplainedVariance       0.945096
PolicyExecTime                                0.099714
ProcessExecTime                               0.0112658
TotalEnvSteps                            157872
policy/Entropy                                0.689313
policy/KL                                     0.00918629
policy/KLBefore                               0
policy/LossAfter                             -0.0161148
policy/LossBefore                            -4.47624e-09
policy/Perplexity                             1.99235
policy/dLoss                                  0.0161148
---------------------------------------  ----------------
2022-04-23 14:20:10 | [train_policy] epoch #156 | Obtaining samples for iteration 156...
2022-04-23 14:20:10 | [train_policy] epoch #156 | Logging diagnostics...
2022-04-23 14:20:10 | [train_policy] epoch #156 | Optimizing policy...
2022-04-23 14:20:10 | [train_policy] epoch #156 | Computing loss before
2022-04-23 14:20:10 | [train_policy] epoch #156 | Computing KL before
2022-04-23 14:20:10 | [train_policy] epoch #156 | Optimizing
2022-04-23 14:20:10 | [train_policy] epoch #156 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:10 | [train_policy] epoch #156 | computing loss before
2022-04-23 14:20:10 | [train_policy] epoch #156 | computing gradient
2022-04-23 14:20:10 | [train_policy] epoch #156 | gradient computed
2022-04-23 14:20:10 | [train_policy] epoch #156 | computing descent direction
2022-04-23 14:20:10 | [train_policy] epoch #156 | descent direction computed
2022-04-23 14:20:10 | [train_policy] epoch #156 | backtrack iters: 1
2022-04-23 14:20:10 | [train_policy] epoch #156 | optimization finished
2022-04-23 14:20:10 | [train_policy] epoch #156 | Computing KL after
2022-04-23 14:20:10 | [train_policy] epoch #156 | Computing loss after
2022-04-23 14:20:10 | [train_policy] epoch #156 | Fitting baseline...
2022-04-23 14:20:10 | [train_policy] epoch #156 | Saving snapshot...
2022-04-23 14:20:10 | [train_policy] epoch #156 | Saved
2022-04-23 14:20:10 | [train_policy] epoch #156 | Time 58.08 s
2022-04-23 14:20:10 | [train_policy] epoch #156 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.127189
Evaluation/AverageDiscountedReturn          -69.3241
Evaluation/AverageReturn                    -69.3241
Evaluation/CompletionRate                     0
Evaluation/Iteration                        156
Evaluation/MaxReturn                        -33.4527
Evaluation/MinReturn                      -2062.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.285
Extras/EpisodeRewardMean                    -67.2455
LinearFeatureBaseline/ExplainedVariance       0.0113513
PolicyExecTime                                0.107751
ProcessExecTime                               0.0124168
TotalEnvSteps                            158884
policy/Entropy                                0.659677
policy/KL                                     0.00656437
policy/KLBefore                               0
policy/LossAfter                             -0.0112747
policy/LossBefore                             8.48129e-09
policy/Perplexity                             1.93417
policy/dLoss                                  0.0112747
---------------------------------------  ----------------
2022-04-23 14:20:10 | [train_policy] epoch #157 | Obtaining samples for iteration 157...
2022-04-23 14:20:10 | [train_policy] epoch #157 | Logging diagnostics...
2022-04-23 14:20:10 | [train_policy] epoch #157 | Optimizing policy...
2022-04-23 14:20:10 | [train_policy] epoch #157 | Computing loss before
2022-04-23 14:20:10 | [train_policy] epoch #157 | Computing KL before
2022-04-23 14:20:10 | [train_policy] epoch #157 | Optimizing
2022-04-23 14:20:10 | [train_policy] epoch #157 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:10 | [train_policy] epoch #157 | computing loss before
2022-04-23 14:20:10 | [train_policy] epoch #157 | computing gradient
2022-04-23 14:20:10 | [train_policy] epoch #157 | gradient computed
2022-04-23 14:20:10 | [train_policy] epoch #157 | computing descent direction
2022-04-23 14:20:10 | [train_policy] epoch #157 | descent direction computed
2022-04-23 14:20:10 | [train_policy] epoch #157 | backtrack iters: 0
2022-04-23 14:20:10 | [train_policy] epoch #157 | optimization finished
2022-04-23 14:20:10 | [train_policy] epoch #157 | Computing KL after
2022-04-23 14:20:10 | [train_policy] epoch #157 | Computing loss after
2022-04-23 14:20:10 | [train_policy] epoch #157 | Fitting baseline...
2022-04-23 14:20:10 | [train_policy] epoch #157 | Saving snapshot...
2022-04-23 14:20:10 | [train_policy] epoch #157 | Saved
2022-04-23 14:20:10 | [train_policy] epoch #157 | Time 58.43 s
2022-04-23 14:20:10 | [train_policy] epoch #157 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117985
Evaluation/AverageDiscountedReturn          -46.8877
Evaluation/AverageReturn                    -46.8877
Evaluation/CompletionRate                     0
Evaluation/Iteration                        157
Evaluation/MaxReturn                        -31.7514
Evaluation/MinReturn                       -201.675
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         18.5762
Extras/EpisodeRewardMean                    -46.9222
LinearFeatureBaseline/ExplainedVariance      -9.16621
PolicyExecTime                                0.0999506
ProcessExecTime                               0.0112538
TotalEnvSteps                            159896
policy/Entropy                                0.658628
policy/KL                                     0.00991607
policy/KLBefore                               0
policy/LossAfter                             -0.0253755
policy/LossBefore                             1.90829e-08
policy/Perplexity                             1.93214
policy/dLoss                                  0.0253755
---------------------------------------  ----------------
2022-04-23 14:20:10 | [train_policy] epoch #158 | Obtaining samples for iteration 158...
2022-04-23 14:20:10 | [train_policy] epoch #158 | Logging diagnostics...
2022-04-23 14:20:10 | [train_policy] epoch #158 | Optimizing policy...
2022-04-23 14:20:10 | [train_policy] epoch #158 | Computing loss before
2022-04-23 14:20:10 | [train_policy] epoch #158 | Computing KL before
2022-04-23 14:20:10 | [train_policy] epoch #158 | Optimizing
2022-04-23 14:20:10 | [train_policy] epoch #158 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:10 | [train_policy] epoch #158 | computing loss before
2022-04-23 14:20:10 | [train_policy] epoch #158 | computing gradient
2022-04-23 14:20:10 | [train_policy] epoch #158 | gradient computed
2022-04-23 14:20:10 | [train_policy] epoch #158 | computing descent direction
2022-04-23 14:20:11 | [train_policy] epoch #158 | descent direction computed
2022-04-23 14:20:11 | [train_policy] epoch #158 | backtrack iters: 1
2022-04-23 14:20:11 | [train_policy] epoch #158 | optimization finished
2022-04-23 14:20:11 | [train_policy] epoch #158 | Computing KL after
2022-04-23 14:20:11 | [train_policy] epoch #158 | Computing loss after
2022-04-23 14:20:11 | [train_policy] epoch #158 | Fitting baseline...
2022-04-23 14:20:11 | [train_policy] epoch #158 | Saving snapshot...
2022-04-23 14:20:11 | [train_policy] epoch #158 | Saved
2022-04-23 14:20:11 | [train_policy] epoch #158 | Time 58.78 s
2022-04-23 14:20:11 | [train_policy] epoch #158 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118224
Evaluation/AverageDiscountedReturn          -68.3646
Evaluation/AverageReturn                    -68.3646
Evaluation/CompletionRate                     0
Evaluation/Iteration                        158
Evaluation/MaxReturn                        -32.3361
Evaluation/MinReturn                      -2062.79
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.211
Extras/EpisodeRewardMean                    -68.2862
LinearFeatureBaseline/ExplainedVariance       0.0237158
PolicyExecTime                                0.101831
ProcessExecTime                               0.0112612
TotalEnvSteps                            160908
policy/Entropy                                0.643067
policy/KL                                     0.00665627
policy/KLBefore                               0
policy/LossAfter                             -0.0174508
policy/LossBefore                             1.31931e-08
policy/Perplexity                             1.90231
policy/dLoss                                  0.0174508
---------------------------------------  ----------------
2022-04-23 14:20:11 | [train_policy] epoch #159 | Obtaining samples for iteration 159...
2022-04-23 14:20:11 | [train_policy] epoch #159 | Logging diagnostics...
2022-04-23 14:20:11 | [train_policy] epoch #159 | Optimizing policy...
2022-04-23 14:20:11 | [train_policy] epoch #159 | Computing loss before
2022-04-23 14:20:11 | [train_policy] epoch #159 | Computing KL before
2022-04-23 14:20:11 | [train_policy] epoch #159 | Optimizing
2022-04-23 14:20:11 | [train_policy] epoch #159 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:11 | [train_policy] epoch #159 | computing loss before
2022-04-23 14:20:11 | [train_policy] epoch #159 | computing gradient
2022-04-23 14:20:11 | [train_policy] epoch #159 | gradient computed
2022-04-23 14:20:11 | [train_policy] epoch #159 | computing descent direction
2022-04-23 14:20:11 | [train_policy] epoch #159 | descent direction computed
2022-04-23 14:20:11 | [train_policy] epoch #159 | backtrack iters: 0
2022-04-23 14:20:11 | [train_policy] epoch #159 | optimization finished
2022-04-23 14:20:11 | [train_policy] epoch #159 | Computing KL after
2022-04-23 14:20:11 | [train_policy] epoch #159 | Computing loss after
2022-04-23 14:20:11 | [train_policy] epoch #159 | Fitting baseline...
2022-04-23 14:20:11 | [train_policy] epoch #159 | Saving snapshot...
2022-04-23 14:20:11 | [train_policy] epoch #159 | Saved
2022-04-23 14:20:11 | [train_policy] epoch #159 | Time 59.13 s
2022-04-23 14:20:11 | [train_policy] epoch #159 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.12167
Evaluation/AverageDiscountedReturn          -46.8384
Evaluation/AverageReturn                    -46.8384
Evaluation/CompletionRate                     0
Evaluation/Iteration                        159
Evaluation/MaxReturn                        -34.4314
Evaluation/MinReturn                       -268.444
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         23.9345
Extras/EpisodeRewardMean                    -46.9589
LinearFeatureBaseline/ExplainedVariance      -3.4711
PolicyExecTime                                0.102807
ProcessExecTime                               0.0117095
TotalEnvSteps                            161920
policy/Entropy                                0.641752
policy/KL                                     0.00979092
policy/KLBefore                               0
policy/LossAfter                             -0.0145487
policy/LossBefore                            -0
policy/Perplexity                             1.89981
policy/dLoss                                  0.0145487
---------------------------------------  ---------------
2022-04-23 14:20:11 | [train_policy] epoch #160 | Obtaining samples for iteration 160...
2022-04-23 14:20:11 | [train_policy] epoch #160 | Logging diagnostics...
2022-04-23 14:20:11 | [train_policy] epoch #160 | Optimizing policy...
2022-04-23 14:20:11 | [train_policy] epoch #160 | Computing loss before
2022-04-23 14:20:11 | [train_policy] epoch #160 | Computing KL before
2022-04-23 14:20:11 | [train_policy] epoch #160 | Optimizing
2022-04-23 14:20:11 | [train_policy] epoch #160 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:11 | [train_policy] epoch #160 | computing loss before
2022-04-23 14:20:11 | [train_policy] epoch #160 | computing gradient
2022-04-23 14:20:11 | [train_policy] epoch #160 | gradient computed
2022-04-23 14:20:11 | [train_policy] epoch #160 | computing descent direction
2022-04-23 14:20:11 | [train_policy] epoch #160 | descent direction computed
2022-04-23 14:20:11 | [train_policy] epoch #160 | backtrack iters: 1
2022-04-23 14:20:11 | [train_policy] epoch #160 | optimization finished
2022-04-23 14:20:11 | [train_policy] epoch #160 | Computing KL after
2022-04-23 14:20:11 | [train_policy] epoch #160 | Computing loss after
2022-04-23 14:20:11 | [train_policy] epoch #160 | Fitting baseline...
2022-04-23 14:20:11 | [train_policy] epoch #160 | Saving snapshot...
2022-04-23 14:20:11 | [train_policy] epoch #160 | Saved
2022-04-23 14:20:11 | [train_policy] epoch #160 | Time 59.48 s
2022-04-23 14:20:11 | [train_policy] epoch #160 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116836
Evaluation/AverageDiscountedReturn          -46.415
Evaluation/AverageReturn                    -46.415
Evaluation/CompletionRate                     0
Evaluation/Iteration                        160
Evaluation/MaxReturn                        -35.2169
Evaluation/MinReturn                        -81.0051
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.63416
Extras/EpisodeRewardMean                    -46.416
LinearFeatureBaseline/ExplainedVariance       0.573325
PolicyExecTime                                0.103085
ProcessExecTime                               0.0111415
TotalEnvSteps                            162932
policy/Entropy                                0.627017
policy/KL                                     0.00643868
policy/KLBefore                               0
policy/LossAfter                             -0.0174152
policy/LossBefore                             2.59151e-09
policy/Perplexity                             1.87202
policy/dLoss                                  0.0174153
---------------------------------------  ----------------
2022-04-23 14:20:11 | [train_policy] epoch #161 | Obtaining samples for iteration 161...
2022-04-23 14:20:12 | [train_policy] epoch #161 | Logging diagnostics...
2022-04-23 14:20:12 | [train_policy] epoch #161 | Optimizing policy...
2022-04-23 14:20:12 | [train_policy] epoch #161 | Computing loss before
2022-04-23 14:20:12 | [train_policy] epoch #161 | Computing KL before
2022-04-23 14:20:12 | [train_policy] epoch #161 | Optimizing
2022-04-23 14:20:12 | [train_policy] epoch #161 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:12 | [train_policy] epoch #161 | computing loss before
2022-04-23 14:20:12 | [train_policy] epoch #161 | computing gradient
2022-04-23 14:20:12 | [train_policy] epoch #161 | gradient computed
2022-04-23 14:20:12 | [train_policy] epoch #161 | computing descent direction
2022-04-23 14:20:12 | [train_policy] epoch #161 | descent direction computed
2022-04-23 14:20:12 | [train_policy] epoch #161 | backtrack iters: 0
2022-04-23 14:20:12 | [train_policy] epoch #161 | optimization finished
2022-04-23 14:20:12 | [train_policy] epoch #161 | Computing KL after
2022-04-23 14:20:12 | [train_policy] epoch #161 | Computing loss after
2022-04-23 14:20:12 | [train_policy] epoch #161 | Fitting baseline...
2022-04-23 14:20:12 | [train_policy] epoch #161 | Saving snapshot...
2022-04-23 14:20:12 | [train_policy] epoch #161 | Saved
2022-04-23 14:20:12 | [train_policy] epoch #161 | Time 59.83 s
2022-04-23 14:20:12 | [train_policy] epoch #161 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.126107
Evaluation/AverageDiscountedReturn          -89.3833
Evaluation/AverageReturn                    -89.3833
Evaluation/CompletionRate                     0
Evaluation/Iteration                        161
Evaluation/MaxReturn                        -33.1637
Evaluation/MinReturn                      -2072.68
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.156
Extras/EpisodeRewardMean                    -85.7042
LinearFeatureBaseline/ExplainedVariance       0.0117433
PolicyExecTime                                0.10871
ProcessExecTime                               0.012203
TotalEnvSteps                            163944
policy/Entropy                                0.62623
policy/KL                                     0.00957061
policy/KLBefore                               0
policy/LossAfter                             -0.0301667
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             1.87055
policy/dLoss                                  0.0301667
---------------------------------------  ----------------
2022-04-23 14:20:12 | [train_policy] epoch #162 | Obtaining samples for iteration 162...
2022-04-23 14:20:12 | [train_policy] epoch #162 | Logging diagnostics...
2022-04-23 14:20:12 | [train_policy] epoch #162 | Optimizing policy...
2022-04-23 14:20:12 | [train_policy] epoch #162 | Computing loss before
2022-04-23 14:20:12 | [train_policy] epoch #162 | Computing KL before
2022-04-23 14:20:12 | [train_policy] epoch #162 | Optimizing
2022-04-23 14:20:12 | [train_policy] epoch #162 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:12 | [train_policy] epoch #162 | computing loss before
2022-04-23 14:20:12 | [train_policy] epoch #162 | computing gradient
2022-04-23 14:20:12 | [train_policy] epoch #162 | gradient computed
2022-04-23 14:20:12 | [train_policy] epoch #162 | computing descent direction
2022-04-23 14:20:12 | [train_policy] epoch #162 | descent direction computed
2022-04-23 14:20:12 | [train_policy] epoch #162 | backtrack iters: 0
2022-04-23 14:20:12 | [train_policy] epoch #162 | optimization finished
2022-04-23 14:20:12 | [train_policy] epoch #162 | Computing KL after
2022-04-23 14:20:12 | [train_policy] epoch #162 | Computing loss after
2022-04-23 14:20:12 | [train_policy] epoch #162 | Fitting baseline...
2022-04-23 14:20:12 | [train_policy] epoch #162 | Saving snapshot...
2022-04-23 14:20:12 | [train_policy] epoch #162 | Saved
2022-04-23 14:20:12 | [train_policy] epoch #162 | Time 60.18 s
2022-04-23 14:20:12 | [train_policy] epoch #162 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118549
Evaluation/AverageDiscountedReturn          -46.1526
Evaluation/AverageReturn                    -46.1526
Evaluation/CompletionRate                     0
Evaluation/Iteration                        162
Evaluation/MaxReturn                        -34.7659
Evaluation/MinReturn                       -124.212
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.0891
Extras/EpisodeRewardMean                    -45.9425
LinearFeatureBaseline/ExplainedVariance     -58.9449
PolicyExecTime                                0.103799
ProcessExecTime                               0.0112591
TotalEnvSteps                            164956
policy/Entropy                                0.677321
policy/KL                                     0.00982889
policy/KLBefore                               0
policy/LossAfter                             -0.0210602
policy/LossBefore                             3.10981e-08
policy/Perplexity                             1.9686
policy/dLoss                                  0.0210603
---------------------------------------  ----------------
2022-04-23 14:20:12 | [train_policy] epoch #163 | Obtaining samples for iteration 163...
2022-04-23 14:20:12 | [train_policy] epoch #163 | Logging diagnostics...
2022-04-23 14:20:12 | [train_policy] epoch #163 | Optimizing policy...
2022-04-23 14:20:12 | [train_policy] epoch #163 | Computing loss before
2022-04-23 14:20:12 | [train_policy] epoch #163 | Computing KL before
2022-04-23 14:20:12 | [train_policy] epoch #163 | Optimizing
2022-04-23 14:20:12 | [train_policy] epoch #163 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:12 | [train_policy] epoch #163 | computing loss before
2022-04-23 14:20:12 | [train_policy] epoch #163 | computing gradient
2022-04-23 14:20:12 | [train_policy] epoch #163 | gradient computed
2022-04-23 14:20:12 | [train_policy] epoch #163 | computing descent direction
2022-04-23 14:20:12 | [train_policy] epoch #163 | descent direction computed
2022-04-23 14:20:12 | [train_policy] epoch #163 | backtrack iters: 1
2022-04-23 14:20:12 | [train_policy] epoch #163 | optimization finished
2022-04-23 14:20:12 | [train_policy] epoch #163 | Computing KL after
2022-04-23 14:20:12 | [train_policy] epoch #163 | Computing loss after
2022-04-23 14:20:12 | [train_policy] epoch #163 | Fitting baseline...
2022-04-23 14:20:12 | [train_policy] epoch #163 | Saving snapshot...
2022-04-23 14:20:12 | [train_policy] epoch #163 | Saved
2022-04-23 14:20:12 | [train_policy] epoch #163 | Time 60.53 s
2022-04-23 14:20:12 | [train_policy] epoch #163 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118037
Evaluation/AverageDiscountedReturn          -47.0508
Evaluation/AverageReturn                    -47.0508
Evaluation/CompletionRate                     0
Evaluation/Iteration                        163
Evaluation/MaxReturn                        -34.4701
Evaluation/MinReturn                       -100.2
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.35558
Extras/EpisodeRewardMean                    -47.3533
LinearFeatureBaseline/ExplainedVariance       0.73247
PolicyExecTime                                0.105351
ProcessExecTime                               0.0111485
TotalEnvSteps                            165968
policy/Entropy                                0.649987
policy/KL                                     0.00653737
policy/KLBefore                               0
policy/LossAfter                             -0.0321167
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             1.91552
policy/dLoss                                  0.0321167
---------------------------------------  ----------------
2022-04-23 14:20:12 | [train_policy] epoch #164 | Obtaining samples for iteration 164...
2022-04-23 14:20:13 | [train_policy] epoch #164 | Logging diagnostics...
2022-04-23 14:20:13 | [train_policy] epoch #164 | Optimizing policy...
2022-04-23 14:20:13 | [train_policy] epoch #164 | Computing loss before
2022-04-23 14:20:13 | [train_policy] epoch #164 | Computing KL before
2022-04-23 14:20:13 | [train_policy] epoch #164 | Optimizing
2022-04-23 14:20:13 | [train_policy] epoch #164 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:13 | [train_policy] epoch #164 | computing loss before
2022-04-23 14:20:13 | [train_policy] epoch #164 | computing gradient
2022-04-23 14:20:13 | [train_policy] epoch #164 | gradient computed
2022-04-23 14:20:13 | [train_policy] epoch #164 | computing descent direction
2022-04-23 14:20:13 | [train_policy] epoch #164 | descent direction computed
2022-04-23 14:20:13 | [train_policy] epoch #164 | backtrack iters: 1
2022-04-23 14:20:13 | [train_policy] epoch #164 | optimization finished
2022-04-23 14:20:13 | [train_policy] epoch #164 | Computing KL after
2022-04-23 14:20:13 | [train_policy] epoch #164 | Computing loss after
2022-04-23 14:20:13 | [train_policy] epoch #164 | Fitting baseline...
2022-04-23 14:20:13 | [train_policy] epoch #164 | Saving snapshot...
2022-04-23 14:20:13 | [train_policy] epoch #164 | Saved
2022-04-23 14:20:13 | [train_policy] epoch #164 | Time 60.87 s
2022-04-23 14:20:13 | [train_policy] epoch #164 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117688
Evaluation/AverageDiscountedReturn          -45.1809
Evaluation/AverageReturn                    -45.1809
Evaluation/CompletionRate                     0
Evaluation/Iteration                        164
Evaluation/MaxReturn                        -31.7475
Evaluation/MinReturn                        -64.1534
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.79261
Extras/EpisodeRewardMean                    -45.0878
LinearFeatureBaseline/ExplainedVariance       0.93424
PolicyExecTime                                0.102044
ProcessExecTime                               0.0112655
TotalEnvSteps                            166980
policy/Entropy                                0.630262
policy/KL                                     0.00659233
policy/KLBefore                               0
policy/LossAfter                             -0.018974
policy/LossBefore                            -1.22508e-08
policy/Perplexity                             1.8781
policy/dLoss                                  0.018974
---------------------------------------  ----------------
2022-04-23 14:20:13 | [train_policy] epoch #165 | Obtaining samples for iteration 165...
2022-04-23 14:20:13 | [train_policy] epoch #165 | Logging diagnostics...
2022-04-23 14:20:13 | [train_policy] epoch #165 | Optimizing policy...
2022-04-23 14:20:13 | [train_policy] epoch #165 | Computing loss before
2022-04-23 14:20:13 | [train_policy] epoch #165 | Computing KL before
2022-04-23 14:20:13 | [train_policy] epoch #165 | Optimizing
2022-04-23 14:20:13 | [train_policy] epoch #165 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:13 | [train_policy] epoch #165 | computing loss before
2022-04-23 14:20:13 | [train_policy] epoch #165 | computing gradient
2022-04-23 14:20:13 | [train_policy] epoch #165 | gradient computed
2022-04-23 14:20:13 | [train_policy] epoch #165 | computing descent direction
2022-04-23 14:20:13 | [train_policy] epoch #165 | descent direction computed
2022-04-23 14:20:13 | [train_policy] epoch #165 | backtrack iters: 1
2022-04-23 14:20:13 | [train_policy] epoch #165 | optimization finished
2022-04-23 14:20:13 | [train_policy] epoch #165 | Computing KL after
2022-04-23 14:20:13 | [train_policy] epoch #165 | Computing loss after
2022-04-23 14:20:13 | [train_policy] epoch #165 | Fitting baseline...
2022-04-23 14:20:13 | [train_policy] epoch #165 | Saving snapshot...
2022-04-23 14:20:13 | [train_policy] epoch #165 | Saved
2022-04-23 14:20:13 | [train_policy] epoch #165 | Time 61.22 s
2022-04-23 14:20:13 | [train_policy] epoch #165 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.120792
Evaluation/AverageDiscountedReturn          -68.6052
Evaluation/AverageReturn                    -68.6052
Evaluation/CompletionRate                     0
Evaluation/Iteration                        165
Evaluation/MaxReturn                        -35.239
Evaluation/MinReturn                      -2062.34
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.38
Extras/EpisodeRewardMean                    -66.812
LinearFeatureBaseline/ExplainedVariance       0.00930348
PolicyExecTime                                0.102771
ProcessExecTime                               0.0117152
TotalEnvSteps                            167992
policy/Entropy                                0.640498
policy/KL                                     0.00651894
policy/KLBefore                               0
policy/LossAfter                             -0.0146988
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             1.89742
policy/dLoss                                  0.0146988
---------------------------------------  ---------------
2022-04-23 14:20:13 | [train_policy] epoch #166 | Obtaining samples for iteration 166...
2022-04-23 14:20:13 | [train_policy] epoch #166 | Logging diagnostics...
2022-04-23 14:20:13 | [train_policy] epoch #166 | Optimizing policy...
2022-04-23 14:20:13 | [train_policy] epoch #166 | Computing loss before
2022-04-23 14:20:13 | [train_policy] epoch #166 | Computing KL before
2022-04-23 14:20:13 | [train_policy] epoch #166 | Optimizing
2022-04-23 14:20:13 | [train_policy] epoch #166 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:13 | [train_policy] epoch #166 | computing loss before
2022-04-23 14:20:13 | [train_policy] epoch #166 | computing gradient
2022-04-23 14:20:13 | [train_policy] epoch #166 | gradient computed
2022-04-23 14:20:13 | [train_policy] epoch #166 | computing descent direction
2022-04-23 14:20:13 | [train_policy] epoch #166 | descent direction computed
2022-04-23 14:20:13 | [train_policy] epoch #166 | backtrack iters: 1
2022-04-23 14:20:13 | [train_policy] epoch #166 | optimization finished
2022-04-23 14:20:13 | [train_policy] epoch #166 | Computing KL after
2022-04-23 14:20:13 | [train_policy] epoch #166 | Computing loss after
2022-04-23 14:20:13 | [train_policy] epoch #166 | Fitting baseline...
2022-04-23 14:20:13 | [train_policy] epoch #166 | Saving snapshot...
2022-04-23 14:20:13 | [train_policy] epoch #166 | Saved
2022-04-23 14:20:13 | [train_policy] epoch #166 | Time 61.59 s
2022-04-23 14:20:13 | [train_policy] epoch #166 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.126705
Evaluation/AverageDiscountedReturn          -89.2348
Evaluation/AverageReturn                    -89.2348
Evaluation/CompletionRate                     0
Evaluation/Iteration                        166
Evaluation/MaxReturn                        -32.8991
Evaluation/MinReturn                      -2064.77
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.394
Extras/EpisodeRewardMean                    -85.5089
LinearFeatureBaseline/ExplainedVariance       0.197279
PolicyExecTime                                0.10916
ProcessExecTime                               0.0123968
TotalEnvSteps                            169004
policy/Entropy                                0.62298
policy/KL                                     0.00642035
policy/KLBefore                               0
policy/LossAfter                             -0.017128
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             1.86448
policy/dLoss                                  0.017128
---------------------------------------  ----------------
2022-04-23 14:20:13 | [train_policy] epoch #167 | Obtaining samples for iteration 167...
2022-04-23 14:20:14 | [train_policy] epoch #167 | Logging diagnostics...
2022-04-23 14:20:14 | [train_policy] epoch #167 | Optimizing policy...
2022-04-23 14:20:14 | [train_policy] epoch #167 | Computing loss before
2022-04-23 14:20:14 | [train_policy] epoch #167 | Computing KL before
2022-04-23 14:20:14 | [train_policy] epoch #167 | Optimizing
2022-04-23 14:20:14 | [train_policy] epoch #167 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:14 | [train_policy] epoch #167 | computing loss before
2022-04-23 14:20:14 | [train_policy] epoch #167 | computing gradient
2022-04-23 14:20:14 | [train_policy] epoch #167 | gradient computed
2022-04-23 14:20:14 | [train_policy] epoch #167 | computing descent direction
2022-04-23 14:20:14 | [train_policy] epoch #167 | descent direction computed
2022-04-23 14:20:14 | [train_policy] epoch #167 | backtrack iters: 0
2022-04-23 14:20:14 | [train_policy] epoch #167 | optimization finished
2022-04-23 14:20:14 | [train_policy] epoch #167 | Computing KL after
2022-04-23 14:20:14 | [train_policy] epoch #167 | Computing loss after
2022-04-23 14:20:14 | [train_policy] epoch #167 | Fitting baseline...
2022-04-23 14:20:14 | [train_policy] epoch #167 | Saving snapshot...
2022-04-23 14:20:14 | [train_policy] epoch #167 | Saved
2022-04-23 14:20:14 | [train_policy] epoch #167 | Time 61.95 s
2022-04-23 14:20:14 | [train_policy] epoch #167 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117
Evaluation/AverageDiscountedReturn          -66.9835
Evaluation/AverageReturn                    -66.9835
Evaluation/CompletionRate                     0
Evaluation/Iteration                        167
Evaluation/MaxReturn                        -33.6688
Evaluation/MinReturn                      -2070.2
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.089
Extras/EpisodeRewardMean                    -64.9756
LinearFeatureBaseline/ExplainedVariance       0.0706806
PolicyExecTime                                0.111568
ProcessExecTime                               0.0115194
TotalEnvSteps                            170016
policy/Entropy                                0.617861
policy/KL                                     0.00981013
policy/KLBefore                               0
policy/LossAfter                             -0.0254609
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             1.85496
policy/dLoss                                  0.0254609
---------------------------------------  ----------------
2022-04-23 14:20:14 | [train_policy] epoch #168 | Obtaining samples for iteration 168...
2022-04-23 14:20:14 | [train_policy] epoch #168 | Logging diagnostics...
2022-04-23 14:20:14 | [train_policy] epoch #168 | Optimizing policy...
2022-04-23 14:20:14 | [train_policy] epoch #168 | Computing loss before
2022-04-23 14:20:14 | [train_policy] epoch #168 | Computing KL before
2022-04-23 14:20:14 | [train_policy] epoch #168 | Optimizing
2022-04-23 14:20:14 | [train_policy] epoch #168 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:14 | [train_policy] epoch #168 | computing loss before
2022-04-23 14:20:14 | [train_policy] epoch #168 | computing gradient
2022-04-23 14:20:14 | [train_policy] epoch #168 | gradient computed
2022-04-23 14:20:14 | [train_policy] epoch #168 | computing descent direction
2022-04-23 14:20:14 | [train_policy] epoch #168 | descent direction computed
2022-04-23 14:20:14 | [train_policy] epoch #168 | backtrack iters: 0
2022-04-23 14:20:14 | [train_policy] epoch #168 | optimization finished
2022-04-23 14:20:14 | [train_policy] epoch #168 | Computing KL after
2022-04-23 14:20:14 | [train_policy] epoch #168 | Computing loss after
2022-04-23 14:20:14 | [train_policy] epoch #168 | Fitting baseline...
2022-04-23 14:20:14 | [train_policy] epoch #168 | Saving snapshot...
2022-04-23 14:20:14 | [train_policy] epoch #168 | Saved
2022-04-23 14:20:14 | [train_policy] epoch #168 | Time 62.30 s
2022-04-23 14:20:14 | [train_policy] epoch #168 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116679
Evaluation/AverageDiscountedReturn          -45.1524
Evaluation/AverageReturn                    -45.1524
Evaluation/CompletionRate                     0
Evaluation/Iteration                        168
Evaluation/MaxReturn                        -32.5161
Evaluation/MinReturn                        -87.4518
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.81197
Extras/EpisodeRewardMean                    -44.8877
LinearFeatureBaseline/ExplainedVariance     -24.2813
PolicyExecTime                                0.10735
ProcessExecTime                               0.0115654
TotalEnvSteps                            171028
policy/Entropy                                0.618162
policy/KL                                     0.00985749
policy/KLBefore                               0
policy/LossAfter                             -0.0288249
policy/LossBefore                            -2.34414e-08
policy/Perplexity                             1.85551
policy/dLoss                                  0.0288249
---------------------------------------  ----------------
2022-04-23 14:20:14 | [train_policy] epoch #169 | Obtaining samples for iteration 169...
2022-04-23 14:20:14 | [train_policy] epoch #169 | Logging diagnostics...
2022-04-23 14:20:14 | [train_policy] epoch #169 | Optimizing policy...
2022-04-23 14:20:14 | [train_policy] epoch #169 | Computing loss before
2022-04-23 14:20:14 | [train_policy] epoch #169 | Computing KL before
2022-04-23 14:20:14 | [train_policy] epoch #169 | Optimizing
2022-04-23 14:20:14 | [train_policy] epoch #169 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:14 | [train_policy] epoch #169 | computing loss before
2022-04-23 14:20:14 | [train_policy] epoch #169 | computing gradient
2022-04-23 14:20:14 | [train_policy] epoch #169 | gradient computed
2022-04-23 14:20:14 | [train_policy] epoch #169 | computing descent direction
2022-04-23 14:20:14 | [train_policy] epoch #169 | descent direction computed
2022-04-23 14:20:14 | [train_policy] epoch #169 | backtrack iters: 1
2022-04-23 14:20:14 | [train_policy] epoch #169 | optimization finished
2022-04-23 14:20:14 | [train_policy] epoch #169 | Computing KL after
2022-04-23 14:20:14 | [train_policy] epoch #169 | Computing loss after
2022-04-23 14:20:14 | [train_policy] epoch #169 | Fitting baseline...
2022-04-23 14:20:14 | [train_policy] epoch #169 | Saving snapshot...
2022-04-23 14:20:14 | [train_policy] epoch #169 | Saved
2022-04-23 14:20:14 | [train_policy] epoch #169 | Time 62.65 s
2022-04-23 14:20:14 | [train_policy] epoch #169 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116528
Evaluation/AverageDiscountedReturn          -89.146
Evaluation/AverageReturn                    -89.146
Evaluation/CompletionRate                     0
Evaluation/Iteration                        169
Evaluation/MaxReturn                        -34.3319
Evaluation/MinReturn                      -2089.85
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        296.275
Extras/EpisodeRewardMean                    -85.3173
LinearFeatureBaseline/ExplainedVariance       0.00836294
PolicyExecTime                                0.104074
ProcessExecTime                               0.0120413
TotalEnvSteps                            172040
policy/Entropy                                0.609031
policy/KL                                     0.00715958
policy/KLBefore                               0
policy/LossAfter                             -0.0336858
policy/LossBefore                             6.12538e-09
policy/Perplexity                             1.83865
policy/dLoss                                  0.0336858
---------------------------------------  ----------------
2022-04-23 14:20:14 | [train_policy] epoch #170 | Obtaining samples for iteration 170...
2022-04-23 14:20:15 | [train_policy] epoch #170 | Logging diagnostics...
2022-04-23 14:20:15 | [train_policy] epoch #170 | Optimizing policy...
2022-04-23 14:20:15 | [train_policy] epoch #170 | Computing loss before
2022-04-23 14:20:15 | [train_policy] epoch #170 | Computing KL before
2022-04-23 14:20:15 | [train_policy] epoch #170 | Optimizing
2022-04-23 14:20:15 | [train_policy] epoch #170 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:15 | [train_policy] epoch #170 | computing loss before
2022-04-23 14:20:15 | [train_policy] epoch #170 | computing gradient
2022-04-23 14:20:15 | [train_policy] epoch #170 | gradient computed
2022-04-23 14:20:15 | [train_policy] epoch #170 | computing descent direction
2022-04-23 14:20:15 | [train_policy] epoch #170 | descent direction computed
2022-04-23 14:20:15 | [train_policy] epoch #170 | backtrack iters: 0
2022-04-23 14:20:15 | [train_policy] epoch #170 | optimization finished
2022-04-23 14:20:15 | [train_policy] epoch #170 | Computing KL after
2022-04-23 14:20:15 | [train_policy] epoch #170 | Computing loss after
2022-04-23 14:20:15 | [train_policy] epoch #170 | Fitting baseline...
2022-04-23 14:20:15 | [train_policy] epoch #170 | Saving snapshot...
2022-04-23 14:20:15 | [train_policy] epoch #170 | Saved
2022-04-23 14:20:15 | [train_policy] epoch #170 | Time 63.00 s
2022-04-23 14:20:15 | [train_policy] epoch #170 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116592
Evaluation/AverageDiscountedReturn          -68.3344
Evaluation/AverageReturn                    -68.3344
Evaluation/CompletionRate                     0
Evaluation/Iteration                        170
Evaluation/MaxReturn                        -35.524
Evaluation/MinReturn                      -2066.98
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.016
Extras/EpisodeRewardMean                    -66.4098
LinearFeatureBaseline/ExplainedVariance      -0.199793
PolicyExecTime                                0.109114
ProcessExecTime                               0.0122347
TotalEnvSteps                            173052
policy/Entropy                                0.596352
policy/KL                                     0.0096336
policy/KLBefore                               0
policy/LossAfter                             -0.0212728
policy/LossBefore                             2.35591e-09
policy/Perplexity                             1.81548
policy/dLoss                                  0.0212728
---------------------------------------  ----------------
2022-04-23 14:20:15 | [train_policy] epoch #171 | Obtaining samples for iteration 171...
2022-04-23 14:20:15 | [train_policy] epoch #171 | Logging diagnostics...
2022-04-23 14:20:15 | [train_policy] epoch #171 | Optimizing policy...
2022-04-23 14:20:15 | [train_policy] epoch #171 | Computing loss before
2022-04-23 14:20:15 | [train_policy] epoch #171 | Computing KL before
2022-04-23 14:20:15 | [train_policy] epoch #171 | Optimizing
2022-04-23 14:20:15 | [train_policy] epoch #171 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:15 | [train_policy] epoch #171 | computing loss before
2022-04-23 14:20:15 | [train_policy] epoch #171 | computing gradient
2022-04-23 14:20:15 | [train_policy] epoch #171 | gradient computed
2022-04-23 14:20:15 | [train_policy] epoch #171 | computing descent direction
2022-04-23 14:20:15 | [train_policy] epoch #171 | descent direction computed
2022-04-23 14:20:15 | [train_policy] epoch #171 | backtrack iters: 0
2022-04-23 14:20:15 | [train_policy] epoch #171 | optimization finished
2022-04-23 14:20:15 | [train_policy] epoch #171 | Computing KL after
2022-04-23 14:20:15 | [train_policy] epoch #171 | Computing loss after
2022-04-23 14:20:15 | [train_policy] epoch #171 | Fitting baseline...
2022-04-23 14:20:15 | [train_policy] epoch #171 | Saving snapshot...
2022-04-23 14:20:15 | [train_policy] epoch #171 | Saved
2022-04-23 14:20:15 | [train_policy] epoch #171 | Time 63.36 s
2022-04-23 14:20:15 | [train_policy] epoch #171 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117136
Evaluation/AverageDiscountedReturn          -66.7393
Evaluation/AverageReturn                    -66.7393
Evaluation/CompletionRate                     0
Evaluation/Iteration                        171
Evaluation/MaxReturn                        -33.797
Evaluation/MinReturn                      -2066.35
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.734
Extras/EpisodeRewardMean                    -85.2854
LinearFeatureBaseline/ExplainedVariance       0.0915492
PolicyExecTime                                0.111037
ProcessExecTime                               0.0117428
TotalEnvSteps                            174064
policy/Entropy                                0.634308
policy/KL                                     0.00906562
policy/KLBefore                               0
policy/LossAfter                             -0.0209303
policy/LossBefore                            -1.20152e-08
policy/Perplexity                             1.88572
policy/dLoss                                  0.0209303
---------------------------------------  ----------------
2022-04-23 14:20:15 | [train_policy] epoch #172 | Obtaining samples for iteration 172...
2022-04-23 14:20:15 | [train_policy] epoch #172 | Logging diagnostics...
2022-04-23 14:20:15 | [train_policy] epoch #172 | Optimizing policy...
2022-04-23 14:20:15 | [train_policy] epoch #172 | Computing loss before
2022-04-23 14:20:15 | [train_policy] epoch #172 | Computing KL before
2022-04-23 14:20:15 | [train_policy] epoch #172 | Optimizing
2022-04-23 14:20:15 | [train_policy] epoch #172 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:15 | [train_policy] epoch #172 | computing loss before
2022-04-23 14:20:15 | [train_policy] epoch #172 | computing gradient
2022-04-23 14:20:15 | [train_policy] epoch #172 | gradient computed
2022-04-23 14:20:15 | [train_policy] epoch #172 | computing descent direction
2022-04-23 14:20:15 | [train_policy] epoch #172 | descent direction computed
2022-04-23 14:20:15 | [train_policy] epoch #172 | backtrack iters: 1
2022-04-23 14:20:15 | [train_policy] epoch #172 | optimization finished
2022-04-23 14:20:15 | [train_policy] epoch #172 | Computing KL after
2022-04-23 14:20:15 | [train_policy] epoch #172 | Computing loss after
2022-04-23 14:20:15 | [train_policy] epoch #172 | Fitting baseline...
2022-04-23 14:20:15 | [train_policy] epoch #172 | Saving snapshot...
2022-04-23 14:20:15 | [train_policy] epoch #172 | Saved
2022-04-23 14:20:15 | [train_policy] epoch #172 | Time 63.70 s
2022-04-23 14:20:15 | [train_policy] epoch #172 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.116009
Evaluation/AverageDiscountedReturn          -44.789
Evaluation/AverageReturn                    -44.789
Evaluation/CompletionRate                     0
Evaluation/Iteration                        172
Evaluation/MaxReturn                        -35.0121
Evaluation/MinReturn                        -64.6991
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.9882
Extras/EpisodeRewardMean                    -44.7621
LinearFeatureBaseline/ExplainedVariance     -16.704
PolicyExecTime                                0.106182
ProcessExecTime                               0.011379
TotalEnvSteps                            175076
policy/Entropy                                0.662279
policy/KL                                     0.00736082
policy/KLBefore                               0
policy/LossAfter                             -0.0139594
policy/LossBefore                             2.3353e-08
policy/Perplexity                             1.93921
policy/dLoss                                  0.0139594
---------------------------------------  ---------------
2022-04-23 14:20:15 | [train_policy] epoch #173 | Obtaining samples for iteration 173...
2022-04-23 14:20:16 | [train_policy] epoch #173 | Logging diagnostics...
2022-04-23 14:20:16 | [train_policy] epoch #173 | Optimizing policy...
2022-04-23 14:20:16 | [train_policy] epoch #173 | Computing loss before
2022-04-23 14:20:16 | [train_policy] epoch #173 | Computing KL before
2022-04-23 14:20:16 | [train_policy] epoch #173 | Optimizing
2022-04-23 14:20:16 | [train_policy] epoch #173 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:16 | [train_policy] epoch #173 | computing loss before
2022-04-23 14:20:16 | [train_policy] epoch #173 | computing gradient
2022-04-23 14:20:16 | [train_policy] epoch #173 | gradient computed
2022-04-23 14:20:16 | [train_policy] epoch #173 | computing descent direction
2022-04-23 14:20:16 | [train_policy] epoch #173 | descent direction computed
2022-04-23 14:20:16 | [train_policy] epoch #173 | backtrack iters: 1
2022-04-23 14:20:16 | [train_policy] epoch #173 | optimization finished
2022-04-23 14:20:16 | [train_policy] epoch #173 | Computing KL after
2022-04-23 14:20:16 | [train_policy] epoch #173 | Computing loss after
2022-04-23 14:20:16 | [train_policy] epoch #173 | Fitting baseline...
2022-04-23 14:20:16 | [train_policy] epoch #173 | Saving snapshot...
2022-04-23 14:20:16 | [train_policy] epoch #173 | Saved
2022-04-23 14:20:16 | [train_policy] epoch #173 | Time 64.06 s
2022-04-23 14:20:16 | [train_policy] epoch #173 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116686
Evaluation/AverageDiscountedReturn          -91.0674
Evaluation/AverageReturn                    -91.0674
Evaluation/CompletionRate                     0
Evaluation/Iteration                        173
Evaluation/MaxReturn                        -35.3596
Evaluation/MinReturn                      -2065.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.36
Extras/EpisodeRewardMean                    -87.4222
LinearFeatureBaseline/ExplainedVariance       0.00899939
PolicyExecTime                                0.106477
ProcessExecTime                               0.011899
TotalEnvSteps                            176088
policy/Entropy                                0.655765
policy/KL                                     0.00724618
policy/KLBefore                               0
policy/LossAfter                             -0.0104822
policy/LossBefore                             6.59656e-09
policy/Perplexity                             1.92662
policy/dLoss                                  0.0104822
---------------------------------------  ----------------
2022-04-23 14:20:16 | [train_policy] epoch #174 | Obtaining samples for iteration 174...
2022-04-23 14:20:16 | [train_policy] epoch #174 | Logging diagnostics...
2022-04-23 14:20:16 | [train_policy] epoch #174 | Optimizing policy...
2022-04-23 14:20:16 | [train_policy] epoch #174 | Computing loss before
2022-04-23 14:20:16 | [train_policy] epoch #174 | Computing KL before
2022-04-23 14:20:16 | [train_policy] epoch #174 | Optimizing
2022-04-23 14:20:16 | [train_policy] epoch #174 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:16 | [train_policy] epoch #174 | computing loss before
2022-04-23 14:20:16 | [train_policy] epoch #174 | computing gradient
2022-04-23 14:20:16 | [train_policy] epoch #174 | gradient computed
2022-04-23 14:20:16 | [train_policy] epoch #174 | computing descent direction
2022-04-23 14:20:16 | [train_policy] epoch #174 | descent direction computed
2022-04-23 14:20:16 | [train_policy] epoch #174 | backtrack iters: 1
2022-04-23 14:20:16 | [train_policy] epoch #174 | optimization finished
2022-04-23 14:20:16 | [train_policy] epoch #174 | Computing KL after
2022-04-23 14:20:16 | [train_policy] epoch #174 | Computing loss after
2022-04-23 14:20:16 | [train_policy] epoch #174 | Fitting baseline...
2022-04-23 14:20:16 | [train_policy] epoch #174 | Saving snapshot...
2022-04-23 14:20:16 | [train_policy] epoch #174 | Saved
2022-04-23 14:20:16 | [train_policy] epoch #174 | Time 64.42 s
2022-04-23 14:20:16 | [train_policy] epoch #174 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116998
Evaluation/AverageDiscountedReturn         -133.431
Evaluation/AverageReturn                   -133.431
Evaluation/CompletionRate                     0
Evaluation/Iteration                        174
Evaluation/MaxReturn                        -34.1394
Evaluation/MinReturn                      -2068.93
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        411.352
Extras/EpisodeRewardMean                   -126.311
LinearFeatureBaseline/ExplainedVariance       0.220874
PolicyExecTime                                0.107862
ProcessExecTime                               0.0115056
TotalEnvSteps                            177100
policy/Entropy                                0.612583
policy/KL                                     0.00683588
policy/KLBefore                               0
policy/LossAfter                             -0.0194995
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             1.84519
policy/dLoss                                  0.0194995
---------------------------------------  ----------------
2022-04-23 14:20:16 | [train_policy] epoch #175 | Obtaining samples for iteration 175...
2022-04-23 14:20:16 | [train_policy] epoch #175 | Logging diagnostics...
2022-04-23 14:20:16 | [train_policy] epoch #175 | Optimizing policy...
2022-04-23 14:20:16 | [train_policy] epoch #175 | Computing loss before
2022-04-23 14:20:16 | [train_policy] epoch #175 | Computing KL before
2022-04-23 14:20:16 | [train_policy] epoch #175 | Optimizing
2022-04-23 14:20:16 | [train_policy] epoch #175 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:16 | [train_policy] epoch #175 | computing loss before
2022-04-23 14:20:16 | [train_policy] epoch #175 | computing gradient
2022-04-23 14:20:16 | [train_policy] epoch #175 | gradient computed
2022-04-23 14:20:16 | [train_policy] epoch #175 | computing descent direction
2022-04-23 14:20:17 | [train_policy] epoch #175 | descent direction computed
2022-04-23 14:20:17 | [train_policy] epoch #175 | backtrack iters: 1
2022-04-23 14:20:17 | [train_policy] epoch #175 | optimization finished
2022-04-23 14:20:17 | [train_policy] epoch #175 | Computing KL after
2022-04-23 14:20:17 | [train_policy] epoch #175 | Computing loss after
2022-04-23 14:20:17 | [train_policy] epoch #175 | Fitting baseline...
2022-04-23 14:20:17 | [train_policy] epoch #175 | Saving snapshot...
2022-04-23 14:20:17 | [train_policy] epoch #175 | Saved
2022-04-23 14:20:17 | [train_policy] epoch #175 | Time 64.77 s
2022-04-23 14:20:17 | [train_policy] epoch #175 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116822
Evaluation/AverageDiscountedReturn          -67.4492
Evaluation/AverageReturn                    -67.4492
Evaluation/CompletionRate                     0
Evaluation/Iteration                        175
Evaluation/MaxReturn                        -35.9931
Evaluation/MinReturn                      -2066.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.621
Extras/EpisodeRewardMean                    -86.3902
LinearFeatureBaseline/ExplainedVariance      -0.577797
PolicyExecTime                                0.100862
ProcessExecTime                               0.0118556
TotalEnvSteps                            178112
policy/Entropy                                0.597919
policy/KL                                     0.00660606
policy/KLBefore                               0
policy/LossAfter                             -0.0142162
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             1.81833
policy/dLoss                                  0.0142162
---------------------------------------  ----------------
2022-04-23 14:20:17 | [train_policy] epoch #176 | Obtaining samples for iteration 176...
2022-04-23 14:20:17 | [train_policy] epoch #176 | Logging diagnostics...
2022-04-23 14:20:17 | [train_policy] epoch #176 | Optimizing policy...
2022-04-23 14:20:17 | [train_policy] epoch #176 | Computing loss before
2022-04-23 14:20:17 | [train_policy] epoch #176 | Computing KL before
2022-04-23 14:20:17 | [train_policy] epoch #176 | Optimizing
2022-04-23 14:20:17 | [train_policy] epoch #176 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:17 | [train_policy] epoch #176 | computing loss before
2022-04-23 14:20:17 | [train_policy] epoch #176 | computing gradient
2022-04-23 14:20:17 | [train_policy] epoch #176 | gradient computed
2022-04-23 14:20:17 | [train_policy] epoch #176 | computing descent direction
2022-04-23 14:20:17 | [train_policy] epoch #176 | descent direction computed
2022-04-23 14:20:17 | [train_policy] epoch #176 | backtrack iters: 1
2022-04-23 14:20:17 | [train_policy] epoch #176 | optimization finished
2022-04-23 14:20:17 | [train_policy] epoch #176 | Computing KL after
2022-04-23 14:20:17 | [train_policy] epoch #176 | Computing loss after
2022-04-23 14:20:17 | [train_policy] epoch #176 | Fitting baseline...
2022-04-23 14:20:17 | [train_policy] epoch #176 | Saving snapshot...
2022-04-23 14:20:17 | [train_policy] epoch #176 | Saved
2022-04-23 14:20:17 | [train_policy] epoch #176 | Time 65.12 s
2022-04-23 14:20:17 | [train_policy] epoch #176 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116135
Evaluation/AverageDiscountedReturn          -46.1484
Evaluation/AverageReturn                    -46.1484
Evaluation/CompletionRate                     0
Evaluation/Iteration                        176
Evaluation/MaxReturn                        -35.3759
Evaluation/MinReturn                        -66.1802
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.04748
Extras/EpisodeRewardMean                    -45.9404
LinearFeatureBaseline/ExplainedVariance     -32.7782
PolicyExecTime                                0.105464
ProcessExecTime                               0.011308
TotalEnvSteps                            179124
policy/Entropy                                0.594907
policy/KL                                     0.00670639
policy/KLBefore                               0
policy/LossAfter                             -0.0188423
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             1.81286
policy/dLoss                                  0.0188423
---------------------------------------  ----------------
2022-04-23 14:20:17 | [train_policy] epoch #177 | Obtaining samples for iteration 177...
2022-04-23 14:20:17 | [train_policy] epoch #177 | Logging diagnostics...
2022-04-23 14:20:17 | [train_policy] epoch #177 | Optimizing policy...
2022-04-23 14:20:17 | [train_policy] epoch #177 | Computing loss before
2022-04-23 14:20:17 | [train_policy] epoch #177 | Computing KL before
2022-04-23 14:20:17 | [train_policy] epoch #177 | Optimizing
2022-04-23 14:20:17 | [train_policy] epoch #177 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:17 | [train_policy] epoch #177 | computing loss before
2022-04-23 14:20:17 | [train_policy] epoch #177 | computing gradient
2022-04-23 14:20:17 | [train_policy] epoch #177 | gradient computed
2022-04-23 14:20:17 | [train_policy] epoch #177 | computing descent direction
2022-04-23 14:20:17 | [train_policy] epoch #177 | descent direction computed
2022-04-23 14:20:17 | [train_policy] epoch #177 | backtrack iters: 1
2022-04-23 14:20:17 | [train_policy] epoch #177 | optimization finished
2022-04-23 14:20:17 | [train_policy] epoch #177 | Computing KL after
2022-04-23 14:20:17 | [train_policy] epoch #177 | Computing loss after
2022-04-23 14:20:17 | [train_policy] epoch #177 | Fitting baseline...
2022-04-23 14:20:17 | [train_policy] epoch #177 | Saving snapshot...
2022-04-23 14:20:17 | [train_policy] epoch #177 | Saved
2022-04-23 14:20:17 | [train_policy] epoch #177 | Time 65.49 s
2022-04-23 14:20:17 | [train_policy] epoch #177 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.116199
Evaluation/AverageDiscountedReturn          -44.8504
Evaluation/AverageReturn                    -44.8504
Evaluation/CompletionRate                     0
Evaluation/Iteration                        177
Evaluation/MaxReturn                        -32.8379
Evaluation/MinReturn                        -64.7798
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.37092
Extras/EpisodeRewardMean                    -44.8125
LinearFeatureBaseline/ExplainedVariance       0.952665
PolicyExecTime                                0.103607
ProcessExecTime                               0.0112092
TotalEnvSteps                            180136
policy/Entropy                                0.629261
policy/KL                                     0.00641468
policy/KLBefore                               0
policy/LossAfter                             -0.0161389
policy/LossBefore                             6.12538e-09
policy/Perplexity                             1.87622
policy/dLoss                                  0.0161389
---------------------------------------  ----------------
2022-04-23 14:20:17 | [train_policy] epoch #178 | Obtaining samples for iteration 178...
2022-04-23 14:20:18 | [train_policy] epoch #178 | Logging diagnostics...
2022-04-23 14:20:18 | [train_policy] epoch #178 | Optimizing policy...
2022-04-23 14:20:18 | [train_policy] epoch #178 | Computing loss before
2022-04-23 14:20:18 | [train_policy] epoch #178 | Computing KL before
2022-04-23 14:20:18 | [train_policy] epoch #178 | Optimizing
2022-04-23 14:20:18 | [train_policy] epoch #178 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:18 | [train_policy] epoch #178 | computing loss before
2022-04-23 14:20:18 | [train_policy] epoch #178 | computing gradient
2022-04-23 14:20:18 | [train_policy] epoch #178 | gradient computed
2022-04-23 14:20:18 | [train_policy] epoch #178 | computing descent direction
2022-04-23 14:20:18 | [train_policy] epoch #178 | descent direction computed
2022-04-23 14:20:18 | [train_policy] epoch #178 | backtrack iters: 2
2022-04-23 14:20:18 | [train_policy] epoch #178 | optimization finished
2022-04-23 14:20:18 | [train_policy] epoch #178 | Computing KL after
2022-04-23 14:20:18 | [train_policy] epoch #178 | Computing loss after
2022-04-23 14:20:18 | [train_policy] epoch #178 | Fitting baseline...
2022-04-23 14:20:18 | [train_policy] epoch #178 | Saving snapshot...
2022-04-23 14:20:18 | [train_policy] epoch #178 | Saved
2022-04-23 14:20:18 | [train_policy] epoch #178 | Time 65.85 s
2022-04-23 14:20:18 | [train_policy] epoch #178 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118736
Evaluation/AverageDiscountedReturn         -111.309
Evaluation/AverageReturn                   -111.309
Evaluation/CompletionRate                     0
Evaluation/Iteration                        178
Evaluation/MaxReturn                        -33.0714
Evaluation/MinReturn                      -2128.06
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        361.664
Extras/EpisodeRewardMean                   -106.051
LinearFeatureBaseline/ExplainedVariance       0.00677962
PolicyExecTime                                0.100054
ProcessExecTime                               0.0114107
TotalEnvSteps                            181148
policy/Entropy                                0.6166
policy/KL                                     0.00525689
policy/KLBefore                               0
policy/LossAfter                             -0.0269432
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             1.85262
policy/dLoss                                  0.0269432
---------------------------------------  ----------------
2022-04-23 14:20:18 | [train_policy] epoch #179 | Obtaining samples for iteration 179...
2022-04-23 14:20:18 | [train_policy] epoch #179 | Logging diagnostics...
2022-04-23 14:20:18 | [train_policy] epoch #179 | Optimizing policy...
2022-04-23 14:20:18 | [train_policy] epoch #179 | Computing loss before
2022-04-23 14:20:18 | [train_policy] epoch #179 | Computing KL before
2022-04-23 14:20:18 | [train_policy] epoch #179 | Optimizing
2022-04-23 14:20:18 | [train_policy] epoch #179 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:18 | [train_policy] epoch #179 | computing loss before
2022-04-23 14:20:18 | [train_policy] epoch #179 | computing gradient
2022-04-23 14:20:18 | [train_policy] epoch #179 | gradient computed
2022-04-23 14:20:18 | [train_policy] epoch #179 | computing descent direction
2022-04-23 14:20:18 | [train_policy] epoch #179 | descent direction computed
2022-04-23 14:20:18 | [train_policy] epoch #179 | backtrack iters: 1
2022-04-23 14:20:18 | [train_policy] epoch #179 | optimization finished
2022-04-23 14:20:18 | [train_policy] epoch #179 | Computing KL after
2022-04-23 14:20:18 | [train_policy] epoch #179 | Computing loss after
2022-04-23 14:20:18 | [train_policy] epoch #179 | Fitting baseline...
2022-04-23 14:20:18 | [train_policy] epoch #179 | Saving snapshot...
2022-04-23 14:20:18 | [train_policy] epoch #179 | Saved
2022-04-23 14:20:18 | [train_policy] epoch #179 | Time 66.20 s
2022-04-23 14:20:18 | [train_policy] epoch #179 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.117004
Evaluation/AverageDiscountedReturn          -88.3427
Evaluation/AverageReturn                    -88.3427
Evaluation/CompletionRate                     0
Evaluation/Iteration                        179
Evaluation/MaxReturn                        -32.2319
Evaluation/MinReturn                      -2065.6
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.568
Extras/EpisodeRewardMean                   -104.997
LinearFeatureBaseline/ExplainedVariance       0.184788
PolicyExecTime                                0.106505
ProcessExecTime                               0.0118759
TotalEnvSteps                            182160
policy/Entropy                                0.617298
policy/KL                                     0.00679373
policy/KLBefore                               0
policy/LossAfter                             -0.0276963
policy/LossBefore                            -1.6727e-08
policy/Perplexity                             1.85391
policy/dLoss                                  0.0276963
---------------------------------------  ---------------
2022-04-23 14:20:18 | [train_policy] epoch #180 | Obtaining samples for iteration 180...
2022-04-23 14:20:18 | [train_policy] epoch #180 | Logging diagnostics...
2022-04-23 14:20:18 | [train_policy] epoch #180 | Optimizing policy...
2022-04-23 14:20:18 | [train_policy] epoch #180 | Computing loss before
2022-04-23 14:20:18 | [train_policy] epoch #180 | Computing KL before
2022-04-23 14:20:18 | [train_policy] epoch #180 | Optimizing
2022-04-23 14:20:18 | [train_policy] epoch #180 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:18 | [train_policy] epoch #180 | computing loss before
2022-04-23 14:20:18 | [train_policy] epoch #180 | computing gradient
2022-04-23 14:20:18 | [train_policy] epoch #180 | gradient computed
2022-04-23 14:20:18 | [train_policy] epoch #180 | computing descent direction
2022-04-23 14:20:18 | [train_policy] epoch #180 | descent direction computed
2022-04-23 14:20:18 | [train_policy] epoch #180 | backtrack iters: 0
2022-04-23 14:20:18 | [train_policy] epoch #180 | optimization finished
2022-04-23 14:20:18 | [train_policy] epoch #180 | Computing KL after
2022-04-23 14:20:18 | [train_policy] epoch #180 | Computing loss after
2022-04-23 14:20:18 | [train_policy] epoch #180 | Fitting baseline...
2022-04-23 14:20:18 | [train_policy] epoch #180 | Saving snapshot...
2022-04-23 14:20:18 | [train_policy] epoch #180 | Saved
2022-04-23 14:20:18 | [train_policy] epoch #180 | Time 66.55 s
2022-04-23 14:20:18 | [train_policy] epoch #180 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116381
Evaluation/AverageDiscountedReturn          -45.9526
Evaluation/AverageReturn                    -45.9526
Evaluation/CompletionRate                     0
Evaluation/Iteration                        180
Evaluation/MaxReturn                        -35.1577
Evaluation/MinReturn                        -78.9797
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.79861
Extras/EpisodeRewardMean                    -45.6605
LinearFeatureBaseline/ExplainedVariance     -66.0342
PolicyExecTime                                0.100562
ProcessExecTime                               0.0113151
TotalEnvSteps                            183172
policy/Entropy                                0.609684
policy/KL                                     0.00918405
policy/KLBefore                               0
policy/LossAfter                             -0.0172642
policy/LossBefore                            -2.42659e-08
policy/Perplexity                             1.83985
policy/dLoss                                  0.0172642
---------------------------------------  ----------------
2022-04-23 14:20:18 | [train_policy] epoch #181 | Obtaining samples for iteration 181...
2022-04-23 14:20:19 | [train_policy] epoch #181 | Logging diagnostics...
2022-04-23 14:20:19 | [train_policy] epoch #181 | Optimizing policy...
2022-04-23 14:20:19 | [train_policy] epoch #181 | Computing loss before
2022-04-23 14:20:19 | [train_policy] epoch #181 | Computing KL before
2022-04-23 14:20:19 | [train_policy] epoch #181 | Optimizing
2022-04-23 14:20:19 | [train_policy] epoch #181 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:19 | [train_policy] epoch #181 | computing loss before
2022-04-23 14:20:19 | [train_policy] epoch #181 | computing gradient
2022-04-23 14:20:19 | [train_policy] epoch #181 | gradient computed
2022-04-23 14:20:19 | [train_policy] epoch #181 | computing descent direction
2022-04-23 14:20:19 | [train_policy] epoch #181 | descent direction computed
2022-04-23 14:20:19 | [train_policy] epoch #181 | backtrack iters: 1
2022-04-23 14:20:19 | [train_policy] epoch #181 | optimization finished
2022-04-23 14:20:19 | [train_policy] epoch #181 | Computing KL after
2022-04-23 14:20:19 | [train_policy] epoch #181 | Computing loss after
2022-04-23 14:20:19 | [train_policy] epoch #181 | Fitting baseline...
2022-04-23 14:20:19 | [train_policy] epoch #181 | Saving snapshot...
2022-04-23 14:20:19 | [train_policy] epoch #181 | Saved
2022-04-23 14:20:19 | [train_policy] epoch #181 | Time 66.90 s
2022-04-23 14:20:19 | [train_policy] epoch #181 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117157
Evaluation/AverageDiscountedReturn          -75.3379
Evaluation/AverageReturn                    -75.3379
Evaluation/CompletionRate                     0
Evaluation/Iteration                        181
Evaluation/MaxReturn                        -34.4867
Evaluation/MinReturn                      -2063.97
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        224.079
Extras/EpisodeRewardMean                    -73.287
LinearFeatureBaseline/ExplainedVariance       0.0114142
PolicyExecTime                                0.101343
ProcessExecTime                               0.0115192
TotalEnvSteps                            184184
policy/Entropy                                0.641126
policy/KL                                     0.00704789
policy/KLBefore                               0
policy/LossAfter                             -0.015173
policy/LossBefore                             3.29828e-09
policy/Perplexity                             1.89862
policy/dLoss                                  0.015173
---------------------------------------  ----------------
2022-04-23 14:20:19 | [train_policy] epoch #182 | Obtaining samples for iteration 182...
2022-04-23 14:20:19 | [train_policy] epoch #182 | Logging diagnostics...
2022-04-23 14:20:19 | [train_policy] epoch #182 | Optimizing policy...
2022-04-23 14:20:19 | [train_policy] epoch #182 | Computing loss before
2022-04-23 14:20:19 | [train_policy] epoch #182 | Computing KL before
2022-04-23 14:20:19 | [train_policy] epoch #182 | Optimizing
2022-04-23 14:20:19 | [train_policy] epoch #182 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:19 | [train_policy] epoch #182 | computing loss before
2022-04-23 14:20:19 | [train_policy] epoch #182 | computing gradient
2022-04-23 14:20:19 | [train_policy] epoch #182 | gradient computed
2022-04-23 14:20:19 | [train_policy] epoch #182 | computing descent direction
2022-04-23 14:20:19 | [train_policy] epoch #182 | descent direction computed
2022-04-23 14:20:19 | [train_policy] epoch #182 | backtrack iters: 1
2022-04-23 14:20:19 | [train_policy] epoch #182 | optimization finished
2022-04-23 14:20:19 | [train_policy] epoch #182 | Computing KL after
2022-04-23 14:20:19 | [train_policy] epoch #182 | Computing loss after
2022-04-23 14:20:19 | [train_policy] epoch #182 | Fitting baseline...
2022-04-23 14:20:19 | [train_policy] epoch #182 | Saving snapshot...
2022-04-23 14:20:19 | [train_policy] epoch #182 | Saved
2022-04-23 14:20:19 | [train_policy] epoch #182 | Time 67.25 s
2022-04-23 14:20:19 | [train_policy] epoch #182 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116546
Evaluation/AverageDiscountedReturn          -68.6882
Evaluation/AverageReturn                    -68.6882
Evaluation/CompletionRate                     0
Evaluation/Iteration                        182
Evaluation/MaxReturn                        -30.3823
Evaluation/MinReturn                      -2071.37
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.081
Extras/EpisodeRewardMean                    -66.8329
LinearFeatureBaseline/ExplainedVariance       0.0179162
PolicyExecTime                                0.109107
ProcessExecTime                               0.0116043
TotalEnvSteps                            185196
policy/Entropy                                0.623152
policy/KL                                     0.00805387
policy/KLBefore                               0
policy/LossAfter                             -0.0130787
policy/LossBefore                            -3.46319e-08
policy/Perplexity                             1.8648
policy/dLoss                                  0.0130786
---------------------------------------  ----------------
2022-04-23 14:20:19 | [train_policy] epoch #183 | Obtaining samples for iteration 183...
2022-04-23 14:20:19 | [train_policy] epoch #183 | Logging diagnostics...
2022-04-23 14:20:19 | [train_policy] epoch #183 | Optimizing policy...
2022-04-23 14:20:19 | [train_policy] epoch #183 | Computing loss before
2022-04-23 14:20:19 | [train_policy] epoch #183 | Computing KL before
2022-04-23 14:20:19 | [train_policy] epoch #183 | Optimizing
2022-04-23 14:20:19 | [train_policy] epoch #183 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:19 | [train_policy] epoch #183 | computing loss before
2022-04-23 14:20:19 | [train_policy] epoch #183 | computing gradient
2022-04-23 14:20:19 | [train_policy] epoch #183 | gradient computed
2022-04-23 14:20:19 | [train_policy] epoch #183 | computing descent direction
2022-04-23 14:20:19 | [train_policy] epoch #183 | descent direction computed
2022-04-23 14:20:19 | [train_policy] epoch #183 | backtrack iters: 0
2022-04-23 14:20:19 | [train_policy] epoch #183 | optimization finished
2022-04-23 14:20:19 | [train_policy] epoch #183 | Computing KL after
2022-04-23 14:20:19 | [train_policy] epoch #183 | Computing loss after
2022-04-23 14:20:19 | [train_policy] epoch #183 | Fitting baseline...
2022-04-23 14:20:19 | [train_policy] epoch #183 | Saving snapshot...
2022-04-23 14:20:19 | [train_policy] epoch #183 | Saved
2022-04-23 14:20:19 | [train_policy] epoch #183 | Time 67.60 s
2022-04-23 14:20:19 | [train_policy] epoch #183 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117263
Evaluation/AverageDiscountedReturn          -45.1044
Evaluation/AverageReturn                    -45.1044
Evaluation/CompletionRate                     0
Evaluation/Iteration                        183
Evaluation/MaxReturn                        -32.7253
Evaluation/MinReturn                       -115.196
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.86354
Extras/EpisodeRewardMean                    -45.3295
LinearFeatureBaseline/ExplainedVariance     -10.441
PolicyExecTime                                0.104079
ProcessExecTime                               0.0116405
TotalEnvSteps                            186208
policy/Entropy                                0.637451
policy/KL                                     0.00969403
policy/KLBefore                               0
policy/LossAfter                             -0.0157262
policy/LossBefore                            -7.06774e-10
policy/Perplexity                             1.89165
policy/dLoss                                  0.0157262
---------------------------------------  ----------------
2022-04-23 14:20:19 | [train_policy] epoch #184 | Obtaining samples for iteration 184...
2022-04-23 14:20:20 | [train_policy] epoch #184 | Logging diagnostics...
2022-04-23 14:20:20 | [train_policy] epoch #184 | Optimizing policy...
2022-04-23 14:20:20 | [train_policy] epoch #184 | Computing loss before
2022-04-23 14:20:20 | [train_policy] epoch #184 | Computing KL before
2022-04-23 14:20:20 | [train_policy] epoch #184 | Optimizing
2022-04-23 14:20:20 | [train_policy] epoch #184 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:20 | [train_policy] epoch #184 | computing loss before
2022-04-23 14:20:20 | [train_policy] epoch #184 | computing gradient
2022-04-23 14:20:20 | [train_policy] epoch #184 | gradient computed
2022-04-23 14:20:20 | [train_policy] epoch #184 | computing descent direction
2022-04-23 14:20:20 | [train_policy] epoch #184 | descent direction computed
2022-04-23 14:20:20 | [train_policy] epoch #184 | backtrack iters: 1
2022-04-23 14:20:20 | [train_policy] epoch #184 | optimization finished
2022-04-23 14:20:20 | [train_policy] epoch #184 | Computing KL after
2022-04-23 14:20:20 | [train_policy] epoch #184 | Computing loss after
2022-04-23 14:20:20 | [train_policy] epoch #184 | Fitting baseline...
2022-04-23 14:20:20 | [train_policy] epoch #184 | Saving snapshot...
2022-04-23 14:20:20 | [train_policy] epoch #184 | Saved
2022-04-23 14:20:20 | [train_policy] epoch #184 | Time 67.94 s
2022-04-23 14:20:20 | [train_policy] epoch #184 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.115755
Evaluation/AverageDiscountedReturn          -66.43
Evaluation/AverageReturn                    -66.43
Evaluation/CompletionRate                     0
Evaluation/Iteration                        184
Evaluation/MaxReturn                        -34.3467
Evaluation/MinReturn                      -2103.91
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        213.663
Extras/EpisodeRewardMean                    -65.0152
LinearFeatureBaseline/ExplainedVariance       0.0167069
PolicyExecTime                                0.103826
ProcessExecTime                               0.0113266
TotalEnvSteps                            187220
policy/Entropy                                0.619911
policy/KL                                     0.00744722
policy/KLBefore                               0
policy/LossAfter                             -0.018147
policy/LossBefore                             2.8271e-09
policy/Perplexity                             1.85876
policy/dLoss                                  0.0181471
---------------------------------------  ---------------
2022-04-23 14:20:20 | [train_policy] epoch #185 | Obtaining samples for iteration 185...
2022-04-23 14:20:20 | [train_policy] epoch #185 | Logging diagnostics...
2022-04-23 14:20:20 | [train_policy] epoch #185 | Optimizing policy...
2022-04-23 14:20:20 | [train_policy] epoch #185 | Computing loss before
2022-04-23 14:20:20 | [train_policy] epoch #185 | Computing KL before
2022-04-23 14:20:20 | [train_policy] epoch #185 | Optimizing
2022-04-23 14:20:20 | [train_policy] epoch #185 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:20 | [train_policy] epoch #185 | computing loss before
2022-04-23 14:20:20 | [train_policy] epoch #185 | computing gradient
2022-04-23 14:20:20 | [train_policy] epoch #185 | gradient computed
2022-04-23 14:20:20 | [train_policy] epoch #185 | computing descent direction
2022-04-23 14:20:20 | [train_policy] epoch #185 | descent direction computed
2022-04-23 14:20:20 | [train_policy] epoch #185 | backtrack iters: 1
2022-04-23 14:20:20 | [train_policy] epoch #185 | optimization finished
2022-04-23 14:20:20 | [train_policy] epoch #185 | Computing KL after
2022-04-23 14:20:20 | [train_policy] epoch #185 | Computing loss after
2022-04-23 14:20:20 | [train_policy] epoch #185 | Fitting baseline...
2022-04-23 14:20:20 | [train_policy] epoch #185 | Saving snapshot...
2022-04-23 14:20:20 | [train_policy] epoch #185 | Saved
2022-04-23 14:20:20 | [train_policy] epoch #185 | Time 68.29 s
2022-04-23 14:20:20 | [train_policy] epoch #185 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116099
Evaluation/AverageDiscountedReturn          -68.4724
Evaluation/AverageReturn                    -68.4724
Evaluation/CompletionRate                     0
Evaluation/Iteration                        185
Evaluation/MaxReturn                        -34.488
Evaluation/MinReturn                      -2075.64
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.574
Extras/EpisodeRewardMean                    -66.3386
LinearFeatureBaseline/ExplainedVariance       0.0591892
PolicyExecTime                                0.0976813
ProcessExecTime                               0.011179
TotalEnvSteps                            188232
policy/Entropy                                0.605766
policy/KL                                     0.00724929
policy/KLBefore                               0
policy/LossAfter                             -0.0146739
policy/LossBefore                            -1.90829e-08
policy/Perplexity                             1.83266
policy/dLoss                                  0.0146739
---------------------------------------  ----------------
2022-04-23 14:20:20 | [train_policy] epoch #186 | Obtaining samples for iteration 186...
2022-04-23 14:20:20 | [train_policy] epoch #186 | Logging diagnostics...
2022-04-23 14:20:20 | [train_policy] epoch #186 | Optimizing policy...
2022-04-23 14:20:20 | [train_policy] epoch #186 | Computing loss before
2022-04-23 14:20:20 | [train_policy] epoch #186 | Computing KL before
2022-04-23 14:20:20 | [train_policy] epoch #186 | Optimizing
2022-04-23 14:20:20 | [train_policy] epoch #186 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:20 | [train_policy] epoch #186 | computing loss before
2022-04-23 14:20:20 | [train_policy] epoch #186 | computing gradient
2022-04-23 14:20:20 | [train_policy] epoch #186 | gradient computed
2022-04-23 14:20:20 | [train_policy] epoch #186 | computing descent direction
2022-04-23 14:20:20 | [train_policy] epoch #186 | descent direction computed
2022-04-23 14:20:20 | [train_policy] epoch #186 | backtrack iters: 0
2022-04-23 14:20:20 | [train_policy] epoch #186 | optimization finished
2022-04-23 14:20:20 | [train_policy] epoch #186 | Computing KL after
2022-04-23 14:20:20 | [train_policy] epoch #186 | Computing loss after
2022-04-23 14:20:20 | [train_policy] epoch #186 | Fitting baseline...
2022-04-23 14:20:20 | [train_policy] epoch #186 | Saving snapshot...
2022-04-23 14:20:20 | [train_policy] epoch #186 | Saved
2022-04-23 14:20:20 | [train_policy] epoch #186 | Time 68.64 s
2022-04-23 14:20:20 | [train_policy] epoch #186 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117744
Evaluation/AverageDiscountedReturn          -87.7852
Evaluation/AverageReturn                    -87.7852
Evaluation/CompletionRate                     0
Evaluation/Iteration                        186
Evaluation/MaxReturn                        -33.5743
Evaluation/MinReturn                      -2057.44
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        293.385
Extras/EpisodeRewardMean                    -84.6155
LinearFeatureBaseline/ExplainedVariance       0.118204
PolicyExecTime                                0.104543
ProcessExecTime                               0.0119867
TotalEnvSteps                            189244
policy/Entropy                                0.602588
policy/KL                                     0.00969575
policy/KLBefore                               0
policy/LossAfter                             -0.0246076
policy/LossBefore                             1.88473e-09
policy/Perplexity                             1.82684
policy/dLoss                                  0.0246076
---------------------------------------  ----------------
2022-04-23 14:20:20 | [train_policy] epoch #187 | Obtaining samples for iteration 187...
2022-04-23 14:20:21 | [train_policy] epoch #187 | Logging diagnostics...
2022-04-23 14:20:21 | [train_policy] epoch #187 | Optimizing policy...
2022-04-23 14:20:21 | [train_policy] epoch #187 | Computing loss before
2022-04-23 14:20:21 | [train_policy] epoch #187 | Computing KL before
2022-04-23 14:20:21 | [train_policy] epoch #187 | Optimizing
2022-04-23 14:20:21 | [train_policy] epoch #187 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:21 | [train_policy] epoch #187 | computing loss before
2022-04-23 14:20:21 | [train_policy] epoch #187 | computing gradient
2022-04-23 14:20:21 | [train_policy] epoch #187 | gradient computed
2022-04-23 14:20:21 | [train_policy] epoch #187 | computing descent direction
2022-04-23 14:20:21 | [train_policy] epoch #187 | descent direction computed
2022-04-23 14:20:21 | [train_policy] epoch #187 | backtrack iters: 0
2022-04-23 14:20:21 | [train_policy] epoch #187 | optimization finished
2022-04-23 14:20:21 | [train_policy] epoch #187 | Computing KL after
2022-04-23 14:20:21 | [train_policy] epoch #187 | Computing loss after
2022-04-23 14:20:21 | [train_policy] epoch #187 | Fitting baseline...
2022-04-23 14:20:21 | [train_policy] epoch #187 | Saving snapshot...
2022-04-23 14:20:21 | [train_policy] epoch #187 | Saved
2022-04-23 14:20:21 | [train_policy] epoch #187 | Time 69.00 s
2022-04-23 14:20:21 | [train_policy] epoch #187 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118996
Evaluation/AverageDiscountedReturn          -66.11
Evaluation/AverageReturn                    -66.11
Evaluation/CompletionRate                     0
Evaluation/Iteration                        187
Evaluation/MaxReturn                        -34.9572
Evaluation/MinReturn                      -2065.87
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.716
Extras/EpisodeRewardMean                    -64.0753
LinearFeatureBaseline/ExplainedVariance       0.0693694
PolicyExecTime                                0.111267
ProcessExecTime                               0.0114737
TotalEnvSteps                            190256
policy/Entropy                                0.640169
policy/KL                                     0.00973296
policy/KLBefore                               0
policy/LossAfter                             -0.0282278
policy/LossBefore                            -1.60202e-08
policy/Perplexity                             1.8968
policy/dLoss                                  0.0282278
---------------------------------------  ----------------
2022-04-23 14:20:21 | [train_policy] epoch #188 | Obtaining samples for iteration 188...
2022-04-23 14:20:21 | [train_policy] epoch #188 | Logging diagnostics...
2022-04-23 14:20:21 | [train_policy] epoch #188 | Optimizing policy...
2022-04-23 14:20:21 | [train_policy] epoch #188 | Computing loss before
2022-04-23 14:20:21 | [train_policy] epoch #188 | Computing KL before
2022-04-23 14:20:21 | [train_policy] epoch #188 | Optimizing
2022-04-23 14:20:21 | [train_policy] epoch #188 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:21 | [train_policy] epoch #188 | computing loss before
2022-04-23 14:20:21 | [train_policy] epoch #188 | computing gradient
2022-04-23 14:20:21 | [train_policy] epoch #188 | gradient computed
2022-04-23 14:20:21 | [train_policy] epoch #188 | computing descent direction
2022-04-23 14:20:21 | [train_policy] epoch #188 | descent direction computed
2022-04-23 14:20:21 | [train_policy] epoch #188 | backtrack iters: 1
2022-04-23 14:20:21 | [train_policy] epoch #188 | optimization finished
2022-04-23 14:20:21 | [train_policy] epoch #188 | Computing KL after
2022-04-23 14:20:21 | [train_policy] epoch #188 | Computing loss after
2022-04-23 14:20:21 | [train_policy] epoch #188 | Fitting baseline...
2022-04-23 14:20:21 | [train_policy] epoch #188 | Saving snapshot...
2022-04-23 14:20:21 | [train_policy] epoch #188 | Saved
2022-04-23 14:20:21 | [train_policy] epoch #188 | Time 69.37 s
2022-04-23 14:20:21 | [train_policy] epoch #188 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.121613
Evaluation/AverageDiscountedReturn          -45.6117
Evaluation/AverageReturn                    -45.6117
Evaluation/CompletionRate                     0
Evaluation/Iteration                        188
Evaluation/MaxReturn                        -34.9117
Evaluation/MinReturn                        -70.5481
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.37893
Extras/EpisodeRewardMean                    -65.9749
LinearFeatureBaseline/ExplainedVariance     -34.0684
PolicyExecTime                                0.111345
ProcessExecTime                               0.0117474
TotalEnvSteps                            191268
policy/Entropy                                0.628105
policy/KL                                     0.00678114
policy/KLBefore                               0
policy/LossAfter                             -0.0185977
policy/LossBefore                            -4.57047e-08
policy/Perplexity                             1.87406
policy/dLoss                                  0.0185976
---------------------------------------  ----------------
2022-04-23 14:20:21 | [train_policy] epoch #189 | Obtaining samples for iteration 189...
2022-04-23 14:20:21 | [train_policy] epoch #189 | Logging diagnostics...
2022-04-23 14:20:21 | [train_policy] epoch #189 | Optimizing policy...
2022-04-23 14:20:21 | [train_policy] epoch #189 | Computing loss before
2022-04-23 14:20:21 | [train_policy] epoch #189 | Computing KL before
2022-04-23 14:20:21 | [train_policy] epoch #189 | Optimizing
2022-04-23 14:20:21 | [train_policy] epoch #189 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:21 | [train_policy] epoch #189 | computing loss before
2022-04-23 14:20:21 | [train_policy] epoch #189 | computing gradient
2022-04-23 14:20:21 | [train_policy] epoch #189 | gradient computed
2022-04-23 14:20:21 | [train_policy] epoch #189 | computing descent direction
2022-04-23 14:20:21 | [train_policy] epoch #189 | descent direction computed
2022-04-23 14:20:21 | [train_policy] epoch #189 | backtrack iters: 0
2022-04-23 14:20:21 | [train_policy] epoch #189 | optimization finished
2022-04-23 14:20:21 | [train_policy] epoch #189 | Computing KL after
2022-04-23 14:20:21 | [train_policy] epoch #189 | Computing loss after
2022-04-23 14:20:21 | [train_policy] epoch #189 | Fitting baseline...
2022-04-23 14:20:21 | [train_policy] epoch #189 | Saving snapshot...
2022-04-23 14:20:22 | [train_policy] epoch #189 | Saved
2022-04-23 14:20:22 | [train_policy] epoch #189 | Time 69.72 s
2022-04-23 14:20:22 | [train_policy] epoch #189 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118345
Evaluation/AverageDiscountedReturn          -70.7377
Evaluation/AverageReturn                    -70.7377
Evaluation/CompletionRate                     0
Evaluation/Iteration                        189
Evaluation/MaxReturn                        -33.4159
Evaluation/MinReturn                      -2069.27
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.086
Extras/EpisodeRewardMean                    -68.74
LinearFeatureBaseline/ExplainedVariance       0.0138842
PolicyExecTime                                0.103453
ProcessExecTime                               0.0109949
TotalEnvSteps                            192280
policy/Entropy                                0.636929
policy/KL                                     0.00922212
policy/KLBefore                               0
policy/LossAfter                             -0.0207081
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             1.89067
policy/dLoss                                  0.0207081
---------------------------------------  ----------------
2022-04-23 14:20:22 | [train_policy] epoch #190 | Obtaining samples for iteration 190...
2022-04-23 14:20:22 | [train_policy] epoch #190 | Logging diagnostics...
2022-04-23 14:20:22 | [train_policy] epoch #190 | Optimizing policy...
2022-04-23 14:20:22 | [train_policy] epoch #190 | Computing loss before
2022-04-23 14:20:22 | [train_policy] epoch #190 | Computing KL before
2022-04-23 14:20:22 | [train_policy] epoch #190 | Optimizing
2022-04-23 14:20:22 | [train_policy] epoch #190 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:22 | [train_policy] epoch #190 | computing loss before
2022-04-23 14:20:22 | [train_policy] epoch #190 | computing gradient
2022-04-23 14:20:22 | [train_policy] epoch #190 | gradient computed
2022-04-23 14:20:22 | [train_policy] epoch #190 | computing descent direction
2022-04-23 14:20:22 | [train_policy] epoch #190 | descent direction computed
2022-04-23 14:20:22 | [train_policy] epoch #190 | backtrack iters: 1
2022-04-23 14:20:22 | [train_policy] epoch #190 | optimization finished
2022-04-23 14:20:22 | [train_policy] epoch #190 | Computing KL after
2022-04-23 14:20:22 | [train_policy] epoch #190 | Computing loss after
2022-04-23 14:20:22 | [train_policy] epoch #190 | Fitting baseline...
2022-04-23 14:20:22 | [train_policy] epoch #190 | Saving snapshot...
2022-04-23 14:20:22 | [train_policy] epoch #190 | Saved
2022-04-23 14:20:22 | [train_policy] epoch #190 | Time 70.07 s
2022-04-23 14:20:22 | [train_policy] epoch #190 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117057
Evaluation/AverageDiscountedReturn          -67.7068
Evaluation/AverageReturn                    -67.7068
Evaluation/CompletionRate                     0
Evaluation/Iteration                        190
Evaluation/MaxReturn                        -33.8557
Evaluation/MinReturn                      -2056.44
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.625
Extras/EpisodeRewardMean                    -65.8463
LinearFeatureBaseline/ExplainedVariance       0.0526095
PolicyExecTime                                0.109039
ProcessExecTime                               0.0112417
TotalEnvSteps                            193292
policy/Entropy                                0.626643
policy/KL                                     0.00782304
policy/KLBefore                               0
policy/LossAfter                             -0.023303
policy/LossBefore                             8.71688e-09
policy/Perplexity                             1.87132
policy/dLoss                                  0.023303
---------------------------------------  ----------------
2022-04-23 14:20:22 | [train_policy] epoch #191 | Obtaining samples for iteration 191...
2022-04-23 14:20:22 | [train_policy] epoch #191 | Logging diagnostics...
2022-04-23 14:20:22 | [train_policy] epoch #191 | Optimizing policy...
2022-04-23 14:20:22 | [train_policy] epoch #191 | Computing loss before
2022-04-23 14:20:22 | [train_policy] epoch #191 | Computing KL before
2022-04-23 14:20:22 | [train_policy] epoch #191 | Optimizing
2022-04-23 14:20:22 | [train_policy] epoch #191 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:22 | [train_policy] epoch #191 | computing loss before
2022-04-23 14:20:22 | [train_policy] epoch #191 | computing gradient
2022-04-23 14:20:22 | [train_policy] epoch #191 | gradient computed
2022-04-23 14:20:22 | [train_policy] epoch #191 | computing descent direction
2022-04-23 14:20:22 | [train_policy] epoch #191 | descent direction computed
2022-04-23 14:20:22 | [train_policy] epoch #191 | backtrack iters: 1
2022-04-23 14:20:22 | [train_policy] epoch #191 | optimization finished
2022-04-23 14:20:22 | [train_policy] epoch #191 | Computing KL after
2022-04-23 14:20:22 | [train_policy] epoch #191 | Computing loss after
2022-04-23 14:20:22 | [train_policy] epoch #191 | Fitting baseline...
2022-04-23 14:20:22 | [train_policy] epoch #191 | Saving snapshot...
2022-04-23 14:20:22 | [train_policy] epoch #191 | Saved
2022-04-23 14:20:22 | [train_policy] epoch #191 | Time 70.42 s
2022-04-23 14:20:22 | [train_policy] epoch #191 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118583
Evaluation/AverageDiscountedReturn         -135.214
Evaluation/AverageReturn                   -135.214
Evaluation/CompletionRate                     0
Evaluation/Iteration                        191
Evaluation/MaxReturn                        -34.4267
Evaluation/MinReturn                      -2139.77
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        415.402
Extras/EpisodeRewardMean                   -127.929
LinearFeatureBaseline/ExplainedVariance       0.148133
PolicyExecTime                                0.104921
ProcessExecTime                               0.0111167
TotalEnvSteps                            194304
policy/Entropy                                0.659015
policy/KL                                     0.00808689
policy/KLBefore                               0
policy/LossAfter                             -0.01627
policy/LossBefore                             1.17796e-09
policy/Perplexity                             1.93289
policy/dLoss                                  0.01627
---------------------------------------  ----------------
2022-04-23 14:20:22 | [train_policy] epoch #192 | Obtaining samples for iteration 192...
2022-04-23 14:20:22 | [train_policy] epoch #192 | Logging diagnostics...
2022-04-23 14:20:22 | [train_policy] epoch #192 | Optimizing policy...
2022-04-23 14:20:22 | [train_policy] epoch #192 | Computing loss before
2022-04-23 14:20:22 | [train_policy] epoch #192 | Computing KL before
2022-04-23 14:20:22 | [train_policy] epoch #192 | Optimizing
2022-04-23 14:20:22 | [train_policy] epoch #192 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:22 | [train_policy] epoch #192 | computing loss before
2022-04-23 14:20:22 | [train_policy] epoch #192 | computing gradient
2022-04-23 14:20:22 | [train_policy] epoch #192 | gradient computed
2022-04-23 14:20:22 | [train_policy] epoch #192 | computing descent direction
2022-04-23 14:20:23 | [train_policy] epoch #192 | descent direction computed
2022-04-23 14:20:23 | [train_policy] epoch #192 | backtrack iters: 0
2022-04-23 14:20:23 | [train_policy] epoch #192 | optimization finished
2022-04-23 14:20:23 | [train_policy] epoch #192 | Computing KL after
2022-04-23 14:20:23 | [train_policy] epoch #192 | Computing loss after
2022-04-23 14:20:23 | [train_policy] epoch #192 | Fitting baseline...
2022-04-23 14:20:23 | [train_policy] epoch #192 | Saving snapshot...
2022-04-23 14:20:23 | [train_policy] epoch #192 | Saved
2022-04-23 14:20:23 | [train_policy] epoch #192 | Time 70.78 s
2022-04-23 14:20:23 | [train_policy] epoch #192 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.123273
Evaluation/AverageDiscountedReturn          -68.4309
Evaluation/AverageReturn                    -68.4309
Evaluation/CompletionRate                     0
Evaluation/Iteration                        192
Evaluation/MaxReturn                        -33.8213
Evaluation/MinReturn                      -2054.44
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.617
Extras/EpisodeRewardMean                    -66.5512
LinearFeatureBaseline/ExplainedVariance      -0.241102
PolicyExecTime                                0.107142
ProcessExecTime                               0.0116885
TotalEnvSteps                            195316
policy/Entropy                                0.671209
policy/KL                                     0.00914324
policy/KLBefore                               0
policy/LossAfter                             -0.0275939
policy/LossBefore                             7.06774e-10
policy/Perplexity                             1.9566
policy/dLoss                                  0.0275939
---------------------------------------  ----------------
2022-04-23 14:20:23 | [train_policy] epoch #193 | Obtaining samples for iteration 193...
2022-04-23 14:20:23 | [train_policy] epoch #193 | Logging diagnostics...
2022-04-23 14:20:23 | [train_policy] epoch #193 | Optimizing policy...
2022-04-23 14:20:23 | [train_policy] epoch #193 | Computing loss before
2022-04-23 14:20:23 | [train_policy] epoch #193 | Computing KL before
2022-04-23 14:20:23 | [train_policy] epoch #193 | Optimizing
2022-04-23 14:20:23 | [train_policy] epoch #193 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:23 | [train_policy] epoch #193 | computing loss before
2022-04-23 14:20:23 | [train_policy] epoch #193 | computing gradient
2022-04-23 14:20:23 | [train_policy] epoch #193 | gradient computed
2022-04-23 14:20:23 | [train_policy] epoch #193 | computing descent direction
2022-04-23 14:20:23 | [train_policy] epoch #193 | descent direction computed
2022-04-23 14:20:23 | [train_policy] epoch #193 | backtrack iters: 1
2022-04-23 14:20:23 | [train_policy] epoch #193 | optimization finished
2022-04-23 14:20:23 | [train_policy] epoch #193 | Computing KL after
2022-04-23 14:20:23 | [train_policy] epoch #193 | Computing loss after
2022-04-23 14:20:23 | [train_policy] epoch #193 | Fitting baseline...
2022-04-23 14:20:23 | [train_policy] epoch #193 | Saving snapshot...
2022-04-23 14:20:23 | [train_policy] epoch #193 | Saved
2022-04-23 14:20:23 | [train_policy] epoch #193 | Time 71.14 s
2022-04-23 14:20:23 | [train_policy] epoch #193 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117919
Evaluation/AverageDiscountedReturn          -46.626
Evaluation/AverageReturn                    -46.626
Evaluation/CompletionRate                     0
Evaluation/Iteration                        193
Evaluation/MaxReturn                        -35.0153
Evaluation/MinReturn                        -74.2747
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.90898
Extras/EpisodeRewardMean                    -46.3693
LinearFeatureBaseline/ExplainedVariance     -17.0071
PolicyExecTime                                0.102541
ProcessExecTime                               0.0112433
TotalEnvSteps                            196328
policy/Entropy                                0.686114
policy/KL                                     0.00671442
policy/KLBefore                               0
policy/LossAfter                             -0.018577
policy/LossBefore                             3.10981e-08
policy/Perplexity                             1.98598
policy/dLoss                                  0.018577
---------------------------------------  ----------------
2022-04-23 14:20:23 | [train_policy] epoch #194 | Obtaining samples for iteration 194...
2022-04-23 14:20:23 | [train_policy] epoch #194 | Logging diagnostics...
2022-04-23 14:20:23 | [train_policy] epoch #194 | Optimizing policy...
2022-04-23 14:20:23 | [train_policy] epoch #194 | Computing loss before
2022-04-23 14:20:23 | [train_policy] epoch #194 | Computing KL before
2022-04-23 14:20:23 | [train_policy] epoch #194 | Optimizing
2022-04-23 14:20:23 | [train_policy] epoch #194 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:23 | [train_policy] epoch #194 | computing loss before
2022-04-23 14:20:23 | [train_policy] epoch #194 | computing gradient
2022-04-23 14:20:23 | [train_policy] epoch #194 | gradient computed
2022-04-23 14:20:23 | [train_policy] epoch #194 | computing descent direction
2022-04-23 14:20:23 | [train_policy] epoch #194 | descent direction computed
2022-04-23 14:20:23 | [train_policy] epoch #194 | backtrack iters: 0
2022-04-23 14:20:23 | [train_policy] epoch #194 | optimization finished
2022-04-23 14:20:23 | [train_policy] epoch #194 | Computing KL after
2022-04-23 14:20:23 | [train_policy] epoch #194 | Computing loss after
2022-04-23 14:20:23 | [train_policy] epoch #194 | Fitting baseline...
2022-04-23 14:20:23 | [train_policy] epoch #194 | Saving snapshot...
2022-04-23 14:20:23 | [train_policy] epoch #194 | Saved
2022-04-23 14:20:23 | [train_policy] epoch #194 | Time 71.50 s
2022-04-23 14:20:23 | [train_policy] epoch #194 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.123198
Evaluation/AverageDiscountedReturn          -45.3538
Evaluation/AverageReturn                    -45.3538
Evaluation/CompletionRate                     0
Evaluation/Iteration                        194
Evaluation/MaxReturn                        -31.6201
Evaluation/MinReturn                        -84.1359
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.90365
Extras/EpisodeRewardMean                    -45.766
LinearFeatureBaseline/ExplainedVariance       0.800088
PolicyExecTime                                0.107247
ProcessExecTime                               0.0122116
TotalEnvSteps                            197340
policy/Entropy                                0.62066
policy/KL                                     0.00978882
policy/KLBefore                               0
policy/LossAfter                             -0.0144931
policy/LossBefore                            -1.64914e-09
policy/Perplexity                             1.86016
policy/dLoss                                  0.0144931
---------------------------------------  ----------------
2022-04-23 14:20:23 | [train_policy] epoch #195 | Obtaining samples for iteration 195...
2022-04-23 14:20:24 | [train_policy] epoch #195 | Logging diagnostics...
2022-04-23 14:20:24 | [train_policy] epoch #195 | Optimizing policy...
2022-04-23 14:20:24 | [train_policy] epoch #195 | Computing loss before
2022-04-23 14:20:24 | [train_policy] epoch #195 | Computing KL before
2022-04-23 14:20:24 | [train_policy] epoch #195 | Optimizing
2022-04-23 14:20:24 | [train_policy] epoch #195 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:24 | [train_policy] epoch #195 | computing loss before
2022-04-23 14:20:24 | [train_policy] epoch #195 | computing gradient
2022-04-23 14:20:24 | [train_policy] epoch #195 | gradient computed
2022-04-23 14:20:24 | [train_policy] epoch #195 | computing descent direction
2022-04-23 14:20:24 | [train_policy] epoch #195 | descent direction computed
2022-04-23 14:20:24 | [train_policy] epoch #195 | backtrack iters: 0
2022-04-23 14:20:24 | [train_policy] epoch #195 | optimization finished
2022-04-23 14:20:24 | [train_policy] epoch #195 | Computing KL after
2022-04-23 14:20:24 | [train_policy] epoch #195 | Computing loss after
2022-04-23 14:20:24 | [train_policy] epoch #195 | Fitting baseline...
2022-04-23 14:20:24 | [train_policy] epoch #195 | Saving snapshot...
2022-04-23 14:20:24 | [train_policy] epoch #195 | Saved
2022-04-23 14:20:24 | [train_policy] epoch #195 | Time 71.86 s
2022-04-23 14:20:24 | [train_policy] epoch #195 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.123483
Evaluation/AverageDiscountedReturn          -89.8874
Evaluation/AverageReturn                    -89.8874
Evaluation/CompletionRate                     0
Evaluation/Iteration                        195
Evaluation/MaxReturn                        -35.2355
Evaluation/MinReturn                      -2076.5
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        296.11
Extras/EpisodeRewardMean                    -86.2862
LinearFeatureBaseline/ExplainedVariance       0.00692271
PolicyExecTime                                0.108206
ProcessExecTime                               0.0121834
TotalEnvSteps                            198352
policy/Entropy                                0.647373
policy/KL                                     0.009155
policy/KLBefore                               0
policy/LossAfter                             -0.023355
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             1.91052
policy/dLoss                                  0.023355
---------------------------------------  ----------------
2022-04-23 14:20:24 | [train_policy] epoch #196 | Obtaining samples for iteration 196...
2022-04-23 14:20:24 | [train_policy] epoch #196 | Logging diagnostics...
2022-04-23 14:20:24 | [train_policy] epoch #196 | Optimizing policy...
2022-04-23 14:20:24 | [train_policy] epoch #196 | Computing loss before
2022-04-23 14:20:24 | [train_policy] epoch #196 | Computing KL before
2022-04-23 14:20:24 | [train_policy] epoch #196 | Optimizing
2022-04-23 14:20:24 | [train_policy] epoch #196 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:24 | [train_policy] epoch #196 | computing loss before
2022-04-23 14:20:24 | [train_policy] epoch #196 | computing gradient
2022-04-23 14:20:24 | [train_policy] epoch #196 | gradient computed
2022-04-23 14:20:24 | [train_policy] epoch #196 | computing descent direction
2022-04-23 14:20:24 | [train_policy] epoch #196 | descent direction computed
2022-04-23 14:20:24 | [train_policy] epoch #196 | backtrack iters: 1
2022-04-23 14:20:24 | [train_policy] epoch #196 | optimization finished
2022-04-23 14:20:24 | [train_policy] epoch #196 | Computing KL after
2022-04-23 14:20:24 | [train_policy] epoch #196 | Computing loss after
2022-04-23 14:20:24 | [train_policy] epoch #196 | Fitting baseline...
2022-04-23 14:20:24 | [train_policy] epoch #196 | Saving snapshot...
2022-04-23 14:20:24 | [train_policy] epoch #196 | Saved
2022-04-23 14:20:24 | [train_policy] epoch #196 | Time 72.20 s
2022-04-23 14:20:24 | [train_policy] epoch #196 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116309
Evaluation/AverageDiscountedReturn          -68.3437
Evaluation/AverageReturn                    -68.3437
Evaluation/CompletionRate                     0
Evaluation/Iteration                        196
Evaluation/MaxReturn                        -34.3462
Evaluation/MinReturn                      -2061.9
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.175
Extras/EpisodeRewardMean                    -66.5032
LinearFeatureBaseline/ExplainedVariance       0.0215744
PolicyExecTime                                0.102106
ProcessExecTime                               0.0110011
TotalEnvSteps                            199364
policy/Entropy                                0.627232
policy/KL                                     0.006674
policy/KLBefore                               0
policy/LossAfter                             -0.0260217
policy/LossBefore                            -2.12032e-09
policy/Perplexity                             1.87242
policy/dLoss                                  0.0260217
---------------------------------------  ----------------
2022-04-23 14:20:24 | [train_policy] epoch #197 | Obtaining samples for iteration 197...
2022-04-23 14:20:24 | [train_policy] epoch #197 | Logging diagnostics...
2022-04-23 14:20:24 | [train_policy] epoch #197 | Optimizing policy...
2022-04-23 14:20:24 | [train_policy] epoch #197 | Computing loss before
2022-04-23 14:20:24 | [train_policy] epoch #197 | Computing KL before
2022-04-23 14:20:24 | [train_policy] epoch #197 | Optimizing
2022-04-23 14:20:24 | [train_policy] epoch #197 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:24 | [train_policy] epoch #197 | computing loss before
2022-04-23 14:20:24 | [train_policy] epoch #197 | computing gradient
2022-04-23 14:20:24 | [train_policy] epoch #197 | gradient computed
2022-04-23 14:20:24 | [train_policy] epoch #197 | computing descent direction
2022-04-23 14:20:24 | [train_policy] epoch #197 | descent direction computed
2022-04-23 14:20:24 | [train_policy] epoch #197 | backtrack iters: 1
2022-04-23 14:20:24 | [train_policy] epoch #197 | optimization finished
2022-04-23 14:20:24 | [train_policy] epoch #197 | Computing KL after
2022-04-23 14:20:24 | [train_policy] epoch #197 | Computing loss after
2022-04-23 14:20:24 | [train_policy] epoch #197 | Fitting baseline...
2022-04-23 14:20:24 | [train_policy] epoch #197 | Saving snapshot...
2022-04-23 14:20:24 | [train_policy] epoch #197 | Saved
2022-04-23 14:20:24 | [train_policy] epoch #197 | Time 72.55 s
2022-04-23 14:20:24 | [train_policy] epoch #197 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11687
Evaluation/AverageDiscountedReturn          -45.8419
Evaluation/AverageReturn                    -45.8419
Evaluation/CompletionRate                     0
Evaluation/Iteration                        197
Evaluation/MaxReturn                        -36.1668
Evaluation/MinReturn                        -93.432
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5014
Extras/EpisodeRewardMean                    -45.8743
LinearFeatureBaseline/ExplainedVariance     -10.4902
PolicyExecTime                                0.100514
ProcessExecTime                               0.0109825
TotalEnvSteps                            200376
policy/Entropy                                0.608524
policy/KL                                     0.00663576
policy/KLBefore                               0
policy/LossAfter                             -0.0187492
policy/LossBefore                             1.88473e-08
policy/Perplexity                             1.83772
policy/dLoss                                  0.0187492
---------------------------------------  ----------------
2022-04-23 14:20:24 | [train_policy] epoch #198 | Obtaining samples for iteration 198...
2022-04-23 14:20:25 | [train_policy] epoch #198 | Logging diagnostics...
2022-04-23 14:20:25 | [train_policy] epoch #198 | Optimizing policy...
2022-04-23 14:20:25 | [train_policy] epoch #198 | Computing loss before
2022-04-23 14:20:25 | [train_policy] epoch #198 | Computing KL before
2022-04-23 14:20:25 | [train_policy] epoch #198 | Optimizing
2022-04-23 14:20:25 | [train_policy] epoch #198 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:25 | [train_policy] epoch #198 | computing loss before
2022-04-23 14:20:25 | [train_policy] epoch #198 | computing gradient
2022-04-23 14:20:25 | [train_policy] epoch #198 | gradient computed
2022-04-23 14:20:25 | [train_policy] epoch #198 | computing descent direction
2022-04-23 14:20:25 | [train_policy] epoch #198 | descent direction computed
2022-04-23 14:20:25 | [train_policy] epoch #198 | backtrack iters: 0
2022-04-23 14:20:25 | [train_policy] epoch #198 | optimization finished
2022-04-23 14:20:25 | [train_policy] epoch #198 | Computing KL after
2022-04-23 14:20:25 | [train_policy] epoch #198 | Computing loss after
2022-04-23 14:20:25 | [train_policy] epoch #198 | Fitting baseline...
2022-04-23 14:20:25 | [train_policy] epoch #198 | Saving snapshot...
2022-04-23 14:20:25 | [train_policy] epoch #198 | Saved
2022-04-23 14:20:25 | [train_policy] epoch #198 | Time 72.89 s
2022-04-23 14:20:25 | [train_policy] epoch #198 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116922
Evaluation/AverageDiscountedReturn          -45.6478
Evaluation/AverageReturn                    -45.6478
Evaluation/CompletionRate                     0
Evaluation/Iteration                        198
Evaluation/MaxReturn                        -33.7244
Evaluation/MinReturn                        -71.4124
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.16906
Extras/EpisodeRewardMean                    -45.6025
LinearFeatureBaseline/ExplainedVariance       0.926343
PolicyExecTime                                0.10082
ProcessExecTime                               0.0110464
TotalEnvSteps                            201388
policy/Entropy                                0.591623
policy/KL                                     0.00964205
policy/KLBefore                               0
policy/LossAfter                             -0.0185239
policy/LossBefore                             6.12538e-09
policy/Perplexity                             1.80692
policy/dLoss                                  0.0185239
---------------------------------------  ----------------
2022-04-23 14:20:25 | [train_policy] epoch #199 | Obtaining samples for iteration 199...
2022-04-23 14:20:25 | [train_policy] epoch #199 | Logging diagnostics...
2022-04-23 14:20:25 | [train_policy] epoch #199 | Optimizing policy...
2022-04-23 14:20:25 | [train_policy] epoch #199 | Computing loss before
2022-04-23 14:20:25 | [train_policy] epoch #199 | Computing KL before
2022-04-23 14:20:25 | [train_policy] epoch #199 | Optimizing
2022-04-23 14:20:25 | [train_policy] epoch #199 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:25 | [train_policy] epoch #199 | computing loss before
2022-04-23 14:20:25 | [train_policy] epoch #199 | computing gradient
2022-04-23 14:20:25 | [train_policy] epoch #199 | gradient computed
2022-04-23 14:20:25 | [train_policy] epoch #199 | computing descent direction
2022-04-23 14:20:25 | [train_policy] epoch #199 | descent direction computed
2022-04-23 14:20:25 | [train_policy] epoch #199 | backtrack iters: 1
2022-04-23 14:20:25 | [train_policy] epoch #199 | optimization finished
2022-04-23 14:20:25 | [train_policy] epoch #199 | Computing KL after
2022-04-23 14:20:25 | [train_policy] epoch #199 | Computing loss after
2022-04-23 14:20:25 | [train_policy] epoch #199 | Fitting baseline...
2022-04-23 14:20:25 | [train_policy] epoch #199 | Saving snapshot...
2022-04-23 14:20:25 | [train_policy] epoch #199 | Saved
2022-04-23 14:20:25 | [train_policy] epoch #199 | Time 73.25 s
2022-04-23 14:20:25 | [train_policy] epoch #199 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117719
Evaluation/AverageDiscountedReturn          -66.7766
Evaluation/AverageReturn                    -66.7766
Evaluation/CompletionRate                     0
Evaluation/Iteration                        199
Evaluation/MaxReturn                        -33.2489
Evaluation/MinReturn                      -2062.75
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.326
Extras/EpisodeRewardMean                    -65.0434
LinearFeatureBaseline/ExplainedVariance       0.0119727
PolicyExecTime                                0.107831
ProcessExecTime                               0.0117731
TotalEnvSteps                            202400
policy/Entropy                                0.596474
policy/KL                                     0.00643503
policy/KLBefore                               0
policy/LossAfter                             -0.0108071
policy/LossBefore                             7.06774e-09
policy/Perplexity                             1.8157
policy/dLoss                                  0.0108071
---------------------------------------  ----------------
2022-04-23 14:20:25 | [train_policy] epoch #200 | Obtaining samples for iteration 200...
2022-04-23 14:20:25 | [train_policy] epoch #200 | Logging diagnostics...
2022-04-23 14:20:25 | [train_policy] epoch #200 | Optimizing policy...
2022-04-23 14:20:25 | [train_policy] epoch #200 | Computing loss before
2022-04-23 14:20:25 | [train_policy] epoch #200 | Computing KL before
2022-04-23 14:20:25 | [train_policy] epoch #200 | Optimizing
2022-04-23 14:20:25 | [train_policy] epoch #200 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:25 | [train_policy] epoch #200 | computing loss before
2022-04-23 14:20:25 | [train_policy] epoch #200 | computing gradient
2022-04-23 14:20:25 | [train_policy] epoch #200 | gradient computed
2022-04-23 14:20:25 | [train_policy] epoch #200 | computing descent direction
2022-04-23 14:20:25 | [train_policy] epoch #200 | descent direction computed
2022-04-23 14:20:25 | [train_policy] epoch #200 | backtrack iters: 1
2022-04-23 14:20:25 | [train_policy] epoch #200 | optimization finished
2022-04-23 14:20:25 | [train_policy] epoch #200 | Computing KL after
2022-04-23 14:20:25 | [train_policy] epoch #200 | Computing loss after
2022-04-23 14:20:25 | [train_policy] epoch #200 | Fitting baseline...
2022-04-23 14:20:25 | [train_policy] epoch #200 | Saving snapshot...
2022-04-23 14:20:25 | [train_policy] epoch #200 | Saved
2022-04-23 14:20:25 | [train_policy] epoch #200 | Time 73.59 s
2022-04-23 14:20:25 | [train_policy] epoch #200 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.118304
Evaluation/AverageDiscountedReturn         -111.152
Evaluation/AverageReturn                   -111.152
Evaluation/CompletionRate                     0
Evaluation/Iteration                        200
Evaluation/MaxReturn                        -33.7276
Evaluation/MinReturn                      -2072.32
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.585
Extras/EpisodeRewardMean                   -105.813
LinearFeatureBaseline/ExplainedVariance       0.226583
PolicyExecTime                                0.104537
ProcessExecTime                               0.0115867
TotalEnvSteps                            203412
policy/Entropy                                0.603894
policy/KL                                     0.00705123
policy/KLBefore                               0
policy/LossAfter                             -0.0214631
policy/LossBefore                             5.4186e-09
policy/Perplexity                             1.82923
policy/dLoss                                  0.0214631
---------------------------------------  ---------------
2022-04-23 14:20:25 | [train_policy] epoch #201 | Obtaining samples for iteration 201...
2022-04-23 14:20:26 | [train_policy] epoch #201 | Logging diagnostics...
2022-04-23 14:20:26 | [train_policy] epoch #201 | Optimizing policy...
2022-04-23 14:20:26 | [train_policy] epoch #201 | Computing loss before
2022-04-23 14:20:26 | [train_policy] epoch #201 | Computing KL before
2022-04-23 14:20:26 | [train_policy] epoch #201 | Optimizing
2022-04-23 14:20:26 | [train_policy] epoch #201 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:26 | [train_policy] epoch #201 | computing loss before
2022-04-23 14:20:26 | [train_policy] epoch #201 | computing gradient
2022-04-23 14:20:26 | [train_policy] epoch #201 | gradient computed
2022-04-23 14:20:26 | [train_policy] epoch #201 | computing descent direction
2022-04-23 14:20:26 | [train_policy] epoch #201 | descent direction computed
2022-04-23 14:20:26 | [train_policy] epoch #201 | backtrack iters: 0
2022-04-23 14:20:26 | [train_policy] epoch #201 | optimization finished
2022-04-23 14:20:26 | [train_policy] epoch #201 | Computing KL after
2022-04-23 14:20:26 | [train_policy] epoch #201 | Computing loss after
2022-04-23 14:20:26 | [train_policy] epoch #201 | Fitting baseline...
2022-04-23 14:20:26 | [train_policy] epoch #201 | Saving snapshot...
2022-04-23 14:20:26 | [train_policy] epoch #201 | Saved
2022-04-23 14:20:26 | [train_policy] epoch #201 | Time 73.95 s
2022-04-23 14:20:26 | [train_policy] epoch #201 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118802
Evaluation/AverageDiscountedReturn          -90.4209
Evaluation/AverageReturn                    -90.4209
Evaluation/CompletionRate                     0
Evaluation/Iteration                        201
Evaluation/MaxReturn                        -33.9535
Evaluation/MinReturn                      -2071.47
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.033
Extras/EpisodeRewardMean                   -106.858
LinearFeatureBaseline/ExplainedVariance       0.0512323
PolicyExecTime                                0.104331
ProcessExecTime                               0.0111816
TotalEnvSteps                            204424
policy/Entropy                                0.601784
policy/KL                                     0.00909901
policy/KLBefore                               0
policy/LossAfter                             -0.0248053
policy/LossBefore                             4.47624e-09
policy/Perplexity                             1.82537
policy/dLoss                                  0.0248053
---------------------------------------  ----------------
2022-04-23 14:20:26 | [train_policy] epoch #202 | Obtaining samples for iteration 202...
2022-04-23 14:20:26 | [train_policy] epoch #202 | Logging diagnostics...
2022-04-23 14:20:26 | [train_policy] epoch #202 | Optimizing policy...
2022-04-23 14:20:26 | [train_policy] epoch #202 | Computing loss before
2022-04-23 14:20:26 | [train_policy] epoch #202 | Computing KL before
2022-04-23 14:20:26 | [train_policy] epoch #202 | Optimizing
2022-04-23 14:20:26 | [train_policy] epoch #202 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:26 | [train_policy] epoch #202 | computing loss before
2022-04-23 14:20:26 | [train_policy] epoch #202 | computing gradient
2022-04-23 14:20:26 | [train_policy] epoch #202 | gradient computed
2022-04-23 14:20:26 | [train_policy] epoch #202 | computing descent direction
2022-04-23 14:20:26 | [train_policy] epoch #202 | descent direction computed
2022-04-23 14:20:26 | [train_policy] epoch #202 | backtrack iters: 1
2022-04-23 14:20:26 | [train_policy] epoch #202 | optimization finished
2022-04-23 14:20:26 | [train_policy] epoch #202 | Computing KL after
2022-04-23 14:20:26 | [train_policy] epoch #202 | Computing loss after
2022-04-23 14:20:26 | [train_policy] epoch #202 | Fitting baseline...
2022-04-23 14:20:26 | [train_policy] epoch #202 | Saving snapshot...
2022-04-23 14:20:26 | [train_policy] epoch #202 | Saved
2022-04-23 14:20:26 | [train_policy] epoch #202 | Time 74.30 s
2022-04-23 14:20:26 | [train_policy] epoch #202 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117856
Evaluation/AverageDiscountedReturn          -45.3758
Evaluation/AverageReturn                    -45.3758
Evaluation/CompletionRate                     0
Evaluation/Iteration                        202
Evaluation/MaxReturn                        -32.1151
Evaluation/MinReturn                        -65.043
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.50037
Extras/EpisodeRewardMean                    -45.0333
LinearFeatureBaseline/ExplainedVariance     -39.3998
PolicyExecTime                                0.106302
ProcessExecTime                               0.0116196
TotalEnvSteps                            205436
policy/Entropy                                0.591737
policy/KL                                     0.00681891
policy/KLBefore                               0
policy/LossAfter                             -0.0180332
policy/LossBefore                             3.12748e-08
policy/Perplexity                             1.80713
policy/dLoss                                  0.0180332
---------------------------------------  ----------------
2022-04-23 14:20:26 | [train_policy] epoch #203 | Obtaining samples for iteration 203...
2022-04-23 14:20:26 | [train_policy] epoch #203 | Logging diagnostics...
2022-04-23 14:20:26 | [train_policy] epoch #203 | Optimizing policy...
2022-04-23 14:20:26 | [train_policy] epoch #203 | Computing loss before
2022-04-23 14:20:26 | [train_policy] epoch #203 | Computing KL before
2022-04-23 14:20:26 | [train_policy] epoch #203 | Optimizing
2022-04-23 14:20:26 | [train_policy] epoch #203 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:26 | [train_policy] epoch #203 | computing loss before
2022-04-23 14:20:26 | [train_policy] epoch #203 | computing gradient
2022-04-23 14:20:26 | [train_policy] epoch #203 | gradient computed
2022-04-23 14:20:26 | [train_policy] epoch #203 | computing descent direction
2022-04-23 14:20:26 | [train_policy] epoch #203 | descent direction computed
2022-04-23 14:20:26 | [train_policy] epoch #203 | backtrack iters: 1
2022-04-23 14:20:26 | [train_policy] epoch #203 | optimization finished
2022-04-23 14:20:26 | [train_policy] epoch #203 | Computing KL after
2022-04-23 14:20:26 | [train_policy] epoch #203 | Computing loss after
2022-04-23 14:20:26 | [train_policy] epoch #203 | Fitting baseline...
2022-04-23 14:20:26 | [train_policy] epoch #203 | Saving snapshot...
2022-04-23 14:20:26 | [train_policy] epoch #203 | Saved
2022-04-23 14:20:26 | [train_policy] epoch #203 | Time 74.65 s
2022-04-23 14:20:26 | [train_policy] epoch #203 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118891
Evaluation/AverageDiscountedReturn          -68.9837
Evaluation/AverageReturn                    -68.9837
Evaluation/CompletionRate                     0
Evaluation/Iteration                        203
Evaluation/MaxReturn                        -35.2713
Evaluation/MinReturn                      -2086.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.745
Extras/EpisodeRewardMean                    -67.0164
LinearFeatureBaseline/ExplainedVariance       0.0109721
PolicyExecTime                                0.104479
ProcessExecTime                               0.0115082
TotalEnvSteps                            206448
policy/Entropy                                0.575573
policy/KL                                     0.0074502
policy/KLBefore                               0
policy/LossAfter                             -0.0266112
policy/LossBefore                             7.53893e-09
policy/Perplexity                             1.77815
policy/dLoss                                  0.0266112
---------------------------------------  ----------------
2022-04-23 14:20:26 | [train_policy] epoch #204 | Obtaining samples for iteration 204...
2022-04-23 14:20:27 | [train_policy] epoch #204 | Logging diagnostics...
2022-04-23 14:20:27 | [train_policy] epoch #204 | Optimizing policy...
2022-04-23 14:20:27 | [train_policy] epoch #204 | Computing loss before
2022-04-23 14:20:27 | [train_policy] epoch #204 | Computing KL before
2022-04-23 14:20:27 | [train_policy] epoch #204 | Optimizing
2022-04-23 14:20:27 | [train_policy] epoch #204 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:27 | [train_policy] epoch #204 | computing loss before
2022-04-23 14:20:27 | [train_policy] epoch #204 | computing gradient
2022-04-23 14:20:27 | [train_policy] epoch #204 | gradient computed
2022-04-23 14:20:27 | [train_policy] epoch #204 | computing descent direction
2022-04-23 14:20:27 | [train_policy] epoch #204 | descent direction computed
2022-04-23 14:20:27 | [train_policy] epoch #204 | backtrack iters: 1
2022-04-23 14:20:27 | [train_policy] epoch #204 | optimization finished
2022-04-23 14:20:27 | [train_policy] epoch #204 | Computing KL after
2022-04-23 14:20:27 | [train_policy] epoch #204 | Computing loss after
2022-04-23 14:20:27 | [train_policy] epoch #204 | Fitting baseline...
2022-04-23 14:20:27 | [train_policy] epoch #204 | Saving snapshot...
2022-04-23 14:20:27 | [train_policy] epoch #204 | Saved
2022-04-23 14:20:27 | [train_policy] epoch #204 | Time 75.00 s
2022-04-23 14:20:27 | [train_policy] epoch #204 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116708
Evaluation/AverageDiscountedReturn          -89.9514
Evaluation/AverageReturn                    -89.9514
Evaluation/CompletionRate                     0
Evaluation/Iteration                        204
Evaluation/MaxReturn                        -33.9242
Evaluation/MinReturn                      -2064.86
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        293.623
Extras/EpisodeRewardMean                    -86.4685
LinearFeatureBaseline/ExplainedVariance       0.151458
PolicyExecTime                                0.108306
ProcessExecTime                               0.0113678
TotalEnvSteps                            207460
policy/Entropy                                0.568378
policy/KL                                     0.00706958
policy/KLBefore                               0
policy/LossAfter                             -0.0218979
policy/LossBefore                             1.31931e-08
policy/Perplexity                             1.7654
policy/dLoss                                  0.0218979
---------------------------------------  ----------------
2022-04-23 14:20:27 | [train_policy] epoch #205 | Obtaining samples for iteration 205...
2022-04-23 14:20:27 | [train_policy] epoch #205 | Logging diagnostics...
2022-04-23 14:20:27 | [train_policy] epoch #205 | Optimizing policy...
2022-04-23 14:20:27 | [train_policy] epoch #205 | Computing loss before
2022-04-23 14:20:27 | [train_policy] epoch #205 | Computing KL before
2022-04-23 14:20:27 | [train_policy] epoch #205 | Optimizing
2022-04-23 14:20:27 | [train_policy] epoch #205 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:27 | [train_policy] epoch #205 | computing loss before
2022-04-23 14:20:27 | [train_policy] epoch #205 | computing gradient
2022-04-23 14:20:27 | [train_policy] epoch #205 | gradient computed
2022-04-23 14:20:27 | [train_policy] epoch #205 | computing descent direction
2022-04-23 14:20:27 | [train_policy] epoch #205 | descent direction computed
2022-04-23 14:20:27 | [train_policy] epoch #205 | backtrack iters: 1
2022-04-23 14:20:27 | [train_policy] epoch #205 | optimization finished
2022-04-23 14:20:27 | [train_policy] epoch #205 | Computing KL after
2022-04-23 14:20:27 | [train_policy] epoch #205 | Computing loss after
2022-04-23 14:20:27 | [train_policy] epoch #205 | Fitting baseline...
2022-04-23 14:20:27 | [train_policy] epoch #205 | Saving snapshot...
2022-04-23 14:20:27 | [train_policy] epoch #205 | Saved
2022-04-23 14:20:27 | [train_policy] epoch #205 | Time 75.35 s
2022-04-23 14:20:27 | [train_policy] epoch #205 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.115422
Evaluation/AverageDiscountedReturn          -47.2041
Evaluation/AverageReturn                    -47.2041
Evaluation/CompletionRate                     0
Evaluation/Iteration                        205
Evaluation/MaxReturn                        -32.5791
Evaluation/MinReturn                       -130.33
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.773
Extras/EpisodeRewardMean                    -47.1337
LinearFeatureBaseline/ExplainedVariance     -54.8025
PolicyExecTime                                0.109292
ProcessExecTime                               0.0112736
TotalEnvSteps                            208472
policy/Entropy                                0.545939
policy/KL                                     0.00706501
policy/KLBefore                               0
policy/LossAfter                             -0.0141361
policy/LossBefore                             1.69626e-08
policy/Perplexity                             1.72623
policy/dLoss                                  0.0141361
---------------------------------------  ----------------
2022-04-23 14:20:27 | [train_policy] epoch #206 | Obtaining samples for iteration 206...
2022-04-23 14:20:27 | [train_policy] epoch #206 | Logging diagnostics...
2022-04-23 14:20:27 | [train_policy] epoch #206 | Optimizing policy...
2022-04-23 14:20:27 | [train_policy] epoch #206 | Computing loss before
2022-04-23 14:20:27 | [train_policy] epoch #206 | Computing KL before
2022-04-23 14:20:27 | [train_policy] epoch #206 | Optimizing
2022-04-23 14:20:27 | [train_policy] epoch #206 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:27 | [train_policy] epoch #206 | computing loss before
2022-04-23 14:20:27 | [train_policy] epoch #206 | computing gradient
2022-04-23 14:20:27 | [train_policy] epoch #206 | gradient computed
2022-04-23 14:20:27 | [train_policy] epoch #206 | computing descent direction
2022-04-23 14:20:27 | [train_policy] epoch #206 | descent direction computed
2022-04-23 14:20:27 | [train_policy] epoch #206 | backtrack iters: 0
2022-04-23 14:20:27 | [train_policy] epoch #206 | optimization finished
2022-04-23 14:20:27 | [train_policy] epoch #206 | Computing KL after
2022-04-23 14:20:27 | [train_policy] epoch #206 | Computing loss after
2022-04-23 14:20:27 | [train_policy] epoch #206 | Fitting baseline...
2022-04-23 14:20:27 | [train_policy] epoch #206 | Saving snapshot...
2022-04-23 14:20:27 | [train_policy] epoch #206 | Saved
2022-04-23 14:20:27 | [train_policy] epoch #206 | Time 75.70 s
2022-04-23 14:20:27 | [train_policy] epoch #206 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116229
Evaluation/AverageDiscountedReturn          -67.6348
Evaluation/AverageReturn                    -67.6348
Evaluation/CompletionRate                     0
Evaluation/Iteration                        206
Evaluation/MaxReturn                        -34.8288
Evaluation/MinReturn                      -2062.16
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.2
Extras/EpisodeRewardMean                    -66.0233
LinearFeatureBaseline/ExplainedVariance       0.0180077
PolicyExecTime                                0.105303
ProcessExecTime                               0.0112236
TotalEnvSteps                            209484
policy/Entropy                                0.542254
policy/KL                                     0.0095271
policy/KLBefore                               0
policy/LossAfter                             -0.0264919
policy/LossBefore                             9.42366e-09
policy/Perplexity                             1.71988
policy/dLoss                                  0.0264919
---------------------------------------  ----------------
2022-04-23 14:20:27 | [train_policy] epoch #207 | Obtaining samples for iteration 207...
2022-04-23 14:20:28 | [train_policy] epoch #207 | Logging diagnostics...
2022-04-23 14:20:28 | [train_policy] epoch #207 | Optimizing policy...
2022-04-23 14:20:28 | [train_policy] epoch #207 | Computing loss before
2022-04-23 14:20:28 | [train_policy] epoch #207 | Computing KL before
2022-04-23 14:20:28 | [train_policy] epoch #207 | Optimizing
2022-04-23 14:20:28 | [train_policy] epoch #207 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:28 | [train_policy] epoch #207 | computing loss before
2022-04-23 14:20:28 | [train_policy] epoch #207 | computing gradient
2022-04-23 14:20:28 | [train_policy] epoch #207 | gradient computed
2022-04-23 14:20:28 | [train_policy] epoch #207 | computing descent direction
2022-04-23 14:20:28 | [train_policy] epoch #207 | descent direction computed
2022-04-23 14:20:28 | [train_policy] epoch #207 | backtrack iters: 1
2022-04-23 14:20:28 | [train_policy] epoch #207 | optimization finished
2022-04-23 14:20:28 | [train_policy] epoch #207 | Computing KL after
2022-04-23 14:20:28 | [train_policy] epoch #207 | Computing loss after
2022-04-23 14:20:28 | [train_policy] epoch #207 | Fitting baseline...
2022-04-23 14:20:28 | [train_policy] epoch #207 | Saving snapshot...
2022-04-23 14:20:28 | [train_policy] epoch #207 | Saved
2022-04-23 14:20:28 | [train_policy] epoch #207 | Time 76.03 s
2022-04-23 14:20:28 | [train_policy] epoch #207 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115382
Evaluation/AverageDiscountedReturn          -68.4647
Evaluation/AverageReturn                    -68.4647
Evaluation/CompletionRate                     0
Evaluation/Iteration                        207
Evaluation/MaxReturn                        -35.9018
Evaluation/MinReturn                      -2060.04
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.962
Extras/EpisodeRewardMean                    -67.4443
LinearFeatureBaseline/ExplainedVariance       0.0899131
PolicyExecTime                                0.0932906
ProcessExecTime                               0.0108273
TotalEnvSteps                            210496
policy/Entropy                                0.551889
policy/KL                                     0.00727084
policy/KLBefore                               0
policy/LossAfter                             -0.0309735
policy/LossBefore                             1.41355e-09
policy/Perplexity                             1.73653
policy/dLoss                                  0.0309735
---------------------------------------  ----------------
2022-04-23 14:20:28 | [train_policy] epoch #208 | Obtaining samples for iteration 208...
2022-04-23 14:20:28 | [train_policy] epoch #208 | Logging diagnostics...
2022-04-23 14:20:28 | [train_policy] epoch #208 | Optimizing policy...
2022-04-23 14:20:28 | [train_policy] epoch #208 | Computing loss before
2022-04-23 14:20:28 | [train_policy] epoch #208 | Computing KL before
2022-04-23 14:20:28 | [train_policy] epoch #208 | Optimizing
2022-04-23 14:20:28 | [train_policy] epoch #208 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:28 | [train_policy] epoch #208 | computing loss before
2022-04-23 14:20:28 | [train_policy] epoch #208 | computing gradient
2022-04-23 14:20:28 | [train_policy] epoch #208 | gradient computed
2022-04-23 14:20:28 | [train_policy] epoch #208 | computing descent direction
2022-04-23 14:20:28 | [train_policy] epoch #208 | descent direction computed
2022-04-23 14:20:28 | [train_policy] epoch #208 | backtrack iters: 1
2022-04-23 14:20:28 | [train_policy] epoch #208 | optimization finished
2022-04-23 14:20:28 | [train_policy] epoch #208 | Computing KL after
2022-04-23 14:20:28 | [train_policy] epoch #208 | Computing loss after
2022-04-23 14:20:28 | [train_policy] epoch #208 | Fitting baseline...
2022-04-23 14:20:28 | [train_policy] epoch #208 | Saving snapshot...
2022-04-23 14:20:28 | [train_policy] epoch #208 | Saved
2022-04-23 14:20:28 | [train_policy] epoch #208 | Time 76.39 s
2022-04-23 14:20:28 | [train_policy] epoch #208 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119236
Evaluation/AverageDiscountedReturn          -69.1427
Evaluation/AverageReturn                    -69.1427
Evaluation/CompletionRate                     0
Evaluation/Iteration                        208
Evaluation/MaxReturn                        -33.4454
Evaluation/MinReturn                      -2055.05
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.302
Extras/EpisodeRewardMean                    -67.0504
LinearFeatureBaseline/ExplainedVariance      -0.0351694
PolicyExecTime                                0.105036
ProcessExecTime                               0.0115323
TotalEnvSteps                            211508
policy/Entropy                                0.538533
policy/KL                                     0.00709147
policy/KLBefore                               0
policy/LossAfter                             -0.0215033
policy/LossBefore                            -1.01304e-08
policy/Perplexity                             1.71349
policy/dLoss                                  0.0215033
---------------------------------------  ----------------
2022-04-23 14:20:28 | [train_policy] epoch #209 | Obtaining samples for iteration 209...
2022-04-23 14:20:28 | [train_policy] epoch #209 | Logging diagnostics...
2022-04-23 14:20:28 | [train_policy] epoch #209 | Optimizing policy...
2022-04-23 14:20:28 | [train_policy] epoch #209 | Computing loss before
2022-04-23 14:20:28 | [train_policy] epoch #209 | Computing KL before
2022-04-23 14:20:28 | [train_policy] epoch #209 | Optimizing
2022-04-23 14:20:28 | [train_policy] epoch #209 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:28 | [train_policy] epoch #209 | computing loss before
2022-04-23 14:20:28 | [train_policy] epoch #209 | computing gradient
2022-04-23 14:20:28 | [train_policy] epoch #209 | gradient computed
2022-04-23 14:20:28 | [train_policy] epoch #209 | computing descent direction
2022-04-23 14:20:29 | [train_policy] epoch #209 | descent direction computed
2022-04-23 14:20:29 | [train_policy] epoch #209 | backtrack iters: 1
2022-04-23 14:20:29 | [train_policy] epoch #209 | optimization finished
2022-04-23 14:20:29 | [train_policy] epoch #209 | Computing KL after
2022-04-23 14:20:29 | [train_policy] epoch #209 | Computing loss after
2022-04-23 14:20:29 | [train_policy] epoch #209 | Fitting baseline...
2022-04-23 14:20:29 | [train_policy] epoch #209 | Saving snapshot...
2022-04-23 14:20:29 | [train_policy] epoch #209 | Saved
2022-04-23 14:20:29 | [train_policy] epoch #209 | Time 76.74 s
2022-04-23 14:20:29 | [train_policy] epoch #209 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117318
Evaluation/AverageDiscountedReturn          -69.8725
Evaluation/AverageReturn                    -69.8725
Evaluation/CompletionRate                     0
Evaluation/Iteration                        209
Evaluation/MaxReturn                        -33.9361
Evaluation/MinReturn                      -2063.59
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.313
Extras/EpisodeRewardMean                    -68.1806
LinearFeatureBaseline/ExplainedVariance       0.0706157
PolicyExecTime                                0.101932
ProcessExecTime                               0.0112598
TotalEnvSteps                            212520
policy/Entropy                                0.534301
policy/KL                                     0.00696179
policy/KLBefore                               0
policy/LossAfter                             -0.0428432
policy/LossBefore                            -2.35591e-10
policy/Perplexity                             1.70625
policy/dLoss                                  0.0428432
---------------------------------------  ----------------
2022-04-23 14:20:29 | [train_policy] epoch #210 | Obtaining samples for iteration 210...
2022-04-23 14:20:29 | [train_policy] epoch #210 | Logging diagnostics...
2022-04-23 14:20:29 | [train_policy] epoch #210 | Optimizing policy...
2022-04-23 14:20:29 | [train_policy] epoch #210 | Computing loss before
2022-04-23 14:20:29 | [train_policy] epoch #210 | Computing KL before
2022-04-23 14:20:29 | [train_policy] epoch #210 | Optimizing
2022-04-23 14:20:29 | [train_policy] epoch #210 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:29 | [train_policy] epoch #210 | computing loss before
2022-04-23 14:20:29 | [train_policy] epoch #210 | computing gradient
2022-04-23 14:20:29 | [train_policy] epoch #210 | gradient computed
2022-04-23 14:20:29 | [train_policy] epoch #210 | computing descent direction
2022-04-23 14:20:29 | [train_policy] epoch #210 | descent direction computed
2022-04-23 14:20:29 | [train_policy] epoch #210 | backtrack iters: 0
2022-04-23 14:20:29 | [train_policy] epoch #210 | optimization finished
2022-04-23 14:20:29 | [train_policy] epoch #210 | Computing KL after
2022-04-23 14:20:29 | [train_policy] epoch #210 | Computing loss after
2022-04-23 14:20:29 | [train_policy] epoch #210 | Fitting baseline...
2022-04-23 14:20:29 | [train_policy] epoch #210 | Saving snapshot...
2022-04-23 14:20:29 | [train_policy] epoch #210 | Saved
2022-04-23 14:20:29 | [train_policy] epoch #210 | Time 77.09 s
2022-04-23 14:20:29 | [train_policy] epoch #210 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11679
Evaluation/AverageDiscountedReturn          -45.9625
Evaluation/AverageReturn                    -45.9625
Evaluation/CompletionRate                     0
Evaluation/Iteration                        210
Evaluation/MaxReturn                        -35.6907
Evaluation/MinReturn                        -91.1171
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.26423
Extras/EpisodeRewardMean                    -45.9891
LinearFeatureBaseline/ExplainedVariance     -17.0437
PolicyExecTime                                0.104926
ProcessExecTime                               0.0120389
TotalEnvSteps                            213532
policy/Entropy                                0.537239
policy/KL                                     0.00930782
policy/KLBefore                               0
policy/LossAfter                             -0.033509
policy/LossBefore                             2.35591e-08
policy/Perplexity                             1.71128
policy/dLoss                                  0.033509
---------------------------------------  ----------------
2022-04-23 14:20:29 | [train_policy] epoch #211 | Obtaining samples for iteration 211...
2022-04-23 14:20:29 | [train_policy] epoch #211 | Logging diagnostics...
2022-04-23 14:20:29 | [train_policy] epoch #211 | Optimizing policy...
2022-04-23 14:20:29 | [train_policy] epoch #211 | Computing loss before
2022-04-23 14:20:29 | [train_policy] epoch #211 | Computing KL before
2022-04-23 14:20:29 | [train_policy] epoch #211 | Optimizing
2022-04-23 14:20:29 | [train_policy] epoch #211 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:29 | [train_policy] epoch #211 | computing loss before
2022-04-23 14:20:29 | [train_policy] epoch #211 | computing gradient
2022-04-23 14:20:29 | [train_policy] epoch #211 | gradient computed
2022-04-23 14:20:29 | [train_policy] epoch #211 | computing descent direction
2022-04-23 14:20:29 | [train_policy] epoch #211 | descent direction computed
2022-04-23 14:20:29 | [train_policy] epoch #211 | backtrack iters: 1
2022-04-23 14:20:29 | [train_policy] epoch #211 | optimization finished
2022-04-23 14:20:29 | [train_policy] epoch #211 | Computing KL after
2022-04-23 14:20:29 | [train_policy] epoch #211 | Computing loss after
2022-04-23 14:20:29 | [train_policy] epoch #211 | Fitting baseline...
2022-04-23 14:20:29 | [train_policy] epoch #211 | Saving snapshot...
2022-04-23 14:20:29 | [train_policy] epoch #211 | Saved
2022-04-23 14:20:29 | [train_policy] epoch #211 | Time 77.44 s
2022-04-23 14:20:29 | [train_policy] epoch #211 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.115654
Evaluation/AverageDiscountedReturn          -90.8455
Evaluation/AverageReturn                    -90.8455
Evaluation/CompletionRate                     0
Evaluation/Iteration                        211
Evaluation/MaxReturn                        -36.4985
Evaluation/MinReturn                      -2062.72
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        293.981
Extras/EpisodeRewardMean                    -87.0498
LinearFeatureBaseline/ExplainedVariance       0.0112092
PolicyExecTime                                0.107376
ProcessExecTime                               0.0117557
TotalEnvSteps                            214544
policy/Entropy                                0.536053
policy/KL                                     0.00675803
policy/KLBefore                               0
policy/LossAfter                             -0.0201837
policy/LossBefore                             4.94742e-09
policy/Perplexity                             1.70925
policy/dLoss                                  0.0201837
---------------------------------------  ----------------
2022-04-23 14:20:29 | [train_policy] epoch #212 | Obtaining samples for iteration 212...
2022-04-23 14:20:29 | [train_policy] epoch #212 | Logging diagnostics...
2022-04-23 14:20:29 | [train_policy] epoch #212 | Optimizing policy...
2022-04-23 14:20:29 | [train_policy] epoch #212 | Computing loss before
2022-04-23 14:20:29 | [train_policy] epoch #212 | Computing KL before
2022-04-23 14:20:29 | [train_policy] epoch #212 | Optimizing
2022-04-23 14:20:29 | [train_policy] epoch #212 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:29 | [train_policy] epoch #212 | computing loss before
2022-04-23 14:20:29 | [train_policy] epoch #212 | computing gradient
2022-04-23 14:20:30 | [train_policy] epoch #212 | gradient computed
2022-04-23 14:20:30 | [train_policy] epoch #212 | computing descent direction
2022-04-23 14:20:30 | [train_policy] epoch #212 | descent direction computed
2022-04-23 14:20:30 | [train_policy] epoch #212 | backtrack iters: 1
2022-04-23 14:20:30 | [train_policy] epoch #212 | optimization finished
2022-04-23 14:20:30 | [train_policy] epoch #212 | Computing KL after
2022-04-23 14:20:30 | [train_policy] epoch #212 | Computing loss after
2022-04-23 14:20:30 | [train_policy] epoch #212 | Fitting baseline...
2022-04-23 14:20:30 | [train_policy] epoch #212 | Saving snapshot...
2022-04-23 14:20:30 | [train_policy] epoch #212 | Saved
2022-04-23 14:20:30 | [train_policy] epoch #212 | Time 77.80 s
2022-04-23 14:20:30 | [train_policy] epoch #212 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.116492
Evaluation/AverageDiscountedReturn          -46.4644
Evaluation/AverageReturn                    -46.4644
Evaluation/CompletionRate                     0
Evaluation/Iteration                        212
Evaluation/MaxReturn                        -35.6623
Evaluation/MinReturn                        -64.7582
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.87511
Extras/EpisodeRewardMean                    -66.8761
LinearFeatureBaseline/ExplainedVariance     -45.0017
PolicyExecTime                                0.107932
ProcessExecTime                               0.0112786
TotalEnvSteps                            215556
policy/Entropy                                0.53114
policy/KL                                     0.00641314
policy/KLBefore                               0
policy/LossAfter                             -0.0254224
policy/LossBefore                             4.00506e-08
policy/Perplexity                             1.70087
policy/dLoss                                  0.0254224
---------------------------------------  ----------------
2022-04-23 14:20:30 | [train_policy] epoch #213 | Obtaining samples for iteration 213...
2022-04-23 14:20:30 | [train_policy] epoch #213 | Logging diagnostics...
2022-04-23 14:20:30 | [train_policy] epoch #213 | Optimizing policy...
2022-04-23 14:20:30 | [train_policy] epoch #213 | Computing loss before
2022-04-23 14:20:30 | [train_policy] epoch #213 | Computing KL before
2022-04-23 14:20:30 | [train_policy] epoch #213 | Optimizing
2022-04-23 14:20:30 | [train_policy] epoch #213 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:30 | [train_policy] epoch #213 | computing loss before
2022-04-23 14:20:30 | [train_policy] epoch #213 | computing gradient
2022-04-23 14:20:30 | [train_policy] epoch #213 | gradient computed
2022-04-23 14:20:30 | [train_policy] epoch #213 | computing descent direction
2022-04-23 14:20:30 | [train_policy] epoch #213 | descent direction computed
2022-04-23 14:20:30 | [train_policy] epoch #213 | backtrack iters: 1
2022-04-23 14:20:30 | [train_policy] epoch #213 | optimization finished
2022-04-23 14:20:30 | [train_policy] epoch #213 | Computing KL after
2022-04-23 14:20:30 | [train_policy] epoch #213 | Computing loss after
2022-04-23 14:20:30 | [train_policy] epoch #213 | Fitting baseline...
2022-04-23 14:20:30 | [train_policy] epoch #213 | Saving snapshot...
2022-04-23 14:20:30 | [train_policy] epoch #213 | Saved
2022-04-23 14:20:30 | [train_policy] epoch #213 | Time 78.17 s
2022-04-23 14:20:30 | [train_policy] epoch #213 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.12506
Evaluation/AverageDiscountedReturn          -46.3493
Evaluation/AverageReturn                    -46.3493
Evaluation/CompletionRate                     0
Evaluation/Iteration                        213
Evaluation/MaxReturn                        -35.4455
Evaluation/MinReturn                        -64.711
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.71651
Extras/EpisodeRewardMean                    -46.3127
LinearFeatureBaseline/ExplainedVariance       0.956966
PolicyExecTime                                0.111551
ProcessExecTime                               0.0125587
TotalEnvSteps                            216568
policy/Entropy                                0.522769
policy/KL                                     0.00656489
policy/KLBefore                               0
policy/LossAfter                             -0.0196643
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             1.68669
policy/dLoss                                  0.0196643
---------------------------------------  ----------------
2022-04-23 14:20:30 | [train_policy] epoch #214 | Obtaining samples for iteration 214...
2022-04-23 14:20:30 | [train_policy] epoch #214 | Logging diagnostics...
2022-04-23 14:20:30 | [train_policy] epoch #214 | Optimizing policy...
2022-04-23 14:20:30 | [train_policy] epoch #214 | Computing loss before
2022-04-23 14:20:30 | [train_policy] epoch #214 | Computing KL before
2022-04-23 14:20:30 | [train_policy] epoch #214 | Optimizing
2022-04-23 14:20:30 | [train_policy] epoch #214 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:30 | [train_policy] epoch #214 | computing loss before
2022-04-23 14:20:30 | [train_policy] epoch #214 | computing gradient
2022-04-23 14:20:30 | [train_policy] epoch #214 | gradient computed
2022-04-23 14:20:30 | [train_policy] epoch #214 | computing descent direction
2022-04-23 14:20:30 | [train_policy] epoch #214 | descent direction computed
2022-04-23 14:20:30 | [train_policy] epoch #214 | backtrack iters: 1
2022-04-23 14:20:30 | [train_policy] epoch #214 | optimization finished
2022-04-23 14:20:30 | [train_policy] epoch #214 | Computing KL after
2022-04-23 14:20:30 | [train_policy] epoch #214 | Computing loss after
2022-04-23 14:20:30 | [train_policy] epoch #214 | Fitting baseline...
2022-04-23 14:20:30 | [train_policy] epoch #214 | Saving snapshot...
2022-04-23 14:20:30 | [train_policy] epoch #214 | Saved
2022-04-23 14:20:30 | [train_policy] epoch #214 | Time 78.52 s
2022-04-23 14:20:30 | [train_policy] epoch #214 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116455
Evaluation/AverageDiscountedReturn          -46.9482
Evaluation/AverageReturn                    -46.9482
Evaluation/CompletionRate                     0
Evaluation/Iteration                        214
Evaluation/MaxReturn                        -31.7786
Evaluation/MinReturn                        -78.6999
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83506
Extras/EpisodeRewardMean                    -46.9996
LinearFeatureBaseline/ExplainedVariance       0.915134
PolicyExecTime                                0.106687
ProcessExecTime                               0.0116696
TotalEnvSteps                            217580
policy/Entropy                                0.521069
policy/KL                                     0.00692897
policy/KLBefore                               0
policy/LossAfter                             -0.0129155
policy/LossBefore                             3.32184e-08
policy/Perplexity                             1.68383
policy/dLoss                                  0.0129155
---------------------------------------  ----------------
2022-04-23 14:20:30 | [train_policy] epoch #215 | Obtaining samples for iteration 215...
2022-04-23 14:20:31 | [train_policy] epoch #215 | Logging diagnostics...
2022-04-23 14:20:31 | [train_policy] epoch #215 | Optimizing policy...
2022-04-23 14:20:31 | [train_policy] epoch #215 | Computing loss before
2022-04-23 14:20:31 | [train_policy] epoch #215 | Computing KL before
2022-04-23 14:20:31 | [train_policy] epoch #215 | Optimizing
2022-04-23 14:20:31 | [train_policy] epoch #215 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:31 | [train_policy] epoch #215 | computing loss before
2022-04-23 14:20:31 | [train_policy] epoch #215 | computing gradient
2022-04-23 14:20:31 | [train_policy] epoch #215 | gradient computed
2022-04-23 14:20:31 | [train_policy] epoch #215 | computing descent direction
2022-04-23 14:20:31 | [train_policy] epoch #215 | descent direction computed
2022-04-23 14:20:31 | [train_policy] epoch #215 | backtrack iters: 1
2022-04-23 14:20:31 | [train_policy] epoch #215 | optimization finished
2022-04-23 14:20:31 | [train_policy] epoch #215 | Computing KL after
2022-04-23 14:20:31 | [train_policy] epoch #215 | Computing loss after
2022-04-23 14:20:31 | [train_policy] epoch #215 | Fitting baseline...
2022-04-23 14:20:31 | [train_policy] epoch #215 | Saving snapshot...
2022-04-23 14:20:31 | [train_policy] epoch #215 | Saved
2022-04-23 14:20:31 | [train_policy] epoch #215 | Time 78.87 s
2022-04-23 14:20:31 | [train_policy] epoch #215 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116935
Evaluation/AverageDiscountedReturn          -46.1412
Evaluation/AverageReturn                    -46.1412
Evaluation/CompletionRate                     0
Evaluation/Iteration                        215
Evaluation/MaxReturn                        -36.6001
Evaluation/MinReturn                        -73.5353
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.95385
Extras/EpisodeRewardMean                    -46.2977
LinearFeatureBaseline/ExplainedVariance       0.906574
PolicyExecTime                                0.104091
ProcessExecTime                               0.011318
TotalEnvSteps                            218592
policy/Entropy                                0.519383
policy/KL                                     0.00669958
policy/KLBefore                               0
policy/LossAfter                             -0.0238837
policy/LossBefore                            -1.97897e-08
policy/Perplexity                             1.68099
policy/dLoss                                  0.0238837
---------------------------------------  ----------------
2022-04-23 14:20:31 | [train_policy] epoch #216 | Obtaining samples for iteration 216...
2022-04-23 14:20:31 | [train_policy] epoch #216 | Logging diagnostics...
2022-04-23 14:20:31 | [train_policy] epoch #216 | Optimizing policy...
2022-04-23 14:20:31 | [train_policy] epoch #216 | Computing loss before
2022-04-23 14:20:31 | [train_policy] epoch #216 | Computing KL before
2022-04-23 14:20:31 | [train_policy] epoch #216 | Optimizing
2022-04-23 14:20:31 | [train_policy] epoch #216 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:31 | [train_policy] epoch #216 | computing loss before
2022-04-23 14:20:31 | [train_policy] epoch #216 | computing gradient
2022-04-23 14:20:31 | [train_policy] epoch #216 | gradient computed
2022-04-23 14:20:31 | [train_policy] epoch #216 | computing descent direction
2022-04-23 14:20:31 | [train_policy] epoch #216 | descent direction computed
2022-04-23 14:20:31 | [train_policy] epoch #216 | backtrack iters: 1
2022-04-23 14:20:31 | [train_policy] epoch #216 | optimization finished
2022-04-23 14:20:31 | [train_policy] epoch #216 | Computing KL after
2022-04-23 14:20:31 | [train_policy] epoch #216 | Computing loss after
2022-04-23 14:20:31 | [train_policy] epoch #216 | Fitting baseline...
2022-04-23 14:20:31 | [train_policy] epoch #216 | Saving snapshot...
2022-04-23 14:20:31 | [train_policy] epoch #216 | Saved
2022-04-23 14:20:31 | [train_policy] epoch #216 | Time 79.23 s
2022-04-23 14:20:31 | [train_policy] epoch #216 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.121798
Evaluation/AverageDiscountedReturn          -44.7179
Evaluation/AverageReturn                    -44.7179
Evaluation/CompletionRate                     0
Evaluation/Iteration                        216
Evaluation/MaxReturn                        -35.0564
Evaluation/MinReturn                        -74.7674
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.9564
Extras/EpisodeRewardMean                    -44.7154
LinearFeatureBaseline/ExplainedVariance       0.928936
PolicyExecTime                                0.10591
ProcessExecTime                               0.012362
TotalEnvSteps                            219604
policy/Entropy                                0.508013
policy/KL                                     0.00676573
policy/KLBefore                               0
policy/LossAfter                             -0.0205557
policy/LossBefore                             2.8271e-09
policy/Perplexity                             1.66198
policy/dLoss                                  0.0205557
---------------------------------------  ---------------
2022-04-23 14:20:31 | [train_policy] epoch #217 | Obtaining samples for iteration 217...
2022-04-23 14:20:31 | [train_policy] epoch #217 | Logging diagnostics...
2022-04-23 14:20:31 | [train_policy] epoch #217 | Optimizing policy...
2022-04-23 14:20:31 | [train_policy] epoch #217 | Computing loss before
2022-04-23 14:20:31 | [train_policy] epoch #217 | Computing KL before
2022-04-23 14:20:31 | [train_policy] epoch #217 | Optimizing
2022-04-23 14:20:31 | [train_policy] epoch #217 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:31 | [train_policy] epoch #217 | computing loss before
2022-04-23 14:20:31 | [train_policy] epoch #217 | computing gradient
2022-04-23 14:20:31 | [train_policy] epoch #217 | gradient computed
2022-04-23 14:20:31 | [train_policy] epoch #217 | computing descent direction
2022-04-23 14:20:31 | [train_policy] epoch #217 | descent direction computed
2022-04-23 14:20:31 | [train_policy] epoch #217 | backtrack iters: 0
2022-04-23 14:20:31 | [train_policy] epoch #217 | optimization finished
2022-04-23 14:20:31 | [train_policy] epoch #217 | Computing KL after
2022-04-23 14:20:31 | [train_policy] epoch #217 | Computing loss after
2022-04-23 14:20:31 | [train_policy] epoch #217 | Fitting baseline...
2022-04-23 14:20:31 | [train_policy] epoch #217 | Saving snapshot...
2022-04-23 14:20:31 | [train_policy] epoch #217 | Saved
2022-04-23 14:20:31 | [train_policy] epoch #217 | Time 79.57 s
2022-04-23 14:20:31 | [train_policy] epoch #217 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.115885
Evaluation/AverageDiscountedReturn          -88.3477
Evaluation/AverageReturn                    -88.3477
Evaluation/CompletionRate                     0
Evaluation/Iteration                        217
Evaluation/MaxReturn                        -34.8263
Evaluation/MinReturn                      -2064.62
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.611
Extras/EpisodeRewardMean                    -85.1022
LinearFeatureBaseline/ExplainedVariance       0.0104885
PolicyExecTime                                0.100188
ProcessExecTime                               0.0116203
TotalEnvSteps                            220616
policy/Entropy                                0.44811
policy/KL                                     0.00961975
policy/KLBefore                               0
policy/LossAfter                             -0.0210716
policy/LossBefore                            -8.2457e-09
policy/Perplexity                             1.56535
policy/dLoss                                  0.0210716
---------------------------------------  ---------------
2022-04-23 14:20:31 | [train_policy] epoch #218 | Obtaining samples for iteration 218...
2022-04-23 14:20:32 | [train_policy] epoch #218 | Logging diagnostics...
2022-04-23 14:20:32 | [train_policy] epoch #218 | Optimizing policy...
2022-04-23 14:20:32 | [train_policy] epoch #218 | Computing loss before
2022-04-23 14:20:32 | [train_policy] epoch #218 | Computing KL before
2022-04-23 14:20:32 | [train_policy] epoch #218 | Optimizing
2022-04-23 14:20:32 | [train_policy] epoch #218 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:32 | [train_policy] epoch #218 | computing loss before
2022-04-23 14:20:32 | [train_policy] epoch #218 | computing gradient
2022-04-23 14:20:32 | [train_policy] epoch #218 | gradient computed
2022-04-23 14:20:32 | [train_policy] epoch #218 | computing descent direction
2022-04-23 14:20:32 | [train_policy] epoch #218 | descent direction computed
2022-04-23 14:20:32 | [train_policy] epoch #218 | backtrack iters: 1
2022-04-23 14:20:32 | [train_policy] epoch #218 | optimization finished
2022-04-23 14:20:32 | [train_policy] epoch #218 | Computing KL after
2022-04-23 14:20:32 | [train_policy] epoch #218 | Computing loss after
2022-04-23 14:20:32 | [train_policy] epoch #218 | Fitting baseline...
2022-04-23 14:20:32 | [train_policy] epoch #218 | Saving snapshot...
2022-04-23 14:20:32 | [train_policy] epoch #218 | Saved
2022-04-23 14:20:32 | [train_policy] epoch #218 | Time 79.92 s
2022-04-23 14:20:32 | [train_policy] epoch #218 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116401
Evaluation/AverageDiscountedReturn          -89.6347
Evaluation/AverageReturn                    -89.6347
Evaluation/CompletionRate                     0
Evaluation/Iteration                        218
Evaluation/MaxReturn                        -35.6093
Evaluation/MinReturn                      -2064.35
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        293.649
Extras/EpisodeRewardMean                    -85.9198
LinearFeatureBaseline/ExplainedVariance       0.0879554
PolicyExecTime                                0.105793
ProcessExecTime                               0.0120156
TotalEnvSteps                            221628
policy/Entropy                                0.440411
policy/KL                                     0.00671954
policy/KLBefore                               0
policy/LossAfter                             -0.0159368
policy/LossBefore                             6.36097e-09
policy/Perplexity                             1.55335
policy/dLoss                                  0.0159368
---------------------------------------  ----------------
2022-04-23 14:20:32 | [train_policy] epoch #219 | Obtaining samples for iteration 219...
2022-04-23 14:20:32 | [train_policy] epoch #219 | Logging diagnostics...
2022-04-23 14:20:32 | [train_policy] epoch #219 | Optimizing policy...
2022-04-23 14:20:32 | [train_policy] epoch #219 | Computing loss before
2022-04-23 14:20:32 | [train_policy] epoch #219 | Computing KL before
2022-04-23 14:20:32 | [train_policy] epoch #219 | Optimizing
2022-04-23 14:20:32 | [train_policy] epoch #219 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:32 | [train_policy] epoch #219 | computing loss before
2022-04-23 14:20:32 | [train_policy] epoch #219 | computing gradient
2022-04-23 14:20:32 | [train_policy] epoch #219 | gradient computed
2022-04-23 14:20:32 | [train_policy] epoch #219 | computing descent direction
2022-04-23 14:20:32 | [train_policy] epoch #219 | descent direction computed
2022-04-23 14:20:32 | [train_policy] epoch #219 | backtrack iters: 1
2022-04-23 14:20:32 | [train_policy] epoch #219 | optimization finished
2022-04-23 14:20:32 | [train_policy] epoch #219 | Computing KL after
2022-04-23 14:20:32 | [train_policy] epoch #219 | Computing loss after
2022-04-23 14:20:32 | [train_policy] epoch #219 | Fitting baseline...
2022-04-23 14:20:32 | [train_policy] epoch #219 | Saving snapshot...
2022-04-23 14:20:32 | [train_policy] epoch #219 | Saved
2022-04-23 14:20:32 | [train_policy] epoch #219 | Time 80.27 s
2022-04-23 14:20:32 | [train_policy] epoch #219 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115641
Evaluation/AverageDiscountedReturn          -45.0415
Evaluation/AverageReturn                    -45.0415
Evaluation/CompletionRate                     0
Evaluation/Iteration                        219
Evaluation/MaxReturn                        -35.2951
Evaluation/MinReturn                        -74.7063
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.67933
Extras/EpisodeRewardMean                    -45.0162
LinearFeatureBaseline/ExplainedVariance     -45.8159
PolicyExecTime                                0.103127
ProcessExecTime                               0.0111778
TotalEnvSteps                            222640
policy/Entropy                                0.435226
policy/KL                                     0.00646685
policy/KLBefore                               0
policy/LossAfter                             -0.0110061
policy/LossBefore                             1.22508e-08
policy/Perplexity                             1.54531
policy/dLoss                                  0.0110061
---------------------------------------  ----------------
2022-04-23 14:20:32 | [train_policy] epoch #220 | Obtaining samples for iteration 220...
2022-04-23 14:20:32 | [train_policy] epoch #220 | Logging diagnostics...
2022-04-23 14:20:32 | [train_policy] epoch #220 | Optimizing policy...
2022-04-23 14:20:32 | [train_policy] epoch #220 | Computing loss before
2022-04-23 14:20:32 | [train_policy] epoch #220 | Computing KL before
2022-04-23 14:20:32 | [train_policy] epoch #220 | Optimizing
2022-04-23 14:20:32 | [train_policy] epoch #220 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:32 | [train_policy] epoch #220 | computing loss before
2022-04-23 14:20:32 | [train_policy] epoch #220 | computing gradient
2022-04-23 14:20:32 | [train_policy] epoch #220 | gradient computed
2022-04-23 14:20:32 | [train_policy] epoch #220 | computing descent direction
2022-04-23 14:20:32 | [train_policy] epoch #220 | descent direction computed
2022-04-23 14:20:32 | [train_policy] epoch #220 | backtrack iters: 1
2022-04-23 14:20:32 | [train_policy] epoch #220 | optimization finished
2022-04-23 14:20:32 | [train_policy] epoch #220 | Computing KL after
2022-04-23 14:20:32 | [train_policy] epoch #220 | Computing loss after
2022-04-23 14:20:32 | [train_policy] epoch #220 | Fitting baseline...
2022-04-23 14:20:32 | [train_policy] epoch #220 | Saving snapshot...
2022-04-23 14:20:32 | [train_policy] epoch #220 | Saved
2022-04-23 14:20:32 | [train_policy] epoch #220 | Time 80.64 s
2022-04-23 14:20:32 | [train_policy] epoch #220 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.125033
Evaluation/AverageDiscountedReturn          -45.8654
Evaluation/AverageReturn                    -45.8654
Evaluation/CompletionRate                     0
Evaluation/Iteration                        220
Evaluation/MaxReturn                        -35.0817
Evaluation/MinReturn                        -70.1679
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.74438
Extras/EpisodeRewardMean                    -45.9103
LinearFeatureBaseline/ExplainedVariance       0.931138
PolicyExecTime                                0.112852
ProcessExecTime                               0.0125287
TotalEnvSteps                            223652
policy/Entropy                                0.425694
policy/KL                                     0.00672348
policy/KLBefore                               0
policy/LossAfter                             -0.015753
policy/LossBefore                             1.45478e-08
policy/Perplexity                             1.53065
policy/dLoss                                  0.015753
---------------------------------------  ----------------
2022-04-23 14:20:32 | [train_policy] epoch #221 | Obtaining samples for iteration 221...
2022-04-23 14:20:33 | [train_policy] epoch #221 | Logging diagnostics...
2022-04-23 14:20:33 | [train_policy] epoch #221 | Optimizing policy...
2022-04-23 14:20:33 | [train_policy] epoch #221 | Computing loss before
2022-04-23 14:20:33 | [train_policy] epoch #221 | Computing KL before
2022-04-23 14:20:33 | [train_policy] epoch #221 | Optimizing
2022-04-23 14:20:33 | [train_policy] epoch #221 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:33 | [train_policy] epoch #221 | computing loss before
2022-04-23 14:20:33 | [train_policy] epoch #221 | computing gradient
2022-04-23 14:20:33 | [train_policy] epoch #221 | gradient computed
2022-04-23 14:20:33 | [train_policy] epoch #221 | computing descent direction
2022-04-23 14:20:33 | [train_policy] epoch #221 | descent direction computed
2022-04-23 14:20:33 | [train_policy] epoch #221 | backtrack iters: 1
2022-04-23 14:20:33 | [train_policy] epoch #221 | optimization finished
2022-04-23 14:20:33 | [train_policy] epoch #221 | Computing KL after
2022-04-23 14:20:33 | [train_policy] epoch #221 | Computing loss after
2022-04-23 14:20:33 | [train_policy] epoch #221 | Fitting baseline...
2022-04-23 14:20:33 | [train_policy] epoch #221 | Saving snapshot...
2022-04-23 14:20:33 | [train_policy] epoch #221 | Saved
2022-04-23 14:20:33 | [train_policy] epoch #221 | Time 80.99 s
2022-04-23 14:20:33 | [train_policy] epoch #221 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.115354
Evaluation/AverageDiscountedReturn          -91.2169
Evaluation/AverageReturn                    -91.2169
Evaluation/CompletionRate                     0
Evaluation/Iteration                        221
Evaluation/MaxReturn                        -36.1702
Evaluation/MinReturn                      -2069.12
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.308
Extras/EpisodeRewardMean                    -87.4965
LinearFeatureBaseline/ExplainedVariance       0.00914145
PolicyExecTime                                0.105304
ProcessExecTime                               0.0112422
TotalEnvSteps                            224664
policy/Entropy                                0.421859
policy/KL                                     0.00683753
policy/KLBefore                               0
policy/LossAfter                             -0.0264811
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             1.52479
policy/dLoss                                  0.0264811
---------------------------------------  ---------------
2022-04-23 14:20:33 | [train_policy] epoch #222 | Obtaining samples for iteration 222...
2022-04-23 14:20:33 | [train_policy] epoch #222 | Logging diagnostics...
2022-04-23 14:20:33 | [train_policy] epoch #222 | Optimizing policy...
2022-04-23 14:20:33 | [train_policy] epoch #222 | Computing loss before
2022-04-23 14:20:33 | [train_policy] epoch #222 | Computing KL before
2022-04-23 14:20:33 | [train_policy] epoch #222 | Optimizing
2022-04-23 14:20:33 | [train_policy] epoch #222 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:33 | [train_policy] epoch #222 | computing loss before
2022-04-23 14:20:33 | [train_policy] epoch #222 | computing gradient
2022-04-23 14:20:33 | [train_policy] epoch #222 | gradient computed
2022-04-23 14:20:33 | [train_policy] epoch #222 | computing descent direction
2022-04-23 14:20:33 | [train_policy] epoch #222 | descent direction computed
2022-04-23 14:20:33 | [train_policy] epoch #222 | backtrack iters: 0
2022-04-23 14:20:33 | [train_policy] epoch #222 | optimization finished
2022-04-23 14:20:33 | [train_policy] epoch #222 | Computing KL after
2022-04-23 14:20:33 | [train_policy] epoch #222 | Computing loss after
2022-04-23 14:20:33 | [train_policy] epoch #222 | Fitting baseline...
2022-04-23 14:20:33 | [train_policy] epoch #222 | Saving snapshot...
2022-04-23 14:20:33 | [train_policy] epoch #222 | Saved
2022-04-23 14:20:33 | [train_policy] epoch #222 | Time 81.33 s
2022-04-23 14:20:33 | [train_policy] epoch #222 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116144
Evaluation/AverageDiscountedReturn          -66.6772
Evaluation/AverageReturn                    -66.6772
Evaluation/CompletionRate                     0
Evaluation/Iteration                        222
Evaluation/MaxReturn                        -35.2345
Evaluation/MinReturn                      -2063.26
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.383
Extras/EpisodeRewardMean                    -65.1526
LinearFeatureBaseline/ExplainedVariance       0.115441
PolicyExecTime                                0.103142
ProcessExecTime                               0.0117431
TotalEnvSteps                            225676
policy/Entropy                                0.455051
policy/KL                                     0.00903937
policy/KLBefore                               0
policy/LossAfter                             -0.0258574
policy/LossBefore                             1.41355e-09
policy/Perplexity                             1.57625
policy/dLoss                                  0.0258574
---------------------------------------  ----------------
2022-04-23 14:20:33 | [train_policy] epoch #223 | Obtaining samples for iteration 223...
2022-04-23 14:20:33 | [train_policy] epoch #223 | Logging diagnostics...
2022-04-23 14:20:33 | [train_policy] epoch #223 | Optimizing policy...
2022-04-23 14:20:33 | [train_policy] epoch #223 | Computing loss before
2022-04-23 14:20:33 | [train_policy] epoch #223 | Computing KL before
2022-04-23 14:20:33 | [train_policy] epoch #223 | Optimizing
2022-04-23 14:20:33 | [train_policy] epoch #223 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:33 | [train_policy] epoch #223 | computing loss before
2022-04-23 14:20:33 | [train_policy] epoch #223 | computing gradient
2022-04-23 14:20:33 | [train_policy] epoch #223 | gradient computed
2022-04-23 14:20:33 | [train_policy] epoch #223 | computing descent direction
2022-04-23 14:20:33 | [train_policy] epoch #223 | descent direction computed
2022-04-23 14:20:33 | [train_policy] epoch #223 | backtrack iters: 1
2022-04-23 14:20:33 | [train_policy] epoch #223 | optimization finished
2022-04-23 14:20:33 | [train_policy] epoch #223 | Computing KL after
2022-04-23 14:20:33 | [train_policy] epoch #223 | Computing loss after
2022-04-23 14:20:33 | [train_policy] epoch #223 | Fitting baseline...
2022-04-23 14:20:33 | [train_policy] epoch #223 | Saving snapshot...
2022-04-23 14:20:33 | [train_policy] epoch #223 | Saved
2022-04-23 14:20:33 | [train_policy] epoch #223 | Time 81.68 s
2022-04-23 14:20:33 | [train_policy] epoch #223 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116316
Evaluation/AverageDiscountedReturn          -44.3865
Evaluation/AverageReturn                    -44.3865
Evaluation/CompletionRate                     0
Evaluation/Iteration                        223
Evaluation/MaxReturn                        -35.3115
Evaluation/MinReturn                        -63.8739
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.48304
Extras/EpisodeRewardMean                    -44.792
LinearFeatureBaseline/ExplainedVariance     -72.4768
PolicyExecTime                                0.101682
ProcessExecTime                               0.0116313
TotalEnvSteps                            226688
policy/Entropy                                0.432511
policy/KL                                     0.00697714
policy/KLBefore                               0
policy/LossAfter                             -0.0162125
policy/LossBefore                            -1.28397e-08
policy/Perplexity                             1.54112
policy/dLoss                                  0.0162124
---------------------------------------  ----------------
2022-04-23 14:20:33 | [train_policy] epoch #224 | Obtaining samples for iteration 224...
2022-04-23 14:20:34 | [train_policy] epoch #224 | Logging diagnostics...
2022-04-23 14:20:34 | [train_policy] epoch #224 | Optimizing policy...
2022-04-23 14:20:34 | [train_policy] epoch #224 | Computing loss before
2022-04-23 14:20:34 | [train_policy] epoch #224 | Computing KL before
2022-04-23 14:20:34 | [train_policy] epoch #224 | Optimizing
2022-04-23 14:20:34 | [train_policy] epoch #224 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:34 | [train_policy] epoch #224 | computing loss before
2022-04-23 14:20:34 | [train_policy] epoch #224 | computing gradient
2022-04-23 14:20:34 | [train_policy] epoch #224 | gradient computed
2022-04-23 14:20:34 | [train_policy] epoch #224 | computing descent direction
2022-04-23 14:20:34 | [train_policy] epoch #224 | descent direction computed
2022-04-23 14:20:34 | [train_policy] epoch #224 | backtrack iters: 0
2022-04-23 14:20:34 | [train_policy] epoch #224 | optimization finished
2022-04-23 14:20:34 | [train_policy] epoch #224 | Computing KL after
2022-04-23 14:20:34 | [train_policy] epoch #224 | Computing loss after
2022-04-23 14:20:34 | [train_policy] epoch #224 | Fitting baseline...
2022-04-23 14:20:34 | [train_policy] epoch #224 | Saving snapshot...
2022-04-23 14:20:34 | [train_policy] epoch #224 | Saved
2022-04-23 14:20:34 | [train_policy] epoch #224 | Time 82.02 s
2022-04-23 14:20:34 | [train_policy] epoch #224 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115541
Evaluation/AverageDiscountedReturn          -69.1761
Evaluation/AverageReturn                    -69.1761
Evaluation/CompletionRate                     0
Evaluation/Iteration                        224
Evaluation/MaxReturn                        -34.7931
Evaluation/MinReturn                      -2066.75
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.617
Extras/EpisodeRewardMean                    -66.9603
LinearFeatureBaseline/ExplainedVariance       0.0105843
PolicyExecTime                                0.106976
ProcessExecTime                               0.0112453
TotalEnvSteps                            227700
policy/Entropy                                0.422071
policy/KL                                     0.00998382
policy/KLBefore                               0
policy/LossAfter                             -0.0186579
policy/LossBefore                             4.71183e-09
policy/Perplexity                             1.52512
policy/dLoss                                  0.0186579
---------------------------------------  ----------------
2022-04-23 14:20:34 | [train_policy] epoch #225 | Obtaining samples for iteration 225...
2022-04-23 14:20:34 | [train_policy] epoch #225 | Logging diagnostics...
2022-04-23 14:20:34 | [train_policy] epoch #225 | Optimizing policy...
2022-04-23 14:20:34 | [train_policy] epoch #225 | Computing loss before
2022-04-23 14:20:34 | [train_policy] epoch #225 | Computing KL before
2022-04-23 14:20:34 | [train_policy] epoch #225 | Optimizing
2022-04-23 14:20:34 | [train_policy] epoch #225 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:34 | [train_policy] epoch #225 | computing loss before
2022-04-23 14:20:34 | [train_policy] epoch #225 | computing gradient
2022-04-23 14:20:34 | [train_policy] epoch #225 | gradient computed
2022-04-23 14:20:34 | [train_policy] epoch #225 | computing descent direction
2022-04-23 14:20:34 | [train_policy] epoch #225 | descent direction computed
2022-04-23 14:20:34 | [train_policy] epoch #225 | backtrack iters: 1
2022-04-23 14:20:34 | [train_policy] epoch #225 | optimization finished
2022-04-23 14:20:34 | [train_policy] epoch #225 | Computing KL after
2022-04-23 14:20:34 | [train_policy] epoch #225 | Computing loss after
2022-04-23 14:20:34 | [train_policy] epoch #225 | Fitting baseline...
2022-04-23 14:20:34 | [train_policy] epoch #225 | Saving snapshot...
2022-04-23 14:20:34 | [train_policy] epoch #225 | Saved
2022-04-23 14:20:34 | [train_policy] epoch #225 | Time 82.37 s
2022-04-23 14:20:34 | [train_policy] epoch #225 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116737
Evaluation/AverageDiscountedReturn          -67.6771
Evaluation/AverageReturn                    -67.6771
Evaluation/CompletionRate                     0
Evaluation/Iteration                        225
Evaluation/MaxReturn                        -35.6747
Evaluation/MinReturn                      -2068.51
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.831
Extras/EpisodeRewardMean                    -66.1731
LinearFeatureBaseline/ExplainedVariance       0.119288
PolicyExecTime                                0.106033
ProcessExecTime                               0.011282
TotalEnvSteps                            228712
policy/Entropy                                0.423112
policy/KL                                     0.00671797
policy/KLBefore                               0
policy/LossAfter                             -0.0192518
policy/LossBefore                             4.47624e-09
policy/Perplexity                             1.52671
policy/dLoss                                  0.0192518
---------------------------------------  ----------------
2022-04-23 14:20:34 | [train_policy] epoch #226 | Obtaining samples for iteration 226...
2022-04-23 14:20:34 | [train_policy] epoch #226 | Logging diagnostics...
2022-04-23 14:20:34 | [train_policy] epoch #226 | Optimizing policy...
2022-04-23 14:20:34 | [train_policy] epoch #226 | Computing loss before
2022-04-23 14:20:34 | [train_policy] epoch #226 | Computing KL before
2022-04-23 14:20:34 | [train_policy] epoch #226 | Optimizing
2022-04-23 14:20:34 | [train_policy] epoch #226 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:34 | [train_policy] epoch #226 | computing loss before
2022-04-23 14:20:34 | [train_policy] epoch #226 | computing gradient
2022-04-23 14:20:34 | [train_policy] epoch #226 | gradient computed
2022-04-23 14:20:34 | [train_policy] epoch #226 | computing descent direction
2022-04-23 14:20:34 | [train_policy] epoch #226 | descent direction computed
2022-04-23 14:20:34 | [train_policy] epoch #226 | backtrack iters: 0
2022-04-23 14:20:34 | [train_policy] epoch #226 | optimization finished
2022-04-23 14:20:34 | [train_policy] epoch #226 | Computing KL after
2022-04-23 14:20:34 | [train_policy] epoch #226 | Computing loss after
2022-04-23 14:20:35 | [train_policy] epoch #226 | Fitting baseline...
2022-04-23 14:20:35 | [train_policy] epoch #226 | Saving snapshot...
2022-04-23 14:20:35 | [train_policy] epoch #226 | Saved
2022-04-23 14:20:35 | [train_policy] epoch #226 | Time 82.73 s
2022-04-23 14:20:35 | [train_policy] epoch #226 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118024
Evaluation/AverageDiscountedReturn          -66.9153
Evaluation/AverageReturn                    -66.9153
Evaluation/CompletionRate                     0
Evaluation/Iteration                        226
Evaluation/MaxReturn                        -34.8031
Evaluation/MinReturn                      -2063.15
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.414
Extras/EpisodeRewardMean                    -85.4052
LinearFeatureBaseline/ExplainedVariance      -0.0440735
PolicyExecTime                                0.103455
ProcessExecTime                               0.0112956
TotalEnvSteps                            229724
policy/Entropy                                0.44658
policy/KL                                     0.00946189
policy/KLBefore                               0
policy/LossAfter                             -0.0411753
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             1.56296
policy/dLoss                                  0.0411753
---------------------------------------  ----------------
2022-04-23 14:20:35 | [train_policy] epoch #227 | Obtaining samples for iteration 227...
2022-04-23 14:20:35 | [train_policy] epoch #227 | Logging diagnostics...
2022-04-23 14:20:35 | [train_policy] epoch #227 | Optimizing policy...
2022-04-23 14:20:35 | [train_policy] epoch #227 | Computing loss before
2022-04-23 14:20:35 | [train_policy] epoch #227 | Computing KL before
2022-04-23 14:20:35 | [train_policy] epoch #227 | Optimizing
2022-04-23 14:20:35 | [train_policy] epoch #227 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:35 | [train_policy] epoch #227 | computing loss before
2022-04-23 14:20:35 | [train_policy] epoch #227 | computing gradient
2022-04-23 14:20:35 | [train_policy] epoch #227 | gradient computed
2022-04-23 14:20:35 | [train_policy] epoch #227 | computing descent direction
2022-04-23 14:20:35 | [train_policy] epoch #227 | descent direction computed
2022-04-23 14:20:35 | [train_policy] epoch #227 | backtrack iters: 0
2022-04-23 14:20:35 | [train_policy] epoch #227 | optimization finished
2022-04-23 14:20:35 | [train_policy] epoch #227 | Computing KL after
2022-04-23 14:20:35 | [train_policy] epoch #227 | Computing loss after
2022-04-23 14:20:35 | [train_policy] epoch #227 | Fitting baseline...
2022-04-23 14:20:35 | [train_policy] epoch #227 | Saving snapshot...
2022-04-23 14:20:35 | [train_policy] epoch #227 | Saved
2022-04-23 14:20:35 | [train_policy] epoch #227 | Time 83.08 s
2022-04-23 14:20:35 | [train_policy] epoch #227 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.121346
Evaluation/AverageDiscountedReturn          -66.1829
Evaluation/AverageReturn                    -66.1829
Evaluation/CompletionRate                     0
Evaluation/Iteration                        227
Evaluation/MaxReturn                        -36.01
Evaluation/MinReturn                      -2065.38
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.648
Extras/EpisodeRewardMean                    -64.4776
LinearFeatureBaseline/ExplainedVariance       0.130707
PolicyExecTime                                0.102846
ProcessExecTime                               0.0121658
TotalEnvSteps                            230736
policy/Entropy                                0.444748
policy/KL                                     0.00926658
policy/KLBefore                               0
policy/LossAfter                             -0.0280758
policy/LossBefore                             9.42366e-10
policy/Perplexity                             1.5601
policy/dLoss                                  0.0280758
---------------------------------------  ----------------
2022-04-23 14:20:35 | [train_policy] epoch #228 | Obtaining samples for iteration 228...
2022-04-23 14:20:35 | [train_policy] epoch #228 | Logging diagnostics...
2022-04-23 14:20:35 | [train_policy] epoch #228 | Optimizing policy...
2022-04-23 14:20:35 | [train_policy] epoch #228 | Computing loss before
2022-04-23 14:20:35 | [train_policy] epoch #228 | Computing KL before
2022-04-23 14:20:35 | [train_policy] epoch #228 | Optimizing
2022-04-23 14:20:35 | [train_policy] epoch #228 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:35 | [train_policy] epoch #228 | computing loss before
2022-04-23 14:20:35 | [train_policy] epoch #228 | computing gradient
2022-04-23 14:20:35 | [train_policy] epoch #228 | gradient computed
2022-04-23 14:20:35 | [train_policy] epoch #228 | computing descent direction
2022-04-23 14:20:35 | [train_policy] epoch #228 | descent direction computed
2022-04-23 14:20:35 | [train_policy] epoch #228 | backtrack iters: 1
2022-04-23 14:20:35 | [train_policy] epoch #228 | optimization finished
2022-04-23 14:20:35 | [train_policy] epoch #228 | Computing KL after
2022-04-23 14:20:35 | [train_policy] epoch #228 | Computing loss after
2022-04-23 14:20:35 | [train_policy] epoch #228 | Fitting baseline...
2022-04-23 14:20:35 | [train_policy] epoch #228 | Saving snapshot...
2022-04-23 14:20:35 | [train_policy] epoch #228 | Saved
2022-04-23 14:20:35 | [train_policy] epoch #228 | Time 83.44 s
2022-04-23 14:20:35 | [train_policy] epoch #228 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116599
Evaluation/AverageDiscountedReturn          -46.1491
Evaluation/AverageReturn                    -46.1491
Evaluation/CompletionRate                     0
Evaluation/Iteration                        228
Evaluation/MaxReturn                        -35.7042
Evaluation/MinReturn                       -178.269
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         14.7893
Extras/EpisodeRewardMean                    -45.882
LinearFeatureBaseline/ExplainedVariance     -20.9128
PolicyExecTime                                0.10522
ProcessExecTime                               0.0119255
TotalEnvSteps                            231748
policy/Entropy                                0.44949
policy/KL                                     0.00642315
policy/KLBefore                               0
policy/LossAfter                             -0.0292969
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             1.56751
policy/dLoss                                  0.0292969
---------------------------------------  ----------------
2022-04-23 14:20:35 | [train_policy] epoch #229 | Obtaining samples for iteration 229...
2022-04-23 14:20:35 | [train_policy] epoch #229 | Logging diagnostics...
2022-04-23 14:20:35 | [train_policy] epoch #229 | Optimizing policy...
2022-04-23 14:20:35 | [train_policy] epoch #229 | Computing loss before
2022-04-23 14:20:35 | [train_policy] epoch #229 | Computing KL before
2022-04-23 14:20:35 | [train_policy] epoch #229 | Optimizing
2022-04-23 14:20:35 | [train_policy] epoch #229 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:35 | [train_policy] epoch #229 | computing loss before
2022-04-23 14:20:35 | [train_policy] epoch #229 | computing gradient
2022-04-23 14:20:36 | [train_policy] epoch #229 | gradient computed
2022-04-23 14:20:36 | [train_policy] epoch #229 | computing descent direction
2022-04-23 14:20:36 | [train_policy] epoch #229 | descent direction computed
2022-04-23 14:20:36 | [train_policy] epoch #229 | backtrack iters: 0
2022-04-23 14:20:36 | [train_policy] epoch #229 | optimization finished
2022-04-23 14:20:36 | [train_policy] epoch #229 | Computing KL after
2022-04-23 14:20:36 | [train_policy] epoch #229 | Computing loss after
2022-04-23 14:20:36 | [train_policy] epoch #229 | Fitting baseline...
2022-04-23 14:20:36 | [train_policy] epoch #229 | Saving snapshot...
2022-04-23 14:20:36 | [train_policy] epoch #229 | Saved
2022-04-23 14:20:36 | [train_policy] epoch #229 | Time 83.79 s
2022-04-23 14:20:36 | [train_policy] epoch #229 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11638
Evaluation/AverageDiscountedReturn          -88.2683
Evaluation/AverageReturn                    -88.2683
Evaluation/CompletionRate                     0
Evaluation/Iteration                        229
Evaluation/MaxReturn                        -36.0527
Evaluation/MinReturn                      -2071.57
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.066
Extras/EpisodeRewardMean                    -84.8663
LinearFeatureBaseline/ExplainedVariance       0.0202177
PolicyExecTime                                0.106604
ProcessExecTime                               0.0124483
TotalEnvSteps                            232760
policy/Entropy                                0.467345
policy/KL                                     0.00963808
policy/KLBefore                               0
policy/LossAfter                             -0.0240357
policy/LossBefore                             1.24863e-08
policy/Perplexity                             1.59575
policy/dLoss                                  0.0240357
---------------------------------------  ----------------
2022-04-23 14:20:36 | [train_policy] epoch #230 | Obtaining samples for iteration 230...
2022-04-23 14:20:36 | [train_policy] epoch #230 | Logging diagnostics...
2022-04-23 14:20:36 | [train_policy] epoch #230 | Optimizing policy...
2022-04-23 14:20:36 | [train_policy] epoch #230 | Computing loss before
2022-04-23 14:20:36 | [train_policy] epoch #230 | Computing KL before
2022-04-23 14:20:36 | [train_policy] epoch #230 | Optimizing
2022-04-23 14:20:36 | [train_policy] epoch #230 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:36 | [train_policy] epoch #230 | computing loss before
2022-04-23 14:20:36 | [train_policy] epoch #230 | computing gradient
2022-04-23 14:20:36 | [train_policy] epoch #230 | gradient computed
2022-04-23 14:20:36 | [train_policy] epoch #230 | computing descent direction
2022-04-23 14:20:36 | [train_policy] epoch #230 | descent direction computed
2022-04-23 14:20:36 | [train_policy] epoch #230 | backtrack iters: 0
2022-04-23 14:20:36 | [train_policy] epoch #230 | optimization finished
2022-04-23 14:20:36 | [train_policy] epoch #230 | Computing KL after
2022-04-23 14:20:36 | [train_policy] epoch #230 | Computing loss after
2022-04-23 14:20:36 | [train_policy] epoch #230 | Fitting baseline...
2022-04-23 14:20:36 | [train_policy] epoch #230 | Saving snapshot...
2022-04-23 14:20:36 | [train_policy] epoch #230 | Saved
2022-04-23 14:20:36 | [train_policy] epoch #230 | Time 84.13 s
2022-04-23 14:20:36 | [train_policy] epoch #230 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118114
Evaluation/AverageDiscountedReturn          -67.4671
Evaluation/AverageReturn                    -67.4671
Evaluation/CompletionRate                     0
Evaluation/Iteration                        230
Evaluation/MaxReturn                        -35.5129
Evaluation/MinReturn                      -2063.1
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.29
Extras/EpisodeRewardMean                    -65.8319
LinearFeatureBaseline/ExplainedVariance      -0.0200947
PolicyExecTime                                0.0976462
ProcessExecTime                               0.0113211
TotalEnvSteps                            233772
policy/Entropy                                0.426555
policy/KL                                     0.00941573
policy/KLBefore                               0
policy/LossAfter                             -0.0180621
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             1.53197
policy/dLoss                                  0.0180621
---------------------------------------  ----------------
2022-04-23 14:20:36 | [train_policy] epoch #231 | Obtaining samples for iteration 231...
2022-04-23 14:20:36 | [train_policy] epoch #231 | Logging diagnostics...
2022-04-23 14:20:36 | [train_policy] epoch #231 | Optimizing policy...
2022-04-23 14:20:36 | [train_policy] epoch #231 | Computing loss before
2022-04-23 14:20:36 | [train_policy] epoch #231 | Computing KL before
2022-04-23 14:20:36 | [train_policy] epoch #231 | Optimizing
2022-04-23 14:20:36 | [train_policy] epoch #231 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:36 | [train_policy] epoch #231 | computing loss before
2022-04-23 14:20:36 | [train_policy] epoch #231 | computing gradient
2022-04-23 14:20:36 | [train_policy] epoch #231 | gradient computed
2022-04-23 14:20:36 | [train_policy] epoch #231 | computing descent direction
2022-04-23 14:20:36 | [train_policy] epoch #231 | descent direction computed
2022-04-23 14:20:36 | [train_policy] epoch #231 | backtrack iters: 1
2022-04-23 14:20:36 | [train_policy] epoch #231 | optimization finished
2022-04-23 14:20:36 | [train_policy] epoch #231 | Computing KL after
2022-04-23 14:20:36 | [train_policy] epoch #231 | Computing loss after
2022-04-23 14:20:36 | [train_policy] epoch #231 | Fitting baseline...
2022-04-23 14:20:36 | [train_policy] epoch #231 | Saving snapshot...
2022-04-23 14:20:36 | [train_policy] epoch #231 | Saved
2022-04-23 14:20:36 | [train_policy] epoch #231 | Time 84.49 s
2022-04-23 14:20:36 | [train_policy] epoch #231 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.119792
Evaluation/AverageDiscountedReturn          -67.5963
Evaluation/AverageReturn                    -67.5963
Evaluation/CompletionRate                     0
Evaluation/Iteration                        231
Evaluation/MaxReturn                        -35.3208
Evaluation/MinReturn                      -2067.97
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.798
Extras/EpisodeRewardMean                    -65.9771
LinearFeatureBaseline/ExplainedVariance       0.1069
PolicyExecTime                                0.107246
ProcessExecTime                               0.0116792
TotalEnvSteps                            234784
policy/Entropy                                0.433565
policy/KL                                     0.00655851
policy/KLBefore                               0
policy/LossAfter                             -0.00620264
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             1.54275
policy/dLoss                                  0.00620263
---------------------------------------  ---------------
2022-04-23 14:20:36 | [train_policy] epoch #232 | Obtaining samples for iteration 232...
2022-04-23 14:20:37 | [train_policy] epoch #232 | Logging diagnostics...
2022-04-23 14:20:37 | [train_policy] epoch #232 | Optimizing policy...
2022-04-23 14:20:37 | [train_policy] epoch #232 | Computing loss before
2022-04-23 14:20:37 | [train_policy] epoch #232 | Computing KL before
2022-04-23 14:20:37 | [train_policy] epoch #232 | Optimizing
2022-04-23 14:20:37 | [train_policy] epoch #232 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:37 | [train_policy] epoch #232 | computing loss before
2022-04-23 14:20:37 | [train_policy] epoch #232 | computing gradient
2022-04-23 14:20:37 | [train_policy] epoch #232 | gradient computed
2022-04-23 14:20:37 | [train_policy] epoch #232 | computing descent direction
2022-04-23 14:20:37 | [train_policy] epoch #232 | descent direction computed
2022-04-23 14:20:37 | [train_policy] epoch #232 | backtrack iters: 1
2022-04-23 14:20:37 | [train_policy] epoch #232 | optimization finished
2022-04-23 14:20:37 | [train_policy] epoch #232 | Computing KL after
2022-04-23 14:20:37 | [train_policy] epoch #232 | Computing loss after
2022-04-23 14:20:37 | [train_policy] epoch #232 | Fitting baseline...
2022-04-23 14:20:37 | [train_policy] epoch #232 | Saving snapshot...
2022-04-23 14:20:37 | [train_policy] epoch #232 | Saved
2022-04-23 14:20:37 | [train_policy] epoch #232 | Time 84.85 s
2022-04-23 14:20:37 | [train_policy] epoch #232 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11766
Evaluation/AverageDiscountedReturn          -89.6555
Evaluation/AverageReturn                    -89.6555
Evaluation/CompletionRate                     0
Evaluation/Iteration                        232
Evaluation/MaxReturn                        -36.6718
Evaluation/MinReturn                      -2074.9
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.852
Extras/EpisodeRewardMean                    -86.1819
LinearFeatureBaseline/ExplainedVariance       0.191034
PolicyExecTime                                0.102209
ProcessExecTime                               0.0114534
TotalEnvSteps                            235796
policy/Entropy                                0.423773
policy/KL                                     0.00694708
policy/KLBefore                               0
policy/LossAfter                             -0.0202376
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             1.52771
policy/dLoss                                  0.0202376
---------------------------------------  ----------------
2022-04-23 14:20:37 | [train_policy] epoch #233 | Obtaining samples for iteration 233...
2022-04-23 14:20:37 | [train_policy] epoch #233 | Logging diagnostics...
2022-04-23 14:20:37 | [train_policy] epoch #233 | Optimizing policy...
2022-04-23 14:20:37 | [train_policy] epoch #233 | Computing loss before
2022-04-23 14:20:37 | [train_policy] epoch #233 | Computing KL before
2022-04-23 14:20:37 | [train_policy] epoch #233 | Optimizing
2022-04-23 14:20:37 | [train_policy] epoch #233 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:37 | [train_policy] epoch #233 | computing loss before
2022-04-23 14:20:37 | [train_policy] epoch #233 | computing gradient
2022-04-23 14:20:37 | [train_policy] epoch #233 | gradient computed
2022-04-23 14:20:37 | [train_policy] epoch #233 | computing descent direction
2022-04-23 14:20:37 | [train_policy] epoch #233 | descent direction computed
2022-04-23 14:20:37 | [train_policy] epoch #233 | backtrack iters: 1
2022-04-23 14:20:37 | [train_policy] epoch #233 | optimization finished
2022-04-23 14:20:37 | [train_policy] epoch #233 | Computing KL after
2022-04-23 14:20:37 | [train_policy] epoch #233 | Computing loss after
2022-04-23 14:20:37 | [train_policy] epoch #233 | Fitting baseline...
2022-04-23 14:20:37 | [train_policy] epoch #233 | Saving snapshot...
2022-04-23 14:20:37 | [train_policy] epoch #233 | Saved
2022-04-23 14:20:37 | [train_policy] epoch #233 | Time 85.20 s
2022-04-23 14:20:37 | [train_policy] epoch #233 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.1168
Evaluation/AverageDiscountedReturn          -45.412
Evaluation/AverageReturn                    -45.412
Evaluation/CompletionRate                     0
Evaluation/Iteration                        233
Evaluation/MaxReturn                        -34.1717
Evaluation/MinReturn                        -65.0085
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.88196
Extras/EpisodeRewardMean                    -45.7837
LinearFeatureBaseline/ExplainedVariance     -79.7756
PolicyExecTime                                0.10051
ProcessExecTime                               0.0114062
TotalEnvSteps                            236808
policy/Entropy                                0.421694
policy/KL                                     0.00650812
policy/KLBefore                               0
policy/LossAfter                             -0.0156415
policy/LossBefore                             4.35844e-08
policy/Perplexity                             1.52454
policy/dLoss                                  0.0156416
---------------------------------------  ----------------
2022-04-23 14:20:37 | [train_policy] epoch #234 | Obtaining samples for iteration 234...
2022-04-23 14:20:37 | [train_policy] epoch #234 | Logging diagnostics...
2022-04-23 14:20:37 | [train_policy] epoch #234 | Optimizing policy...
2022-04-23 14:20:37 | [train_policy] epoch #234 | Computing loss before
2022-04-23 14:20:37 | [train_policy] epoch #234 | Computing KL before
2022-04-23 14:20:37 | [train_policy] epoch #234 | Optimizing
2022-04-23 14:20:37 | [train_policy] epoch #234 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:37 | [train_policy] epoch #234 | computing loss before
2022-04-23 14:20:37 | [train_policy] epoch #234 | computing gradient
2022-04-23 14:20:37 | [train_policy] epoch #234 | gradient computed
2022-04-23 14:20:37 | [train_policy] epoch #234 | computing descent direction
2022-04-23 14:20:37 | [train_policy] epoch #234 | descent direction computed
2022-04-23 14:20:37 | [train_policy] epoch #234 | backtrack iters: 1
2022-04-23 14:20:37 | [train_policy] epoch #234 | optimization finished
2022-04-23 14:20:37 | [train_policy] epoch #234 | Computing KL after
2022-04-23 14:20:37 | [train_policy] epoch #234 | Computing loss after
2022-04-23 14:20:37 | [train_policy] epoch #234 | Fitting baseline...
2022-04-23 14:20:37 | [train_policy] epoch #234 | Saving snapshot...
2022-04-23 14:20:37 | [train_policy] epoch #234 | Saved
2022-04-23 14:20:37 | [train_policy] epoch #234 | Time 85.56 s
2022-04-23 14:20:37 | [train_policy] epoch #234 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.120072
Evaluation/AverageDiscountedReturn          -46.0108
Evaluation/AverageReturn                    -46.0108
Evaluation/CompletionRate                     0
Evaluation/Iteration                        234
Evaluation/MaxReturn                        -34.857
Evaluation/MinReturn                        -68.6793
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.7604
Extras/EpisodeRewardMean                    -45.6523
LinearFeatureBaseline/ExplainedVariance       0.931542
PolicyExecTime                                0.108685
ProcessExecTime                               0.0119169
TotalEnvSteps                            237820
policy/Entropy                                0.425235
policy/KL                                     0.00666047
policy/KLBefore                               0
policy/LossAfter                             -0.0161203
policy/LossBefore                             1.71982e-08
policy/Perplexity                             1.52995
policy/dLoss                                  0.0161203
---------------------------------------  ----------------
2022-04-23 14:20:37 | [train_policy] epoch #235 | Obtaining samples for iteration 235...
2022-04-23 14:20:38 | [train_policy] epoch #235 | Logging diagnostics...
2022-04-23 14:20:38 | [train_policy] epoch #235 | Optimizing policy...
2022-04-23 14:20:38 | [train_policy] epoch #235 | Computing loss before
2022-04-23 14:20:38 | [train_policy] epoch #235 | Computing KL before
2022-04-23 14:20:38 | [train_policy] epoch #235 | Optimizing
2022-04-23 14:20:38 | [train_policy] epoch #235 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:38 | [train_policy] epoch #235 | computing loss before
2022-04-23 14:20:38 | [train_policy] epoch #235 | computing gradient
2022-04-23 14:20:38 | [train_policy] epoch #235 | gradient computed
2022-04-23 14:20:38 | [train_policy] epoch #235 | computing descent direction
2022-04-23 14:20:38 | [train_policy] epoch #235 | descent direction computed
2022-04-23 14:20:38 | [train_policy] epoch #235 | backtrack iters: 0
2022-04-23 14:20:38 | [train_policy] epoch #235 | optimization finished
2022-04-23 14:20:38 | [train_policy] epoch #235 | Computing KL after
2022-04-23 14:20:38 | [train_policy] epoch #235 | Computing loss after
2022-04-23 14:20:38 | [train_policy] epoch #235 | Fitting baseline...
2022-04-23 14:20:38 | [train_policy] epoch #235 | Saving snapshot...
2022-04-23 14:20:38 | [train_policy] epoch #235 | Saved
2022-04-23 14:20:38 | [train_policy] epoch #235 | Time 85.91 s
2022-04-23 14:20:38 | [train_policy] epoch #235 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118871
Evaluation/AverageDiscountedReturn          -66.6214
Evaluation/AverageReturn                    -66.6214
Evaluation/CompletionRate                     0
Evaluation/Iteration                        235
Evaluation/MaxReturn                        -35.8277
Evaluation/MinReturn                      -2069.63
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.078
Extras/EpisodeRewardMean                    -64.7449
LinearFeatureBaseline/ExplainedVariance       0.0113147
PolicyExecTime                                0.106533
ProcessExecTime                               0.0115821
TotalEnvSteps                            238832
policy/Entropy                                0.445662
policy/KL                                     0.00982769
policy/KLBefore                               0
policy/LossAfter                             -0.0220698
policy/LossBefore                             1.17796e-08
policy/Perplexity                             1.56152
policy/dLoss                                  0.0220698
---------------------------------------  ----------------
2022-04-23 14:20:38 | [train_policy] epoch #236 | Obtaining samples for iteration 236...
2022-04-23 14:20:38 | [train_policy] epoch #236 | Logging diagnostics...
2022-04-23 14:20:38 | [train_policy] epoch #236 | Optimizing policy...
2022-04-23 14:20:38 | [train_policy] epoch #236 | Computing loss before
2022-04-23 14:20:38 | [train_policy] epoch #236 | Computing KL before
2022-04-23 14:20:38 | [train_policy] epoch #236 | Optimizing
2022-04-23 14:20:38 | [train_policy] epoch #236 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:38 | [train_policy] epoch #236 | computing loss before
2022-04-23 14:20:38 | [train_policy] epoch #236 | computing gradient
2022-04-23 14:20:38 | [train_policy] epoch #236 | gradient computed
2022-04-23 14:20:38 | [train_policy] epoch #236 | computing descent direction
2022-04-23 14:20:38 | [train_policy] epoch #236 | descent direction computed
2022-04-23 14:20:38 | [train_policy] epoch #236 | backtrack iters: 0
2022-04-23 14:20:38 | [train_policy] epoch #236 | optimization finished
2022-04-23 14:20:38 | [train_policy] epoch #236 | Computing KL after
2022-04-23 14:20:38 | [train_policy] epoch #236 | Computing loss after
2022-04-23 14:20:38 | [train_policy] epoch #236 | Fitting baseline...
2022-04-23 14:20:38 | [train_policy] epoch #236 | Saving snapshot...
2022-04-23 14:20:38 | [train_policy] epoch #236 | Saved
2022-04-23 14:20:38 | [train_policy] epoch #236 | Time 86.26 s
2022-04-23 14:20:38 | [train_policy] epoch #236 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118346
Evaluation/AverageDiscountedReturn          -68.4084
Evaluation/AverageReturn                    -68.4084
Evaluation/CompletionRate                     0
Evaluation/Iteration                        236
Evaluation/MaxReturn                        -35.9436
Evaluation/MinReturn                      -2064.31
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.382
Extras/EpisodeRewardMean                    -66.8868
LinearFeatureBaseline/ExplainedVariance       0.100929
PolicyExecTime                                0.100212
ProcessExecTime                               0.0114002
TotalEnvSteps                            239844
policy/Entropy                                0.448687
policy/KL                                     0.0086835
policy/KLBefore                               0
policy/LossAfter                             -0.0226623
policy/LossBefore                            -1.53134e-08
policy/Perplexity                             1.56625
policy/dLoss                                  0.0226623
---------------------------------------  ----------------
2022-04-23 14:20:38 | [train_policy] epoch #237 | Obtaining samples for iteration 237...
2022-04-23 14:20:38 | [train_policy] epoch #237 | Logging diagnostics...
2022-04-23 14:20:38 | [train_policy] epoch #237 | Optimizing policy...
2022-04-23 14:20:38 | [train_policy] epoch #237 | Computing loss before
2022-04-23 14:20:38 | [train_policy] epoch #237 | Computing KL before
2022-04-23 14:20:38 | [train_policy] epoch #237 | Optimizing
2022-04-23 14:20:38 | [train_policy] epoch #237 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:38 | [train_policy] epoch #237 | computing loss before
2022-04-23 14:20:38 | [train_policy] epoch #237 | computing gradient
2022-04-23 14:20:38 | [train_policy] epoch #237 | gradient computed
2022-04-23 14:20:38 | [train_policy] epoch #237 | computing descent direction
2022-04-23 14:20:38 | [train_policy] epoch #237 | descent direction computed
2022-04-23 14:20:38 | [train_policy] epoch #237 | backtrack iters: 0
2022-04-23 14:20:38 | [train_policy] epoch #237 | optimization finished
2022-04-23 14:20:38 | [train_policy] epoch #237 | Computing KL after
2022-04-23 14:20:38 | [train_policy] epoch #237 | Computing loss after
2022-04-23 14:20:38 | [train_policy] epoch #237 | Fitting baseline...
2022-04-23 14:20:38 | [train_policy] epoch #237 | Saving snapshot...
2022-04-23 14:20:38 | [train_policy] epoch #237 | Saved
2022-04-23 14:20:38 | [train_policy] epoch #237 | Time 86.61 s
2022-04-23 14:20:38 | [train_policy] epoch #237 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.119388
Evaluation/AverageDiscountedReturn          -66.6569
Evaluation/AverageReturn                    -66.6569
Evaluation/CompletionRate                     0
Evaluation/Iteration                        237
Evaluation/MaxReturn                        -36.2596
Evaluation/MinReturn                      -2060.89
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.15
Extras/EpisodeRewardMean                    -65.3625
LinearFeatureBaseline/ExplainedVariance      -0.102531
PolicyExecTime                                0.104687
ProcessExecTime                               0.0118661
TotalEnvSteps                            240856
policy/Entropy                                0.483451
policy/KL                                     0.0097354
policy/KLBefore                               0
policy/LossAfter                             -0.0249568
policy/LossBefore                             5.6542e-09
policy/Perplexity                             1.62166
policy/dLoss                                  0.0249568
---------------------------------------  ---------------
2022-04-23 14:20:38 | [train_policy] epoch #238 | Obtaining samples for iteration 238...
2022-04-23 14:20:39 | [train_policy] epoch #238 | Logging diagnostics...
2022-04-23 14:20:39 | [train_policy] epoch #238 | Optimizing policy...
2022-04-23 14:20:39 | [train_policy] epoch #238 | Computing loss before
2022-04-23 14:20:39 | [train_policy] epoch #238 | Computing KL before
2022-04-23 14:20:39 | [train_policy] epoch #238 | Optimizing
2022-04-23 14:20:39 | [train_policy] epoch #238 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:39 | [train_policy] epoch #238 | computing loss before
2022-04-23 14:20:39 | [train_policy] epoch #238 | computing gradient
2022-04-23 14:20:39 | [train_policy] epoch #238 | gradient computed
2022-04-23 14:20:39 | [train_policy] epoch #238 | computing descent direction
2022-04-23 14:20:39 | [train_policy] epoch #238 | descent direction computed
2022-04-23 14:20:39 | [train_policy] epoch #238 | backtrack iters: 1
2022-04-23 14:20:39 | [train_policy] epoch #238 | optimization finished
2022-04-23 14:20:39 | [train_policy] epoch #238 | Computing KL after
2022-04-23 14:20:39 | [train_policy] epoch #238 | Computing loss after
2022-04-23 14:20:39 | [train_policy] epoch #238 | Fitting baseline...
2022-04-23 14:20:39 | [train_policy] epoch #238 | Saving snapshot...
2022-04-23 14:20:39 | [train_policy] epoch #238 | Saved
2022-04-23 14:20:39 | [train_policy] epoch #238 | Time 86.97 s
2022-04-23 14:20:39 | [train_policy] epoch #238 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.118909
Evaluation/AverageDiscountedReturn          -67.8235
Evaluation/AverageReturn                    -67.8235
Evaluation/CompletionRate                     0
Evaluation/Iteration                        238
Evaluation/MaxReturn                        -35.802
Evaluation/MinReturn                      -2063.14
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.287
Extras/EpisodeRewardMean                    -65.9969
LinearFeatureBaseline/ExplainedVariance       0.0747955
PolicyExecTime                                0.105521
ProcessExecTime                               0.0120802
TotalEnvSteps                            241868
policy/Entropy                                0.483757
policy/KL                                     0.00961957
policy/KLBefore                               0
policy/LossAfter                             -0.0128879
policy/LossBefore                            -0
policy/Perplexity                             1.62216
policy/dLoss                                  0.0128879
---------------------------------------  ---------------
2022-04-23 14:20:39 | [train_policy] epoch #239 | Obtaining samples for iteration 239...
2022-04-23 14:20:39 | [train_policy] epoch #239 | Logging diagnostics...
2022-04-23 14:20:39 | [train_policy] epoch #239 | Optimizing policy...
2022-04-23 14:20:39 | [train_policy] epoch #239 | Computing loss before
2022-04-23 14:20:39 | [train_policy] epoch #239 | Computing KL before
2022-04-23 14:20:39 | [train_policy] epoch #239 | Optimizing
2022-04-23 14:20:39 | [train_policy] epoch #239 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:39 | [train_policy] epoch #239 | computing loss before
2022-04-23 14:20:39 | [train_policy] epoch #239 | computing gradient
2022-04-23 14:20:39 | [train_policy] epoch #239 | gradient computed
2022-04-23 14:20:39 | [train_policy] epoch #239 | computing descent direction
2022-04-23 14:20:39 | [train_policy] epoch #239 | descent direction computed
2022-04-23 14:20:39 | [train_policy] epoch #239 | backtrack iters: 1
2022-04-23 14:20:39 | [train_policy] epoch #239 | optimization finished
2022-04-23 14:20:39 | [train_policy] epoch #239 | Computing KL after
2022-04-23 14:20:39 | [train_policy] epoch #239 | Computing loss after
2022-04-23 14:20:39 | [train_policy] epoch #239 | Fitting baseline...
2022-04-23 14:20:39 | [train_policy] epoch #239 | Saving snapshot...
2022-04-23 14:20:39 | [train_policy] epoch #239 | Saved
2022-04-23 14:20:39 | [train_policy] epoch #239 | Time 87.34 s
2022-04-23 14:20:39 | [train_policy] epoch #239 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.120972
Evaluation/AverageDiscountedReturn          -44.4042
Evaluation/AverageReturn                    -44.4042
Evaluation/CompletionRate                     0
Evaluation/Iteration                        239
Evaluation/MaxReturn                        -35.3849
Evaluation/MinReturn                        -64.6509
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.98257
Extras/EpisodeRewardMean                    -44.3657
LinearFeatureBaseline/ExplainedVariance     -20.1136
PolicyExecTime                                0.108136
ProcessExecTime                               0.0125308
TotalEnvSteps                            242880
policy/Entropy                                0.47556
policy/KL                                     0.00643585
policy/KLBefore                               0
policy/LossAfter                             -0.0194697
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             1.60892
policy/dLoss                                  0.0194697
---------------------------------------  ----------------
2022-04-23 14:20:39 | [train_policy] epoch #240 | Obtaining samples for iteration 240...
2022-04-23 14:20:39 | [train_policy] epoch #240 | Logging diagnostics...
2022-04-23 14:20:39 | [train_policy] epoch #240 | Optimizing policy...
2022-04-23 14:20:39 | [train_policy] epoch #240 | Computing loss before
2022-04-23 14:20:39 | [train_policy] epoch #240 | Computing KL before
2022-04-23 14:20:39 | [train_policy] epoch #240 | Optimizing
2022-04-23 14:20:39 | [train_policy] epoch #240 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:39 | [train_policy] epoch #240 | computing loss before
2022-04-23 14:20:39 | [train_policy] epoch #240 | computing gradient
2022-04-23 14:20:39 | [train_policy] epoch #240 | gradient computed
2022-04-23 14:20:39 | [train_policy] epoch #240 | computing descent direction
2022-04-23 14:20:39 | [train_policy] epoch #240 | descent direction computed
2022-04-23 14:20:39 | [train_policy] epoch #240 | backtrack iters: 1
2022-04-23 14:20:39 | [train_policy] epoch #240 | optimization finished
2022-04-23 14:20:39 | [train_policy] epoch #240 | Computing KL after
2022-04-23 14:20:39 | [train_policy] epoch #240 | Computing loss after
2022-04-23 14:20:39 | [train_policy] epoch #240 | Fitting baseline...
2022-04-23 14:20:39 | [train_policy] epoch #240 | Saving snapshot...
2022-04-23 14:20:39 | [train_policy] epoch #240 | Saved
2022-04-23 14:20:39 | [train_policy] epoch #240 | Time 87.70 s
2022-04-23 14:20:39 | [train_policy] epoch #240 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.120484
Evaluation/AverageDiscountedReturn          -44.3892
Evaluation/AverageReturn                    -44.3892
Evaluation/CompletionRate                     0
Evaluation/Iteration                        240
Evaluation/MaxReturn                        -34.3723
Evaluation/MinReturn                        -62.8282
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.7329
Extras/EpisodeRewardMean                    -44.404
LinearFeatureBaseline/ExplainedVariance       0.926592
PolicyExecTime                                0.112843
ProcessExecTime                               0.0125589
TotalEnvSteps                            243892
policy/Entropy                                0.499002
policy/KL                                     0.00646555
policy/KLBefore                               0
policy/LossAfter                             -0.0211419
policy/LossBefore                            -1.83761e-08
policy/Perplexity                             1.64708
policy/dLoss                                  0.0211419
---------------------------------------  ----------------
2022-04-23 14:20:39 | [train_policy] epoch #241 | Obtaining samples for iteration 241...
2022-04-23 14:20:40 | [train_policy] epoch #241 | Logging diagnostics...
2022-04-23 14:20:40 | [train_policy] epoch #241 | Optimizing policy...
2022-04-23 14:20:40 | [train_policy] epoch #241 | Computing loss before
2022-04-23 14:20:40 | [train_policy] epoch #241 | Computing KL before
2022-04-23 14:20:40 | [train_policy] epoch #241 | Optimizing
2022-04-23 14:20:40 | [train_policy] epoch #241 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:40 | [train_policy] epoch #241 | computing loss before
2022-04-23 14:20:40 | [train_policy] epoch #241 | computing gradient
2022-04-23 14:20:40 | [train_policy] epoch #241 | gradient computed
2022-04-23 14:20:40 | [train_policy] epoch #241 | computing descent direction
2022-04-23 14:20:40 | [train_policy] epoch #241 | descent direction computed
2022-04-23 14:20:40 | [train_policy] epoch #241 | backtrack iters: 1
2022-04-23 14:20:40 | [train_policy] epoch #241 | optimization finished
2022-04-23 14:20:40 | [train_policy] epoch #241 | Computing KL after
2022-04-23 14:20:40 | [train_policy] epoch #241 | Computing loss after
2022-04-23 14:20:40 | [train_policy] epoch #241 | Fitting baseline...
2022-04-23 14:20:40 | [train_policy] epoch #241 | Saving snapshot...
2022-04-23 14:20:40 | [train_policy] epoch #241 | Saved
2022-04-23 14:20:40 | [train_policy] epoch #241 | Time 88.06 s
2022-04-23 14:20:40 | [train_policy] epoch #241 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117966
Evaluation/AverageDiscountedReturn          -66.2788
Evaluation/AverageReturn                    -66.2788
Evaluation/CompletionRate                     0
Evaluation/Iteration                        241
Evaluation/MaxReturn                        -33.1603
Evaluation/MinReturn                      -2064.01
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.52
Extras/EpisodeRewardMean                    -64.5042
LinearFeatureBaseline/ExplainedVariance       0.01018
PolicyExecTime                                0.10573
ProcessExecTime                               0.0117567
TotalEnvSteps                            244904
policy/Entropy                                0.498987
policy/KL                                     0.00767587
policy/KLBefore                               0
policy/LossAfter                             -0.0206863
policy/LossBefore                             8.95248e-09
policy/Perplexity                             1.64705
policy/dLoss                                  0.0206863
---------------------------------------  ----------------
2022-04-23 14:20:40 | [train_policy] epoch #242 | Obtaining samples for iteration 242...
2022-04-23 14:20:40 | [train_policy] epoch #242 | Logging diagnostics...
2022-04-23 14:20:40 | [train_policy] epoch #242 | Optimizing policy...
2022-04-23 14:20:40 | [train_policy] epoch #242 | Computing loss before
2022-04-23 14:20:40 | [train_policy] epoch #242 | Computing KL before
2022-04-23 14:20:40 | [train_policy] epoch #242 | Optimizing
2022-04-23 14:20:40 | [train_policy] epoch #242 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:40 | [train_policy] epoch #242 | computing loss before
2022-04-23 14:20:40 | [train_policy] epoch #242 | computing gradient
2022-04-23 14:20:40 | [train_policy] epoch #242 | gradient computed
2022-04-23 14:20:40 | [train_policy] epoch #242 | computing descent direction
2022-04-23 14:20:40 | [train_policy] epoch #242 | descent direction computed
2022-04-23 14:20:40 | [train_policy] epoch #242 | backtrack iters: 1
2022-04-23 14:20:40 | [train_policy] epoch #242 | optimization finished
2022-04-23 14:20:40 | [train_policy] epoch #242 | Computing KL after
2022-04-23 14:20:40 | [train_policy] epoch #242 | Computing loss after
2022-04-23 14:20:40 | [train_policy] epoch #242 | Fitting baseline...
2022-04-23 14:20:40 | [train_policy] epoch #242 | Saving snapshot...
2022-04-23 14:20:40 | [train_policy] epoch #242 | Saved
2022-04-23 14:20:40 | [train_policy] epoch #242 | Time 88.42 s
2022-04-23 14:20:40 | [train_policy] epoch #242 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118513
Evaluation/AverageDiscountedReturn          -44.8442
Evaluation/AverageReturn                    -44.8442
Evaluation/CompletionRate                     0
Evaluation/Iteration                        242
Evaluation/MaxReturn                        -34.7108
Evaluation/MinReturn                        -60.3718
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.9008
Extras/EpisodeRewardMean                    -44.6681
LinearFeatureBaseline/ExplainedVariance     -40.4693
PolicyExecTime                                0.106401
ProcessExecTime                               0.0113125
TotalEnvSteps                            245916
policy/Entropy                                0.48994
policy/KL                                     0.00779959
policy/KLBefore                               0
policy/LossAfter                             -0.0144019
policy/LossBefore                            -5.06522e-08
policy/Perplexity                             1.63222
policy/dLoss                                  0.0144018
---------------------------------------  ----------------
2022-04-23 14:20:40 | [train_policy] epoch #243 | Obtaining samples for iteration 243...
2022-04-23 14:20:40 | [train_policy] epoch #243 | Logging diagnostics...
2022-04-23 14:20:40 | [train_policy] epoch #243 | Optimizing policy...
2022-04-23 14:20:40 | [train_policy] epoch #243 | Computing loss before
2022-04-23 14:20:40 | [train_policy] epoch #243 | Computing KL before
2022-04-23 14:20:40 | [train_policy] epoch #243 | Optimizing
2022-04-23 14:20:40 | [train_policy] epoch #243 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:40 | [train_policy] epoch #243 | computing loss before
2022-04-23 14:20:40 | [train_policy] epoch #243 | computing gradient
2022-04-23 14:20:40 | [train_policy] epoch #243 | gradient computed
2022-04-23 14:20:40 | [train_policy] epoch #243 | computing descent direction
2022-04-23 14:20:41 | [train_policy] epoch #243 | descent direction computed
2022-04-23 14:20:41 | [train_policy] epoch #243 | backtrack iters: 1
2022-04-23 14:20:41 | [train_policy] epoch #243 | optimization finished
2022-04-23 14:20:41 | [train_policy] epoch #243 | Computing KL after
2022-04-23 14:20:41 | [train_policy] epoch #243 | Computing loss after
2022-04-23 14:20:41 | [train_policy] epoch #243 | Fitting baseline...
2022-04-23 14:20:41 | [train_policy] epoch #243 | Saving snapshot...
2022-04-23 14:20:41 | [train_policy] epoch #243 | Saved
2022-04-23 14:20:41 | [train_policy] epoch #243 | Time 88.77 s
2022-04-23 14:20:41 | [train_policy] epoch #243 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118409
Evaluation/AverageDiscountedReturn          -44.9341
Evaluation/AverageReturn                    -44.9341
Evaluation/CompletionRate                     0
Evaluation/Iteration                        243
Evaluation/MaxReturn                        -35.6095
Evaluation/MinReturn                        -64.7322
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.75178
Extras/EpisodeRewardMean                    -44.8273
LinearFeatureBaseline/ExplainedVariance       0.935425
PolicyExecTime                                0.104605
ProcessExecTime                               0.0113397
TotalEnvSteps                            246928
policy/Entropy                                0.472593
policy/KL                                     0.00649952
policy/KLBefore                               0
policy/LossAfter                             -0.0124611
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             1.60415
policy/dLoss                                  0.0124611
---------------------------------------  ----------------
2022-04-23 14:20:41 | [train_policy] epoch #244 | Obtaining samples for iteration 244...
2022-04-23 14:20:41 | [train_policy] epoch #244 | Logging diagnostics...
2022-04-23 14:20:41 | [train_policy] epoch #244 | Optimizing policy...
2022-04-23 14:20:41 | [train_policy] epoch #244 | Computing loss before
2022-04-23 14:20:41 | [train_policy] epoch #244 | Computing KL before
2022-04-23 14:20:41 | [train_policy] epoch #244 | Optimizing
2022-04-23 14:20:41 | [train_policy] epoch #244 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:41 | [train_policy] epoch #244 | computing loss before
2022-04-23 14:20:41 | [train_policy] epoch #244 | computing gradient
2022-04-23 14:20:41 | [train_policy] epoch #244 | gradient computed
2022-04-23 14:20:41 | [train_policy] epoch #244 | computing descent direction
2022-04-23 14:20:41 | [train_policy] epoch #244 | descent direction computed
2022-04-23 14:20:41 | [train_policy] epoch #244 | backtrack iters: 1
2022-04-23 14:20:41 | [train_policy] epoch #244 | optimization finished
2022-04-23 14:20:41 | [train_policy] epoch #244 | Computing KL after
2022-04-23 14:20:41 | [train_policy] epoch #244 | Computing loss after
2022-04-23 14:20:41 | [train_policy] epoch #244 | Fitting baseline...
2022-04-23 14:20:41 | [train_policy] epoch #244 | Saving snapshot...
2022-04-23 14:20:41 | [train_policy] epoch #244 | Saved
2022-04-23 14:20:41 | [train_policy] epoch #244 | Time 89.12 s
2022-04-23 14:20:41 | [train_policy] epoch #244 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118548
Evaluation/AverageDiscountedReturn          -44.8049
Evaluation/AverageReturn                    -44.8049
Evaluation/CompletionRate                     0
Evaluation/Iteration                        244
Evaluation/MaxReturn                        -35.3058
Evaluation/MinReturn                        -98.0515
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12451
Extras/EpisodeRewardMean                    -44.6769
LinearFeatureBaseline/ExplainedVariance       0.922191
PolicyExecTime                                0.100794
ProcessExecTime                               0.0119061
TotalEnvSteps                            247940
policy/Entropy                                0.455258
policy/KL                                     0.00700272
policy/KLBefore                               0
policy/LossAfter                             -0.0298898
policy/LossBefore                             1.18974e-08
policy/Perplexity                             1.57658
policy/dLoss                                  0.0298898
---------------------------------------  ----------------
2022-04-23 14:20:41 | [train_policy] epoch #245 | Obtaining samples for iteration 245...
2022-04-23 14:20:41 | [train_policy] epoch #245 | Logging diagnostics...
2022-04-23 14:20:41 | [train_policy] epoch #245 | Optimizing policy...
2022-04-23 14:20:41 | [train_policy] epoch #245 | Computing loss before
2022-04-23 14:20:41 | [train_policy] epoch #245 | Computing KL before
2022-04-23 14:20:41 | [train_policy] epoch #245 | Optimizing
2022-04-23 14:20:41 | [train_policy] epoch #245 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:41 | [train_policy] epoch #245 | computing loss before
2022-04-23 14:20:41 | [train_policy] epoch #245 | computing gradient
2022-04-23 14:20:41 | [train_policy] epoch #245 | gradient computed
2022-04-23 14:20:41 | [train_policy] epoch #245 | computing descent direction
2022-04-23 14:20:41 | [train_policy] epoch #245 | descent direction computed
2022-04-23 14:20:41 | [train_policy] epoch #245 | backtrack iters: 0
2022-04-23 14:20:41 | [train_policy] epoch #245 | optimization finished
2022-04-23 14:20:41 | [train_policy] epoch #245 | Computing KL after
2022-04-23 14:20:41 | [train_policy] epoch #245 | Computing loss after
2022-04-23 14:20:41 | [train_policy] epoch #245 | Fitting baseline...
2022-04-23 14:20:41 | [train_policy] epoch #245 | Saving snapshot...
2022-04-23 14:20:41 | [train_policy] epoch #245 | Saved
2022-04-23 14:20:41 | [train_policy] epoch #245 | Time 89.46 s
2022-04-23 14:20:41 | [train_policy] epoch #245 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115778
Evaluation/AverageDiscountedReturn          -66.4166
Evaluation/AverageReturn                    -66.4166
Evaluation/CompletionRate                     0
Evaluation/Iteration                        245
Evaluation/MaxReturn                        -34.6163
Evaluation/MinReturn                      -2053.51
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.444
Extras/EpisodeRewardMean                    -64.6196
LinearFeatureBaseline/ExplainedVariance       0.00934309
PolicyExecTime                                0.102635
ProcessExecTime                               0.0112541
TotalEnvSteps                            248952
policy/Entropy                                0.449064
policy/KL                                     0.00926734
policy/KLBefore                               0
policy/LossAfter                             -0.0359267
policy/LossBefore                             6.59656e-09
policy/Perplexity                             1.56685
policy/dLoss                                  0.0359268
---------------------------------------  ----------------
2022-04-23 14:20:41 | [train_policy] epoch #246 | Obtaining samples for iteration 246...
2022-04-23 14:20:42 | [train_policy] epoch #246 | Logging diagnostics...
2022-04-23 14:20:42 | [train_policy] epoch #246 | Optimizing policy...
2022-04-23 14:20:42 | [train_policy] epoch #246 | Computing loss before
2022-04-23 14:20:42 | [train_policy] epoch #246 | Computing KL before
2022-04-23 14:20:42 | [train_policy] epoch #246 | Optimizing
2022-04-23 14:20:42 | [train_policy] epoch #246 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:42 | [train_policy] epoch #246 | computing loss before
2022-04-23 14:20:42 | [train_policy] epoch #246 | computing gradient
2022-04-23 14:20:42 | [train_policy] epoch #246 | gradient computed
2022-04-23 14:20:42 | [train_policy] epoch #246 | computing descent direction
2022-04-23 14:20:42 | [train_policy] epoch #246 | descent direction computed
2022-04-23 14:20:42 | [train_policy] epoch #246 | backtrack iters: 1
2022-04-23 14:20:42 | [train_policy] epoch #246 | optimization finished
2022-04-23 14:20:42 | [train_policy] epoch #246 | Computing KL after
2022-04-23 14:20:42 | [train_policy] epoch #246 | Computing loss after
2022-04-23 14:20:42 | [train_policy] epoch #246 | Fitting baseline...
2022-04-23 14:20:42 | [train_policy] epoch #246 | Saving snapshot...
2022-04-23 14:20:42 | [train_policy] epoch #246 | Saved
2022-04-23 14:20:42 | [train_policy] epoch #246 | Time 89.82 s
2022-04-23 14:20:42 | [train_policy] epoch #246 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.116653
Evaluation/AverageDiscountedReturn          -67.2477
Evaluation/AverageReturn                    -67.2477
Evaluation/CompletionRate                     0
Evaluation/Iteration                        246
Evaluation/MaxReturn                        -34.1092
Evaluation/MinReturn                      -2100.21
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        213.256
Extras/EpisodeRewardMean                    -65.1267
LinearFeatureBaseline/ExplainedVariance       0.0875993
PolicyExecTime                                0.110323
ProcessExecTime                               0.0113971
TotalEnvSteps                            249964
policy/Entropy                                0.413784
policy/KL                                     0.00643923
policy/KLBefore                               0
policy/LossAfter                             -0.0237108
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             1.51253
policy/dLoss                                  0.0237108
---------------------------------------  ---------------
2022-04-23 14:20:42 | [train_policy] epoch #247 | Obtaining samples for iteration 247...
2022-04-23 14:20:42 | [train_policy] epoch #247 | Logging diagnostics...
2022-04-23 14:20:42 | [train_policy] epoch #247 | Optimizing policy...
2022-04-23 14:20:42 | [train_policy] epoch #247 | Computing loss before
2022-04-23 14:20:42 | [train_policy] epoch #247 | Computing KL before
2022-04-23 14:20:42 | [train_policy] epoch #247 | Optimizing
2022-04-23 14:20:42 | [train_policy] epoch #247 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:42 | [train_policy] epoch #247 | computing loss before
2022-04-23 14:20:42 | [train_policy] epoch #247 | computing gradient
2022-04-23 14:20:42 | [train_policy] epoch #247 | gradient computed
2022-04-23 14:20:42 | [train_policy] epoch #247 | computing descent direction
2022-04-23 14:20:42 | [train_policy] epoch #247 | descent direction computed
2022-04-23 14:20:42 | [train_policy] epoch #247 | backtrack iters: 1
2022-04-23 14:20:42 | [train_policy] epoch #247 | optimization finished
2022-04-23 14:20:42 | [train_policy] epoch #247 | Computing KL after
2022-04-23 14:20:42 | [train_policy] epoch #247 | Computing loss after
2022-04-23 14:20:42 | [train_policy] epoch #247 | Fitting baseline...
2022-04-23 14:20:42 | [train_policy] epoch #247 | Saving snapshot...
2022-04-23 14:20:42 | [train_policy] epoch #247 | Saved
2022-04-23 14:20:42 | [train_policy] epoch #247 | Time 90.17 s
2022-04-23 14:20:42 | [train_policy] epoch #247 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118323
Evaluation/AverageDiscountedReturn          -67.2327
Evaluation/AverageReturn                    -67.2327
Evaluation/CompletionRate                     0
Evaluation/Iteration                        247
Evaluation/MaxReturn                        -36.4073
Evaluation/MinReturn                      -2063.9
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.386
Extras/EpisodeRewardMean                    -65.6501
LinearFeatureBaseline/ExplainedVariance       0.00726264
PolicyExecTime                                0.107702
ProcessExecTime                               0.0113995
TotalEnvSteps                            250976
policy/Entropy                                0.421639
policy/KL                                     0.00676535
policy/KLBefore                               0
policy/LossAfter                             -0.0158604
policy/LossBefore                             3.29828e-09
policy/Perplexity                             1.52446
policy/dLoss                                  0.0158604
---------------------------------------  ----------------
2022-04-23 14:20:42 | [train_policy] epoch #248 | Obtaining samples for iteration 248...
2022-04-23 14:20:42 | [train_policy] epoch #248 | Logging diagnostics...
2022-04-23 14:20:42 | [train_policy] epoch #248 | Optimizing policy...
2022-04-23 14:20:42 | [train_policy] epoch #248 | Computing loss before
2022-04-23 14:20:42 | [train_policy] epoch #248 | Computing KL before
2022-04-23 14:20:42 | [train_policy] epoch #248 | Optimizing
2022-04-23 14:20:42 | [train_policy] epoch #248 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:42 | [train_policy] epoch #248 | computing loss before
2022-04-23 14:20:42 | [train_policy] epoch #248 | computing gradient
2022-04-23 14:20:42 | [train_policy] epoch #248 | gradient computed
2022-04-23 14:20:42 | [train_policy] epoch #248 | computing descent direction
2022-04-23 14:20:42 | [train_policy] epoch #248 | descent direction computed
2022-04-23 14:20:42 | [train_policy] epoch #248 | backtrack iters: 1
2022-04-23 14:20:42 | [train_policy] epoch #248 | optimization finished
2022-04-23 14:20:42 | [train_policy] epoch #248 | Computing KL after
2022-04-23 14:20:42 | [train_policy] epoch #248 | Computing loss after
2022-04-23 14:20:42 | [train_policy] epoch #248 | Fitting baseline...
2022-04-23 14:20:42 | [train_policy] epoch #248 | Saving snapshot...
2022-04-23 14:20:42 | [train_policy] epoch #248 | Saved
2022-04-23 14:20:42 | [train_policy] epoch #248 | Time 90.54 s
2022-04-23 14:20:42 | [train_policy] epoch #248 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.129148
Evaluation/AverageDiscountedReturn          -44.7474
Evaluation/AverageReturn                    -44.7474
Evaluation/CompletionRate                     0
Evaluation/Iteration                        248
Evaluation/MaxReturn                        -33.7557
Evaluation/MinReturn                        -82.4432
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.81975
Extras/EpisodeRewardMean                    -44.7317
LinearFeatureBaseline/ExplainedVariance     -33.7223
PolicyExecTime                                0.109764
ProcessExecTime                               0.0128362
TotalEnvSteps                            251988
policy/Entropy                                0.438525
policy/KL                                     0.00710009
policy/KLBefore                               0
policy/LossAfter                             -0.0302441
policy/LossBefore                             3.67523e-08
policy/Perplexity                             1.55042
policy/dLoss                                  0.0302442
---------------------------------------  ----------------
2022-04-23 14:20:42 | [train_policy] epoch #249 | Obtaining samples for iteration 249...
2022-04-23 14:20:43 | [train_policy] epoch #249 | Logging diagnostics...
2022-04-23 14:20:43 | [train_policy] epoch #249 | Optimizing policy...
2022-04-23 14:20:43 | [train_policy] epoch #249 | Computing loss before
2022-04-23 14:20:43 | [train_policy] epoch #249 | Computing KL before
2022-04-23 14:20:43 | [train_policy] epoch #249 | Optimizing
2022-04-23 14:20:43 | [train_policy] epoch #249 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:43 | [train_policy] epoch #249 | computing loss before
2022-04-23 14:20:43 | [train_policy] epoch #249 | computing gradient
2022-04-23 14:20:43 | [train_policy] epoch #249 | gradient computed
2022-04-23 14:20:43 | [train_policy] epoch #249 | computing descent direction
2022-04-23 14:20:43 | [train_policy] epoch #249 | descent direction computed
2022-04-23 14:20:43 | [train_policy] epoch #249 | backtrack iters: 0
2022-04-23 14:20:43 | [train_policy] epoch #249 | optimization finished
2022-04-23 14:20:43 | [train_policy] epoch #249 | Computing KL after
2022-04-23 14:20:43 | [train_policy] epoch #249 | Computing loss after
2022-04-23 14:20:43 | [train_policy] epoch #249 | Fitting baseline...
2022-04-23 14:20:43 | [train_policy] epoch #249 | Saving snapshot...
2022-04-23 14:20:43 | [train_policy] epoch #249 | Saved
2022-04-23 14:20:43 | [train_policy] epoch #249 | Time 90.89 s
2022-04-23 14:20:43 | [train_policy] epoch #249 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118927
Evaluation/AverageDiscountedReturn          -67.7463
Evaluation/AverageReturn                    -67.7463
Evaluation/CompletionRate                     0
Evaluation/Iteration                        249
Evaluation/MaxReturn                        -31.5501
Evaluation/MinReturn                      -2066.23
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.802
Extras/EpisodeRewardMean                    -65.6753
LinearFeatureBaseline/ExplainedVariance       0.011986
PolicyExecTime                                0.104892
ProcessExecTime                               0.0114412
TotalEnvSteps                            253000
policy/Entropy                                0.459507
policy/KL                                     0.009871
policy/KLBefore                               0
policy/LossAfter                             -0.0185972
policy/LossBefore                             1.46067e-08
policy/Perplexity                             1.58329
policy/dLoss                                  0.0185973
---------------------------------------  ----------------
2022-04-23 14:20:43 | [train_policy] epoch #250 | Obtaining samples for iteration 250...
2022-04-23 14:20:43 | [train_policy] epoch #250 | Logging diagnostics...
2022-04-23 14:20:43 | [train_policy] epoch #250 | Optimizing policy...
2022-04-23 14:20:43 | [train_policy] epoch #250 | Computing loss before
2022-04-23 14:20:43 | [train_policy] epoch #250 | Computing KL before
2022-04-23 14:20:43 | [train_policy] epoch #250 | Optimizing
2022-04-23 14:20:43 | [train_policy] epoch #250 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:43 | [train_policy] epoch #250 | computing loss before
2022-04-23 14:20:43 | [train_policy] epoch #250 | computing gradient
2022-04-23 14:20:43 | [train_policy] epoch #250 | gradient computed
2022-04-23 14:20:43 | [train_policy] epoch #250 | computing descent direction
2022-04-23 14:20:43 | [train_policy] epoch #250 | descent direction computed
2022-04-23 14:20:43 | [train_policy] epoch #250 | backtrack iters: 0
2022-04-23 14:20:43 | [train_policy] epoch #250 | optimization finished
2022-04-23 14:20:43 | [train_policy] epoch #250 | Computing KL after
2022-04-23 14:20:43 | [train_policy] epoch #250 | Computing loss after
2022-04-23 14:20:43 | [train_policy] epoch #250 | Fitting baseline...
2022-04-23 14:20:43 | [train_policy] epoch #250 | Saving snapshot...
2022-04-23 14:20:43 | [train_policy] epoch #250 | Saved
2022-04-23 14:20:43 | [train_policy] epoch #250 | Time 91.25 s
2022-04-23 14:20:43 | [train_policy] epoch #250 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.124296
Evaluation/AverageDiscountedReturn          -66.605
Evaluation/AverageReturn                    -66.605
Evaluation/CompletionRate                     0
Evaluation/Iteration                        250
Evaluation/MaxReturn                        -32.4762
Evaluation/MinReturn                      -2069.45
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.203
Extras/EpisodeRewardMean                    -64.5784
LinearFeatureBaseline/ExplainedVariance       0.118788
PolicyExecTime                                0.104541
ProcessExecTime                               0.0124469
TotalEnvSteps                            254012
policy/Entropy                                0.447778
policy/KL                                     0.00922959
policy/KLBefore                               0
policy/LossAfter                             -0.0225786
policy/LossBefore                             1.13084e-08
policy/Perplexity                             1.56483
policy/dLoss                                  0.0225786
---------------------------------------  ----------------
2022-04-23 14:20:43 | [train_policy] epoch #251 | Obtaining samples for iteration 251...
2022-04-23 14:20:43 | [train_policy] epoch #251 | Logging diagnostics...
2022-04-23 14:20:43 | [train_policy] epoch #251 | Optimizing policy...
2022-04-23 14:20:43 | [train_policy] epoch #251 | Computing loss before
2022-04-23 14:20:43 | [train_policy] epoch #251 | Computing KL before
2022-04-23 14:20:43 | [train_policy] epoch #251 | Optimizing
2022-04-23 14:20:43 | [train_policy] epoch #251 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:43 | [train_policy] epoch #251 | computing loss before
2022-04-23 14:20:43 | [train_policy] epoch #251 | computing gradient
2022-04-23 14:20:43 | [train_policy] epoch #251 | gradient computed
2022-04-23 14:20:43 | [train_policy] epoch #251 | computing descent direction
2022-04-23 14:20:43 | [train_policy] epoch #251 | descent direction computed
2022-04-23 14:20:43 | [train_policy] epoch #251 | backtrack iters: 1
2022-04-23 14:20:43 | [train_policy] epoch #251 | optimization finished
2022-04-23 14:20:43 | [train_policy] epoch #251 | Computing KL after
2022-04-23 14:20:43 | [train_policy] epoch #251 | Computing loss after
2022-04-23 14:20:43 | [train_policy] epoch #251 | Fitting baseline...
2022-04-23 14:20:43 | [train_policy] epoch #251 | Saving snapshot...
2022-04-23 14:20:43 | [train_policy] epoch #251 | Saved
2022-04-23 14:20:43 | [train_policy] epoch #251 | Time 91.59 s
2022-04-23 14:20:43 | [train_policy] epoch #251 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119189
Evaluation/AverageDiscountedReturn          -44.082
Evaluation/AverageReturn                    -44.082
Evaluation/CompletionRate                     0
Evaluation/Iteration                        251
Evaluation/MaxReturn                        -32.417
Evaluation/MinReturn                        -65.7231
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.25021
Extras/EpisodeRewardMean                    -44.201
LinearFeatureBaseline/ExplainedVariance     -21.4918
PolicyExecTime                                0.104743
ProcessExecTime                               0.0114803
TotalEnvSteps                            255024
policy/Entropy                                0.448715
policy/KL                                     0.00644143
policy/KLBefore                               0
policy/LossAfter                             -0.015609
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             1.5663
policy/dLoss                                  0.015609
---------------------------------------  ----------------
2022-04-23 14:20:43 | [train_policy] epoch #252 | Obtaining samples for iteration 252...
2022-04-23 14:20:44 | [train_policy] epoch #252 | Logging diagnostics...
2022-04-23 14:20:44 | [train_policy] epoch #252 | Optimizing policy...
2022-04-23 14:20:44 | [train_policy] epoch #252 | Computing loss before
2022-04-23 14:20:44 | [train_policy] epoch #252 | Computing KL before
2022-04-23 14:20:44 | [train_policy] epoch #252 | Optimizing
2022-04-23 14:20:44 | [train_policy] epoch #252 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:44 | [train_policy] epoch #252 | computing loss before
2022-04-23 14:20:44 | [train_policy] epoch #252 | computing gradient
2022-04-23 14:20:44 | [train_policy] epoch #252 | gradient computed
2022-04-23 14:20:44 | [train_policy] epoch #252 | computing descent direction
2022-04-23 14:20:44 | [train_policy] epoch #252 | descent direction computed
2022-04-23 14:20:44 | [train_policy] epoch #252 | backtrack iters: 1
2022-04-23 14:20:44 | [train_policy] epoch #252 | optimization finished
2022-04-23 14:20:44 | [train_policy] epoch #252 | Computing KL after
2022-04-23 14:20:44 | [train_policy] epoch #252 | Computing loss after
2022-04-23 14:20:44 | [train_policy] epoch #252 | Fitting baseline...
2022-04-23 14:20:44 | [train_policy] epoch #252 | Saving snapshot...
2022-04-23 14:20:44 | [train_policy] epoch #252 | Saved
2022-04-23 14:20:44 | [train_policy] epoch #252 | Time 91.94 s
2022-04-23 14:20:44 | [train_policy] epoch #252 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118062
Evaluation/AverageDiscountedReturn          -88.7661
Evaluation/AverageReturn                    -88.7661
Evaluation/CompletionRate                     0
Evaluation/Iteration                        252
Evaluation/MaxReturn                        -32.056
Evaluation/MinReturn                      -2109.18
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        298.051
Extras/EpisodeRewardMean                    -85.2099
LinearFeatureBaseline/ExplainedVariance       0.0104825
PolicyExecTime                                0.0998306
ProcessExecTime                               0.0112386
TotalEnvSteps                            256036
policy/Entropy                                0.475557
policy/KL                                     0.006525
policy/KLBefore                               0
policy/LossAfter                             -0.0155466
policy/LossBefore                             8.01011e-09
policy/Perplexity                             1.60891
policy/dLoss                                  0.0155466
---------------------------------------  ----------------
2022-04-23 14:20:44 | [train_policy] epoch #253 | Obtaining samples for iteration 253...
2022-04-23 14:20:44 | [train_policy] epoch #253 | Logging diagnostics...
2022-04-23 14:20:44 | [train_policy] epoch #253 | Optimizing policy...
2022-04-23 14:20:44 | [train_policy] epoch #253 | Computing loss before
2022-04-23 14:20:44 | [train_policy] epoch #253 | Computing KL before
2022-04-23 14:20:44 | [train_policy] epoch #253 | Optimizing
2022-04-23 14:20:44 | [train_policy] epoch #253 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:44 | [train_policy] epoch #253 | computing loss before
2022-04-23 14:20:44 | [train_policy] epoch #253 | computing gradient
2022-04-23 14:20:44 | [train_policy] epoch #253 | gradient computed
2022-04-23 14:20:44 | [train_policy] epoch #253 | computing descent direction
2022-04-23 14:20:44 | [train_policy] epoch #253 | descent direction computed
2022-04-23 14:20:44 | [train_policy] epoch #253 | backtrack iters: 0
2022-04-23 14:20:44 | [train_policy] epoch #253 | optimization finished
2022-04-23 14:20:44 | [train_policy] epoch #253 | Computing KL after
2022-04-23 14:20:44 | [train_policy] epoch #253 | Computing loss after
2022-04-23 14:20:44 | [train_policy] epoch #253 | Fitting baseline...
2022-04-23 14:20:44 | [train_policy] epoch #253 | Saving snapshot...
2022-04-23 14:20:44 | [train_policy] epoch #253 | Saved
2022-04-23 14:20:44 | [train_policy] epoch #253 | Time 92.28 s
2022-04-23 14:20:44 | [train_policy] epoch #253 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118406
Evaluation/AverageDiscountedReturn          -88.4636
Evaluation/AverageReturn                    -88.4636
Evaluation/CompletionRate                     0
Evaluation/Iteration                        253
Evaluation/MaxReturn                        -32.4899
Evaluation/MinReturn                      -2083.3
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        296.812
Extras/EpisodeRewardMean                    -85.1445
LinearFeatureBaseline/ExplainedVariance       0.206545
PolicyExecTime                                0.101122
ProcessExecTime                               0.011322
TotalEnvSteps                            257048
policy/Entropy                                0.485493
policy/KL                                     0.0086939
policy/KLBefore                               0
policy/LossAfter                             -0.0219682
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             1.62498
policy/dLoss                                  0.0219682
---------------------------------------  ----------------
2022-04-23 14:20:44 | [train_policy] epoch #254 | Obtaining samples for iteration 254...
2022-04-23 14:20:44 | [train_policy] epoch #254 | Logging diagnostics...
2022-04-23 14:20:44 | [train_policy] epoch #254 | Optimizing policy...
2022-04-23 14:20:44 | [train_policy] epoch #254 | Computing loss before
2022-04-23 14:20:44 | [train_policy] epoch #254 | Computing KL before
2022-04-23 14:20:44 | [train_policy] epoch #254 | Optimizing
2022-04-23 14:20:44 | [train_policy] epoch #254 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:44 | [train_policy] epoch #254 | computing loss before
2022-04-23 14:20:44 | [train_policy] epoch #254 | computing gradient
2022-04-23 14:20:44 | [train_policy] epoch #254 | gradient computed
2022-04-23 14:20:44 | [train_policy] epoch #254 | computing descent direction
2022-04-23 14:20:44 | [train_policy] epoch #254 | descent direction computed
2022-04-23 14:20:44 | [train_policy] epoch #254 | backtrack iters: 1
2022-04-23 14:20:44 | [train_policy] epoch #254 | optimization finished
2022-04-23 14:20:44 | [train_policy] epoch #254 | Computing KL after
2022-04-23 14:20:44 | [train_policy] epoch #254 | Computing loss after
2022-04-23 14:20:44 | [train_policy] epoch #254 | Fitting baseline...
2022-04-23 14:20:44 | [train_policy] epoch #254 | Saving snapshot...
2022-04-23 14:20:44 | [train_policy] epoch #254 | Saved
2022-04-23 14:20:44 | [train_policy] epoch #254 | Time 92.62 s
2022-04-23 14:20:44 | [train_policy] epoch #254 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118872
Evaluation/AverageDiscountedReturn          -67.38
Evaluation/AverageReturn                    -67.38
Evaluation/CompletionRate                     0
Evaluation/Iteration                        254
Evaluation/MaxReturn                        -32.0038
Evaluation/MinReturn                      -2072.84
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.45
Extras/EpisodeRewardMean                    -65.1253
LinearFeatureBaseline/ExplainedVariance       0.102185
PolicyExecTime                                0.101981
ProcessExecTime                               0.0114453
TotalEnvSteps                            258060
policy/Entropy                                0.502075
policy/KL                                     0.00679884
policy/KLBefore                               0
policy/LossAfter                             -0.0167722
policy/LossBefore                            -3.06269e-09
policy/Perplexity                             1.65215
policy/dLoss                                  0.0167722
---------------------------------------  ----------------
2022-04-23 14:20:44 | [train_policy] epoch #255 | Obtaining samples for iteration 255...
2022-04-23 14:20:45 | [train_policy] epoch #255 | Logging diagnostics...
2022-04-23 14:20:45 | [train_policy] epoch #255 | Optimizing policy...
2022-04-23 14:20:45 | [train_policy] epoch #255 | Computing loss before
2022-04-23 14:20:45 | [train_policy] epoch #255 | Computing KL before
2022-04-23 14:20:45 | [train_policy] epoch #255 | Optimizing
2022-04-23 14:20:45 | [train_policy] epoch #255 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:45 | [train_policy] epoch #255 | computing loss before
2022-04-23 14:20:45 | [train_policy] epoch #255 | computing gradient
2022-04-23 14:20:45 | [train_policy] epoch #255 | gradient computed
2022-04-23 14:20:45 | [train_policy] epoch #255 | computing descent direction
2022-04-23 14:20:45 | [train_policy] epoch #255 | descent direction computed
2022-04-23 14:20:45 | [train_policy] epoch #255 | backtrack iters: 0
2022-04-23 14:20:45 | [train_policy] epoch #255 | optimization finished
2022-04-23 14:20:45 | [train_policy] epoch #255 | Computing KL after
2022-04-23 14:20:45 | [train_policy] epoch #255 | Computing loss after
2022-04-23 14:20:45 | [train_policy] epoch #255 | Fitting baseline...
2022-04-23 14:20:45 | [train_policy] epoch #255 | Saving snapshot...
2022-04-23 14:20:45 | [train_policy] epoch #255 | Saved
2022-04-23 14:20:45 | [train_policy] epoch #255 | Time 92.97 s
2022-04-23 14:20:45 | [train_policy] epoch #255 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118067
Evaluation/AverageDiscountedReturn          -66.2717
Evaluation/AverageReturn                    -66.2717
Evaluation/CompletionRate                     0
Evaluation/Iteration                        255
Evaluation/MaxReturn                        -34.0719
Evaluation/MinReturn                      -2054.54
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.539
Extras/EpisodeRewardMean                    -64.5242
LinearFeatureBaseline/ExplainedVariance      -0.0246194
PolicyExecTime                                0.10535
ProcessExecTime                               0.0114863
TotalEnvSteps                            259072
policy/Entropy                                0.46999
policy/KL                                     0.00983315
policy/KLBefore                               0
policy/LossAfter                             -0.0255723
policy/LossBefore                            -3.41608e-09
policy/Perplexity                             1.59998
policy/dLoss                                  0.0255723
---------------------------------------  ----------------
2022-04-23 14:20:45 | [train_policy] epoch #256 | Obtaining samples for iteration 256...
2022-04-23 14:20:45 | [train_policy] epoch #256 | Logging diagnostics...
2022-04-23 14:20:45 | [train_policy] epoch #256 | Optimizing policy...
2022-04-23 14:20:45 | [train_policy] epoch #256 | Computing loss before
2022-04-23 14:20:45 | [train_policy] epoch #256 | Computing KL before
2022-04-23 14:20:45 | [train_policy] epoch #256 | Optimizing
2022-04-23 14:20:45 | [train_policy] epoch #256 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:45 | [train_policy] epoch #256 | computing loss before
2022-04-23 14:20:45 | [train_policy] epoch #256 | computing gradient
2022-04-23 14:20:45 | [train_policy] epoch #256 | gradient computed
2022-04-23 14:20:45 | [train_policy] epoch #256 | computing descent direction
2022-04-23 14:20:45 | [train_policy] epoch #256 | descent direction computed
2022-04-23 14:20:45 | [train_policy] epoch #256 | backtrack iters: 0
2022-04-23 14:20:45 | [train_policy] epoch #256 | optimization finished
2022-04-23 14:20:45 | [train_policy] epoch #256 | Computing KL after
2022-04-23 14:20:45 | [train_policy] epoch #256 | Computing loss after
2022-04-23 14:20:45 | [train_policy] epoch #256 | Fitting baseline...
2022-04-23 14:20:45 | [train_policy] epoch #256 | Saving snapshot...
2022-04-23 14:20:45 | [train_policy] epoch #256 | Saved
2022-04-23 14:20:45 | [train_policy] epoch #256 | Time 93.31 s
2022-04-23 14:20:45 | [train_policy] epoch #256 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118013
Evaluation/AverageDiscountedReturn          -66.7726
Evaluation/AverageReturn                    -66.7726
Evaluation/CompletionRate                     0
Evaluation/Iteration                        256
Evaluation/MaxReturn                        -32.9601
Evaluation/MinReturn                      -2068.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.012
Extras/EpisodeRewardMean                    -85.0736
LinearFeatureBaseline/ExplainedVariance       0.0829531
PolicyExecTime                                0.103138
ProcessExecTime                               0.0112352
TotalEnvSteps                            260084
policy/Entropy                                0.509595
policy/KL                                     0.00860894
policy/KLBefore                               0
policy/LossAfter                             -0.0240799
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             1.66462
policy/dLoss                                  0.0240799
---------------------------------------  ----------------
2022-04-23 14:20:45 | [train_policy] epoch #257 | Obtaining samples for iteration 257...
2022-04-23 14:20:45 | [train_policy] epoch #257 | Logging diagnostics...
2022-04-23 14:20:45 | [train_policy] epoch #257 | Optimizing policy...
2022-04-23 14:20:45 | [train_policy] epoch #257 | Computing loss before
2022-04-23 14:20:45 | [train_policy] epoch #257 | Computing KL before
2022-04-23 14:20:45 | [train_policy] epoch #257 | Optimizing
2022-04-23 14:20:45 | [train_policy] epoch #257 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:45 | [train_policy] epoch #257 | computing loss before
2022-04-23 14:20:45 | [train_policy] epoch #257 | computing gradient
2022-04-23 14:20:45 | [train_policy] epoch #257 | gradient computed
2022-04-23 14:20:45 | [train_policy] epoch #257 | computing descent direction
2022-04-23 14:20:45 | [train_policy] epoch #257 | descent direction computed
2022-04-23 14:20:45 | [train_policy] epoch #257 | backtrack iters: 1
2022-04-23 14:20:45 | [train_policy] epoch #257 | optimization finished
2022-04-23 14:20:45 | [train_policy] epoch #257 | Computing KL after
2022-04-23 14:20:45 | [train_policy] epoch #257 | Computing loss after
2022-04-23 14:20:45 | [train_policy] epoch #257 | Fitting baseline...
2022-04-23 14:20:45 | [train_policy] epoch #257 | Saving snapshot...
2022-04-23 14:20:45 | [train_policy] epoch #257 | Saved
2022-04-23 14:20:45 | [train_policy] epoch #257 | Time 93.65 s
2022-04-23 14:20:45 | [train_policy] epoch #257 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117865
Evaluation/AverageDiscountedReturn          -67.7624
Evaluation/AverageReturn                    -67.7624
Evaluation/CompletionRate                     0
Evaluation/Iteration                        257
Evaluation/MaxReturn                        -31.8061
Evaluation/MinReturn                      -2063.59
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.399
Extras/EpisodeRewardMean                    -65.8093
LinearFeatureBaseline/ExplainedVariance       0.135924
PolicyExecTime                                0.0982709
ProcessExecTime                               0.0112858
TotalEnvSteps                            261096
policy/Entropy                                0.494086
policy/KL                                     0.00755903
policy/KLBefore                               0
policy/LossAfter                             -0.0158499
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             1.639
policy/dLoss                                  0.0158499
---------------------------------------  ----------------
2022-04-23 14:20:45 | [train_policy] epoch #258 | Obtaining samples for iteration 258...
2022-04-23 14:20:46 | [train_policy] epoch #258 | Logging diagnostics...
2022-04-23 14:20:46 | [train_policy] epoch #258 | Optimizing policy...
2022-04-23 14:20:46 | [train_policy] epoch #258 | Computing loss before
2022-04-23 14:20:46 | [train_policy] epoch #258 | Computing KL before
2022-04-23 14:20:46 | [train_policy] epoch #258 | Optimizing
2022-04-23 14:20:46 | [train_policy] epoch #258 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:46 | [train_policy] epoch #258 | computing loss before
2022-04-23 14:20:46 | [train_policy] epoch #258 | computing gradient
2022-04-23 14:20:46 | [train_policy] epoch #258 | gradient computed
2022-04-23 14:20:46 | [train_policy] epoch #258 | computing descent direction
2022-04-23 14:20:46 | [train_policy] epoch #258 | descent direction computed
2022-04-23 14:20:46 | [train_policy] epoch #258 | backtrack iters: 1
2022-04-23 14:20:46 | [train_policy] epoch #258 | optimization finished
2022-04-23 14:20:46 | [train_policy] epoch #258 | Computing KL after
2022-04-23 14:20:46 | [train_policy] epoch #258 | Computing loss after
2022-04-23 14:20:46 | [train_policy] epoch #258 | Fitting baseline...
2022-04-23 14:20:46 | [train_policy] epoch #258 | Saving snapshot...
2022-04-23 14:20:46 | [train_policy] epoch #258 | Saved
2022-04-23 14:20:46 | [train_policy] epoch #258 | Time 93.98 s
2022-04-23 14:20:46 | [train_policy] epoch #258 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.121141
Evaluation/AverageDiscountedReturn          -44.1981
Evaluation/AverageReturn                    -44.1981
Evaluation/CompletionRate                     0
Evaluation/Iteration                        258
Evaluation/MaxReturn                        -32.5285
Evaluation/MinReturn                        -76.8415
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.61833
Extras/EpisodeRewardMean                    -64.4883
LinearFeatureBaseline/ExplainedVariance     -40.2327
PolicyExecTime                                0.0990365
ProcessExecTime                               0.011493
TotalEnvSteps                            262108
policy/Entropy                                0.468614
policy/KL                                     0.00654528
policy/KLBefore                               0
policy/LossAfter                             -0.0147105
policy/LossBefore                            -1.86117e-08
policy/Perplexity                             1.59778
policy/dLoss                                  0.0147105
---------------------------------------  ----------------
2022-04-23 14:20:46 | [train_policy] epoch #259 | Obtaining samples for iteration 259...
2022-04-23 14:20:46 | [train_policy] epoch #259 | Logging diagnostics...
2022-04-23 14:20:46 | [train_policy] epoch #259 | Optimizing policy...
2022-04-23 14:20:46 | [train_policy] epoch #259 | Computing loss before
2022-04-23 14:20:46 | [train_policy] epoch #259 | Computing KL before
2022-04-23 14:20:46 | [train_policy] epoch #259 | Optimizing
2022-04-23 14:20:46 | [train_policy] epoch #259 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:46 | [train_policy] epoch #259 | computing loss before
2022-04-23 14:20:46 | [train_policy] epoch #259 | computing gradient
2022-04-23 14:20:46 | [train_policy] epoch #259 | gradient computed
2022-04-23 14:20:46 | [train_policy] epoch #259 | computing descent direction
2022-04-23 14:20:46 | [train_policy] epoch #259 | descent direction computed
2022-04-23 14:20:46 | [train_policy] epoch #259 | backtrack iters: 1
2022-04-23 14:20:46 | [train_policy] epoch #259 | optimization finished
2022-04-23 14:20:46 | [train_policy] epoch #259 | Computing KL after
2022-04-23 14:20:46 | [train_policy] epoch #259 | Computing loss after
2022-04-23 14:20:46 | [train_policy] epoch #259 | Fitting baseline...
2022-04-23 14:20:46 | [train_policy] epoch #259 | Saving snapshot...
2022-04-23 14:20:46 | [train_policy] epoch #259 | Saved
2022-04-23 14:20:46 | [train_policy] epoch #259 | Time 94.34 s
2022-04-23 14:20:46 | [train_policy] epoch #259 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.126976
Evaluation/AverageDiscountedReturn          -44.1902
Evaluation/AverageReturn                    -44.1902
Evaluation/CompletionRate                     0
Evaluation/Iteration                        259
Evaluation/MaxReturn                        -32.854
Evaluation/MinReturn                        -73.432
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.05108
Extras/EpisodeRewardMean                    -43.8395
LinearFeatureBaseline/ExplainedVariance       0.913323
PolicyExecTime                                0.103956
ProcessExecTime                               0.0124838
TotalEnvSteps                            263120
policy/Entropy                                0.420809
policy/KL                                     0.00688993
policy/KLBefore                               0
policy/LossAfter                             -0.0156425
policy/LossBefore                             7.53893e-09
policy/Perplexity                             1.52319
policy/dLoss                                  0.0156425
---------------------------------------  ----------------
2022-04-23 14:20:46 | [train_policy] epoch #260 | Obtaining samples for iteration 260...
2022-04-23 14:20:46 | [train_policy] epoch #260 | Logging diagnostics...
2022-04-23 14:20:46 | [train_policy] epoch #260 | Optimizing policy...
2022-04-23 14:20:46 | [train_policy] epoch #260 | Computing loss before
2022-04-23 14:20:46 | [train_policy] epoch #260 | Computing KL before
2022-04-23 14:20:46 | [train_policy] epoch #260 | Optimizing
2022-04-23 14:20:46 | [train_policy] epoch #260 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:46 | [train_policy] epoch #260 | computing loss before
2022-04-23 14:20:46 | [train_policy] epoch #260 | computing gradient
2022-04-23 14:20:46 | [train_policy] epoch #260 | gradient computed
2022-04-23 14:20:46 | [train_policy] epoch #260 | computing descent direction
2022-04-23 14:20:46 | [train_policy] epoch #260 | descent direction computed
2022-04-23 14:20:46 | [train_policy] epoch #260 | backtrack iters: 1
2022-04-23 14:20:46 | [train_policy] epoch #260 | optimization finished
2022-04-23 14:20:46 | [train_policy] epoch #260 | Computing KL after
2022-04-23 14:20:46 | [train_policy] epoch #260 | Computing loss after
2022-04-23 14:20:46 | [train_policy] epoch #260 | Fitting baseline...
2022-04-23 14:20:46 | [train_policy] epoch #260 | Saving snapshot...
2022-04-23 14:20:46 | [train_policy] epoch #260 | Saved
2022-04-23 14:20:46 | [train_policy] epoch #260 | Time 94.71 s
2022-04-23 14:20:46 | [train_policy] epoch #260 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.130379
Evaluation/AverageDiscountedReturn          -88.2204
Evaluation/AverageReturn                    -88.2204
Evaluation/CompletionRate                     0
Evaluation/Iteration                        260
Evaluation/MaxReturn                        -32.5994
Evaluation/MinReturn                      -2067.95
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.12
Extras/EpisodeRewardMean                    -84.8515
LinearFeatureBaseline/ExplainedVariance       0.00973417
PolicyExecTime                                0.108404
ProcessExecTime                               0.0127678
TotalEnvSteps                            264132
policy/Entropy                                0.390684
policy/KL                                     0.00750708
policy/KLBefore                               0
policy/LossAfter                             -0.0056871
policy/LossBefore                            -1.34287e-08
policy/Perplexity                             1.47799
policy/dLoss                                  0.00568708
---------------------------------------  ----------------
2022-04-23 14:20:47 | [train_policy] epoch #261 | Obtaining samples for iteration 261...
2022-04-23 14:20:47 | [train_policy] epoch #261 | Logging diagnostics...
2022-04-23 14:20:47 | [train_policy] epoch #261 | Optimizing policy...
2022-04-23 14:20:47 | [train_policy] epoch #261 | Computing loss before
2022-04-23 14:20:47 | [train_policy] epoch #261 | Computing KL before
2022-04-23 14:20:47 | [train_policy] epoch #261 | Optimizing
2022-04-23 14:20:47 | [train_policy] epoch #261 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:47 | [train_policy] epoch #261 | computing loss before
2022-04-23 14:20:47 | [train_policy] epoch #261 | computing gradient
2022-04-23 14:20:47 | [train_policy] epoch #261 | gradient computed
2022-04-23 14:20:47 | [train_policy] epoch #261 | computing descent direction
2022-04-23 14:20:47 | [train_policy] epoch #261 | descent direction computed
2022-04-23 14:20:47 | [train_policy] epoch #261 | backtrack iters: 1
2022-04-23 14:20:47 | [train_policy] epoch #261 | optimization finished
2022-04-23 14:20:47 | [train_policy] epoch #261 | Computing KL after
2022-04-23 14:20:47 | [train_policy] epoch #261 | Computing loss after
2022-04-23 14:20:47 | [train_policy] epoch #261 | Fitting baseline...
2022-04-23 14:20:47 | [train_policy] epoch #261 | Saving snapshot...
2022-04-23 14:20:47 | [train_policy] epoch #261 | Saved
2022-04-23 14:20:47 | [train_policy] epoch #261 | Time 95.08 s
2022-04-23 14:20:47 | [train_policy] epoch #261 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.130794
Evaluation/AverageDiscountedReturn          -66.0873
Evaluation/AverageReturn                    -66.0873
Evaluation/CompletionRate                     0
Evaluation/Iteration                        261
Evaluation/MaxReturn                        -35.6535
Evaluation/MinReturn                      -2064.2
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.54
Extras/EpisodeRewardMean                    -64.3018
LinearFeatureBaseline/ExplainedVariance      -0.0150641
PolicyExecTime                                0.103227
ProcessExecTime                               0.0126448
TotalEnvSteps                            265144
policy/Entropy                                0.392467
policy/KL                                     0.00696905
policy/KLBefore                               0
policy/LossAfter                             -0.0240561
policy/LossBefore                             8.71688e-09
policy/Perplexity                             1.48063
policy/dLoss                                  0.0240561
---------------------------------------  ----------------
2022-04-23 14:20:47 | [train_policy] epoch #262 | Obtaining samples for iteration 262...
2022-04-23 14:20:47 | [train_policy] epoch #262 | Logging diagnostics...
2022-04-23 14:20:47 | [train_policy] epoch #262 | Optimizing policy...
2022-04-23 14:20:47 | [train_policy] epoch #262 | Computing loss before
2022-04-23 14:20:47 | [train_policy] epoch #262 | Computing KL before
2022-04-23 14:20:47 | [train_policy] epoch #262 | Optimizing
2022-04-23 14:20:47 | [train_policy] epoch #262 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:47 | [train_policy] epoch #262 | computing loss before
2022-04-23 14:20:47 | [train_policy] epoch #262 | computing gradient
2022-04-23 14:20:47 | [train_policy] epoch #262 | gradient computed
2022-04-23 14:20:47 | [train_policy] epoch #262 | computing descent direction
2022-04-23 14:20:47 | [train_policy] epoch #262 | descent direction computed
2022-04-23 14:20:47 | [train_policy] epoch #262 | backtrack iters: 2
2022-04-23 14:20:47 | [train_policy] epoch #262 | optimization finished
2022-04-23 14:20:47 | [train_policy] epoch #262 | Computing KL after
2022-04-23 14:20:47 | [train_policy] epoch #262 | Computing loss after
2022-04-23 14:20:47 | [train_policy] epoch #262 | Fitting baseline...
2022-04-23 14:20:47 | [train_policy] epoch #262 | Saving snapshot...
2022-04-23 14:20:47 | [train_policy] epoch #262 | Saved
2022-04-23 14:20:47 | [train_policy] epoch #262 | Time 95.45 s
2022-04-23 14:20:47 | [train_policy] epoch #262 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.131338
Evaluation/AverageDiscountedReturn          -90.364
Evaluation/AverageReturn                    -90.364
Evaluation/CompletionRate                     0
Evaluation/Iteration                        262
Evaluation/MaxReturn                        -33.5057
Evaluation/MinReturn                      -2310.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        313.157
Extras/EpisodeRewardMean                    -86.5289
LinearFeatureBaseline/ExplainedVariance       0.19031
PolicyExecTime                                0.106301
ProcessExecTime                               0.012711
TotalEnvSteps                            266156
policy/Entropy                                0.352105
policy/KL                                     0.00610558
policy/KLBefore                               0
policy/LossAfter                             -0.0171601
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             1.42206
policy/dLoss                                  0.0171601
---------------------------------------  ----------------
2022-04-23 14:20:47 | [train_policy] epoch #263 | Obtaining samples for iteration 263...
2022-04-23 14:20:48 | [train_policy] epoch #263 | Logging diagnostics...
2022-04-23 14:20:48 | [train_policy] epoch #263 | Optimizing policy...
2022-04-23 14:20:48 | [train_policy] epoch #263 | Computing loss before
2022-04-23 14:20:48 | [train_policy] epoch #263 | Computing KL before
2022-04-23 14:20:48 | [train_policy] epoch #263 | Optimizing
2022-04-23 14:20:48 | [train_policy] epoch #263 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:48 | [train_policy] epoch #263 | computing loss before
2022-04-23 14:20:48 | [train_policy] epoch #263 | computing gradient
2022-04-23 14:20:48 | [train_policy] epoch #263 | gradient computed
2022-04-23 14:20:48 | [train_policy] epoch #263 | computing descent direction
2022-04-23 14:20:48 | [train_policy] epoch #263 | descent direction computed
2022-04-23 14:20:48 | [train_policy] epoch #263 | backtrack iters: 1
2022-04-23 14:20:48 | [train_policy] epoch #263 | optimization finished
2022-04-23 14:20:48 | [train_policy] epoch #263 | Computing KL after
2022-04-23 14:20:48 | [train_policy] epoch #263 | Computing loss after
2022-04-23 14:20:48 | [train_policy] epoch #263 | Fitting baseline...
2022-04-23 14:20:48 | [train_policy] epoch #263 | Saving snapshot...
2022-04-23 14:20:48 | [train_policy] epoch #263 | Saved
2022-04-23 14:20:48 | [train_policy] epoch #263 | Time 95.80 s
2022-04-23 14:20:48 | [train_policy] epoch #263 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.123986
Evaluation/AverageDiscountedReturn          -45.5979
Evaluation/AverageReturn                    -45.5979
Evaluation/CompletionRate                     0
Evaluation/Iteration                        263
Evaluation/MaxReturn                        -34.8368
Evaluation/MinReturn                        -64.8462
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.9812
Extras/EpisodeRewardMean                    -45.5753
LinearFeatureBaseline/ExplainedVariance    -103.957
PolicyExecTime                                0.101398
ProcessExecTime                               0.0119317
TotalEnvSteps                            267168
policy/Entropy                                0.355663
policy/KL                                     0.00642829
policy/KLBefore                               0
policy/LossAfter                             -0.0156191
policy/LossBefore                            -2.54439e-08
policy/Perplexity                             1.42713
policy/dLoss                                  0.015619
---------------------------------------  ----------------
2022-04-23 14:20:48 | [train_policy] epoch #264 | Obtaining samples for iteration 264...
2022-04-23 14:20:48 | [train_policy] epoch #264 | Logging diagnostics...
2022-04-23 14:20:48 | [train_policy] epoch #264 | Optimizing policy...
2022-04-23 14:20:48 | [train_policy] epoch #264 | Computing loss before
2022-04-23 14:20:48 | [train_policy] epoch #264 | Computing KL before
2022-04-23 14:20:48 | [train_policy] epoch #264 | Optimizing
2022-04-23 14:20:48 | [train_policy] epoch #264 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:48 | [train_policy] epoch #264 | computing loss before
2022-04-23 14:20:48 | [train_policy] epoch #264 | computing gradient
2022-04-23 14:20:48 | [train_policy] epoch #264 | gradient computed
2022-04-23 14:20:48 | [train_policy] epoch #264 | computing descent direction
2022-04-23 14:20:48 | [train_policy] epoch #264 | descent direction computed
2022-04-23 14:20:48 | [train_policy] epoch #264 | backtrack iters: 1
2022-04-23 14:20:48 | [train_policy] epoch #264 | optimization finished
2022-04-23 14:20:48 | [train_policy] epoch #264 | Computing KL after
2022-04-23 14:20:48 | [train_policy] epoch #264 | Computing loss after
2022-04-23 14:20:48 | [train_policy] epoch #264 | Fitting baseline...
2022-04-23 14:20:48 | [train_policy] epoch #264 | Saving snapshot...
2022-04-23 14:20:48 | [train_policy] epoch #264 | Saved
2022-04-23 14:20:48 | [train_policy] epoch #264 | Time 96.15 s
2022-04-23 14:20:48 | [train_policy] epoch #264 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.128727
Evaluation/AverageDiscountedReturn          -66.0378
Evaluation/AverageReturn                    -66.0378
Evaluation/CompletionRate                     0
Evaluation/Iteration                        264
Evaluation/MaxReturn                        -31.9719
Evaluation/MinReturn                      -2064.15
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.544
Extras/EpisodeRewardMean                    -64.4188
LinearFeatureBaseline/ExplainedVariance       0.0112012
PolicyExecTime                                0.10287
ProcessExecTime                               0.0127118
TotalEnvSteps                            268180
policy/Entropy                                0.384532
policy/KL                                     0.00769432
policy/KLBefore                               0
policy/LossAfter                             -0.0107261
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             1.46893
policy/dLoss                                  0.0107261
---------------------------------------  ---------------
2022-04-23 14:20:48 | [train_policy] epoch #265 | Obtaining samples for iteration 265...
2022-04-23 14:20:48 | [train_policy] epoch #265 | Logging diagnostics...
2022-04-23 14:20:48 | [train_policy] epoch #265 | Optimizing policy...
2022-04-23 14:20:48 | [train_policy] epoch #265 | Computing loss before
2022-04-23 14:20:48 | [train_policy] epoch #265 | Computing KL before
2022-04-23 14:20:48 | [train_policy] epoch #265 | Optimizing
2022-04-23 14:20:48 | [train_policy] epoch #265 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:48 | [train_policy] epoch #265 | computing loss before
2022-04-23 14:20:48 | [train_policy] epoch #265 | computing gradient
2022-04-23 14:20:48 | [train_policy] epoch #265 | gradient computed
2022-04-23 14:20:48 | [train_policy] epoch #265 | computing descent direction
2022-04-23 14:20:48 | [train_policy] epoch #265 | descent direction computed
2022-04-23 14:20:48 | [train_policy] epoch #265 | backtrack iters: 1
2022-04-23 14:20:48 | [train_policy] epoch #265 | optimization finished
2022-04-23 14:20:48 | [train_policy] epoch #265 | Computing KL after
2022-04-23 14:20:48 | [train_policy] epoch #265 | Computing loss after
2022-04-23 14:20:48 | [train_policy] epoch #265 | Fitting baseline...
2022-04-23 14:20:48 | [train_policy] epoch #265 | Saving snapshot...
2022-04-23 14:20:48 | [train_policy] epoch #265 | Saved
2022-04-23 14:20:48 | [train_policy] epoch #265 | Time 96.49 s
2022-04-23 14:20:48 | [train_policy] epoch #265 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117707
Evaluation/AverageDiscountedReturn         -110.618
Evaluation/AverageReturn                   -110.618
Evaluation/CompletionRate                     0
Evaluation/Iteration                        265
Evaluation/MaxReturn                        -34.8379
Evaluation/MinReturn                      -2073.73
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.316
Extras/EpisodeRewardMean                   -105.288
LinearFeatureBaseline/ExplainedVariance       0.190759
PolicyExecTime                                0.0972509
ProcessExecTime                               0.0112116
TotalEnvSteps                            269192
policy/Entropy                                0.370374
policy/KL                                     0.00644124
policy/KLBefore                               0
policy/LossAfter                             -0.0163613
policy/LossBefore                            -1.29575e-08
policy/Perplexity                             1.44828
policy/dLoss                                  0.0163613
---------------------------------------  ----------------
2022-04-23 14:20:48 | [train_policy] epoch #266 | Obtaining samples for iteration 266...
2022-04-23 14:20:49 | [train_policy] epoch #266 | Logging diagnostics...
2022-04-23 14:20:49 | [train_policy] epoch #266 | Optimizing policy...
2022-04-23 14:20:49 | [train_policy] epoch #266 | Computing loss before
2022-04-23 14:20:49 | [train_policy] epoch #266 | Computing KL before
2022-04-23 14:20:49 | [train_policy] epoch #266 | Optimizing
2022-04-23 14:20:49 | [train_policy] epoch #266 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:49 | [train_policy] epoch #266 | computing loss before
2022-04-23 14:20:49 | [train_policy] epoch #266 | computing gradient
2022-04-23 14:20:49 | [train_policy] epoch #266 | gradient computed
2022-04-23 14:20:49 | [train_policy] epoch #266 | computing descent direction
2022-04-23 14:20:49 | [train_policy] epoch #266 | descent direction computed
2022-04-23 14:20:49 | [train_policy] epoch #266 | backtrack iters: 1
2022-04-23 14:20:49 | [train_policy] epoch #266 | optimization finished
2022-04-23 14:20:49 | [train_policy] epoch #266 | Computing KL after
2022-04-23 14:20:49 | [train_policy] epoch #266 | Computing loss after
2022-04-23 14:20:49 | [train_policy] epoch #266 | Fitting baseline...
2022-04-23 14:20:49 | [train_policy] epoch #266 | Saving snapshot...
2022-04-23 14:20:49 | [train_policy] epoch #266 | Saved
2022-04-23 14:20:49 | [train_policy] epoch #266 | Time 96.85 s
2022-04-23 14:20:49 | [train_policy] epoch #266 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.128937
Evaluation/AverageDiscountedReturn         -111.479
Evaluation/AverageReturn                   -111.479
Evaluation/CompletionRate                     0
Evaluation/Iteration                        266
Evaluation/MaxReturn                        -33.5339
Evaluation/MinReturn                      -2100.06
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        361.077
Extras/EpisodeRewardMean                   -106.406
LinearFeatureBaseline/ExplainedVariance       0.236305
PolicyExecTime                                0.101935
ProcessExecTime                               0.011981
TotalEnvSteps                            270204
policy/Entropy                                0.385614
policy/KL                                     0.00667081
policy/KLBefore                               0
policy/LossAfter                             -0.0115199
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             1.47052
policy/dLoss                                  0.0115199
---------------------------------------  ----------------
2022-04-23 14:20:49 | [train_policy] epoch #267 | Obtaining samples for iteration 267...
2022-04-23 14:20:49 | [train_policy] epoch #267 | Logging diagnostics...
2022-04-23 14:20:49 | [train_policy] epoch #267 | Optimizing policy...
2022-04-23 14:20:49 | [train_policy] epoch #267 | Computing loss before
2022-04-23 14:20:49 | [train_policy] epoch #267 | Computing KL before
2022-04-23 14:20:49 | [train_policy] epoch #267 | Optimizing
2022-04-23 14:20:49 | [train_policy] epoch #267 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:49 | [train_policy] epoch #267 | computing loss before
2022-04-23 14:20:49 | [train_policy] epoch #267 | computing gradient
2022-04-23 14:20:49 | [train_policy] epoch #267 | gradient computed
2022-04-23 14:20:49 | [train_policy] epoch #267 | computing descent direction
2022-04-23 14:20:49 | [train_policy] epoch #267 | descent direction computed
2022-04-23 14:20:49 | [train_policy] epoch #267 | backtrack iters: 0
2022-04-23 14:20:49 | [train_policy] epoch #267 | optimization finished
2022-04-23 14:20:49 | [train_policy] epoch #267 | Computing KL after
2022-04-23 14:20:49 | [train_policy] epoch #267 | Computing loss after
2022-04-23 14:20:49 | [train_policy] epoch #267 | Fitting baseline...
2022-04-23 14:20:49 | [train_policy] epoch #267 | Saving snapshot...
2022-04-23 14:20:49 | [train_policy] epoch #267 | Saved
2022-04-23 14:20:49 | [train_policy] epoch #267 | Time 97.19 s
2022-04-23 14:20:49 | [train_policy] epoch #267 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11789
Evaluation/AverageDiscountedReturn          -66.2392
Evaluation/AverageReturn                    -66.2392
Evaluation/CompletionRate                     0
Evaluation/Iteration                        267
Evaluation/MaxReturn                        -32.082
Evaluation/MinReturn                      -2065.21
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.71
Extras/EpisodeRewardMean                    -64.2104
LinearFeatureBaseline/ExplainedVariance      -0.117117
PolicyExecTime                                0.102867
ProcessExecTime                               0.0114081
TotalEnvSteps                            271216
policy/Entropy                                0.348332
policy/KL                                     0.00980712
policy/KLBefore                               0
policy/LossAfter                             -0.0204742
policy/LossBefore                             7.06774e-09
policy/Perplexity                             1.4167
policy/dLoss                                  0.0204742
---------------------------------------  ----------------
2022-04-23 14:20:49 | [train_policy] epoch #268 | Obtaining samples for iteration 268...
2022-04-23 14:20:49 | [train_policy] epoch #268 | Logging diagnostics...
2022-04-23 14:20:49 | [train_policy] epoch #268 | Optimizing policy...
2022-04-23 14:20:49 | [train_policy] epoch #268 | Computing loss before
2022-04-23 14:20:49 | [train_policy] epoch #268 | Computing KL before
2022-04-23 14:20:49 | [train_policy] epoch #268 | Optimizing
2022-04-23 14:20:49 | [train_policy] epoch #268 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:49 | [train_policy] epoch #268 | computing loss before
2022-04-23 14:20:49 | [train_policy] epoch #268 | computing gradient
2022-04-23 14:20:49 | [train_policy] epoch #268 | gradient computed
2022-04-23 14:20:49 | [train_policy] epoch #268 | computing descent direction
2022-04-23 14:20:49 | [train_policy] epoch #268 | descent direction computed
2022-04-23 14:20:49 | [train_policy] epoch #268 | backtrack iters: 1
2022-04-23 14:20:49 | [train_policy] epoch #268 | optimization finished
2022-04-23 14:20:49 | [train_policy] epoch #268 | Computing KL after
2022-04-23 14:20:49 | [train_policy] epoch #268 | Computing loss after
2022-04-23 14:20:49 | [train_policy] epoch #268 | Fitting baseline...
2022-04-23 14:20:49 | [train_policy] epoch #268 | Saving snapshot...
2022-04-23 14:20:49 | [train_policy] epoch #268 | Saved
2022-04-23 14:20:49 | [train_policy] epoch #268 | Time 97.55 s
2022-04-23 14:20:49 | [train_policy] epoch #268 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.127671
Evaluation/AverageDiscountedReturn          -89.5191
Evaluation/AverageReturn                    -89.5191
Evaluation/CompletionRate                     0
Evaluation/Iteration                        268
Evaluation/MaxReturn                        -32.3726
Evaluation/MinReturn                      -2067.18
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.869
Extras/EpisodeRewardMean                    -86.0942
LinearFeatureBaseline/ExplainedVariance       0.158178
PolicyExecTime                                0.101635
ProcessExecTime                               0.0121825
TotalEnvSteps                            272228
policy/Entropy                                0.368292
policy/KL                                     0.00839993
policy/KLBefore                               0
policy/LossAfter                             -0.0264825
policy/LossBefore                            -0
policy/Perplexity                             1.44526
policy/dLoss                                  0.0264825
---------------------------------------  ---------------
2022-04-23 14:20:49 | [train_policy] epoch #269 | Obtaining samples for iteration 269...
2022-04-23 14:20:50 | [train_policy] epoch #269 | Logging diagnostics...
2022-04-23 14:20:50 | [train_policy] epoch #269 | Optimizing policy...
2022-04-23 14:20:50 | [train_policy] epoch #269 | Computing loss before
2022-04-23 14:20:50 | [train_policy] epoch #269 | Computing KL before
2022-04-23 14:20:50 | [train_policy] epoch #269 | Optimizing
2022-04-23 14:20:50 | [train_policy] epoch #269 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:50 | [train_policy] epoch #269 | computing loss before
2022-04-23 14:20:50 | [train_policy] epoch #269 | computing gradient
2022-04-23 14:20:50 | [train_policy] epoch #269 | gradient computed
2022-04-23 14:20:50 | [train_policy] epoch #269 | computing descent direction
2022-04-23 14:20:50 | [train_policy] epoch #269 | descent direction computed
2022-04-23 14:20:50 | [train_policy] epoch #269 | backtrack iters: 1
2022-04-23 14:20:50 | [train_policy] epoch #269 | optimization finished
2022-04-23 14:20:50 | [train_policy] epoch #269 | Computing KL after
2022-04-23 14:20:50 | [train_policy] epoch #269 | Computing loss after
2022-04-23 14:20:50 | [train_policy] epoch #269 | Fitting baseline...
2022-04-23 14:20:50 | [train_policy] epoch #269 | Saving snapshot...
2022-04-23 14:20:50 | [train_policy] epoch #269 | Saved
2022-04-23 14:20:50 | [train_policy] epoch #269 | Time 97.92 s
2022-04-23 14:20:50 | [train_policy] epoch #269 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.131063
Evaluation/AverageDiscountedReturn          -87.7241
Evaluation/AverageReturn                    -87.7241
Evaluation/CompletionRate                     0
Evaluation/Iteration                        269
Evaluation/MaxReturn                        -31.7626
Evaluation/MinReturn                      -2064.28
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.536
Extras/EpisodeRewardMean                    -84.3287
LinearFeatureBaseline/ExplainedVariance       0.0524425
PolicyExecTime                                0.105955
ProcessExecTime                               0.0126591
TotalEnvSteps                            273240
policy/Entropy                                0.340683
policy/KL                                     0.00649981
policy/KLBefore                               0
policy/LossAfter                             -0.0129273
policy/LossBefore                             8.48129e-09
policy/Perplexity                             1.40591
policy/dLoss                                  0.0129274
---------------------------------------  ----------------
2022-04-23 14:20:50 | [train_policy] epoch #270 | Obtaining samples for iteration 270...
2022-04-23 14:20:50 | [train_policy] epoch #270 | Logging diagnostics...
2022-04-23 14:20:50 | [train_policy] epoch #270 | Optimizing policy...
2022-04-23 14:20:50 | [train_policy] epoch #270 | Computing loss before
2022-04-23 14:20:50 | [train_policy] epoch #270 | Computing KL before
2022-04-23 14:20:50 | [train_policy] epoch #270 | Optimizing
2022-04-23 14:20:50 | [train_policy] epoch #270 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:50 | [train_policy] epoch #270 | computing loss before
2022-04-23 14:20:50 | [train_policy] epoch #270 | computing gradient
2022-04-23 14:20:50 | [train_policy] epoch #270 | gradient computed
2022-04-23 14:20:50 | [train_policy] epoch #270 | computing descent direction
2022-04-23 14:20:50 | [train_policy] epoch #270 | descent direction computed
2022-04-23 14:20:50 | [train_policy] epoch #270 | backtrack iters: 0
2022-04-23 14:20:50 | [train_policy] epoch #270 | optimization finished
2022-04-23 14:20:50 | [train_policy] epoch #270 | Computing KL after
2022-04-23 14:20:50 | [train_policy] epoch #270 | Computing loss after
2022-04-23 14:20:50 | [train_policy] epoch #270 | Fitting baseline...
2022-04-23 14:20:50 | [train_policy] epoch #270 | Saving snapshot...
2022-04-23 14:20:50 | [train_policy] epoch #270 | Saved
2022-04-23 14:20:50 | [train_policy] epoch #270 | Time 98.27 s
2022-04-23 14:20:50 | [train_policy] epoch #270 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.125118
Evaluation/AverageDiscountedReturn          -67.5438
Evaluation/AverageReturn                    -67.5438
Evaluation/CompletionRate                     0
Evaluation/Iteration                        270
Evaluation/MaxReturn                        -31.9632
Evaluation/MinReturn                      -2063.64
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.372
Extras/EpisodeRewardMean                    -86.1361
LinearFeatureBaseline/ExplainedVariance      -0.00181842
PolicyExecTime                                0.107099
ProcessExecTime                               0.0120151
TotalEnvSteps                            274252
policy/Entropy                                0.376214
policy/KL                                     0.00946432
policy/KLBefore                               0
policy/LossAfter                             -0.0157574
policy/LossBefore                            -1.97897e-08
policy/Perplexity                             1.45676
policy/dLoss                                  0.0157574
---------------------------------------  ----------------
2022-04-23 14:20:50 | [train_policy] epoch #271 | Obtaining samples for iteration 271...
2022-04-23 14:20:50 | [train_policy] epoch #271 | Logging diagnostics...
2022-04-23 14:20:50 | [train_policy] epoch #271 | Optimizing policy...
2022-04-23 14:20:50 | [train_policy] epoch #271 | Computing loss before
2022-04-23 14:20:50 | [train_policy] epoch #271 | Computing KL before
2022-04-23 14:20:50 | [train_policy] epoch #271 | Optimizing
2022-04-23 14:20:50 | [train_policy] epoch #271 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:50 | [train_policy] epoch #271 | computing loss before
2022-04-23 14:20:50 | [train_policy] epoch #271 | computing gradient
2022-04-23 14:20:50 | [train_policy] epoch #271 | gradient computed
2022-04-23 14:20:50 | [train_policy] epoch #271 | computing descent direction
2022-04-23 14:20:50 | [train_policy] epoch #271 | descent direction computed
2022-04-23 14:20:50 | [train_policy] epoch #271 | backtrack iters: 1
2022-04-23 14:20:50 | [train_policy] epoch #271 | optimization finished
2022-04-23 14:20:50 | [train_policy] epoch #271 | Computing KL after
2022-04-23 14:20:50 | [train_policy] epoch #271 | Computing loss after
2022-04-23 14:20:50 | [train_policy] epoch #271 | Fitting baseline...
2022-04-23 14:20:50 | [train_policy] epoch #271 | Saving snapshot...
2022-04-23 14:20:50 | [train_policy] epoch #271 | Saved
2022-04-23 14:20:50 | [train_policy] epoch #271 | Time 98.64 s
2022-04-23 14:20:50 | [train_policy] epoch #271 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.126068
Evaluation/AverageDiscountedReturn          -67.2056
Evaluation/AverageReturn                    -67.2056
Evaluation/CompletionRate                     0
Evaluation/Iteration                        271
Evaluation/MaxReturn                        -31.7214
Evaluation/MinReturn                      -2066.81
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.773
Extras/EpisodeRewardMean                    -65.8805
LinearFeatureBaseline/ExplainedVariance       0.0886508
PolicyExecTime                                0.105658
ProcessExecTime                               0.0121093
TotalEnvSteps                            275264
policy/Entropy                                0.338093
policy/KL                                     0.00662385
policy/KLBefore                               0
policy/LossAfter                             -0.0137523
policy/LossBefore                            -3.53387e-09
policy/Perplexity                             1.40227
policy/dLoss                                  0.0137522
---------------------------------------  ----------------
2022-04-23 14:20:50 | [train_policy] epoch #272 | Obtaining samples for iteration 272...
2022-04-23 14:20:51 | [train_policy] epoch #272 | Logging diagnostics...
2022-04-23 14:20:51 | [train_policy] epoch #272 | Optimizing policy...
2022-04-23 14:20:51 | [train_policy] epoch #272 | Computing loss before
2022-04-23 14:20:51 | [train_policy] epoch #272 | Computing KL before
2022-04-23 14:20:51 | [train_policy] epoch #272 | Optimizing
2022-04-23 14:20:51 | [train_policy] epoch #272 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:51 | [train_policy] epoch #272 | computing loss before
2022-04-23 14:20:51 | [train_policy] epoch #272 | computing gradient
2022-04-23 14:20:51 | [train_policy] epoch #272 | gradient computed
2022-04-23 14:20:51 | [train_policy] epoch #272 | computing descent direction
2022-04-23 14:20:51 | [train_policy] epoch #272 | descent direction computed
2022-04-23 14:20:51 | [train_policy] epoch #272 | backtrack iters: 0
2022-04-23 14:20:51 | [train_policy] epoch #272 | optimization finished
2022-04-23 14:20:51 | [train_policy] epoch #272 | Computing KL after
2022-04-23 14:20:51 | [train_policy] epoch #272 | Computing loss after
2022-04-23 14:20:51 | [train_policy] epoch #272 | Fitting baseline...
2022-04-23 14:20:51 | [train_policy] epoch #272 | Saving snapshot...
2022-04-23 14:20:51 | [train_policy] epoch #272 | Saved
2022-04-23 14:20:51 | [train_policy] epoch #272 | Time 99.00 s
2022-04-23 14:20:51 | [train_policy] epoch #272 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.130612
Evaluation/AverageDiscountedReturn          -67.2472
Evaluation/AverageReturn                    -67.2472
Evaluation/CompletionRate                     0
Evaluation/Iteration                        272
Evaluation/MaxReturn                        -33.2684
Evaluation/MinReturn                      -2071.49
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.652
Extras/EpisodeRewardMean                    -65.3537
LinearFeatureBaseline/ExplainedVariance       0.131587
PolicyExecTime                                0.105284
ProcessExecTime                               0.0128117
TotalEnvSteps                            276276
policy/Entropy                                0.387337
policy/KL                                     0.00935
policy/KLBefore                               0
policy/LossAfter                             -0.0216841
policy/LossBefore                             4.24065e-09
policy/Perplexity                             1.47305
policy/dLoss                                  0.0216841
---------------------------------------  ----------------
2022-04-23 14:20:51 | [train_policy] epoch #273 | Obtaining samples for iteration 273...
2022-04-23 14:20:51 | [train_policy] epoch #273 | Logging diagnostics...
2022-04-23 14:20:51 | [train_policy] epoch #273 | Optimizing policy...
2022-04-23 14:20:51 | [train_policy] epoch #273 | Computing loss before
2022-04-23 14:20:51 | [train_policy] epoch #273 | Computing KL before
2022-04-23 14:20:51 | [train_policy] epoch #273 | Optimizing
2022-04-23 14:20:51 | [train_policy] epoch #273 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:51 | [train_policy] epoch #273 | computing loss before
2022-04-23 14:20:51 | [train_policy] epoch #273 | computing gradient
2022-04-23 14:20:51 | [train_policy] epoch #273 | gradient computed
2022-04-23 14:20:51 | [train_policy] epoch #273 | computing descent direction
2022-04-23 14:20:51 | [train_policy] epoch #273 | descent direction computed
2022-04-23 14:20:51 | [train_policy] epoch #273 | backtrack iters: 1
2022-04-23 14:20:51 | [train_policy] epoch #273 | optimization finished
2022-04-23 14:20:51 | [train_policy] epoch #273 | Computing KL after
2022-04-23 14:20:51 | [train_policy] epoch #273 | Computing loss after
2022-04-23 14:20:51 | [train_policy] epoch #273 | Fitting baseline...
2022-04-23 14:20:51 | [train_policy] epoch #273 | Saving snapshot...
2022-04-23 14:20:51 | [train_policy] epoch #273 | Saved
2022-04-23 14:20:51 | [train_policy] epoch #273 | Time 99.38 s
2022-04-23 14:20:51 | [train_policy] epoch #273 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                   0.131367
Evaluation/AverageDiscountedReturn          -47.2804
Evaluation/AverageReturn                    -47.2804
Evaluation/CompletionRate                     0
Evaluation/Iteration                        273
Evaluation/MaxReturn                        -35.2159
Evaluation/MinReturn                       -125.914
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.2735
Extras/EpisodeRewardMean                    -46.8429
LinearFeatureBaseline/ExplainedVariance     -23.1448
PolicyExecTime                                0.109168
ProcessExecTime                               0.0129852
TotalEnvSteps                            277288
policy/Entropy                                0.416715
policy/KL                                     0.00792511
policy/KLBefore                               0
policy/LossAfter                             -0.0323278
policy/LossBefore                             4.9003e-08
policy/Perplexity                             1.51697
policy/dLoss                                  0.0323279
---------------------------------------  ---------------
2022-04-23 14:20:51 | [train_policy] epoch #274 | Obtaining samples for iteration 274...
2022-04-23 14:20:51 | [train_policy] epoch #274 | Logging diagnostics...
2022-04-23 14:20:51 | [train_policy] epoch #274 | Optimizing policy...
2022-04-23 14:20:51 | [train_policy] epoch #274 | Computing loss before
2022-04-23 14:20:51 | [train_policy] epoch #274 | Computing KL before
2022-04-23 14:20:51 | [train_policy] epoch #274 | Optimizing
2022-04-23 14:20:51 | [train_policy] epoch #274 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:51 | [train_policy] epoch #274 | computing loss before
2022-04-23 14:20:51 | [train_policy] epoch #274 | computing gradient
2022-04-23 14:20:51 | [train_policy] epoch #274 | gradient computed
2022-04-23 14:20:51 | [train_policy] epoch #274 | computing descent direction
2022-04-23 14:20:52 | [train_policy] epoch #274 | descent direction computed
2022-04-23 14:20:52 | [train_policy] epoch #274 | backtrack iters: 1
2022-04-23 14:20:52 | [train_policy] epoch #274 | optimization finished
2022-04-23 14:20:52 | [train_policy] epoch #274 | Computing KL after
2022-04-23 14:20:52 | [train_policy] epoch #274 | Computing loss after
2022-04-23 14:20:52 | [train_policy] epoch #274 | Fitting baseline...
2022-04-23 14:20:52 | [train_policy] epoch #274 | Saving snapshot...
2022-04-23 14:20:52 | [train_policy] epoch #274 | Saved
2022-04-23 14:20:52 | [train_policy] epoch #274 | Time 99.75 s
2022-04-23 14:20:52 | [train_policy] epoch #274 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.124156
Evaluation/AverageDiscountedReturn          -66.108
Evaluation/AverageReturn                    -66.108
Evaluation/CompletionRate                     0
Evaluation/Iteration                        274
Evaluation/MaxReturn                        -31.7466
Evaluation/MinReturn                      -2069.57
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.122
Extras/EpisodeRewardMean                    -64.4001
LinearFeatureBaseline/ExplainedVariance       0.0143932
PolicyExecTime                                0.113817
ProcessExecTime                               0.0117805
TotalEnvSteps                            278300
policy/Entropy                                0.385539
policy/KL                                     0.00818834
policy/KLBefore                               0
policy/LossAfter                             -0.0358174
policy/LossBefore                             1.60202e-08
policy/Perplexity                             1.47041
policy/dLoss                                  0.0358174
---------------------------------------  ----------------
2022-04-23 14:20:52 | [train_policy] epoch #275 | Obtaining samples for iteration 275...
2022-04-23 14:20:52 | [train_policy] epoch #275 | Logging diagnostics...
2022-04-23 14:20:52 | [train_policy] epoch #275 | Optimizing policy...
2022-04-23 14:20:52 | [train_policy] epoch #275 | Computing loss before
2022-04-23 14:20:52 | [train_policy] epoch #275 | Computing KL before
2022-04-23 14:20:52 | [train_policy] epoch #275 | Optimizing
2022-04-23 14:20:52 | [train_policy] epoch #275 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:52 | [train_policy] epoch #275 | computing loss before
2022-04-23 14:20:52 | [train_policy] epoch #275 | computing gradient
2022-04-23 14:20:52 | [train_policy] epoch #275 | gradient computed
2022-04-23 14:20:52 | [train_policy] epoch #275 | computing descent direction
2022-04-23 14:20:52 | [train_policy] epoch #275 | descent direction computed
2022-04-23 14:20:52 | [train_policy] epoch #275 | backtrack iters: 0
2022-04-23 14:20:52 | [train_policy] epoch #275 | optimization finished
2022-04-23 14:20:52 | [train_policy] epoch #275 | Computing KL after
2022-04-23 14:20:52 | [train_policy] epoch #275 | Computing loss after
2022-04-23 14:20:52 | [train_policy] epoch #275 | Fitting baseline...
2022-04-23 14:20:52 | [train_policy] epoch #275 | Saving snapshot...
2022-04-23 14:20:52 | [train_policy] epoch #275 | Saved
2022-04-23 14:20:52 | [train_policy] epoch #275 | Time 100.10 s
2022-04-23 14:20:52 | [train_policy] epoch #275 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.120842
Evaluation/AverageDiscountedReturn          -43.4173
Evaluation/AverageReturn                    -43.4173
Evaluation/CompletionRate                     0
Evaluation/Iteration                        275
Evaluation/MaxReturn                        -31.7675
Evaluation/MinReturn                        -66.6963
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.23876
Extras/EpisodeRewardMean                    -43.3083
LinearFeatureBaseline/ExplainedVariance     -33.9006
PolicyExecTime                                0.102933
ProcessExecTime                               0.0117657
TotalEnvSteps                            279312
policy/Entropy                                0.39712
policy/KL                                     0.00980844
policy/KLBefore                               0
policy/LossAfter                             -0.0342428
policy/LossBefore                            -4.14641e-08
policy/Perplexity                             1.48753
policy/dLoss                                  0.0342427
---------------------------------------  ----------------
2022-04-23 14:20:52 | [train_policy] epoch #276 | Obtaining samples for iteration 276...
2022-04-23 14:20:52 | [train_policy] epoch #276 | Logging diagnostics...
2022-04-23 14:20:52 | [train_policy] epoch #276 | Optimizing policy...
2022-04-23 14:20:52 | [train_policy] epoch #276 | Computing loss before
2022-04-23 14:20:52 | [train_policy] epoch #276 | Computing KL before
2022-04-23 14:20:52 | [train_policy] epoch #276 | Optimizing
2022-04-23 14:20:52 | [train_policy] epoch #276 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:52 | [train_policy] epoch #276 | computing loss before
2022-04-23 14:20:52 | [train_policy] epoch #276 | computing gradient
2022-04-23 14:20:52 | [train_policy] epoch #276 | gradient computed
2022-04-23 14:20:52 | [train_policy] epoch #276 | computing descent direction
2022-04-23 14:20:52 | [train_policy] epoch #276 | descent direction computed
2022-04-23 14:20:52 | [train_policy] epoch #276 | backtrack iters: 1
2022-04-23 14:20:52 | [train_policy] epoch #276 | optimization finished
2022-04-23 14:20:52 | [train_policy] epoch #276 | Computing KL after
2022-04-23 14:20:52 | [train_policy] epoch #276 | Computing loss after
2022-04-23 14:20:52 | [train_policy] epoch #276 | Fitting baseline...
2022-04-23 14:20:52 | [train_policy] epoch #276 | Saving snapshot...
2022-04-23 14:20:52 | [train_policy] epoch #276 | Saved
2022-04-23 14:20:52 | [train_policy] epoch #276 | Time 100.46 s
2022-04-23 14:20:52 | [train_policy] epoch #276 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.126895
Evaluation/AverageDiscountedReturn          -66.5158
Evaluation/AverageReturn                    -66.5158
Evaluation/CompletionRate                     0
Evaluation/Iteration                        276
Evaluation/MaxReturn                        -30.7426
Evaluation/MinReturn                      -2068.54
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.994
Extras/EpisodeRewardMean                    -64.5578
LinearFeatureBaseline/ExplainedVariance       0.0111476
PolicyExecTime                                0.105733
ProcessExecTime                               0.0120187
TotalEnvSteps                            280324
policy/Entropy                                0.40734
policy/KL                                     0.00695962
policy/KLBefore                               0
policy/LossAfter                             -0.0175754
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             1.50282
policy/dLoss                                  0.0175754
---------------------------------------  ----------------
2022-04-23 14:20:52 | [train_policy] epoch #277 | Obtaining samples for iteration 277...
2022-04-23 14:20:53 | [train_policy] epoch #277 | Logging diagnostics...
2022-04-23 14:20:53 | [train_policy] epoch #277 | Optimizing policy...
2022-04-23 14:20:53 | [train_policy] epoch #277 | Computing loss before
2022-04-23 14:20:53 | [train_policy] epoch #277 | Computing KL before
2022-04-23 14:20:53 | [train_policy] epoch #277 | Optimizing
2022-04-23 14:20:53 | [train_policy] epoch #277 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:53 | [train_policy] epoch #277 | computing loss before
2022-04-23 14:20:53 | [train_policy] epoch #277 | computing gradient
2022-04-23 14:20:53 | [train_policy] epoch #277 | gradient computed
2022-04-23 14:20:53 | [train_policy] epoch #277 | computing descent direction
2022-04-23 14:20:53 | [train_policy] epoch #277 | descent direction computed
2022-04-23 14:20:53 | [train_policy] epoch #277 | backtrack iters: 1
2022-04-23 14:20:53 | [train_policy] epoch #277 | optimization finished
2022-04-23 14:20:53 | [train_policy] epoch #277 | Computing KL after
2022-04-23 14:20:53 | [train_policy] epoch #277 | Computing loss after
2022-04-23 14:20:53 | [train_policy] epoch #277 | Fitting baseline...
2022-04-23 14:20:53 | [train_policy] epoch #277 | Saving snapshot...
2022-04-23 14:20:53 | [train_policy] epoch #277 | Saved
2022-04-23 14:20:53 | [train_policy] epoch #277 | Time 100.81 s
2022-04-23 14:20:53 | [train_policy] epoch #277 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.12672
Evaluation/AverageDiscountedReturn          -66.2553
Evaluation/AverageReturn                    -66.2553
Evaluation/CompletionRate                     0
Evaluation/Iteration                        277
Evaluation/MaxReturn                        -33.1653
Evaluation/MinReturn                      -2067.73
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.951
Extras/EpisodeRewardMean                    -64.3022
LinearFeatureBaseline/ExplainedVariance       0.117979
PolicyExecTime                                0.104667
ProcessExecTime                               0.012126
TotalEnvSteps                            281336
policy/Entropy                                0.412586
policy/KL                                     0.00686422
policy/KLBefore                               0
policy/LossAfter                             -0.0165972
policy/LossBefore                             4.71183e-10
policy/Perplexity                             1.51072
policy/dLoss                                  0.0165972
---------------------------------------  ----------------
2022-04-23 14:20:53 | [train_policy] epoch #278 | Obtaining samples for iteration 278...
2022-04-23 14:20:53 | [train_policy] epoch #278 | Logging diagnostics...
2022-04-23 14:20:53 | [train_policy] epoch #278 | Optimizing policy...
2022-04-23 14:20:53 | [train_policy] epoch #278 | Computing loss before
2022-04-23 14:20:53 | [train_policy] epoch #278 | Computing KL before
2022-04-23 14:20:53 | [train_policy] epoch #278 | Optimizing
2022-04-23 14:20:53 | [train_policy] epoch #278 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:53 | [train_policy] epoch #278 | computing loss before
2022-04-23 14:20:53 | [train_policy] epoch #278 | computing gradient
2022-04-23 14:20:53 | [train_policy] epoch #278 | gradient computed
2022-04-23 14:20:53 | [train_policy] epoch #278 | computing descent direction
2022-04-23 14:20:53 | [train_policy] epoch #278 | descent direction computed
2022-04-23 14:20:53 | [train_policy] epoch #278 | backtrack iters: 0
2022-04-23 14:20:53 | [train_policy] epoch #278 | optimization finished
2022-04-23 14:20:53 | [train_policy] epoch #278 | Computing KL after
2022-04-23 14:20:53 | [train_policy] epoch #278 | Computing loss after
2022-04-23 14:20:53 | [train_policy] epoch #278 | Fitting baseline...
2022-04-23 14:20:53 | [train_policy] epoch #278 | Saving snapshot...
2022-04-23 14:20:53 | [train_policy] epoch #278 | Saved
2022-04-23 14:20:53 | [train_policy] epoch #278 | Time 101.17 s
2022-04-23 14:20:53 | [train_policy] epoch #278 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116752
Evaluation/AverageDiscountedReturn          -66.9202
Evaluation/AverageReturn                    -66.9202
Evaluation/CompletionRate                     0
Evaluation/Iteration                        278
Evaluation/MaxReturn                        -32.3632
Evaluation/MinReturn                      -2061.58
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.195
Extras/EpisodeRewardMean                    -65.4191
LinearFeatureBaseline/ExplainedVariance       0.0944079
PolicyExecTime                                0.108296
ProcessExecTime                               0.0122225
TotalEnvSteps                            282348
policy/Entropy                                0.469151
policy/KL                                     0.00945035
policy/KLBefore                               0
policy/LossAfter                             -0.013005
policy/LossBefore                             2.85066e-08
policy/Perplexity                             1.59864
policy/dLoss                                  0.0130051
---------------------------------------  ----------------
2022-04-23 14:20:53 | [train_policy] epoch #279 | Obtaining samples for iteration 279...
2022-04-23 14:20:53 | [train_policy] epoch #279 | Logging diagnostics...
2022-04-23 14:20:53 | [train_policy] epoch #279 | Optimizing policy...
2022-04-23 14:20:53 | [train_policy] epoch #279 | Computing loss before
2022-04-23 14:20:53 | [train_policy] epoch #279 | Computing KL before
2022-04-23 14:20:53 | [train_policy] epoch #279 | Optimizing
2022-04-23 14:20:53 | [train_policy] epoch #279 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:53 | [train_policy] epoch #279 | computing loss before
2022-04-23 14:20:53 | [train_policy] epoch #279 | computing gradient
2022-04-23 14:20:53 | [train_policy] epoch #279 | gradient computed
2022-04-23 14:20:53 | [train_policy] epoch #279 | computing descent direction
2022-04-23 14:20:53 | [train_policy] epoch #279 | descent direction computed
2022-04-23 14:20:53 | [train_policy] epoch #279 | backtrack iters: 1
2022-04-23 14:20:53 | [train_policy] epoch #279 | optimization finished
2022-04-23 14:20:53 | [train_policy] epoch #279 | Computing KL after
2022-04-23 14:20:53 | [train_policy] epoch #279 | Computing loss after
2022-04-23 14:20:53 | [train_policy] epoch #279 | Fitting baseline...
2022-04-23 14:20:53 | [train_policy] epoch #279 | Saving snapshot...
2022-04-23 14:20:53 | [train_policy] epoch #279 | Saved
2022-04-23 14:20:53 | [train_policy] epoch #279 | Time 101.52 s
2022-04-23 14:20:53 | [train_policy] epoch #279 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.121274
Evaluation/AverageDiscountedReturn          -44.0507
Evaluation/AverageReturn                    -44.0507
Evaluation/CompletionRate                     0
Evaluation/Iteration                        279
Evaluation/MaxReturn                        -32.8228
Evaluation/MinReturn                        -68.7098
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.13163
Extras/EpisodeRewardMean                    -64.4135
LinearFeatureBaseline/ExplainedVariance     -26.7251
PolicyExecTime                                0.105226
ProcessExecTime                               0.0119803
TotalEnvSteps                            283360
policy/Entropy                                0.448027
policy/KL                                     0.00658971
policy/KLBefore                               0
policy/LossAfter                             -0.0204327
policy/LossBefore                             2.14388e-08
policy/Perplexity                             1.56522
policy/dLoss                                  0.0204327
---------------------------------------  ----------------
2022-04-23 14:20:53 | [train_policy] epoch #280 | Obtaining samples for iteration 280...
2022-04-23 14:20:54 | [train_policy] epoch #280 | Logging diagnostics...
2022-04-23 14:20:54 | [train_policy] epoch #280 | Optimizing policy...
2022-04-23 14:20:54 | [train_policy] epoch #280 | Computing loss before
2022-04-23 14:20:54 | [train_policy] epoch #280 | Computing KL before
2022-04-23 14:20:54 | [train_policy] epoch #280 | Optimizing
2022-04-23 14:20:54 | [train_policy] epoch #280 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:54 | [train_policy] epoch #280 | computing loss before
2022-04-23 14:20:54 | [train_policy] epoch #280 | computing gradient
2022-04-23 14:20:54 | [train_policy] epoch #280 | gradient computed
2022-04-23 14:20:54 | [train_policy] epoch #280 | computing descent direction
2022-04-23 14:20:54 | [train_policy] epoch #280 | descent direction computed
2022-04-23 14:20:54 | [train_policy] epoch #280 | backtrack iters: 1
2022-04-23 14:20:54 | [train_policy] epoch #280 | optimization finished
2022-04-23 14:20:54 | [train_policy] epoch #280 | Computing KL after
2022-04-23 14:20:54 | [train_policy] epoch #280 | Computing loss after
2022-04-23 14:20:54 | [train_policy] epoch #280 | Fitting baseline...
2022-04-23 14:20:54 | [train_policy] epoch #280 | Saving snapshot...
2022-04-23 14:20:54 | [train_policy] epoch #280 | Saved
2022-04-23 14:20:54 | [train_policy] epoch #280 | Time 101.88 s
2022-04-23 14:20:54 | [train_policy] epoch #280 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117613
Evaluation/AverageDiscountedReturn          -44.3721
Evaluation/AverageReturn                    -44.3721
Evaluation/CompletionRate                     0
Evaluation/Iteration                        280
Evaluation/MaxReturn                        -32.9123
Evaluation/MinReturn                        -64.6725
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.86168
Extras/EpisodeRewardMean                    -44.2726
LinearFeatureBaseline/ExplainedVariance       0.946671
PolicyExecTime                                0.106036
ProcessExecTime                               0.0114033
TotalEnvSteps                            284372
policy/Entropy                                0.408344
policy/KL                                     0.00728202
policy/KLBefore                               0
policy/LossAfter                             -0.0218894
policy/LossBefore                            -6.36097e-09
policy/Perplexity                             1.50433
policy/dLoss                                  0.0218894
---------------------------------------  ----------------
2022-04-23 14:20:54 | [train_policy] epoch #281 | Obtaining samples for iteration 281...
2022-04-23 14:20:54 | [train_policy] epoch #281 | Logging diagnostics...
2022-04-23 14:20:54 | [train_policy] epoch #281 | Optimizing policy...
2022-04-23 14:20:54 | [train_policy] epoch #281 | Computing loss before
2022-04-23 14:20:54 | [train_policy] epoch #281 | Computing KL before
2022-04-23 14:20:54 | [train_policy] epoch #281 | Optimizing
2022-04-23 14:20:54 | [train_policy] epoch #281 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:54 | [train_policy] epoch #281 | computing loss before
2022-04-23 14:20:54 | [train_policy] epoch #281 | computing gradient
2022-04-23 14:20:54 | [train_policy] epoch #281 | gradient computed
2022-04-23 14:20:54 | [train_policy] epoch #281 | computing descent direction
2022-04-23 14:20:54 | [train_policy] epoch #281 | descent direction computed
2022-04-23 14:20:54 | [train_policy] epoch #281 | backtrack iters: 0
2022-04-23 14:20:54 | [train_policy] epoch #281 | optimization finished
2022-04-23 14:20:54 | [train_policy] epoch #281 | Computing KL after
2022-04-23 14:20:54 | [train_policy] epoch #281 | Computing loss after
2022-04-23 14:20:54 | [train_policy] epoch #281 | Fitting baseline...
2022-04-23 14:20:54 | [train_policy] epoch #281 | Saving snapshot...
2022-04-23 14:20:54 | [train_policy] epoch #281 | Saved
2022-04-23 14:20:54 | [train_policy] epoch #281 | Time 102.24 s
2022-04-23 14:20:54 | [train_policy] epoch #281 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.118421
Evaluation/AverageDiscountedReturn          -45.1333
Evaluation/AverageReturn                    -45.1333
Evaluation/CompletionRate                     0
Evaluation/Iteration                        281
Evaluation/MaxReturn                        -32.199
Evaluation/MinReturn                        -68.9847
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.31054
Extras/EpisodeRewardMean                    -45.28
LinearFeatureBaseline/ExplainedVariance       0.883635
PolicyExecTime                                0.108482
ProcessExecTime                               0.0113347
TotalEnvSteps                            285384
policy/Entropy                                0.343572
policy/KL                                     0.00948779
policy/KLBefore                               0
policy/LossAfter                             -0.0200231
policy/LossBefore                            -3.06269e-08
policy/Perplexity                             1.40998
policy/dLoss                                  0.0200231
---------------------------------------  ----------------
2022-04-23 14:20:54 | [train_policy] epoch #282 | Obtaining samples for iteration 282...
2022-04-23 14:20:54 | [train_policy] epoch #282 | Logging diagnostics...
2022-04-23 14:20:54 | [train_policy] epoch #282 | Optimizing policy...
2022-04-23 14:20:54 | [train_policy] epoch #282 | Computing loss before
2022-04-23 14:20:54 | [train_policy] epoch #282 | Computing KL before
2022-04-23 14:20:54 | [train_policy] epoch #282 | Optimizing
2022-04-23 14:20:54 | [train_policy] epoch #282 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:54 | [train_policy] epoch #282 | computing loss before
2022-04-23 14:20:54 | [train_policy] epoch #282 | computing gradient
2022-04-23 14:20:54 | [train_policy] epoch #282 | gradient computed
2022-04-23 14:20:54 | [train_policy] epoch #282 | computing descent direction
2022-04-23 14:20:54 | [train_policy] epoch #282 | descent direction computed
2022-04-23 14:20:54 | [train_policy] epoch #282 | backtrack iters: 1
2022-04-23 14:20:54 | [train_policy] epoch #282 | optimization finished
2022-04-23 14:20:54 | [train_policy] epoch #282 | Computing KL after
2022-04-23 14:20:54 | [train_policy] epoch #282 | Computing loss after
2022-04-23 14:20:54 | [train_policy] epoch #282 | Fitting baseline...
2022-04-23 14:20:54 | [train_policy] epoch #282 | Saving snapshot...
2022-04-23 14:20:54 | [train_policy] epoch #282 | Saved
2022-04-23 14:20:54 | [train_policy] epoch #282 | Time 102.61 s
2022-04-23 14:20:54 | [train_policy] epoch #282 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.127414
Evaluation/AverageDiscountedReturn          -42.645
Evaluation/AverageReturn                    -42.645
Evaluation/CompletionRate                     0
Evaluation/Iteration                        282
Evaluation/MaxReturn                        -32.459
Evaluation/MinReturn                        -60.8112
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.90487
Extras/EpisodeRewardMean                    -42.9528
LinearFeatureBaseline/ExplainedVariance       0.935441
PolicyExecTime                                0.104959
ProcessExecTime                               0.0126672
TotalEnvSteps                            286396
policy/Entropy                                0.332903
policy/KL                                     0.00754144
policy/KLBefore                               0
policy/LossAfter                             -0.0203957
policy/LossBefore                            -2.62685e-08
policy/Perplexity                             1.39501
policy/dLoss                                  0.0203957
---------------------------------------  ----------------
2022-04-23 14:20:54 | [train_policy] epoch #283 | Obtaining samples for iteration 283...
2022-04-23 14:20:55 | [train_policy] epoch #283 | Logging diagnostics...
2022-04-23 14:20:55 | [train_policy] epoch #283 | Optimizing policy...
2022-04-23 14:20:55 | [train_policy] epoch #283 | Computing loss before
2022-04-23 14:20:55 | [train_policy] epoch #283 | Computing KL before
2022-04-23 14:20:55 | [train_policy] epoch #283 | Optimizing
2022-04-23 14:20:55 | [train_policy] epoch #283 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:55 | [train_policy] epoch #283 | computing loss before
2022-04-23 14:20:55 | [train_policy] epoch #283 | computing gradient
2022-04-23 14:20:55 | [train_policy] epoch #283 | gradient computed
2022-04-23 14:20:55 | [train_policy] epoch #283 | computing descent direction
2022-04-23 14:20:55 | [train_policy] epoch #283 | descent direction computed
2022-04-23 14:20:55 | [train_policy] epoch #283 | backtrack iters: 1
2022-04-23 14:20:55 | [train_policy] epoch #283 | optimization finished
2022-04-23 14:20:55 | [train_policy] epoch #283 | Computing KL after
2022-04-23 14:20:55 | [train_policy] epoch #283 | Computing loss after
2022-04-23 14:20:55 | [train_policy] epoch #283 | Fitting baseline...
2022-04-23 14:20:55 | [train_policy] epoch #283 | Saving snapshot...
2022-04-23 14:20:55 | [train_policy] epoch #283 | Saved
2022-04-23 14:20:55 | [train_policy] epoch #283 | Time 102.99 s
2022-04-23 14:20:55 | [train_policy] epoch #283 | EpochTime 0.38 s
---------------------------------------  ---------------
EnvExecTime                                   0.130428
Evaluation/AverageDiscountedReturn          -44.6814
Evaluation/AverageReturn                    -44.6814
Evaluation/CompletionRate                     0
Evaluation/Iteration                        283
Evaluation/MaxReturn                        -33.6683
Evaluation/MinReturn                       -101.394
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.03753
Extras/EpisodeRewardMean                    -44.5198
LinearFeatureBaseline/ExplainedVariance       0.876663
PolicyExecTime                                0.114667
ProcessExecTime                               0.0130007
TotalEnvSteps                            287408
policy/Entropy                                0.310346
policy/KL                                     0.0066695
policy/KLBefore                               0
policy/LossAfter                             -0.0242472
policy/LossBefore                             2.8271e-09
policy/Perplexity                             1.3639
policy/dLoss                                  0.0242472
---------------------------------------  ---------------
2022-04-23 14:20:55 | [train_policy] epoch #284 | Obtaining samples for iteration 284...
2022-04-23 14:20:55 | [train_policy] epoch #284 | Logging diagnostics...
2022-04-23 14:20:55 | [train_policy] epoch #284 | Optimizing policy...
2022-04-23 14:20:55 | [train_policy] epoch #284 | Computing loss before
2022-04-23 14:20:55 | [train_policy] epoch #284 | Computing KL before
2022-04-23 14:20:55 | [train_policy] epoch #284 | Optimizing
2022-04-23 14:20:55 | [train_policy] epoch #284 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:55 | [train_policy] epoch #284 | computing loss before
2022-04-23 14:20:55 | [train_policy] epoch #284 | computing gradient
2022-04-23 14:20:55 | [train_policy] epoch #284 | gradient computed
2022-04-23 14:20:55 | [train_policy] epoch #284 | computing descent direction
2022-04-23 14:20:55 | [train_policy] epoch #284 | descent direction computed
2022-04-23 14:20:55 | [train_policy] epoch #284 | backtrack iters: 1
2022-04-23 14:20:55 | [train_policy] epoch #284 | optimization finished
2022-04-23 14:20:55 | [train_policy] epoch #284 | Computing KL after
2022-04-23 14:20:55 | [train_policy] epoch #284 | Computing loss after
2022-04-23 14:20:55 | [train_policy] epoch #284 | Fitting baseline...
2022-04-23 14:20:55 | [train_policy] epoch #284 | Saving snapshot...
2022-04-23 14:20:55 | [train_policy] epoch #284 | Saved
2022-04-23 14:20:55 | [train_policy] epoch #284 | Time 103.36 s
2022-04-23 14:20:55 | [train_policy] epoch #284 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.130942
Evaluation/AverageDiscountedReturn          -88.3412
Evaluation/AverageReturn                    -88.3412
Evaluation/CompletionRate                     0
Evaluation/Iteration                        284
Evaluation/MaxReturn                        -32.341
Evaluation/MinReturn                      -2096.89
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        297.257
Extras/EpisodeRewardMean                    -85.0699
LinearFeatureBaseline/ExplainedVariance       0.0101061
PolicyExecTime                                0.10955
ProcessExecTime                               0.0128703
TotalEnvSteps                            288420
policy/Entropy                                0.303891
policy/KL                                     0.00710813
policy/KLBefore                               0
policy/LossAfter                             -0.0334842
policy/LossBefore                            -1.41355e-08
policy/Perplexity                             1.35512
policy/dLoss                                  0.0334841
---------------------------------------  ----------------
2022-04-23 14:20:55 | [train_policy] epoch #285 | Obtaining samples for iteration 285...
2022-04-23 14:20:55 | [train_policy] epoch #285 | Logging diagnostics...
2022-04-23 14:20:55 | [train_policy] epoch #285 | Optimizing policy...
2022-04-23 14:20:55 | [train_policy] epoch #285 | Computing loss before
2022-04-23 14:20:55 | [train_policy] epoch #285 | Computing KL before
2022-04-23 14:20:55 | [train_policy] epoch #285 | Optimizing
2022-04-23 14:20:55 | [train_policy] epoch #285 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:55 | [train_policy] epoch #285 | computing loss before
2022-04-23 14:20:55 | [train_policy] epoch #285 | computing gradient
2022-04-23 14:20:55 | [train_policy] epoch #285 | gradient computed
2022-04-23 14:20:55 | [train_policy] epoch #285 | computing descent direction
2022-04-23 14:20:55 | [train_policy] epoch #285 | descent direction computed
2022-04-23 14:20:55 | [train_policy] epoch #285 | backtrack iters: 1
2022-04-23 14:20:55 | [train_policy] epoch #285 | optimization finished
2022-04-23 14:20:55 | [train_policy] epoch #285 | Computing KL after
2022-04-23 14:20:55 | [train_policy] epoch #285 | Computing loss after
2022-04-23 14:20:56 | [train_policy] epoch #285 | Fitting baseline...
2022-04-23 14:20:56 | [train_policy] epoch #285 | Saving snapshot...
2022-04-23 14:20:56 | [train_policy] epoch #285 | Saved
2022-04-23 14:20:56 | [train_policy] epoch #285 | Time 103.73 s
2022-04-23 14:20:56 | [train_policy] epoch #285 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.131353
Evaluation/AverageDiscountedReturn          -65.6811
Evaluation/AverageReturn                    -65.6811
Evaluation/CompletionRate                     0
Evaluation/Iteration                        285
Evaluation/MaxReturn                        -31.0013
Evaluation/MinReturn                      -2066.37
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.845
Extras/EpisodeRewardMean                    -64.349
LinearFeatureBaseline/ExplainedVariance      -0.248436
PolicyExecTime                                0.107194
ProcessExecTime                               0.0128865
TotalEnvSteps                            289432
policy/Entropy                                0.297996
policy/KL                                     0.0067522
policy/KLBefore                               0
policy/LossAfter                             -0.024689
policy/LossBefore                             8.71688e-09
policy/Perplexity                             1.34716
policy/dLoss                                  0.024689
---------------------------------------  ----------------
2022-04-23 14:20:56 | [train_policy] epoch #286 | Obtaining samples for iteration 286...
2022-04-23 14:20:56 | [train_policy] epoch #286 | Logging diagnostics...
2022-04-23 14:20:56 | [train_policy] epoch #286 | Optimizing policy...
2022-04-23 14:20:56 | [train_policy] epoch #286 | Computing loss before
2022-04-23 14:20:56 | [train_policy] epoch #286 | Computing KL before
2022-04-23 14:20:56 | [train_policy] epoch #286 | Optimizing
2022-04-23 14:20:56 | [train_policy] epoch #286 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:56 | [train_policy] epoch #286 | computing loss before
2022-04-23 14:20:56 | [train_policy] epoch #286 | computing gradient
2022-04-23 14:20:56 | [train_policy] epoch #286 | gradient computed
2022-04-23 14:20:56 | [train_policy] epoch #286 | computing descent direction
2022-04-23 14:20:56 | [train_policy] epoch #286 | descent direction computed
2022-04-23 14:20:56 | [train_policy] epoch #286 | backtrack iters: 1
2022-04-23 14:20:56 | [train_policy] epoch #286 | optimization finished
2022-04-23 14:20:56 | [train_policy] epoch #286 | Computing KL after
2022-04-23 14:20:56 | [train_policy] epoch #286 | Computing loss after
2022-04-23 14:20:56 | [train_policy] epoch #286 | Fitting baseline...
2022-04-23 14:20:56 | [train_policy] epoch #286 | Saving snapshot...
2022-04-23 14:20:56 | [train_policy] epoch #286 | Saved
2022-04-23 14:20:56 | [train_policy] epoch #286 | Time 104.10 s
2022-04-23 14:20:56 | [train_policy] epoch #286 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.128196
Evaluation/AverageDiscountedReturn          -43.6006
Evaluation/AverageReturn                    -43.6006
Evaluation/CompletionRate                     0
Evaluation/Iteration                        286
Evaluation/MaxReturn                        -31.6444
Evaluation/MinReturn                        -70.2906
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43404
Extras/EpisodeRewardMean                    -43.5065
LinearFeatureBaseline/ExplainedVariance     -24.5859
PolicyExecTime                                0.105838
ProcessExecTime                               0.0123932
TotalEnvSteps                            290444
policy/Entropy                                0.323885
policy/KL                                     0.00647265
policy/KLBefore                               0
policy/LossAfter                             -0.0198478
policy/LossBefore                            -0
policy/Perplexity                             1.38249
policy/dLoss                                  0.0198478
---------------------------------------  ---------------
2022-04-23 14:20:56 | [train_policy] epoch #287 | Obtaining samples for iteration 287...
2022-04-23 14:20:56 | [train_policy] epoch #287 | Logging diagnostics...
2022-04-23 14:20:56 | [train_policy] epoch #287 | Optimizing policy...
2022-04-23 14:20:56 | [train_policy] epoch #287 | Computing loss before
2022-04-23 14:20:56 | [train_policy] epoch #287 | Computing KL before
2022-04-23 14:20:56 | [train_policy] epoch #287 | Optimizing
2022-04-23 14:20:56 | [train_policy] epoch #287 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:56 | [train_policy] epoch #287 | computing loss before
2022-04-23 14:20:56 | [train_policy] epoch #287 | computing gradient
2022-04-23 14:20:56 | [train_policy] epoch #287 | gradient computed
2022-04-23 14:20:56 | [train_policy] epoch #287 | computing descent direction
2022-04-23 14:20:56 | [train_policy] epoch #287 | descent direction computed
2022-04-23 14:20:56 | [train_policy] epoch #287 | backtrack iters: 1
2022-04-23 14:20:56 | [train_policy] epoch #287 | optimization finished
2022-04-23 14:20:56 | [train_policy] epoch #287 | Computing KL after
2022-04-23 14:20:56 | [train_policy] epoch #287 | Computing loss after
2022-04-23 14:20:56 | [train_policy] epoch #287 | Fitting baseline...
2022-04-23 14:20:56 | [train_policy] epoch #287 | Saving snapshot...
2022-04-23 14:20:56 | [train_policy] epoch #287 | Saved
2022-04-23 14:20:56 | [train_policy] epoch #287 | Time 104.47 s
2022-04-23 14:20:56 | [train_policy] epoch #287 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.127177
Evaluation/AverageDiscountedReturn          -45.3793
Evaluation/AverageReturn                    -45.3793
Evaluation/CompletionRate                     0
Evaluation/Iteration                        287
Evaluation/MaxReturn                        -36.3483
Evaluation/MinReturn                        -69.6322
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.77302
Extras/EpisodeRewardMean                    -45.7315
LinearFeatureBaseline/ExplainedVariance       0.918144
PolicyExecTime                                0.107918
ProcessExecTime                               0.0122924
TotalEnvSteps                            291456
policy/Entropy                                0.355703
policy/KL                                     0.00810373
policy/KLBefore                               0
policy/LossAfter                             -0.022969
policy/LossBefore                            -3.39252e-08
policy/Perplexity                             1.42718
policy/dLoss                                  0.022969
---------------------------------------  ----------------
2022-04-23 14:20:56 | [train_policy] epoch #288 | Obtaining samples for iteration 288...
2022-04-23 14:20:57 | [train_policy] epoch #288 | Logging diagnostics...
2022-04-23 14:20:57 | [train_policy] epoch #288 | Optimizing policy...
2022-04-23 14:20:57 | [train_policy] epoch #288 | Computing loss before
2022-04-23 14:20:57 | [train_policy] epoch #288 | Computing KL before
2022-04-23 14:20:57 | [train_policy] epoch #288 | Optimizing
2022-04-23 14:20:57 | [train_policy] epoch #288 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:57 | [train_policy] epoch #288 | computing loss before
2022-04-23 14:20:57 | [train_policy] epoch #288 | computing gradient
2022-04-23 14:20:57 | [train_policy] epoch #288 | gradient computed
2022-04-23 14:20:57 | [train_policy] epoch #288 | computing descent direction
2022-04-23 14:20:57 | [train_policy] epoch #288 | descent direction computed
2022-04-23 14:20:57 | [train_policy] epoch #288 | backtrack iters: 1
2022-04-23 14:20:57 | [train_policy] epoch #288 | optimization finished
2022-04-23 14:20:57 | [train_policy] epoch #288 | Computing KL after
2022-04-23 14:20:57 | [train_policy] epoch #288 | Computing loss after
2022-04-23 14:20:57 | [train_policy] epoch #288 | Fitting baseline...
2022-04-23 14:20:57 | [train_policy] epoch #288 | Saving snapshot...
2022-04-23 14:20:57 | [train_policy] epoch #288 | Saved
2022-04-23 14:20:57 | [train_policy] epoch #288 | Time 104.84 s
2022-04-23 14:20:57 | [train_policy] epoch #288 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.12815
Evaluation/AverageDiscountedReturn          -64.8023
Evaluation/AverageReturn                    -64.8023
Evaluation/CompletionRate                     0
Evaluation/Iteration                        288
Evaluation/MaxReturn                        -29.9004
Evaluation/MinReturn                      -2062.89
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.551
Extras/EpisodeRewardMean                    -63.2022
LinearFeatureBaseline/ExplainedVariance       0.0139926
PolicyExecTime                                0.107032
ProcessExecTime                               0.0126865
TotalEnvSteps                            292468
policy/Entropy                                0.345068
policy/KL                                     0.0064094
policy/KLBefore                               0
policy/LossAfter                             -0.0191224
policy/LossBefore                             6.12538e-09
policy/Perplexity                             1.41209
policy/dLoss                                  0.0191224
---------------------------------------  ----------------
2022-04-23 14:20:57 | [train_policy] epoch #289 | Obtaining samples for iteration 289...
2022-04-23 14:20:57 | [train_policy] epoch #289 | Logging diagnostics...
2022-04-23 14:20:57 | [train_policy] epoch #289 | Optimizing policy...
2022-04-23 14:20:57 | [train_policy] epoch #289 | Computing loss before
2022-04-23 14:20:57 | [train_policy] epoch #289 | Computing KL before
2022-04-23 14:20:57 | [train_policy] epoch #289 | Optimizing
2022-04-23 14:20:57 | [train_policy] epoch #289 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:57 | [train_policy] epoch #289 | computing loss before
2022-04-23 14:20:57 | [train_policy] epoch #289 | computing gradient
2022-04-23 14:20:57 | [train_policy] epoch #289 | gradient computed
2022-04-23 14:20:57 | [train_policy] epoch #289 | computing descent direction
2022-04-23 14:20:57 | [train_policy] epoch #289 | descent direction computed
2022-04-23 14:20:57 | [train_policy] epoch #289 | backtrack iters: 1
2022-04-23 14:20:57 | [train_policy] epoch #289 | optimization finished
2022-04-23 14:20:57 | [train_policy] epoch #289 | Computing KL after
2022-04-23 14:20:57 | [train_policy] epoch #289 | Computing loss after
2022-04-23 14:20:57 | [train_policy] epoch #289 | Fitting baseline...
2022-04-23 14:20:57 | [train_policy] epoch #289 | Saving snapshot...
2022-04-23 14:20:57 | [train_policy] epoch #289 | Saved
2022-04-23 14:20:57 | [train_policy] epoch #289 | Time 105.19 s
2022-04-23 14:20:57 | [train_policy] epoch #289 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119206
Evaluation/AverageDiscountedReturn          -44.0392
Evaluation/AverageReturn                    -44.0392
Evaluation/CompletionRate                     0
Evaluation/Iteration                        289
Evaluation/MaxReturn                        -32.5811
Evaluation/MinReturn                        -66.6803
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.21329
Extras/EpisodeRewardMean                    -43.9339
LinearFeatureBaseline/ExplainedVariance     -42.7015
PolicyExecTime                                0.103218
ProcessExecTime                               0.0114331
TotalEnvSteps                            293480
policy/Entropy                                0.34088
policy/KL                                     0.00712779
policy/KLBefore                               0
policy/LossAfter                             -0.0194406
policy/LossBefore                            -9.42366e-09
policy/Perplexity                             1.40618
policy/dLoss                                  0.0194406
---------------------------------------  ----------------
2022-04-23 14:20:57 | [train_policy] epoch #290 | Obtaining samples for iteration 290...
2022-04-23 14:20:57 | [train_policy] epoch #290 | Logging diagnostics...
2022-04-23 14:20:57 | [train_policy] epoch #290 | Optimizing policy...
2022-04-23 14:20:57 | [train_policy] epoch #290 | Computing loss before
2022-04-23 14:20:57 | [train_policy] epoch #290 | Computing KL before
2022-04-23 14:20:57 | [train_policy] epoch #290 | Optimizing
2022-04-23 14:20:57 | [train_policy] epoch #290 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:57 | [train_policy] epoch #290 | computing loss before
2022-04-23 14:20:57 | [train_policy] epoch #290 | computing gradient
2022-04-23 14:20:57 | [train_policy] epoch #290 | gradient computed
2022-04-23 14:20:57 | [train_policy] epoch #290 | computing descent direction
2022-04-23 14:20:57 | [train_policy] epoch #290 | descent direction computed
2022-04-23 14:20:57 | [train_policy] epoch #290 | backtrack iters: 0
2022-04-23 14:20:57 | [train_policy] epoch #290 | optimization finished
2022-04-23 14:20:57 | [train_policy] epoch #290 | Computing KL after
2022-04-23 14:20:57 | [train_policy] epoch #290 | Computing loss after
2022-04-23 14:20:57 | [train_policy] epoch #290 | Fitting baseline...
2022-04-23 14:20:57 | [train_policy] epoch #290 | Saving snapshot...
2022-04-23 14:20:57 | [train_policy] epoch #290 | Saved
2022-04-23 14:20:57 | [train_policy] epoch #290 | Time 105.54 s
2022-04-23 14:20:57 | [train_policy] epoch #290 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117847
Evaluation/AverageDiscountedReturn         -108.926
Evaluation/AverageReturn                   -108.926
Evaluation/CompletionRate                     0
Evaluation/Iteration                        290
Evaluation/MaxReturn                        -31.1469
Evaluation/MinReturn                      -4052.36
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        463.706
Extras/EpisodeRewardMean                   -103.585
LinearFeatureBaseline/ExplainedVariance       0.00689328
PolicyExecTime                                0.105295
ProcessExecTime                               0.0113914
TotalEnvSteps                            294492
policy/Entropy                                0.332192
policy/KL                                     0.00930477
policy/KLBefore                               0
policy/LossAfter                             -0.0377752
policy/LossBefore                             5.88979e-09
policy/Perplexity                             1.39402
policy/dLoss                                  0.0377752
---------------------------------------  ----------------
2022-04-23 14:20:57 | [train_policy] epoch #291 | Obtaining samples for iteration 291...
2022-04-23 14:20:58 | [train_policy] epoch #291 | Logging diagnostics...
2022-04-23 14:20:58 | [train_policy] epoch #291 | Optimizing policy...
2022-04-23 14:20:58 | [train_policy] epoch #291 | Computing loss before
2022-04-23 14:20:58 | [train_policy] epoch #291 | Computing KL before
2022-04-23 14:20:58 | [train_policy] epoch #291 | Optimizing
2022-04-23 14:20:58 | [train_policy] epoch #291 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:58 | [train_policy] epoch #291 | computing loss before
2022-04-23 14:20:58 | [train_policy] epoch #291 | computing gradient
2022-04-23 14:20:58 | [train_policy] epoch #291 | gradient computed
2022-04-23 14:20:58 | [train_policy] epoch #291 | computing descent direction
2022-04-23 14:20:58 | [train_policy] epoch #291 | descent direction computed
2022-04-23 14:20:58 | [train_policy] epoch #291 | backtrack iters: 1
2022-04-23 14:20:58 | [train_policy] epoch #291 | optimization finished
2022-04-23 14:20:58 | [train_policy] epoch #291 | Computing KL after
2022-04-23 14:20:58 | [train_policy] epoch #291 | Computing loss after
2022-04-23 14:20:58 | [train_policy] epoch #291 | Fitting baseline...
2022-04-23 14:20:58 | [train_policy] epoch #291 | Saving snapshot...
2022-04-23 14:20:58 | [train_policy] epoch #291 | Saved
2022-04-23 14:20:58 | [train_policy] epoch #291 | Time 105.89 s
2022-04-23 14:20:58 | [train_policy] epoch #291 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116018
Evaluation/AverageDiscountedReturn          -87.7761
Evaluation/AverageReturn                    -87.7761
Evaluation/CompletionRate                     0
Evaluation/Iteration                        291
Evaluation/MaxReturn                        -31.7977
Evaluation/MinReturn                      -2068.89
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.078
Extras/EpisodeRewardMean                    -84.3859
LinearFeatureBaseline/ExplainedVariance      -0.103761
PolicyExecTime                                0.0960529
ProcessExecTime                               0.0110705
TotalEnvSteps                            295504
policy/Entropy                                0.327761
policy/KL                                     0.00705867
policy/KLBefore                               0
policy/LossAfter                             -0.0150405
policy/LossBefore                             4.02861e-08
policy/Perplexity                             1.38786
policy/dLoss                                  0.0150406
---------------------------------------  ----------------
2022-04-23 14:20:58 | [train_policy] epoch #292 | Obtaining samples for iteration 292...
2022-04-23 14:20:58 | [train_policy] epoch #292 | Logging diagnostics...
2022-04-23 14:20:58 | [train_policy] epoch #292 | Optimizing policy...
2022-04-23 14:20:58 | [train_policy] epoch #292 | Computing loss before
2022-04-23 14:20:58 | [train_policy] epoch #292 | Computing KL before
2022-04-23 14:20:58 | [train_policy] epoch #292 | Optimizing
2022-04-23 14:20:58 | [train_policy] epoch #292 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:58 | [train_policy] epoch #292 | computing loss before
2022-04-23 14:20:58 | [train_policy] epoch #292 | computing gradient
2022-04-23 14:20:58 | [train_policy] epoch #292 | gradient computed
2022-04-23 14:20:58 | [train_policy] epoch #292 | computing descent direction
2022-04-23 14:20:58 | [train_policy] epoch #292 | descent direction computed
2022-04-23 14:20:58 | [train_policy] epoch #292 | backtrack iters: 0
2022-04-23 14:20:58 | [train_policy] epoch #292 | optimization finished
2022-04-23 14:20:58 | [train_policy] epoch #292 | Computing KL after
2022-04-23 14:20:58 | [train_policy] epoch #292 | Computing loss after
2022-04-23 14:20:58 | [train_policy] epoch #292 | Fitting baseline...
2022-04-23 14:20:58 | [train_policy] epoch #292 | Saving snapshot...
2022-04-23 14:20:58 | [train_policy] epoch #292 | Saved
2022-04-23 14:20:58 | [train_policy] epoch #292 | Time 106.23 s
2022-04-23 14:20:58 | [train_policy] epoch #292 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115921
Evaluation/AverageDiscountedReturn          -42.9198
Evaluation/AverageReturn                    -42.9198
Evaluation/CompletionRate                     0
Evaluation/Iteration                        292
Evaluation/MaxReturn                        -30.4948
Evaluation/MinReturn                        -66.424
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.2526
Extras/EpisodeRewardMean                    -83.7401
LinearFeatureBaseline/ExplainedVariance     -58.7644
PolicyExecTime                                0.0974009
ProcessExecTime                               0.0110948
TotalEnvSteps                            296516
policy/Entropy                                0.38114
policy/KL                                     0.0099242
policy/KLBefore                               0
policy/LossAfter                             -0.0404691
policy/LossBefore                            -4.98276e-08
policy/Perplexity                             1.46395
policy/dLoss                                  0.040469
---------------------------------------  ----------------
2022-04-23 14:20:58 | [train_policy] epoch #293 | Obtaining samples for iteration 293...
2022-04-23 14:20:58 | [train_policy] epoch #293 | Logging diagnostics...
2022-04-23 14:20:58 | [train_policy] epoch #293 | Optimizing policy...
2022-04-23 14:20:58 | [train_policy] epoch #293 | Computing loss before
2022-04-23 14:20:58 | [train_policy] epoch #293 | Computing KL before
2022-04-23 14:20:58 | [train_policy] epoch #293 | Optimizing
2022-04-23 14:20:58 | [train_policy] epoch #293 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:58 | [train_policy] epoch #293 | computing loss before
2022-04-23 14:20:58 | [train_policy] epoch #293 | computing gradient
2022-04-23 14:20:58 | [train_policy] epoch #293 | gradient computed
2022-04-23 14:20:58 | [train_policy] epoch #293 | computing descent direction
2022-04-23 14:20:58 | [train_policy] epoch #293 | descent direction computed
2022-04-23 14:20:58 | [train_policy] epoch #293 | backtrack iters: 1
2022-04-23 14:20:58 | [train_policy] epoch #293 | optimization finished
2022-04-23 14:20:58 | [train_policy] epoch #293 | Computing KL after
2022-04-23 14:20:58 | [train_policy] epoch #293 | Computing loss after
2022-04-23 14:20:58 | [train_policy] epoch #293 | Fitting baseline...
2022-04-23 14:20:58 | [train_policy] epoch #293 | Saving snapshot...
2022-04-23 14:20:58 | [train_policy] epoch #293 | Saved
2022-04-23 14:20:58 | [train_policy] epoch #293 | Time 106.58 s
2022-04-23 14:20:58 | [train_policy] epoch #293 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116774
Evaluation/AverageDiscountedReturn          -64.7764
Evaluation/AverageReturn                    -64.7764
Evaluation/CompletionRate                     0
Evaluation/Iteration                        293
Evaluation/MaxReturn                        -31.9016
Evaluation/MinReturn                      -2067.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.009
Extras/EpisodeRewardMean                    -62.9004
LinearFeatureBaseline/ExplainedVariance       0.0119781
PolicyExecTime                                0.104035
ProcessExecTime                               0.0115097
TotalEnvSteps                            297528
policy/Entropy                                0.373699
policy/KL                                     0.0065941
policy/KLBefore                               0
policy/LossAfter                             -0.0209065
policy/LossBefore                             2.35591e-09
policy/Perplexity                             1.4531
policy/dLoss                                  0.0209065
---------------------------------------  ----------------
2022-04-23 14:20:58 | [train_policy] epoch #294 | Obtaining samples for iteration 294...
2022-04-23 14:20:59 | [train_policy] epoch #294 | Logging diagnostics...
2022-04-23 14:20:59 | [train_policy] epoch #294 | Optimizing policy...
2022-04-23 14:20:59 | [train_policy] epoch #294 | Computing loss before
2022-04-23 14:20:59 | [train_policy] epoch #294 | Computing KL before
2022-04-23 14:20:59 | [train_policy] epoch #294 | Optimizing
2022-04-23 14:20:59 | [train_policy] epoch #294 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:59 | [train_policy] epoch #294 | computing loss before
2022-04-23 14:20:59 | [train_policy] epoch #294 | computing gradient
2022-04-23 14:20:59 | [train_policy] epoch #294 | gradient computed
2022-04-23 14:20:59 | [train_policy] epoch #294 | computing descent direction
2022-04-23 14:20:59 | [train_policy] epoch #294 | descent direction computed
2022-04-23 14:20:59 | [train_policy] epoch #294 | backtrack iters: 0
2022-04-23 14:20:59 | [train_policy] epoch #294 | optimization finished
2022-04-23 14:20:59 | [train_policy] epoch #294 | Computing KL after
2022-04-23 14:20:59 | [train_policy] epoch #294 | Computing loss after
2022-04-23 14:20:59 | [train_policy] epoch #294 | Fitting baseline...
2022-04-23 14:20:59 | [train_policy] epoch #294 | Saving snapshot...
2022-04-23 14:20:59 | [train_policy] epoch #294 | Saved
2022-04-23 14:20:59 | [train_policy] epoch #294 | Time 106.95 s
2022-04-23 14:20:59 | [train_policy] epoch #294 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.116827
Evaluation/AverageDiscountedReturn          -65.7833
Evaluation/AverageReturn                    -65.7833
Evaluation/CompletionRate                     0
Evaluation/Iteration                        294
Evaluation/MaxReturn                        -30.245
Evaluation/MinReturn                      -2114.13
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        214.841
Extras/EpisodeRewardMean                    -84.2993
LinearFeatureBaseline/ExplainedVariance       0.0579537
PolicyExecTime                                0.118252
ProcessExecTime                               0.0116305
TotalEnvSteps                            298540
policy/Entropy                                0.38154
policy/KL                                     0.00924873
policy/KLBefore                               0
policy/LossAfter                             -0.0377885
policy/LossBefore                             1.41355e-08
policy/Perplexity                             1.46454
policy/dLoss                                  0.0377885
---------------------------------------  ----------------
2022-04-23 14:20:59 | [train_policy] epoch #295 | Obtaining samples for iteration 295...
2022-04-23 14:20:59 | [train_policy] epoch #295 | Logging diagnostics...
2022-04-23 14:20:59 | [train_policy] epoch #295 | Optimizing policy...
2022-04-23 14:20:59 | [train_policy] epoch #295 | Computing loss before
2022-04-23 14:20:59 | [train_policy] epoch #295 | Computing KL before
2022-04-23 14:20:59 | [train_policy] epoch #295 | Optimizing
2022-04-23 14:20:59 | [train_policy] epoch #295 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:59 | [train_policy] epoch #295 | computing loss before
2022-04-23 14:20:59 | [train_policy] epoch #295 | computing gradient
2022-04-23 14:20:59 | [train_policy] epoch #295 | gradient computed
2022-04-23 14:20:59 | [train_policy] epoch #295 | computing descent direction
2022-04-23 14:20:59 | [train_policy] epoch #295 | descent direction computed
2022-04-23 14:20:59 | [train_policy] epoch #295 | backtrack iters: 1
2022-04-23 14:20:59 | [train_policy] epoch #295 | optimization finished
2022-04-23 14:20:59 | [train_policy] epoch #295 | Computing KL after
2022-04-23 14:20:59 | [train_policy] epoch #295 | Computing loss after
2022-04-23 14:20:59 | [train_policy] epoch #295 | Fitting baseline...
2022-04-23 14:20:59 | [train_policy] epoch #295 | Saving snapshot...
2022-04-23 14:20:59 | [train_policy] epoch #295 | Saved
2022-04-23 14:20:59 | [train_policy] epoch #295 | Time 107.29 s
2022-04-23 14:20:59 | [train_policy] epoch #295 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115978
Evaluation/AverageDiscountedReturn         -131.904
Evaluation/AverageReturn                   -131.904
Evaluation/CompletionRate                     0
Evaluation/Iteration                        295
Evaluation/MaxReturn                        -30.8887
Evaluation/MinReturn                      -4053.8
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        507.909
Extras/EpisodeRewardMean                   -124.861
LinearFeatureBaseline/ExplainedVariance       0.157255
PolicyExecTime                                0.0967774
ProcessExecTime                               0.0114484
TotalEnvSteps                            299552
policy/Entropy                                0.360463
policy/KL                                     0.00699729
policy/KLBefore                               0
policy/LossAfter                             -0.0228529
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             1.43399
policy/dLoss                                  0.0228529
---------------------------------------  ----------------
2022-04-23 14:20:59 | [train_policy] epoch #296 | Obtaining samples for iteration 296...
2022-04-23 14:20:59 | [train_policy] epoch #296 | Logging diagnostics...
2022-04-23 14:20:59 | [train_policy] epoch #296 | Optimizing policy...
2022-04-23 14:20:59 | [train_policy] epoch #296 | Computing loss before
2022-04-23 14:20:59 | [train_policy] epoch #296 | Computing KL before
2022-04-23 14:20:59 | [train_policy] epoch #296 | Optimizing
2022-04-23 14:20:59 | [train_policy] epoch #296 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:20:59 | [train_policy] epoch #296 | computing loss before
2022-04-23 14:20:59 | [train_policy] epoch #296 | computing gradient
2022-04-23 14:20:59 | [train_policy] epoch #296 | gradient computed
2022-04-23 14:20:59 | [train_policy] epoch #296 | computing descent direction
2022-04-23 14:20:59 | [train_policy] epoch #296 | descent direction computed
2022-04-23 14:20:59 | [train_policy] epoch #296 | backtrack iters: 1
2022-04-23 14:20:59 | [train_policy] epoch #296 | optimization finished
2022-04-23 14:20:59 | [train_policy] epoch #296 | Computing KL after
2022-04-23 14:20:59 | [train_policy] epoch #296 | Computing loss after
2022-04-23 14:20:59 | [train_policy] epoch #296 | Fitting baseline...
2022-04-23 14:20:59 | [train_policy] epoch #296 | Saving snapshot...
2022-04-23 14:20:59 | [train_policy] epoch #296 | Saved
2022-04-23 14:20:59 | [train_policy] epoch #296 | Time 107.64 s
2022-04-23 14:20:59 | [train_policy] epoch #296 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.115909
Evaluation/AverageDiscountedReturn          -65.738
Evaluation/AverageReturn                    -65.738
Evaluation/CompletionRate                     0
Evaluation/Iteration                        296
Evaluation/MaxReturn                        -31.2648
Evaluation/MinReturn                      -2083.03
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.583
Extras/EpisodeRewardMean                    -84.2385
LinearFeatureBaseline/ExplainedVariance      -0.474337
PolicyExecTime                                0.103442
ProcessExecTime                               0.011147
TotalEnvSteps                            300564
policy/Entropy                                0.373183
policy/KL                                     0.00656405
policy/KLBefore                               0
policy/LossAfter                             -0.0332788
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             1.45235
policy/dLoss                                  0.0332788
---------------------------------------  ---------------
2022-04-23 14:20:59 | [train_policy] epoch #297 | Obtaining samples for iteration 297...
2022-04-23 14:21:00 | [train_policy] epoch #297 | Logging diagnostics...
2022-04-23 14:21:00 | [train_policy] epoch #297 | Optimizing policy...
2022-04-23 14:21:00 | [train_policy] epoch #297 | Computing loss before
2022-04-23 14:21:00 | [train_policy] epoch #297 | Computing KL before
2022-04-23 14:21:00 | [train_policy] epoch #297 | Optimizing
2022-04-23 14:21:00 | [train_policy] epoch #297 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:00 | [train_policy] epoch #297 | computing loss before
2022-04-23 14:21:00 | [train_policy] epoch #297 | computing gradient
2022-04-23 14:21:00 | [train_policy] epoch #297 | gradient computed
2022-04-23 14:21:00 | [train_policy] epoch #297 | computing descent direction
2022-04-23 14:21:00 | [train_policy] epoch #297 | descent direction computed
2022-04-23 14:21:00 | [train_policy] epoch #297 | backtrack iters: 0
2022-04-23 14:21:00 | [train_policy] epoch #297 | optimization finished
2022-04-23 14:21:00 | [train_policy] epoch #297 | Computing KL after
2022-04-23 14:21:00 | [train_policy] epoch #297 | Computing loss after
2022-04-23 14:21:00 | [train_policy] epoch #297 | Fitting baseline...
2022-04-23 14:21:00 | [train_policy] epoch #297 | Saving snapshot...
2022-04-23 14:21:00 | [train_policy] epoch #297 | Saved
2022-04-23 14:21:00 | [train_policy] epoch #297 | Time 107.99 s
2022-04-23 14:21:00 | [train_policy] epoch #297 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119561
Evaluation/AverageDiscountedReturn          -87.4517
Evaluation/AverageReturn                    -87.4517
Evaluation/CompletionRate                     0
Evaluation/Iteration                        297
Evaluation/MaxReturn                        -33.8671
Evaluation/MinReturn                      -4053.72
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        415.819
Extras/EpisodeRewardMean                    -84.1911
LinearFeatureBaseline/ExplainedVariance       0.128056
PolicyExecTime                                0.110378
ProcessExecTime                               0.0116754
TotalEnvSteps                            301576
policy/Entropy                                0.351956
policy/KL                                     0.00996839
policy/KLBefore                               0
policy/LossAfter                             -0.0268818
policy/LossBefore                             8.95248e-09
policy/Perplexity                             1.42185
policy/dLoss                                  0.0268818
---------------------------------------  ----------------
2022-04-23 14:21:00 | [train_policy] epoch #298 | Obtaining samples for iteration 298...
2022-04-23 14:21:00 | [train_policy] epoch #298 | Logging diagnostics...
2022-04-23 14:21:00 | [train_policy] epoch #298 | Optimizing policy...
2022-04-23 14:21:00 | [train_policy] epoch #298 | Computing loss before
2022-04-23 14:21:00 | [train_policy] epoch #298 | Computing KL before
2022-04-23 14:21:00 | [train_policy] epoch #298 | Optimizing
2022-04-23 14:21:00 | [train_policy] epoch #298 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:00 | [train_policy] epoch #298 | computing loss before
2022-04-23 14:21:00 | [train_policy] epoch #298 | computing gradient
2022-04-23 14:21:00 | [train_policy] epoch #298 | gradient computed
2022-04-23 14:21:00 | [train_policy] epoch #298 | computing descent direction
2022-04-23 14:21:00 | [train_policy] epoch #298 | descent direction computed
2022-04-23 14:21:00 | [train_policy] epoch #298 | backtrack iters: 0
2022-04-23 14:21:00 | [train_policy] epoch #298 | optimization finished
2022-04-23 14:21:00 | [train_policy] epoch #298 | Computing KL after
2022-04-23 14:21:00 | [train_policy] epoch #298 | Computing loss after
2022-04-23 14:21:00 | [train_policy] epoch #298 | Fitting baseline...
2022-04-23 14:21:00 | [train_policy] epoch #298 | Saving snapshot...
2022-04-23 14:21:00 | [train_policy] epoch #298 | Saved
2022-04-23 14:21:00 | [train_policy] epoch #298 | Time 108.35 s
2022-04-23 14:21:00 | [train_policy] epoch #298 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.120276
Evaluation/AverageDiscountedReturn          -88.3075
Evaluation/AverageReturn                    -88.3075
Evaluation/CompletionRate                     0
Evaluation/Iteration                        298
Evaluation/MaxReturn                        -34.1661
Evaluation/MinReturn                      -4059.17
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        416.348
Extras/EpisodeRewardMean                    -84.6191
LinearFeatureBaseline/ExplainedVariance       0.132031
PolicyExecTime                                0.109557
ProcessExecTime                               0.0117562
TotalEnvSteps                            302588
policy/Entropy                                0.352451
policy/KL                                     0.00936277
policy/KLBefore                               0
policy/LossAfter                             -0.254816
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             1.42255
policy/dLoss                                  0.254816
---------------------------------------  ----------------
2022-04-23 14:21:00 | [train_policy] epoch #299 | Obtaining samples for iteration 299...
2022-04-23 14:21:00 | [train_policy] epoch #299 | Logging diagnostics...
2022-04-23 14:21:00 | [train_policy] epoch #299 | Optimizing policy...
2022-04-23 14:21:00 | [train_policy] epoch #299 | Computing loss before
2022-04-23 14:21:00 | [train_policy] epoch #299 | Computing KL before
2022-04-23 14:21:00 | [train_policy] epoch #299 | Optimizing
2022-04-23 14:21:00 | [train_policy] epoch #299 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:00 | [train_policy] epoch #299 | computing loss before
2022-04-23 14:21:00 | [train_policy] epoch #299 | computing gradient
2022-04-23 14:21:00 | [train_policy] epoch #299 | gradient computed
2022-04-23 14:21:00 | [train_policy] epoch #299 | computing descent direction
2022-04-23 14:21:00 | [train_policy] epoch #299 | descent direction computed
2022-04-23 14:21:00 | [train_policy] epoch #299 | backtrack iters: 0
2022-04-23 14:21:00 | [train_policy] epoch #299 | optimization finished
2022-04-23 14:21:00 | [train_policy] epoch #299 | Computing KL after
2022-04-23 14:21:00 | [train_policy] epoch #299 | Computing loss after
2022-04-23 14:21:00 | [train_policy] epoch #299 | Fitting baseline...
2022-04-23 14:21:00 | [train_policy] epoch #299 | Saving snapshot...
2022-04-23 14:21:00 | [train_policy] epoch #299 | Saved
2022-04-23 14:21:00 | [train_policy] epoch #299 | Time 108.71 s
2022-04-23 14:21:00 | [train_policy] epoch #299 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.120277
Evaluation/AverageDiscountedReturn          -44.3281
Evaluation/AverageReturn                    -44.3281
Evaluation/CompletionRate                     0
Evaluation/Iteration                        299
Evaluation/MaxReturn                        -32.3922
Evaluation/MinReturn                        -74.3258
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.75596
Extras/EpisodeRewardMean                    -43.973
LinearFeatureBaseline/ExplainedVariance    -125.256
PolicyExecTime                                0.108208
ProcessExecTime                               0.0116842
TotalEnvSteps                            303600
policy/Entropy                                0.364698
policy/KL                                     0.00954424
policy/KLBefore                               0
policy/LossAfter                             -0.0180438
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             1.44008
policy/dLoss                                  0.0180438
---------------------------------------  ---------------
2022-04-23 14:21:00 | [train_policy] epoch #300 | Obtaining samples for iteration 300...
2022-04-23 14:21:01 | [train_policy] epoch #300 | Logging diagnostics...
2022-04-23 14:21:01 | [train_policy] epoch #300 | Optimizing policy...
2022-04-23 14:21:01 | [train_policy] epoch #300 | Computing loss before
2022-04-23 14:21:01 | [train_policy] epoch #300 | Computing KL before
2022-04-23 14:21:01 | [train_policy] epoch #300 | Optimizing
2022-04-23 14:21:01 | [train_policy] epoch #300 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:01 | [train_policy] epoch #300 | computing loss before
2022-04-23 14:21:01 | [train_policy] epoch #300 | computing gradient
2022-04-23 14:21:01 | [train_policy] epoch #300 | gradient computed
2022-04-23 14:21:01 | [train_policy] epoch #300 | computing descent direction
2022-04-23 14:21:01 | [train_policy] epoch #300 | descent direction computed
2022-04-23 14:21:01 | [train_policy] epoch #300 | backtrack iters: 0
2022-04-23 14:21:01 | [train_policy] epoch #300 | optimization finished
2022-04-23 14:21:01 | [train_policy] epoch #300 | Computing KL after
2022-04-23 14:21:01 | [train_policy] epoch #300 | Computing loss after
2022-04-23 14:21:01 | [train_policy] epoch #300 | Fitting baseline...
2022-04-23 14:21:01 | [train_policy] epoch #300 | Saving snapshot...
2022-04-23 14:21:01 | [train_policy] epoch #300 | Saved
2022-04-23 14:21:01 | [train_policy] epoch #300 | Time 109.06 s
2022-04-23 14:21:01 | [train_policy] epoch #300 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119946
Evaluation/AverageDiscountedReturn          -87.7344
Evaluation/AverageReturn                    -87.7344
Evaluation/CompletionRate                     0
Evaluation/Iteration                        300
Evaluation/MaxReturn                        -30.935
Evaluation/MinReturn                      -2069.56
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.166
Extras/EpisodeRewardMean                    -84.1727
LinearFeatureBaseline/ExplainedVariance       0.0110985
PolicyExecTime                                0.103399
ProcessExecTime                               0.0114379
TotalEnvSteps                            304612
policy/Entropy                                0.364705
policy/KL                                     0.00938799
policy/KLBefore                               0
policy/LossAfter                             -0.020516
policy/LossBefore                             1.06016e-08
policy/Perplexity                             1.44009
policy/dLoss                                  0.020516
---------------------------------------  ----------------
2022-04-23 14:21:01 | [train_policy] epoch #301 | Obtaining samples for iteration 301...
2022-04-23 14:21:01 | [train_policy] epoch #301 | Logging diagnostics...
2022-04-23 14:21:01 | [train_policy] epoch #301 | Optimizing policy...
2022-04-23 14:21:01 | [train_policy] epoch #301 | Computing loss before
2022-04-23 14:21:01 | [train_policy] epoch #301 | Computing KL before
2022-04-23 14:21:01 | [train_policy] epoch #301 | Optimizing
2022-04-23 14:21:01 | [train_policy] epoch #301 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:01 | [train_policy] epoch #301 | computing loss before
2022-04-23 14:21:01 | [train_policy] epoch #301 | computing gradient
2022-04-23 14:21:01 | [train_policy] epoch #301 | gradient computed
2022-04-23 14:21:01 | [train_policy] epoch #301 | computing descent direction
2022-04-23 14:21:01 | [train_policy] epoch #301 | descent direction computed
2022-04-23 14:21:01 | [train_policy] epoch #301 | backtrack iters: 0
2022-04-23 14:21:01 | [train_policy] epoch #301 | optimization finished
2022-04-23 14:21:01 | [train_policy] epoch #301 | Computing KL after
2022-04-23 14:21:01 | [train_policy] epoch #301 | Computing loss after
2022-04-23 14:21:01 | [train_policy] epoch #301 | Fitting baseline...
2022-04-23 14:21:01 | [train_policy] epoch #301 | Saving snapshot...
2022-04-23 14:21:01 | [train_policy] epoch #301 | Saved
2022-04-23 14:21:01 | [train_policy] epoch #301 | Time 109.41 s
2022-04-23 14:21:01 | [train_policy] epoch #301 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.120149
Evaluation/AverageDiscountedReturn         -131.357
Evaluation/AverageReturn                   -131.357
Evaluation/CompletionRate                     0
Evaluation/Iteration                        301
Evaluation/MaxReturn                        -31.0839
Evaluation/MinReturn                      -4051.55
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        506.463
Extras/EpisodeRewardMean                   -124.294
LinearFeatureBaseline/ExplainedVariance       0.185748
PolicyExecTime                                0.0987484
ProcessExecTime                               0.0112948
TotalEnvSteps                            305624
policy/Entropy                                0.328002
policy/KL                                     0.0092527
policy/KLBefore                               0
policy/LossAfter                             -0.0370547
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             1.38819
policy/dLoss                                  0.0370547
---------------------------------------  ---------------
2022-04-23 14:21:01 | [train_policy] epoch #302 | Obtaining samples for iteration 302...
2022-04-23 14:21:01 | [train_policy] epoch #302 | Logging diagnostics...
2022-04-23 14:21:01 | [train_policy] epoch #302 | Optimizing policy...
2022-04-23 14:21:01 | [train_policy] epoch #302 | Computing loss before
2022-04-23 14:21:01 | [train_policy] epoch #302 | Computing KL before
2022-04-23 14:21:01 | [train_policy] epoch #302 | Optimizing
2022-04-23 14:21:01 | [train_policy] epoch #302 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:01 | [train_policy] epoch #302 | computing loss before
2022-04-23 14:21:01 | [train_policy] epoch #302 | computing gradient
2022-04-23 14:21:01 | [train_policy] epoch #302 | gradient computed
2022-04-23 14:21:01 | [train_policy] epoch #302 | computing descent direction
2022-04-23 14:21:02 | [train_policy] epoch #302 | descent direction computed
2022-04-23 14:21:02 | [train_policy] epoch #302 | backtrack iters: 1
2022-04-23 14:21:02 | [train_policy] epoch #302 | optimization finished
2022-04-23 14:21:02 | [train_policy] epoch #302 | Computing KL after
2022-04-23 14:21:02 | [train_policy] epoch #302 | Computing loss after
2022-04-23 14:21:02 | [train_policy] epoch #302 | Fitting baseline...
2022-04-23 14:21:02 | [train_policy] epoch #302 | Saving snapshot...
2022-04-23 14:21:02 | [train_policy] epoch #302 | Saved
2022-04-23 14:21:02 | [train_policy] epoch #302 | Time 109.75 s
2022-04-23 14:21:02 | [train_policy] epoch #302 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118706
Evaluation/AverageDiscountedReturn          -43.0648
Evaluation/AverageReturn                    -43.0648
Evaluation/CompletionRate                     0
Evaluation/Iteration                        302
Evaluation/MaxReturn                        -31.0204
Evaluation/MinReturn                        -74.09
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.11161
Extras/EpisodeRewardMean                    -43.2089
LinearFeatureBaseline/ExplainedVariance    -160.984
PolicyExecTime                                0.09565
ProcessExecTime                               0.0111296
TotalEnvSteps                            306636
policy/Entropy                                0.349043
policy/KL                                     0.00702482
policy/KLBefore                               0
policy/LossAfter                             -0.0179956
policy/LossBefore                             5.12411e-08
policy/Perplexity                             1.41771
policy/dLoss                                  0.0179957
---------------------------------------  ----------------
2022-04-23 14:21:02 | [train_policy] epoch #303 | Obtaining samples for iteration 303...
2022-04-23 14:21:02 | [train_policy] epoch #303 | Logging diagnostics...
2022-04-23 14:21:02 | [train_policy] epoch #303 | Optimizing policy...
2022-04-23 14:21:02 | [train_policy] epoch #303 | Computing loss before
2022-04-23 14:21:02 | [train_policy] epoch #303 | Computing KL before
2022-04-23 14:21:02 | [train_policy] epoch #303 | Optimizing
2022-04-23 14:21:02 | [train_policy] epoch #303 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:02 | [train_policy] epoch #303 | computing loss before
2022-04-23 14:21:02 | [train_policy] epoch #303 | computing gradient
2022-04-23 14:21:02 | [train_policy] epoch #303 | gradient computed
2022-04-23 14:21:02 | [train_policy] epoch #303 | computing descent direction
2022-04-23 14:21:02 | [train_policy] epoch #303 | descent direction computed
2022-04-23 14:21:02 | [train_policy] epoch #303 | backtrack iters: 0
2022-04-23 14:21:02 | [train_policy] epoch #303 | optimization finished
2022-04-23 14:21:02 | [train_policy] epoch #303 | Computing KL after
2022-04-23 14:21:02 | [train_policy] epoch #303 | Computing loss after
2022-04-23 14:21:02 | [train_policy] epoch #303 | Fitting baseline...
2022-04-23 14:21:02 | [train_policy] epoch #303 | Saving snapshot...
2022-04-23 14:21:02 | [train_policy] epoch #303 | Saved
2022-04-23 14:21:02 | [train_policy] epoch #303 | Time 110.12 s
2022-04-23 14:21:02 | [train_policy] epoch #303 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                   0.118923
Evaluation/AverageDiscountedReturn          -43.8455
Evaluation/AverageReturn                    -43.8455
Evaluation/CompletionRate                     0
Evaluation/Iteration                        303
Evaluation/MaxReturn                        -32.8175
Evaluation/MinReturn                        -64.4282
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.27699
Extras/EpisodeRewardMean                    -43.9646
LinearFeatureBaseline/ExplainedVariance       0.937888
PolicyExecTime                                0.10603
ProcessExecTime                               0.0117726
TotalEnvSteps                            307648
policy/Entropy                                0.320073
policy/KL                                     0.00933503
policy/KLBefore                               0
policy/LossAfter                             -0.0212525
policy/LossBefore                             5.6542e-09
policy/Perplexity                             1.37723
policy/dLoss                                  0.0212525
---------------------------------------  ---------------
2022-04-23 14:21:02 | [train_policy] epoch #304 | Obtaining samples for iteration 304...
2022-04-23 14:21:02 | [train_policy] epoch #304 | Logging diagnostics...
2022-04-23 14:21:02 | [train_policy] epoch #304 | Optimizing policy...
2022-04-23 14:21:02 | [train_policy] epoch #304 | Computing loss before
2022-04-23 14:21:02 | [train_policy] epoch #304 | Computing KL before
2022-04-23 14:21:02 | [train_policy] epoch #304 | Optimizing
2022-04-23 14:21:02 | [train_policy] epoch #304 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:02 | [train_policy] epoch #304 | computing loss before
2022-04-23 14:21:02 | [train_policy] epoch #304 | computing gradient
2022-04-23 14:21:02 | [train_policy] epoch #304 | gradient computed
2022-04-23 14:21:02 | [train_policy] epoch #304 | computing descent direction
2022-04-23 14:21:02 | [train_policy] epoch #304 | descent direction computed
2022-04-23 14:21:02 | [train_policy] epoch #304 | backtrack iters: 1
2022-04-23 14:21:02 | [train_policy] epoch #304 | optimization finished
2022-04-23 14:21:02 | [train_policy] epoch #304 | Computing KL after
2022-04-23 14:21:02 | [train_policy] epoch #304 | Computing loss after
2022-04-23 14:21:02 | [train_policy] epoch #304 | Fitting baseline...
2022-04-23 14:21:02 | [train_policy] epoch #304 | Saving snapshot...
2022-04-23 14:21:02 | [train_policy] epoch #304 | Saved
2022-04-23 14:21:02 | [train_policy] epoch #304 | Time 110.48 s
2022-04-23 14:21:02 | [train_policy] epoch #304 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117413
Evaluation/AverageDiscountedReturn          -43.884
Evaluation/AverageReturn                    -43.884
Evaluation/CompletionRate                     0
Evaluation/Iteration                        304
Evaluation/MaxReturn                        -29.8942
Evaluation/MinReturn                        -68.8999
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25245
Extras/EpisodeRewardMean                    -43.9362
LinearFeatureBaseline/ExplainedVariance       0.894684
PolicyExecTime                                0.108143
ProcessExecTime                               0.0115178
TotalEnvSteps                            308660
policy/Entropy                                0.283219
policy/KL                                     0.00690133
policy/KLBefore                               0
policy/LossAfter                             -0.01746
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             1.3274
policy/dLoss                                  0.01746
---------------------------------------  ----------------
2022-04-23 14:21:02 | [train_policy] epoch #305 | Obtaining samples for iteration 305...
2022-04-23 14:21:03 | [train_policy] epoch #305 | Logging diagnostics...
2022-04-23 14:21:03 | [train_policy] epoch #305 | Optimizing policy...
2022-04-23 14:21:03 | [train_policy] epoch #305 | Computing loss before
2022-04-23 14:21:03 | [train_policy] epoch #305 | Computing KL before
2022-04-23 14:21:03 | [train_policy] epoch #305 | Optimizing
2022-04-23 14:21:03 | [train_policy] epoch #305 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:03 | [train_policy] epoch #305 | computing loss before
2022-04-23 14:21:03 | [train_policy] epoch #305 | computing gradient
2022-04-23 14:21:03 | [train_policy] epoch #305 | gradient computed
2022-04-23 14:21:03 | [train_policy] epoch #305 | computing descent direction
2022-04-23 14:21:03 | [train_policy] epoch #305 | descent direction computed
2022-04-23 14:21:03 | [train_policy] epoch #305 | backtrack iters: 1
2022-04-23 14:21:03 | [train_policy] epoch #305 | optimization finished
2022-04-23 14:21:03 | [train_policy] epoch #305 | Computing KL after
2022-04-23 14:21:03 | [train_policy] epoch #305 | Computing loss after
2022-04-23 14:21:03 | [train_policy] epoch #305 | Fitting baseline...
2022-04-23 14:21:03 | [train_policy] epoch #305 | Saving snapshot...
2022-04-23 14:21:03 | [train_policy] epoch #305 | Saved
2022-04-23 14:21:03 | [train_policy] epoch #305 | Time 110.82 s
2022-04-23 14:21:03 | [train_policy] epoch #305 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115594
Evaluation/AverageDiscountedReturn          -44.3811
Evaluation/AverageReturn                    -44.3811
Evaluation/CompletionRate                     0
Evaluation/Iteration                        305
Evaluation/MaxReturn                        -31.1285
Evaluation/MinReturn                        -74.6902
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.89224
Extras/EpisodeRewardMean                    -44.4972
LinearFeatureBaseline/ExplainedVariance       0.899638
PolicyExecTime                                0.0957937
ProcessExecTime                               0.0107942
TotalEnvSteps                            309672
policy/Entropy                                0.290889
policy/KL                                     0.00682487
policy/KLBefore                               0
policy/LossAfter                             -0.0209758
policy/LossBefore                            -7.30334e-09
policy/Perplexity                             1.33762
policy/dLoss                                  0.0209758
---------------------------------------  ----------------
2022-04-23 14:21:03 | [train_policy] epoch #306 | Obtaining samples for iteration 306...
2022-04-23 14:21:03 | [train_policy] epoch #306 | Logging diagnostics...
2022-04-23 14:21:03 | [train_policy] epoch #306 | Optimizing policy...
2022-04-23 14:21:03 | [train_policy] epoch #306 | Computing loss before
2022-04-23 14:21:03 | [train_policy] epoch #306 | Computing KL before
2022-04-23 14:21:03 | [train_policy] epoch #306 | Optimizing
2022-04-23 14:21:03 | [train_policy] epoch #306 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:03 | [train_policy] epoch #306 | computing loss before
2022-04-23 14:21:03 | [train_policy] epoch #306 | computing gradient
2022-04-23 14:21:03 | [train_policy] epoch #306 | gradient computed
2022-04-23 14:21:03 | [train_policy] epoch #306 | computing descent direction
2022-04-23 14:21:03 | [train_policy] epoch #306 | descent direction computed
2022-04-23 14:21:03 | [train_policy] epoch #306 | backtrack iters: 1
2022-04-23 14:21:03 | [train_policy] epoch #306 | optimization finished
2022-04-23 14:21:03 | [train_policy] epoch #306 | Computing KL after
2022-04-23 14:21:03 | [train_policy] epoch #306 | Computing loss after
2022-04-23 14:21:03 | [train_policy] epoch #306 | Fitting baseline...
2022-04-23 14:21:03 | [train_policy] epoch #306 | Saving snapshot...
2022-04-23 14:21:03 | [train_policy] epoch #306 | Saved
2022-04-23 14:21:03 | [train_policy] epoch #306 | Time 111.16 s
2022-04-23 14:21:03 | [train_policy] epoch #306 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115938
Evaluation/AverageDiscountedReturn          -65.041
Evaluation/AverageReturn                    -65.041
Evaluation/CompletionRate                     0
Evaluation/Iteration                        306
Evaluation/MaxReturn                        -30.6867
Evaluation/MinReturn                      -2079.1
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.235
Extras/EpisodeRewardMean                    -63.6892
LinearFeatureBaseline/ExplainedVariance       0.0114125
PolicyExecTime                                0.0960095
ProcessExecTime                               0.0116558
TotalEnvSteps                            310684
policy/Entropy                                0.27779
policy/KL                                     0.00901757
policy/KLBefore                               0
policy/LossAfter                             -0.0262459
policy/LossBefore                             1.46067e-08
policy/Perplexity                             1.32021
policy/dLoss                                  0.0262459
---------------------------------------  ----------------
2022-04-23 14:21:03 | [train_policy] epoch #307 | Obtaining samples for iteration 307...
2022-04-23 14:21:03 | [train_policy] epoch #307 | Logging diagnostics...
2022-04-23 14:21:03 | [train_policy] epoch #307 | Optimizing policy...
2022-04-23 14:21:03 | [train_policy] epoch #307 | Computing loss before
2022-04-23 14:21:03 | [train_policy] epoch #307 | Computing KL before
2022-04-23 14:21:03 | [train_policy] epoch #307 | Optimizing
2022-04-23 14:21:03 | [train_policy] epoch #307 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:03 | [train_policy] epoch #307 | computing loss before
2022-04-23 14:21:03 | [train_policy] epoch #307 | computing gradient
2022-04-23 14:21:03 | [train_policy] epoch #307 | gradient computed
2022-04-23 14:21:03 | [train_policy] epoch #307 | computing descent direction
2022-04-23 14:21:03 | [train_policy] epoch #307 | descent direction computed
2022-04-23 14:21:03 | [train_policy] epoch #307 | backtrack iters: 1
2022-04-23 14:21:03 | [train_policy] epoch #307 | optimization finished
2022-04-23 14:21:03 | [train_policy] epoch #307 | Computing KL after
2022-04-23 14:21:03 | [train_policy] epoch #307 | Computing loss after
2022-04-23 14:21:03 | [train_policy] epoch #307 | Fitting baseline...
2022-04-23 14:21:03 | [train_policy] epoch #307 | Saving snapshot...
2022-04-23 14:21:03 | [train_policy] epoch #307 | Saved
2022-04-23 14:21:03 | [train_policy] epoch #307 | Time 111.50 s
2022-04-23 14:21:03 | [train_policy] epoch #307 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116338
Evaluation/AverageDiscountedReturn          -43.4078
Evaluation/AverageReturn                    -43.4078
Evaluation/CompletionRate                     0
Evaluation/Iteration                        307
Evaluation/MaxReturn                        -31.4538
Evaluation/MinReturn                        -67.73
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.70201
Extras/EpisodeRewardMean                    -43.4257
LinearFeatureBaseline/ExplainedVariance     -30.0185
PolicyExecTime                                0.0962608
ProcessExecTime                               0.0111074
TotalEnvSteps                            311696
policy/Entropy                                0.239914
policy/KL                                     0.00683896
policy/KLBefore                               0
policy/LossAfter                             -0.017752
policy/LossBefore                             2.63862e-08
policy/Perplexity                             1.27114
policy/dLoss                                  0.017752
---------------------------------------  ----------------
2022-04-23 14:21:03 | [train_policy] epoch #308 | Obtaining samples for iteration 308...
2022-04-23 14:21:04 | [train_policy] epoch #308 | Logging diagnostics...
2022-04-23 14:21:04 | [train_policy] epoch #308 | Optimizing policy...
2022-04-23 14:21:04 | [train_policy] epoch #308 | Computing loss before
2022-04-23 14:21:04 | [train_policy] epoch #308 | Computing KL before
2022-04-23 14:21:04 | [train_policy] epoch #308 | Optimizing
2022-04-23 14:21:04 | [train_policy] epoch #308 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:04 | [train_policy] epoch #308 | computing loss before
2022-04-23 14:21:04 | [train_policy] epoch #308 | computing gradient
2022-04-23 14:21:04 | [train_policy] epoch #308 | gradient computed
2022-04-23 14:21:04 | [train_policy] epoch #308 | computing descent direction
2022-04-23 14:21:04 | [train_policy] epoch #308 | descent direction computed
2022-04-23 14:21:04 | [train_policy] epoch #308 | backtrack iters: 1
2022-04-23 14:21:04 | [train_policy] epoch #308 | optimization finished
2022-04-23 14:21:04 | [train_policy] epoch #308 | Computing KL after
2022-04-23 14:21:04 | [train_policy] epoch #308 | Computing loss after
2022-04-23 14:21:04 | [train_policy] epoch #308 | Fitting baseline...
2022-04-23 14:21:04 | [train_policy] epoch #308 | Saving snapshot...
2022-04-23 14:21:04 | [train_policy] epoch #308 | Saved
2022-04-23 14:21:04 | [train_policy] epoch #308 | Time 111.87 s
2022-04-23 14:21:04 | [train_policy] epoch #308 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.128985
Evaluation/AverageDiscountedReturn          -44.074
Evaluation/AverageReturn                    -44.074
Evaluation/CompletionRate                     0
Evaluation/Iteration                        308
Evaluation/MaxReturn                        -30.7034
Evaluation/MinReturn                        -68.8921
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.39742
Extras/EpisodeRewardMean                    -44.1787
LinearFeatureBaseline/ExplainedVariance       0.906072
PolicyExecTime                                0.111771
ProcessExecTime                               0.0128956
TotalEnvSteps                            312708
policy/Entropy                                0.232815
policy/KL                                     0.00720953
policy/KLBefore                               0
policy/LossAfter                             -0.0141688
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             1.26215
policy/dLoss                                  0.0141688
---------------------------------------  ----------------
2022-04-23 14:21:04 | [train_policy] epoch #309 | Obtaining samples for iteration 309...
2022-04-23 14:21:04 | [train_policy] epoch #309 | Logging diagnostics...
2022-04-23 14:21:04 | [train_policy] epoch #309 | Optimizing policy...
2022-04-23 14:21:04 | [train_policy] epoch #309 | Computing loss before
2022-04-23 14:21:04 | [train_policy] epoch #309 | Computing KL before
2022-04-23 14:21:04 | [train_policy] epoch #309 | Optimizing
2022-04-23 14:21:04 | [train_policy] epoch #309 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:04 | [train_policy] epoch #309 | computing loss before
2022-04-23 14:21:04 | [train_policy] epoch #309 | computing gradient
2022-04-23 14:21:04 | [train_policy] epoch #309 | gradient computed
2022-04-23 14:21:04 | [train_policy] epoch #309 | computing descent direction
2022-04-23 14:21:04 | [train_policy] epoch #309 | descent direction computed
2022-04-23 14:21:04 | [train_policy] epoch #309 | backtrack iters: 1
2022-04-23 14:21:04 | [train_policy] epoch #309 | optimization finished
2022-04-23 14:21:04 | [train_policy] epoch #309 | Computing KL after
2022-04-23 14:21:04 | [train_policy] epoch #309 | Computing loss after
2022-04-23 14:21:04 | [train_policy] epoch #309 | Fitting baseline...
2022-04-23 14:21:04 | [train_policy] epoch #309 | Saving snapshot...
2022-04-23 14:21:04 | [train_policy] epoch #309 | Saved
2022-04-23 14:21:04 | [train_policy] epoch #309 | Time 112.22 s
2022-04-23 14:21:04 | [train_policy] epoch #309 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.118002
Evaluation/AverageDiscountedReturn         -109.974
Evaluation/AverageReturn                   -109.974
Evaluation/CompletionRate                     0
Evaluation/Iteration                        309
Evaluation/MaxReturn                        -30.7702
Evaluation/MinReturn                      -2072.28
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.755
Extras/EpisodeRewardMean                   -104.637
LinearFeatureBaseline/ExplainedVariance       0.00990928
PolicyExecTime                                0.103485
ProcessExecTime                               0.0115242
TotalEnvSteps                            313720
policy/Entropy                                0.249785
policy/KL                                     0.00638948
policy/KLBefore                               0
policy/LossAfter                             -0.0107167
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             1.28375
policy/dLoss                                  0.0107167
---------------------------------------  ---------------
2022-04-23 14:21:04 | [train_policy] epoch #310 | Obtaining samples for iteration 310...
2022-04-23 14:21:04 | [train_policy] epoch #310 | Logging diagnostics...
2022-04-23 14:21:04 | [train_policy] epoch #310 | Optimizing policy...
2022-04-23 14:21:04 | [train_policy] epoch #310 | Computing loss before
2022-04-23 14:21:04 | [train_policy] epoch #310 | Computing KL before
2022-04-23 14:21:04 | [train_policy] epoch #310 | Optimizing
2022-04-23 14:21:04 | [train_policy] epoch #310 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:04 | [train_policy] epoch #310 | computing loss before
2022-04-23 14:21:04 | [train_policy] epoch #310 | computing gradient
2022-04-23 14:21:04 | [train_policy] epoch #310 | gradient computed
2022-04-23 14:21:04 | [train_policy] epoch #310 | computing descent direction
2022-04-23 14:21:04 | [train_policy] epoch #310 | descent direction computed
2022-04-23 14:21:04 | [train_policy] epoch #310 | backtrack iters: 0
2022-04-23 14:21:04 | [train_policy] epoch #310 | optimization finished
2022-04-23 14:21:04 | [train_policy] epoch #310 | Computing KL after
2022-04-23 14:21:04 | [train_policy] epoch #310 | Computing loss after
2022-04-23 14:21:04 | [train_policy] epoch #310 | Fitting baseline...
2022-04-23 14:21:04 | [train_policy] epoch #310 | Saving snapshot...
2022-04-23 14:21:04 | [train_policy] epoch #310 | Saved
2022-04-23 14:21:04 | [train_policy] epoch #310 | Time 112.58 s
2022-04-23 14:21:04 | [train_policy] epoch #310 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118939
Evaluation/AverageDiscountedReturn          -66.4352
Evaluation/AverageReturn                    -66.4352
Evaluation/CompletionRate                     0
Evaluation/Iteration                        310
Evaluation/MaxReturn                        -31.0634
Evaluation/MinReturn                      -2073.73
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.655
Extras/EpisodeRewardMean                    -64.8053
LinearFeatureBaseline/ExplainedVariance      -0.149595
PolicyExecTime                                0.106587
ProcessExecTime                               0.0117786
TotalEnvSteps                            314732
policy/Entropy                                0.247361
policy/KL                                     0.00985225
policy/KLBefore                               0
policy/LossAfter                             -0.0189632
policy/LossBefore                             4.71183e-09
policy/Perplexity                             1.28064
policy/dLoss                                  0.0189633
---------------------------------------  ----------------
2022-04-23 14:21:04 | [train_policy] epoch #311 | Obtaining samples for iteration 311...
2022-04-23 14:21:05 | [train_policy] epoch #311 | Logging diagnostics...
2022-04-23 14:21:05 | [train_policy] epoch #311 | Optimizing policy...
2022-04-23 14:21:05 | [train_policy] epoch #311 | Computing loss before
2022-04-23 14:21:05 | [train_policy] epoch #311 | Computing KL before
2022-04-23 14:21:05 | [train_policy] epoch #311 | Optimizing
2022-04-23 14:21:05 | [train_policy] epoch #311 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:05 | [train_policy] epoch #311 | computing loss before
2022-04-23 14:21:05 | [train_policy] epoch #311 | computing gradient
2022-04-23 14:21:05 | [train_policy] epoch #311 | gradient computed
2022-04-23 14:21:05 | [train_policy] epoch #311 | computing descent direction
2022-04-23 14:21:05 | [train_policy] epoch #311 | descent direction computed
2022-04-23 14:21:05 | [train_policy] epoch #311 | backtrack iters: 0
2022-04-23 14:21:05 | [train_policy] epoch #311 | optimization finished
2022-04-23 14:21:05 | [train_policy] epoch #311 | Computing KL after
2022-04-23 14:21:05 | [train_policy] epoch #311 | Computing loss after
2022-04-23 14:21:05 | [train_policy] epoch #311 | Fitting baseline...
2022-04-23 14:21:05 | [train_policy] epoch #311 | Saving snapshot...
2022-04-23 14:21:05 | [train_policy] epoch #311 | Saved
2022-04-23 14:21:05 | [train_policy] epoch #311 | Time 112.93 s
2022-04-23 14:21:05 | [train_policy] epoch #311 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116476
Evaluation/AverageDiscountedReturn          -44.4542
Evaluation/AverageReturn                    -44.4542
Evaluation/CompletionRate                     0
Evaluation/Iteration                        311
Evaluation/MaxReturn                        -33.1502
Evaluation/MinReturn                        -87.5214
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.199
Extras/EpisodeRewardMean                    -44.4866
LinearFeatureBaseline/ExplainedVariance     -27.2007
PolicyExecTime                                0.105554
ProcessExecTime                               0.0113921
TotalEnvSteps                            315744
policy/Entropy                                0.256251
policy/KL                                     0.00919206
policy/KLBefore                               0
policy/LossAfter                             -0.0113501
policy/LossBefore                             3.82836e-08
policy/Perplexity                             1.29208
policy/dLoss                                  0.0113501
---------------------------------------  ----------------
2022-04-23 14:21:05 | [train_policy] epoch #312 | Obtaining samples for iteration 312...
2022-04-23 14:21:05 | [train_policy] epoch #312 | Logging diagnostics...
2022-04-23 14:21:05 | [train_policy] epoch #312 | Optimizing policy...
2022-04-23 14:21:05 | [train_policy] epoch #312 | Computing loss before
2022-04-23 14:21:05 | [train_policy] epoch #312 | Computing KL before
2022-04-23 14:21:05 | [train_policy] epoch #312 | Optimizing
2022-04-23 14:21:05 | [train_policy] epoch #312 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:05 | [train_policy] epoch #312 | computing loss before
2022-04-23 14:21:05 | [train_policy] epoch #312 | computing gradient
2022-04-23 14:21:05 | [train_policy] epoch #312 | gradient computed
2022-04-23 14:21:05 | [train_policy] epoch #312 | computing descent direction
2022-04-23 14:21:05 | [train_policy] epoch #312 | descent direction computed
2022-04-23 14:21:05 | [train_policy] epoch #312 | backtrack iters: 1
2022-04-23 14:21:05 | [train_policy] epoch #312 | optimization finished
2022-04-23 14:21:05 | [train_policy] epoch #312 | Computing KL after
2022-04-23 14:21:05 | [train_policy] epoch #312 | Computing loss after
2022-04-23 14:21:05 | [train_policy] epoch #312 | Fitting baseline...
2022-04-23 14:21:05 | [train_policy] epoch #312 | Saving snapshot...
2022-04-23 14:21:05 | [train_policy] epoch #312 | Saved
2022-04-23 14:21:05 | [train_policy] epoch #312 | Time 113.28 s
2022-04-23 14:21:05 | [train_policy] epoch #312 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116392
Evaluation/AverageDiscountedReturn          -44.0159
Evaluation/AverageReturn                    -44.0159
Evaluation/CompletionRate                     0
Evaluation/Iteration                        312
Evaluation/MaxReturn                        -31.0622
Evaluation/MinReturn                        -65.6658
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.37319
Extras/EpisodeRewardMean                    -43.7541
LinearFeatureBaseline/ExplainedVariance       0.926671
PolicyExecTime                                0.100614
ProcessExecTime                               0.0114143
TotalEnvSteps                            316756
policy/Entropy                                0.241729
policy/KL                                     0.00649147
policy/KLBefore                               0
policy/LossAfter                             -0.018617
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             1.27345
policy/dLoss                                  0.0186169
---------------------------------------  ----------------
2022-04-23 14:21:05 | [train_policy] epoch #313 | Obtaining samples for iteration 313...
2022-04-23 14:21:05 | [train_policy] epoch #313 | Logging diagnostics...
2022-04-23 14:21:05 | [train_policy] epoch #313 | Optimizing policy...
2022-04-23 14:21:05 | [train_policy] epoch #313 | Computing loss before
2022-04-23 14:21:05 | [train_policy] epoch #313 | Computing KL before
2022-04-23 14:21:05 | [train_policy] epoch #313 | Optimizing
2022-04-23 14:21:05 | [train_policy] epoch #313 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:05 | [train_policy] epoch #313 | computing loss before
2022-04-23 14:21:05 | [train_policy] epoch #313 | computing gradient
2022-04-23 14:21:05 | [train_policy] epoch #313 | gradient computed
2022-04-23 14:21:05 | [train_policy] epoch #313 | computing descent direction
2022-04-23 14:21:05 | [train_policy] epoch #313 | descent direction computed
2022-04-23 14:21:05 | [train_policy] epoch #313 | backtrack iters: 1
2022-04-23 14:21:05 | [train_policy] epoch #313 | optimization finished
2022-04-23 14:21:05 | [train_policy] epoch #313 | Computing KL after
2022-04-23 14:21:05 | [train_policy] epoch #313 | Computing loss after
2022-04-23 14:21:05 | [train_policy] epoch #313 | Fitting baseline...
2022-04-23 14:21:05 | [train_policy] epoch #313 | Saving snapshot...
2022-04-23 14:21:05 | [train_policy] epoch #313 | Saved
2022-04-23 14:21:05 | [train_policy] epoch #313 | Time 113.62 s
2022-04-23 14:21:05 | [train_policy] epoch #313 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116533
Evaluation/AverageDiscountedReturn          -42.6345
Evaluation/AverageReturn                    -42.6345
Evaluation/CompletionRate                     0
Evaluation/Iteration                        313
Evaluation/MaxReturn                        -31.6986
Evaluation/MinReturn                        -73.965
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.51129
Extras/EpisodeRewardMean                    -42.8329
LinearFeatureBaseline/ExplainedVariance       0.840871
PolicyExecTime                                0.0967543
ProcessExecTime                               0.0109787
TotalEnvSteps                            317768
policy/Entropy                                0.179356
policy/KL                                     0.00664749
policy/KLBefore                               0
policy/LossAfter                             -0.00953311
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             1.19645
policy/dLoss                                  0.0095331
---------------------------------------  ----------------
2022-04-23 14:21:05 | [train_policy] epoch #314 | Obtaining samples for iteration 314...
2022-04-23 14:21:06 | [train_policy] epoch #314 | Logging diagnostics...
2022-04-23 14:21:06 | [train_policy] epoch #314 | Optimizing policy...
2022-04-23 14:21:06 | [train_policy] epoch #314 | Computing loss before
2022-04-23 14:21:06 | [train_policy] epoch #314 | Computing KL before
2022-04-23 14:21:06 | [train_policy] epoch #314 | Optimizing
2022-04-23 14:21:06 | [train_policy] epoch #314 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:06 | [train_policy] epoch #314 | computing loss before
2022-04-23 14:21:06 | [train_policy] epoch #314 | computing gradient
2022-04-23 14:21:06 | [train_policy] epoch #314 | gradient computed
2022-04-23 14:21:06 | [train_policy] epoch #314 | computing descent direction
2022-04-23 14:21:06 | [train_policy] epoch #314 | descent direction computed
2022-04-23 14:21:06 | [train_policy] epoch #314 | backtrack iters: 1
2022-04-23 14:21:06 | [train_policy] epoch #314 | optimization finished
2022-04-23 14:21:06 | [train_policy] epoch #314 | Computing KL after
2022-04-23 14:21:06 | [train_policy] epoch #314 | Computing loss after
2022-04-23 14:21:06 | [train_policy] epoch #314 | Fitting baseline...
2022-04-23 14:21:06 | [train_policy] epoch #314 | Saving snapshot...
2022-04-23 14:21:06 | [train_policy] epoch #314 | Saved
2022-04-23 14:21:06 | [train_policy] epoch #314 | Time 113.97 s
2022-04-23 14:21:06 | [train_policy] epoch #314 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.115618
Evaluation/AverageDiscountedReturn          -86.746
Evaluation/AverageReturn                    -86.746
Evaluation/CompletionRate                     0
Evaluation/Iteration                        314
Evaluation/MaxReturn                        -32.2034
Evaluation/MinReturn                      -2064.8
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.784
Extras/EpisodeRewardMean                    -83.0529
LinearFeatureBaseline/ExplainedVariance       0.00883838
PolicyExecTime                                0.101906
ProcessExecTime                               0.0115659
TotalEnvSteps                            318780
policy/Entropy                                0.173683
policy/KL                                     0.00648268
policy/KLBefore                               0
policy/LossAfter                             -0.0184415
policy/LossBefore                            -2.40303e-08
policy/Perplexity                             1.18968
policy/dLoss                                  0.0184415
---------------------------------------  ----------------
2022-04-23 14:21:06 | [train_policy] epoch #315 | Obtaining samples for iteration 315...
2022-04-23 14:21:06 | [train_policy] epoch #315 | Logging diagnostics...
2022-04-23 14:21:06 | [train_policy] epoch #315 | Optimizing policy...
2022-04-23 14:21:06 | [train_policy] epoch #315 | Computing loss before
2022-04-23 14:21:06 | [train_policy] epoch #315 | Computing KL before
2022-04-23 14:21:06 | [train_policy] epoch #315 | Optimizing
2022-04-23 14:21:06 | [train_policy] epoch #315 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:06 | [train_policy] epoch #315 | computing loss before
2022-04-23 14:21:06 | [train_policy] epoch #315 | computing gradient
2022-04-23 14:21:06 | [train_policy] epoch #315 | gradient computed
2022-04-23 14:21:06 | [train_policy] epoch #315 | computing descent direction
2022-04-23 14:21:06 | [train_policy] epoch #315 | descent direction computed
2022-04-23 14:21:06 | [train_policy] epoch #315 | backtrack iters: 1
2022-04-23 14:21:06 | [train_policy] epoch #315 | optimization finished
2022-04-23 14:21:06 | [train_policy] epoch #315 | Computing KL after
2022-04-23 14:21:06 | [train_policy] epoch #315 | Computing loss after
2022-04-23 14:21:06 | [train_policy] epoch #315 | Fitting baseline...
2022-04-23 14:21:06 | [train_policy] epoch #315 | Saving snapshot...
2022-04-23 14:21:06 | [train_policy] epoch #315 | Saved
2022-04-23 14:21:06 | [train_policy] epoch #315 | Time 114.31 s
2022-04-23 14:21:06 | [train_policy] epoch #315 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115692
Evaluation/AverageDiscountedReturn          -88.3452
Evaluation/AverageReturn                    -88.3452
Evaluation/CompletionRate                     0
Evaluation/Iteration                        315
Evaluation/MaxReturn                        -33.9248
Evaluation/MinReturn                      -2065.55
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.674
Extras/EpisodeRewardMean                    -84.6734
LinearFeatureBaseline/ExplainedVariance       0.167623
PolicyExecTime                                0.0961587
ProcessExecTime                               0.0108349
TotalEnvSteps                            319792
policy/Entropy                                0.182529
policy/KL                                     0.00705549
policy/KLBefore                               0
policy/LossAfter                             -0.0178849
policy/LossBefore                             1.50779e-08
policy/Perplexity                             1.20025
policy/dLoss                                  0.0178849
---------------------------------------  ----------------
2022-04-23 14:21:06 | [train_policy] epoch #316 | Obtaining samples for iteration 316...
2022-04-23 14:21:06 | [train_policy] epoch #316 | Logging diagnostics...
2022-04-23 14:21:06 | [train_policy] epoch #316 | Optimizing policy...
2022-04-23 14:21:06 | [train_policy] epoch #316 | Computing loss before
2022-04-23 14:21:06 | [train_policy] epoch #316 | Computing KL before
2022-04-23 14:21:06 | [train_policy] epoch #316 | Optimizing
2022-04-23 14:21:06 | [train_policy] epoch #316 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:06 | [train_policy] epoch #316 | computing loss before
2022-04-23 14:21:06 | [train_policy] epoch #316 | computing gradient
2022-04-23 14:21:06 | [train_policy] epoch #316 | gradient computed
2022-04-23 14:21:06 | [train_policy] epoch #316 | computing descent direction
2022-04-23 14:21:06 | [train_policy] epoch #316 | descent direction computed
2022-04-23 14:21:06 | [train_policy] epoch #316 | backtrack iters: 1
2022-04-23 14:21:06 | [train_policy] epoch #316 | optimization finished
2022-04-23 14:21:06 | [train_policy] epoch #316 | Computing KL after
2022-04-23 14:21:06 | [train_policy] epoch #316 | Computing loss after
2022-04-23 14:21:06 | [train_policy] epoch #316 | Fitting baseline...
2022-04-23 14:21:06 | [train_policy] epoch #316 | Saving snapshot...
2022-04-23 14:21:06 | [train_policy] epoch #316 | Saved
2022-04-23 14:21:06 | [train_policy] epoch #316 | Time 114.65 s
2022-04-23 14:21:06 | [train_policy] epoch #316 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.115827
Evaluation/AverageDiscountedReturn          -87.8388
Evaluation/AverageReturn                    -87.8388
Evaluation/CompletionRate                     0
Evaluation/Iteration                        316
Evaluation/MaxReturn                        -31.9713
Evaluation/MinReturn                      -4051.13
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        415.559
Extras/EpisodeRewardMean                    -84.3832
LinearFeatureBaseline/ExplainedVariance       0.0403587
PolicyExecTime                                0.0925763
ProcessExecTime                               0.0108643
TotalEnvSteps                            320804
policy/Entropy                                0.208373
policy/KL                                     0.00645592
policy/KLBefore                               0
policy/LossAfter                             -0.0207929
policy/LossBefore                             2.3088e-08
policy/Perplexity                             1.23167
policy/dLoss                                  0.0207929
---------------------------------------  ---------------
2022-04-23 14:21:06 | [train_policy] epoch #317 | Obtaining samples for iteration 317...
2022-04-23 14:21:07 | [train_policy] epoch #317 | Logging diagnostics...
2022-04-23 14:21:07 | [train_policy] epoch #317 | Optimizing policy...
2022-04-23 14:21:07 | [train_policy] epoch #317 | Computing loss before
2022-04-23 14:21:07 | [train_policy] epoch #317 | Computing KL before
2022-04-23 14:21:07 | [train_policy] epoch #317 | Optimizing
2022-04-23 14:21:07 | [train_policy] epoch #317 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:07 | [train_policy] epoch #317 | computing loss before
2022-04-23 14:21:07 | [train_policy] epoch #317 | computing gradient
2022-04-23 14:21:07 | [train_policy] epoch #317 | gradient computed
2022-04-23 14:21:07 | [train_policy] epoch #317 | computing descent direction
2022-04-23 14:21:07 | [train_policy] epoch #317 | descent direction computed
2022-04-23 14:21:07 | [train_policy] epoch #317 | backtrack iters: 1
2022-04-23 14:21:07 | [train_policy] epoch #317 | optimization finished
2022-04-23 14:21:07 | [train_policy] epoch #317 | Computing KL after
2022-04-23 14:21:07 | [train_policy] epoch #317 | Computing loss after
2022-04-23 14:21:07 | [train_policy] epoch #317 | Fitting baseline...
2022-04-23 14:21:07 | [train_policy] epoch #317 | Saving snapshot...
2022-04-23 14:21:07 | [train_policy] epoch #317 | Saved
2022-04-23 14:21:07 | [train_policy] epoch #317 | Time 114.98 s
2022-04-23 14:21:07 | [train_policy] epoch #317 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115527
Evaluation/AverageDiscountedReturn          -43.8263
Evaluation/AverageReturn                    -43.8263
Evaluation/CompletionRate                     0
Evaluation/Iteration                        317
Evaluation/MaxReturn                        -34.316
Evaluation/MinReturn                        -82.8109
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.72353
Extras/EpisodeRewardMean                    -43.6796
LinearFeatureBaseline/ExplainedVariance     -56.0314
PolicyExecTime                                0.0922585
ProcessExecTime                               0.0109951
TotalEnvSteps                            321816
policy/Entropy                                0.209058
policy/KL                                     0.0067224
policy/KLBefore                               0
policy/LossAfter                             -0.0174438
policy/LossBefore                            -3.20404e-08
policy/Perplexity                             1.23252
policy/dLoss                                  0.0174438
---------------------------------------  ----------------
2022-04-23 14:21:07 | [train_policy] epoch #318 | Obtaining samples for iteration 318...
2022-04-23 14:21:07 | [train_policy] epoch #318 | Logging diagnostics...
2022-04-23 14:21:07 | [train_policy] epoch #318 | Optimizing policy...
2022-04-23 14:21:07 | [train_policy] epoch #318 | Computing loss before
2022-04-23 14:21:07 | [train_policy] epoch #318 | Computing KL before
2022-04-23 14:21:07 | [train_policy] epoch #318 | Optimizing
2022-04-23 14:21:07 | [train_policy] epoch #318 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:07 | [train_policy] epoch #318 | computing loss before
2022-04-23 14:21:07 | [train_policy] epoch #318 | computing gradient
2022-04-23 14:21:07 | [train_policy] epoch #318 | gradient computed
2022-04-23 14:21:07 | [train_policy] epoch #318 | computing descent direction
2022-04-23 14:21:07 | [train_policy] epoch #318 | descent direction computed
2022-04-23 14:21:07 | [train_policy] epoch #318 | backtrack iters: 0
2022-04-23 14:21:07 | [train_policy] epoch #318 | optimization finished
2022-04-23 14:21:07 | [train_policy] epoch #318 | Computing KL after
2022-04-23 14:21:07 | [train_policy] epoch #318 | Computing loss after
2022-04-23 14:21:07 | [train_policy] epoch #318 | Fitting baseline...
2022-04-23 14:21:07 | [train_policy] epoch #318 | Saving snapshot...
2022-04-23 14:21:07 | [train_policy] epoch #318 | Saved
2022-04-23 14:21:07 | [train_policy] epoch #318 | Time 115.32 s
2022-04-23 14:21:07 | [train_policy] epoch #318 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116145
Evaluation/AverageDiscountedReturn          -65.0629
Evaluation/AverageReturn                    -65.0629
Evaluation/CompletionRate                     0
Evaluation/Iteration                        318
Evaluation/MaxReturn                        -29.7471
Evaluation/MinReturn                      -2071.67
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.464
Extras/EpisodeRewardMean                    -63.6456
LinearFeatureBaseline/ExplainedVariance       0.0128627
PolicyExecTime                                0.0916171
ProcessExecTime                               0.0133049
TotalEnvSteps                            322828
policy/Entropy                                0.236197
policy/KL                                     0.00945584
policy/KLBefore                               0
policy/LossAfter                             -0.0193465
policy/LossBefore                             1.93185e-08
policy/Perplexity                             1.26642
policy/dLoss                                  0.0193465
---------------------------------------  ----------------
2022-04-23 14:21:07 | [train_policy] epoch #319 | Obtaining samples for iteration 319...
2022-04-23 14:21:07 | [train_policy] epoch #319 | Logging diagnostics...
2022-04-23 14:21:07 | [train_policy] epoch #319 | Optimizing policy...
2022-04-23 14:21:07 | [train_policy] epoch #319 | Computing loss before
2022-04-23 14:21:07 | [train_policy] epoch #319 | Computing KL before
2022-04-23 14:21:07 | [train_policy] epoch #319 | Optimizing
2022-04-23 14:21:07 | [train_policy] epoch #319 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:07 | [train_policy] epoch #319 | computing loss before
2022-04-23 14:21:07 | [train_policy] epoch #319 | computing gradient
2022-04-23 14:21:07 | [train_policy] epoch #319 | gradient computed
2022-04-23 14:21:07 | [train_policy] epoch #319 | computing descent direction
2022-04-23 14:21:07 | [train_policy] epoch #319 | descent direction computed
2022-04-23 14:21:07 | [train_policy] epoch #319 | backtrack iters: 1
2022-04-23 14:21:07 | [train_policy] epoch #319 | optimization finished
2022-04-23 14:21:07 | [train_policy] epoch #319 | Computing KL after
2022-04-23 14:21:07 | [train_policy] epoch #319 | Computing loss after
2022-04-23 14:21:07 | [train_policy] epoch #319 | Fitting baseline...
2022-04-23 14:21:07 | [train_policy] epoch #319 | Saving snapshot...
2022-04-23 14:21:07 | [train_policy] epoch #319 | Saved
2022-04-23 14:21:07 | [train_policy] epoch #319 | Time 115.65 s
2022-04-23 14:21:07 | [train_policy] epoch #319 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.114806
Evaluation/AverageDiscountedReturn         -109.899
Evaluation/AverageReturn                   -109.899
Evaluation/CompletionRate                     0
Evaluation/Iteration                        319
Evaluation/MaxReturn                        -32.263
Evaluation/MinReturn                      -2069.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        358.839
Extras/EpisodeRewardMean                   -104.488
LinearFeatureBaseline/ExplainedVariance       0.161401
PolicyExecTime                                0.091362
ProcessExecTime                               0.0109916
TotalEnvSteps                            323840
policy/Entropy                                0.233637
policy/KL                                     0.00651866
policy/KLBefore                               0
policy/LossAfter                             -0.0200725
policy/LossBefore                            -1.93185e-08
policy/Perplexity                             1.26319
policy/dLoss                                  0.0200725
---------------------------------------  ----------------
2022-04-23 14:21:07 | [train_policy] epoch #320 | Obtaining samples for iteration 320...
2022-04-23 14:21:08 | [train_policy] epoch #320 | Logging diagnostics...
2022-04-23 14:21:08 | [train_policy] epoch #320 | Optimizing policy...
2022-04-23 14:21:08 | [train_policy] epoch #320 | Computing loss before
2022-04-23 14:21:08 | [train_policy] epoch #320 | Computing KL before
2022-04-23 14:21:08 | [train_policy] epoch #320 | Optimizing
2022-04-23 14:21:08 | [train_policy] epoch #320 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:08 | [train_policy] epoch #320 | computing loss before
2022-04-23 14:21:08 | [train_policy] epoch #320 | computing gradient
2022-04-23 14:21:08 | [train_policy] epoch #320 | gradient computed
2022-04-23 14:21:08 | [train_policy] epoch #320 | computing descent direction
2022-04-23 14:21:08 | [train_policy] epoch #320 | descent direction computed
2022-04-23 14:21:08 | [train_policy] epoch #320 | backtrack iters: 1
2022-04-23 14:21:08 | [train_policy] epoch #320 | optimization finished
2022-04-23 14:21:08 | [train_policy] epoch #320 | Computing KL after
2022-04-23 14:21:08 | [train_policy] epoch #320 | Computing loss after
2022-04-23 14:21:08 | [train_policy] epoch #320 | Fitting baseline...
2022-04-23 14:21:08 | [train_policy] epoch #320 | Saving snapshot...
2022-04-23 14:21:08 | [train_policy] epoch #320 | Saved
2022-04-23 14:21:08 | [train_policy] epoch #320 | Time 116.01 s
2022-04-23 14:21:08 | [train_policy] epoch #320 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.11926
Evaluation/AverageDiscountedReturn          -43.654
Evaluation/AverageReturn                    -43.654
Evaluation/CompletionRate                     0
Evaluation/Iteration                        320
Evaluation/MaxReturn                        -33.7646
Evaluation/MinReturn                        -69.9553
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.37862
Extras/EpisodeRewardMean                    -63.8024
LinearFeatureBaseline/ExplainedVariance     -94.2652
PolicyExecTime                                0.107113
ProcessExecTime                               0.0117564
TotalEnvSteps                            324852
policy/Entropy                                0.222745
policy/KL                                     0.00638982
policy/KLBefore                               0
policy/LossAfter                             -0.0142682
policy/LossBefore                            -1.06959e-07
policy/Perplexity                             1.2495
policy/dLoss                                  0.0142681
---------------------------------------  ----------------
2022-04-23 14:21:08 | [train_policy] epoch #321 | Obtaining samples for iteration 321...
2022-04-23 14:21:08 | [train_policy] epoch #321 | Logging diagnostics...
2022-04-23 14:21:08 | [train_policy] epoch #321 | Optimizing policy...
2022-04-23 14:21:08 | [train_policy] epoch #321 | Computing loss before
2022-04-23 14:21:08 | [train_policy] epoch #321 | Computing KL before
2022-04-23 14:21:08 | [train_policy] epoch #321 | Optimizing
2022-04-23 14:21:08 | [train_policy] epoch #321 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:08 | [train_policy] epoch #321 | computing loss before
2022-04-23 14:21:08 | [train_policy] epoch #321 | computing gradient
2022-04-23 14:21:08 | [train_policy] epoch #321 | gradient computed
2022-04-23 14:21:08 | [train_policy] epoch #321 | computing descent direction
2022-04-23 14:21:08 | [train_policy] epoch #321 | descent direction computed
2022-04-23 14:21:08 | [train_policy] epoch #321 | backtrack iters: 1
2022-04-23 14:21:08 | [train_policy] epoch #321 | optimization finished
2022-04-23 14:21:08 | [train_policy] epoch #321 | Computing KL after
2022-04-23 14:21:08 | [train_policy] epoch #321 | Computing loss after
2022-04-23 14:21:08 | [train_policy] epoch #321 | Fitting baseline...
2022-04-23 14:21:08 | [train_policy] epoch #321 | Saving snapshot...
2022-04-23 14:21:08 | [train_policy] epoch #321 | Saved
2022-04-23 14:21:08 | [train_policy] epoch #321 | Time 116.40 s
2022-04-23 14:21:08 | [train_policy] epoch #321 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.125908
Evaluation/AverageDiscountedReturn          -65.4288
Evaluation/AverageReturn                    -65.4288
Evaluation/CompletionRate                     0
Evaluation/Iteration                        321
Evaluation/MaxReturn                        -31.2865
Evaluation/MinReturn                      -2073.52
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.654
Extras/EpisodeRewardMean                    -63.547
LinearFeatureBaseline/ExplainedVariance       0.0130538
PolicyExecTime                                0.113928
ProcessExecTime                               0.0124896
TotalEnvSteps                            325864
policy/Entropy                                0.222764
policy/KL                                     0.00662441
policy/KLBefore                               0
policy/LossAfter                             -0.0174676
policy/LossBefore                             1.88473e-09
policy/Perplexity                             1.24953
policy/dLoss                                  0.0174677
---------------------------------------  ----------------
2022-04-23 14:21:08 | [train_policy] epoch #322 | Obtaining samples for iteration 322...
2022-04-23 14:21:08 | [train_policy] epoch #322 | Logging diagnostics...
2022-04-23 14:21:08 | [train_policy] epoch #322 | Optimizing policy...
2022-04-23 14:21:08 | [train_policy] epoch #322 | Computing loss before
2022-04-23 14:21:08 | [train_policy] epoch #322 | Computing KL before
2022-04-23 14:21:08 | [train_policy] epoch #322 | Optimizing
2022-04-23 14:21:08 | [train_policy] epoch #322 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:08 | [train_policy] epoch #322 | computing loss before
2022-04-23 14:21:08 | [train_policy] epoch #322 | computing gradient
2022-04-23 14:21:08 | [train_policy] epoch #322 | gradient computed
2022-04-23 14:21:08 | [train_policy] epoch #322 | computing descent direction
2022-04-23 14:21:09 | [train_policy] epoch #322 | descent direction computed
2022-04-23 14:21:09 | [train_policy] epoch #322 | backtrack iters: 0
2022-04-23 14:21:09 | [train_policy] epoch #322 | optimization finished
2022-04-23 14:21:09 | [train_policy] epoch #322 | Computing KL after
2022-04-23 14:21:09 | [train_policy] epoch #322 | Computing loss after
2022-04-23 14:21:09 | [train_policy] epoch #322 | Fitting baseline...
2022-04-23 14:21:09 | [train_policy] epoch #322 | Saving snapshot...
2022-04-23 14:21:09 | [train_policy] epoch #322 | Saved
2022-04-23 14:21:09 | [train_policy] epoch #322 | Time 116.78 s
2022-04-23 14:21:09 | [train_policy] epoch #322 | EpochTime 0.38 s
---------------------------------------  ----------------
EnvExecTime                                   0.128283
Evaluation/AverageDiscountedReturn          -42.6514
Evaluation/AverageReturn                    -42.6514
Evaluation/CompletionRate                     0
Evaluation/Iteration                        322
Evaluation/MaxReturn                        -31.8332
Evaluation/MinReturn                        -73.7383
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.3078
Extras/EpisodeRewardMean                    -42.6596
LinearFeatureBaseline/ExplainedVariance     -24.3329
PolicyExecTime                                0.115272
ProcessExecTime                               0.0127494
TotalEnvSteps                            326876
policy/Entropy                                0.218646
policy/KL                                     0.00915224
policy/KLBefore                               0
policy/LossAfter                             -0.025185
policy/LossBefore                            -4.20531e-08
policy/Perplexity                             1.24439
policy/dLoss                                  0.025185
---------------------------------------  ----------------
2022-04-23 14:21:09 | [train_policy] epoch #323 | Obtaining samples for iteration 323...
2022-04-23 14:21:09 | [train_policy] epoch #323 | Logging diagnostics...
2022-04-23 14:21:09 | [train_policy] epoch #323 | Optimizing policy...
2022-04-23 14:21:09 | [train_policy] epoch #323 | Computing loss before
2022-04-23 14:21:09 | [train_policy] epoch #323 | Computing KL before
2022-04-23 14:21:09 | [train_policy] epoch #323 | Optimizing
2022-04-23 14:21:09 | [train_policy] epoch #323 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:09 | [train_policy] epoch #323 | computing loss before
2022-04-23 14:21:09 | [train_policy] epoch #323 | computing gradient
2022-04-23 14:21:09 | [train_policy] epoch #323 | gradient computed
2022-04-23 14:21:09 | [train_policy] epoch #323 | computing descent direction
2022-04-23 14:21:09 | [train_policy] epoch #323 | descent direction computed
2022-04-23 14:21:09 | [train_policy] epoch #323 | backtrack iters: 1
2022-04-23 14:21:09 | [train_policy] epoch #323 | optimization finished
2022-04-23 14:21:09 | [train_policy] epoch #323 | Computing KL after
2022-04-23 14:21:09 | [train_policy] epoch #323 | Computing loss after
2022-04-23 14:21:09 | [train_policy] epoch #323 | Fitting baseline...
2022-04-23 14:21:09 | [train_policy] epoch #323 | Saving snapshot...
2022-04-23 14:21:09 | [train_policy] epoch #323 | Saved
2022-04-23 14:21:09 | [train_policy] epoch #323 | Time 117.12 s
2022-04-23 14:21:09 | [train_policy] epoch #323 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115928
Evaluation/AverageDiscountedReturn          -42.5411
Evaluation/AverageReturn                    -42.5411
Evaluation/CompletionRate                     0
Evaluation/Iteration                        323
Evaluation/MaxReturn                        -31.2787
Evaluation/MinReturn                        -66.6434
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.49639
Extras/EpisodeRewardMean                    -42.8467
LinearFeatureBaseline/ExplainedVariance       0.9157
PolicyExecTime                                0.0952332
ProcessExecTime                               0.0110137
TotalEnvSteps                            327888
policy/Entropy                                0.161489
policy/KL                                     0.00682768
policy/KLBefore                               0
policy/LossAfter                             -0.0197901
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             1.17526
policy/dLoss                                  0.0197901
---------------------------------------  ----------------
2022-04-23 14:21:09 | [train_policy] epoch #324 | Obtaining samples for iteration 324...
2022-04-23 14:21:09 | [train_policy] epoch #324 | Logging diagnostics...
2022-04-23 14:21:09 | [train_policy] epoch #324 | Optimizing policy...
2022-04-23 14:21:09 | [train_policy] epoch #324 | Computing loss before
2022-04-23 14:21:09 | [train_policy] epoch #324 | Computing KL before
2022-04-23 14:21:09 | [train_policy] epoch #324 | Optimizing
2022-04-23 14:21:09 | [train_policy] epoch #324 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:09 | [train_policy] epoch #324 | computing loss before
2022-04-23 14:21:09 | [train_policy] epoch #324 | computing gradient
2022-04-23 14:21:09 | [train_policy] epoch #324 | gradient computed
2022-04-23 14:21:09 | [train_policy] epoch #324 | computing descent direction
2022-04-23 14:21:09 | [train_policy] epoch #324 | descent direction computed
2022-04-23 14:21:09 | [train_policy] epoch #324 | backtrack iters: 0
2022-04-23 14:21:09 | [train_policy] epoch #324 | optimization finished
2022-04-23 14:21:09 | [train_policy] epoch #324 | Computing KL after
2022-04-23 14:21:09 | [train_policy] epoch #324 | Computing loss after
2022-04-23 14:21:09 | [train_policy] epoch #324 | Fitting baseline...
2022-04-23 14:21:09 | [train_policy] epoch #324 | Saving snapshot...
2022-04-23 14:21:09 | [train_policy] epoch #324 | Saved
2022-04-23 14:21:09 | [train_policy] epoch #324 | Time 117.46 s
2022-04-23 14:21:09 | [train_policy] epoch #324 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116406
Evaluation/AverageDiscountedReturn          -65.5558
Evaluation/AverageReturn                    -65.5558
Evaluation/CompletionRate                     0
Evaluation/Iteration                        324
Evaluation/MaxReturn                        -31.4908
Evaluation/MinReturn                      -2053.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.595
Extras/EpisodeRewardMean                    -63.6941
LinearFeatureBaseline/ExplainedVariance       0.00779864
PolicyExecTime                                0.0980489
ProcessExecTime                               0.0110743
TotalEnvSteps                            328900
policy/Entropy                                0.162303
policy/KL                                     0.00969297
policy/KLBefore                               0
policy/LossAfter                             -0.0267066
policy/LossBefore                             7.53893e-09
policy/Perplexity                             1.17622
policy/dLoss                                  0.0267066
---------------------------------------  ----------------
2022-04-23 14:21:09 | [train_policy] epoch #325 | Obtaining samples for iteration 325...
2022-04-23 14:21:09 | [train_policy] epoch #325 | Logging diagnostics...
2022-04-23 14:21:09 | [train_policy] epoch #325 | Optimizing policy...
2022-04-23 14:21:09 | [train_policy] epoch #325 | Computing loss before
2022-04-23 14:21:09 | [train_policy] epoch #325 | Computing KL before
2022-04-23 14:21:10 | [train_policy] epoch #325 | Optimizing
2022-04-23 14:21:10 | [train_policy] epoch #325 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:10 | [train_policy] epoch #325 | computing loss before
2022-04-23 14:21:10 | [train_policy] epoch #325 | computing gradient
2022-04-23 14:21:10 | [train_policy] epoch #325 | gradient computed
2022-04-23 14:21:10 | [train_policy] epoch #325 | computing descent direction
2022-04-23 14:21:10 | [train_policy] epoch #325 | descent direction computed
2022-04-23 14:21:10 | [train_policy] epoch #325 | backtrack iters: 0
2022-04-23 14:21:10 | [train_policy] epoch #325 | optimization finished
2022-04-23 14:21:10 | [train_policy] epoch #325 | Computing KL after
2022-04-23 14:21:10 | [train_policy] epoch #325 | Computing loss after
2022-04-23 14:21:10 | [train_policy] epoch #325 | Fitting baseline...
2022-04-23 14:21:10 | [train_policy] epoch #325 | Saving snapshot...
2022-04-23 14:21:10 | [train_policy] epoch #325 | Saved
2022-04-23 14:21:10 | [train_policy] epoch #325 | Time 117.79 s
2022-04-23 14:21:10 | [train_policy] epoch #325 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115291
Evaluation/AverageDiscountedReturn          -65.1355
Evaluation/AverageReturn                    -65.1355
Evaluation/CompletionRate                     0
Evaluation/Iteration                        325
Evaluation/MaxReturn                        -31.5731
Evaluation/MinReturn                      -2098.24
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        213.222
Extras/EpisodeRewardMean                    -63.5986
LinearFeatureBaseline/ExplainedVariance       0.0951503
PolicyExecTime                                0.0932477
ProcessExecTime                               0.010843
TotalEnvSteps                            329912
policy/Entropy                                0.172665
policy/KL                                     0.00814195
policy/KLBefore                               0
policy/LossAfter                             -0.0299851
policy/LossBefore                            -1.17796e-09
policy/Perplexity                             1.18847
policy/dLoss                                  0.0299851
---------------------------------------  ----------------
2022-04-23 14:21:10 | [train_policy] epoch #326 | Obtaining samples for iteration 326...
2022-04-23 14:21:10 | [train_policy] epoch #326 | Logging diagnostics...
2022-04-23 14:21:10 | [train_policy] epoch #326 | Optimizing policy...
2022-04-23 14:21:10 | [train_policy] epoch #326 | Computing loss before
2022-04-23 14:21:10 | [train_policy] epoch #326 | Computing KL before
2022-04-23 14:21:10 | [train_policy] epoch #326 | Optimizing
2022-04-23 14:21:10 | [train_policy] epoch #326 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:10 | [train_policy] epoch #326 | computing loss before
2022-04-23 14:21:10 | [train_policy] epoch #326 | computing gradient
2022-04-23 14:21:10 | [train_policy] epoch #326 | gradient computed
2022-04-23 14:21:10 | [train_policy] epoch #326 | computing descent direction
2022-04-23 14:21:10 | [train_policy] epoch #326 | descent direction computed
2022-04-23 14:21:10 | [train_policy] epoch #326 | backtrack iters: 1
2022-04-23 14:21:10 | [train_policy] epoch #326 | optimization finished
2022-04-23 14:21:10 | [train_policy] epoch #326 | Computing KL after
2022-04-23 14:21:10 | [train_policy] epoch #326 | Computing loss after
2022-04-23 14:21:10 | [train_policy] epoch #326 | Fitting baseline...
2022-04-23 14:21:10 | [train_policy] epoch #326 | Saving snapshot...
2022-04-23 14:21:10 | [train_policy] epoch #326 | Saved
2022-04-23 14:21:10 | [train_policy] epoch #326 | Time 118.14 s
2022-04-23 14:21:10 | [train_policy] epoch #326 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115971
Evaluation/AverageDiscountedReturn          -44.5127
Evaluation/AverageReturn                    -44.5127
Evaluation/CompletionRate                     0
Evaluation/Iteration                        326
Evaluation/MaxReturn                        -31.207
Evaluation/MinReturn                        -76.0487
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.48498
Extras/EpisodeRewardMean                    -44.5127
LinearFeatureBaseline/ExplainedVariance     -28.1027
PolicyExecTime                                0.103015
ProcessExecTime                               0.0114162
TotalEnvSteps                            330924
policy/Entropy                                0.148389
policy/KL                                     0.00655812
policy/KLBefore                               0
policy/LossAfter                             -0.0134097
policy/LossBefore                            -3.18048e-08
policy/Perplexity                             1.15996
policy/dLoss                                  0.0134096
---------------------------------------  ----------------
2022-04-23 14:21:10 | [train_policy] epoch #327 | Obtaining samples for iteration 327...
2022-04-23 14:21:10 | [train_policy] epoch #327 | Logging diagnostics...
2022-04-23 14:21:10 | [train_policy] epoch #327 | Optimizing policy...
2022-04-23 14:21:10 | [train_policy] epoch #327 | Computing loss before
2022-04-23 14:21:10 | [train_policy] epoch #327 | Computing KL before
2022-04-23 14:21:10 | [train_policy] epoch #327 | Optimizing
2022-04-23 14:21:10 | [train_policy] epoch #327 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:10 | [train_policy] epoch #327 | computing loss before
2022-04-23 14:21:10 | [train_policy] epoch #327 | computing gradient
2022-04-23 14:21:10 | [train_policy] epoch #327 | gradient computed
2022-04-23 14:21:10 | [train_policy] epoch #327 | computing descent direction
2022-04-23 14:21:10 | [train_policy] epoch #327 | descent direction computed
2022-04-23 14:21:10 | [train_policy] epoch #327 | backtrack iters: 1
2022-04-23 14:21:10 | [train_policy] epoch #327 | optimization finished
2022-04-23 14:21:10 | [train_policy] epoch #327 | Computing KL after
2022-04-23 14:21:10 | [train_policy] epoch #327 | Computing loss after
2022-04-23 14:21:10 | [train_policy] epoch #327 | Fitting baseline...
2022-04-23 14:21:10 | [train_policy] epoch #327 | Saving snapshot...
2022-04-23 14:21:10 | [train_policy] epoch #327 | Saved
2022-04-23 14:21:10 | [train_policy] epoch #327 | Time 118.49 s
2022-04-23 14:21:10 | [train_policy] epoch #327 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115633
Evaluation/AverageDiscountedReturn          -64.9987
Evaluation/AverageReturn                    -64.9987
Evaluation/CompletionRate                     0
Evaluation/Iteration                        327
Evaluation/MaxReturn                        -30.6208
Evaluation/MinReturn                      -2065.57
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.865
Extras/EpisodeRewardMean                    -63.2211
LinearFeatureBaseline/ExplainedVariance       0.0145943
PolicyExecTime                                0.101545
ProcessExecTime                               0.0109546
TotalEnvSteps                            331936
policy/Entropy                                0.144591
policy/KL                                     0.00688716
policy/KLBefore                               0
policy/LossAfter                             -0.0230624
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             1.15557
policy/dLoss                                  0.0230624
---------------------------------------  ----------------
2022-04-23 14:21:10 | [train_policy] epoch #328 | Obtaining samples for iteration 328...
2022-04-23 14:21:11 | [train_policy] epoch #328 | Logging diagnostics...
2022-04-23 14:21:11 | [train_policy] epoch #328 | Optimizing policy...
2022-04-23 14:21:11 | [train_policy] epoch #328 | Computing loss before
2022-04-23 14:21:11 | [train_policy] epoch #328 | Computing KL before
2022-04-23 14:21:11 | [train_policy] epoch #328 | Optimizing
2022-04-23 14:21:11 | [train_policy] epoch #328 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:11 | [train_policy] epoch #328 | computing loss before
2022-04-23 14:21:11 | [train_policy] epoch #328 | computing gradient
2022-04-23 14:21:11 | [train_policy] epoch #328 | gradient computed
2022-04-23 14:21:11 | [train_policy] epoch #328 | computing descent direction
2022-04-23 14:21:11 | [train_policy] epoch #328 | descent direction computed
2022-04-23 14:21:11 | [train_policy] epoch #328 | backtrack iters: 1
2022-04-23 14:21:11 | [train_policy] epoch #328 | optimization finished
2022-04-23 14:21:11 | [train_policy] epoch #328 | Computing KL after
2022-04-23 14:21:11 | [train_policy] epoch #328 | Computing loss after
2022-04-23 14:21:11 | [train_policy] epoch #328 | Fitting baseline...
2022-04-23 14:21:11 | [train_policy] epoch #328 | Saving snapshot...
2022-04-23 14:21:11 | [train_policy] epoch #328 | Saved
2022-04-23 14:21:11 | [train_policy] epoch #328 | Time 118.85 s
2022-04-23 14:21:11 | [train_policy] epoch #328 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.116456
Evaluation/AverageDiscountedReturn          -65.6629
Evaluation/AverageReturn                    -65.6629
Evaluation/CompletionRate                     0
Evaluation/Iteration                        328
Evaluation/MaxReturn                        -32.1181
Evaluation/MinReturn                      -2063.73
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.568
Extras/EpisodeRewardMean                    -83.7971
LinearFeatureBaseline/ExplainedVariance       0.130108
PolicyExecTime                                0.112495
ProcessExecTime                               0.0117121
TotalEnvSteps                            332948
policy/Entropy                                0.14977
policy/KL                                     0.00694515
policy/KLBefore                               0
policy/LossAfter                             -0.0355987
policy/LossBefore                            -7.77452e-09
policy/Perplexity                             1.16157
policy/dLoss                                  0.0355987
---------------------------------------  ----------------
2022-04-23 14:21:11 | [train_policy] epoch #329 | Obtaining samples for iteration 329...
2022-04-23 14:21:11 | [train_policy] epoch #329 | Logging diagnostics...
2022-04-23 14:21:11 | [train_policy] epoch #329 | Optimizing policy...
2022-04-23 14:21:11 | [train_policy] epoch #329 | Computing loss before
2022-04-23 14:21:11 | [train_policy] epoch #329 | Computing KL before
2022-04-23 14:21:11 | [train_policy] epoch #329 | Optimizing
2022-04-23 14:21:11 | [train_policy] epoch #329 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:11 | [train_policy] epoch #329 | computing loss before
2022-04-23 14:21:11 | [train_policy] epoch #329 | computing gradient
2022-04-23 14:21:11 | [train_policy] epoch #329 | gradient computed
2022-04-23 14:21:11 | [train_policy] epoch #329 | computing descent direction
2022-04-23 14:21:11 | [train_policy] epoch #329 | descent direction computed
2022-04-23 14:21:11 | [train_policy] epoch #329 | backtrack iters: 0
2022-04-23 14:21:11 | [train_policy] epoch #329 | optimization finished
2022-04-23 14:21:11 | [train_policy] epoch #329 | Computing KL after
2022-04-23 14:21:11 | [train_policy] epoch #329 | Computing loss after
2022-04-23 14:21:11 | [train_policy] epoch #329 | Fitting baseline...
2022-04-23 14:21:11 | [train_policy] epoch #329 | Saving snapshot...
2022-04-23 14:21:11 | [train_policy] epoch #329 | Saved
2022-04-23 14:21:11 | [train_policy] epoch #329 | Time 119.19 s
2022-04-23 14:21:11 | [train_policy] epoch #329 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.116012
Evaluation/AverageDiscountedReturn          -65.813
Evaluation/AverageReturn                    -65.813
Evaluation/CompletionRate                     0
Evaluation/Iteration                        329
Evaluation/MaxReturn                        -31.5373
Evaluation/MinReturn                      -2062.66
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.471
Extras/EpisodeRewardMean                    -64.1052
LinearFeatureBaseline/ExplainedVariance       0.112995
PolicyExecTime                                0.099905
ProcessExecTime                               0.0111363
TotalEnvSteps                            333960
policy/Entropy                                0.159787
policy/KL                                     0.00872695
policy/KLBefore                               0
policy/LossAfter                             -0.0230323
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             1.17326
policy/dLoss                                  0.0230322
---------------------------------------  ---------------
2022-04-23 14:21:11 | [train_policy] epoch #330 | Obtaining samples for iteration 330...
2022-04-23 14:21:11 | [train_policy] epoch #330 | Logging diagnostics...
2022-04-23 14:21:11 | [train_policy] epoch #330 | Optimizing policy...
2022-04-23 14:21:11 | [train_policy] epoch #330 | Computing loss before
2022-04-23 14:21:11 | [train_policy] epoch #330 | Computing KL before
2022-04-23 14:21:11 | [train_policy] epoch #330 | Optimizing
2022-04-23 14:21:11 | [train_policy] epoch #330 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:11 | [train_policy] epoch #330 | computing loss before
2022-04-23 14:21:11 | [train_policy] epoch #330 | computing gradient
2022-04-23 14:21:11 | [train_policy] epoch #330 | gradient computed
2022-04-23 14:21:11 | [train_policy] epoch #330 | computing descent direction
2022-04-23 14:21:11 | [train_policy] epoch #330 | descent direction computed
2022-04-23 14:21:11 | [train_policy] epoch #330 | backtrack iters: 1
2022-04-23 14:21:11 | [train_policy] epoch #330 | optimization finished
2022-04-23 14:21:11 | [train_policy] epoch #330 | Computing KL after
2022-04-23 14:21:11 | [train_policy] epoch #330 | Computing loss after
2022-04-23 14:21:11 | [train_policy] epoch #330 | Fitting baseline...
2022-04-23 14:21:11 | [train_policy] epoch #330 | Saving snapshot...
2022-04-23 14:21:11 | [train_policy] epoch #330 | Saved
2022-04-23 14:21:11 | [train_policy] epoch #330 | Time 119.52 s
2022-04-23 14:21:11 | [train_policy] epoch #330 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.115408
Evaluation/AverageDiscountedReturn          -44.0972
Evaluation/AverageReturn                    -44.0972
Evaluation/CompletionRate                     0
Evaluation/Iteration                        330
Evaluation/MaxReturn                        -29.9031
Evaluation/MinReturn                       -147.123
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.5257
Extras/EpisodeRewardMean                    -43.9366
LinearFeatureBaseline/ExplainedVariance     -34.2542
PolicyExecTime                                0.0975173
ProcessExecTime                               0.0109539
TotalEnvSteps                            334972
policy/Entropy                                0.158958
policy/KL                                     0.00674422
policy/KLBefore                               0
policy/LossAfter                             -0.0161678
policy/LossBefore                             5.4186e-09
policy/Perplexity                             1.17229
policy/dLoss                                  0.0161678
---------------------------------------  ---------------
2022-04-23 14:21:11 | [train_policy] epoch #331 | Obtaining samples for iteration 331...
2022-04-23 14:21:12 | [train_policy] epoch #331 | Logging diagnostics...
2022-04-23 14:21:12 | [train_policy] epoch #331 | Optimizing policy...
2022-04-23 14:21:12 | [train_policy] epoch #331 | Computing loss before
2022-04-23 14:21:12 | [train_policy] epoch #331 | Computing KL before
2022-04-23 14:21:12 | [train_policy] epoch #331 | Optimizing
2022-04-23 14:21:12 | [train_policy] epoch #331 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:12 | [train_policy] epoch #331 | computing loss before
2022-04-23 14:21:12 | [train_policy] epoch #331 | computing gradient
2022-04-23 14:21:12 | [train_policy] epoch #331 | gradient computed
2022-04-23 14:21:12 | [train_policy] epoch #331 | computing descent direction
2022-04-23 14:21:12 | [train_policy] epoch #331 | descent direction computed
2022-04-23 14:21:12 | [train_policy] epoch #331 | backtrack iters: 1
2022-04-23 14:21:12 | [train_policy] epoch #331 | optimization finished
2022-04-23 14:21:12 | [train_policy] epoch #331 | Computing KL after
2022-04-23 14:21:12 | [train_policy] epoch #331 | Computing loss after
2022-04-23 14:21:12 | [train_policy] epoch #331 | Fitting baseline...
2022-04-23 14:21:12 | [train_policy] epoch #331 | Saving snapshot...
2022-04-23 14:21:12 | [train_policy] epoch #331 | Saved
2022-04-23 14:21:12 | [train_policy] epoch #331 | Time 119.86 s
2022-04-23 14:21:12 | [train_policy] epoch #331 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11595
Evaluation/AverageDiscountedReturn          -44.1742
Evaluation/AverageReturn                    -44.1742
Evaluation/CompletionRate                     0
Evaluation/Iteration                        331
Evaluation/MaxReturn                        -30.7599
Evaluation/MinReturn                        -74.3239
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.66061
Extras/EpisodeRewardMean                    -43.953
LinearFeatureBaseline/ExplainedVariance       0.892625
PolicyExecTime                                0.0931358
ProcessExecTime                               0.0108626
TotalEnvSteps                            335984
policy/Entropy                                0.139973
policy/KL                                     0.00666441
policy/KLBefore                               0
policy/LossAfter                             -0.0132885
policy/LossBefore                             6.83215e-09
policy/Perplexity                             1.15024
policy/dLoss                                  0.0132885
---------------------------------------  ----------------
2022-04-23 14:21:12 | [train_policy] epoch #332 | Obtaining samples for iteration 332...
2022-04-23 14:21:12 | [train_policy] epoch #332 | Logging diagnostics...
2022-04-23 14:21:12 | [train_policy] epoch #332 | Optimizing policy...
2022-04-23 14:21:12 | [train_policy] epoch #332 | Computing loss before
2022-04-23 14:21:12 | [train_policy] epoch #332 | Computing KL before
2022-04-23 14:21:12 | [train_policy] epoch #332 | Optimizing
2022-04-23 14:21:12 | [train_policy] epoch #332 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:12 | [train_policy] epoch #332 | computing loss before
2022-04-23 14:21:12 | [train_policy] epoch #332 | computing gradient
2022-04-23 14:21:12 | [train_policy] epoch #332 | gradient computed
2022-04-23 14:21:12 | [train_policy] epoch #332 | computing descent direction
2022-04-23 14:21:12 | [train_policy] epoch #332 | descent direction computed
2022-04-23 14:21:12 | [train_policy] epoch #332 | backtrack iters: 0
2022-04-23 14:21:12 | [train_policy] epoch #332 | optimization finished
2022-04-23 14:21:12 | [train_policy] epoch #332 | Computing KL after
2022-04-23 14:21:12 | [train_policy] epoch #332 | Computing loss after
2022-04-23 14:21:12 | [train_policy] epoch #332 | Fitting baseline...
2022-04-23 14:21:12 | [train_policy] epoch #332 | Saving snapshot...
2022-04-23 14:21:12 | [train_policy] epoch #332 | Saved
2022-04-23 14:21:12 | [train_policy] epoch #332 | Time 120.19 s
2022-04-23 14:21:12 | [train_policy] epoch #332 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.1151
Evaluation/AverageDiscountedReturn          -45.2397
Evaluation/AverageReturn                    -45.2397
Evaluation/CompletionRate                     0
Evaluation/Iteration                        332
Evaluation/MaxReturn                        -29.4503
Evaluation/MinReturn                        -67.4121
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83595
Extras/EpisodeRewardMean                    -45.0382
LinearFeatureBaseline/ExplainedVariance       0.896266
PolicyExecTime                                0.0916378
ProcessExecTime                               0.010891
TotalEnvSteps                            336996
policy/Entropy                                0.127323
policy/KL                                     0.00954546
policy/KLBefore                               0
policy/LossAfter                             -0.0132689
policy/LossBefore                            -3.27472e-08
policy/Perplexity                             1.13578
policy/dLoss                                  0.0132689
---------------------------------------  ----------------
2022-04-23 14:21:12 | [train_policy] epoch #333 | Obtaining samples for iteration 333...
2022-04-23 14:21:12 | [train_policy] epoch #333 | Logging diagnostics...
2022-04-23 14:21:12 | [train_policy] epoch #333 | Optimizing policy...
2022-04-23 14:21:12 | [train_policy] epoch #333 | Computing loss before
2022-04-23 14:21:12 | [train_policy] epoch #333 | Computing KL before
2022-04-23 14:21:12 | [train_policy] epoch #333 | Optimizing
2022-04-23 14:21:12 | [train_policy] epoch #333 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:12 | [train_policy] epoch #333 | computing loss before
2022-04-23 14:21:12 | [train_policy] epoch #333 | computing gradient
2022-04-23 14:21:12 | [train_policy] epoch #333 | gradient computed
2022-04-23 14:21:12 | [train_policy] epoch #333 | computing descent direction
2022-04-23 14:21:12 | [train_policy] epoch #333 | descent direction computed
2022-04-23 14:21:12 | [train_policy] epoch #333 | backtrack iters: 0
2022-04-23 14:21:12 | [train_policy] epoch #333 | optimization finished
2022-04-23 14:21:12 | [train_policy] epoch #333 | Computing KL after
2022-04-23 14:21:12 | [train_policy] epoch #333 | Computing loss after
2022-04-23 14:21:12 | [train_policy] epoch #333 | Fitting baseline...
2022-04-23 14:21:12 | [train_policy] epoch #333 | Saving snapshot...
2022-04-23 14:21:12 | [train_policy] epoch #333 | Saved
2022-04-23 14:21:12 | [train_policy] epoch #333 | Time 120.53 s
2022-04-23 14:21:12 | [train_policy] epoch #333 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115733
Evaluation/AverageDiscountedReturn          -43.209
Evaluation/AverageReturn                    -43.209
Evaluation/CompletionRate                     0
Evaluation/Iteration                        333
Evaluation/MaxReturn                        -30.7081
Evaluation/MinReturn                        -66.3783
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.66647
Extras/EpisodeRewardMean                    -43.4099
LinearFeatureBaseline/ExplainedVariance       0.903266
PolicyExecTime                                0.0937612
ProcessExecTime                               0.0108705
TotalEnvSteps                            338008
policy/Entropy                                0.084677
policy/KL                                     0.00941996
policy/KLBefore                               0
policy/LossAfter                             -0.0156505
policy/LossBefore                             1.74338e-08
policy/Perplexity                             1.08837
policy/dLoss                                  0.0156506
---------------------------------------  ----------------
2022-04-23 14:21:12 | [train_policy] epoch #334 | Obtaining samples for iteration 334...
2022-04-23 14:21:13 | [train_policy] epoch #334 | Logging diagnostics...
2022-04-23 14:21:13 | [train_policy] epoch #334 | Optimizing policy...
2022-04-23 14:21:13 | [train_policy] epoch #334 | Computing loss before
2022-04-23 14:21:13 | [train_policy] epoch #334 | Computing KL before
2022-04-23 14:21:13 | [train_policy] epoch #334 | Optimizing
2022-04-23 14:21:13 | [train_policy] epoch #334 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:13 | [train_policy] epoch #334 | computing loss before
2022-04-23 14:21:13 | [train_policy] epoch #334 | computing gradient
2022-04-23 14:21:13 | [train_policy] epoch #334 | gradient computed
2022-04-23 14:21:13 | [train_policy] epoch #334 | computing descent direction
2022-04-23 14:21:13 | [train_policy] epoch #334 | descent direction computed
2022-04-23 14:21:13 | [train_policy] epoch #334 | backtrack iters: 1
2022-04-23 14:21:13 | [train_policy] epoch #334 | optimization finished
2022-04-23 14:21:13 | [train_policy] epoch #334 | Computing KL after
2022-04-23 14:21:13 | [train_policy] epoch #334 | Computing loss after
2022-04-23 14:21:13 | [train_policy] epoch #334 | Fitting baseline...
2022-04-23 14:21:13 | [train_policy] epoch #334 | Saving snapshot...
2022-04-23 14:21:13 | [train_policy] epoch #334 | Saved
2022-04-23 14:21:13 | [train_policy] epoch #334 | Time 120.87 s
2022-04-23 14:21:13 | [train_policy] epoch #334 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.11794
Evaluation/AverageDiscountedReturn          -43.4479
Evaluation/AverageReturn                    -43.4479
Evaluation/CompletionRate                     0
Evaluation/Iteration                        334
Evaluation/MaxReturn                        -33.0113
Evaluation/MinReturn                        -72.9349
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.96286
Extras/EpisodeRewardMean                    -43.554
LinearFeatureBaseline/ExplainedVariance       0.907003
PolicyExecTime                                0.0964551
ProcessExecTime                               0.0108409
TotalEnvSteps                            339020
policy/Entropy                                0.0790193
policy/KL                                     0.00740436
policy/KLBefore                               0
policy/LossAfter                             -0.0207156
policy/LossBefore                            -0
policy/Perplexity                             1.08223
policy/dLoss                                  0.0207156
---------------------------------------  ---------------
2022-04-23 14:21:13 | [train_policy] epoch #335 | Obtaining samples for iteration 335...
2022-04-23 14:21:13 | [train_policy] epoch #335 | Logging diagnostics...
2022-04-23 14:21:13 | [train_policy] epoch #335 | Optimizing policy...
2022-04-23 14:21:13 | [train_policy] epoch #335 | Computing loss before
2022-04-23 14:21:13 | [train_policy] epoch #335 | Computing KL before
2022-04-23 14:21:13 | [train_policy] epoch #335 | Optimizing
2022-04-23 14:21:13 | [train_policy] epoch #335 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:13 | [train_policy] epoch #335 | computing loss before
2022-04-23 14:21:13 | [train_policy] epoch #335 | computing gradient
2022-04-23 14:21:13 | [train_policy] epoch #335 | gradient computed
2022-04-23 14:21:13 | [train_policy] epoch #335 | computing descent direction
2022-04-23 14:21:13 | [train_policy] epoch #335 | descent direction computed
2022-04-23 14:21:13 | [train_policy] epoch #335 | backtrack iters: 0
2022-04-23 14:21:13 | [train_policy] epoch #335 | optimization finished
2022-04-23 14:21:13 | [train_policy] epoch #335 | Computing KL after
2022-04-23 14:21:13 | [train_policy] epoch #335 | Computing loss after
2022-04-23 14:21:13 | [train_policy] epoch #335 | Fitting baseline...
2022-04-23 14:21:13 | [train_policy] epoch #335 | Saving snapshot...
2022-04-23 14:21:13 | [train_policy] epoch #335 | Saved
2022-04-23 14:21:13 | [train_policy] epoch #335 | Time 121.22 s
2022-04-23 14:21:13 | [train_policy] epoch #335 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116459
Evaluation/AverageDiscountedReturn          -87.9235
Evaluation/AverageReturn                    -87.9235
Evaluation/CompletionRate                     0
Evaluation/Iteration                        335
Evaluation/MaxReturn                        -33.2065
Evaluation/MinReturn                      -2120.12
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        302.382
Extras/EpisodeRewardMean                    -84.3584
LinearFeatureBaseline/ExplainedVariance       0.0105419
PolicyExecTime                                0.102795
ProcessExecTime                               0.0112567
TotalEnvSteps                            340032
policy/Entropy                                0.0732188
policy/KL                                     0.00943575
policy/KLBefore                               0
policy/LossAfter                             -0.0330625
policy/LossBefore                            -1.43711e-08
policy/Perplexity                             1.07597
policy/dLoss                                  0.0330625
---------------------------------------  ----------------
2022-04-23 14:21:13 | [train_policy] epoch #336 | Obtaining samples for iteration 336...
2022-04-23 14:21:13 | [train_policy] epoch #336 | Logging diagnostics...
2022-04-23 14:21:13 | [train_policy] epoch #336 | Optimizing policy...
2022-04-23 14:21:13 | [train_policy] epoch #336 | Computing loss before
2022-04-23 14:21:13 | [train_policy] epoch #336 | Computing KL before
2022-04-23 14:21:13 | [train_policy] epoch #336 | Optimizing
2022-04-23 14:21:13 | [train_policy] epoch #336 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:13 | [train_policy] epoch #336 | computing loss before
2022-04-23 14:21:13 | [train_policy] epoch #336 | computing gradient
2022-04-23 14:21:13 | [train_policy] epoch #336 | gradient computed
2022-04-23 14:21:13 | [train_policy] epoch #336 | computing descent direction
2022-04-23 14:21:13 | [train_policy] epoch #336 | descent direction computed
2022-04-23 14:21:13 | [train_policy] epoch #336 | backtrack iters: 1
2022-04-23 14:21:13 | [train_policy] epoch #336 | optimization finished
2022-04-23 14:21:13 | [train_policy] epoch #336 | Computing KL after
2022-04-23 14:21:13 | [train_policy] epoch #336 | Computing loss after
2022-04-23 14:21:13 | [train_policy] epoch #336 | Fitting baseline...
2022-04-23 14:21:13 | [train_policy] epoch #336 | Saving snapshot...
2022-04-23 14:21:13 | [train_policy] epoch #336 | Saved
2022-04-23 14:21:13 | [train_policy] epoch #336 | Time 121.59 s
2022-04-23 14:21:13 | [train_policy] epoch #336 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                   0.11957
Evaluation/AverageDiscountedReturn          -65.7739
Evaluation/AverageReturn                    -65.7739
Evaluation/CompletionRate                     0
Evaluation/Iteration                        336
Evaluation/MaxReturn                        -31.665
Evaluation/MinReturn                      -2067.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.883
Extras/EpisodeRewardMean                    -64.1012
LinearFeatureBaseline/ExplainedVariance       0.0802883
PolicyExecTime                                0.112584
ProcessExecTime                               0.0119669
TotalEnvSteps                            341044
policy/Entropy                                0.057709
policy/KL                                     0.00667008
policy/KLBefore                               0
policy/LossAfter                             -0.0150986
policy/LossBefore                             1.1544e-08
policy/Perplexity                             1.05941
policy/dLoss                                  0.0150986
---------------------------------------  ---------------
2022-04-23 14:21:13 | [train_policy] epoch #337 | Obtaining samples for iteration 337...
2022-04-23 14:21:14 | [train_policy] epoch #337 | Logging diagnostics...
2022-04-23 14:21:14 | [train_policy] epoch #337 | Optimizing policy...
2022-04-23 14:21:14 | [train_policy] epoch #337 | Computing loss before
2022-04-23 14:21:14 | [train_policy] epoch #337 | Computing KL before
2022-04-23 14:21:14 | [train_policy] epoch #337 | Optimizing
2022-04-23 14:21:14 | [train_policy] epoch #337 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:14 | [train_policy] epoch #337 | computing loss before
2022-04-23 14:21:14 | [train_policy] epoch #337 | computing gradient
2022-04-23 14:21:14 | [train_policy] epoch #337 | gradient computed
2022-04-23 14:21:14 | [train_policy] epoch #337 | computing descent direction
2022-04-23 14:21:14 | [train_policy] epoch #337 | descent direction computed
2022-04-23 14:21:14 | [train_policy] epoch #337 | backtrack iters: 1
2022-04-23 14:21:14 | [train_policy] epoch #337 | optimization finished
2022-04-23 14:21:14 | [train_policy] epoch #337 | Computing KL after
2022-04-23 14:21:14 | [train_policy] epoch #337 | Computing loss after
2022-04-23 14:21:14 | [train_policy] epoch #337 | Fitting baseline...
2022-04-23 14:21:14 | [train_policy] epoch #337 | Saving snapshot...
2022-04-23 14:21:14 | [train_policy] epoch #337 | Saved
2022-04-23 14:21:14 | [train_policy] epoch #337 | Time 121.95 s
2022-04-23 14:21:14 | [train_policy] epoch #337 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.119439
Evaluation/AverageDiscountedReturn          -65.0411
Evaluation/AverageReturn                    -65.0411
Evaluation/CompletionRate                     0
Evaluation/Iteration                        337
Evaluation/MaxReturn                        -30.1395
Evaluation/MinReturn                      -2065.27
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.83
Extras/EpisodeRewardMean                    -63.3653
LinearFeatureBaseline/ExplainedVariance       0.0916624
PolicyExecTime                                0.111014
ProcessExecTime                               0.0119369
TotalEnvSteps                            342056
policy/Entropy                                0.104409
policy/KL                                     0.00675086
policy/KLBefore                               0
policy/LossAfter                             -0.0241979
policy/LossBefore                             4.71183e-10
policy/Perplexity                             1.11005
policy/dLoss                                  0.0241979
---------------------------------------  ----------------
2022-04-23 14:21:14 | [train_policy] epoch #338 | Obtaining samples for iteration 338...
2022-04-23 14:21:14 | [train_policy] epoch #338 | Logging diagnostics...
2022-04-23 14:21:14 | [train_policy] epoch #338 | Optimizing policy...
2022-04-23 14:21:14 | [train_policy] epoch #338 | Computing loss before
2022-04-23 14:21:14 | [train_policy] epoch #338 | Computing KL before
2022-04-23 14:21:14 | [train_policy] epoch #338 | Optimizing
2022-04-23 14:21:14 | [train_policy] epoch #338 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:14 | [train_policy] epoch #338 | computing loss before
2022-04-23 14:21:14 | [train_policy] epoch #338 | computing gradient
2022-04-23 14:21:14 | [train_policy] epoch #338 | gradient computed
2022-04-23 14:21:14 | [train_policy] epoch #338 | computing descent direction
2022-04-23 14:21:14 | [train_policy] epoch #338 | descent direction computed
2022-04-23 14:21:14 | [train_policy] epoch #338 | backtrack iters: 1
2022-04-23 14:21:14 | [train_policy] epoch #338 | optimization finished
2022-04-23 14:21:14 | [train_policy] epoch #338 | Computing KL after
2022-04-23 14:21:14 | [train_policy] epoch #338 | Computing loss after
2022-04-23 14:21:14 | [train_policy] epoch #338 | Fitting baseline...
2022-04-23 14:21:14 | [train_policy] epoch #338 | Saving snapshot...
2022-04-23 14:21:14 | [train_policy] epoch #338 | Saved
2022-04-23 14:21:14 | [train_policy] epoch #338 | Time 122.30 s
2022-04-23 14:21:14 | [train_policy] epoch #338 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.11591
Evaluation/AverageDiscountedReturn          -65.4828
Evaluation/AverageReturn                    -65.4828
Evaluation/CompletionRate                     0
Evaluation/Iteration                        338
Evaluation/MaxReturn                        -34.2088
Evaluation/MinReturn                      -2063.87
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.59
Extras/EpisodeRewardMean                    -63.8052
LinearFeatureBaseline/ExplainedVariance       0.127714
PolicyExecTime                                0.0966291
ProcessExecTime                               0.0111053
TotalEnvSteps                            343068
policy/Entropy                                0.100041
policy/KL                                     0.0069368
policy/KLBefore                               0
policy/LossAfter                             -0.0200152
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             1.10522
policy/dLoss                                  0.0200152
---------------------------------------  ---------------
2022-04-23 14:21:14 | [train_policy] epoch #339 | Obtaining samples for iteration 339...
2022-04-23 14:21:14 | [train_policy] epoch #339 | Logging diagnostics...
2022-04-23 14:21:14 | [train_policy] epoch #339 | Optimizing policy...
2022-04-23 14:21:14 | [train_policy] epoch #339 | Computing loss before
2022-04-23 14:21:14 | [train_policy] epoch #339 | Computing KL before
2022-04-23 14:21:14 | [train_policy] epoch #339 | Optimizing
2022-04-23 14:21:14 | [train_policy] epoch #339 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:14 | [train_policy] epoch #339 | computing loss before
2022-04-23 14:21:14 | [train_policy] epoch #339 | computing gradient
2022-04-23 14:21:14 | [train_policy] epoch #339 | gradient computed
2022-04-23 14:21:14 | [train_policy] epoch #339 | computing descent direction
2022-04-23 14:21:14 | [train_policy] epoch #339 | descent direction computed
2022-04-23 14:21:14 | [train_policy] epoch #339 | backtrack iters: 0
2022-04-23 14:21:14 | [train_policy] epoch #339 | optimization finished
2022-04-23 14:21:14 | [train_policy] epoch #339 | Computing KL after
2022-04-23 14:21:14 | [train_policy] epoch #339 | Computing loss after
2022-04-23 14:21:14 | [train_policy] epoch #339 | Fitting baseline...
2022-04-23 14:21:14 | [train_policy] epoch #339 | Saving snapshot...
2022-04-23 14:21:14 | [train_policy] epoch #339 | Saved
2022-04-23 14:21:14 | [train_policy] epoch #339 | Time 122.63 s
2022-04-23 14:21:14 | [train_policy] epoch #339 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115482
Evaluation/AverageDiscountedReturn          -88.2803
Evaluation/AverageReturn                    -88.2803
Evaluation/CompletionRate                     0
Evaluation/Iteration                        339
Evaluation/MaxReturn                        -31.5207
Evaluation/MinReturn                      -2067.72
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.699
Extras/EpisodeRewardMean                    -84.623
LinearFeatureBaseline/ExplainedVariance       0.188177
PolicyExecTime                                0.0955493
ProcessExecTime                               0.0109224
TotalEnvSteps                            344080
policy/Entropy                                0.0919621
policy/KL                                     0.0094686
policy/KLBefore                               0
policy/LossAfter                             -0.020482
policy/LossBefore                             9.42366e-09
policy/Perplexity                             1.09632
policy/dLoss                                  0.020482
---------------------------------------  ----------------
2022-04-23 14:21:14 | [train_policy] epoch #340 | Obtaining samples for iteration 340...
2022-04-23 14:21:15 | [train_policy] epoch #340 | Logging diagnostics...
2022-04-23 14:21:15 | [train_policy] epoch #340 | Optimizing policy...
2022-04-23 14:21:15 | [train_policy] epoch #340 | Computing loss before
2022-04-23 14:21:15 | [train_policy] epoch #340 | Computing KL before
2022-04-23 14:21:15 | [train_policy] epoch #340 | Optimizing
2022-04-23 14:21:15 | [train_policy] epoch #340 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:15 | [train_policy] epoch #340 | computing loss before
2022-04-23 14:21:15 | [train_policy] epoch #340 | computing gradient
2022-04-23 14:21:15 | [train_policy] epoch #340 | gradient computed
2022-04-23 14:21:15 | [train_policy] epoch #340 | computing descent direction
2022-04-23 14:21:15 | [train_policy] epoch #340 | descent direction computed
2022-04-23 14:21:15 | [train_policy] epoch #340 | backtrack iters: 1
2022-04-23 14:21:15 | [train_policy] epoch #340 | optimization finished
2022-04-23 14:21:15 | [train_policy] epoch #340 | Computing KL after
2022-04-23 14:21:15 | [train_policy] epoch #340 | Computing loss after
2022-04-23 14:21:15 | [train_policy] epoch #340 | Fitting baseline...
2022-04-23 14:21:15 | [train_policy] epoch #340 | Saving snapshot...
2022-04-23 14:21:15 | [train_policy] epoch #340 | Saved
2022-04-23 14:21:15 | [train_policy] epoch #340 | Time 123.01 s
2022-04-23 14:21:15 | [train_policy] epoch #340 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.127931
Evaluation/AverageDiscountedReturn         -108.932
Evaluation/AverageReturn                   -108.932
Evaluation/CompletionRate                     0
Evaluation/Iteration                        340
Evaluation/MaxReturn                        -29.8865
Evaluation/MinReturn                      -2067.8
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.292
Extras/EpisodeRewardMean                   -103.521
LinearFeatureBaseline/ExplainedVariance       0.133329
PolicyExecTime                                0.110991
ProcessExecTime                               0.0128019
TotalEnvSteps                            345092
policy/Entropy                                0.09847
policy/KL                                     0.00681697
policy/KLBefore                               0
policy/LossAfter                             -0.0181191
policy/LossBefore                             1.88473e-09
policy/Perplexity                             1.10348
policy/dLoss                                  0.0181191
---------------------------------------  ----------------
2022-04-23 14:21:15 | [train_policy] epoch #341 | Obtaining samples for iteration 341...
2022-04-23 14:21:15 | [train_policy] epoch #341 | Logging diagnostics...
2022-04-23 14:21:15 | [train_policy] epoch #341 | Optimizing policy...
2022-04-23 14:21:15 | [train_policy] epoch #341 | Computing loss before
2022-04-23 14:21:15 | [train_policy] epoch #341 | Computing KL before
2022-04-23 14:21:15 | [train_policy] epoch #341 | Optimizing
2022-04-23 14:21:15 | [train_policy] epoch #341 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:15 | [train_policy] epoch #341 | computing loss before
2022-04-23 14:21:15 | [train_policy] epoch #341 | computing gradient
2022-04-23 14:21:15 | [train_policy] epoch #341 | gradient computed
2022-04-23 14:21:15 | [train_policy] epoch #341 | computing descent direction
2022-04-23 14:21:15 | [train_policy] epoch #341 | descent direction computed
2022-04-23 14:21:15 | [train_policy] epoch #341 | backtrack iters: 1
2022-04-23 14:21:15 | [train_policy] epoch #341 | optimization finished
2022-04-23 14:21:15 | [train_policy] epoch #341 | Computing KL after
2022-04-23 14:21:15 | [train_policy] epoch #341 | Computing loss after
2022-04-23 14:21:15 | [train_policy] epoch #341 | Fitting baseline...
2022-04-23 14:21:15 | [train_policy] epoch #341 | Saving snapshot...
2022-04-23 14:21:15 | [train_policy] epoch #341 | Saved
2022-04-23 14:21:15 | [train_policy] epoch #341 | Time 123.35 s
2022-04-23 14:21:15 | [train_policy] epoch #341 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.117697
Evaluation/AverageDiscountedReturn          -89.7832
Evaluation/AverageReturn                    -89.7832
Evaluation/CompletionRate                     0
Evaluation/Iteration                        341
Evaluation/MaxReturn                        -31.3487
Evaluation/MinReturn                      -2072.52
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.413
Extras/EpisodeRewardMean                    -85.9179
LinearFeatureBaseline/ExplainedVariance       0.14691
PolicyExecTime                                0.0976229
ProcessExecTime                               0.0115259
TotalEnvSteps                            346104
policy/Entropy                                0.0905578
policy/KL                                     0.00692851
policy/KLBefore                               0
policy/LossAfter                             -0.0125253
policy/LossBefore                             8.2457e-09
policy/Perplexity                             1.09478
policy/dLoss                                  0.0125253
---------------------------------------  ---------------
2022-04-23 14:21:15 | [train_policy] epoch #342 | Obtaining samples for iteration 342...
2022-04-23 14:21:15 | [train_policy] epoch #342 | Logging diagnostics...
2022-04-23 14:21:15 | [train_policy] epoch #342 | Optimizing policy...
2022-04-23 14:21:15 | [train_policy] epoch #342 | Computing loss before
2022-04-23 14:21:15 | [train_policy] epoch #342 | Computing KL before
2022-04-23 14:21:15 | [train_policy] epoch #342 | Optimizing
2022-04-23 14:21:15 | [train_policy] epoch #342 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:15 | [train_policy] epoch #342 | computing loss before
2022-04-23 14:21:15 | [train_policy] epoch #342 | computing gradient
2022-04-23 14:21:15 | [train_policy] epoch #342 | gradient computed
2022-04-23 14:21:15 | [train_policy] epoch #342 | computing descent direction
2022-04-23 14:21:15 | [train_policy] epoch #342 | descent direction computed
2022-04-23 14:21:15 | [train_policy] epoch #342 | backtrack iters: 0
2022-04-23 14:21:15 | [train_policy] epoch #342 | optimization finished
2022-04-23 14:21:15 | [train_policy] epoch #342 | Computing KL after
2022-04-23 14:21:15 | [train_policy] epoch #342 | Computing loss after
2022-04-23 14:21:15 | [train_policy] epoch #342 | Fitting baseline...
2022-04-23 14:21:15 | [train_policy] epoch #342 | Saving snapshot...
2022-04-23 14:21:15 | [train_policy] epoch #342 | Saved
2022-04-23 14:21:15 | [train_policy] epoch #342 | Time 123.70 s
2022-04-23 14:21:15 | [train_policy] epoch #342 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116305
Evaluation/AverageDiscountedReturn          -44.7473
Evaluation/AverageReturn                    -44.7473
Evaluation/CompletionRate                     0
Evaluation/Iteration                        342
Evaluation/MaxReturn                        -29.6548
Evaluation/MinReturn                        -74.5343
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.75332
Extras/EpisodeRewardMean                    -44.9857
LinearFeatureBaseline/ExplainedVariance     -58.2784
PolicyExecTime                                0.104612
ProcessExecTime                               0.011466
TotalEnvSteps                            347116
policy/Entropy                                0.104575
policy/KL                                     0.00978357
policy/KLBefore                               0
policy/LossAfter                             -0.0260499
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             1.11024
policy/dLoss                                  0.0260499
---------------------------------------  ----------------
2022-04-23 14:21:15 | [train_policy] epoch #343 | Obtaining samples for iteration 343...
2022-04-23 14:21:16 | [train_policy] epoch #343 | Logging diagnostics...
2022-04-23 14:21:16 | [train_policy] epoch #343 | Optimizing policy...
2022-04-23 14:21:16 | [train_policy] epoch #343 | Computing loss before
2022-04-23 14:21:16 | [train_policy] epoch #343 | Computing KL before
2022-04-23 14:21:16 | [train_policy] epoch #343 | Optimizing
2022-04-23 14:21:16 | [train_policy] epoch #343 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:16 | [train_policy] epoch #343 | computing loss before
2022-04-23 14:21:16 | [train_policy] epoch #343 | computing gradient
2022-04-23 14:21:16 | [train_policy] epoch #343 | gradient computed
2022-04-23 14:21:16 | [train_policy] epoch #343 | computing descent direction
2022-04-23 14:21:16 | [train_policy] epoch #343 | descent direction computed
2022-04-23 14:21:16 | [train_policy] epoch #343 | backtrack iters: 1
2022-04-23 14:21:16 | [train_policy] epoch #343 | optimization finished
2022-04-23 14:21:16 | [train_policy] epoch #343 | Computing KL after
2022-04-23 14:21:16 | [train_policy] epoch #343 | Computing loss after
2022-04-23 14:21:16 | [train_policy] epoch #343 | Fitting baseline...
2022-04-23 14:21:16 | [train_policy] epoch #343 | Saving snapshot...
2022-04-23 14:21:16 | [train_policy] epoch #343 | Saved
2022-04-23 14:21:16 | [train_policy] epoch #343 | Time 124.04 s
2022-04-23 14:21:16 | [train_policy] epoch #343 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116522
Evaluation/AverageDiscountedReturn          -43.5126
Evaluation/AverageReturn                    -43.5126
Evaluation/CompletionRate                     0
Evaluation/Iteration                        343
Evaluation/MaxReturn                        -29.9636
Evaluation/MinReturn                        -82.4743
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.66829
Extras/EpisodeRewardMean                    -43.6537
LinearFeatureBaseline/ExplainedVariance       0.87222
PolicyExecTime                                0.0971665
ProcessExecTime                               0.011049
TotalEnvSteps                            348128
policy/Entropy                                0.0979908
policy/KL                                     0.007021
policy/KLBefore                               0
policy/LossAfter                             -0.0181709
policy/LossBefore                             1.41355e-09
policy/Perplexity                             1.10295
policy/dLoss                                  0.0181709
---------------------------------------  ----------------
2022-04-23 14:21:16 | [train_policy] epoch #344 | Obtaining samples for iteration 344...
2022-04-23 14:21:16 | [train_policy] epoch #344 | Logging diagnostics...
2022-04-23 14:21:16 | [train_policy] epoch #344 | Optimizing policy...
2022-04-23 14:21:16 | [train_policy] epoch #344 | Computing loss before
2022-04-23 14:21:16 | [train_policy] epoch #344 | Computing KL before
2022-04-23 14:21:16 | [train_policy] epoch #344 | Optimizing
2022-04-23 14:21:16 | [train_policy] epoch #344 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:16 | [train_policy] epoch #344 | computing loss before
2022-04-23 14:21:16 | [train_policy] epoch #344 | computing gradient
2022-04-23 14:21:16 | [train_policy] epoch #344 | gradient computed
2022-04-23 14:21:16 | [train_policy] epoch #344 | computing descent direction
2022-04-23 14:21:16 | [train_policy] epoch #344 | descent direction computed
2022-04-23 14:21:16 | [train_policy] epoch #344 | backtrack iters: 0
2022-04-23 14:21:16 | [train_policy] epoch #344 | optimization finished
2022-04-23 14:21:16 | [train_policy] epoch #344 | Computing KL after
2022-04-23 14:21:16 | [train_policy] epoch #344 | Computing loss after
2022-04-23 14:21:16 | [train_policy] epoch #344 | Fitting baseline...
2022-04-23 14:21:16 | [train_policy] epoch #344 | Saving snapshot...
2022-04-23 14:21:16 | [train_policy] epoch #344 | Saved
2022-04-23 14:21:16 | [train_policy] epoch #344 | Time 124.39 s
2022-04-23 14:21:16 | [train_policy] epoch #344 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117611
Evaluation/AverageDiscountedReturn          -67.1359
Evaluation/AverageReturn                    -67.1359
Evaluation/CompletionRate                     0
Evaluation/Iteration                        344
Evaluation/MaxReturn                        -30.2564
Evaluation/MinReturn                      -2076.33
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.752
Extras/EpisodeRewardMean                    -65.2998
LinearFeatureBaseline/ExplainedVariance       0.0125863
PolicyExecTime                                0.105253
ProcessExecTime                               0.0115097
TotalEnvSteps                            349140
policy/Entropy                                0.0456259
policy/KL                                     0.00847274
policy/KLBefore                               0
policy/LossAfter                             -0.0116266
policy/LossBefore                             3.76946e-09
policy/Perplexity                             1.04668
policy/dLoss                                  0.0116266
---------------------------------------  ----------------
2022-04-23 14:21:16 | [train_policy] epoch #345 | Obtaining samples for iteration 345...
2022-04-23 14:21:16 | [train_policy] epoch #345 | Logging diagnostics...
2022-04-23 14:21:16 | [train_policy] epoch #345 | Optimizing policy...
2022-04-23 14:21:16 | [train_policy] epoch #345 | Computing loss before
2022-04-23 14:21:16 | [train_policy] epoch #345 | Computing KL before
2022-04-23 14:21:16 | [train_policy] epoch #345 | Optimizing
2022-04-23 14:21:16 | [train_policy] epoch #345 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:16 | [train_policy] epoch #345 | computing loss before
2022-04-23 14:21:16 | [train_policy] epoch #345 | computing gradient
2022-04-23 14:21:16 | [train_policy] epoch #345 | gradient computed
2022-04-23 14:21:16 | [train_policy] epoch #345 | computing descent direction
2022-04-23 14:21:16 | [train_policy] epoch #345 | descent direction computed
2022-04-23 14:21:16 | [train_policy] epoch #345 | backtrack iters: 0
2022-04-23 14:21:16 | [train_policy] epoch #345 | optimization finished
2022-04-23 14:21:16 | [train_policy] epoch #345 | Computing KL after
2022-04-23 14:21:16 | [train_policy] epoch #345 | Computing loss after
2022-04-23 14:21:17 | [train_policy] epoch #345 | Fitting baseline...
2022-04-23 14:21:17 | [train_policy] epoch #345 | Saving snapshot...
2022-04-23 14:21:17 | [train_policy] epoch #345 | Saved
2022-04-23 14:21:17 | [train_policy] epoch #345 | Time 124.73 s
2022-04-23 14:21:17 | [train_policy] epoch #345 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116031
Evaluation/AverageDiscountedReturn         -111.561
Evaluation/AverageReturn                   -111.561
Evaluation/CompletionRate                     0
Evaluation/Iteration                        345
Evaluation/MaxReturn                        -31.6177
Evaluation/MinReturn                      -4051.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        472.613
Extras/EpisodeRewardMean                   -105.946
LinearFeatureBaseline/ExplainedVariance       0.12317
PolicyExecTime                                0.0981166
ProcessExecTime                               0.0108981
TotalEnvSteps                            350152
policy/Entropy                                0.028096
policy/KL                                     0.00915517
policy/KLBefore                               0
policy/LossAfter                             -0.0415416
policy/LossBefore                             1.31931e-08
policy/Perplexity                             1.02849
policy/dLoss                                  0.0415416
---------------------------------------  ----------------
2022-04-23 14:21:17 | [train_policy] epoch #346 | Obtaining samples for iteration 346...
2022-04-23 14:21:17 | [train_policy] epoch #346 | Logging diagnostics...
2022-04-23 14:21:17 | [train_policy] epoch #346 | Optimizing policy...
2022-04-23 14:21:17 | [train_policy] epoch #346 | Computing loss before
2022-04-23 14:21:17 | [train_policy] epoch #346 | Computing KL before
2022-04-23 14:21:17 | [train_policy] epoch #346 | Optimizing
2022-04-23 14:21:17 | [train_policy] epoch #346 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:17 | [train_policy] epoch #346 | computing loss before
2022-04-23 14:21:17 | [train_policy] epoch #346 | computing gradient
2022-04-23 14:21:17 | [train_policy] epoch #346 | gradient computed
2022-04-23 14:21:17 | [train_policy] epoch #346 | computing descent direction
2022-04-23 14:21:17 | [train_policy] epoch #346 | descent direction computed
2022-04-23 14:21:17 | [train_policy] epoch #346 | backtrack iters: 0
2022-04-23 14:21:17 | [train_policy] epoch #346 | optimization finished
2022-04-23 14:21:17 | [train_policy] epoch #346 | Computing KL after
2022-04-23 14:21:17 | [train_policy] epoch #346 | Computing loss after
2022-04-23 14:21:17 | [train_policy] epoch #346 | Fitting baseline...
2022-04-23 14:21:17 | [train_policy] epoch #346 | Saving snapshot...
2022-04-23 14:21:17 | [train_policy] epoch #346 | Saved
2022-04-23 14:21:17 | [train_policy] epoch #346 | Time 125.08 s
2022-04-23 14:21:17 | [train_policy] epoch #346 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116672
Evaluation/AverageDiscountedReturn          -42.5713
Evaluation/AverageReturn                    -42.5713
Evaluation/CompletionRate                     0
Evaluation/Iteration                        346
Evaluation/MaxReturn                        -31.9455
Evaluation/MinReturn                        -66.1436
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.62833
Extras/EpisodeRewardMean                    -42.9694
LinearFeatureBaseline/ExplainedVariance    -210.12
PolicyExecTime                                0.108527
ProcessExecTime                               0.0117919
TotalEnvSteps                            351164
policy/Entropy                                0.0279176
policy/KL                                     0.00985804
policy/KLBefore                               0
policy/LossAfter                             -0.0258216
policy/LossBefore                             1.13084e-08
policy/Perplexity                             1.02831
policy/dLoss                                  0.0258216
---------------------------------------  ----------------
2022-04-23 14:21:17 | [train_policy] epoch #347 | Obtaining samples for iteration 347...
2022-04-23 14:21:17 | [train_policy] epoch #347 | Logging diagnostics...
2022-04-23 14:21:17 | [train_policy] epoch #347 | Optimizing policy...
2022-04-23 14:21:17 | [train_policy] epoch #347 | Computing loss before
2022-04-23 14:21:17 | [train_policy] epoch #347 | Computing KL before
2022-04-23 14:21:17 | [train_policy] epoch #347 | Optimizing
2022-04-23 14:21:17 | [train_policy] epoch #347 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:17 | [train_policy] epoch #347 | computing loss before
2022-04-23 14:21:17 | [train_policy] epoch #347 | computing gradient
2022-04-23 14:21:17 | [train_policy] epoch #347 | gradient computed
2022-04-23 14:21:17 | [train_policy] epoch #347 | computing descent direction
2022-04-23 14:21:17 | [train_policy] epoch #347 | descent direction computed
2022-04-23 14:21:17 | [train_policy] epoch #347 | backtrack iters: 1
2022-04-23 14:21:17 | [train_policy] epoch #347 | optimization finished
2022-04-23 14:21:17 | [train_policy] epoch #347 | Computing KL after
2022-04-23 14:21:17 | [train_policy] epoch #347 | Computing loss after
2022-04-23 14:21:17 | [train_policy] epoch #347 | Fitting baseline...
2022-04-23 14:21:17 | [train_policy] epoch #347 | Saving snapshot...
2022-04-23 14:21:17 | [train_policy] epoch #347 | Saved
2022-04-23 14:21:17 | [train_policy] epoch #347 | Time 125.43 s
2022-04-23 14:21:17 | [train_policy] epoch #347 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117034
Evaluation/AverageDiscountedReturn          -65.5514
Evaluation/AverageReturn                    -65.5514
Evaluation/CompletionRate                     0
Evaluation/Iteration                        347
Evaluation/MaxReturn                        -29.4261
Evaluation/MinReturn                      -2067.61
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.016
Extras/EpisodeRewardMean                    -63.7163
LinearFeatureBaseline/ExplainedVariance       0.0132662
PolicyExecTime                                0.115367
ProcessExecTime                               0.0113506
TotalEnvSteps                            352176
policy/Entropy                                0.00269008
policy/KL                                     0.00822212
policy/KLBefore                               0
policy/LossAfter                             -0.0227375
policy/LossBefore                             6.59656e-09
policy/Perplexity                             1.00269
policy/dLoss                                  0.0227375
---------------------------------------  ----------------
2022-04-23 14:21:17 | [train_policy] epoch #348 | Obtaining samples for iteration 348...
2022-04-23 14:21:17 | [train_policy] epoch #348 | Logging diagnostics...
2022-04-23 14:21:17 | [train_policy] epoch #348 | Optimizing policy...
2022-04-23 14:21:17 | [train_policy] epoch #348 | Computing loss before
2022-04-23 14:21:17 | [train_policy] epoch #348 | Computing KL before
2022-04-23 14:21:17 | [train_policy] epoch #348 | Optimizing
2022-04-23 14:21:17 | [train_policy] epoch #348 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:17 | [train_policy] epoch #348 | computing loss before
2022-04-23 14:21:17 | [train_policy] epoch #348 | computing gradient
2022-04-23 14:21:17 | [train_policy] epoch #348 | gradient computed
2022-04-23 14:21:17 | [train_policy] epoch #348 | computing descent direction
2022-04-23 14:21:18 | [train_policy] epoch #348 | descent direction computed
2022-04-23 14:21:18 | [train_policy] epoch #348 | backtrack iters: 1
2022-04-23 14:21:18 | [train_policy] epoch #348 | optimization finished
2022-04-23 14:21:18 | [train_policy] epoch #348 | Computing KL after
2022-04-23 14:21:18 | [train_policy] epoch #348 | Computing loss after
2022-04-23 14:21:18 | [train_policy] epoch #348 | Fitting baseline...
2022-04-23 14:21:18 | [train_policy] epoch #348 | Saving snapshot...
2022-04-23 14:21:18 | [train_policy] epoch #348 | Saved
2022-04-23 14:21:18 | [train_policy] epoch #348 | Time 125.78 s
2022-04-23 14:21:18 | [train_policy] epoch #348 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116647
Evaluation/AverageDiscountedReturn          -42.4842
Evaluation/AverageReturn                    -42.4842
Evaluation/CompletionRate                     0
Evaluation/Iteration                        348
Evaluation/MaxReturn                        -30.1681
Evaluation/MinReturn                        -70.5616
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.19735
Extras/EpisodeRewardMean                    -42.8733
LinearFeatureBaseline/ExplainedVariance     -17.819
PolicyExecTime                                0.101129
ProcessExecTime                               0.0110915
TotalEnvSteps                            353188
policy/Entropy                                0.00799131
policy/KL                                     0.0064923
policy/KLBefore                               0
policy/LossAfter                             -0.0308523
policy/LossBefore                            -3.53387e-09
policy/Perplexity                             1.00802
policy/dLoss                                  0.0308523
---------------------------------------  ----------------
2022-04-23 14:21:18 | [train_policy] epoch #349 | Obtaining samples for iteration 349...
2022-04-23 14:21:18 | [train_policy] epoch #349 | Logging diagnostics...
2022-04-23 14:21:18 | [train_policy] epoch #349 | Optimizing policy...
2022-04-23 14:21:18 | [train_policy] epoch #349 | Computing loss before
2022-04-23 14:21:18 | [train_policy] epoch #349 | Computing KL before
2022-04-23 14:21:18 | [train_policy] epoch #349 | Optimizing
2022-04-23 14:21:18 | [train_policy] epoch #349 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:18 | [train_policy] epoch #349 | computing loss before
2022-04-23 14:21:18 | [train_policy] epoch #349 | computing gradient
2022-04-23 14:21:18 | [train_policy] epoch #349 | gradient computed
2022-04-23 14:21:18 | [train_policy] epoch #349 | computing descent direction
2022-04-23 14:21:18 | [train_policy] epoch #349 | descent direction computed
2022-04-23 14:21:18 | [train_policy] epoch #349 | backtrack iters: 1
2022-04-23 14:21:18 | [train_policy] epoch #349 | optimization finished
2022-04-23 14:21:18 | [train_policy] epoch #349 | Computing KL after
2022-04-23 14:21:18 | [train_policy] epoch #349 | Computing loss after
2022-04-23 14:21:18 | [train_policy] epoch #349 | Fitting baseline...
2022-04-23 14:21:18 | [train_policy] epoch #349 | Saving snapshot...
2022-04-23 14:21:18 | [train_policy] epoch #349 | Saved
2022-04-23 14:21:18 | [train_policy] epoch #349 | Time 126.14 s
2022-04-23 14:21:18 | [train_policy] epoch #349 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.117666
Evaluation/AverageDiscountedReturn          -65.1839
Evaluation/AverageReturn                    -65.1839
Evaluation/CompletionRate                     0
Evaluation/Iteration                        349
Evaluation/MaxReturn                        -30.9307
Evaluation/MinReturn                      -2065.21
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.782
Extras/EpisodeRewardMean                    -63.193
LinearFeatureBaseline/ExplainedVariance       0.0122597
PolicyExecTime                                0.110461
ProcessExecTime                               0.0114338
TotalEnvSteps                            354200
policy/Entropy                                0.00922656
policy/KL                                     0.00744451
policy/KLBefore                               0
policy/LossAfter                             -0.0324706
policy/LossBefore                             1.60202e-08
policy/Perplexity                             1.00927
policy/dLoss                                  0.0324706
---------------------------------------  ----------------
2022-04-23 14:21:18 | [train_policy] epoch #350 | Obtaining samples for iteration 350...
2022-04-23 14:21:18 | [train_policy] epoch #350 | Logging diagnostics...
2022-04-23 14:21:18 | [train_policy] epoch #350 | Optimizing policy...
2022-04-23 14:21:18 | [train_policy] epoch #350 | Computing loss before
2022-04-23 14:21:18 | [train_policy] epoch #350 | Computing KL before
2022-04-23 14:21:18 | [train_policy] epoch #350 | Optimizing
2022-04-23 14:21:18 | [train_policy] epoch #350 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:18 | [train_policy] epoch #350 | computing loss before
2022-04-23 14:21:18 | [train_policy] epoch #350 | computing gradient
2022-04-23 14:21:18 | [train_policy] epoch #350 | gradient computed
2022-04-23 14:21:18 | [train_policy] epoch #350 | computing descent direction
2022-04-23 14:21:18 | [train_policy] epoch #350 | descent direction computed
2022-04-23 14:21:18 | [train_policy] epoch #350 | backtrack iters: 0
2022-04-23 14:21:18 | [train_policy] epoch #350 | optimization finished
2022-04-23 14:21:18 | [train_policy] epoch #350 | Computing KL after
2022-04-23 14:21:18 | [train_policy] epoch #350 | Computing loss after
2022-04-23 14:21:18 | [train_policy] epoch #350 | Fitting baseline...
2022-04-23 14:21:18 | [train_policy] epoch #350 | Saving snapshot...
2022-04-23 14:21:18 | [train_policy] epoch #350 | Saved
2022-04-23 14:21:18 | [train_policy] epoch #350 | Time 126.51 s
2022-04-23 14:21:18 | [train_policy] epoch #350 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.123591
Evaluation/AverageDiscountedReturn         -109.415
Evaluation/AverageReturn                   -109.415
Evaluation/CompletionRate                     0
Evaluation/Iteration                        350
Evaluation/MaxReturn                        -30.8663
Evaluation/MinReturn                      -2100.6
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        362.436
Extras/EpisodeRewardMean                   -104.063
LinearFeatureBaseline/ExplainedVariance       0.212084
PolicyExecTime                                0.1128
ProcessExecTime                               0.0121727
TotalEnvSteps                            355212
policy/Entropy                                0.0249918
policy/KL                                     0.00976807
policy/KLBefore                               0
policy/LossAfter                             -0.0213772
policy/LossBefore                             1.31931e-08
policy/Perplexity                             1.02531
policy/dLoss                                  0.0213772
---------------------------------------  ----------------
2022-04-23 14:21:18 | [train_policy] epoch #351 | Obtaining samples for iteration 351...
2022-04-23 14:21:19 | [train_policy] epoch #351 | Logging diagnostics...
2022-04-23 14:21:19 | [train_policy] epoch #351 | Optimizing policy...
2022-04-23 14:21:19 | [train_policy] epoch #351 | Computing loss before
2022-04-23 14:21:19 | [train_policy] epoch #351 | Computing KL before
2022-04-23 14:21:19 | [train_policy] epoch #351 | Optimizing
2022-04-23 14:21:19 | [train_policy] epoch #351 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:19 | [train_policy] epoch #351 | computing loss before
2022-04-23 14:21:19 | [train_policy] epoch #351 | computing gradient
2022-04-23 14:21:19 | [train_policy] epoch #351 | gradient computed
2022-04-23 14:21:19 | [train_policy] epoch #351 | computing descent direction
2022-04-23 14:21:19 | [train_policy] epoch #351 | descent direction computed
2022-04-23 14:21:19 | [train_policy] epoch #351 | backtrack iters: 0
2022-04-23 14:21:19 | [train_policy] epoch #351 | optimization finished
2022-04-23 14:21:19 | [train_policy] epoch #351 | Computing KL after
2022-04-23 14:21:19 | [train_policy] epoch #351 | Computing loss after
2022-04-23 14:21:19 | [train_policy] epoch #351 | Fitting baseline...
2022-04-23 14:21:19 | [train_policy] epoch #351 | Saving snapshot...
2022-04-23 14:21:19 | [train_policy] epoch #351 | Saved
2022-04-23 14:21:19 | [train_policy] epoch #351 | Time 126.85 s
2022-04-23 14:21:19 | [train_policy] epoch #351 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115772
Evaluation/AverageDiscountedReturn          -65.7391
Evaluation/AverageReturn                    -65.7391
Evaluation/CompletionRate                     0
Evaluation/Iteration                        351
Evaluation/MaxReturn                        -31.3708
Evaluation/MinReturn                      -2067.39
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.987
Extras/EpisodeRewardMean                    -63.8391
LinearFeatureBaseline/ExplainedVariance      -0.171093
PolicyExecTime                                0.105522
ProcessExecTime                               0.0111663
TotalEnvSteps                            356224
policy/Entropy                                0.016773
policy/KL                                     0.00999723
policy/KLBefore                               0
policy/LossAfter                             -0.0175144
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             1.01691
policy/dLoss                                  0.0175144
---------------------------------------  ----------------
2022-04-23 14:21:19 | [train_policy] epoch #352 | Obtaining samples for iteration 352...
2022-04-23 14:21:19 | [train_policy] epoch #352 | Logging diagnostics...
2022-04-23 14:21:19 | [train_policy] epoch #352 | Optimizing policy...
2022-04-23 14:21:19 | [train_policy] epoch #352 | Computing loss before
2022-04-23 14:21:19 | [train_policy] epoch #352 | Computing KL before
2022-04-23 14:21:19 | [train_policy] epoch #352 | Optimizing
2022-04-23 14:21:19 | [train_policy] epoch #352 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:19 | [train_policy] epoch #352 | computing loss before
2022-04-23 14:21:19 | [train_policy] epoch #352 | computing gradient
2022-04-23 14:21:19 | [train_policy] epoch #352 | gradient computed
2022-04-23 14:21:19 | [train_policy] epoch #352 | computing descent direction
2022-04-23 14:21:19 | [train_policy] epoch #352 | descent direction computed
2022-04-23 14:21:19 | [train_policy] epoch #352 | backtrack iters: 1
2022-04-23 14:21:19 | [train_policy] epoch #352 | optimization finished
2022-04-23 14:21:19 | [train_policy] epoch #352 | Computing KL after
2022-04-23 14:21:19 | [train_policy] epoch #352 | Computing loss after
2022-04-23 14:21:19 | [train_policy] epoch #352 | Fitting baseline...
2022-04-23 14:21:19 | [train_policy] epoch #352 | Saving snapshot...
2022-04-23 14:21:19 | [train_policy] epoch #352 | Saved
2022-04-23 14:21:19 | [train_policy] epoch #352 | Time 127.19 s
2022-04-23 14:21:19 | [train_policy] epoch #352 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116036
Evaluation/AverageDiscountedReturn          -64.5002
Evaluation/AverageReturn                    -64.5002
Evaluation/CompletionRate                     0
Evaluation/Iteration                        352
Evaluation/MaxReturn                        -34.0908
Evaluation/MinReturn                      -2067.85
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.102
Extras/EpisodeRewardMean                    -62.8992
LinearFeatureBaseline/ExplainedVariance       0.152508
PolicyExecTime                                0.102404
ProcessExecTime                               0.0114925
TotalEnvSteps                            357236
policy/Entropy                                0.0168409
policy/KL                                     0.00788083
policy/KLBefore                               0
policy/LossAfter                             -0.0179978
policy/LossBefore                             7.06774e-09
policy/Perplexity                             1.01698
policy/dLoss                                  0.0179978
---------------------------------------  ----------------
2022-04-23 14:21:19 | [train_policy] epoch #353 | Obtaining samples for iteration 353...
2022-04-23 14:21:19 | [train_policy] epoch #353 | Logging diagnostics...
2022-04-23 14:21:19 | [train_policy] epoch #353 | Optimizing policy...
2022-04-23 14:21:19 | [train_policy] epoch #353 | Computing loss before
2022-04-23 14:21:19 | [train_policy] epoch #353 | Computing KL before
2022-04-23 14:21:19 | [train_policy] epoch #353 | Optimizing
2022-04-23 14:21:19 | [train_policy] epoch #353 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:19 | [train_policy] epoch #353 | computing loss before
2022-04-23 14:21:19 | [train_policy] epoch #353 | computing gradient
2022-04-23 14:21:19 | [train_policy] epoch #353 | gradient computed
2022-04-23 14:21:19 | [train_policy] epoch #353 | computing descent direction
2022-04-23 14:21:19 | [train_policy] epoch #353 | descent direction computed
2022-04-23 14:21:19 | [train_policy] epoch #353 | backtrack iters: 0
2022-04-23 14:21:19 | [train_policy] epoch #353 | optimization finished
2022-04-23 14:21:19 | [train_policy] epoch #353 | Computing KL after
2022-04-23 14:21:19 | [train_policy] epoch #353 | Computing loss after
2022-04-23 14:21:19 | [train_policy] epoch #353 | Fitting baseline...
2022-04-23 14:21:19 | [train_policy] epoch #353 | Saving snapshot...
2022-04-23 14:21:19 | [train_policy] epoch #353 | Saved
2022-04-23 14:21:19 | [train_policy] epoch #353 | Time 127.54 s
2022-04-23 14:21:19 | [train_policy] epoch #353 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118724
Evaluation/AverageDiscountedReturn          -66.5097
Evaluation/AverageReturn                    -66.5097
Evaluation/CompletionRate                     0
Evaluation/Iteration                        353
Evaluation/MaxReturn                        -31.0917
Evaluation/MinReturn                      -2067.97
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.932
Extras/EpisodeRewardMean                    -64.634
LinearFeatureBaseline/ExplainedVariance       0.11747
PolicyExecTime                                0.102468
ProcessExecTime                               0.0119107
TotalEnvSteps                            358248
policy/Entropy                                0.0151982
policy/KL                                     0.00979483
policy/KLBefore                               0
policy/LossAfter                             -0.0109561
policy/LossBefore                            -1.76694e-08
policy/Perplexity                             1.01531
policy/dLoss                                  0.0109561
---------------------------------------  ----------------
2022-04-23 14:21:19 | [train_policy] epoch #354 | Obtaining samples for iteration 354...
2022-04-23 14:21:20 | [train_policy] epoch #354 | Logging diagnostics...
2022-04-23 14:21:20 | [train_policy] epoch #354 | Optimizing policy...
2022-04-23 14:21:20 | [train_policy] epoch #354 | Computing loss before
2022-04-23 14:21:20 | [train_policy] epoch #354 | Computing KL before
2022-04-23 14:21:20 | [train_policy] epoch #354 | Optimizing
2022-04-23 14:21:20 | [train_policy] epoch #354 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:20 | [train_policy] epoch #354 | computing loss before
2022-04-23 14:21:20 | [train_policy] epoch #354 | computing gradient
2022-04-23 14:21:20 | [train_policy] epoch #354 | gradient computed
2022-04-23 14:21:20 | [train_policy] epoch #354 | computing descent direction
2022-04-23 14:21:20 | [train_policy] epoch #354 | descent direction computed
2022-04-23 14:21:20 | [train_policy] epoch #354 | backtrack iters: 1
2022-04-23 14:21:20 | [train_policy] epoch #354 | optimization finished
2022-04-23 14:21:20 | [train_policy] epoch #354 | Computing KL after
2022-04-23 14:21:20 | [train_policy] epoch #354 | Computing loss after
2022-04-23 14:21:20 | [train_policy] epoch #354 | Fitting baseline...
2022-04-23 14:21:20 | [train_policy] epoch #354 | Saving snapshot...
2022-04-23 14:21:20 | [train_policy] epoch #354 | Saved
2022-04-23 14:21:20 | [train_policy] epoch #354 | Time 127.89 s
2022-04-23 14:21:20 | [train_policy] epoch #354 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117176
Evaluation/AverageDiscountedReturn          -65.728
Evaluation/AverageReturn                    -65.728
Evaluation/CompletionRate                     0
Evaluation/Iteration                        354
Evaluation/MaxReturn                        -32.6807
Evaluation/MinReturn                      -2067.11
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.924
Extras/EpisodeRewardMean                    -84.0602
LinearFeatureBaseline/ExplainedVariance      -0.734776
PolicyExecTime                                0.104308
ProcessExecTime                               0.0114648
TotalEnvSteps                            359260
policy/Entropy                                0.0118496
policy/KL                                     0.00646071
policy/KLBefore                               0
policy/LossAfter                             -0.0106168
policy/LossBefore                             8.48129e-09
policy/Perplexity                             1.01192
policy/dLoss                                  0.0106168
---------------------------------------  ----------------
2022-04-23 14:21:20 | [train_policy] epoch #355 | Obtaining samples for iteration 355...
2022-04-23 14:21:20 | [train_policy] epoch #355 | Logging diagnostics...
2022-04-23 14:21:20 | [train_policy] epoch #355 | Optimizing policy...
2022-04-23 14:21:20 | [train_policy] epoch #355 | Computing loss before
2022-04-23 14:21:20 | [train_policy] epoch #355 | Computing KL before
2022-04-23 14:21:20 | [train_policy] epoch #355 | Optimizing
2022-04-23 14:21:20 | [train_policy] epoch #355 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:20 | [train_policy] epoch #355 | computing loss before
2022-04-23 14:21:20 | [train_policy] epoch #355 | computing gradient
2022-04-23 14:21:20 | [train_policy] epoch #355 | gradient computed
2022-04-23 14:21:20 | [train_policy] epoch #355 | computing descent direction
2022-04-23 14:21:20 | [train_policy] epoch #355 | descent direction computed
2022-04-23 14:21:20 | [train_policy] epoch #355 | backtrack iters: 1
2022-04-23 14:21:20 | [train_policy] epoch #355 | optimization finished
2022-04-23 14:21:20 | [train_policy] epoch #355 | Computing KL after
2022-04-23 14:21:20 | [train_policy] epoch #355 | Computing loss after
2022-04-23 14:21:20 | [train_policy] epoch #355 | Fitting baseline...
2022-04-23 14:21:20 | [train_policy] epoch #355 | Saving snapshot...
2022-04-23 14:21:20 | [train_policy] epoch #355 | Saved
2022-04-23 14:21:20 | [train_policy] epoch #355 | Time 128.25 s
2022-04-23 14:21:20 | [train_policy] epoch #355 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.116901
Evaluation/AverageDiscountedReturn          -43.0612
Evaluation/AverageReturn                    -43.0612
Evaluation/CompletionRate                     0
Evaluation/Iteration                        355
Evaluation/MaxReturn                        -33.6523
Evaluation/MinReturn                        -65.2766
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.126
Extras/EpisodeRewardMean                    -42.97
LinearFeatureBaseline/ExplainedVariance     -25.0099
PolicyExecTime                                0.105897
ProcessExecTime                               0.0111654
TotalEnvSteps                            360272
policy/Entropy                               -0.00657535
policy/KL                                     0.00643863
policy/KLBefore                               0
policy/LossAfter                             -0.0215373
policy/LossBefore                             3.20404e-08
policy/Perplexity                             0.993446
policy/dLoss                                  0.0215374
---------------------------------------  ----------------
2022-04-23 14:21:20 | [train_policy] epoch #356 | Obtaining samples for iteration 356...
2022-04-23 14:21:20 | [train_policy] epoch #356 | Logging diagnostics...
2022-04-23 14:21:20 | [train_policy] epoch #356 | Optimizing policy...
2022-04-23 14:21:20 | [train_policy] epoch #356 | Computing loss before
2022-04-23 14:21:20 | [train_policy] epoch #356 | Computing KL before
2022-04-23 14:21:20 | [train_policy] epoch #356 | Optimizing
2022-04-23 14:21:20 | [train_policy] epoch #356 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:20 | [train_policy] epoch #356 | computing loss before
2022-04-23 14:21:20 | [train_policy] epoch #356 | computing gradient
2022-04-23 14:21:20 | [train_policy] epoch #356 | gradient computed
2022-04-23 14:21:20 | [train_policy] epoch #356 | computing descent direction
2022-04-23 14:21:20 | [train_policy] epoch #356 | descent direction computed
2022-04-23 14:21:20 | [train_policy] epoch #356 | backtrack iters: 1
2022-04-23 14:21:20 | [train_policy] epoch #356 | optimization finished
2022-04-23 14:21:20 | [train_policy] epoch #356 | Computing KL after
2022-04-23 14:21:20 | [train_policy] epoch #356 | Computing loss after
2022-04-23 14:21:20 | [train_policy] epoch #356 | Fitting baseline...
2022-04-23 14:21:20 | [train_policy] epoch #356 | Saving snapshot...
2022-04-23 14:21:20 | [train_policy] epoch #356 | Saved
2022-04-23 14:21:20 | [train_policy] epoch #356 | Time 128.61 s
2022-04-23 14:21:20 | [train_policy] epoch #356 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117237
Evaluation/AverageDiscountedReturn          -44.1137
Evaluation/AverageReturn                    -44.1137
Evaluation/CompletionRate                     0
Evaluation/Iteration                        356
Evaluation/MaxReturn                        -33.7398
Evaluation/MinReturn                        -65.2452
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.64217
Extras/EpisodeRewardMean                    -43.8779
LinearFeatureBaseline/ExplainedVariance       0.925255
PolicyExecTime                                0.10629
ProcessExecTime                               0.0115006
TotalEnvSteps                            361284
policy/Entropy                               -0.00435209
policy/KL                                     0.00743424
policy/KLBefore                               0
policy/LossAfter                             -0.012125
policy/LossBefore                            -1.47245e-08
policy/Perplexity                             0.995657
policy/dLoss                                  0.012125
---------------------------------------  ----------------
2022-04-23 14:21:20 | [train_policy] epoch #357 | Obtaining samples for iteration 357...
2022-04-23 14:21:21 | [train_policy] epoch #357 | Logging diagnostics...
2022-04-23 14:21:21 | [train_policy] epoch #357 | Optimizing policy...
2022-04-23 14:21:21 | [train_policy] epoch #357 | Computing loss before
2022-04-23 14:21:21 | [train_policy] epoch #357 | Computing KL before
2022-04-23 14:21:21 | [train_policy] epoch #357 | Optimizing
2022-04-23 14:21:21 | [train_policy] epoch #357 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:21 | [train_policy] epoch #357 | computing loss before
2022-04-23 14:21:21 | [train_policy] epoch #357 | computing gradient
2022-04-23 14:21:21 | [train_policy] epoch #357 | gradient computed
2022-04-23 14:21:21 | [train_policy] epoch #357 | computing descent direction
2022-04-23 14:21:21 | [train_policy] epoch #357 | descent direction computed
2022-04-23 14:21:21 | [train_policy] epoch #357 | backtrack iters: 1
2022-04-23 14:21:21 | [train_policy] epoch #357 | optimization finished
2022-04-23 14:21:21 | [train_policy] epoch #357 | Computing KL after
2022-04-23 14:21:21 | [train_policy] epoch #357 | Computing loss after
2022-04-23 14:21:21 | [train_policy] epoch #357 | Fitting baseline...
2022-04-23 14:21:21 | [train_policy] epoch #357 | Saving snapshot...
2022-04-23 14:21:21 | [train_policy] epoch #357 | Saved
2022-04-23 14:21:21 | [train_policy] epoch #357 | Time 128.97 s
2022-04-23 14:21:21 | [train_policy] epoch #357 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11837
Evaluation/AverageDiscountedReturn          -66.6066
Evaluation/AverageReturn                    -66.6066
Evaluation/CompletionRate                     0
Evaluation/Iteration                        357
Evaluation/MaxReturn                        -30.1675
Evaluation/MinReturn                      -2063.36
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.449
Extras/EpisodeRewardMean                    -65.1639
LinearFeatureBaseline/ExplainedVariance       0.0116253
PolicyExecTime                                0.10945
ProcessExecTime                               0.0116043
TotalEnvSteps                            362296
policy/Entropy                               -0.0191717
policy/KL                                     0.0065617
policy/KLBefore                               0
policy/LossAfter                             -0.0228926
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.981011
policy/dLoss                                  0.0228926
---------------------------------------  ----------------
2022-04-23 14:21:21 | [train_policy] epoch #358 | Obtaining samples for iteration 358...
2022-04-23 14:21:21 | [train_policy] epoch #358 | Logging diagnostics...
2022-04-23 14:21:21 | [train_policy] epoch #358 | Optimizing policy...
2022-04-23 14:21:21 | [train_policy] epoch #358 | Computing loss before
2022-04-23 14:21:21 | [train_policy] epoch #358 | Computing KL before
2022-04-23 14:21:21 | [train_policy] epoch #358 | Optimizing
2022-04-23 14:21:21 | [train_policy] epoch #358 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:21 | [train_policy] epoch #358 | computing loss before
2022-04-23 14:21:21 | [train_policy] epoch #358 | computing gradient
2022-04-23 14:21:21 | [train_policy] epoch #358 | gradient computed
2022-04-23 14:21:21 | [train_policy] epoch #358 | computing descent direction
2022-04-23 14:21:21 | [train_policy] epoch #358 | descent direction computed
2022-04-23 14:21:21 | [train_policy] epoch #358 | backtrack iters: 0
2022-04-23 14:21:21 | [train_policy] epoch #358 | optimization finished
2022-04-23 14:21:21 | [train_policy] epoch #358 | Computing KL after
2022-04-23 14:21:21 | [train_policy] epoch #358 | Computing loss after
2022-04-23 14:21:21 | [train_policy] epoch #358 | Fitting baseline...
2022-04-23 14:21:21 | [train_policy] epoch #358 | Saving snapshot...
2022-04-23 14:21:21 | [train_policy] epoch #358 | Saved
2022-04-23 14:21:21 | [train_policy] epoch #358 | Time 129.31 s
2022-04-23 14:21:21 | [train_policy] epoch #358 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118092
Evaluation/AverageDiscountedReturn          -64.7452
Evaluation/AverageReturn                    -64.7452
Evaluation/CompletionRate                     0
Evaluation/Iteration                        358
Evaluation/MaxReturn                        -30.9803
Evaluation/MinReturn                      -2063.16
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.589
Extras/EpisodeRewardMean                    -62.9221
LinearFeatureBaseline/ExplainedVariance       0.101199
PolicyExecTime                                0.10263
ProcessExecTime                               0.0112398
TotalEnvSteps                            363308
policy/Entropy                               -0.00384355
policy/KL                                     0.0093361
policy/KLBefore                               0
policy/LossAfter                             -0.022844
policy/LossBefore                             9.65925e-09
policy/Perplexity                             0.996164
policy/dLoss                                  0.0228441
---------------------------------------  ----------------
2022-04-23 14:21:21 | [train_policy] epoch #359 | Obtaining samples for iteration 359...
2022-04-23 14:21:21 | [train_policy] epoch #359 | Logging diagnostics...
2022-04-23 14:21:21 | [train_policy] epoch #359 | Optimizing policy...
2022-04-23 14:21:21 | [train_policy] epoch #359 | Computing loss before
2022-04-23 14:21:21 | [train_policy] epoch #359 | Computing KL before
2022-04-23 14:21:21 | [train_policy] epoch #359 | Optimizing
2022-04-23 14:21:21 | [train_policy] epoch #359 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:21 | [train_policy] epoch #359 | computing loss before
2022-04-23 14:21:21 | [train_policy] epoch #359 | computing gradient
2022-04-23 14:21:21 | [train_policy] epoch #359 | gradient computed
2022-04-23 14:21:21 | [train_policy] epoch #359 | computing descent direction
2022-04-23 14:21:21 | [train_policy] epoch #359 | descent direction computed
2022-04-23 14:21:21 | [train_policy] epoch #359 | backtrack iters: 0
2022-04-23 14:21:21 | [train_policy] epoch #359 | optimization finished
2022-04-23 14:21:21 | [train_policy] epoch #359 | Computing KL after
2022-04-23 14:21:21 | [train_policy] epoch #359 | Computing loss after
2022-04-23 14:21:21 | [train_policy] epoch #359 | Fitting baseline...
2022-04-23 14:21:21 | [train_policy] epoch #359 | Saving snapshot...
2022-04-23 14:21:21 | [train_policy] epoch #359 | Saved
2022-04-23 14:21:21 | [train_policy] epoch #359 | Time 129.66 s
2022-04-23 14:21:21 | [train_policy] epoch #359 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11794
Evaluation/AverageDiscountedReturn          -87.5822
Evaluation/AverageReturn                    -87.5822
Evaluation/CompletionRate                     0
Evaluation/Iteration                        359
Evaluation/MaxReturn                        -30.8495
Evaluation/MinReturn                      -2066.58
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.943
Extras/EpisodeRewardMean                   -104.368
LinearFeatureBaseline/ExplainedVariance       0.16513
PolicyExecTime                                0.107366
ProcessExecTime                               0.0111318
TotalEnvSteps                            364320
policy/Entropy                               -0.0133708
policy/KL                                     0.00961592
policy/KLBefore                               0
policy/LossAfter                             -0.0338869
policy/LossBefore                            -2.16744e-08
policy/Perplexity                             0.986718
policy/dLoss                                  0.0338869
---------------------------------------  ----------------
2022-04-23 14:21:21 | [train_policy] epoch #360 | Obtaining samples for iteration 360...
2022-04-23 14:21:22 | [train_policy] epoch #360 | Logging diagnostics...
2022-04-23 14:21:22 | [train_policy] epoch #360 | Optimizing policy...
2022-04-23 14:21:22 | [train_policy] epoch #360 | Computing loss before
2022-04-23 14:21:22 | [train_policy] epoch #360 | Computing KL before
2022-04-23 14:21:22 | [train_policy] epoch #360 | Optimizing
2022-04-23 14:21:22 | [train_policy] epoch #360 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:22 | [train_policy] epoch #360 | computing loss before
2022-04-23 14:21:22 | [train_policy] epoch #360 | computing gradient
2022-04-23 14:21:22 | [train_policy] epoch #360 | gradient computed
2022-04-23 14:21:22 | [train_policy] epoch #360 | computing descent direction
2022-04-23 14:21:22 | [train_policy] epoch #360 | descent direction computed
2022-04-23 14:21:22 | [train_policy] epoch #360 | backtrack iters: 2
2022-04-23 14:21:22 | [train_policy] epoch #360 | optimization finished
2022-04-23 14:21:22 | [train_policy] epoch #360 | Computing KL after
2022-04-23 14:21:22 | [train_policy] epoch #360 | Computing loss after
2022-04-23 14:21:22 | [train_policy] epoch #360 | Fitting baseline...
2022-04-23 14:21:22 | [train_policy] epoch #360 | Saving snapshot...
2022-04-23 14:21:22 | [train_policy] epoch #360 | Saved
2022-04-23 14:21:22 | [train_policy] epoch #360 | Time 130.02 s
2022-04-23 14:21:22 | [train_policy] epoch #360 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117631
Evaluation/AverageDiscountedReturn         -109.142
Evaluation/AverageReturn                   -109.142
Evaluation/CompletionRate                     0
Evaluation/Iteration                        360
Evaluation/MaxReturn                        -30.9884
Evaluation/MinReturn                      -2075.19
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        360.501
Extras/EpisodeRewardMean                   -103.758
LinearFeatureBaseline/ExplainedVariance       0.252342
PolicyExecTime                                0.105093
ProcessExecTime                               0.011091
TotalEnvSteps                            365332
policy/Entropy                               -0.0244701
policy/KL                                     0.00639184
policy/KLBefore                               0
policy/LossAfter                             -0.0133594
policy/LossBefore                            -1.17796e-08
policy/Perplexity                             0.975827
policy/dLoss                                  0.0133594
---------------------------------------  ----------------
2022-04-23 14:21:22 | [train_policy] epoch #361 | Obtaining samples for iteration 361...
2022-04-23 14:21:22 | [train_policy] epoch #361 | Logging diagnostics...
2022-04-23 14:21:22 | [train_policy] epoch #361 | Optimizing policy...
2022-04-23 14:21:22 | [train_policy] epoch #361 | Computing loss before
2022-04-23 14:21:22 | [train_policy] epoch #361 | Computing KL before
2022-04-23 14:21:22 | [train_policy] epoch #361 | Optimizing
2022-04-23 14:21:22 | [train_policy] epoch #361 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:22 | [train_policy] epoch #361 | computing loss before
2022-04-23 14:21:22 | [train_policy] epoch #361 | computing gradient
2022-04-23 14:21:22 | [train_policy] epoch #361 | gradient computed
2022-04-23 14:21:22 | [train_policy] epoch #361 | computing descent direction
2022-04-23 14:21:22 | [train_policy] epoch #361 | descent direction computed
2022-04-23 14:21:22 | [train_policy] epoch #361 | backtrack iters: 1
2022-04-23 14:21:22 | [train_policy] epoch #361 | optimization finished
2022-04-23 14:21:22 | [train_policy] epoch #361 | Computing KL after
2022-04-23 14:21:22 | [train_policy] epoch #361 | Computing loss after
2022-04-23 14:21:22 | [train_policy] epoch #361 | Fitting baseline...
2022-04-23 14:21:22 | [train_policy] epoch #361 | Saving snapshot...
2022-04-23 14:21:22 | [train_policy] epoch #361 | Saved
2022-04-23 14:21:22 | [train_policy] epoch #361 | Time 130.37 s
2022-04-23 14:21:22 | [train_policy] epoch #361 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11631
Evaluation/AverageDiscountedReturn          -42.602
Evaluation/AverageReturn                    -42.602
Evaluation/CompletionRate                     0
Evaluation/Iteration                        361
Evaluation/MaxReturn                        -32.0508
Evaluation/MinReturn                        -58.2725
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.97499
Extras/EpisodeRewardMean                    -42.7787
LinearFeatureBaseline/ExplainedVariance    -198.434
PolicyExecTime                                0.102327
ProcessExecTime                               0.0111659
TotalEnvSteps                            366344
policy/Entropy                               -0.0303724
policy/KL                                     0.006551
policy/KLBefore                               0
policy/LossAfter                             -0.0232335
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.970084
policy/dLoss                                  0.0232335
---------------------------------------  ----------------
2022-04-23 14:21:22 | [train_policy] epoch #362 | Obtaining samples for iteration 362...
2022-04-23 14:21:22 | [train_policy] epoch #362 | Logging diagnostics...
2022-04-23 14:21:22 | [train_policy] epoch #362 | Optimizing policy...
2022-04-23 14:21:22 | [train_policy] epoch #362 | Computing loss before
2022-04-23 14:21:22 | [train_policy] epoch #362 | Computing KL before
2022-04-23 14:21:22 | [train_policy] epoch #362 | Optimizing
2022-04-23 14:21:22 | [train_policy] epoch #362 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:22 | [train_policy] epoch #362 | computing loss before
2022-04-23 14:21:22 | [train_policy] epoch #362 | computing gradient
2022-04-23 14:21:22 | [train_policy] epoch #362 | gradient computed
2022-04-23 14:21:22 | [train_policy] epoch #362 | computing descent direction
2022-04-23 14:21:22 | [train_policy] epoch #362 | descent direction computed
2022-04-23 14:21:22 | [train_policy] epoch #362 | backtrack iters: 1
2022-04-23 14:21:22 | [train_policy] epoch #362 | optimization finished
2022-04-23 14:21:22 | [train_policy] epoch #362 | Computing KL after
2022-04-23 14:21:22 | [train_policy] epoch #362 | Computing loss after
2022-04-23 14:21:22 | [train_policy] epoch #362 | Fitting baseline...
2022-04-23 14:21:23 | [train_policy] epoch #362 | Saving snapshot...
2022-04-23 14:21:23 | [train_policy] epoch #362 | Saved
2022-04-23 14:21:23 | [train_policy] epoch #362 | Time 130.72 s
2022-04-23 14:21:23 | [train_policy] epoch #362 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117982
Evaluation/AverageDiscountedReturn          -43.3387
Evaluation/AverageReturn                    -43.3387
Evaluation/CompletionRate                     0
Evaluation/Iteration                        362
Evaluation/MaxReturn                        -34.1849
Evaluation/MinReturn                        -65.8995
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.76811
Extras/EpisodeRewardMean                    -43.2245
LinearFeatureBaseline/ExplainedVariance       0.922788
PolicyExecTime                                0.104923
ProcessExecTime                               0.0114357
TotalEnvSteps                            367356
policy/Entropy                               -0.0590639
policy/KL                                     0.00693198
policy/KLBefore                               0
policy/LossAfter                             -0.017758
policy/LossBefore                            -2.12032e-09
policy/Perplexity                             0.942647
policy/dLoss                                  0.017758
---------------------------------------  ----------------
2022-04-23 14:21:23 | [train_policy] epoch #363 | Obtaining samples for iteration 363...
2022-04-23 14:21:23 | [train_policy] epoch #363 | Logging diagnostics...
2022-04-23 14:21:23 | [train_policy] epoch #363 | Optimizing policy...
2022-04-23 14:21:23 | [train_policy] epoch #363 | Computing loss before
2022-04-23 14:21:23 | [train_policy] epoch #363 | Computing KL before
2022-04-23 14:21:23 | [train_policy] epoch #363 | Optimizing
2022-04-23 14:21:23 | [train_policy] epoch #363 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:23 | [train_policy] epoch #363 | computing loss before
2022-04-23 14:21:23 | [train_policy] epoch #363 | computing gradient
2022-04-23 14:21:23 | [train_policy] epoch #363 | gradient computed
2022-04-23 14:21:23 | [train_policy] epoch #363 | computing descent direction
2022-04-23 14:21:23 | [train_policy] epoch #363 | descent direction computed
2022-04-23 14:21:23 | [train_policy] epoch #363 | backtrack iters: 1
2022-04-23 14:21:23 | [train_policy] epoch #363 | optimization finished
2022-04-23 14:21:23 | [train_policy] epoch #363 | Computing KL after
2022-04-23 14:21:23 | [train_policy] epoch #363 | Computing loss after
2022-04-23 14:21:23 | [train_policy] epoch #363 | Fitting baseline...
2022-04-23 14:21:23 | [train_policy] epoch #363 | Saving snapshot...
2022-04-23 14:21:23 | [train_policy] epoch #363 | Saved
2022-04-23 14:21:23 | [train_policy] epoch #363 | Time 131.08 s
2022-04-23 14:21:23 | [train_policy] epoch #363 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.118017
Evaluation/AverageDiscountedReturn          -64.2945
Evaluation/AverageReturn                    -64.2945
Evaluation/CompletionRate                     0
Evaluation/Iteration                        363
Evaluation/MaxReturn                        -31.2834
Evaluation/MinReturn                      -2067.05
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.064
Extras/EpisodeRewardMean                    -62.4677
LinearFeatureBaseline/ExplainedVariance       0.011783
PolicyExecTime                                0.109908
ProcessExecTime                               0.0112162
TotalEnvSteps                            368368
policy/Entropy                               -0.0621436
policy/KL                                     0.00639916
policy/KLBefore                               0
policy/LossAfter                             -0.0166727
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.939748
policy/dLoss                                  0.0166727
---------------------------------------  ----------------
2022-04-23 14:21:23 | [train_policy] epoch #364 | Obtaining samples for iteration 364...
2022-04-23 14:21:23 | [train_policy] epoch #364 | Logging diagnostics...
2022-04-23 14:21:23 | [train_policy] epoch #364 | Optimizing policy...
2022-04-23 14:21:23 | [train_policy] epoch #364 | Computing loss before
2022-04-23 14:21:23 | [train_policy] epoch #364 | Computing KL before
2022-04-23 14:21:23 | [train_policy] epoch #364 | Optimizing
2022-04-23 14:21:23 | [train_policy] epoch #364 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:23 | [train_policy] epoch #364 | computing loss before
2022-04-23 14:21:23 | [train_policy] epoch #364 | computing gradient
2022-04-23 14:21:23 | [train_policy] epoch #364 | gradient computed
2022-04-23 14:21:23 | [train_policy] epoch #364 | computing descent direction
2022-04-23 14:21:23 | [train_policy] epoch #364 | descent direction computed
2022-04-23 14:21:23 | [train_policy] epoch #364 | backtrack iters: 0
2022-04-23 14:21:23 | [train_policy] epoch #364 | optimization finished
2022-04-23 14:21:23 | [train_policy] epoch #364 | Computing KL after
2022-04-23 14:21:23 | [train_policy] epoch #364 | Computing loss after
2022-04-23 14:21:23 | [train_policy] epoch #364 | Fitting baseline...
2022-04-23 14:21:23 | [train_policy] epoch #364 | Saving snapshot...
2022-04-23 14:21:23 | [train_policy] epoch #364 | Saved
2022-04-23 14:21:23 | [train_policy] epoch #364 | Time 131.45 s
2022-04-23 14:21:23 | [train_policy] epoch #364 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.12929
Evaluation/AverageDiscountedReturn          -64.9632
Evaluation/AverageReturn                    -64.9632
Evaluation/CompletionRate                     0
Evaluation/Iteration                        364
Evaluation/MaxReturn                        -31.4504
Evaluation/MinReturn                      -2085.89
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.962
Extras/EpisodeRewardMean                    -63.2396
LinearFeatureBaseline/ExplainedVariance       0.12756
PolicyExecTime                                0.111065
ProcessExecTime                               0.012732
TotalEnvSteps                            369380
policy/Entropy                               -0.0555971
policy/KL                                     0.00949447
policy/KLBefore                               0
policy/LossAfter                             -0.0381402
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.94592
policy/dLoss                                  0.0381402
---------------------------------------  ----------------
2022-04-23 14:21:23 | [train_policy] epoch #365 | Obtaining samples for iteration 365...
2022-04-23 14:21:24 | [train_policy] epoch #365 | Logging diagnostics...
2022-04-23 14:21:24 | [train_policy] epoch #365 | Optimizing policy...
2022-04-23 14:21:24 | [train_policy] epoch #365 | Computing loss before
2022-04-23 14:21:24 | [train_policy] epoch #365 | Computing KL before
2022-04-23 14:21:24 | [train_policy] epoch #365 | Optimizing
2022-04-23 14:21:24 | [train_policy] epoch #365 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:24 | [train_policy] epoch #365 | computing loss before
2022-04-23 14:21:24 | [train_policy] epoch #365 | computing gradient
2022-04-23 14:21:24 | [train_policy] epoch #365 | gradient computed
2022-04-23 14:21:24 | [train_policy] epoch #365 | computing descent direction
2022-04-23 14:21:24 | [train_policy] epoch #365 | descent direction computed
2022-04-23 14:21:24 | [train_policy] epoch #365 | backtrack iters: 0
2022-04-23 14:21:24 | [train_policy] epoch #365 | optimization finished
2022-04-23 14:21:24 | [train_policy] epoch #365 | Computing KL after
2022-04-23 14:21:24 | [train_policy] epoch #365 | Computing loss after
2022-04-23 14:21:24 | [train_policy] epoch #365 | Fitting baseline...
2022-04-23 14:21:24 | [train_policy] epoch #365 | Saving snapshot...
2022-04-23 14:21:24 | [train_policy] epoch #365 | Saved
2022-04-23 14:21:24 | [train_policy] epoch #365 | Time 131.81 s
2022-04-23 14:21:24 | [train_policy] epoch #365 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.128098
Evaluation/AverageDiscountedReturn         -109.677
Evaluation/AverageReturn                   -109.677
Evaluation/CompletionRate                     0
Evaluation/Iteration                        365
Evaluation/MaxReturn                        -31.6846
Evaluation/MinReturn                      -2077.66
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        360.028
Extras/EpisodeRewardMean                   -104.246
LinearFeatureBaseline/ExplainedVariance       0.171982
PolicyExecTime                                0.103867
ProcessExecTime                               0.0122252
TotalEnvSteps                            370392
policy/Entropy                               -0.0686591
policy/KL                                     0.00975947
policy/KLBefore                               0
policy/LossAfter                             -0.0198445
policy/LossBefore                             3.53387e-09
policy/Perplexity                             0.933645
policy/dLoss                                  0.0198445
---------------------------------------  ----------------
2022-04-23 14:21:24 | [train_policy] epoch #366 | Obtaining samples for iteration 366...
2022-04-23 14:21:24 | [train_policy] epoch #366 | Logging diagnostics...
2022-04-23 14:21:24 | [train_policy] epoch #366 | Optimizing policy...
2022-04-23 14:21:24 | [train_policy] epoch #366 | Computing loss before
2022-04-23 14:21:24 | [train_policy] epoch #366 | Computing KL before
2022-04-23 14:21:24 | [train_policy] epoch #366 | Optimizing
2022-04-23 14:21:24 | [train_policy] epoch #366 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:24 | [train_policy] epoch #366 | computing loss before
2022-04-23 14:21:24 | [train_policy] epoch #366 | computing gradient
2022-04-23 14:21:24 | [train_policy] epoch #366 | gradient computed
2022-04-23 14:21:24 | [train_policy] epoch #366 | computing descent direction
2022-04-23 14:21:24 | [train_policy] epoch #366 | descent direction computed
2022-04-23 14:21:24 | [train_policy] epoch #366 | backtrack iters: 1
2022-04-23 14:21:24 | [train_policy] epoch #366 | optimization finished
2022-04-23 14:21:24 | [train_policy] epoch #366 | Computing KL after
2022-04-23 14:21:24 | [train_policy] epoch #366 | Computing loss after
2022-04-23 14:21:24 | [train_policy] epoch #366 | Fitting baseline...
2022-04-23 14:21:24 | [train_policy] epoch #366 | Saving snapshot...
2022-04-23 14:21:24 | [train_policy] epoch #366 | Saved
2022-04-23 14:21:24 | [train_policy] epoch #366 | Time 132.17 s
2022-04-23 14:21:24 | [train_policy] epoch #366 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116867
Evaluation/AverageDiscountedReturn          -42.0343
Evaluation/AverageReturn                    -42.0343
Evaluation/CompletionRate                     0
Evaluation/Iteration                        366
Evaluation/MaxReturn                        -30.567
Evaluation/MinReturn                        -66.9159
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.51105
Extras/EpisodeRewardMean                    -62.5182
LinearFeatureBaseline/ExplainedVariance     -95.7445
PolicyExecTime                                0.107123
ProcessExecTime                               0.0114307
TotalEnvSteps                            371404
policy/Entropy                               -0.0791795
policy/KL                                     0.00738699
policy/KLBefore                               0
policy/LossAfter                             -0.0188653
policy/LossBefore                             6.97351e-08
policy/Perplexity                             0.923874
policy/dLoss                                  0.0188654
---------------------------------------  ----------------
2022-04-23 14:21:24 | [train_policy] epoch #367 | Obtaining samples for iteration 367...
2022-04-23 14:21:24 | [train_policy] epoch #367 | Logging diagnostics...
2022-04-23 14:21:24 | [train_policy] epoch #367 | Optimizing policy...
2022-04-23 14:21:24 | [train_policy] epoch #367 | Computing loss before
2022-04-23 14:21:24 | [train_policy] epoch #367 | Computing KL before
2022-04-23 14:21:24 | [train_policy] epoch #367 | Optimizing
2022-04-23 14:21:24 | [train_policy] epoch #367 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:24 | [train_policy] epoch #367 | computing loss before
2022-04-23 14:21:24 | [train_policy] epoch #367 | computing gradient
2022-04-23 14:21:24 | [train_policy] epoch #367 | gradient computed
2022-04-23 14:21:24 | [train_policy] epoch #367 | computing descent direction
2022-04-23 14:21:24 | [train_policy] epoch #367 | descent direction computed
2022-04-23 14:21:24 | [train_policy] epoch #367 | backtrack iters: 1
2022-04-23 14:21:24 | [train_policy] epoch #367 | optimization finished
2022-04-23 14:21:24 | [train_policy] epoch #367 | Computing KL after
2022-04-23 14:21:24 | [train_policy] epoch #367 | Computing loss after
2022-04-23 14:21:24 | [train_policy] epoch #367 | Fitting baseline...
2022-04-23 14:21:24 | [train_policy] epoch #367 | Saving snapshot...
2022-04-23 14:21:24 | [train_policy] epoch #367 | Saved
2022-04-23 14:21:24 | [train_policy] epoch #367 | Time 132.53 s
2022-04-23 14:21:24 | [train_policy] epoch #367 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116811
Evaluation/AverageDiscountedReturn          -42.0354
Evaluation/AverageReturn                    -42.0354
Evaluation/CompletionRate                     0
Evaluation/Iteration                        367
Evaluation/MaxReturn                        -31.4986
Evaluation/MinReturn                        -56.2316
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.6633
Extras/EpisodeRewardMean                    -41.7968
LinearFeatureBaseline/ExplainedVariance       0.939695
PolicyExecTime                                0.109188
ProcessExecTime                               0.0118754
TotalEnvSteps                            372416
policy/Entropy                               -0.115655
policy/KL                                     0.00666854
policy/KLBefore                               0
policy/LossAfter                             -0.0193027
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.890783
policy/dLoss                                  0.0193027
---------------------------------------  ----------------
2022-04-23 14:21:24 | [train_policy] epoch #368 | Obtaining samples for iteration 368...
2022-04-23 14:21:25 | [train_policy] epoch #368 | Logging diagnostics...
2022-04-23 14:21:25 | [train_policy] epoch #368 | Optimizing policy...
2022-04-23 14:21:25 | [train_policy] epoch #368 | Computing loss before
2022-04-23 14:21:25 | [train_policy] epoch #368 | Computing KL before
2022-04-23 14:21:25 | [train_policy] epoch #368 | Optimizing
2022-04-23 14:21:25 | [train_policy] epoch #368 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:25 | [train_policy] epoch #368 | computing loss before
2022-04-23 14:21:25 | [train_policy] epoch #368 | computing gradient
2022-04-23 14:21:25 | [train_policy] epoch #368 | gradient computed
2022-04-23 14:21:25 | [train_policy] epoch #368 | computing descent direction
2022-04-23 14:21:25 | [train_policy] epoch #368 | descent direction computed
2022-04-23 14:21:25 | [train_policy] epoch #368 | backtrack iters: 1
2022-04-23 14:21:25 | [train_policy] epoch #368 | optimization finished
2022-04-23 14:21:25 | [train_policy] epoch #368 | Computing KL after
2022-04-23 14:21:25 | [train_policy] epoch #368 | Computing loss after
2022-04-23 14:21:25 | [train_policy] epoch #368 | Fitting baseline...
2022-04-23 14:21:25 | [train_policy] epoch #368 | Saving snapshot...
2022-04-23 14:21:25 | [train_policy] epoch #368 | Saved
2022-04-23 14:21:25 | [train_policy] epoch #368 | Time 132.87 s
2022-04-23 14:21:25 | [train_policy] epoch #368 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116208
Evaluation/AverageDiscountedReturn          -88.1336
Evaluation/AverageReturn                    -88.1336
Evaluation/CompletionRate                     0
Evaluation/Iteration                        368
Evaluation/MaxReturn                        -31.9691
Evaluation/MinReturn                      -2128.31
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        300.26
Extras/EpisodeRewardMean                    -84.2252
LinearFeatureBaseline/ExplainedVariance       0.00678966
PolicyExecTime                                0.102596
ProcessExecTime                               0.0112207
TotalEnvSteps                            373428
policy/Entropy                               -0.129766
policy/KL                                     0.00663599
policy/KLBefore                               0
policy/LossAfter                             -0.0190639
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.878301
policy/dLoss                                  0.0190639
---------------------------------------  ----------------
2022-04-23 14:21:25 | [train_policy] epoch #369 | Obtaining samples for iteration 369...
2022-04-23 14:21:25 | [train_policy] epoch #369 | Logging diagnostics...
2022-04-23 14:21:25 | [train_policy] epoch #369 | Optimizing policy...
2022-04-23 14:21:25 | [train_policy] epoch #369 | Computing loss before
2022-04-23 14:21:25 | [train_policy] epoch #369 | Computing KL before
2022-04-23 14:21:25 | [train_policy] epoch #369 | Optimizing
2022-04-23 14:21:25 | [train_policy] epoch #369 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:25 | [train_policy] epoch #369 | computing loss before
2022-04-23 14:21:25 | [train_policy] epoch #369 | computing gradient
2022-04-23 14:21:25 | [train_policy] epoch #369 | gradient computed
2022-04-23 14:21:25 | [train_policy] epoch #369 | computing descent direction
2022-04-23 14:21:25 | [train_policy] epoch #369 | descent direction computed
2022-04-23 14:21:25 | [train_policy] epoch #369 | backtrack iters: 0
2022-04-23 14:21:25 | [train_policy] epoch #369 | optimization finished
2022-04-23 14:21:25 | [train_policy] epoch #369 | Computing KL after
2022-04-23 14:21:25 | [train_policy] epoch #369 | Computing loss after
2022-04-23 14:21:25 | [train_policy] epoch #369 | Fitting baseline...
2022-04-23 14:21:25 | [train_policy] epoch #369 | Saving snapshot...
2022-04-23 14:21:25 | [train_policy] epoch #369 | Saved
2022-04-23 14:21:25 | [train_policy] epoch #369 | Time 133.22 s
2022-04-23 14:21:25 | [train_policy] epoch #369 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116214
Evaluation/AverageDiscountedReturn          -42.13
Evaluation/AverageReturn                    -42.13
Evaluation/CompletionRate                     0
Evaluation/Iteration                        369
Evaluation/MaxReturn                        -29.726
Evaluation/MinReturn                        -65.1513
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.99162
Extras/EpisodeRewardMean                    -63.0323
LinearFeatureBaseline/ExplainedVariance     -97.2878
PolicyExecTime                                0.108667
ProcessExecTime                               0.0112724
TotalEnvSteps                            374440
policy/Entropy                               -0.111879
policy/KL                                     0.00889889
policy/KLBefore                               0
policy/LossAfter                             -0.0354053
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.894152
policy/dLoss                                  0.0354053
---------------------------------------  ----------------
2022-04-23 14:21:25 | [train_policy] epoch #370 | Obtaining samples for iteration 370...
2022-04-23 14:21:25 | [train_policy] epoch #370 | Logging diagnostics...
2022-04-23 14:21:25 | [train_policy] epoch #370 | Optimizing policy...
2022-04-23 14:21:25 | [train_policy] epoch #370 | Computing loss before
2022-04-23 14:21:25 | [train_policy] epoch #370 | Computing KL before
2022-04-23 14:21:25 | [train_policy] epoch #370 | Optimizing
2022-04-23 14:21:25 | [train_policy] epoch #370 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:25 | [train_policy] epoch #370 | computing loss before
2022-04-23 14:21:25 | [train_policy] epoch #370 | computing gradient
2022-04-23 14:21:25 | [train_policy] epoch #370 | gradient computed
2022-04-23 14:21:25 | [train_policy] epoch #370 | computing descent direction
2022-04-23 14:21:25 | [train_policy] epoch #370 | descent direction computed
2022-04-23 14:21:25 | [train_policy] epoch #370 | backtrack iters: 0
2022-04-23 14:21:25 | [train_policy] epoch #370 | optimization finished
2022-04-23 14:21:25 | [train_policy] epoch #370 | Computing KL after
2022-04-23 14:21:25 | [train_policy] epoch #370 | Computing loss after
2022-04-23 14:21:25 | [train_policy] epoch #370 | Fitting baseline...
2022-04-23 14:21:25 | [train_policy] epoch #370 | Saving snapshot...
2022-04-23 14:21:25 | [train_policy] epoch #370 | Saved
2022-04-23 14:21:25 | [train_policy] epoch #370 | Time 133.57 s
2022-04-23 14:21:25 | [train_policy] epoch #370 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.115663
Evaluation/AverageDiscountedReturn          -86.8349
Evaluation/AverageReturn                    -86.8349
Evaluation/CompletionRate                     0
Evaluation/Iteration                        370
Evaluation/MaxReturn                        -31.9561
Evaluation/MinReturn                      -2134.31
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        300.432
Extras/EpisodeRewardMean                    -82.9734
LinearFeatureBaseline/ExplainedVariance       0.0111017
PolicyExecTime                                0.10715
ProcessExecTime                               0.01143
TotalEnvSteps                            375452
policy/Entropy                               -0.106397
policy/KL                                     0.00993374
policy/KLBefore                               0
policy/LossAfter                             -0.0228164
policy/LossBefore                             1.5549e-08
policy/Perplexity                             0.899068
policy/dLoss                                  0.0228165
---------------------------------------  ---------------
2022-04-23 14:21:25 | [train_policy] epoch #371 | Obtaining samples for iteration 371...
2022-04-23 14:21:26 | [train_policy] epoch #371 | Logging diagnostics...
2022-04-23 14:21:26 | [train_policy] epoch #371 | Optimizing policy...
2022-04-23 14:21:26 | [train_policy] epoch #371 | Computing loss before
2022-04-23 14:21:26 | [train_policy] epoch #371 | Computing KL before
2022-04-23 14:21:26 | [train_policy] epoch #371 | Optimizing
2022-04-23 14:21:26 | [train_policy] epoch #371 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:26 | [train_policy] epoch #371 | computing loss before
2022-04-23 14:21:26 | [train_policy] epoch #371 | computing gradient
2022-04-23 14:21:26 | [train_policy] epoch #371 | gradient computed
2022-04-23 14:21:26 | [train_policy] epoch #371 | computing descent direction
2022-04-23 14:21:26 | [train_policy] epoch #371 | descent direction computed
2022-04-23 14:21:26 | [train_policy] epoch #371 | backtrack iters: 1
2022-04-23 14:21:26 | [train_policy] epoch #371 | optimization finished
2022-04-23 14:21:26 | [train_policy] epoch #371 | Computing KL after
2022-04-23 14:21:26 | [train_policy] epoch #371 | Computing loss after
2022-04-23 14:21:26 | [train_policy] epoch #371 | Fitting baseline...
2022-04-23 14:21:26 | [train_policy] epoch #371 | Saving snapshot...
2022-04-23 14:21:26 | [train_policy] epoch #371 | Saved
2022-04-23 14:21:26 | [train_policy] epoch #371 | Time 133.92 s
2022-04-23 14:21:26 | [train_policy] epoch #371 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116219
Evaluation/AverageDiscountedReturn          -87.7089
Evaluation/AverageReturn                    -87.7089
Evaluation/CompletionRate                     0
Evaluation/Iteration                        371
Evaluation/MaxReturn                        -32.7703
Evaluation/MinReturn                      -2092.41
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        298.032
Extras/EpisodeRewardMean                    -84.0855
LinearFeatureBaseline/ExplainedVariance       0.217998
PolicyExecTime                                0.0981338
ProcessExecTime                               0.0114069
TotalEnvSteps                            376464
policy/Entropy                               -0.1064
policy/KL                                     0.00651903
policy/KLBefore                               0
policy/LossAfter                             -0.0249898
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.899065
policy/dLoss                                  0.0249898
---------------------------------------  ----------------
2022-04-23 14:21:26 | [train_policy] epoch #372 | Obtaining samples for iteration 372...
2022-04-23 14:21:26 | [train_policy] epoch #372 | Logging diagnostics...
2022-04-23 14:21:26 | [train_policy] epoch #372 | Optimizing policy...
2022-04-23 14:21:26 | [train_policy] epoch #372 | Computing loss before
2022-04-23 14:21:26 | [train_policy] epoch #372 | Computing KL before
2022-04-23 14:21:26 | [train_policy] epoch #372 | Optimizing
2022-04-23 14:21:26 | [train_policy] epoch #372 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:26 | [train_policy] epoch #372 | computing loss before
2022-04-23 14:21:26 | [train_policy] epoch #372 | computing gradient
2022-04-23 14:21:26 | [train_policy] epoch #372 | gradient computed
2022-04-23 14:21:26 | [train_policy] epoch #372 | computing descent direction
2022-04-23 14:21:26 | [train_policy] epoch #372 | descent direction computed
2022-04-23 14:21:26 | [train_policy] epoch #372 | backtrack iters: 0
2022-04-23 14:21:26 | [train_policy] epoch #372 | optimization finished
2022-04-23 14:21:26 | [train_policy] epoch #372 | Computing KL after
2022-04-23 14:21:26 | [train_policy] epoch #372 | Computing loss after
2022-04-23 14:21:26 | [train_policy] epoch #372 | Fitting baseline...
2022-04-23 14:21:26 | [train_policy] epoch #372 | Saving snapshot...
2022-04-23 14:21:26 | [train_policy] epoch #372 | Saved
2022-04-23 14:21:26 | [train_policy] epoch #372 | Time 134.25 s
2022-04-23 14:21:26 | [train_policy] epoch #372 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116225
Evaluation/AverageDiscountedReturn          -64.8464
Evaluation/AverageReturn                    -64.8464
Evaluation/CompletionRate                     0
Evaluation/Iteration                        372
Evaluation/MaxReturn                        -31.1179
Evaluation/MinReturn                      -2064.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.745
Extras/EpisodeRewardMean                    -83.3977
LinearFeatureBaseline/ExplainedVariance       0.0191388
PolicyExecTime                                0.0931182
ProcessExecTime                               0.0110915
TotalEnvSteps                            377476
policy/Entropy                               -0.096627
policy/KL                                     0.00816835
policy/KLBefore                               0
policy/LossAfter                             -0.0341752
policy/LossBefore                             6.71436e-09
policy/Perplexity                             0.907895
policy/dLoss                                  0.0341752
---------------------------------------  ----------------
2022-04-23 14:21:26 | [train_policy] epoch #373 | Obtaining samples for iteration 373...
2022-04-23 14:21:26 | [train_policy] epoch #373 | Logging diagnostics...
2022-04-23 14:21:26 | [train_policy] epoch #373 | Optimizing policy...
2022-04-23 14:21:26 | [train_policy] epoch #373 | Computing loss before
2022-04-23 14:21:26 | [train_policy] epoch #373 | Computing KL before
2022-04-23 14:21:26 | [train_policy] epoch #373 | Optimizing
2022-04-23 14:21:26 | [train_policy] epoch #373 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:26 | [train_policy] epoch #373 | computing loss before
2022-04-23 14:21:26 | [train_policy] epoch #373 | computing gradient
2022-04-23 14:21:26 | [train_policy] epoch #373 | gradient computed
2022-04-23 14:21:26 | [train_policy] epoch #373 | computing descent direction
2022-04-23 14:21:26 | [train_policy] epoch #373 | descent direction computed
2022-04-23 14:21:26 | [train_policy] epoch #373 | backtrack iters: 0
2022-04-23 14:21:26 | [train_policy] epoch #373 | optimization finished
2022-04-23 14:21:26 | [train_policy] epoch #373 | Computing KL after
2022-04-23 14:21:26 | [train_policy] epoch #373 | Computing loss after
2022-04-23 14:21:26 | [train_policy] epoch #373 | Fitting baseline...
2022-04-23 14:21:26 | [train_policy] epoch #373 | Saving snapshot...
2022-04-23 14:21:26 | [train_policy] epoch #373 | Saved
2022-04-23 14:21:26 | [train_policy] epoch #373 | Time 134.59 s
2022-04-23 14:21:26 | [train_policy] epoch #373 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116401
Evaluation/AverageDiscountedReturn          -42.1589
Evaluation/AverageReturn                    -42.1589
Evaluation/CompletionRate                     0
Evaluation/Iteration                        373
Evaluation/MaxReturn                        -30.3617
Evaluation/MinReturn                        -76.3006
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.0291
Extras/EpisodeRewardMean                    -42.2391
LinearFeatureBaseline/ExplainedVariance     -30.0298
PolicyExecTime                                0.0920982
ProcessExecTime                               0.0110092
TotalEnvSteps                            378488
policy/Entropy                               -0.0939376
policy/KL                                     0.00968911
policy/KLBefore                               0
policy/LossAfter                             -0.0375651
policy/LossBefore                             2.07321e-08
policy/Perplexity                             0.91034
policy/dLoss                                  0.0375651
---------------------------------------  ----------------
2022-04-23 14:21:26 | [train_policy] epoch #374 | Obtaining samples for iteration 374...
2022-04-23 14:21:27 | [train_policy] epoch #374 | Logging diagnostics...
2022-04-23 14:21:27 | [train_policy] epoch #374 | Optimizing policy...
2022-04-23 14:21:27 | [train_policy] epoch #374 | Computing loss before
2022-04-23 14:21:27 | [train_policy] epoch #374 | Computing KL before
2022-04-23 14:21:27 | [train_policy] epoch #374 | Optimizing
2022-04-23 14:21:27 | [train_policy] epoch #374 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:27 | [train_policy] epoch #374 | computing loss before
2022-04-23 14:21:27 | [train_policy] epoch #374 | computing gradient
2022-04-23 14:21:27 | [train_policy] epoch #374 | gradient computed
2022-04-23 14:21:27 | [train_policy] epoch #374 | computing descent direction
2022-04-23 14:21:27 | [train_policy] epoch #374 | descent direction computed
2022-04-23 14:21:27 | [train_policy] epoch #374 | backtrack iters: 1
2022-04-23 14:21:27 | [train_policy] epoch #374 | optimization finished
2022-04-23 14:21:27 | [train_policy] epoch #374 | Computing KL after
2022-04-23 14:21:27 | [train_policy] epoch #374 | Computing loss after
2022-04-23 14:21:27 | [train_policy] epoch #374 | Fitting baseline...
2022-04-23 14:21:27 | [train_policy] epoch #374 | Saving snapshot...
2022-04-23 14:21:27 | [train_policy] epoch #374 | Saved
2022-04-23 14:21:27 | [train_policy] epoch #374 | Time 134.94 s
2022-04-23 14:21:27 | [train_policy] epoch #374 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117526
Evaluation/AverageDiscountedReturn          -87.7427
Evaluation/AverageReturn                    -87.7427
Evaluation/CompletionRate                     0
Evaluation/Iteration                        374
Evaluation/MaxReturn                        -31.3348
Evaluation/MinReturn                      -2065.82
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.821
Extras/EpisodeRewardMean                    -84.2243
LinearFeatureBaseline/ExplainedVariance       0.0122767
PolicyExecTime                                0.101926
ProcessExecTime                               0.0115969
TotalEnvSteps                            379500
policy/Entropy                               -0.100167
policy/KL                                     0.00795072
policy/KLBefore                               0
policy/LossAfter                             -0.0164578
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.904687
policy/dLoss                                  0.0164578
---------------------------------------  ----------------
2022-04-23 14:21:27 | [train_policy] epoch #375 | Obtaining samples for iteration 375...
2022-04-23 14:21:27 | [train_policy] epoch #375 | Logging diagnostics...
2022-04-23 14:21:27 | [train_policy] epoch #375 | Optimizing policy...
2022-04-23 14:21:27 | [train_policy] epoch #375 | Computing loss before
2022-04-23 14:21:27 | [train_policy] epoch #375 | Computing KL before
2022-04-23 14:21:27 | [train_policy] epoch #375 | Optimizing
2022-04-23 14:21:27 | [train_policy] epoch #375 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:27 | [train_policy] epoch #375 | computing loss before
2022-04-23 14:21:27 | [train_policy] epoch #375 | computing gradient
2022-04-23 14:21:27 | [train_policy] epoch #375 | gradient computed
2022-04-23 14:21:27 | [train_policy] epoch #375 | computing descent direction
2022-04-23 14:21:27 | [train_policy] epoch #375 | descent direction computed
2022-04-23 14:21:27 | [train_policy] epoch #375 | backtrack iters: 1
2022-04-23 14:21:27 | [train_policy] epoch #375 | optimization finished
2022-04-23 14:21:27 | [train_policy] epoch #375 | Computing KL after
2022-04-23 14:21:27 | [train_policy] epoch #375 | Computing loss after
2022-04-23 14:21:27 | [train_policy] epoch #375 | Fitting baseline...
2022-04-23 14:21:27 | [train_policy] epoch #375 | Saving snapshot...
2022-04-23 14:21:27 | [train_policy] epoch #375 | Saved
2022-04-23 14:21:27 | [train_policy] epoch #375 | Time 135.29 s
2022-04-23 14:21:27 | [train_policy] epoch #375 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117749
Evaluation/AverageDiscountedReturn          -65.2377
Evaluation/AverageReturn                    -65.2377
Evaluation/CompletionRate                     0
Evaluation/Iteration                        375
Evaluation/MaxReturn                        -31.3506
Evaluation/MinReturn                      -2064.77
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.725
Extras/EpisodeRewardMean                    -63.5787
LinearFeatureBaseline/ExplainedVariance       0.10511
PolicyExecTime                                0.100697
ProcessExecTime                               0.0112581
TotalEnvSteps                            380512
policy/Entropy                               -0.105137
policy/KL                                     0.00755053
policy/KLBefore                               0
policy/LossAfter                             -0.0158388
policy/LossBefore                            -1.20152e-08
policy/Perplexity                             0.900202
policy/dLoss                                  0.0158388
---------------------------------------  ----------------
2022-04-23 14:21:27 | [train_policy] epoch #376 | Obtaining samples for iteration 376...
2022-04-23 14:21:27 | [train_policy] epoch #376 | Logging diagnostics...
2022-04-23 14:21:27 | [train_policy] epoch #376 | Optimizing policy...
2022-04-23 14:21:27 | [train_policy] epoch #376 | Computing loss before
2022-04-23 14:21:27 | [train_policy] epoch #376 | Computing KL before
2022-04-23 14:21:27 | [train_policy] epoch #376 | Optimizing
2022-04-23 14:21:27 | [train_policy] epoch #376 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:27 | [train_policy] epoch #376 | computing loss before
2022-04-23 14:21:27 | [train_policy] epoch #376 | computing gradient
2022-04-23 14:21:27 | [train_policy] epoch #376 | gradient computed
2022-04-23 14:21:27 | [train_policy] epoch #376 | computing descent direction
2022-04-23 14:21:27 | [train_policy] epoch #376 | descent direction computed
2022-04-23 14:21:27 | [train_policy] epoch #376 | backtrack iters: 1
2022-04-23 14:21:27 | [train_policy] epoch #376 | optimization finished
2022-04-23 14:21:27 | [train_policy] epoch #376 | Computing KL after
2022-04-23 14:21:27 | [train_policy] epoch #376 | Computing loss after
2022-04-23 14:21:27 | [train_policy] epoch #376 | Fitting baseline...
2022-04-23 14:21:27 | [train_policy] epoch #376 | Saving snapshot...
2022-04-23 14:21:27 | [train_policy] epoch #376 | Saved
2022-04-23 14:21:27 | [train_policy] epoch #376 | Time 135.65 s
2022-04-23 14:21:27 | [train_policy] epoch #376 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.118908
Evaluation/AverageDiscountedReturn          -63.5066
Evaluation/AverageReturn                    -63.5066
Evaluation/CompletionRate                     0
Evaluation/Iteration                        376
Evaluation/MaxReturn                        -31.3754
Evaluation/MinReturn                      -2095.09
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        213.031
Extras/EpisodeRewardMean                    -62.0436
LinearFeatureBaseline/ExplainedVariance       0.161553
PolicyExecTime                                0.111031
ProcessExecTime                               0.0116467
TotalEnvSteps                            381524
policy/Entropy                               -0.110066
policy/KL                                     0.00643011
policy/KLBefore                               0
policy/LossAfter                             -0.0262752
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.895775
policy/dLoss                                  0.0262752
---------------------------------------  ----------------
2022-04-23 14:21:27 | [train_policy] epoch #377 | Obtaining samples for iteration 377...
2022-04-23 14:21:28 | [train_policy] epoch #377 | Logging diagnostics...
2022-04-23 14:21:28 | [train_policy] epoch #377 | Optimizing policy...
2022-04-23 14:21:28 | [train_policy] epoch #377 | Computing loss before
2022-04-23 14:21:28 | [train_policy] epoch #377 | Computing KL before
2022-04-23 14:21:28 | [train_policy] epoch #377 | Optimizing
2022-04-23 14:21:28 | [train_policy] epoch #377 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:28 | [train_policy] epoch #377 | computing loss before
2022-04-23 14:21:28 | [train_policy] epoch #377 | computing gradient
2022-04-23 14:21:28 | [train_policy] epoch #377 | gradient computed
2022-04-23 14:21:28 | [train_policy] epoch #377 | computing descent direction
2022-04-23 14:21:28 | [train_policy] epoch #377 | descent direction computed
2022-04-23 14:21:28 | [train_policy] epoch #377 | backtrack iters: 0
2022-04-23 14:21:28 | [train_policy] epoch #377 | optimization finished
2022-04-23 14:21:28 | [train_policy] epoch #377 | Computing KL after
2022-04-23 14:21:28 | [train_policy] epoch #377 | Computing loss after
2022-04-23 14:21:28 | [train_policy] epoch #377 | Fitting baseline...
2022-04-23 14:21:28 | [train_policy] epoch #377 | Saving snapshot...
2022-04-23 14:21:28 | [train_policy] epoch #377 | Saved
2022-04-23 14:21:28 | [train_policy] epoch #377 | Time 136.01 s
2022-04-23 14:21:28 | [train_policy] epoch #377 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118712
Evaluation/AverageDiscountedReturn          -65.9275
Evaluation/AverageReturn                    -65.9275
Evaluation/CompletionRate                     0
Evaluation/Iteration                        377
Evaluation/MaxReturn                        -31.2975
Evaluation/MinReturn                      -2063.43
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.521
Extras/EpisodeRewardMean                    -63.9215
LinearFeatureBaseline/ExplainedVariance       0.0235123
PolicyExecTime                                0.108165
ProcessExecTime                               0.0117772
TotalEnvSteps                            382536
policy/Entropy                               -0.118421
policy/KL                                     0.00964387
policy/KLBefore                               0
policy/LossAfter                             -0.0270794
policy/LossBefore                             9.89484e-09
policy/Perplexity                             0.888322
policy/dLoss                                  0.0270794
---------------------------------------  ----------------
2022-04-23 14:21:28 | [train_policy] epoch #378 | Obtaining samples for iteration 378...
2022-04-23 14:21:28 | [train_policy] epoch #378 | Logging diagnostics...
2022-04-23 14:21:28 | [train_policy] epoch #378 | Optimizing policy...
2022-04-23 14:21:28 | [train_policy] epoch #378 | Computing loss before
2022-04-23 14:21:28 | [train_policy] epoch #378 | Computing KL before
2022-04-23 14:21:28 | [train_policy] epoch #378 | Optimizing
2022-04-23 14:21:28 | [train_policy] epoch #378 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:28 | [train_policy] epoch #378 | computing loss before
2022-04-23 14:21:28 | [train_policy] epoch #378 | computing gradient
2022-04-23 14:21:28 | [train_policy] epoch #378 | gradient computed
2022-04-23 14:21:28 | [train_policy] epoch #378 | computing descent direction
2022-04-23 14:21:28 | [train_policy] epoch #378 | descent direction computed
2022-04-23 14:21:28 | [train_policy] epoch #378 | backtrack iters: 1
2022-04-23 14:21:28 | [train_policy] epoch #378 | optimization finished
2022-04-23 14:21:28 | [train_policy] epoch #378 | Computing KL after
2022-04-23 14:21:28 | [train_policy] epoch #378 | Computing loss after
2022-04-23 14:21:28 | [train_policy] epoch #378 | Fitting baseline...
2022-04-23 14:21:28 | [train_policy] epoch #378 | Saving snapshot...
2022-04-23 14:21:28 | [train_policy] epoch #378 | Saved
2022-04-23 14:21:28 | [train_policy] epoch #378 | Time 136.37 s
2022-04-23 14:21:28 | [train_policy] epoch #378 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117416
Evaluation/AverageDiscountedReturn          -42.3663
Evaluation/AverageReturn                    -42.3663
Evaluation/CompletionRate                     0
Evaluation/Iteration                        378
Evaluation/MaxReturn                        -31.7173
Evaluation/MinReturn                        -65.3235
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.34998
Extras/EpisodeRewardMean                    -42.3725
LinearFeatureBaseline/ExplainedVariance     -20.4644
PolicyExecTime                                0.105208
ProcessExecTime                               0.0120299
TotalEnvSteps                            383548
policy/Entropy                               -0.146528
policy/KL                                     0.00706063
policy/KLBefore                               0
policy/LossAfter                             -0.0107785
policy/LossBefore                            -2.21456e-08
policy/Perplexity                             0.863701
policy/dLoss                                  0.0107784
---------------------------------------  ----------------
2022-04-23 14:21:28 | [train_policy] epoch #379 | Obtaining samples for iteration 379...
2022-04-23 14:21:28 | [train_policy] epoch #379 | Logging diagnostics...
2022-04-23 14:21:28 | [train_policy] epoch #379 | Optimizing policy...
2022-04-23 14:21:28 | [train_policy] epoch #379 | Computing loss before
2022-04-23 14:21:28 | [train_policy] epoch #379 | Computing KL before
2022-04-23 14:21:28 | [train_policy] epoch #379 | Optimizing
2022-04-23 14:21:28 | [train_policy] epoch #379 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:28 | [train_policy] epoch #379 | computing loss before
2022-04-23 14:21:28 | [train_policy] epoch #379 | computing gradient
2022-04-23 14:21:28 | [train_policy] epoch #379 | gradient computed
2022-04-23 14:21:28 | [train_policy] epoch #379 | computing descent direction
2022-04-23 14:21:28 | [train_policy] epoch #379 | descent direction computed
2022-04-23 14:21:28 | [train_policy] epoch #379 | backtrack iters: 1
2022-04-23 14:21:28 | [train_policy] epoch #379 | optimization finished
2022-04-23 14:21:28 | [train_policy] epoch #379 | Computing KL after
2022-04-23 14:21:28 | [train_policy] epoch #379 | Computing loss after
2022-04-23 14:21:29 | [train_policy] epoch #379 | Fitting baseline...
2022-04-23 14:21:29 | [train_policy] epoch #379 | Saving snapshot...
2022-04-23 14:21:29 | [train_policy] epoch #379 | Saved
2022-04-23 14:21:29 | [train_policy] epoch #379 | Time 136.73 s
2022-04-23 14:21:29 | [train_policy] epoch #379 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.117824
Evaluation/AverageDiscountedReturn          -89.1968
Evaluation/AverageReturn                    -89.1968
Evaluation/CompletionRate                     0
Evaluation/Iteration                        379
Evaluation/MaxReturn                        -34.5926
Evaluation/MinReturn                      -2067.54
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.931
Extras/EpisodeRewardMean                    -85.5706
LinearFeatureBaseline/ExplainedVariance       0.00888585
PolicyExecTime                                0.110188
ProcessExecTime                               0.0112545
TotalEnvSteps                            384560
policy/Entropy                               -0.143658
policy/KL                                     0.00693876
policy/KLBefore                               0
policy/LossAfter                             -0.0200324
policy/LossBefore                             1.20152e-08
policy/Perplexity                             0.866184
policy/dLoss                                  0.0200324
---------------------------------------  ----------------
2022-04-23 14:21:29 | [train_policy] epoch #380 | Obtaining samples for iteration 380...
2022-04-23 14:21:29 | [train_policy] epoch #380 | Logging diagnostics...
2022-04-23 14:21:29 | [train_policy] epoch #380 | Optimizing policy...
2022-04-23 14:21:29 | [train_policy] epoch #380 | Computing loss before
2022-04-23 14:21:29 | [train_policy] epoch #380 | Computing KL before
2022-04-23 14:21:29 | [train_policy] epoch #380 | Optimizing
2022-04-23 14:21:29 | [train_policy] epoch #380 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:29 | [train_policy] epoch #380 | computing loss before
2022-04-23 14:21:29 | [train_policy] epoch #380 | computing gradient
2022-04-23 14:21:29 | [train_policy] epoch #380 | gradient computed
2022-04-23 14:21:29 | [train_policy] epoch #380 | computing descent direction
2022-04-23 14:21:29 | [train_policy] epoch #380 | descent direction computed
2022-04-23 14:21:29 | [train_policy] epoch #380 | backtrack iters: 1
2022-04-23 14:21:29 | [train_policy] epoch #380 | optimization finished
2022-04-23 14:21:29 | [train_policy] epoch #380 | Computing KL after
2022-04-23 14:21:29 | [train_policy] epoch #380 | Computing loss after
2022-04-23 14:21:29 | [train_policy] epoch #380 | Fitting baseline...
2022-04-23 14:21:29 | [train_policy] epoch #380 | Saving snapshot...
2022-04-23 14:21:29 | [train_policy] epoch #380 | Saved
2022-04-23 14:21:29 | [train_policy] epoch #380 | Time 137.08 s
2022-04-23 14:21:29 | [train_policy] epoch #380 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116816
Evaluation/AverageDiscountedReturn          -42.9504
Evaluation/AverageReturn                    -42.9504
Evaluation/CompletionRate                     0
Evaluation/Iteration                        380
Evaluation/MaxReturn                        -30.9686
Evaluation/MinReturn                        -64.7352
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.60025
Extras/EpisodeRewardMean                    -42.9081
LinearFeatureBaseline/ExplainedVariance     -96.553
PolicyExecTime                                0.106459
ProcessExecTime                               0.0112746
TotalEnvSteps                            385572
policy/Entropy                               -0.151342
policy/KL                                     0.0067586
policy/KLBefore                               0
policy/LossAfter                             -0.0315629
policy/LossBefore                            -3.62811e-08
policy/Perplexity                             0.859554
policy/dLoss                                  0.0315628
---------------------------------------  ----------------
2022-04-23 14:21:29 | [train_policy] epoch #381 | Obtaining samples for iteration 381...
2022-04-23 14:21:29 | [train_policy] epoch #381 | Logging diagnostics...
2022-04-23 14:21:29 | [train_policy] epoch #381 | Optimizing policy...
2022-04-23 14:21:29 | [train_policy] epoch #381 | Computing loss before
2022-04-23 14:21:29 | [train_policy] epoch #381 | Computing KL before
2022-04-23 14:21:29 | [train_policy] epoch #381 | Optimizing
2022-04-23 14:21:29 | [train_policy] epoch #381 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:29 | [train_policy] epoch #381 | computing loss before
2022-04-23 14:21:29 | [train_policy] epoch #381 | computing gradient
2022-04-23 14:21:29 | [train_policy] epoch #381 | gradient computed
2022-04-23 14:21:29 | [train_policy] epoch #381 | computing descent direction
2022-04-23 14:21:29 | [train_policy] epoch #381 | descent direction computed
2022-04-23 14:21:29 | [train_policy] epoch #381 | backtrack iters: 1
2022-04-23 14:21:29 | [train_policy] epoch #381 | optimization finished
2022-04-23 14:21:29 | [train_policy] epoch #381 | Computing KL after
2022-04-23 14:21:29 | [train_policy] epoch #381 | Computing loss after
2022-04-23 14:21:29 | [train_policy] epoch #381 | Fitting baseline...
2022-04-23 14:21:29 | [train_policy] epoch #381 | Saving snapshot...
2022-04-23 14:21:29 | [train_policy] epoch #381 | Saved
2022-04-23 14:21:29 | [train_policy] epoch #381 | Time 137.42 s
2022-04-23 14:21:29 | [train_policy] epoch #381 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116391
Evaluation/AverageDiscountedReturn          -86.9241
Evaluation/AverageReturn                    -86.9241
Evaluation/CompletionRate                     0
Evaluation/Iteration                        381
Evaluation/MaxReturn                        -32.5323
Evaluation/MinReturn                      -2090.18
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        297.295
Extras/EpisodeRewardMean                    -83.2555
LinearFeatureBaseline/ExplainedVariance       0.00980275
PolicyExecTime                                0.0985227
ProcessExecTime                               0.0112317
TotalEnvSteps                            386584
policy/Entropy                               -0.15007
policy/KL                                     0.00690425
policy/KLBefore                               0
policy/LossAfter                             -0.0307802
policy/LossBefore                            -1.62558e-08
policy/Perplexity                             0.860647
policy/dLoss                                  0.0307802
---------------------------------------  ----------------
2022-04-23 14:21:29 | [train_policy] epoch #382 | Obtaining samples for iteration 382...
2022-04-23 14:21:29 | [train_policy] epoch #382 | Logging diagnostics...
2022-04-23 14:21:29 | [train_policy] epoch #382 | Optimizing policy...
2022-04-23 14:21:29 | [train_policy] epoch #382 | Computing loss before
2022-04-23 14:21:29 | [train_policy] epoch #382 | Computing KL before
2022-04-23 14:21:29 | [train_policy] epoch #382 | Optimizing
2022-04-23 14:21:29 | [train_policy] epoch #382 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:29 | [train_policy] epoch #382 | computing loss before
2022-04-23 14:21:29 | [train_policy] epoch #382 | computing gradient
2022-04-23 14:21:29 | [train_policy] epoch #382 | gradient computed
2022-04-23 14:21:29 | [train_policy] epoch #382 | computing descent direction
2022-04-23 14:21:30 | [train_policy] epoch #382 | descent direction computed
2022-04-23 14:21:30 | [train_policy] epoch #382 | backtrack iters: 1
2022-04-23 14:21:30 | [train_policy] epoch #382 | optimization finished
2022-04-23 14:21:30 | [train_policy] epoch #382 | Computing KL after
2022-04-23 14:21:30 | [train_policy] epoch #382 | Computing loss after
2022-04-23 14:21:30 | [train_policy] epoch #382 | Fitting baseline...
2022-04-23 14:21:30 | [train_policy] epoch #382 | Saving snapshot...
2022-04-23 14:21:30 | [train_policy] epoch #382 | Saved
2022-04-23 14:21:30 | [train_policy] epoch #382 | Time 137.78 s
2022-04-23 14:21:30 | [train_policy] epoch #382 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116464
Evaluation/AverageDiscountedReturn          -65.2995
Evaluation/AverageReturn                    -65.2995
Evaluation/CompletionRate                     0
Evaluation/Iteration                        382
Evaluation/MaxReturn                        -30.7787
Evaluation/MinReturn                      -2067.46
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.985
Extras/EpisodeRewardMean                    -63.34
LinearFeatureBaseline/ExplainedVariance       0.0885502
PolicyExecTime                                0.10179
ProcessExecTime                               0.0115623
TotalEnvSteps                            387596
policy/Entropy                               -0.153953
policy/KL                                     0.00719226
policy/KLBefore                               0
policy/LossAfter                             -0.0161647
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.857312
policy/dLoss                                  0.0161648
---------------------------------------  ----------------
2022-04-23 14:21:30 | [train_policy] epoch #383 | Obtaining samples for iteration 383...
2022-04-23 14:21:30 | [train_policy] epoch #383 | Logging diagnostics...
2022-04-23 14:21:30 | [train_policy] epoch #383 | Optimizing policy...
2022-04-23 14:21:30 | [train_policy] epoch #383 | Computing loss before
2022-04-23 14:21:30 | [train_policy] epoch #383 | Computing KL before
2022-04-23 14:21:30 | [train_policy] epoch #383 | Optimizing
2022-04-23 14:21:30 | [train_policy] epoch #383 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:30 | [train_policy] epoch #383 | computing loss before
2022-04-23 14:21:30 | [train_policy] epoch #383 | computing gradient
2022-04-23 14:21:30 | [train_policy] epoch #383 | gradient computed
2022-04-23 14:21:30 | [train_policy] epoch #383 | computing descent direction
2022-04-23 14:21:30 | [train_policy] epoch #383 | descent direction computed
2022-04-23 14:21:30 | [train_policy] epoch #383 | backtrack iters: 0
2022-04-23 14:21:30 | [train_policy] epoch #383 | optimization finished
2022-04-23 14:21:30 | [train_policy] epoch #383 | Computing KL after
2022-04-23 14:21:30 | [train_policy] epoch #383 | Computing loss after
2022-04-23 14:21:30 | [train_policy] epoch #383 | Fitting baseline...
2022-04-23 14:21:30 | [train_policy] epoch #383 | Saving snapshot...
2022-04-23 14:21:30 | [train_policy] epoch #383 | Saved
2022-04-23 14:21:30 | [train_policy] epoch #383 | Time 138.13 s
2022-04-23 14:21:30 | [train_policy] epoch #383 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118446
Evaluation/AverageDiscountedReturn          -42.0816
Evaluation/AverageReturn                    -42.0816
Evaluation/CompletionRate                     0
Evaluation/Iteration                        383
Evaluation/MaxReturn                        -31.1229
Evaluation/MinReturn                        -66.6216
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.40754
Extras/EpisodeRewardMean                    -41.998
LinearFeatureBaseline/ExplainedVariance     -25.7617
PolicyExecTime                                0.108344
ProcessExecTime                               0.0120153
TotalEnvSteps                            388608
policy/Entropy                               -0.12987
policy/KL                                     0.00836395
policy/KLBefore                               0
policy/LossAfter                             -0.0208216
policy/LossBefore                            -4.28776e-08
policy/Perplexity                             0.87821
policy/dLoss                                  0.0208215
---------------------------------------  ----------------
2022-04-23 14:21:30 | [train_policy] epoch #384 | Obtaining samples for iteration 384...
2022-04-23 14:21:30 | [train_policy] epoch #384 | Logging diagnostics...
2022-04-23 14:21:30 | [train_policy] epoch #384 | Optimizing policy...
2022-04-23 14:21:30 | [train_policy] epoch #384 | Computing loss before
2022-04-23 14:21:30 | [train_policy] epoch #384 | Computing KL before
2022-04-23 14:21:30 | [train_policy] epoch #384 | Optimizing
2022-04-23 14:21:30 | [train_policy] epoch #384 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:30 | [train_policy] epoch #384 | computing loss before
2022-04-23 14:21:30 | [train_policy] epoch #384 | computing gradient
2022-04-23 14:21:30 | [train_policy] epoch #384 | gradient computed
2022-04-23 14:21:30 | [train_policy] epoch #384 | computing descent direction
2022-04-23 14:21:30 | [train_policy] epoch #384 | descent direction computed
2022-04-23 14:21:30 | [train_policy] epoch #384 | backtrack iters: 0
2022-04-23 14:21:30 | [train_policy] epoch #384 | optimization finished
2022-04-23 14:21:30 | [train_policy] epoch #384 | Computing KL after
2022-04-23 14:21:30 | [train_policy] epoch #384 | Computing loss after
2022-04-23 14:21:30 | [train_policy] epoch #384 | Fitting baseline...
2022-04-23 14:21:30 | [train_policy] epoch #384 | Saving snapshot...
2022-04-23 14:21:30 | [train_policy] epoch #384 | Saved
2022-04-23 14:21:30 | [train_policy] epoch #384 | Time 138.50 s
2022-04-23 14:21:30 | [train_policy] epoch #384 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.117804
Evaluation/AverageDiscountedReturn         -108.308
Evaluation/AverageReturn                   -108.308
Evaluation/CompletionRate                     0
Evaluation/Iteration                        384
Evaluation/MaxReturn                        -31.9971
Evaluation/MinReturn                      -2067.94
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.564
Extras/EpisodeRewardMean                   -102.98
LinearFeatureBaseline/ExplainedVariance       0.00996094
PolicyExecTime                                0.112808
ProcessExecTime                               0.0120418
TotalEnvSteps                            389620
policy/Entropy                               -0.139879
policy/KL                                     0.00941123
policy/KLBefore                               0
policy/LossAfter                             -0.029395
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.869463
policy/dLoss                                  0.029395
---------------------------------------  ----------------
2022-04-23 14:21:30 | [train_policy] epoch #385 | Obtaining samples for iteration 385...
2022-04-23 14:21:31 | [train_policy] epoch #385 | Logging diagnostics...
2022-04-23 14:21:31 | [train_policy] epoch #385 | Optimizing policy...
2022-04-23 14:21:31 | [train_policy] epoch #385 | Computing loss before
2022-04-23 14:21:31 | [train_policy] epoch #385 | Computing KL before
2022-04-23 14:21:31 | [train_policy] epoch #385 | Optimizing
2022-04-23 14:21:31 | [train_policy] epoch #385 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:31 | [train_policy] epoch #385 | computing loss before
2022-04-23 14:21:31 | [train_policy] epoch #385 | computing gradient
2022-04-23 14:21:31 | [train_policy] epoch #385 | gradient computed
2022-04-23 14:21:31 | [train_policy] epoch #385 | computing descent direction
2022-04-23 14:21:31 | [train_policy] epoch #385 | descent direction computed
2022-04-23 14:21:31 | [train_policy] epoch #385 | backtrack iters: 0
2022-04-23 14:21:31 | [train_policy] epoch #385 | optimization finished
2022-04-23 14:21:31 | [train_policy] epoch #385 | Computing KL after
2022-04-23 14:21:31 | [train_policy] epoch #385 | Computing loss after
2022-04-23 14:21:31 | [train_policy] epoch #385 | Fitting baseline...
2022-04-23 14:21:31 | [train_policy] epoch #385 | Saving snapshot...
2022-04-23 14:21:31 | [train_policy] epoch #385 | Saved
2022-04-23 14:21:31 | [train_policy] epoch #385 | Time 138.85 s
2022-04-23 14:21:31 | [train_policy] epoch #385 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117433
Evaluation/AverageDiscountedReturn          -41.6306
Evaluation/AverageReturn                    -41.6306
Evaluation/CompletionRate                     0
Evaluation/Iteration                        385
Evaluation/MaxReturn                        -31.2387
Evaluation/MinReturn                        -64.9128
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.32045
Extras/EpisodeRewardMean                    -41.792
LinearFeatureBaseline/ExplainedVariance    -125.522
PolicyExecTime                                0.104679
ProcessExecTime                               0.0114965
TotalEnvSteps                            390632
policy/Entropy                               -0.157741
policy/KL                                     0.00969506
policy/KLBefore                               0
policy/LossAfter                             -0.0284617
policy/LossBefore                            -3.58099e-08
policy/Perplexity                             0.854071
policy/dLoss                                  0.0284616
---------------------------------------  ----------------
2022-04-23 14:21:31 | [train_policy] epoch #386 | Obtaining samples for iteration 386...
2022-04-23 14:21:31 | [train_policy] epoch #386 | Logging diagnostics...
2022-04-23 14:21:31 | [train_policy] epoch #386 | Optimizing policy...
2022-04-23 14:21:31 | [train_policy] epoch #386 | Computing loss before
2022-04-23 14:21:31 | [train_policy] epoch #386 | Computing KL before
2022-04-23 14:21:31 | [train_policy] epoch #386 | Optimizing
2022-04-23 14:21:31 | [train_policy] epoch #386 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:31 | [train_policy] epoch #386 | computing loss before
2022-04-23 14:21:31 | [train_policy] epoch #386 | computing gradient
2022-04-23 14:21:31 | [train_policy] epoch #386 | gradient computed
2022-04-23 14:21:31 | [train_policy] epoch #386 | computing descent direction
2022-04-23 14:21:31 | [train_policy] epoch #386 | descent direction computed
2022-04-23 14:21:31 | [train_policy] epoch #386 | backtrack iters: 1
2022-04-23 14:21:31 | [train_policy] epoch #386 | optimization finished
2022-04-23 14:21:31 | [train_policy] epoch #386 | Computing KL after
2022-04-23 14:21:31 | [train_policy] epoch #386 | Computing loss after
2022-04-23 14:21:31 | [train_policy] epoch #386 | Fitting baseline...
2022-04-23 14:21:31 | [train_policy] epoch #386 | Saving snapshot...
2022-04-23 14:21:31 | [train_policy] epoch #386 | Saved
2022-04-23 14:21:31 | [train_policy] epoch #386 | Time 139.21 s
2022-04-23 14:21:31 | [train_policy] epoch #386 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.116424
Evaluation/AverageDiscountedReturn          -66.7145
Evaluation/AverageReturn                    -66.7145
Evaluation/CompletionRate                     0
Evaluation/Iteration                        386
Evaluation/MaxReturn                        -30.3219
Evaluation/MinReturn                      -2065.9
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.749
Extras/EpisodeRewardMean                    -64.787
LinearFeatureBaseline/ExplainedVariance       0.0106787
PolicyExecTime                                0.105491
ProcessExecTime                               0.011492
TotalEnvSteps                            391644
policy/Entropy                               -0.15314
policy/KL                                     0.00754657
policy/KLBefore                               0
policy/LossAfter                             -0.0231389
policy/LossBefore                            -2.12032e-08
policy/Perplexity                             0.858009
policy/dLoss                                  0.0231389
---------------------------------------  ----------------
2022-04-23 14:21:31 | [train_policy] epoch #387 | Obtaining samples for iteration 387...
2022-04-23 14:21:31 | [train_policy] epoch #387 | Logging diagnostics...
2022-04-23 14:21:31 | [train_policy] epoch #387 | Optimizing policy...
2022-04-23 14:21:31 | [train_policy] epoch #387 | Computing loss before
2022-04-23 14:21:31 | [train_policy] epoch #387 | Computing KL before
2022-04-23 14:21:31 | [train_policy] epoch #387 | Optimizing
2022-04-23 14:21:31 | [train_policy] epoch #387 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:31 | [train_policy] epoch #387 | computing loss before
2022-04-23 14:21:31 | [train_policy] epoch #387 | computing gradient
2022-04-23 14:21:31 | [train_policy] epoch #387 | gradient computed
2022-04-23 14:21:31 | [train_policy] epoch #387 | computing descent direction
2022-04-23 14:21:31 | [train_policy] epoch #387 | descent direction computed
2022-04-23 14:21:31 | [train_policy] epoch #387 | backtrack iters: 1
2022-04-23 14:21:31 | [train_policy] epoch #387 | optimization finished
2022-04-23 14:21:31 | [train_policy] epoch #387 | Computing KL after
2022-04-23 14:21:31 | [train_policy] epoch #387 | Computing loss after
2022-04-23 14:21:31 | [train_policy] epoch #387 | Fitting baseline...
2022-04-23 14:21:31 | [train_policy] epoch #387 | Saving snapshot...
2022-04-23 14:21:31 | [train_policy] epoch #387 | Saved
2022-04-23 14:21:31 | [train_policy] epoch #387 | Time 139.56 s
2022-04-23 14:21:31 | [train_policy] epoch #387 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116292
Evaluation/AverageDiscountedReturn          -42.302
Evaluation/AverageReturn                    -42.302
Evaluation/CompletionRate                     0
Evaluation/Iteration                        387
Evaluation/MaxReturn                        -31.5163
Evaluation/MinReturn                        -64.4618
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.52061
Extras/EpisodeRewardMean                    -42.3902
LinearFeatureBaseline/ExplainedVariance     -12.7225
PolicyExecTime                                0.106369
ProcessExecTime                               0.0114071
TotalEnvSteps                            392656
policy/Entropy                               -0.16085
policy/KL                                     0.00639588
policy/KLBefore                               0
policy/LossAfter                             -0.0154712
policy/LossBefore                             3.13337e-08
policy/Perplexity                             0.85142
policy/dLoss                                  0.0154712
---------------------------------------  ----------------
2022-04-23 14:21:31 | [train_policy] epoch #388 | Obtaining samples for iteration 388...
2022-04-23 14:21:32 | [train_policy] epoch #388 | Logging diagnostics...
2022-04-23 14:21:32 | [train_policy] epoch #388 | Optimizing policy...
2022-04-23 14:21:32 | [train_policy] epoch #388 | Computing loss before
2022-04-23 14:21:32 | [train_policy] epoch #388 | Computing KL before
2022-04-23 14:21:32 | [train_policy] epoch #388 | Optimizing
2022-04-23 14:21:32 | [train_policy] epoch #388 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:32 | [train_policy] epoch #388 | computing loss before
2022-04-23 14:21:32 | [train_policy] epoch #388 | computing gradient
2022-04-23 14:21:32 | [train_policy] epoch #388 | gradient computed
2022-04-23 14:21:32 | [train_policy] epoch #388 | computing descent direction
2022-04-23 14:21:32 | [train_policy] epoch #388 | descent direction computed
2022-04-23 14:21:32 | [train_policy] epoch #388 | backtrack iters: 1
2022-04-23 14:21:32 | [train_policy] epoch #388 | optimization finished
2022-04-23 14:21:32 | [train_policy] epoch #388 | Computing KL after
2022-04-23 14:21:32 | [train_policy] epoch #388 | Computing loss after
2022-04-23 14:21:32 | [train_policy] epoch #388 | Fitting baseline...
2022-04-23 14:21:32 | [train_policy] epoch #388 | Saving snapshot...
2022-04-23 14:21:32 | [train_policy] epoch #388 | Saved
2022-04-23 14:21:32 | [train_policy] epoch #388 | Time 139.91 s
2022-04-23 14:21:32 | [train_policy] epoch #388 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116397
Evaluation/AverageDiscountedReturn          -44.0729
Evaluation/AverageReturn                    -44.0729
Evaluation/CompletionRate                     0
Evaluation/Iteration                        388
Evaluation/MaxReturn                        -29.3618
Evaluation/MinReturn                        -74.6645
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.5753
Extras/EpisodeRewardMean                    -44.0011
LinearFeatureBaseline/ExplainedVariance       0.888418
PolicyExecTime                                0.103282
ProcessExecTime                               0.0112081
TotalEnvSteps                            393668
policy/Entropy                               -0.186755
policy/KL                                     0.00648379
policy/KLBefore                               0
policy/LossAfter                             -0.0210215
policy/LossBefore                            -1.23686e-08
policy/Perplexity                             0.829647
policy/dLoss                                  0.0210215
---------------------------------------  ----------------
2022-04-23 14:21:32 | [train_policy] epoch #389 | Obtaining samples for iteration 389...
2022-04-23 14:21:32 | [train_policy] epoch #389 | Logging diagnostics...
2022-04-23 14:21:32 | [train_policy] epoch #389 | Optimizing policy...
2022-04-23 14:21:32 | [train_policy] epoch #389 | Computing loss before
2022-04-23 14:21:32 | [train_policy] epoch #389 | Computing KL before
2022-04-23 14:21:32 | [train_policy] epoch #389 | Optimizing
2022-04-23 14:21:32 | [train_policy] epoch #389 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:32 | [train_policy] epoch #389 | computing loss before
2022-04-23 14:21:32 | [train_policy] epoch #389 | computing gradient
2022-04-23 14:21:32 | [train_policy] epoch #389 | gradient computed
2022-04-23 14:21:32 | [train_policy] epoch #389 | computing descent direction
2022-04-23 14:21:32 | [train_policy] epoch #389 | descent direction computed
2022-04-23 14:21:32 | [train_policy] epoch #389 | backtrack iters: 1
2022-04-23 14:21:32 | [train_policy] epoch #389 | optimization finished
2022-04-23 14:21:32 | [train_policy] epoch #389 | Computing KL after
2022-04-23 14:21:32 | [train_policy] epoch #389 | Computing loss after
2022-04-23 14:21:32 | [train_policy] epoch #389 | Fitting baseline...
2022-04-23 14:21:32 | [train_policy] epoch #389 | Saving snapshot...
2022-04-23 14:21:32 | [train_policy] epoch #389 | Saved
2022-04-23 14:21:32 | [train_policy] epoch #389 | Time 140.26 s
2022-04-23 14:21:32 | [train_policy] epoch #389 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115819
Evaluation/AverageDiscountedReturn          -43.8705
Evaluation/AverageReturn                    -43.8705
Evaluation/CompletionRate                     0
Evaluation/Iteration                        389
Evaluation/MaxReturn                        -30.5583
Evaluation/MinReturn                        -64.5008
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.24216
Extras/EpisodeRewardMean                    -43.9423
LinearFeatureBaseline/ExplainedVariance       0.920042
PolicyExecTime                                0.102815
ProcessExecTime                               0.0112553
TotalEnvSteps                            394680
policy/Entropy                               -0.228158
policy/KL                                     0.00717406
policy/KLBefore                               0
policy/LossAfter                             -0.0192013
policy/LossBefore                             3.53387e-09
policy/Perplexity                             0.795998
policy/dLoss                                  0.0192013
---------------------------------------  ----------------
2022-04-23 14:21:32 | [train_policy] epoch #390 | Obtaining samples for iteration 390...
2022-04-23 14:21:32 | [train_policy] epoch #390 | Logging diagnostics...
2022-04-23 14:21:32 | [train_policy] epoch #390 | Optimizing policy...
2022-04-23 14:21:32 | [train_policy] epoch #390 | Computing loss before
2022-04-23 14:21:32 | [train_policy] epoch #390 | Computing KL before
2022-04-23 14:21:32 | [train_policy] epoch #390 | Optimizing
2022-04-23 14:21:32 | [train_policy] epoch #390 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:32 | [train_policy] epoch #390 | computing loss before
2022-04-23 14:21:32 | [train_policy] epoch #390 | computing gradient
2022-04-23 14:21:32 | [train_policy] epoch #390 | gradient computed
2022-04-23 14:21:32 | [train_policy] epoch #390 | computing descent direction
2022-04-23 14:21:32 | [train_policy] epoch #390 | descent direction computed
2022-04-23 14:21:32 | [train_policy] epoch #390 | backtrack iters: 1
2022-04-23 14:21:32 | [train_policy] epoch #390 | optimization finished
2022-04-23 14:21:32 | [train_policy] epoch #390 | Computing KL after
2022-04-23 14:21:32 | [train_policy] epoch #390 | Computing loss after
2022-04-23 14:21:32 | [train_policy] epoch #390 | Fitting baseline...
2022-04-23 14:21:32 | [train_policy] epoch #390 | Saving snapshot...
2022-04-23 14:21:32 | [train_policy] epoch #390 | Saved
2022-04-23 14:21:32 | [train_policy] epoch #390 | Time 140.63 s
2022-04-23 14:21:32 | [train_policy] epoch #390 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.125229
Evaluation/AverageDiscountedReturn          -42.8172
Evaluation/AverageReturn                    -42.8172
Evaluation/CompletionRate                     0
Evaluation/Iteration                        390
Evaluation/MaxReturn                        -30.0297
Evaluation/MinReturn                        -75.6828
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.99068
Extras/EpisodeRewardMean                    -42.9697
LinearFeatureBaseline/ExplainedVariance       0.924864
PolicyExecTime                                0.111736
ProcessExecTime                               0.0123196
TotalEnvSteps                            395692
policy/Entropy                               -0.275878
policy/KL                                     0.00673057
policy/KLBefore                               0
policy/LossAfter                             -0.015496
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.758906
policy/dLoss                                  0.015496
---------------------------------------  ----------------
2022-04-23 14:21:32 | [train_policy] epoch #391 | Obtaining samples for iteration 391...
2022-04-23 14:21:33 | [train_policy] epoch #391 | Logging diagnostics...
2022-04-23 14:21:33 | [train_policy] epoch #391 | Optimizing policy...
2022-04-23 14:21:33 | [train_policy] epoch #391 | Computing loss before
2022-04-23 14:21:33 | [train_policy] epoch #391 | Computing KL before
2022-04-23 14:21:33 | [train_policy] epoch #391 | Optimizing
2022-04-23 14:21:33 | [train_policy] epoch #391 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:33 | [train_policy] epoch #391 | computing loss before
2022-04-23 14:21:33 | [train_policy] epoch #391 | computing gradient
2022-04-23 14:21:33 | [train_policy] epoch #391 | gradient computed
2022-04-23 14:21:33 | [train_policy] epoch #391 | computing descent direction
2022-04-23 14:21:33 | [train_policy] epoch #391 | descent direction computed
2022-04-23 14:21:33 | [train_policy] epoch #391 | backtrack iters: 1
2022-04-23 14:21:33 | [train_policy] epoch #391 | optimization finished
2022-04-23 14:21:33 | [train_policy] epoch #391 | Computing KL after
2022-04-23 14:21:33 | [train_policy] epoch #391 | Computing loss after
2022-04-23 14:21:33 | [train_policy] epoch #391 | Fitting baseline...
2022-04-23 14:21:33 | [train_policy] epoch #391 | Saving snapshot...
2022-04-23 14:21:33 | [train_policy] epoch #391 | Saved
2022-04-23 14:21:33 | [train_policy] epoch #391 | Time 141.00 s
2022-04-23 14:21:33 | [train_policy] epoch #391 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.122701
Evaluation/AverageDiscountedReturn          -65.6341
Evaluation/AverageReturn                    -65.6341
Evaluation/CompletionRate                     0
Evaluation/Iteration                        391
Evaluation/MaxReturn                        -30.7316
Evaluation/MinReturn                      -2063.06
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.54
Extras/EpisodeRewardMean                    -63.8735
LinearFeatureBaseline/ExplainedVariance       0.00995303
PolicyExecTime                                0.10966
ProcessExecTime                               0.0122738
TotalEnvSteps                            396704
policy/Entropy                               -0.282893
policy/KL                                     0.00673582
policy/KLBefore                               0
policy/LossAfter                             -0.0153761
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.7536
policy/dLoss                                  0.0153761
---------------------------------------  ----------------
2022-04-23 14:21:33 | [train_policy] epoch #392 | Obtaining samples for iteration 392...
2022-04-23 14:21:33 | [train_policy] epoch #392 | Logging diagnostics...
2022-04-23 14:21:33 | [train_policy] epoch #392 | Optimizing policy...
2022-04-23 14:21:33 | [train_policy] epoch #392 | Computing loss before
2022-04-23 14:21:33 | [train_policy] epoch #392 | Computing KL before
2022-04-23 14:21:33 | [train_policy] epoch #392 | Optimizing
2022-04-23 14:21:33 | [train_policy] epoch #392 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:33 | [train_policy] epoch #392 | computing loss before
2022-04-23 14:21:33 | [train_policy] epoch #392 | computing gradient
2022-04-23 14:21:33 | [train_policy] epoch #392 | gradient computed
2022-04-23 14:21:33 | [train_policy] epoch #392 | computing descent direction
2022-04-23 14:21:33 | [train_policy] epoch #392 | descent direction computed
2022-04-23 14:21:33 | [train_policy] epoch #392 | backtrack iters: 0
2022-04-23 14:21:33 | [train_policy] epoch #392 | optimization finished
2022-04-23 14:21:33 | [train_policy] epoch #392 | Computing KL after
2022-04-23 14:21:33 | [train_policy] epoch #392 | Computing loss after
2022-04-23 14:21:33 | [train_policy] epoch #392 | Fitting baseline...
2022-04-23 14:21:33 | [train_policy] epoch #392 | Saving snapshot...
2022-04-23 14:21:33 | [train_policy] epoch #392 | Saved
2022-04-23 14:21:33 | [train_policy] epoch #392 | Time 141.35 s
2022-04-23 14:21:33 | [train_policy] epoch #392 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118419
Evaluation/AverageDiscountedReturn          -42.2488
Evaluation/AverageReturn                    -42.2488
Evaluation/CompletionRate                     0
Evaluation/Iteration                        392
Evaluation/MaxReturn                        -30.4674
Evaluation/MinReturn                        -64.7112
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.78752
Extras/EpisodeRewardMean                    -42.5783
LinearFeatureBaseline/ExplainedVariance     -15.5847
PolicyExecTime                                0.108019
ProcessExecTime                               0.0117564
TotalEnvSteps                            397716
policy/Entropy                               -0.274813
policy/KL                                     0.0097815
policy/KLBefore                               0
policy/LossAfter                             -0.0177344
policy/LossBefore                             4.33488e-08
policy/Perplexity                             0.759714
policy/dLoss                                  0.0177345
---------------------------------------  ----------------
2022-04-23 14:21:33 | [train_policy] epoch #393 | Obtaining samples for iteration 393...
2022-04-23 14:21:33 | [train_policy] epoch #393 | Logging diagnostics...
2022-04-23 14:21:33 | [train_policy] epoch #393 | Optimizing policy...
2022-04-23 14:21:33 | [train_policy] epoch #393 | Computing loss before
2022-04-23 14:21:33 | [train_policy] epoch #393 | Computing KL before
2022-04-23 14:21:33 | [train_policy] epoch #393 | Optimizing
2022-04-23 14:21:33 | [train_policy] epoch #393 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:33 | [train_policy] epoch #393 | computing loss before
2022-04-23 14:21:33 | [train_policy] epoch #393 | computing gradient
2022-04-23 14:21:33 | [train_policy] epoch #393 | gradient computed
2022-04-23 14:21:33 | [train_policy] epoch #393 | computing descent direction
2022-04-23 14:21:33 | [train_policy] epoch #393 | descent direction computed
2022-04-23 14:21:33 | [train_policy] epoch #393 | backtrack iters: 1
2022-04-23 14:21:33 | [train_policy] epoch #393 | optimization finished
2022-04-23 14:21:33 | [train_policy] epoch #393 | Computing KL after
2022-04-23 14:21:33 | [train_policy] epoch #393 | Computing loss after
2022-04-23 14:21:33 | [train_policy] epoch #393 | Fitting baseline...
2022-04-23 14:21:33 | [train_policy] epoch #393 | Saving snapshot...
2022-04-23 14:21:33 | [train_policy] epoch #393 | Saved
2022-04-23 14:21:33 | [train_policy] epoch #393 | Time 141.71 s
2022-04-23 14:21:33 | [train_policy] epoch #393 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.124063
Evaluation/AverageDiscountedReturn          -65.0101
Evaluation/AverageReturn                    -65.0101
Evaluation/CompletionRate                     0
Evaluation/Iteration                        393
Evaluation/MaxReturn                        -31.3047
Evaluation/MinReturn                      -2064.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.739
Extras/EpisodeRewardMean                    -63.419
LinearFeatureBaseline/ExplainedVariance       0.0102741
PolicyExecTime                                0.10881
ProcessExecTime                               0.0123832
TotalEnvSteps                            398728
policy/Entropy                               -0.266628
policy/KL                                     0.00671429
policy/KLBefore                               0
policy/LossAfter                             -0.0226594
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             0.765958
policy/dLoss                                  0.0226594
---------------------------------------  ----------------
2022-04-23 14:21:33 | [train_policy] epoch #394 | Obtaining samples for iteration 394...
2022-04-23 14:21:34 | [train_policy] epoch #394 | Logging diagnostics...
2022-04-23 14:21:34 | [train_policy] epoch #394 | Optimizing policy...
2022-04-23 14:21:34 | [train_policy] epoch #394 | Computing loss before
2022-04-23 14:21:34 | [train_policy] epoch #394 | Computing KL before
2022-04-23 14:21:34 | [train_policy] epoch #394 | Optimizing
2022-04-23 14:21:34 | [train_policy] epoch #394 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:34 | [train_policy] epoch #394 | computing loss before
2022-04-23 14:21:34 | [train_policy] epoch #394 | computing gradient
2022-04-23 14:21:34 | [train_policy] epoch #394 | gradient computed
2022-04-23 14:21:34 | [train_policy] epoch #394 | computing descent direction
2022-04-23 14:21:34 | [train_policy] epoch #394 | descent direction computed
2022-04-23 14:21:34 | [train_policy] epoch #394 | backtrack iters: 0
2022-04-23 14:21:34 | [train_policy] epoch #394 | optimization finished
2022-04-23 14:21:34 | [train_policy] epoch #394 | Computing KL after
2022-04-23 14:21:34 | [train_policy] epoch #394 | Computing loss after
2022-04-23 14:21:34 | [train_policy] epoch #394 | Fitting baseline...
2022-04-23 14:21:34 | [train_policy] epoch #394 | Saving snapshot...
2022-04-23 14:21:34 | [train_policy] epoch #394 | Saved
2022-04-23 14:21:34 | [train_policy] epoch #394 | Time 142.05 s
2022-04-23 14:21:34 | [train_policy] epoch #394 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.120301
Evaluation/AverageDiscountedReturn          -65.6539
Evaluation/AverageReturn                    -65.6539
Evaluation/CompletionRate                     0
Evaluation/Iteration                        394
Evaluation/MaxReturn                        -32.535
Evaluation/MinReturn                      -2066.52
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.855
Extras/EpisodeRewardMean                    -63.7376
LinearFeatureBaseline/ExplainedVariance       0.0214672
PolicyExecTime                                0.100195
ProcessExecTime                               0.0115957
TotalEnvSteps                            399740
policy/Entropy                               -0.274083
policy/KL                                     0.00994857
policy/KLBefore                               0
policy/LossAfter                             -0.03534
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.760269
policy/dLoss                                  0.03534
---------------------------------------  ----------------
2022-04-23 14:21:34 | [train_policy] epoch #395 | Obtaining samples for iteration 395...
2022-04-23 14:21:34 | [train_policy] epoch #395 | Logging diagnostics...
2022-04-23 14:21:34 | [train_policy] epoch #395 | Optimizing policy...
2022-04-23 14:21:34 | [train_policy] epoch #395 | Computing loss before
2022-04-23 14:21:34 | [train_policy] epoch #395 | Computing KL before
2022-04-23 14:21:34 | [train_policy] epoch #395 | Optimizing
2022-04-23 14:21:34 | [train_policy] epoch #395 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:34 | [train_policy] epoch #395 | computing loss before
2022-04-23 14:21:34 | [train_policy] epoch #395 | computing gradient
2022-04-23 14:21:34 | [train_policy] epoch #395 | gradient computed
2022-04-23 14:21:34 | [train_policy] epoch #395 | computing descent direction
2022-04-23 14:21:34 | [train_policy] epoch #395 | descent direction computed
2022-04-23 14:21:34 | [train_policy] epoch #395 | backtrack iters: 1
2022-04-23 14:21:34 | [train_policy] epoch #395 | optimization finished
2022-04-23 14:21:34 | [train_policy] epoch #395 | Computing KL after
2022-04-23 14:21:34 | [train_policy] epoch #395 | Computing loss after
2022-04-23 14:21:34 | [train_policy] epoch #395 | Fitting baseline...
2022-04-23 14:21:34 | [train_policy] epoch #395 | Saving snapshot...
2022-04-23 14:21:34 | [train_policy] epoch #395 | Saved
2022-04-23 14:21:34 | [train_policy] epoch #395 | Time 142.41 s
2022-04-23 14:21:34 | [train_policy] epoch #395 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.117395
Evaluation/AverageDiscountedReturn          -43.3286
Evaluation/AverageReturn                    -43.3286
Evaluation/CompletionRate                     0
Evaluation/Iteration                        395
Evaluation/MaxReturn                        -31.2916
Evaluation/MinReturn                        -64.2745
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.23431
Extras/EpisodeRewardMean                    -43.4695
LinearFeatureBaseline/ExplainedVariance     -25.4402
PolicyExecTime                                0.106399
ProcessExecTime                               0.0114849
TotalEnvSteps                            400752
policy/Entropy                               -0.279693
policy/KL                                     0.00749908
policy/KLBefore                               0
policy/LossAfter                             -0.01583
policy/LossBefore                             1.7905e-08
policy/Perplexity                             0.756016
policy/dLoss                                  0.01583
---------------------------------------  ---------------
2022-04-23 14:21:34 | [train_policy] epoch #396 | Obtaining samples for iteration 396...
2022-04-23 14:21:34 | [train_policy] epoch #396 | Logging diagnostics...
2022-04-23 14:21:34 | [train_policy] epoch #396 | Optimizing policy...
2022-04-23 14:21:34 | [train_policy] epoch #396 | Computing loss before
2022-04-23 14:21:34 | [train_policy] epoch #396 | Computing KL before
2022-04-23 14:21:34 | [train_policy] epoch #396 | Optimizing
2022-04-23 14:21:34 | [train_policy] epoch #396 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:34 | [train_policy] epoch #396 | computing loss before
2022-04-23 14:21:34 | [train_policy] epoch #396 | computing gradient
2022-04-23 14:21:34 | [train_policy] epoch #396 | gradient computed
2022-04-23 14:21:34 | [train_policy] epoch #396 | computing descent direction
2022-04-23 14:21:35 | [train_policy] epoch #396 | descent direction computed
2022-04-23 14:21:35 | [train_policy] epoch #396 | backtrack iters: 0
2022-04-23 14:21:35 | [train_policy] epoch #396 | optimization finished
2022-04-23 14:21:35 | [train_policy] epoch #396 | Computing KL after
2022-04-23 14:21:35 | [train_policy] epoch #396 | Computing loss after
2022-04-23 14:21:35 | [train_policy] epoch #396 | Fitting baseline...
2022-04-23 14:21:35 | [train_policy] epoch #396 | Saving snapshot...
2022-04-23 14:21:35 | [train_policy] epoch #396 | Saved
2022-04-23 14:21:35 | [train_policy] epoch #396 | Time 142.77 s
2022-04-23 14:21:35 | [train_policy] epoch #396 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.119316
Evaluation/AverageDiscountedReturn          -42.67
Evaluation/AverageReturn                    -42.67
Evaluation/CompletionRate                     0
Evaluation/Iteration                        396
Evaluation/MaxReturn                        -32.4024
Evaluation/MinReturn                        -64.0246
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.81074
Extras/EpisodeRewardMean                    -43.0578
LinearFeatureBaseline/ExplainedVariance       0.926199
PolicyExecTime                                0.108835
ProcessExecTime                               0.0115983
TotalEnvSteps                            401764
policy/Entropy                               -0.270121
policy/KL                                     0.00889092
policy/KLBefore                               0
policy/LossAfter                             -0.0129107
policy/LossBefore                            -5.4186e-09
policy/Perplexity                             0.763287
policy/dLoss                                  0.0129107
---------------------------------------  ---------------
2022-04-23 14:21:35 | [train_policy] epoch #397 | Obtaining samples for iteration 397...
2022-04-23 14:21:35 | [train_policy] epoch #397 | Logging diagnostics...
2022-04-23 14:21:35 | [train_policy] epoch #397 | Optimizing policy...
2022-04-23 14:21:35 | [train_policy] epoch #397 | Computing loss before
2022-04-23 14:21:35 | [train_policy] epoch #397 | Computing KL before
2022-04-23 14:21:35 | [train_policy] epoch #397 | Optimizing
2022-04-23 14:21:35 | [train_policy] epoch #397 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:35 | [train_policy] epoch #397 | computing loss before
2022-04-23 14:21:35 | [train_policy] epoch #397 | computing gradient
2022-04-23 14:21:35 | [train_policy] epoch #397 | gradient computed
2022-04-23 14:21:35 | [train_policy] epoch #397 | computing descent direction
2022-04-23 14:21:35 | [train_policy] epoch #397 | descent direction computed
2022-04-23 14:21:35 | [train_policy] epoch #397 | backtrack iters: 0
2022-04-23 14:21:35 | [train_policy] epoch #397 | optimization finished
2022-04-23 14:21:35 | [train_policy] epoch #397 | Computing KL after
2022-04-23 14:21:35 | [train_policy] epoch #397 | Computing loss after
2022-04-23 14:21:35 | [train_policy] epoch #397 | Fitting baseline...
2022-04-23 14:21:35 | [train_policy] epoch #397 | Saving snapshot...
2022-04-23 14:21:35 | [train_policy] epoch #397 | Saved
2022-04-23 14:21:35 | [train_policy] epoch #397 | Time 143.15 s
2022-04-23 14:21:35 | [train_policy] epoch #397 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                   0.123635
Evaluation/AverageDiscountedReturn          -63.7809
Evaluation/AverageReturn                    -63.7809
Evaluation/CompletionRate                     0
Evaluation/Iteration                        397
Evaluation/MaxReturn                        -31.8251
Evaluation/MinReturn                      -2062.15
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.56
Extras/EpisodeRewardMean                    -61.8098
LinearFeatureBaseline/ExplainedVariance       0.0111354
PolicyExecTime                                0.112399
ProcessExecTime                               0.0120103
TotalEnvSteps                            402776
policy/Entropy                               -0.264657
policy/KL                                     0.00901111
policy/KLBefore                               0
policy/LossAfter                             -0.0324326
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.767469
policy/dLoss                                  0.0324326
---------------------------------------  ---------------
2022-04-23 14:21:35 | [train_policy] epoch #398 | Obtaining samples for iteration 398...
2022-04-23 14:21:35 | [train_policy] epoch #398 | Logging diagnostics...
2022-04-23 14:21:35 | [train_policy] epoch #398 | Optimizing policy...
2022-04-23 14:21:35 | [train_policy] epoch #398 | Computing loss before
2022-04-23 14:21:35 | [train_policy] epoch #398 | Computing KL before
2022-04-23 14:21:35 | [train_policy] epoch #398 | Optimizing
2022-04-23 14:21:35 | [train_policy] epoch #398 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:35 | [train_policy] epoch #398 | computing loss before
2022-04-23 14:21:35 | [train_policy] epoch #398 | computing gradient
2022-04-23 14:21:35 | [train_policy] epoch #398 | gradient computed
2022-04-23 14:21:35 | [train_policy] epoch #398 | computing descent direction
2022-04-23 14:21:35 | [train_policy] epoch #398 | descent direction computed
2022-04-23 14:21:35 | [train_policy] epoch #398 | backtrack iters: 1
2022-04-23 14:21:35 | [train_policy] epoch #398 | optimization finished
2022-04-23 14:21:35 | [train_policy] epoch #398 | Computing KL after
2022-04-23 14:21:35 | [train_policy] epoch #398 | Computing loss after
2022-04-23 14:21:35 | [train_policy] epoch #398 | Fitting baseline...
2022-04-23 14:21:35 | [train_policy] epoch #398 | Saving snapshot...
2022-04-23 14:21:35 | [train_policy] epoch #398 | Saved
2022-04-23 14:21:35 | [train_policy] epoch #398 | Time 143.51 s
2022-04-23 14:21:35 | [train_policy] epoch #398 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.118525
Evaluation/AverageDiscountedReturn         -108.533
Evaluation/AverageReturn                   -108.533
Evaluation/CompletionRate                     0
Evaluation/Iteration                        398
Evaluation/MaxReturn                        -30.3931
Evaluation/MinReturn                      -2065.46
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.042
Extras/EpisodeRewardMean                   -103.17
LinearFeatureBaseline/ExplainedVariance       0.222439
PolicyExecTime                                0.105692
ProcessExecTime                               0.0113611
TotalEnvSteps                            403788
policy/Entropy                               -0.283692
policy/KL                                     0.0069487
policy/KLBefore                               0
policy/LossAfter                             -0.019565
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.752999
policy/dLoss                                  0.019565
---------------------------------------  ----------------
2022-04-23 14:21:35 | [train_policy] epoch #399 | Obtaining samples for iteration 399...
2022-04-23 14:21:36 | [train_policy] epoch #399 | Logging diagnostics...
2022-04-23 14:21:36 | [train_policy] epoch #399 | Optimizing policy...
2022-04-23 14:21:36 | [train_policy] epoch #399 | Computing loss before
2022-04-23 14:21:36 | [train_policy] epoch #399 | Computing KL before
2022-04-23 14:21:36 | [train_policy] epoch #399 | Optimizing
2022-04-23 14:21:36 | [train_policy] epoch #399 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:36 | [train_policy] epoch #399 | computing loss before
2022-04-23 14:21:36 | [train_policy] epoch #399 | computing gradient
2022-04-23 14:21:36 | [train_policy] epoch #399 | gradient computed
2022-04-23 14:21:36 | [train_policy] epoch #399 | computing descent direction
2022-04-23 14:21:36 | [train_policy] epoch #399 | descent direction computed
2022-04-23 14:21:36 | [train_policy] epoch #399 | backtrack iters: 0
2022-04-23 14:21:36 | [train_policy] epoch #399 | optimization finished
2022-04-23 14:21:36 | [train_policy] epoch #399 | Computing KL after
2022-04-23 14:21:36 | [train_policy] epoch #399 | Computing loss after
2022-04-23 14:21:36 | [train_policy] epoch #399 | Fitting baseline...
2022-04-23 14:21:36 | [train_policy] epoch #399 | Saving snapshot...
2022-04-23 14:21:36 | [train_policy] epoch #399 | Saved
2022-04-23 14:21:36 | [train_policy] epoch #399 | Time 143.86 s
2022-04-23 14:21:36 | [train_policy] epoch #399 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.116995
Evaluation/AverageDiscountedReturn          -64.8057
Evaluation/AverageReturn                    -64.8057
Evaluation/CompletionRate                     0
Evaluation/Iteration                        399
Evaluation/MaxReturn                        -31.3896
Evaluation/MinReturn                      -2063.35
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.62
Extras/EpisodeRewardMean                    -62.9125
LinearFeatureBaseline/ExplainedVariance      -0.0264892
PolicyExecTime                                0.102666
ProcessExecTime                               0.011363
TotalEnvSteps                            404800
policy/Entropy                               -0.292881
policy/KL                                     0.00786203
policy/KLBefore                               0
policy/LossAfter                             -0.0197894
policy/LossBefore                             8.3635e-09
policy/Perplexity                             0.746111
policy/dLoss                                  0.0197895
---------------------------------------  ---------------
2022-04-23 14:21:36 | [train_policy] epoch #400 | Obtaining samples for iteration 400...
2022-04-23 14:21:36 | [train_policy] epoch #400 | Logging diagnostics...
2022-04-23 14:21:36 | [train_policy] epoch #400 | Optimizing policy...
2022-04-23 14:21:36 | [train_policy] epoch #400 | Computing loss before
2022-04-23 14:21:36 | [train_policy] epoch #400 | Computing KL before
2022-04-23 14:21:36 | [train_policy] epoch #400 | Optimizing
2022-04-23 14:21:36 | [train_policy] epoch #400 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:36 | [train_policy] epoch #400 | computing loss before
2022-04-23 14:21:36 | [train_policy] epoch #400 | computing gradient
2022-04-23 14:21:36 | [train_policy] epoch #400 | gradient computed
2022-04-23 14:21:36 | [train_policy] epoch #400 | computing descent direction
2022-04-23 14:21:36 | [train_policy] epoch #400 | descent direction computed
2022-04-23 14:21:36 | [train_policy] epoch #400 | backtrack iters: 0
2022-04-23 14:21:36 | [train_policy] epoch #400 | optimization finished
2022-04-23 14:21:36 | [train_policy] epoch #400 | Computing KL after
2022-04-23 14:21:36 | [train_policy] epoch #400 | Computing loss after
2022-04-23 14:21:36 | [train_policy] epoch #400 | Fitting baseline...
2022-04-23 14:21:36 | [train_policy] epoch #400 | Saving snapshot...
2022-04-23 14:21:36 | [train_policy] epoch #400 | Saved
2022-04-23 14:21:36 | [train_policy] epoch #400 | Time 144.21 s
2022-04-23 14:21:36 | [train_policy] epoch #400 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117544
Evaluation/AverageDiscountedReturn          -64.4542
Evaluation/AverageReturn                    -64.4542
Evaluation/CompletionRate                     0
Evaluation/Iteration                        400
Evaluation/MaxReturn                        -31.2944
Evaluation/MinReturn                      -2067.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.071
Extras/EpisodeRewardMean                    -62.7729
LinearFeatureBaseline/ExplainedVariance       0.0940705
PolicyExecTime                                0.0975995
ProcessExecTime                               0.0110922
TotalEnvSteps                            405812
policy/Entropy                               -0.278425
policy/KL                                     0.00982169
policy/KLBefore                               0
policy/LossAfter                             -0.0220128
policy/LossBefore                             1.13084e-08
policy/Perplexity                             0.756975
policy/dLoss                                  0.0220129
---------------------------------------  ----------------
2022-04-23 14:21:36 | [train_policy] epoch #401 | Obtaining samples for iteration 401...
2022-04-23 14:21:36 | [train_policy] epoch #401 | Logging diagnostics...
2022-04-23 14:21:36 | [train_policy] epoch #401 | Optimizing policy...
2022-04-23 14:21:36 | [train_policy] epoch #401 | Computing loss before
2022-04-23 14:21:36 | [train_policy] epoch #401 | Computing KL before
2022-04-23 14:21:36 | [train_policy] epoch #401 | Optimizing
2022-04-23 14:21:36 | [train_policy] epoch #401 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:36 | [train_policy] epoch #401 | computing loss before
2022-04-23 14:21:36 | [train_policy] epoch #401 | computing gradient
2022-04-23 14:21:36 | [train_policy] epoch #401 | gradient computed
2022-04-23 14:21:36 | [train_policy] epoch #401 | computing descent direction
2022-04-23 14:21:36 | [train_policy] epoch #401 | descent direction computed
2022-04-23 14:21:36 | [train_policy] epoch #401 | backtrack iters: 1
2022-04-23 14:21:36 | [train_policy] epoch #401 | optimization finished
2022-04-23 14:21:36 | [train_policy] epoch #401 | Computing KL after
2022-04-23 14:21:36 | [train_policy] epoch #401 | Computing loss after
2022-04-23 14:21:36 | [train_policy] epoch #401 | Fitting baseline...
2022-04-23 14:21:36 | [train_policy] epoch #401 | Saving snapshot...
2022-04-23 14:21:36 | [train_policy] epoch #401 | Saved
2022-04-23 14:21:36 | [train_policy] epoch #401 | Time 144.55 s
2022-04-23 14:21:36 | [train_policy] epoch #401 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116608
Evaluation/AverageDiscountedReturn          -65.2653
Evaluation/AverageReturn                    -65.2653
Evaluation/CompletionRate                     0
Evaluation/Iteration                        401
Evaluation/MaxReturn                        -31.3956
Evaluation/MinReturn                      -2082.43
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.616
Extras/EpisodeRewardMean                    -63.4617
LinearFeatureBaseline/ExplainedVariance      -0.119211
PolicyExecTime                                0.0966036
ProcessExecTime                               0.0109594
TotalEnvSteps                            406824
policy/Entropy                               -0.296901
policy/KL                                     0.00718458
policy/KLBefore                               0
policy/LossAfter                             -0.0221127
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.743117
policy/dLoss                                  0.0221127
---------------------------------------  ----------------
2022-04-23 14:21:36 | [train_policy] epoch #402 | Obtaining samples for iteration 402...
2022-04-23 14:21:37 | [train_policy] epoch #402 | Logging diagnostics...
2022-04-23 14:21:37 | [train_policy] epoch #402 | Optimizing policy...
2022-04-23 14:21:37 | [train_policy] epoch #402 | Computing loss before
2022-04-23 14:21:37 | [train_policy] epoch #402 | Computing KL before
2022-04-23 14:21:37 | [train_policy] epoch #402 | Optimizing
2022-04-23 14:21:37 | [train_policy] epoch #402 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:37 | [train_policy] epoch #402 | computing loss before
2022-04-23 14:21:37 | [train_policy] epoch #402 | computing gradient
2022-04-23 14:21:37 | [train_policy] epoch #402 | gradient computed
2022-04-23 14:21:37 | [train_policy] epoch #402 | computing descent direction
2022-04-23 14:21:37 | [train_policy] epoch #402 | descent direction computed
2022-04-23 14:21:37 | [train_policy] epoch #402 | backtrack iters: 0
2022-04-23 14:21:37 | [train_policy] epoch #402 | optimization finished
2022-04-23 14:21:37 | [train_policy] epoch #402 | Computing KL after
2022-04-23 14:21:37 | [train_policy] epoch #402 | Computing loss after
2022-04-23 14:21:37 | [train_policy] epoch #402 | Fitting baseline...
2022-04-23 14:21:37 | [train_policy] epoch #402 | Saving snapshot...
2022-04-23 14:21:37 | [train_policy] epoch #402 | Saved
2022-04-23 14:21:37 | [train_policy] epoch #402 | Time 144.89 s
2022-04-23 14:21:37 | [train_policy] epoch #402 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.122081
Evaluation/AverageDiscountedReturn          -65.3396
Evaluation/AverageReturn                    -65.3396
Evaluation/CompletionRate                     0
Evaluation/Iteration                        402
Evaluation/MaxReturn                        -30.0009
Evaluation/MinReturn                      -2063
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.529
Extras/EpisodeRewardMean                    -63.2747
LinearFeatureBaseline/ExplainedVariance       0.0658114
PolicyExecTime                                0.0984244
ProcessExecTime                               0.0116076
TotalEnvSteps                            407836
policy/Entropy                               -0.297191
policy/KL                                     0.0099147
policy/KLBefore                               0
policy/LossAfter                             -0.0239773
policy/LossBefore                            -2.21456e-08
policy/Perplexity                             0.742902
policy/dLoss                                  0.0239773
---------------------------------------  ----------------
2022-04-23 14:21:37 | [train_policy] epoch #403 | Obtaining samples for iteration 403...
2022-04-23 14:21:37 | [train_policy] epoch #403 | Logging diagnostics...
2022-04-23 14:21:37 | [train_policy] epoch #403 | Optimizing policy...
2022-04-23 14:21:37 | [train_policy] epoch #403 | Computing loss before
2022-04-23 14:21:37 | [train_policy] epoch #403 | Computing KL before
2022-04-23 14:21:37 | [train_policy] epoch #403 | Optimizing
2022-04-23 14:21:37 | [train_policy] epoch #403 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:37 | [train_policy] epoch #403 | computing loss before
2022-04-23 14:21:37 | [train_policy] epoch #403 | computing gradient
2022-04-23 14:21:37 | [train_policy] epoch #403 | gradient computed
2022-04-23 14:21:37 | [train_policy] epoch #403 | computing descent direction
2022-04-23 14:21:37 | [train_policy] epoch #403 | descent direction computed
2022-04-23 14:21:37 | [train_policy] epoch #403 | backtrack iters: 0
2022-04-23 14:21:37 | [train_policy] epoch #403 | optimization finished
2022-04-23 14:21:37 | [train_policy] epoch #403 | Computing KL after
2022-04-23 14:21:37 | [train_policy] epoch #403 | Computing loss after
2022-04-23 14:21:37 | [train_policy] epoch #403 | Fitting baseline...
2022-04-23 14:21:37 | [train_policy] epoch #403 | Saving snapshot...
2022-04-23 14:21:37 | [train_policy] epoch #403 | Saved
2022-04-23 14:21:37 | [train_policy] epoch #403 | Time 145.23 s
2022-04-23 14:21:37 | [train_policy] epoch #403 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116565
Evaluation/AverageDiscountedReturn          -65.1859
Evaluation/AverageReturn                    -65.1859
Evaluation/CompletionRate                     0
Evaluation/Iteration                        403
Evaluation/MaxReturn                        -31.3503
Evaluation/MinReturn                      -2063.52
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.609
Extras/EpisodeRewardMean                    -63.4882
LinearFeatureBaseline/ExplainedVariance       0.106565
PolicyExecTime                                0.0945759
ProcessExecTime                               0.010865
TotalEnvSteps                            408848
policy/Entropy                               -0.314123
policy/KL                                     0.00881698
policy/KLBefore                               0
policy/LossAfter                             -0.0215744
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.730429
policy/dLoss                                  0.0215744
---------------------------------------  ----------------
2022-04-23 14:21:37 | [train_policy] epoch #404 | Obtaining samples for iteration 404...
2022-04-23 14:21:37 | [train_policy] epoch #404 | Logging diagnostics...
2022-04-23 14:21:37 | [train_policy] epoch #404 | Optimizing policy...
2022-04-23 14:21:37 | [train_policy] epoch #404 | Computing loss before
2022-04-23 14:21:37 | [train_policy] epoch #404 | Computing KL before
2022-04-23 14:21:37 | [train_policy] epoch #404 | Optimizing
2022-04-23 14:21:37 | [train_policy] epoch #404 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:37 | [train_policy] epoch #404 | computing loss before
2022-04-23 14:21:37 | [train_policy] epoch #404 | computing gradient
2022-04-23 14:21:37 | [train_policy] epoch #404 | gradient computed
2022-04-23 14:21:37 | [train_policy] epoch #404 | computing descent direction
2022-04-23 14:21:37 | [train_policy] epoch #404 | descent direction computed
2022-04-23 14:21:37 | [train_policy] epoch #404 | backtrack iters: 1
2022-04-23 14:21:37 | [train_policy] epoch #404 | optimization finished
2022-04-23 14:21:37 | [train_policy] epoch #404 | Computing KL after
2022-04-23 14:21:37 | [train_policy] epoch #404 | Computing loss after
2022-04-23 14:21:37 | [train_policy] epoch #404 | Fitting baseline...
2022-04-23 14:21:37 | [train_policy] epoch #404 | Saving snapshot...
2022-04-23 14:21:37 | [train_policy] epoch #404 | Saved
2022-04-23 14:21:37 | [train_policy] epoch #404 | Time 145.58 s
2022-04-23 14:21:37 | [train_policy] epoch #404 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11635
Evaluation/AverageDiscountedReturn          -63.6164
Evaluation/AverageReturn                    -63.6164
Evaluation/CompletionRate                     0
Evaluation/Iteration                        404
Evaluation/MaxReturn                        -33.8852
Evaluation/MinReturn                      -2064.49
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.841
Extras/EpisodeRewardMean                    -62.0221
LinearFeatureBaseline/ExplainedVariance       0.114079
PolicyExecTime                                0.0990434
ProcessExecTime                               0.0108626
TotalEnvSteps                            409860
policy/Entropy                               -0.310915
policy/KL                                     0.00706309
policy/KLBefore                               0
policy/LossAfter                             -0.0182898
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.732776
policy/dLoss                                  0.0182898
---------------------------------------  ----------------
2022-04-23 14:21:37 | [train_policy] epoch #405 | Obtaining samples for iteration 405...
2022-04-23 14:21:38 | [train_policy] epoch #405 | Logging diagnostics...
2022-04-23 14:21:38 | [train_policy] epoch #405 | Optimizing policy...
2022-04-23 14:21:38 | [train_policy] epoch #405 | Computing loss before
2022-04-23 14:21:38 | [train_policy] epoch #405 | Computing KL before
2022-04-23 14:21:38 | [train_policy] epoch #405 | Optimizing
2022-04-23 14:21:38 | [train_policy] epoch #405 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:38 | [train_policy] epoch #405 | computing loss before
2022-04-23 14:21:38 | [train_policy] epoch #405 | computing gradient
2022-04-23 14:21:38 | [train_policy] epoch #405 | gradient computed
2022-04-23 14:21:38 | [train_policy] epoch #405 | computing descent direction
2022-04-23 14:21:38 | [train_policy] epoch #405 | descent direction computed
2022-04-23 14:21:38 | [train_policy] epoch #405 | backtrack iters: 0
2022-04-23 14:21:38 | [train_policy] epoch #405 | optimization finished
2022-04-23 14:21:38 | [train_policy] epoch #405 | Computing KL after
2022-04-23 14:21:38 | [train_policy] epoch #405 | Computing loss after
2022-04-23 14:21:38 | [train_policy] epoch #405 | Fitting baseline...
2022-04-23 14:21:38 | [train_policy] epoch #405 | Saving snapshot...
2022-04-23 14:21:38 | [train_policy] epoch #405 | Saved
2022-04-23 14:21:38 | [train_policy] epoch #405 | Time 145.94 s
2022-04-23 14:21:38 | [train_policy] epoch #405 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.122122
Evaluation/AverageDiscountedReturn          -43.1537
Evaluation/AverageReturn                    -43.1537
Evaluation/CompletionRate                     0
Evaluation/Iteration                        405
Evaluation/MaxReturn                        -32.9037
Evaluation/MinReturn                        -63.7154
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.8042
Extras/EpisodeRewardMean                    -42.9449
LinearFeatureBaseline/ExplainedVariance     -50.4017
PolicyExecTime                                0.107053
ProcessExecTime                               0.0121059
TotalEnvSteps                            410872
policy/Entropy                               -0.297001
policy/KL                                     0.0092459
policy/KLBefore                               0
policy/LossAfter                             -0.0162138
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.743043
policy/dLoss                                  0.0162138
---------------------------------------  ----------------
2022-04-23 14:21:38 | [train_policy] epoch #406 | Obtaining samples for iteration 406...
2022-04-23 14:21:38 | [train_policy] epoch #406 | Logging diagnostics...
2022-04-23 14:21:38 | [train_policy] epoch #406 | Optimizing policy...
2022-04-23 14:21:38 | [train_policy] epoch #406 | Computing loss before
2022-04-23 14:21:38 | [train_policy] epoch #406 | Computing KL before
2022-04-23 14:21:38 | [train_policy] epoch #406 | Optimizing
2022-04-23 14:21:38 | [train_policy] epoch #406 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:38 | [train_policy] epoch #406 | computing loss before
2022-04-23 14:21:38 | [train_policy] epoch #406 | computing gradient
2022-04-23 14:21:38 | [train_policy] epoch #406 | gradient computed
2022-04-23 14:21:38 | [train_policy] epoch #406 | computing descent direction
2022-04-23 14:21:38 | [train_policy] epoch #406 | descent direction computed
2022-04-23 14:21:38 | [train_policy] epoch #406 | backtrack iters: 0
2022-04-23 14:21:38 | [train_policy] epoch #406 | optimization finished
2022-04-23 14:21:38 | [train_policy] epoch #406 | Computing KL after
2022-04-23 14:21:38 | [train_policy] epoch #406 | Computing loss after
2022-04-23 14:21:38 | [train_policy] epoch #406 | Fitting baseline...
2022-04-23 14:21:38 | [train_policy] epoch #406 | Saving snapshot...
2022-04-23 14:21:38 | [train_policy] epoch #406 | Saved
2022-04-23 14:21:38 | [train_policy] epoch #406 | Time 146.30 s
2022-04-23 14:21:38 | [train_policy] epoch #406 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.123172
Evaluation/AverageDiscountedReturn          -42.3098
Evaluation/AverageReturn                    -42.3098
Evaluation/CompletionRate                     0
Evaluation/Iteration                        406
Evaluation/MaxReturn                        -32.6673
Evaluation/MinReturn                        -63.6753
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.71112
Extras/EpisodeRewardMean                    -42.37
LinearFeatureBaseline/ExplainedVariance       0.907388
PolicyExecTime                                0.11124
ProcessExecTime                               0.0123441
TotalEnvSteps                            411884
policy/Entropy                               -0.306548
policy/KL                                     0.00964335
policy/KLBefore                               0
policy/LossAfter                             -0.0154423
policy/LossBefore                            -2.35591e-10
policy/Perplexity                             0.735983
policy/dLoss                                  0.0154423
---------------------------------------  ----------------
2022-04-23 14:21:38 | [train_policy] epoch #407 | Obtaining samples for iteration 407...
2022-04-23 14:21:38 | [train_policy] epoch #407 | Logging diagnostics...
2022-04-23 14:21:38 | [train_policy] epoch #407 | Optimizing policy...
2022-04-23 14:21:38 | [train_policy] epoch #407 | Computing loss before
2022-04-23 14:21:38 | [train_policy] epoch #407 | Computing KL before
2022-04-23 14:21:38 | [train_policy] epoch #407 | Optimizing
2022-04-23 14:21:38 | [train_policy] epoch #407 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:38 | [train_policy] epoch #407 | computing loss before
2022-04-23 14:21:38 | [train_policy] epoch #407 | computing gradient
2022-04-23 14:21:38 | [train_policy] epoch #407 | gradient computed
2022-04-23 14:21:38 | [train_policy] epoch #407 | computing descent direction
2022-04-23 14:21:38 | [train_policy] epoch #407 | descent direction computed
2022-04-23 14:21:38 | [train_policy] epoch #407 | backtrack iters: 0
2022-04-23 14:21:38 | [train_policy] epoch #407 | optimization finished
2022-04-23 14:21:38 | [train_policy] epoch #407 | Computing KL after
2022-04-23 14:21:38 | [train_policy] epoch #407 | Computing loss after
2022-04-23 14:21:38 | [train_policy] epoch #407 | Fitting baseline...
2022-04-23 14:21:38 | [train_policy] epoch #407 | Saving snapshot...
2022-04-23 14:21:38 | [train_policy] epoch #407 | Saved
2022-04-23 14:21:38 | [train_policy] epoch #407 | Time 146.65 s
2022-04-23 14:21:38 | [train_policy] epoch #407 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116952
Evaluation/AverageDiscountedReturn         -108.377
Evaluation/AverageReturn                   -108.377
Evaluation/CompletionRate                     0
Evaluation/Iteration                        407
Evaluation/MaxReturn                        -30.7088
Evaluation/MinReturn                      -4051.11
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        463.688
Extras/EpisodeRewardMean                   -103.115
LinearFeatureBaseline/ExplainedVariance       0.00589157
PolicyExecTime                                0.104935
ProcessExecTime                               0.0115535
TotalEnvSteps                            412896
policy/Entropy                               -0.31557
policy/KL                                     0.00926331
policy/KLBefore                               0
policy/LossAfter                             -0.0283247
policy/LossBefore                             2.02609e-08
policy/Perplexity                             0.729373
policy/dLoss                                  0.0283247
---------------------------------------  ----------------
2022-04-23 14:21:38 | [train_policy] epoch #408 | Obtaining samples for iteration 408...
2022-04-23 14:21:39 | [train_policy] epoch #408 | Logging diagnostics...
2022-04-23 14:21:39 | [train_policy] epoch #408 | Optimizing policy...
2022-04-23 14:21:39 | [train_policy] epoch #408 | Computing loss before
2022-04-23 14:21:39 | [train_policy] epoch #408 | Computing KL before
2022-04-23 14:21:39 | [train_policy] epoch #408 | Optimizing
2022-04-23 14:21:39 | [train_policy] epoch #408 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:39 | [train_policy] epoch #408 | computing loss before
2022-04-23 14:21:39 | [train_policy] epoch #408 | computing gradient
2022-04-23 14:21:39 | [train_policy] epoch #408 | gradient computed
2022-04-23 14:21:39 | [train_policy] epoch #408 | computing descent direction
2022-04-23 14:21:39 | [train_policy] epoch #408 | descent direction computed
2022-04-23 14:21:39 | [train_policy] epoch #408 | backtrack iters: 0
2022-04-23 14:21:39 | [train_policy] epoch #408 | optimization finished
2022-04-23 14:21:39 | [train_policy] epoch #408 | Computing KL after
2022-04-23 14:21:39 | [train_policy] epoch #408 | Computing loss after
2022-04-23 14:21:39 | [train_policy] epoch #408 | Fitting baseline...
2022-04-23 14:21:39 | [train_policy] epoch #408 | Saving snapshot...
2022-04-23 14:21:39 | [train_policy] epoch #408 | Saved
2022-04-23 14:21:39 | [train_policy] epoch #408 | Time 147.00 s
2022-04-23 14:21:39 | [train_policy] epoch #408 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116616
Evaluation/AverageDiscountedReturn          -43.0302
Evaluation/AverageReturn                    -43.0302
Evaluation/CompletionRate                     0
Evaluation/Iteration                        408
Evaluation/MaxReturn                        -29.4805
Evaluation/MinReturn                        -65.7945
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.62833
Extras/EpisodeRewardMean                    -43.2419
LinearFeatureBaseline/ExplainedVariance   -3429.67
PolicyExecTime                                0.104593
ProcessExecTime                               0.011395
TotalEnvSteps                            413908
policy/Entropy                               -0.310531
policy/KL                                     0.00971542
policy/KLBefore                               0
policy/LossAfter                             -0.0474152
policy/LossBefore                             1.34287e-08
policy/Perplexity                             0.733057
policy/dLoss                                  0.0474152
---------------------------------------  ----------------
2022-04-23 14:21:39 | [train_policy] epoch #409 | Obtaining samples for iteration 409...
2022-04-23 14:21:39 | [train_policy] epoch #409 | Logging diagnostics...
2022-04-23 14:21:39 | [train_policy] epoch #409 | Optimizing policy...
2022-04-23 14:21:39 | [train_policy] epoch #409 | Computing loss before
2022-04-23 14:21:39 | [train_policy] epoch #409 | Computing KL before
2022-04-23 14:21:39 | [train_policy] epoch #409 | Optimizing
2022-04-23 14:21:39 | [train_policy] epoch #409 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:39 | [train_policy] epoch #409 | computing loss before
2022-04-23 14:21:39 | [train_policy] epoch #409 | computing gradient
2022-04-23 14:21:39 | [train_policy] epoch #409 | gradient computed
2022-04-23 14:21:39 | [train_policy] epoch #409 | computing descent direction
2022-04-23 14:21:39 | [train_policy] epoch #409 | descent direction computed
2022-04-23 14:21:39 | [train_policy] epoch #409 | backtrack iters: 0
2022-04-23 14:21:39 | [train_policy] epoch #409 | optimization finished
2022-04-23 14:21:39 | [train_policy] epoch #409 | Computing KL after
2022-04-23 14:21:39 | [train_policy] epoch #409 | Computing loss after
2022-04-23 14:21:39 | [train_policy] epoch #409 | Fitting baseline...
2022-04-23 14:21:39 | [train_policy] epoch #409 | Saving snapshot...
2022-04-23 14:21:39 | [train_policy] epoch #409 | Saved
2022-04-23 14:21:39 | [train_policy] epoch #409 | Time 147.35 s
2022-04-23 14:21:39 | [train_policy] epoch #409 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116057
Evaluation/AverageDiscountedReturn          -63.1602
Evaluation/AverageReturn                    -63.1602
Evaluation/CompletionRate                     0
Evaluation/Iteration                        409
Evaluation/MaxReturn                        -28.7916
Evaluation/MinReturn                      -2063.71
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.816
Extras/EpisodeRewardMean                    -61.4408
LinearFeatureBaseline/ExplainedVariance       0.0113224
PolicyExecTime                                0.106931
ProcessExecTime                               0.0114572
TotalEnvSteps                            414920
policy/Entropy                               -0.246141
policy/KL                                     0.00916866
policy/KLBefore                               0
policy/LossAfter                             -0.0198775
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             0.781812
policy/dLoss                                  0.0198775
---------------------------------------  ----------------
2022-04-23 14:21:39 | [train_policy] epoch #410 | Obtaining samples for iteration 410...
2022-04-23 14:21:39 | [train_policy] epoch #410 | Logging diagnostics...
2022-04-23 14:21:39 | [train_policy] epoch #410 | Optimizing policy...
2022-04-23 14:21:39 | [train_policy] epoch #410 | Computing loss before
2022-04-23 14:21:39 | [train_policy] epoch #410 | Computing KL before
2022-04-23 14:21:39 | [train_policy] epoch #410 | Optimizing
2022-04-23 14:21:39 | [train_policy] epoch #410 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:39 | [train_policy] epoch #410 | computing loss before
2022-04-23 14:21:39 | [train_policy] epoch #410 | computing gradient
2022-04-23 14:21:39 | [train_policy] epoch #410 | gradient computed
2022-04-23 14:21:39 | [train_policy] epoch #410 | computing descent direction
2022-04-23 14:21:39 | [train_policy] epoch #410 | descent direction computed
2022-04-23 14:21:39 | [train_policy] epoch #410 | backtrack iters: 1
2022-04-23 14:21:39 | [train_policy] epoch #410 | optimization finished
2022-04-23 14:21:39 | [train_policy] epoch #410 | Computing KL after
2022-04-23 14:21:39 | [train_policy] epoch #410 | Computing loss after
2022-04-23 14:21:39 | [train_policy] epoch #410 | Fitting baseline...
2022-04-23 14:21:39 | [train_policy] epoch #410 | Saving snapshot...
2022-04-23 14:21:39 | [train_policy] epoch #410 | Saved
2022-04-23 14:21:39 | [train_policy] epoch #410 | Time 147.70 s
2022-04-23 14:21:39 | [train_policy] epoch #410 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11635
Evaluation/AverageDiscountedReturn         -109.966
Evaluation/AverageReturn                   -109.966
Evaluation/CompletionRate                     0
Evaluation/Iteration                        410
Evaluation/MaxReturn                        -32.2837
Evaluation/MinReturn                      -2066.49
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.051
Extras/EpisodeRewardMean                   -104.598
LinearFeatureBaseline/ExplainedVariance       0.191995
PolicyExecTime                                0.103778
ProcessExecTime                               0.0116074
TotalEnvSteps                            415932
policy/Entropy                               -0.250502
policy/KL                                     0.00643126
policy/KLBefore                               0
policy/LossAfter                             -0.0110091
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.77841
policy/dLoss                                  0.0110091
---------------------------------------  ----------------
2022-04-23 14:21:39 | [train_policy] epoch #411 | Obtaining samples for iteration 411...
2022-04-23 14:21:40 | [train_policy] epoch #411 | Logging diagnostics...
2022-04-23 14:21:40 | [train_policy] epoch #411 | Optimizing policy...
2022-04-23 14:21:40 | [train_policy] epoch #411 | Computing loss before
2022-04-23 14:21:40 | [train_policy] epoch #411 | Computing KL before
2022-04-23 14:21:40 | [train_policy] epoch #411 | Optimizing
2022-04-23 14:21:40 | [train_policy] epoch #411 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:40 | [train_policy] epoch #411 | computing loss before
2022-04-23 14:21:40 | [train_policy] epoch #411 | computing gradient
2022-04-23 14:21:40 | [train_policy] epoch #411 | gradient computed
2022-04-23 14:21:40 | [train_policy] epoch #411 | computing descent direction
2022-04-23 14:21:40 | [train_policy] epoch #411 | descent direction computed
2022-04-23 14:21:40 | [train_policy] epoch #411 | backtrack iters: 1
2022-04-23 14:21:40 | [train_policy] epoch #411 | optimization finished
2022-04-23 14:21:40 | [train_policy] epoch #411 | Computing KL after
2022-04-23 14:21:40 | [train_policy] epoch #411 | Computing loss after
2022-04-23 14:21:40 | [train_policy] epoch #411 | Fitting baseline...
2022-04-23 14:21:40 | [train_policy] epoch #411 | Saving snapshot...
2022-04-23 14:21:40 | [train_policy] epoch #411 | Saved
2022-04-23 14:21:40 | [train_policy] epoch #411 | Time 148.06 s
2022-04-23 14:21:40 | [train_policy] epoch #411 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.125068
Evaluation/AverageDiscountedReturn          -41.9738
Evaluation/AverageReturn                    -41.9738
Evaluation/CompletionRate                     0
Evaluation/Iteration                        411
Evaluation/MaxReturn                        -29.8685
Evaluation/MinReturn                        -58.5422
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.66442
Extras/EpisodeRewardMean                    -42.1973
LinearFeatureBaseline/ExplainedVariance    -109.098
PolicyExecTime                                0.11
ProcessExecTime                               0.0124977
TotalEnvSteps                            416944
policy/Entropy                               -0.250013
policy/KL                                     0.00653982
policy/KLBefore                               0
policy/LossAfter                             -0.018172
policy/LossBefore                             2.3088e-08
policy/Perplexity                             0.778791
policy/dLoss                                  0.018172
---------------------------------------  ---------------
2022-04-23 14:21:40 | [train_policy] epoch #412 | Obtaining samples for iteration 412...
2022-04-23 14:21:40 | [train_policy] epoch #412 | Logging diagnostics...
2022-04-23 14:21:40 | [train_policy] epoch #412 | Optimizing policy...
2022-04-23 14:21:40 | [train_policy] epoch #412 | Computing loss before
2022-04-23 14:21:40 | [train_policy] epoch #412 | Computing KL before
2022-04-23 14:21:40 | [train_policy] epoch #412 | Optimizing
2022-04-23 14:21:40 | [train_policy] epoch #412 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:40 | [train_policy] epoch #412 | computing loss before
2022-04-23 14:21:40 | [train_policy] epoch #412 | computing gradient
2022-04-23 14:21:40 | [train_policy] epoch #412 | gradient computed
2022-04-23 14:21:40 | [train_policy] epoch #412 | computing descent direction
2022-04-23 14:21:40 | [train_policy] epoch #412 | descent direction computed
2022-04-23 14:21:40 | [train_policy] epoch #412 | backtrack iters: 0
2022-04-23 14:21:40 | [train_policy] epoch #412 | optimization finished
2022-04-23 14:21:40 | [train_policy] epoch #412 | Computing KL after
2022-04-23 14:21:40 | [train_policy] epoch #412 | Computing loss after
2022-04-23 14:21:40 | [train_policy] epoch #412 | Fitting baseline...
2022-04-23 14:21:40 | [train_policy] epoch #412 | Saving snapshot...
2022-04-23 14:21:40 | [train_policy] epoch #412 | Saved
2022-04-23 14:21:40 | [train_policy] epoch #412 | Time 148.42 s
2022-04-23 14:21:40 | [train_policy] epoch #412 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.116299
Evaluation/AverageDiscountedReturn          -65.369
Evaluation/AverageReturn                    -65.369
Evaluation/CompletionRate                     0
Evaluation/Iteration                        412
Evaluation/MaxReturn                        -29.5917
Evaluation/MinReturn                      -2073.38
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.632
Extras/EpisodeRewardMean                    -63.8095
LinearFeatureBaseline/ExplainedVariance       0.00996104
PolicyExecTime                                0.108522
ProcessExecTime                               0.0113025
TotalEnvSteps                            417956
policy/Entropy                               -0.27578
policy/KL                                     0.00969023
policy/KLBefore                               0
policy/LossAfter                             -0.0224311
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.75898
policy/dLoss                                  0.0224311
---------------------------------------  ---------------
2022-04-23 14:21:40 | [train_policy] epoch #413 | Obtaining samples for iteration 413...
2022-04-23 14:21:40 | [train_policy] epoch #413 | Logging diagnostics...
2022-04-23 14:21:40 | [train_policy] epoch #413 | Optimizing policy...
2022-04-23 14:21:40 | [train_policy] epoch #413 | Computing loss before
2022-04-23 14:21:40 | [train_policy] epoch #413 | Computing KL before
2022-04-23 14:21:40 | [train_policy] epoch #413 | Optimizing
2022-04-23 14:21:40 | [train_policy] epoch #413 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:40 | [train_policy] epoch #413 | computing loss before
2022-04-23 14:21:40 | [train_policy] epoch #413 | computing gradient
2022-04-23 14:21:40 | [train_policy] epoch #413 | gradient computed
2022-04-23 14:21:40 | [train_policy] epoch #413 | computing descent direction
2022-04-23 14:21:41 | [train_policy] epoch #413 | descent direction computed
2022-04-23 14:21:41 | [train_policy] epoch #413 | backtrack iters: 3
2022-04-23 14:21:41 | [train_policy] epoch #413 | optimization finished
2022-04-23 14:21:41 | [train_policy] epoch #413 | Computing KL after
2022-04-23 14:21:41 | [train_policy] epoch #413 | Computing loss after
2022-04-23 14:21:41 | [train_policy] epoch #413 | Fitting baseline...
2022-04-23 14:21:41 | [train_policy] epoch #413 | Saving snapshot...
2022-04-23 14:21:41 | [train_policy] epoch #413 | Saved
2022-04-23 14:21:41 | [train_policy] epoch #413 | Time 148.79 s
2022-04-23 14:21:41 | [train_policy] epoch #413 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.12031
Evaluation/AverageDiscountedReturn          -64.413
Evaluation/AverageReturn                    -64.413
Evaluation/CompletionRate                     0
Evaluation/Iteration                        413
Evaluation/MaxReturn                        -31.4792
Evaluation/MinReturn                      -2063.38
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.677
Extras/EpisodeRewardMean                    -62.4785
LinearFeatureBaseline/ExplainedVariance       0.101578
PolicyExecTime                                0.106222
ProcessExecTime                               0.012183
TotalEnvSteps                            418968
policy/Entropy                               -0.283419
policy/KL                                     0.00287735
policy/KLBefore                               0
policy/LossAfter                             -0.000149992
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.753204
policy/dLoss                                  0.000149988
---------------------------------------  ----------------
2022-04-23 14:21:41 | [train_policy] epoch #414 | Obtaining samples for iteration 414...
2022-04-23 14:21:41 | [train_policy] epoch #414 | Logging diagnostics...
2022-04-23 14:21:41 | [train_policy] epoch #414 | Optimizing policy...
2022-04-23 14:21:41 | [train_policy] epoch #414 | Computing loss before
2022-04-23 14:21:41 | [train_policy] epoch #414 | Computing KL before
2022-04-23 14:21:41 | [train_policy] epoch #414 | Optimizing
2022-04-23 14:21:41 | [train_policy] epoch #414 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:41 | [train_policy] epoch #414 | computing loss before
2022-04-23 14:21:41 | [train_policy] epoch #414 | computing gradient
2022-04-23 14:21:41 | [train_policy] epoch #414 | gradient computed
2022-04-23 14:21:41 | [train_policy] epoch #414 | computing descent direction
2022-04-23 14:21:41 | [train_policy] epoch #414 | descent direction computed
2022-04-23 14:21:41 | [train_policy] epoch #414 | backtrack iters: 0
2022-04-23 14:21:41 | [train_policy] epoch #414 | optimization finished
2022-04-23 14:21:41 | [train_policy] epoch #414 | Computing KL after
2022-04-23 14:21:41 | [train_policy] epoch #414 | Computing loss after
2022-04-23 14:21:41 | [train_policy] epoch #414 | Fitting baseline...
2022-04-23 14:21:41 | [train_policy] epoch #414 | Saving snapshot...
2022-04-23 14:21:41 | [train_policy] epoch #414 | Saved
2022-04-23 14:21:41 | [train_policy] epoch #414 | Time 149.14 s
2022-04-23 14:21:41 | [train_policy] epoch #414 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116781
Evaluation/AverageDiscountedReturn          -85.312
Evaluation/AverageReturn                    -85.312
Evaluation/CompletionRate                     0
Evaluation/Iteration                        414
Evaluation/MaxReturn                        -29.7907
Evaluation/MinReturn                      -2065.06
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.117
Extras/EpisodeRewardMean                    -82.0131
LinearFeatureBaseline/ExplainedVariance       0.180597
PolicyExecTime                                0.105531
ProcessExecTime                               0.0113692
TotalEnvSteps                            419980
policy/Entropy                               -0.28586
policy/KL                                     0.00887154
policy/KLBefore                               0
policy/LossAfter                             -0.0338875
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.751368
policy/dLoss                                  0.0338875
---------------------------------------  ----------------
2022-04-23 14:21:41 | [train_policy] epoch #415 | Obtaining samples for iteration 415...
2022-04-23 14:21:41 | [train_policy] epoch #415 | Logging diagnostics...
2022-04-23 14:21:41 | [train_policy] epoch #415 | Optimizing policy...
2022-04-23 14:21:41 | [train_policy] epoch #415 | Computing loss before
2022-04-23 14:21:41 | [train_policy] epoch #415 | Computing KL before
2022-04-23 14:21:41 | [train_policy] epoch #415 | Optimizing
2022-04-23 14:21:41 | [train_policy] epoch #415 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:41 | [train_policy] epoch #415 | computing loss before
2022-04-23 14:21:41 | [train_policy] epoch #415 | computing gradient
2022-04-23 14:21:41 | [train_policy] epoch #415 | gradient computed
2022-04-23 14:21:41 | [train_policy] epoch #415 | computing descent direction
2022-04-23 14:21:41 | [train_policy] epoch #415 | descent direction computed
2022-04-23 14:21:41 | [train_policy] epoch #415 | backtrack iters: 1
2022-04-23 14:21:41 | [train_policy] epoch #415 | optimization finished
2022-04-23 14:21:41 | [train_policy] epoch #415 | Computing KL after
2022-04-23 14:21:41 | [train_policy] epoch #415 | Computing loss after
2022-04-23 14:21:41 | [train_policy] epoch #415 | Fitting baseline...
2022-04-23 14:21:41 | [train_policy] epoch #415 | Saving snapshot...
2022-04-23 14:21:41 | [train_policy] epoch #415 | Saved
2022-04-23 14:21:41 | [train_policy] epoch #415 | Time 149.49 s
2022-04-23 14:21:41 | [train_policy] epoch #415 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116958
Evaluation/AverageDiscountedReturn          -44.1089
Evaluation/AverageReturn                    -44.1089
Evaluation/CompletionRate                     0
Evaluation/Iteration                        415
Evaluation/MaxReturn                        -32.5039
Evaluation/MinReturn                        -65.8656
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.82179
Extras/EpisodeRewardMean                    -43.7685
LinearFeatureBaseline/ExplainedVariance    -121.84
PolicyExecTime                                0.0949614
ProcessExecTime                               0.0117109
TotalEnvSteps                            420992
policy/Entropy                               -0.289477
policy/KL                                     0.00655094
policy/KLBefore                               0
policy/LossAfter                             -0.017989
policy/LossBefore                             2.52083e-08
policy/Perplexity                             0.748655
policy/dLoss                                  0.0179891
---------------------------------------  ----------------
2022-04-23 14:21:41 | [train_policy] epoch #416 | Obtaining samples for iteration 416...
2022-04-23 14:21:42 | [train_policy] epoch #416 | Logging diagnostics...
2022-04-23 14:21:42 | [train_policy] epoch #416 | Optimizing policy...
2022-04-23 14:21:42 | [train_policy] epoch #416 | Computing loss before
2022-04-23 14:21:42 | [train_policy] epoch #416 | Computing KL before
2022-04-23 14:21:42 | [train_policy] epoch #416 | Optimizing
2022-04-23 14:21:42 | [train_policy] epoch #416 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:42 | [train_policy] epoch #416 | computing loss before
2022-04-23 14:21:42 | [train_policy] epoch #416 | computing gradient
2022-04-23 14:21:42 | [train_policy] epoch #416 | gradient computed
2022-04-23 14:21:42 | [train_policy] epoch #416 | computing descent direction
2022-04-23 14:21:42 | [train_policy] epoch #416 | descent direction computed
2022-04-23 14:21:42 | [train_policy] epoch #416 | backtrack iters: 1
2022-04-23 14:21:42 | [train_policy] epoch #416 | optimization finished
2022-04-23 14:21:42 | [train_policy] epoch #416 | Computing KL after
2022-04-23 14:21:42 | [train_policy] epoch #416 | Computing loss after
2022-04-23 14:21:42 | [train_policy] epoch #416 | Fitting baseline...
2022-04-23 14:21:42 | [train_policy] epoch #416 | Saving snapshot...
2022-04-23 14:21:42 | [train_policy] epoch #416 | Saved
2022-04-23 14:21:42 | [train_policy] epoch #416 | Time 149.82 s
2022-04-23 14:21:42 | [train_policy] epoch #416 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.115295
Evaluation/AverageDiscountedReturn          -42.2892
Evaluation/AverageReturn                    -42.2892
Evaluation/CompletionRate                     0
Evaluation/Iteration                        416
Evaluation/MaxReturn                        -31.3796
Evaluation/MinReturn                        -64.9652
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.88158
Extras/EpisodeRewardMean                    -42.485
LinearFeatureBaseline/ExplainedVariance       0.91645
PolicyExecTime                                0.093148
ProcessExecTime                               0.0109499
TotalEnvSteps                            422004
policy/Entropy                               -0.310558
policy/KL                                     0.00684116
policy/KLBefore                               0
policy/LossAfter                             -0.0186429
policy/LossBefore                             8.2457e-10
policy/Perplexity                             0.733038
policy/dLoss                                  0.0186429
---------------------------------------  ---------------
2022-04-23 14:21:42 | [train_policy] epoch #417 | Obtaining samples for iteration 417...
2022-04-23 14:21:42 | [train_policy] epoch #417 | Logging diagnostics...
2022-04-23 14:21:42 | [train_policy] epoch #417 | Optimizing policy...
2022-04-23 14:21:42 | [train_policy] epoch #417 | Computing loss before
2022-04-23 14:21:42 | [train_policy] epoch #417 | Computing KL before
2022-04-23 14:21:42 | [train_policy] epoch #417 | Optimizing
2022-04-23 14:21:42 | [train_policy] epoch #417 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:42 | [train_policy] epoch #417 | computing loss before
2022-04-23 14:21:42 | [train_policy] epoch #417 | computing gradient
2022-04-23 14:21:42 | [train_policy] epoch #417 | gradient computed
2022-04-23 14:21:42 | [train_policy] epoch #417 | computing descent direction
2022-04-23 14:21:42 | [train_policy] epoch #417 | descent direction computed
2022-04-23 14:21:42 | [train_policy] epoch #417 | backtrack iters: 0
2022-04-23 14:21:42 | [train_policy] epoch #417 | optimization finished
2022-04-23 14:21:42 | [train_policy] epoch #417 | Computing KL after
2022-04-23 14:21:42 | [train_policy] epoch #417 | Computing loss after
2022-04-23 14:21:42 | [train_policy] epoch #417 | Fitting baseline...
2022-04-23 14:21:42 | [train_policy] epoch #417 | Saving snapshot...
2022-04-23 14:21:42 | [train_policy] epoch #417 | Saved
2022-04-23 14:21:42 | [train_policy] epoch #417 | Time 150.16 s
2022-04-23 14:21:42 | [train_policy] epoch #417 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115625
Evaluation/AverageDiscountedReturn          -42.0173
Evaluation/AverageReturn                    -42.0173
Evaluation/CompletionRate                     0
Evaluation/Iteration                        417
Evaluation/MaxReturn                        -28.4576
Evaluation/MinReturn                        -71.8067
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.23838
Extras/EpisodeRewardMean                    -41.8771
LinearFeatureBaseline/ExplainedVariance       0.827833
PolicyExecTime                                0.0941846
ProcessExecTime                               0.01103
TotalEnvSteps                            423016
policy/Entropy                               -0.329517
policy/KL                                     0.0096047
policy/KLBefore                               0
policy/LossAfter                             -0.0267061
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.719271
policy/dLoss                                  0.0267061
---------------------------------------  ----------------
2022-04-23 14:21:42 | [train_policy] epoch #418 | Obtaining samples for iteration 418...
2022-04-23 14:21:42 | [train_policy] epoch #418 | Logging diagnostics...
2022-04-23 14:21:42 | [train_policy] epoch #418 | Optimizing policy...
2022-04-23 14:21:42 | [train_policy] epoch #418 | Computing loss before
2022-04-23 14:21:42 | [train_policy] epoch #418 | Computing KL before
2022-04-23 14:21:42 | [train_policy] epoch #418 | Optimizing
2022-04-23 14:21:42 | [train_policy] epoch #418 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:42 | [train_policy] epoch #418 | computing loss before
2022-04-23 14:21:42 | [train_policy] epoch #418 | computing gradient
2022-04-23 14:21:42 | [train_policy] epoch #418 | gradient computed
2022-04-23 14:21:42 | [train_policy] epoch #418 | computing descent direction
2022-04-23 14:21:42 | [train_policy] epoch #418 | descent direction computed
2022-04-23 14:21:42 | [train_policy] epoch #418 | backtrack iters: 0
2022-04-23 14:21:42 | [train_policy] epoch #418 | optimization finished
2022-04-23 14:21:42 | [train_policy] epoch #418 | Computing KL after
2022-04-23 14:21:42 | [train_policy] epoch #418 | Computing loss after
2022-04-23 14:21:42 | [train_policy] epoch #418 | Fitting baseline...
2022-04-23 14:21:42 | [train_policy] epoch #418 | Saving snapshot...
2022-04-23 14:21:42 | [train_policy] epoch #418 | Saved
2022-04-23 14:21:42 | [train_policy] epoch #418 | Time 150.50 s
2022-04-23 14:21:42 | [train_policy] epoch #418 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.116637
Evaluation/AverageDiscountedReturn          -65.3191
Evaluation/AverageReturn                    -65.3191
Evaluation/CompletionRate                     0
Evaluation/Iteration                        418
Evaluation/MaxReturn                        -33.3163
Evaluation/MinReturn                      -2064.46
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.676
Extras/EpisodeRewardMean                    -63.0958
LinearFeatureBaseline/ExplainedVariance       0.0112924
PolicyExecTime                                0.0976768
ProcessExecTime                               0.0117104
TotalEnvSteps                            424028
policy/Entropy                               -0.293882
policy/KL                                     0.0084628
policy/KLBefore                               0
policy/LossAfter                             -0.0174472
policy/LossBefore                             1.0366e-08
policy/Perplexity                             0.745365
policy/dLoss                                  0.0174472
---------------------------------------  ---------------
2022-04-23 14:21:42 | [train_policy] epoch #419 | Obtaining samples for iteration 419...
2022-04-23 14:21:43 | [train_policy] epoch #419 | Logging diagnostics...
2022-04-23 14:21:43 | [train_policy] epoch #419 | Optimizing policy...
2022-04-23 14:21:43 | [train_policy] epoch #419 | Computing loss before
2022-04-23 14:21:43 | [train_policy] epoch #419 | Computing KL before
2022-04-23 14:21:43 | [train_policy] epoch #419 | Optimizing
2022-04-23 14:21:43 | [train_policy] epoch #419 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:43 | [train_policy] epoch #419 | computing loss before
2022-04-23 14:21:43 | [train_policy] epoch #419 | computing gradient
2022-04-23 14:21:43 | [train_policy] epoch #419 | gradient computed
2022-04-23 14:21:43 | [train_policy] epoch #419 | computing descent direction
2022-04-23 14:21:43 | [train_policy] epoch #419 | descent direction computed
2022-04-23 14:21:43 | [train_policy] epoch #419 | backtrack iters: 0
2022-04-23 14:21:43 | [train_policy] epoch #419 | optimization finished
2022-04-23 14:21:43 | [train_policy] epoch #419 | Computing KL after
2022-04-23 14:21:43 | [train_policy] epoch #419 | Computing loss after
2022-04-23 14:21:43 | [train_policy] epoch #419 | Fitting baseline...
2022-04-23 14:21:43 | [train_policy] epoch #419 | Saving snapshot...
2022-04-23 14:21:43 | [train_policy] epoch #419 | Saved
2022-04-23 14:21:43 | [train_policy] epoch #419 | Time 150.85 s
2022-04-23 14:21:43 | [train_policy] epoch #419 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117191
Evaluation/AverageDiscountedReturn         -108.246
Evaluation/AverageReturn                   -108.246
Evaluation/CompletionRate                     0
Evaluation/Iteration                        419
Evaluation/MaxReturn                        -32.7794
Evaluation/MinReturn                      -2065.64
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.108
Extras/EpisodeRewardMean                   -103.053
LinearFeatureBaseline/ExplainedVariance       0.193526
PolicyExecTime                                0.104836
ProcessExecTime                               0.0116246
TotalEnvSteps                            425040
policy/Entropy                               -0.261384
policy/KL                                     0.00907478
policy/KLBefore                               0
policy/LossAfter                             -0.0218209
policy/LossBefore                             2.85066e-08
policy/Perplexity                             0.769985
policy/dLoss                                  0.0218209
---------------------------------------  ----------------
2022-04-23 14:21:43 | [train_policy] epoch #420 | Obtaining samples for iteration 420...
2022-04-23 14:21:43 | [train_policy] epoch #420 | Logging diagnostics...
2022-04-23 14:21:43 | [train_policy] epoch #420 | Optimizing policy...
2022-04-23 14:21:43 | [train_policy] epoch #420 | Computing loss before
2022-04-23 14:21:43 | [train_policy] epoch #420 | Computing KL before
2022-04-23 14:21:43 | [train_policy] epoch #420 | Optimizing
2022-04-23 14:21:43 | [train_policy] epoch #420 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:43 | [train_policy] epoch #420 | computing loss before
2022-04-23 14:21:43 | [train_policy] epoch #420 | computing gradient
2022-04-23 14:21:43 | [train_policy] epoch #420 | gradient computed
2022-04-23 14:21:43 | [train_policy] epoch #420 | computing descent direction
2022-04-23 14:21:43 | [train_policy] epoch #420 | descent direction computed
2022-04-23 14:21:43 | [train_policy] epoch #420 | backtrack iters: 0
2022-04-23 14:21:43 | [train_policy] epoch #420 | optimization finished
2022-04-23 14:21:43 | [train_policy] epoch #420 | Computing KL after
2022-04-23 14:21:43 | [train_policy] epoch #420 | Computing loss after
2022-04-23 14:21:43 | [train_policy] epoch #420 | Fitting baseline...
2022-04-23 14:21:43 | [train_policy] epoch #420 | Saving snapshot...
2022-04-23 14:21:43 | [train_policy] epoch #420 | Saved
2022-04-23 14:21:43 | [train_policy] epoch #420 | Time 151.20 s
2022-04-23 14:21:43 | [train_policy] epoch #420 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116067
Evaluation/AverageDiscountedReturn          -42.3332
Evaluation/AverageReturn                    -42.3332
Evaluation/CompletionRate                     0
Evaluation/Iteration                        420
Evaluation/MaxReturn                        -30.6868
Evaluation/MinReturn                        -63.6983
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.36026
Extras/EpisodeRewardMean                    -42.2667
LinearFeatureBaseline/ExplainedVariance    -144.007
PolicyExecTime                                0.104825
ProcessExecTime                               0.011462
TotalEnvSteps                            426052
policy/Entropy                               -0.200123
policy/KL                                     0.00937654
policy/KLBefore                               0
policy/LossAfter                             -0.0311151
policy/LossBefore                             5.93691e-08
policy/Perplexity                             0.81863
policy/dLoss                                  0.0311152
---------------------------------------  ----------------
2022-04-23 14:21:43 | [train_policy] epoch #421 | Obtaining samples for iteration 421...
2022-04-23 14:21:43 | [train_policy] epoch #421 | Logging diagnostics...
2022-04-23 14:21:43 | [train_policy] epoch #421 | Optimizing policy...
2022-04-23 14:21:43 | [train_policy] epoch #421 | Computing loss before
2022-04-23 14:21:43 | [train_policy] epoch #421 | Computing KL before
2022-04-23 14:21:43 | [train_policy] epoch #421 | Optimizing
2022-04-23 14:21:43 | [train_policy] epoch #421 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:43 | [train_policy] epoch #421 | computing loss before
2022-04-23 14:21:43 | [train_policy] epoch #421 | computing gradient
2022-04-23 14:21:43 | [train_policy] epoch #421 | gradient computed
2022-04-23 14:21:43 | [train_policy] epoch #421 | computing descent direction
2022-04-23 14:21:43 | [train_policy] epoch #421 | descent direction computed
2022-04-23 14:21:43 | [train_policy] epoch #421 | backtrack iters: 1
2022-04-23 14:21:43 | [train_policy] epoch #421 | optimization finished
2022-04-23 14:21:43 | [train_policy] epoch #421 | Computing KL after
2022-04-23 14:21:43 | [train_policy] epoch #421 | Computing loss after
2022-04-23 14:21:43 | [train_policy] epoch #421 | Fitting baseline...
2022-04-23 14:21:43 | [train_policy] epoch #421 | Saving snapshot...
2022-04-23 14:21:43 | [train_policy] epoch #421 | Saved
2022-04-23 14:21:43 | [train_policy] epoch #421 | Time 151.55 s
2022-04-23 14:21:43 | [train_policy] epoch #421 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116403
Evaluation/AverageDiscountedReturn          -87.7681
Evaluation/AverageReturn                    -87.7681
Evaluation/CompletionRate                     0
Evaluation/Iteration                        421
Evaluation/MaxReturn                        -32.8763
Evaluation/MinReturn                      -2067.14
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.814
Extras/EpisodeRewardMean                    -83.8626
LinearFeatureBaseline/ExplainedVariance       0.0100442
PolicyExecTime                                0.104559
ProcessExecTime                               0.0116551
TotalEnvSteps                            427064
policy/Entropy                               -0.185804
policy/KL                                     0.00724084
policy/KLBefore                               0
policy/LossAfter                             -0.0181464
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.830437
policy/dLoss                                  0.0181464
---------------------------------------  ----------------
2022-04-23 14:21:43 | [train_policy] epoch #422 | Obtaining samples for iteration 422...
2022-04-23 14:21:44 | [train_policy] epoch #422 | Logging diagnostics...
2022-04-23 14:21:44 | [train_policy] epoch #422 | Optimizing policy...
2022-04-23 14:21:44 | [train_policy] epoch #422 | Computing loss before
2022-04-23 14:21:44 | [train_policy] epoch #422 | Computing KL before
2022-04-23 14:21:44 | [train_policy] epoch #422 | Optimizing
2022-04-23 14:21:44 | [train_policy] epoch #422 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:44 | [train_policy] epoch #422 | computing loss before
2022-04-23 14:21:44 | [train_policy] epoch #422 | computing gradient
2022-04-23 14:21:44 | [train_policy] epoch #422 | gradient computed
2022-04-23 14:21:44 | [train_policy] epoch #422 | computing descent direction
2022-04-23 14:21:44 | [train_policy] epoch #422 | descent direction computed
2022-04-23 14:21:44 | [train_policy] epoch #422 | backtrack iters: 1
2022-04-23 14:21:44 | [train_policy] epoch #422 | optimization finished
2022-04-23 14:21:44 | [train_policy] epoch #422 | Computing KL after
2022-04-23 14:21:44 | [train_policy] epoch #422 | Computing loss after
2022-04-23 14:21:44 | [train_policy] epoch #422 | Fitting baseline...
2022-04-23 14:21:44 | [train_policy] epoch #422 | Saving snapshot...
2022-04-23 14:21:44 | [train_policy] epoch #422 | Saved
2022-04-23 14:21:44 | [train_policy] epoch #422 | Time 151.89 s
2022-04-23 14:21:44 | [train_policy] epoch #422 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.117089
Evaluation/AverageDiscountedReturn          -88.0304
Evaluation/AverageReturn                    -88.0304
Evaluation/CompletionRate                     0
Evaluation/Iteration                        422
Evaluation/MaxReturn                        -32.7805
Evaluation/MinReturn                      -2064.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.574
Extras/EpisodeRewardMean                    -84.7681
LinearFeatureBaseline/ExplainedVariance       0.133738
PolicyExecTime                                0.102209
ProcessExecTime                               0.0113797
TotalEnvSteps                            428076
policy/Entropy                               -0.188167
policy/KL                                     0.00753248
policy/KLBefore                               0
policy/LossAfter                             -0.0147891
policy/LossBefore                             8.2457e-09
policy/Perplexity                             0.828476
policy/dLoss                                  0.0147891
---------------------------------------  ---------------
2022-04-23 14:21:44 | [train_policy] epoch #423 | Obtaining samples for iteration 423...
2022-04-23 14:21:44 | [train_policy] epoch #423 | Logging diagnostics...
2022-04-23 14:21:44 | [train_policy] epoch #423 | Optimizing policy...
2022-04-23 14:21:44 | [train_policy] epoch #423 | Computing loss before
2022-04-23 14:21:44 | [train_policy] epoch #423 | Computing KL before
2022-04-23 14:21:44 | [train_policy] epoch #423 | Optimizing
2022-04-23 14:21:44 | [train_policy] epoch #423 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:44 | [train_policy] epoch #423 | computing loss before
2022-04-23 14:21:44 | [train_policy] epoch #423 | computing gradient
2022-04-23 14:21:44 | [train_policy] epoch #423 | gradient computed
2022-04-23 14:21:44 | [train_policy] epoch #423 | computing descent direction
2022-04-23 14:21:44 | [train_policy] epoch #423 | descent direction computed
2022-04-23 14:21:44 | [train_policy] epoch #423 | backtrack iters: 1
2022-04-23 14:21:44 | [train_policy] epoch #423 | optimization finished
2022-04-23 14:21:44 | [train_policy] epoch #423 | Computing KL after
2022-04-23 14:21:44 | [train_policy] epoch #423 | Computing loss after
2022-04-23 14:21:44 | [train_policy] epoch #423 | Fitting baseline...
2022-04-23 14:21:44 | [train_policy] epoch #423 | Saving snapshot...
2022-04-23 14:21:44 | [train_policy] epoch #423 | Saved
2022-04-23 14:21:44 | [train_policy] epoch #423 | Time 152.24 s
2022-04-23 14:21:44 | [train_policy] epoch #423 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116188
Evaluation/AverageDiscountedReturn          -87.0944
Evaluation/AverageReturn                    -87.0944
Evaluation/CompletionRate                     0
Evaluation/Iteration                        423
Evaluation/MaxReturn                        -30.3906
Evaluation/MinReturn                      -2066.75
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.896
Extras/EpisodeRewardMean                    -83.2755
LinearFeatureBaseline/ExplainedVariance       0.156906
PolicyExecTime                                0.0997915
ProcessExecTime                               0.0109396
TotalEnvSteps                            429088
policy/Entropy                               -0.174116
policy/KL                                     0.00663881
policy/KLBefore                               0
policy/LossAfter                             -0.0181584
policy/LossBefore                            -1.48423e-08
policy/Perplexity                             0.8402
policy/dLoss                                  0.0181584
---------------------------------------  ----------------
2022-04-23 14:21:44 | [train_policy] epoch #424 | Obtaining samples for iteration 424...
2022-04-23 14:21:44 | [train_policy] epoch #424 | Logging diagnostics...
2022-04-23 14:21:44 | [train_policy] epoch #424 | Optimizing policy...
2022-04-23 14:21:44 | [train_policy] epoch #424 | Computing loss before
2022-04-23 14:21:44 | [train_policy] epoch #424 | Computing KL before
2022-04-23 14:21:44 | [train_policy] epoch #424 | Optimizing
2022-04-23 14:21:44 | [train_policy] epoch #424 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:44 | [train_policy] epoch #424 | computing loss before
2022-04-23 14:21:44 | [train_policy] epoch #424 | computing gradient
2022-04-23 14:21:44 | [train_policy] epoch #424 | gradient computed
2022-04-23 14:21:44 | [train_policy] epoch #424 | computing descent direction
2022-04-23 14:21:44 | [train_policy] epoch #424 | descent direction computed
2022-04-23 14:21:44 | [train_policy] epoch #424 | backtrack iters: 0
2022-04-23 14:21:44 | [train_policy] epoch #424 | optimization finished
2022-04-23 14:21:44 | [train_policy] epoch #424 | Computing KL after
2022-04-23 14:21:44 | [train_policy] epoch #424 | Computing loss after
2022-04-23 14:21:44 | [train_policy] epoch #424 | Fitting baseline...
2022-04-23 14:21:44 | [train_policy] epoch #424 | Saving snapshot...
2022-04-23 14:21:44 | [train_policy] epoch #424 | Saved
2022-04-23 14:21:44 | [train_policy] epoch #424 | Time 152.58 s
2022-04-23 14:21:44 | [train_policy] epoch #424 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115765
Evaluation/AverageDiscountedReturn          -44.2361
Evaluation/AverageReturn                    -44.2361
Evaluation/CompletionRate                     0
Evaluation/Iteration                        424
Evaluation/MaxReturn                        -30.7828
Evaluation/MinReturn                       -167.593
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         14.7727
Extras/EpisodeRewardMean                    -44.0392
LinearFeatureBaseline/ExplainedVariance     -49.1325
PolicyExecTime                                0.0938699
ProcessExecTime                               0.0108473
TotalEnvSteps                            430100
policy/Entropy                               -0.19997
policy/KL                                     0.00975385
policy/KLBefore                               0
policy/LossAfter                             -0.0221709
policy/LossBefore                            -2.92133e-08
policy/Perplexity                             0.818755
policy/dLoss                                  0.0221709
---------------------------------------  ----------------
2022-04-23 14:21:44 | [train_policy] epoch #425 | Obtaining samples for iteration 425...
2022-04-23 14:21:45 | [train_policy] epoch #425 | Logging diagnostics...
2022-04-23 14:21:45 | [train_policy] epoch #425 | Optimizing policy...
2022-04-23 14:21:45 | [train_policy] epoch #425 | Computing loss before
2022-04-23 14:21:45 | [train_policy] epoch #425 | Computing KL before
2022-04-23 14:21:45 | [train_policy] epoch #425 | Optimizing
2022-04-23 14:21:45 | [train_policy] epoch #425 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:45 | [train_policy] epoch #425 | computing loss before
2022-04-23 14:21:45 | [train_policy] epoch #425 | computing gradient
2022-04-23 14:21:45 | [train_policy] epoch #425 | gradient computed
2022-04-23 14:21:45 | [train_policy] epoch #425 | computing descent direction
2022-04-23 14:21:45 | [train_policy] epoch #425 | descent direction computed
2022-04-23 14:21:45 | [train_policy] epoch #425 | backtrack iters: 1
2022-04-23 14:21:45 | [train_policy] epoch #425 | optimization finished
2022-04-23 14:21:45 | [train_policy] epoch #425 | Computing KL after
2022-04-23 14:21:45 | [train_policy] epoch #425 | Computing loss after
2022-04-23 14:21:45 | [train_policy] epoch #425 | Fitting baseline...
2022-04-23 14:21:45 | [train_policy] epoch #425 | Saving snapshot...
2022-04-23 14:21:45 | [train_policy] epoch #425 | Saved
2022-04-23 14:21:45 | [train_policy] epoch #425 | Time 152.92 s
2022-04-23 14:21:45 | [train_policy] epoch #425 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115764
Evaluation/AverageDiscountedReturn          -66.1854
Evaluation/AverageReturn                    -66.1854
Evaluation/CompletionRate                     0
Evaluation/Iteration                        425
Evaluation/MaxReturn                        -33.2426
Evaluation/MinReturn                      -2071.83
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.614
Extras/EpisodeRewardMean                    -64.7694
LinearFeatureBaseline/ExplainedVariance       0.0146246
PolicyExecTime                                0.0969369
ProcessExecTime                               0.0108728
TotalEnvSteps                            431112
policy/Entropy                               -0.20161
policy/KL                                     0.00640896
policy/KLBefore                               0
policy/LossAfter                             -0.0223198
policy/LossBefore                             1.74338e-08
policy/Perplexity                             0.817413
policy/dLoss                                  0.0223198
---------------------------------------  ----------------
2022-04-23 14:21:45 | [train_policy] epoch #426 | Obtaining samples for iteration 426...
2022-04-23 14:21:45 | [train_policy] epoch #426 | Logging diagnostics...
2022-04-23 14:21:45 | [train_policy] epoch #426 | Optimizing policy...
2022-04-23 14:21:45 | [train_policy] epoch #426 | Computing loss before
2022-04-23 14:21:45 | [train_policy] epoch #426 | Computing KL before
2022-04-23 14:21:45 | [train_policy] epoch #426 | Optimizing
2022-04-23 14:21:45 | [train_policy] epoch #426 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:45 | [train_policy] epoch #426 | computing loss before
2022-04-23 14:21:45 | [train_policy] epoch #426 | computing gradient
2022-04-23 14:21:45 | [train_policy] epoch #426 | gradient computed
2022-04-23 14:21:45 | [train_policy] epoch #426 | computing descent direction
2022-04-23 14:21:45 | [train_policy] epoch #426 | descent direction computed
2022-04-23 14:21:45 | [train_policy] epoch #426 | backtrack iters: 1
2022-04-23 14:21:45 | [train_policy] epoch #426 | optimization finished
2022-04-23 14:21:45 | [train_policy] epoch #426 | Computing KL after
2022-04-23 14:21:45 | [train_policy] epoch #426 | Computing loss after
2022-04-23 14:21:45 | [train_policy] epoch #426 | Fitting baseline...
2022-04-23 14:21:45 | [train_policy] epoch #426 | Saving snapshot...
2022-04-23 14:21:45 | [train_policy] epoch #426 | Saved
2022-04-23 14:21:45 | [train_policy] epoch #426 | Time 153.27 s
2022-04-23 14:21:45 | [train_policy] epoch #426 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115836
Evaluation/AverageDiscountedReturn          -87.7602
Evaluation/AverageReturn                    -87.7602
Evaluation/CompletionRate                     0
Evaluation/Iteration                        426
Evaluation/MaxReturn                        -33.593
Evaluation/MinReturn                      -2064.31
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.677
Extras/EpisodeRewardMean                   -104.357
LinearFeatureBaseline/ExplainedVariance       0.137237
PolicyExecTime                                0.101798
ProcessExecTime                               0.011133
TotalEnvSteps                            432124
policy/Entropy                               -0.197059
policy/KL                                     0.00703249
policy/KLBefore                               0
policy/LossAfter                             -0.0157108
policy/LossBefore                            -1.13084e-08
policy/Perplexity                             0.821142
policy/dLoss                                  0.0157108
---------------------------------------  ----------------
2022-04-23 14:21:45 | [train_policy] epoch #427 | Obtaining samples for iteration 427...
2022-04-23 14:21:45 | [train_policy] epoch #427 | Logging diagnostics...
2022-04-23 14:21:45 | [train_policy] epoch #427 | Optimizing policy...
2022-04-23 14:21:45 | [train_policy] epoch #427 | Computing loss before
2022-04-23 14:21:45 | [train_policy] epoch #427 | Computing KL before
2022-04-23 14:21:45 | [train_policy] epoch #427 | Optimizing
2022-04-23 14:21:45 | [train_policy] epoch #427 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:45 | [train_policy] epoch #427 | computing loss before
2022-04-23 14:21:45 | [train_policy] epoch #427 | computing gradient
2022-04-23 14:21:45 | [train_policy] epoch #427 | gradient computed
2022-04-23 14:21:45 | [train_policy] epoch #427 | computing descent direction
2022-04-23 14:21:45 | [train_policy] epoch #427 | descent direction computed
2022-04-23 14:21:45 | [train_policy] epoch #427 | backtrack iters: 1
2022-04-23 14:21:45 | [train_policy] epoch #427 | optimization finished
2022-04-23 14:21:45 | [train_policy] epoch #427 | Computing KL after
2022-04-23 14:21:45 | [train_policy] epoch #427 | Computing loss after
2022-04-23 14:21:45 | [train_policy] epoch #427 | Fitting baseline...
2022-04-23 14:21:45 | [train_policy] epoch #427 | Saving snapshot...
2022-04-23 14:21:45 | [train_policy] epoch #427 | Saved
2022-04-23 14:21:45 | [train_policy] epoch #427 | Time 153.62 s
2022-04-23 14:21:45 | [train_policy] epoch #427 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116223
Evaluation/AverageDiscountedReturn          -87.8107
Evaluation/AverageReturn                    -87.8107
Evaluation/CompletionRate                     0
Evaluation/Iteration                        427
Evaluation/MaxReturn                        -33.3527
Evaluation/MinReturn                      -2062.9
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.519
Extras/EpisodeRewardMean                   -104.443
LinearFeatureBaseline/ExplainedVariance       0.136322
PolicyExecTime                                0.0994887
ProcessExecTime                               0.010937
TotalEnvSteps                            433136
policy/Entropy                               -0.215716
policy/KL                                     0.00669315
policy/KLBefore                               0
policy/LossAfter                             -0.015435
policy/LossBefore                             1.74338e-08
policy/Perplexity                             0.805964
policy/dLoss                                  0.015435
---------------------------------------  ----------------
2022-04-23 14:21:45 | [train_policy] epoch #428 | Obtaining samples for iteration 428...
2022-04-23 14:21:46 | [train_policy] epoch #428 | Logging diagnostics...
2022-04-23 14:21:46 | [train_policy] epoch #428 | Optimizing policy...
2022-04-23 14:21:46 | [train_policy] epoch #428 | Computing loss before
2022-04-23 14:21:46 | [train_policy] epoch #428 | Computing KL before
2022-04-23 14:21:46 | [train_policy] epoch #428 | Optimizing
2022-04-23 14:21:46 | [train_policy] epoch #428 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:46 | [train_policy] epoch #428 | computing loss before
2022-04-23 14:21:46 | [train_policy] epoch #428 | computing gradient
2022-04-23 14:21:46 | [train_policy] epoch #428 | gradient computed
2022-04-23 14:21:46 | [train_policy] epoch #428 | computing descent direction
2022-04-23 14:21:46 | [train_policy] epoch #428 | descent direction computed
2022-04-23 14:21:46 | [train_policy] epoch #428 | backtrack iters: 0
2022-04-23 14:21:46 | [train_policy] epoch #428 | optimization finished
2022-04-23 14:21:46 | [train_policy] epoch #428 | Computing KL after
2022-04-23 14:21:46 | [train_policy] epoch #428 | Computing loss after
2022-04-23 14:21:46 | [train_policy] epoch #428 | Fitting baseline...
2022-04-23 14:21:46 | [train_policy] epoch #428 | Saving snapshot...
2022-04-23 14:21:46 | [train_policy] epoch #428 | Saved
2022-04-23 14:21:46 | [train_policy] epoch #428 | Time 153.97 s
2022-04-23 14:21:46 | [train_policy] epoch #428 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11603
Evaluation/AverageDiscountedReturn          -66.8375
Evaluation/AverageReturn                    -66.8375
Evaluation/CompletionRate                     0
Evaluation/Iteration                        428
Evaluation/MaxReturn                        -33.1248
Evaluation/MinReturn                      -2084.81
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        212.443
Extras/EpisodeRewardMean                    -65.0886
LinearFeatureBaseline/ExplainedVariance       0.144204
PolicyExecTime                                0.104715
ProcessExecTime                               0.0111156
TotalEnvSteps                            434148
policy/Entropy                               -0.214395
policy/KL                                     0.00921222
policy/KLBefore                               0
policy/LossAfter                             -0.0231921
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.80703
policy/dLoss                                  0.0231921
---------------------------------------  ----------------
2022-04-23 14:21:46 | [train_policy] epoch #429 | Obtaining samples for iteration 429...
2022-04-23 14:21:46 | [train_policy] epoch #429 | Logging diagnostics...
2022-04-23 14:21:46 | [train_policy] epoch #429 | Optimizing policy...
2022-04-23 14:21:46 | [train_policy] epoch #429 | Computing loss before
2022-04-23 14:21:46 | [train_policy] epoch #429 | Computing KL before
2022-04-23 14:21:46 | [train_policy] epoch #429 | Optimizing
2022-04-23 14:21:46 | [train_policy] epoch #429 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:46 | [train_policy] epoch #429 | computing loss before
2022-04-23 14:21:46 | [train_policy] epoch #429 | computing gradient
2022-04-23 14:21:46 | [train_policy] epoch #429 | gradient computed
2022-04-23 14:21:46 | [train_policy] epoch #429 | computing descent direction
2022-04-23 14:21:46 | [train_policy] epoch #429 | descent direction computed
2022-04-23 14:21:46 | [train_policy] epoch #429 | backtrack iters: 1
2022-04-23 14:21:46 | [train_policy] epoch #429 | optimization finished
2022-04-23 14:21:46 | [train_policy] epoch #429 | Computing KL after
2022-04-23 14:21:46 | [train_policy] epoch #429 | Computing loss after
2022-04-23 14:21:46 | [train_policy] epoch #429 | Fitting baseline...
2022-04-23 14:21:46 | [train_policy] epoch #429 | Saving snapshot...
2022-04-23 14:21:46 | [train_policy] epoch #429 | Saved
2022-04-23 14:21:46 | [train_policy] epoch #429 | Time 154.30 s
2022-04-23 14:21:46 | [train_policy] epoch #429 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115099
Evaluation/AverageDiscountedReturn          -66.1208
Evaluation/AverageReturn                    -66.1208
Evaluation/CompletionRate                     0
Evaluation/Iteration                        429
Evaluation/MaxReturn                        -31.3565
Evaluation/MinReturn                      -2065.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.718
Extras/EpisodeRewardMean                    -64.2485
LinearFeatureBaseline/ExplainedVariance      -0.0394736
PolicyExecTime                                0.0948105
ProcessExecTime                               0.0108333
TotalEnvSteps                            435160
policy/Entropy                               -0.224325
policy/KL                                     0.00737171
policy/KLBefore                               0
policy/LossAfter                             -0.0176955
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.799055
policy/dLoss                                  0.0176955
---------------------------------------  ----------------
2022-04-23 14:21:46 | [train_policy] epoch #430 | Obtaining samples for iteration 430...
2022-04-23 14:21:46 | [train_policy] epoch #430 | Logging diagnostics...
2022-04-23 14:21:46 | [train_policy] epoch #430 | Optimizing policy...
2022-04-23 14:21:46 | [train_policy] epoch #430 | Computing loss before
2022-04-23 14:21:46 | [train_policy] epoch #430 | Computing KL before
2022-04-23 14:21:46 | [train_policy] epoch #430 | Optimizing
2022-04-23 14:21:46 | [train_policy] epoch #430 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:46 | [train_policy] epoch #430 | computing loss before
2022-04-23 14:21:46 | [train_policy] epoch #430 | computing gradient
2022-04-23 14:21:46 | [train_policy] epoch #430 | gradient computed
2022-04-23 14:21:46 | [train_policy] epoch #430 | computing descent direction
2022-04-23 14:21:46 | [train_policy] epoch #430 | descent direction computed
2022-04-23 14:21:46 | [train_policy] epoch #430 | backtrack iters: 0
2022-04-23 14:21:46 | [train_policy] epoch #430 | optimization finished
2022-04-23 14:21:46 | [train_policy] epoch #430 | Computing KL after
2022-04-23 14:21:46 | [train_policy] epoch #430 | Computing loss after
2022-04-23 14:21:46 | [train_policy] epoch #430 | Fitting baseline...
2022-04-23 14:21:46 | [train_policy] epoch #430 | Saving snapshot...
2022-04-23 14:21:46 | [train_policy] epoch #430 | Saved
2022-04-23 14:21:46 | [train_policy] epoch #430 | Time 154.63 s
2022-04-23 14:21:46 | [train_policy] epoch #430 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115779
Evaluation/AverageDiscountedReturn          -45.8358
Evaluation/AverageReturn                    -45.8358
Evaluation/CompletionRate                     0
Evaluation/Iteration                        430
Evaluation/MaxReturn                        -32.8244
Evaluation/MinReturn                       -108.8
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.1395
Extras/EpisodeRewardMean                    -45.3866
LinearFeatureBaseline/ExplainedVariance     -12.8951
PolicyExecTime                                0.0907958
ProcessExecTime                               0.0108118
TotalEnvSteps                            436172
policy/Entropy                               -0.217804
policy/KL                                     0.00999606
policy/KLBefore                               0
policy/LossAfter                             -0.0252827
policy/LossBefore                            -3.20404e-08
policy/Perplexity                             0.804283
policy/dLoss                                  0.0252826
---------------------------------------  ----------------
2022-04-23 14:21:46 | [train_policy] epoch #431 | Obtaining samples for iteration 431...
2022-04-23 14:21:47 | [train_policy] epoch #431 | Logging diagnostics...
2022-04-23 14:21:47 | [train_policy] epoch #431 | Optimizing policy...
2022-04-23 14:21:47 | [train_policy] epoch #431 | Computing loss before
2022-04-23 14:21:47 | [train_policy] epoch #431 | Computing KL before
2022-04-23 14:21:47 | [train_policy] epoch #431 | Optimizing
2022-04-23 14:21:47 | [train_policy] epoch #431 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:47 | [train_policy] epoch #431 | computing loss before
2022-04-23 14:21:47 | [train_policy] epoch #431 | computing gradient
2022-04-23 14:21:47 | [train_policy] epoch #431 | gradient computed
2022-04-23 14:21:47 | [train_policy] epoch #431 | computing descent direction
2022-04-23 14:21:47 | [train_policy] epoch #431 | descent direction computed
2022-04-23 14:21:47 | [train_policy] epoch #431 | backtrack iters: 1
2022-04-23 14:21:47 | [train_policy] epoch #431 | optimization finished
2022-04-23 14:21:47 | [train_policy] epoch #431 | Computing KL after
2022-04-23 14:21:47 | [train_policy] epoch #431 | Computing loss after
2022-04-23 14:21:47 | [train_policy] epoch #431 | Fitting baseline...
2022-04-23 14:21:47 | [train_policy] epoch #431 | Saving snapshot...
2022-04-23 14:21:47 | [train_policy] epoch #431 | Saved
2022-04-23 14:21:47 | [train_policy] epoch #431 | Time 154.97 s
2022-04-23 14:21:47 | [train_policy] epoch #431 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115838
Evaluation/AverageDiscountedReturn          -42.8864
Evaluation/AverageReturn                    -42.8864
Evaluation/CompletionRate                     0
Evaluation/Iteration                        431
Evaluation/MaxReturn                        -33.2283
Evaluation/MinReturn                        -65.6504
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.76703
Extras/EpisodeRewardMean                    -43.1524
LinearFeatureBaseline/ExplainedVariance       0.19755
PolicyExecTime                                0.093931
ProcessExecTime                               0.0109651
TotalEnvSteps                            437184
policy/Entropy                               -0.267717
policy/KL                                     0.00666387
policy/KLBefore                               0
policy/LossAfter                             -0.00282208
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             0.765124
policy/dLoss                                  0.00282207
---------------------------------------  ----------------
2022-04-23 14:21:47 | [train_policy] epoch #432 | Obtaining samples for iteration 432...
2022-04-23 14:21:47 | [train_policy] epoch #432 | Logging diagnostics...
2022-04-23 14:21:47 | [train_policy] epoch #432 | Optimizing policy...
2022-04-23 14:21:47 | [train_policy] epoch #432 | Computing loss before
2022-04-23 14:21:47 | [train_policy] epoch #432 | Computing KL before
2022-04-23 14:21:47 | [train_policy] epoch #432 | Optimizing
2022-04-23 14:21:47 | [train_policy] epoch #432 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:47 | [train_policy] epoch #432 | computing loss before
2022-04-23 14:21:47 | [train_policy] epoch #432 | computing gradient
2022-04-23 14:21:47 | [train_policy] epoch #432 | gradient computed
2022-04-23 14:21:47 | [train_policy] epoch #432 | computing descent direction
2022-04-23 14:21:47 | [train_policy] epoch #432 | descent direction computed
2022-04-23 14:21:47 | [train_policy] epoch #432 | backtrack iters: 1
2022-04-23 14:21:47 | [train_policy] epoch #432 | optimization finished
2022-04-23 14:21:47 | [train_policy] epoch #432 | Computing KL after
2022-04-23 14:21:47 | [train_policy] epoch #432 | Computing loss after
2022-04-23 14:21:47 | [train_policy] epoch #432 | Fitting baseline...
2022-04-23 14:21:47 | [train_policy] epoch #432 | Saving snapshot...
2022-04-23 14:21:47 | [train_policy] epoch #432 | Saved
2022-04-23 14:21:47 | [train_policy] epoch #432 | Time 155.31 s
2022-04-23 14:21:47 | [train_policy] epoch #432 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115016
Evaluation/AverageDiscountedReturn          -43.9211
Evaluation/AverageReturn                    -43.9211
Evaluation/CompletionRate                     0
Evaluation/Iteration                        432
Evaluation/MaxReturn                        -33.4069
Evaluation/MinReturn                        -63.9859
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.90636
Extras/EpisodeRewardMean                    -43.9764
LinearFeatureBaseline/ExplainedVariance       0.927087
PolicyExecTime                                0.0930221
ProcessExecTime                               0.0107875
TotalEnvSteps                            438196
policy/Entropy                               -0.311249
policy/KL                                     0.00705468
policy/KLBefore                               0
policy/LossAfter                             -0.0161456
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.732532
policy/dLoss                                  0.0161456
---------------------------------------  ----------------
2022-04-23 14:21:47 | [train_policy] epoch #433 | Obtaining samples for iteration 433...
2022-04-23 14:21:47 | [train_policy] epoch #433 | Logging diagnostics...
2022-04-23 14:21:47 | [train_policy] epoch #433 | Optimizing policy...
2022-04-23 14:21:47 | [train_policy] epoch #433 | Computing loss before
2022-04-23 14:21:47 | [train_policy] epoch #433 | Computing KL before
2022-04-23 14:21:47 | [train_policy] epoch #433 | Optimizing
2022-04-23 14:21:47 | [train_policy] epoch #433 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:47 | [train_policy] epoch #433 | computing loss before
2022-04-23 14:21:47 | [train_policy] epoch #433 | computing gradient
2022-04-23 14:21:47 | [train_policy] epoch #433 | gradient computed
2022-04-23 14:21:47 | [train_policy] epoch #433 | computing descent direction
2022-04-23 14:21:47 | [train_policy] epoch #433 | descent direction computed
2022-04-23 14:21:47 | [train_policy] epoch #433 | backtrack iters: 1
2022-04-23 14:21:47 | [train_policy] epoch #433 | optimization finished
2022-04-23 14:21:47 | [train_policy] epoch #433 | Computing KL after
2022-04-23 14:21:47 | [train_policy] epoch #433 | Computing loss after
2022-04-23 14:21:47 | [train_policy] epoch #433 | Fitting baseline...
2022-04-23 14:21:47 | [train_policy] epoch #433 | Saving snapshot...
2022-04-23 14:21:47 | [train_policy] epoch #433 | Saved
2022-04-23 14:21:47 | [train_policy] epoch #433 | Time 155.64 s
2022-04-23 14:21:47 | [train_policy] epoch #433 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115496
Evaluation/AverageDiscountedReturn          -64.8043
Evaluation/AverageReturn                    -64.8043
Evaluation/CompletionRate                     0
Evaluation/Iteration                        433
Evaluation/MaxReturn                        -32.5661
Evaluation/MinReturn                      -2080.17
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.383
Extras/EpisodeRewardMean                    -63.2214
LinearFeatureBaseline/ExplainedVariance       0.0115517
PolicyExecTime                                0.0908766
ProcessExecTime                               0.0108254
TotalEnvSteps                            439208
policy/Entropy                               -0.307522
policy/KL                                     0.00820792
policy/KLBefore                               0
policy/LossAfter                             -0.0212907
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.735267
policy/dLoss                                  0.0212907
---------------------------------------  ----------------
2022-04-23 14:21:47 | [train_policy] epoch #434 | Obtaining samples for iteration 434...
2022-04-23 14:21:48 | [train_policy] epoch #434 | Logging diagnostics...
2022-04-23 14:21:48 | [train_policy] epoch #434 | Optimizing policy...
2022-04-23 14:21:48 | [train_policy] epoch #434 | Computing loss before
2022-04-23 14:21:48 | [train_policy] epoch #434 | Computing KL before
2022-04-23 14:21:48 | [train_policy] epoch #434 | Optimizing
2022-04-23 14:21:48 | [train_policy] epoch #434 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:48 | [train_policy] epoch #434 | computing loss before
2022-04-23 14:21:48 | [train_policy] epoch #434 | computing gradient
2022-04-23 14:21:48 | [train_policy] epoch #434 | gradient computed
2022-04-23 14:21:48 | [train_policy] epoch #434 | computing descent direction
2022-04-23 14:21:48 | [train_policy] epoch #434 | descent direction computed
2022-04-23 14:21:48 | [train_policy] epoch #434 | backtrack iters: 1
2022-04-23 14:21:48 | [train_policy] epoch #434 | optimization finished
2022-04-23 14:21:48 | [train_policy] epoch #434 | Computing KL after
2022-04-23 14:21:48 | [train_policy] epoch #434 | Computing loss after
2022-04-23 14:21:48 | [train_policy] epoch #434 | Fitting baseline...
2022-04-23 14:21:48 | [train_policy] epoch #434 | Saving snapshot...
2022-04-23 14:21:48 | [train_policy] epoch #434 | Saved
2022-04-23 14:21:48 | [train_policy] epoch #434 | Time 155.98 s
2022-04-23 14:21:48 | [train_policy] epoch #434 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115437
Evaluation/AverageDiscountedReturn          -43.0677
Evaluation/AverageReturn                    -43.0677
Evaluation/CompletionRate                     0
Evaluation/Iteration                        434
Evaluation/MaxReturn                        -30.8925
Evaluation/MinReturn                        -69.4097
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12309
Extras/EpisodeRewardMean                    -43.2173
LinearFeatureBaseline/ExplainedVariance     -34.5564
PolicyExecTime                                0.0950346
ProcessExecTime                               0.0109158
TotalEnvSteps                            440220
policy/Entropy                               -0.314554
policy/KL                                     0.00646398
policy/KLBefore                               0
policy/LossAfter                             -0.0278946
policy/LossBefore                            -3.48675e-08
policy/Perplexity                             0.730115
policy/dLoss                                  0.0278945
---------------------------------------  ----------------
2022-04-23 14:21:48 | [train_policy] epoch #435 | Obtaining samples for iteration 435...
2022-04-23 14:21:48 | [train_policy] epoch #435 | Logging diagnostics...
2022-04-23 14:21:48 | [train_policy] epoch #435 | Optimizing policy...
2022-04-23 14:21:48 | [train_policy] epoch #435 | Computing loss before
2022-04-23 14:21:48 | [train_policy] epoch #435 | Computing KL before
2022-04-23 14:21:48 | [train_policy] epoch #435 | Optimizing
2022-04-23 14:21:48 | [train_policy] epoch #435 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:48 | [train_policy] epoch #435 | computing loss before
2022-04-23 14:21:48 | [train_policy] epoch #435 | computing gradient
2022-04-23 14:21:48 | [train_policy] epoch #435 | gradient computed
2022-04-23 14:21:48 | [train_policy] epoch #435 | computing descent direction
2022-04-23 14:21:48 | [train_policy] epoch #435 | descent direction computed
2022-04-23 14:21:48 | [train_policy] epoch #435 | backtrack iters: 1
2022-04-23 14:21:48 | [train_policy] epoch #435 | optimization finished
2022-04-23 14:21:48 | [train_policy] epoch #435 | Computing KL after
2022-04-23 14:21:48 | [train_policy] epoch #435 | Computing loss after
2022-04-23 14:21:48 | [train_policy] epoch #435 | Fitting baseline...
2022-04-23 14:21:48 | [train_policy] epoch #435 | Saving snapshot...
2022-04-23 14:21:48 | [train_policy] epoch #435 | Saved
2022-04-23 14:21:48 | [train_policy] epoch #435 | Time 156.33 s
2022-04-23 14:21:48 | [train_policy] epoch #435 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116513
Evaluation/AverageDiscountedReturn          -86.3025
Evaluation/AverageReturn                    -86.3025
Evaluation/CompletionRate                     0
Evaluation/Iteration                        435
Evaluation/MaxReturn                        -30.4068
Evaluation/MinReturn                      -2070.74
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.501
Extras/EpisodeRewardMean                    -83.2162
LinearFeatureBaseline/ExplainedVariance       0.0097335
PolicyExecTime                                0.100803
ProcessExecTime                               0.0113695
TotalEnvSteps                            441232
policy/Entropy                               -0.301303
policy/KL                                     0.00699103
policy/KLBefore                               0
policy/LossAfter                             -0.0347626
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.739853
policy/dLoss                                  0.0347626
---------------------------------------  ----------------
2022-04-23 14:21:48 | [train_policy] epoch #436 | Obtaining samples for iteration 436...
2022-04-23 14:21:48 | [train_policy] epoch #436 | Logging diagnostics...
2022-04-23 14:21:48 | [train_policy] epoch #436 | Optimizing policy...
2022-04-23 14:21:48 | [train_policy] epoch #436 | Computing loss before
2022-04-23 14:21:48 | [train_policy] epoch #436 | Computing KL before
2022-04-23 14:21:48 | [train_policy] epoch #436 | Optimizing
2022-04-23 14:21:48 | [train_policy] epoch #436 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:48 | [train_policy] epoch #436 | computing loss before
2022-04-23 14:21:48 | [train_policy] epoch #436 | computing gradient
2022-04-23 14:21:48 | [train_policy] epoch #436 | gradient computed
2022-04-23 14:21:48 | [train_policy] epoch #436 | computing descent direction
2022-04-23 14:21:48 | [train_policy] epoch #436 | descent direction computed
2022-04-23 14:21:48 | [train_policy] epoch #436 | backtrack iters: 1
2022-04-23 14:21:48 | [train_policy] epoch #436 | optimization finished
2022-04-23 14:21:48 | [train_policy] epoch #436 | Computing KL after
2022-04-23 14:21:48 | [train_policy] epoch #436 | Computing loss after
2022-04-23 14:21:48 | [train_policy] epoch #436 | Fitting baseline...
2022-04-23 14:21:48 | [train_policy] epoch #436 | Saving snapshot...
2022-04-23 14:21:48 | [train_policy] epoch #436 | Saved
2022-04-23 14:21:48 | [train_policy] epoch #436 | Time 156.68 s
2022-04-23 14:21:48 | [train_policy] epoch #436 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11624
Evaluation/AverageDiscountedReturn          -64.3798
Evaluation/AverageReturn                    -64.3798
Evaluation/CompletionRate                     0
Evaluation/Iteration                        436
Evaluation/MaxReturn                        -30.2561
Evaluation/MinReturn                      -2063.59
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.708
Extras/EpisodeRewardMean                    -83.0648
LinearFeatureBaseline/ExplainedVariance      -0.0617656
PolicyExecTime                                0.104761
ProcessExecTime                               0.0113945
TotalEnvSteps                            442244
policy/Entropy                               -0.310189
policy/KL                                     0.00648608
policy/KLBefore                               0
policy/LossAfter                             -0.0215688
policy/LossBefore                            -2.07321e-08
policy/Perplexity                             0.733308
policy/dLoss                                  0.0215688
---------------------------------------  ----------------
2022-04-23 14:21:48 | [train_policy] epoch #437 | Obtaining samples for iteration 437...
2022-04-23 14:21:49 | [train_policy] epoch #437 | Logging diagnostics...
2022-04-23 14:21:49 | [train_policy] epoch #437 | Optimizing policy...
2022-04-23 14:21:49 | [train_policy] epoch #437 | Computing loss before
2022-04-23 14:21:49 | [train_policy] epoch #437 | Computing KL before
2022-04-23 14:21:49 | [train_policy] epoch #437 | Optimizing
2022-04-23 14:21:49 | [train_policy] epoch #437 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:49 | [train_policy] epoch #437 | computing loss before
2022-04-23 14:21:49 | [train_policy] epoch #437 | computing gradient
2022-04-23 14:21:49 | [train_policy] epoch #437 | gradient computed
2022-04-23 14:21:49 | [train_policy] epoch #437 | computing descent direction
2022-04-23 14:21:49 | [train_policy] epoch #437 | descent direction computed
2022-04-23 14:21:49 | [train_policy] epoch #437 | backtrack iters: 0
2022-04-23 14:21:49 | [train_policy] epoch #437 | optimization finished
2022-04-23 14:21:49 | [train_policy] epoch #437 | Computing KL after
2022-04-23 14:21:49 | [train_policy] epoch #437 | Computing loss after
2022-04-23 14:21:49 | [train_policy] epoch #437 | Fitting baseline...
2022-04-23 14:21:49 | [train_policy] epoch #437 | Saving snapshot...
2022-04-23 14:21:49 | [train_policy] epoch #437 | Saved
2022-04-23 14:21:49 | [train_policy] epoch #437 | Time 157.02 s
2022-04-23 14:21:49 | [train_policy] epoch #437 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115505
Evaluation/AverageDiscountedReturn          -43.4293
Evaluation/AverageReturn                    -43.4293
Evaluation/CompletionRate                     0
Evaluation/Iteration                        437
Evaluation/MaxReturn                        -30.494
Evaluation/MinReturn                        -81.2421
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.40389
Extras/EpisodeRewardMean                    -63.4532
LinearFeatureBaseline/ExplainedVariance     -38.4873
PolicyExecTime                                0.094511
ProcessExecTime                               0.0108218
TotalEnvSteps                            443256
policy/Entropy                               -0.287106
policy/KL                                     0.00914888
policy/KLBefore                               0
policy/LossAfter                             -0.0532614
policy/LossBefore                            -6.36097e-09
policy/Perplexity                             0.750432
policy/dLoss                                  0.0532614
---------------------------------------  ----------------
2022-04-23 14:21:49 | [train_policy] epoch #438 | Obtaining samples for iteration 438...
2022-04-23 14:21:49 | [train_policy] epoch #438 | Logging diagnostics...
2022-04-23 14:21:49 | [train_policy] epoch #438 | Optimizing policy...
2022-04-23 14:21:49 | [train_policy] epoch #438 | Computing loss before
2022-04-23 14:21:49 | [train_policy] epoch #438 | Computing KL before
2022-04-23 14:21:49 | [train_policy] epoch #438 | Optimizing
2022-04-23 14:21:49 | [train_policy] epoch #438 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:49 | [train_policy] epoch #438 | computing loss before
2022-04-23 14:21:49 | [train_policy] epoch #438 | computing gradient
2022-04-23 14:21:49 | [train_policy] epoch #438 | gradient computed
2022-04-23 14:21:49 | [train_policy] epoch #438 | computing descent direction
2022-04-23 14:21:49 | [train_policy] epoch #438 | descent direction computed
2022-04-23 14:21:49 | [train_policy] epoch #438 | backtrack iters: 1
2022-04-23 14:21:49 | [train_policy] epoch #438 | optimization finished
2022-04-23 14:21:49 | [train_policy] epoch #438 | Computing KL after
2022-04-23 14:21:49 | [train_policy] epoch #438 | Computing loss after
2022-04-23 14:21:49 | [train_policy] epoch #438 | Fitting baseline...
2022-04-23 14:21:49 | [train_policy] epoch #438 | Saving snapshot...
2022-04-23 14:21:49 | [train_policy] epoch #438 | Saved
2022-04-23 14:21:49 | [train_policy] epoch #438 | Time 157.35 s
2022-04-23 14:21:49 | [train_policy] epoch #438 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115427
Evaluation/AverageDiscountedReturn         -109.314
Evaluation/AverageReturn                   -109.314
Evaluation/CompletionRate                     0
Evaluation/Iteration                        438
Evaluation/MaxReturn                        -29.333
Evaluation/MinReturn                      -2076.51
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        360.137
Extras/EpisodeRewardMean                   -103.81
LinearFeatureBaseline/ExplainedVariance       0.0126977
PolicyExecTime                                0.0946116
ProcessExecTime                               0.0109868
TotalEnvSteps                            444268
policy/Entropy                               -0.294787
policy/KL                                     0.00846589
policy/KLBefore                               0
policy/LossAfter                             -0.0290203
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.74469
policy/dLoss                                  0.0290203
---------------------------------------  ----------------
2022-04-23 14:21:49 | [train_policy] epoch #439 | Obtaining samples for iteration 439...
2022-04-23 14:21:49 | [train_policy] epoch #439 | Logging diagnostics...
2022-04-23 14:21:49 | [train_policy] epoch #439 | Optimizing policy...
2022-04-23 14:21:49 | [train_policy] epoch #439 | Computing loss before
2022-04-23 14:21:49 | [train_policy] epoch #439 | Computing KL before
2022-04-23 14:21:49 | [train_policy] epoch #439 | Optimizing
2022-04-23 14:21:49 | [train_policy] epoch #439 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:49 | [train_policy] epoch #439 | computing loss before
2022-04-23 14:21:49 | [train_policy] epoch #439 | computing gradient
2022-04-23 14:21:49 | [train_policy] epoch #439 | gradient computed
2022-04-23 14:21:49 | [train_policy] epoch #439 | computing descent direction
2022-04-23 14:21:49 | [train_policy] epoch #439 | descent direction computed
2022-04-23 14:21:49 | [train_policy] epoch #439 | backtrack iters: 0
2022-04-23 14:21:49 | [train_policy] epoch #439 | optimization finished
2022-04-23 14:21:49 | [train_policy] epoch #439 | Computing KL after
2022-04-23 14:21:49 | [train_policy] epoch #439 | Computing loss after
2022-04-23 14:21:49 | [train_policy] epoch #439 | Fitting baseline...
2022-04-23 14:21:49 | [train_policy] epoch #439 | Saving snapshot...
2022-04-23 14:21:49 | [train_policy] epoch #439 | Saved
2022-04-23 14:21:49 | [train_policy] epoch #439 | Time 157.69 s
2022-04-23 14:21:49 | [train_policy] epoch #439 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115632
Evaluation/AverageDiscountedReturn          -42.4952
Evaluation/AverageReturn                    -42.4952
Evaluation/CompletionRate                     0
Evaluation/Iteration                        439
Evaluation/MaxReturn                        -31.6957
Evaluation/MinReturn                        -67.3142
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.85769
Extras/EpisodeRewardMean                    -42.6502
LinearFeatureBaseline/ExplainedVariance    -153.676
PolicyExecTime                                0.0955052
ProcessExecTime                               0.0109758
TotalEnvSteps                            445280
policy/Entropy                               -0.288673
policy/KL                                     0.00948477
policy/KLBefore                               0
policy/LossAfter                             -0.0165182
policy/LossBefore                             5.37149e-08
policy/Perplexity                             0.749257
policy/dLoss                                  0.0165183
---------------------------------------  ----------------
2022-04-23 14:21:49 | [train_policy] epoch #440 | Obtaining samples for iteration 440...
2022-04-23 14:21:50 | [train_policy] epoch #440 | Logging diagnostics...
2022-04-23 14:21:50 | [train_policy] epoch #440 | Optimizing policy...
2022-04-23 14:21:50 | [train_policy] epoch #440 | Computing loss before
2022-04-23 14:21:50 | [train_policy] epoch #440 | Computing KL before
2022-04-23 14:21:50 | [train_policy] epoch #440 | Optimizing
2022-04-23 14:21:50 | [train_policy] epoch #440 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:50 | [train_policy] epoch #440 | computing loss before
2022-04-23 14:21:50 | [train_policy] epoch #440 | computing gradient
2022-04-23 14:21:50 | [train_policy] epoch #440 | gradient computed
2022-04-23 14:21:50 | [train_policy] epoch #440 | computing descent direction
2022-04-23 14:21:50 | [train_policy] epoch #440 | descent direction computed
2022-04-23 14:21:50 | [train_policy] epoch #440 | backtrack iters: 1
2022-04-23 14:21:50 | [train_policy] epoch #440 | optimization finished
2022-04-23 14:21:50 | [train_policy] epoch #440 | Computing KL after
2022-04-23 14:21:50 | [train_policy] epoch #440 | Computing loss after
2022-04-23 14:21:50 | [train_policy] epoch #440 | Fitting baseline...
2022-04-23 14:21:50 | [train_policy] epoch #440 | Saving snapshot...
2022-04-23 14:21:50 | [train_policy] epoch #440 | Saved
2022-04-23 14:21:50 | [train_policy] epoch #440 | Time 158.03 s
2022-04-23 14:21:50 | [train_policy] epoch #440 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11579
Evaluation/AverageDiscountedReturn          -43.7513
Evaluation/AverageReturn                    -43.7513
Evaluation/CompletionRate                     0
Evaluation/Iteration                        440
Evaluation/MaxReturn                        -33.1712
Evaluation/MinReturn                        -75.8665
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.94064
Extras/EpisodeRewardMean                    -43.8328
LinearFeatureBaseline/ExplainedVariance       0.883261
PolicyExecTime                                0.0985284
ProcessExecTime                               0.0110567
TotalEnvSteps                            446292
policy/Entropy                               -0.333555
policy/KL                                     0.00639228
policy/KLBefore                               0
policy/LossAfter                             -0.0128476
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.716373
policy/dLoss                                  0.0128476
---------------------------------------  ----------------
2022-04-23 14:21:50 | [train_policy] epoch #441 | Obtaining samples for iteration 441...
2022-04-23 14:21:50 | [train_policy] epoch #441 | Logging diagnostics...
2022-04-23 14:21:50 | [train_policy] epoch #441 | Optimizing policy...
2022-04-23 14:21:50 | [train_policy] epoch #441 | Computing loss before
2022-04-23 14:21:50 | [train_policy] epoch #441 | Computing KL before
2022-04-23 14:21:50 | [train_policy] epoch #441 | Optimizing
2022-04-23 14:21:50 | [train_policy] epoch #441 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:50 | [train_policy] epoch #441 | computing loss before
2022-04-23 14:21:50 | [train_policy] epoch #441 | computing gradient
2022-04-23 14:21:50 | [train_policy] epoch #441 | gradient computed
2022-04-23 14:21:50 | [train_policy] epoch #441 | computing descent direction
2022-04-23 14:21:50 | [train_policy] epoch #441 | descent direction computed
2022-04-23 14:21:50 | [train_policy] epoch #441 | backtrack iters: 0
2022-04-23 14:21:50 | [train_policy] epoch #441 | optimization finished
2022-04-23 14:21:50 | [train_policy] epoch #441 | Computing KL after
2022-04-23 14:21:50 | [train_policy] epoch #441 | Computing loss after
2022-04-23 14:21:50 | [train_policy] epoch #441 | Fitting baseline...
2022-04-23 14:21:50 | [train_policy] epoch #441 | Saving snapshot...
2022-04-23 14:21:50 | [train_policy] epoch #441 | Saved
2022-04-23 14:21:50 | [train_policy] epoch #441 | Time 158.37 s
2022-04-23 14:21:50 | [train_policy] epoch #441 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116077
Evaluation/AverageDiscountedReturn         -129.727
Evaluation/AverageReturn                   -129.727
Evaluation/CompletionRate                     0
Evaluation/Iteration                        441
Evaluation/MaxReturn                        -33.2688
Evaluation/MinReturn                      -2094.83
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        414.566
Extras/EpisodeRewardMean                   -122.787
LinearFeatureBaseline/ExplainedVariance       0.0121604
PolicyExecTime                                0.0993416
ProcessExecTime                               0.0109866
TotalEnvSteps                            447304
policy/Entropy                               -0.308267
policy/KL                                     0.00948764
policy/KLBefore                               0
policy/LossAfter                             -0.0207629
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.734719
policy/dLoss                                  0.0207629
---------------------------------------  ----------------
2022-04-23 14:21:50 | [train_policy] epoch #442 | Obtaining samples for iteration 442...
2022-04-23 14:21:50 | [train_policy] epoch #442 | Logging diagnostics...
2022-04-23 14:21:50 | [train_policy] epoch #442 | Optimizing policy...
2022-04-23 14:21:50 | [train_policy] epoch #442 | Computing loss before
2022-04-23 14:21:50 | [train_policy] epoch #442 | Computing KL before
2022-04-23 14:21:50 | [train_policy] epoch #442 | Optimizing
2022-04-23 14:21:50 | [train_policy] epoch #442 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:50 | [train_policy] epoch #442 | computing loss before
2022-04-23 14:21:50 | [train_policy] epoch #442 | computing gradient
2022-04-23 14:21:50 | [train_policy] epoch #442 | gradient computed
2022-04-23 14:21:50 | [train_policy] epoch #442 | computing descent direction
2022-04-23 14:21:50 | [train_policy] epoch #442 | descent direction computed
2022-04-23 14:21:50 | [train_policy] epoch #442 | backtrack iters: 1
2022-04-23 14:21:50 | [train_policy] epoch #442 | optimization finished
2022-04-23 14:21:50 | [train_policy] epoch #442 | Computing KL after
2022-04-23 14:21:50 | [train_policy] epoch #442 | Computing loss after
2022-04-23 14:21:50 | [train_policy] epoch #442 | Fitting baseline...
2022-04-23 14:21:50 | [train_policy] epoch #442 | Saving snapshot...
2022-04-23 14:21:51 | [train_policy] epoch #442 | Saved
2022-04-23 14:21:51 | [train_policy] epoch #442 | Time 158.72 s
2022-04-23 14:21:51 | [train_policy] epoch #442 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115697
Evaluation/AverageDiscountedReturn          -65.2987
Evaluation/AverageReturn                    -65.2987
Evaluation/CompletionRate                     0
Evaluation/Iteration                        442
Evaluation/MaxReturn                        -30.9025
Evaluation/MinReturn                      -2060.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.293
Extras/EpisodeRewardMean                    -63.7463
LinearFeatureBaseline/ExplainedVariance      -0.571081
PolicyExecTime                                0.102145
ProcessExecTime                               0.011281
TotalEnvSteps                            448316
policy/Entropy                               -0.303914
policy/KL                                     0.00708625
policy/KLBefore                               0
policy/LossAfter                             -0.0215046
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             0.737924
policy/dLoss                                  0.0215046
---------------------------------------  ----------------
2022-04-23 14:21:51 | [train_policy] epoch #443 | Obtaining samples for iteration 443...
2022-04-23 14:21:51 | [train_policy] epoch #443 | Logging diagnostics...
2022-04-23 14:21:51 | [train_policy] epoch #443 | Optimizing policy...
2022-04-23 14:21:51 | [train_policy] epoch #443 | Computing loss before
2022-04-23 14:21:51 | [train_policy] epoch #443 | Computing KL before
2022-04-23 14:21:51 | [train_policy] epoch #443 | Optimizing
2022-04-23 14:21:51 | [train_policy] epoch #443 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:51 | [train_policy] epoch #443 | computing loss before
2022-04-23 14:21:51 | [train_policy] epoch #443 | computing gradient
2022-04-23 14:21:51 | [train_policy] epoch #443 | gradient computed
2022-04-23 14:21:51 | [train_policy] epoch #443 | computing descent direction
2022-04-23 14:21:51 | [train_policy] epoch #443 | descent direction computed
2022-04-23 14:21:51 | [train_policy] epoch #443 | backtrack iters: 1
2022-04-23 14:21:51 | [train_policy] epoch #443 | optimization finished
2022-04-23 14:21:51 | [train_policy] epoch #443 | Computing KL after
2022-04-23 14:21:51 | [train_policy] epoch #443 | Computing loss after
2022-04-23 14:21:51 | [train_policy] epoch #443 | Fitting baseline...
2022-04-23 14:21:51 | [train_policy] epoch #443 | Saving snapshot...
2022-04-23 14:21:51 | [train_policy] epoch #443 | Saved
2022-04-23 14:21:51 | [train_policy] epoch #443 | Time 159.08 s
2022-04-23 14:21:51 | [train_policy] epoch #443 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.119123
Evaluation/AverageDiscountedReturn          -64.9177
Evaluation/AverageReturn                    -64.9177
Evaluation/CompletionRate                     0
Evaluation/Iteration                        443
Evaluation/MaxReturn                        -29.4112
Evaluation/MinReturn                      -2064.75
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.749
Extras/EpisodeRewardMean                    -63.3684
LinearFeatureBaseline/ExplainedVariance      -0.0830981
PolicyExecTime                                0.103677
ProcessExecTime                               0.0121863
TotalEnvSteps                            449328
policy/Entropy                               -0.307758
policy/KL                                     0.00658065
policy/KLBefore                               0
policy/LossAfter                             -0.0231333
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.735094
policy/dLoss                                  0.0231333
---------------------------------------  ----------------
2022-04-23 14:21:51 | [train_policy] epoch #444 | Obtaining samples for iteration 444...
2022-04-23 14:21:51 | [train_policy] epoch #444 | Logging diagnostics...
2022-04-23 14:21:51 | [train_policy] epoch #444 | Optimizing policy...
2022-04-23 14:21:51 | [train_policy] epoch #444 | Computing loss before
2022-04-23 14:21:51 | [train_policy] epoch #444 | Computing KL before
2022-04-23 14:21:51 | [train_policy] epoch #444 | Optimizing
2022-04-23 14:21:51 | [train_policy] epoch #444 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:51 | [train_policy] epoch #444 | computing loss before
2022-04-23 14:21:51 | [train_policy] epoch #444 | computing gradient
2022-04-23 14:21:51 | [train_policy] epoch #444 | gradient computed
2022-04-23 14:21:51 | [train_policy] epoch #444 | computing descent direction
2022-04-23 14:21:51 | [train_policy] epoch #444 | descent direction computed
2022-04-23 14:21:51 | [train_policy] epoch #444 | backtrack iters: 1
2022-04-23 14:21:51 | [train_policy] epoch #444 | optimization finished
2022-04-23 14:21:51 | [train_policy] epoch #444 | Computing KL after
2022-04-23 14:21:51 | [train_policy] epoch #444 | Computing loss after
2022-04-23 14:21:51 | [train_policy] epoch #444 | Fitting baseline...
2022-04-23 14:21:51 | [train_policy] epoch #444 | Saving snapshot...
2022-04-23 14:21:51 | [train_policy] epoch #444 | Saved
2022-04-23 14:21:51 | [train_policy] epoch #444 | Time 159.45 s
2022-04-23 14:21:51 | [train_policy] epoch #444 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.122637
Evaluation/AverageDiscountedReturn          -86.9482
Evaluation/AverageReturn                    -86.9482
Evaluation/CompletionRate                     0
Evaluation/Iteration                        444
Evaluation/MaxReturn                        -30.8724
Evaluation/MinReturn                      -2064.4
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.232
Extras/EpisodeRewardMean                    -83.3531
LinearFeatureBaseline/ExplainedVariance       0.111479
PolicyExecTime                                0.11109
ProcessExecTime                               0.0119152
TotalEnvSteps                            450340
policy/Entropy                               -0.306239
policy/KL                                     0.00648671
policy/KLBefore                               0
policy/LossAfter                             -0.0269255
policy/LossBefore                             1.83761e-08
policy/Perplexity                             0.736211
policy/dLoss                                  0.0269255
---------------------------------------  ----------------
2022-04-23 14:21:51 | [train_policy] epoch #445 | Obtaining samples for iteration 445...
2022-04-23 14:21:52 | [train_policy] epoch #445 | Logging diagnostics...
2022-04-23 14:21:52 | [train_policy] epoch #445 | Optimizing policy...
2022-04-23 14:21:52 | [train_policy] epoch #445 | Computing loss before
2022-04-23 14:21:52 | [train_policy] epoch #445 | Computing KL before
2022-04-23 14:21:52 | [train_policy] epoch #445 | Optimizing
2022-04-23 14:21:52 | [train_policy] epoch #445 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:52 | [train_policy] epoch #445 | computing loss before
2022-04-23 14:21:52 | [train_policy] epoch #445 | computing gradient
2022-04-23 14:21:52 | [train_policy] epoch #445 | gradient computed
2022-04-23 14:21:52 | [train_policy] epoch #445 | computing descent direction
2022-04-23 14:21:52 | [train_policy] epoch #445 | descent direction computed
2022-04-23 14:21:52 | [train_policy] epoch #445 | backtrack iters: 1
2022-04-23 14:21:52 | [train_policy] epoch #445 | optimization finished
2022-04-23 14:21:52 | [train_policy] epoch #445 | Computing KL after
2022-04-23 14:21:52 | [train_policy] epoch #445 | Computing loss after
2022-04-23 14:21:52 | [train_policy] epoch #445 | Fitting baseline...
2022-04-23 14:21:52 | [train_policy] epoch #445 | Saving snapshot...
2022-04-23 14:21:52 | [train_policy] epoch #445 | Saved
2022-04-23 14:21:52 | [train_policy] epoch #445 | Time 159.81 s
2022-04-23 14:21:52 | [train_policy] epoch #445 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.126742
Evaluation/AverageDiscountedReturn          -63.8249
Evaluation/AverageReturn                    -63.8249
Evaluation/CompletionRate                     0
Evaluation/Iteration                        445
Evaluation/MaxReturn                        -32.0022
Evaluation/MinReturn                      -2064.99
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.93
Extras/EpisodeRewardMean                    -62.1828
LinearFeatureBaseline/ExplainedVariance       0.010379
PolicyExecTime                                0.10322
ProcessExecTime                               0.0124803
TotalEnvSteps                            451352
policy/Entropy                               -0.312842
policy/KL                                     0.00784947
policy/KLBefore                               0
policy/LossAfter                             -0.016677
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.731365
policy/dLoss                                  0.016677
---------------------------------------  ----------------
2022-04-23 14:21:52 | [train_policy] epoch #446 | Obtaining samples for iteration 446...
2022-04-23 14:21:52 | [train_policy] epoch #446 | Logging diagnostics...
2022-04-23 14:21:52 | [train_policy] epoch #446 | Optimizing policy...
2022-04-23 14:21:52 | [train_policy] epoch #446 | Computing loss before
2022-04-23 14:21:52 | [train_policy] epoch #446 | Computing KL before
2022-04-23 14:21:52 | [train_policy] epoch #446 | Optimizing
2022-04-23 14:21:52 | [train_policy] epoch #446 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:52 | [train_policy] epoch #446 | computing loss before
2022-04-23 14:21:52 | [train_policy] epoch #446 | computing gradient
2022-04-23 14:21:52 | [train_policy] epoch #446 | gradient computed
2022-04-23 14:21:52 | [train_policy] epoch #446 | computing descent direction
2022-04-23 14:21:52 | [train_policy] epoch #446 | descent direction computed
2022-04-23 14:21:52 | [train_policy] epoch #446 | backtrack iters: 1
2022-04-23 14:21:52 | [train_policy] epoch #446 | optimization finished
2022-04-23 14:21:52 | [train_policy] epoch #446 | Computing KL after
2022-04-23 14:21:52 | [train_policy] epoch #446 | Computing loss after
2022-04-23 14:21:52 | [train_policy] epoch #446 | Fitting baseline...
2022-04-23 14:21:52 | [train_policy] epoch #446 | Saving snapshot...
2022-04-23 14:21:52 | [train_policy] epoch #446 | Saved
2022-04-23 14:21:52 | [train_policy] epoch #446 | Time 160.15 s
2022-04-23 14:21:52 | [train_policy] epoch #446 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117601
Evaluation/AverageDiscountedReturn          -66.9749
Evaluation/AverageReturn                    -66.9749
Evaluation/CompletionRate                     0
Evaluation/Iteration                        446
Evaluation/MaxReturn                        -32.6407
Evaluation/MinReturn                      -2063.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.571
Extras/EpisodeRewardMean                    -64.9035
LinearFeatureBaseline/ExplainedVariance       0.0290115
PolicyExecTime                                0.0957103
ProcessExecTime                               0.011122
TotalEnvSteps                            452364
policy/Entropy                               -0.323512
policy/KL                                     0.00671601
policy/KLBefore                               0
policy/LossAfter                             -0.0168046
policy/LossBefore                             1.88473e-08
policy/Perplexity                             0.723603
policy/dLoss                                  0.0168046
---------------------------------------  ----------------
2022-04-23 14:21:52 | [train_policy] epoch #447 | Obtaining samples for iteration 447...
2022-04-23 14:21:52 | [train_policy] epoch #447 | Logging diagnostics...
2022-04-23 14:21:52 | [train_policy] epoch #447 | Optimizing policy...
2022-04-23 14:21:52 | [train_policy] epoch #447 | Computing loss before
2022-04-23 14:21:52 | [train_policy] epoch #447 | Computing KL before
2022-04-23 14:21:52 | [train_policy] epoch #447 | Optimizing
2022-04-23 14:21:52 | [train_policy] epoch #447 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:52 | [train_policy] epoch #447 | computing loss before
2022-04-23 14:21:52 | [train_policy] epoch #447 | computing gradient
2022-04-23 14:21:52 | [train_policy] epoch #447 | gradient computed
2022-04-23 14:21:52 | [train_policy] epoch #447 | computing descent direction
2022-04-23 14:21:52 | [train_policy] epoch #447 | descent direction computed
2022-04-23 14:21:52 | [train_policy] epoch #447 | backtrack iters: 0
2022-04-23 14:21:52 | [train_policy] epoch #447 | optimization finished
2022-04-23 14:21:52 | [train_policy] epoch #447 | Computing KL after
2022-04-23 14:21:52 | [train_policy] epoch #447 | Computing loss after
2022-04-23 14:21:52 | [train_policy] epoch #447 | Fitting baseline...
2022-04-23 14:21:52 | [train_policy] epoch #447 | Saving snapshot...
2022-04-23 14:21:52 | [train_policy] epoch #447 | Saved
2022-04-23 14:21:52 | [train_policy] epoch #447 | Time 160.49 s
2022-04-23 14:21:52 | [train_policy] epoch #447 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.116519
Evaluation/AverageDiscountedReturn          -42.3521
Evaluation/AverageReturn                    -42.3521
Evaluation/CompletionRate                     0
Evaluation/Iteration                        447
Evaluation/MaxReturn                        -33.4146
Evaluation/MinReturn                        -78.022
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83986
Extras/EpisodeRewardMean                    -42.4055
LinearFeatureBaseline/ExplainedVariance     -12.305
PolicyExecTime                                0.0995533
ProcessExecTime                               0.0113282
TotalEnvSteps                            453376
policy/Entropy                               -0.300014
policy/KL                                     0.0096812
policy/KLBefore                               0
policy/LossAfter                             -0.0241199
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             0.740808
policy/dLoss                                  0.0241199
---------------------------------------  ---------------
2022-04-23 14:21:52 | [train_policy] epoch #448 | Obtaining samples for iteration 448...
2022-04-23 14:21:53 | [train_policy] epoch #448 | Logging diagnostics...
2022-04-23 14:21:53 | [train_policy] epoch #448 | Optimizing policy...
2022-04-23 14:21:53 | [train_policy] epoch #448 | Computing loss before
2022-04-23 14:21:53 | [train_policy] epoch #448 | Computing KL before
2022-04-23 14:21:53 | [train_policy] epoch #448 | Optimizing
2022-04-23 14:21:53 | [train_policy] epoch #448 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:53 | [train_policy] epoch #448 | computing loss before
2022-04-23 14:21:53 | [train_policy] epoch #448 | computing gradient
2022-04-23 14:21:53 | [train_policy] epoch #448 | gradient computed
2022-04-23 14:21:53 | [train_policy] epoch #448 | computing descent direction
2022-04-23 14:21:53 | [train_policy] epoch #448 | descent direction computed
2022-04-23 14:21:53 | [train_policy] epoch #448 | backtrack iters: 0
2022-04-23 14:21:53 | [train_policy] epoch #448 | optimization finished
2022-04-23 14:21:53 | [train_policy] epoch #448 | Computing KL after
2022-04-23 14:21:53 | [train_policy] epoch #448 | Computing loss after
2022-04-23 14:21:53 | [train_policy] epoch #448 | Fitting baseline...
2022-04-23 14:21:53 | [train_policy] epoch #448 | Saving snapshot...
2022-04-23 14:21:53 | [train_policy] epoch #448 | Saved
2022-04-23 14:21:53 | [train_policy] epoch #448 | Time 160.84 s
2022-04-23 14:21:53 | [train_policy] epoch #448 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117014
Evaluation/AverageDiscountedReturn          -64.1971
Evaluation/AverageReturn                    -64.1971
Evaluation/CompletionRate                     0
Evaluation/Iteration                        448
Evaluation/MaxReturn                        -31.4232
Evaluation/MinReturn                      -2067.83
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.186
Extras/EpisodeRewardMean                    -62.3311
LinearFeatureBaseline/ExplainedVariance       0.0135148
PolicyExecTime                                0.102773
ProcessExecTime                               0.0115004
TotalEnvSteps                            454388
policy/Entropy                               -0.303014
policy/KL                                     0.00965914
policy/KLBefore                               0
policy/LossAfter                             -0.0221638
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.738589
policy/dLoss                                  0.0221638
---------------------------------------  ----------------
2022-04-23 14:21:53 | [train_policy] epoch #449 | Obtaining samples for iteration 449...
2022-04-23 14:21:53 | [train_policy] epoch #449 | Logging diagnostics...
2022-04-23 14:21:53 | [train_policy] epoch #449 | Optimizing policy...
2022-04-23 14:21:53 | [train_policy] epoch #449 | Computing loss before
2022-04-23 14:21:53 | [train_policy] epoch #449 | Computing KL before
2022-04-23 14:21:53 | [train_policy] epoch #449 | Optimizing
2022-04-23 14:21:53 | [train_policy] epoch #449 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:53 | [train_policy] epoch #449 | computing loss before
2022-04-23 14:21:53 | [train_policy] epoch #449 | computing gradient
2022-04-23 14:21:53 | [train_policy] epoch #449 | gradient computed
2022-04-23 14:21:53 | [train_policy] epoch #449 | computing descent direction
2022-04-23 14:21:53 | [train_policy] epoch #449 | descent direction computed
2022-04-23 14:21:53 | [train_policy] epoch #449 | backtrack iters: 0
2022-04-23 14:21:53 | [train_policy] epoch #449 | optimization finished
2022-04-23 14:21:53 | [train_policy] epoch #449 | Computing KL after
2022-04-23 14:21:53 | [train_policy] epoch #449 | Computing loss after
2022-04-23 14:21:53 | [train_policy] epoch #449 | Fitting baseline...
2022-04-23 14:21:53 | [train_policy] epoch #449 | Saving snapshot...
2022-04-23 14:21:53 | [train_policy] epoch #449 | Saved
2022-04-23 14:21:53 | [train_policy] epoch #449 | Time 161.19 s
2022-04-23 14:21:53 | [train_policy] epoch #449 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117028
Evaluation/AverageDiscountedReturn         -108.22
Evaluation/AverageReturn                   -108.22
Evaluation/CompletionRate                     0
Evaluation/Iteration                        449
Evaluation/MaxReturn                        -31.3276
Evaluation/MinReturn                      -2062.68
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        358.836
Extras/EpisodeRewardMean                   -102.991
LinearFeatureBaseline/ExplainedVariance       0.170945
PolicyExecTime                                0.104817
ProcessExecTime                               0.0112946
TotalEnvSteps                            455400
policy/Entropy                               -0.294708
policy/KL                                     0.00995059
policy/KLBefore                               0
policy/LossAfter                             -0.0147526
policy/LossBefore                            -7.30334e-09
policy/Perplexity                             0.744749
policy/dLoss                                  0.0147525
---------------------------------------  ----------------
2022-04-23 14:21:53 | [train_policy] epoch #450 | Obtaining samples for iteration 450...
2022-04-23 14:21:53 | [train_policy] epoch #450 | Logging diagnostics...
2022-04-23 14:21:53 | [train_policy] epoch #450 | Optimizing policy...
2022-04-23 14:21:53 | [train_policy] epoch #450 | Computing loss before
2022-04-23 14:21:53 | [train_policy] epoch #450 | Computing KL before
2022-04-23 14:21:53 | [train_policy] epoch #450 | Optimizing
2022-04-23 14:21:53 | [train_policy] epoch #450 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:53 | [train_policy] epoch #450 | computing loss before
2022-04-23 14:21:53 | [train_policy] epoch #450 | computing gradient
2022-04-23 14:21:53 | [train_policy] epoch #450 | gradient computed
2022-04-23 14:21:53 | [train_policy] epoch #450 | computing descent direction
2022-04-23 14:21:53 | [train_policy] epoch #450 | descent direction computed
2022-04-23 14:21:53 | [train_policy] epoch #450 | backtrack iters: 0
2022-04-23 14:21:53 | [train_policy] epoch #450 | optimization finished
2022-04-23 14:21:53 | [train_policy] epoch #450 | Computing KL after
2022-04-23 14:21:53 | [train_policy] epoch #450 | Computing loss after
2022-04-23 14:21:53 | [train_policy] epoch #450 | Fitting baseline...
2022-04-23 14:21:53 | [train_policy] epoch #450 | Saving snapshot...
2022-04-23 14:21:53 | [train_policy] epoch #450 | Saved
2022-04-23 14:21:53 | [train_policy] epoch #450 | Time 161.54 s
2022-04-23 14:21:53 | [train_policy] epoch #450 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116262
Evaluation/AverageDiscountedReturn          -64.7206
Evaluation/AverageReturn                    -64.7206
Evaluation/CompletionRate                     0
Evaluation/Iteration                        450
Evaluation/MaxReturn                        -29.6345
Evaluation/MinReturn                      -2064.16
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.765
Extras/EpisodeRewardMean                    -63.1915
LinearFeatureBaseline/ExplainedVariance      -0.488516
PolicyExecTime                                0.099493
ProcessExecTime                               0.01138
TotalEnvSteps                            456412
policy/Entropy                               -0.261022
policy/KL                                     0.00917803
policy/KLBefore                               0
policy/LossAfter                             -0.014973
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.770264
policy/dLoss                                  0.014973
---------------------------------------  ----------------
2022-04-23 14:21:53 | [train_policy] epoch #451 | Obtaining samples for iteration 451...
2022-04-23 14:21:54 | [train_policy] epoch #451 | Logging diagnostics...
2022-04-23 14:21:54 | [train_policy] epoch #451 | Optimizing policy...
2022-04-23 14:21:54 | [train_policy] epoch #451 | Computing loss before
2022-04-23 14:21:54 | [train_policy] epoch #451 | Computing KL before
2022-04-23 14:21:54 | [train_policy] epoch #451 | Optimizing
2022-04-23 14:21:54 | [train_policy] epoch #451 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:54 | [train_policy] epoch #451 | computing loss before
2022-04-23 14:21:54 | [train_policy] epoch #451 | computing gradient
2022-04-23 14:21:54 | [train_policy] epoch #451 | gradient computed
2022-04-23 14:21:54 | [train_policy] epoch #451 | computing descent direction
2022-04-23 14:21:54 | [train_policy] epoch #451 | descent direction computed
2022-04-23 14:21:54 | [train_policy] epoch #451 | backtrack iters: 1
2022-04-23 14:21:54 | [train_policy] epoch #451 | optimization finished
2022-04-23 14:21:54 | [train_policy] epoch #451 | Computing KL after
2022-04-23 14:21:54 | [train_policy] epoch #451 | Computing loss after
2022-04-23 14:21:54 | [train_policy] epoch #451 | Fitting baseline...
2022-04-23 14:21:54 | [train_policy] epoch #451 | Saving snapshot...
2022-04-23 14:21:54 | [train_policy] epoch #451 | Saved
2022-04-23 14:21:54 | [train_policy] epoch #451 | Time 161.89 s
2022-04-23 14:21:54 | [train_policy] epoch #451 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11625
Evaluation/AverageDiscountedReturn          -42.1426
Evaluation/AverageReturn                    -42.1426
Evaluation/CompletionRate                     0
Evaluation/Iteration                        451
Evaluation/MaxReturn                        -31.1947
Evaluation/MinReturn                        -64.537
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.81074
Extras/EpisodeRewardMean                    -42.1973
LinearFeatureBaseline/ExplainedVariance     -19.7405
PolicyExecTime                                0.101994
ProcessExecTime                               0.0116394
TotalEnvSteps                            457424
policy/Entropy                               -0.257981
policy/KL                                     0.00636941
policy/KLBefore                               0
policy/LossAfter                             -0.0118488
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.77261
policy/dLoss                                  0.0118488
---------------------------------------  ----------------
2022-04-23 14:21:54 | [train_policy] epoch #452 | Obtaining samples for iteration 452...
2022-04-23 14:21:54 | [train_policy] epoch #452 | Logging diagnostics...
2022-04-23 14:21:54 | [train_policy] epoch #452 | Optimizing policy...
2022-04-23 14:21:54 | [train_policy] epoch #452 | Computing loss before
2022-04-23 14:21:54 | [train_policy] epoch #452 | Computing KL before
2022-04-23 14:21:54 | [train_policy] epoch #452 | Optimizing
2022-04-23 14:21:54 | [train_policy] epoch #452 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:54 | [train_policy] epoch #452 | computing loss before
2022-04-23 14:21:54 | [train_policy] epoch #452 | computing gradient
2022-04-23 14:21:54 | [train_policy] epoch #452 | gradient computed
2022-04-23 14:21:54 | [train_policy] epoch #452 | computing descent direction
2022-04-23 14:21:54 | [train_policy] epoch #452 | descent direction computed
2022-04-23 14:21:54 | [train_policy] epoch #452 | backtrack iters: 1
2022-04-23 14:21:54 | [train_policy] epoch #452 | optimization finished
2022-04-23 14:21:54 | [train_policy] epoch #452 | Computing KL after
2022-04-23 14:21:54 | [train_policy] epoch #452 | Computing loss after
2022-04-23 14:21:54 | [train_policy] epoch #452 | Fitting baseline...
2022-04-23 14:21:54 | [train_policy] epoch #452 | Saving snapshot...
2022-04-23 14:21:54 | [train_policy] epoch #452 | Saved
2022-04-23 14:21:54 | [train_policy] epoch #452 | Time 162.23 s
2022-04-23 14:21:54 | [train_policy] epoch #452 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116175
Evaluation/AverageDiscountedReturn          -43.3428
Evaluation/AverageReturn                    -43.3428
Evaluation/CompletionRate                     0
Evaluation/Iteration                        452
Evaluation/MaxReturn                        -28.2789
Evaluation/MinReturn                       -133.352
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.9704
Extras/EpisodeRewardMean                    -42.9732
LinearFeatureBaseline/ExplainedVariance       0.663619
PolicyExecTime                                0.102584
ProcessExecTime                               0.01126
TotalEnvSteps                            458436
policy/Entropy                               -0.260301
policy/KL                                     0.00723789
policy/KLBefore                               0
policy/LossAfter                             -0.0224953
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.770819
policy/dLoss                                  0.0224953
---------------------------------------  ----------------
2022-04-23 14:21:54 | [train_policy] epoch #453 | Obtaining samples for iteration 453...
2022-04-23 14:21:54 | [train_policy] epoch #453 | Logging diagnostics...
2022-04-23 14:21:54 | [train_policy] epoch #453 | Optimizing policy...
2022-04-23 14:21:54 | [train_policy] epoch #453 | Computing loss before
2022-04-23 14:21:54 | [train_policy] epoch #453 | Computing KL before
2022-04-23 14:21:54 | [train_policy] epoch #453 | Optimizing
2022-04-23 14:21:54 | [train_policy] epoch #453 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:54 | [train_policy] epoch #453 | computing loss before
2022-04-23 14:21:54 | [train_policy] epoch #453 | computing gradient
2022-04-23 14:21:54 | [train_policy] epoch #453 | gradient computed
2022-04-23 14:21:54 | [train_policy] epoch #453 | computing descent direction
2022-04-23 14:21:54 | [train_policy] epoch #453 | descent direction computed
2022-04-23 14:21:54 | [train_policy] epoch #453 | backtrack iters: 1
2022-04-23 14:21:54 | [train_policy] epoch #453 | optimization finished
2022-04-23 14:21:54 | [train_policy] epoch #453 | Computing KL after
2022-04-23 14:21:54 | [train_policy] epoch #453 | Computing loss after
2022-04-23 14:21:54 | [train_policy] epoch #453 | Fitting baseline...
2022-04-23 14:21:54 | [train_policy] epoch #453 | Saving snapshot...
2022-04-23 14:21:54 | [train_policy] epoch #453 | Saved
2022-04-23 14:21:54 | [train_policy] epoch #453 | Time 162.57 s
2022-04-23 14:21:54 | [train_policy] epoch #453 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117167
Evaluation/AverageDiscountedReturn          -42.0946
Evaluation/AverageReturn                    -42.0946
Evaluation/CompletionRate                     0
Evaluation/Iteration                        453
Evaluation/MaxReturn                        -30.8031
Evaluation/MinReturn                        -64.0666
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.36392
Extras/EpisodeRewardMean                    -41.8886
LinearFeatureBaseline/ExplainedVariance       0.883739
PolicyExecTime                                0.0946002
ProcessExecTime                               0.0110703
TotalEnvSteps                            459448
policy/Entropy                               -0.258395
policy/KL                                     0.00650241
policy/KLBefore                               0
policy/LossAfter                             -0.0152585
policy/LossBefore                            -4.09929e-08
policy/Perplexity                             0.77229
policy/dLoss                                  0.0152585
---------------------------------------  ----------------
2022-04-23 14:21:54 | [train_policy] epoch #454 | Obtaining samples for iteration 454...
2022-04-23 14:21:55 | [train_policy] epoch #454 | Logging diagnostics...
2022-04-23 14:21:55 | [train_policy] epoch #454 | Optimizing policy...
2022-04-23 14:21:55 | [train_policy] epoch #454 | Computing loss before
2022-04-23 14:21:55 | [train_policy] epoch #454 | Computing KL before
2022-04-23 14:21:55 | [train_policy] epoch #454 | Optimizing
2022-04-23 14:21:55 | [train_policy] epoch #454 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:55 | [train_policy] epoch #454 | computing loss before
2022-04-23 14:21:55 | [train_policy] epoch #454 | computing gradient
2022-04-23 14:21:55 | [train_policy] epoch #454 | gradient computed
2022-04-23 14:21:55 | [train_policy] epoch #454 | computing descent direction
2022-04-23 14:21:55 | [train_policy] epoch #454 | descent direction computed
2022-04-23 14:21:55 | [train_policy] epoch #454 | backtrack iters: 0
2022-04-23 14:21:55 | [train_policy] epoch #454 | optimization finished
2022-04-23 14:21:55 | [train_policy] epoch #454 | Computing KL after
2022-04-23 14:21:55 | [train_policy] epoch #454 | Computing loss after
2022-04-23 14:21:55 | [train_policy] epoch #454 | Fitting baseline...
2022-04-23 14:21:55 | [train_policy] epoch #454 | Saving snapshot...
2022-04-23 14:21:55 | [train_policy] epoch #454 | Saved
2022-04-23 14:21:55 | [train_policy] epoch #454 | Time 162.91 s
2022-04-23 14:21:55 | [train_policy] epoch #454 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117723
Evaluation/AverageDiscountedReturn          -86.4733
Evaluation/AverageReturn                    -86.4733
Evaluation/CompletionRate                     0
Evaluation/Iteration                        454
Evaluation/MaxReturn                        -29.2475
Evaluation/MinReturn                      -2063.65
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.76
Extras/EpisodeRewardMean                    -82.8532
LinearFeatureBaseline/ExplainedVariance       0.0106964
PolicyExecTime                                0.0976846
ProcessExecTime                               0.0110283
TotalEnvSteps                            460460
policy/Entropy                               -0.275607
policy/KL                                     0.00959616
policy/KLBefore                               0
policy/LossAfter                             -0.0301669
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.759111
policy/dLoss                                  0.0301669
---------------------------------------  ----------------
2022-04-23 14:21:55 | [train_policy] epoch #455 | Obtaining samples for iteration 455...
2022-04-23 14:21:55 | [train_policy] epoch #455 | Logging diagnostics...
2022-04-23 14:21:55 | [train_policy] epoch #455 | Optimizing policy...
2022-04-23 14:21:55 | [train_policy] epoch #455 | Computing loss before
2022-04-23 14:21:55 | [train_policy] epoch #455 | Computing KL before
2022-04-23 14:21:55 | [train_policy] epoch #455 | Optimizing
2022-04-23 14:21:55 | [train_policy] epoch #455 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:55 | [train_policy] epoch #455 | computing loss before
2022-04-23 14:21:55 | [train_policy] epoch #455 | computing gradient
2022-04-23 14:21:55 | [train_policy] epoch #455 | gradient computed
2022-04-23 14:21:55 | [train_policy] epoch #455 | computing descent direction
2022-04-23 14:21:55 | [train_policy] epoch #455 | descent direction computed
2022-04-23 14:21:55 | [train_policy] epoch #455 | backtrack iters: 1
2022-04-23 14:21:55 | [train_policy] epoch #455 | optimization finished
2022-04-23 14:21:55 | [train_policy] epoch #455 | Computing KL after
2022-04-23 14:21:55 | [train_policy] epoch #455 | Computing loss after
2022-04-23 14:21:55 | [train_policy] epoch #455 | Fitting baseline...
2022-04-23 14:21:55 | [train_policy] epoch #455 | Saving snapshot...
2022-04-23 14:21:55 | [train_policy] epoch #455 | Saved
2022-04-23 14:21:55 | [train_policy] epoch #455 | Time 163.26 s
2022-04-23 14:21:55 | [train_policy] epoch #455 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.116245
Evaluation/AverageDiscountedReturn          -87.235
Evaluation/AverageReturn                    -87.235
Evaluation/CompletionRate                     0
Evaluation/Iteration                        455
Evaluation/MaxReturn                        -31.4318
Evaluation/MinReturn                      -2064.35
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.761
Extras/EpisodeRewardMean                   -103.496
LinearFeatureBaseline/ExplainedVariance       0.181801
PolicyExecTime                                0.0988452
ProcessExecTime                               0.0111904
TotalEnvSteps                            461472
policy/Entropy                               -0.269717
policy/KL                                     0.00730705
policy/KLBefore                               0
policy/LossAfter                             -0.0180172
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             0.763595
policy/dLoss                                  0.0180172
---------------------------------------  ---------------
2022-04-23 14:21:55 | [train_policy] epoch #456 | Obtaining samples for iteration 456...
2022-04-23 14:21:55 | [train_policy] epoch #456 | Logging diagnostics...
2022-04-23 14:21:55 | [train_policy] epoch #456 | Optimizing policy...
2022-04-23 14:21:55 | [train_policy] epoch #456 | Computing loss before
2022-04-23 14:21:55 | [train_policy] epoch #456 | Computing KL before
2022-04-23 14:21:55 | [train_policy] epoch #456 | Optimizing
2022-04-23 14:21:55 | [train_policy] epoch #456 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:55 | [train_policy] epoch #456 | computing loss before
2022-04-23 14:21:55 | [train_policy] epoch #456 | computing gradient
2022-04-23 14:21:55 | [train_policy] epoch #456 | gradient computed
2022-04-23 14:21:55 | [train_policy] epoch #456 | computing descent direction
2022-04-23 14:21:55 | [train_policy] epoch #456 | descent direction computed
2022-04-23 14:21:55 | [train_policy] epoch #456 | backtrack iters: 1
2022-04-23 14:21:55 | [train_policy] epoch #456 | optimization finished
2022-04-23 14:21:55 | [train_policy] epoch #456 | Computing KL after
2022-04-23 14:21:55 | [train_policy] epoch #456 | Computing loss after
2022-04-23 14:21:55 | [train_policy] epoch #456 | Fitting baseline...
2022-04-23 14:21:55 | [train_policy] epoch #456 | Saving snapshot...
2022-04-23 14:21:55 | [train_policy] epoch #456 | Saved
2022-04-23 14:21:55 | [train_policy] epoch #456 | Time 163.61 s
2022-04-23 14:21:55 | [train_policy] epoch #456 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116269
Evaluation/AverageDiscountedReturn          -86.6107
Evaluation/AverageReturn                    -86.6107
Evaluation/CompletionRate                     0
Evaluation/Iteration                        456
Evaluation/MaxReturn                        -33.2256
Evaluation/MinReturn                      -2084.63
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        296.388
Extras/EpisodeRewardMean                    -83.3517
LinearFeatureBaseline/ExplainedVariance       0.209466
PolicyExecTime                                0.105961
ProcessExecTime                               0.0115092
TotalEnvSteps                            462484
policy/Entropy                               -0.286131
policy/KL                                     0.00705343
policy/KLBefore                               0
policy/LossAfter                             -0.0332999
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.751164
policy/dLoss                                  0.0332999
---------------------------------------  ----------------
2022-04-23 14:21:55 | [train_policy] epoch #457 | Obtaining samples for iteration 457...
2022-04-23 14:21:56 | [train_policy] epoch #457 | Logging diagnostics...
2022-04-23 14:21:56 | [train_policy] epoch #457 | Optimizing policy...
2022-04-23 14:21:56 | [train_policy] epoch #457 | Computing loss before
2022-04-23 14:21:56 | [train_policy] epoch #457 | Computing KL before
2022-04-23 14:21:56 | [train_policy] epoch #457 | Optimizing
2022-04-23 14:21:56 | [train_policy] epoch #457 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:56 | [train_policy] epoch #457 | computing loss before
2022-04-23 14:21:56 | [train_policy] epoch #457 | computing gradient
2022-04-23 14:21:56 | [train_policy] epoch #457 | gradient computed
2022-04-23 14:21:56 | [train_policy] epoch #457 | computing descent direction
2022-04-23 14:21:56 | [train_policy] epoch #457 | descent direction computed
2022-04-23 14:21:56 | [train_policy] epoch #457 | backtrack iters: 1
2022-04-23 14:21:56 | [train_policy] epoch #457 | optimization finished
2022-04-23 14:21:56 | [train_policy] epoch #457 | Computing KL after
2022-04-23 14:21:56 | [train_policy] epoch #457 | Computing loss after
2022-04-23 14:21:56 | [train_policy] epoch #457 | Fitting baseline...
2022-04-23 14:21:56 | [train_policy] epoch #457 | Saving snapshot...
2022-04-23 14:21:56 | [train_policy] epoch #457 | Saved
2022-04-23 14:21:56 | [train_policy] epoch #457 | Time 163.96 s
2022-04-23 14:21:56 | [train_policy] epoch #457 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.116255
Evaluation/AverageDiscountedReturn          -86.7889
Evaluation/AverageReturn                    -86.7889
Evaluation/CompletionRate                     0
Evaluation/Iteration                        457
Evaluation/MaxReturn                        -29.8209
Evaluation/MinReturn                      -2063.89
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.768
Extras/EpisodeRewardMean                    -83.246
LinearFeatureBaseline/ExplainedVariance       0.135438
PolicyExecTime                                0.106597
ProcessExecTime                               0.0116291
TotalEnvSteps                            463496
policy/Entropy                               -0.24126
policy/KL                                     0.00641788
policy/KLBefore                               0
policy/LossAfter                             -0.0223377
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.785637
policy/dLoss                                  0.0223377
---------------------------------------  ---------------
2022-04-23 14:21:56 | [train_policy] epoch #458 | Obtaining samples for iteration 458...
2022-04-23 14:21:56 | [train_policy] epoch #458 | Logging diagnostics...
2022-04-23 14:21:56 | [train_policy] epoch #458 | Optimizing policy...
2022-04-23 14:21:56 | [train_policy] epoch #458 | Computing loss before
2022-04-23 14:21:56 | [train_policy] epoch #458 | Computing KL before
2022-04-23 14:21:56 | [train_policy] epoch #458 | Optimizing
2022-04-23 14:21:56 | [train_policy] epoch #458 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:56 | [train_policy] epoch #458 | computing loss before
2022-04-23 14:21:56 | [train_policy] epoch #458 | computing gradient
2022-04-23 14:21:56 | [train_policy] epoch #458 | gradient computed
2022-04-23 14:21:56 | [train_policy] epoch #458 | computing descent direction
2022-04-23 14:21:56 | [train_policy] epoch #458 | descent direction computed
2022-04-23 14:21:56 | [train_policy] epoch #458 | backtrack iters: 1
2022-04-23 14:21:56 | [train_policy] epoch #458 | optimization finished
2022-04-23 14:21:56 | [train_policy] epoch #458 | Computing KL after
2022-04-23 14:21:56 | [train_policy] epoch #458 | Computing loss after
2022-04-23 14:21:56 | [train_policy] epoch #458 | Fitting baseline...
2022-04-23 14:21:56 | [train_policy] epoch #458 | Saving snapshot...
2022-04-23 14:21:56 | [train_policy] epoch #458 | Saved
2022-04-23 14:21:56 | [train_policy] epoch #458 | Time 164.31 s
2022-04-23 14:21:56 | [train_policy] epoch #458 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.116452
Evaluation/AverageDiscountedReturn          -64.9737
Evaluation/AverageReturn                    -64.9737
Evaluation/CompletionRate                     0
Evaluation/Iteration                        458
Evaluation/MaxReturn                        -30.5851
Evaluation/MinReturn                      -2063.04
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.602
Extras/EpisodeRewardMean                    -63.0444
LinearFeatureBaseline/ExplainedVariance      -0.0396181
PolicyExecTime                                0.107365
ProcessExecTime                               0.0115979
TotalEnvSteps                            464508
policy/Entropy                               -0.242413
policy/KL                                     0.00715071
policy/KLBefore                               0
policy/LossAfter                             -0.0151215
policy/LossBefore                             1.5549e-08
policy/Perplexity                             0.784732
policy/dLoss                                  0.0151215
---------------------------------------  ---------------
2022-04-23 14:21:56 | [train_policy] epoch #459 | Obtaining samples for iteration 459...
2022-04-23 14:21:56 | [train_policy] epoch #459 | Logging diagnostics...
2022-04-23 14:21:56 | [train_policy] epoch #459 | Optimizing policy...
2022-04-23 14:21:56 | [train_policy] epoch #459 | Computing loss before
2022-04-23 14:21:56 | [train_policy] epoch #459 | Computing KL before
2022-04-23 14:21:56 | [train_policy] epoch #459 | Optimizing
2022-04-23 14:21:56 | [train_policy] epoch #459 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:56 | [train_policy] epoch #459 | computing loss before
2022-04-23 14:21:56 | [train_policy] epoch #459 | computing gradient
2022-04-23 14:21:56 | [train_policy] epoch #459 | gradient computed
2022-04-23 14:21:56 | [train_policy] epoch #459 | computing descent direction
2022-04-23 14:21:56 | [train_policy] epoch #459 | descent direction computed
2022-04-23 14:21:56 | [train_policy] epoch #459 | backtrack iters: 1
2022-04-23 14:21:56 | [train_policy] epoch #459 | optimization finished
2022-04-23 14:21:56 | [train_policy] epoch #459 | Computing KL after
2022-04-23 14:21:56 | [train_policy] epoch #459 | Computing loss after
2022-04-23 14:21:56 | [train_policy] epoch #459 | Fitting baseline...
2022-04-23 14:21:56 | [train_policy] epoch #459 | Saving snapshot...
2022-04-23 14:21:56 | [train_policy] epoch #459 | Saved
2022-04-23 14:21:56 | [train_policy] epoch #459 | Time 164.66 s
2022-04-23 14:21:56 | [train_policy] epoch #459 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117979
Evaluation/AverageDiscountedReturn          -86.891
Evaluation/AverageReturn                    -86.891
Evaluation/CompletionRate                     0
Evaluation/Iteration                        459
Evaluation/MaxReturn                        -28.8994
Evaluation/MinReturn                      -2064.01
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.785
Extras/EpisodeRewardMean                    -83.4352
LinearFeatureBaseline/ExplainedVariance       0.191499
PolicyExecTime                                0.103297
ProcessExecTime                               0.0116458
TotalEnvSteps                            465520
policy/Entropy                               -0.254701
policy/KL                                     0.00665129
policy/KLBefore                               0
policy/LossAfter                             -0.0178709
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.775148
policy/dLoss                                  0.0178708
---------------------------------------  ----------------
2022-04-23 14:21:56 | [train_policy] epoch #460 | Obtaining samples for iteration 460...
2022-04-23 14:21:57 | [train_policy] epoch #460 | Logging diagnostics...
2022-04-23 14:21:57 | [train_policy] epoch #460 | Optimizing policy...
2022-04-23 14:21:57 | [train_policy] epoch #460 | Computing loss before
2022-04-23 14:21:57 | [train_policy] epoch #460 | Computing KL before
2022-04-23 14:21:57 | [train_policy] epoch #460 | Optimizing
2022-04-23 14:21:57 | [train_policy] epoch #460 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:57 | [train_policy] epoch #460 | computing loss before
2022-04-23 14:21:57 | [train_policy] epoch #460 | computing gradient
2022-04-23 14:21:57 | [train_policy] epoch #460 | gradient computed
2022-04-23 14:21:57 | [train_policy] epoch #460 | computing descent direction
2022-04-23 14:21:57 | [train_policy] epoch #460 | descent direction computed
2022-04-23 14:21:57 | [train_policy] epoch #460 | backtrack iters: 1
2022-04-23 14:21:57 | [train_policy] epoch #460 | optimization finished
2022-04-23 14:21:57 | [train_policy] epoch #460 | Computing KL after
2022-04-23 14:21:57 | [train_policy] epoch #460 | Computing loss after
2022-04-23 14:21:57 | [train_policy] epoch #460 | Fitting baseline...
2022-04-23 14:21:57 | [train_policy] epoch #460 | Saving snapshot...
2022-04-23 14:21:57 | [train_policy] epoch #460 | Saved
2022-04-23 14:21:57 | [train_policy] epoch #460 | Time 165.01 s
2022-04-23 14:21:57 | [train_policy] epoch #460 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119533
Evaluation/AverageDiscountedReturn          -42.8959
Evaluation/AverageReturn                    -42.8959
Evaluation/CompletionRate                     0
Evaluation/Iteration                        460
Evaluation/MaxReturn                        -28.5294
Evaluation/MinReturn                        -66.0327
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.44157
Extras/EpisodeRewardMean                    -43.1859
LinearFeatureBaseline/ExplainedVariance     -69.7616
PolicyExecTime                                0.104015
ProcessExecTime                               0.0117915
TotalEnvSteps                            466532
policy/Entropy                               -0.280834
policy/KL                                     0.00639289
policy/KLBefore                               0
policy/LossAfter                             -0.0148188
policy/LossBefore                            -3.48675e-08
policy/Perplexity                             0.755154
policy/dLoss                                  0.0148188
---------------------------------------  ----------------
2022-04-23 14:21:57 | [train_policy] epoch #461 | Obtaining samples for iteration 461...
2022-04-23 14:21:57 | [train_policy] epoch #461 | Logging diagnostics...
2022-04-23 14:21:57 | [train_policy] epoch #461 | Optimizing policy...
2022-04-23 14:21:57 | [train_policy] epoch #461 | Computing loss before
2022-04-23 14:21:57 | [train_policy] epoch #461 | Computing KL before
2022-04-23 14:21:57 | [train_policy] epoch #461 | Optimizing
2022-04-23 14:21:57 | [train_policy] epoch #461 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:57 | [train_policy] epoch #461 | computing loss before
2022-04-23 14:21:57 | [train_policy] epoch #461 | computing gradient
2022-04-23 14:21:57 | [train_policy] epoch #461 | gradient computed
2022-04-23 14:21:57 | [train_policy] epoch #461 | computing descent direction
2022-04-23 14:21:57 | [train_policy] epoch #461 | descent direction computed
2022-04-23 14:21:57 | [train_policy] epoch #461 | backtrack iters: 1
2022-04-23 14:21:57 | [train_policy] epoch #461 | optimization finished
2022-04-23 14:21:57 | [train_policy] epoch #461 | Computing KL after
2022-04-23 14:21:57 | [train_policy] epoch #461 | Computing loss after
2022-04-23 14:21:57 | [train_policy] epoch #461 | Fitting baseline...
2022-04-23 14:21:57 | [train_policy] epoch #461 | Saving snapshot...
2022-04-23 14:21:57 | [train_policy] epoch #461 | Saved
2022-04-23 14:21:57 | [train_policy] epoch #461 | Time 165.34 s
2022-04-23 14:21:57 | [train_policy] epoch #461 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117267
Evaluation/AverageDiscountedReturn          -44.3819
Evaluation/AverageReturn                    -44.3819
Evaluation/CompletionRate                     0
Evaluation/Iteration                        461
Evaluation/MaxReturn                        -29.0702
Evaluation/MinReturn                        -79.1413
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.05063
Extras/EpisodeRewardMean                    -44.2803
LinearFeatureBaseline/ExplainedVariance       0.881427
PolicyExecTime                                0.0925832
ProcessExecTime                               0.0110648
TotalEnvSteps                            467544
policy/Entropy                               -0.304915
policy/KL                                     0.00721952
policy/KLBefore                               0
policy/LossAfter                             -0.02078
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.737186
policy/dLoss                                  0.02078
---------------------------------------  ----------------
2022-04-23 14:21:57 | [train_policy] epoch #462 | Obtaining samples for iteration 462...
2022-04-23 14:21:57 | [train_policy] epoch #462 | Logging diagnostics...
2022-04-23 14:21:57 | [train_policy] epoch #462 | Optimizing policy...
2022-04-23 14:21:57 | [train_policy] epoch #462 | Computing loss before
2022-04-23 14:21:57 | [train_policy] epoch #462 | Computing KL before
2022-04-23 14:21:57 | [train_policy] epoch #462 | Optimizing
2022-04-23 14:21:57 | [train_policy] epoch #462 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:57 | [train_policy] epoch #462 | computing loss before
2022-04-23 14:21:57 | [train_policy] epoch #462 | computing gradient
2022-04-23 14:21:57 | [train_policy] epoch #462 | gradient computed
2022-04-23 14:21:57 | [train_policy] epoch #462 | computing descent direction
2022-04-23 14:21:57 | [train_policy] epoch #462 | descent direction computed
2022-04-23 14:21:57 | [train_policy] epoch #462 | backtrack iters: 1
2022-04-23 14:21:57 | [train_policy] epoch #462 | optimization finished
2022-04-23 14:21:57 | [train_policy] epoch #462 | Computing KL after
2022-04-23 14:21:57 | [train_policy] epoch #462 | Computing loss after
2022-04-23 14:21:57 | [train_policy] epoch #462 | Fitting baseline...
2022-04-23 14:21:57 | [train_policy] epoch #462 | Saving snapshot...
2022-04-23 14:21:57 | [train_policy] epoch #462 | Saved
2022-04-23 14:21:57 | [train_policy] epoch #462 | Time 165.69 s
2022-04-23 14:21:57 | [train_policy] epoch #462 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117424
Evaluation/AverageDiscountedReturn          -87.5592
Evaluation/AverageReturn                    -87.5592
Evaluation/CompletionRate                     0
Evaluation/Iteration                        462
Evaluation/MaxReturn                        -30.3437
Evaluation/MinReturn                      -2063.84
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.636
Extras/EpisodeRewardMean                    -84.5065
LinearFeatureBaseline/ExplainedVariance       0.0102828
PolicyExecTime                                0.0965667
ProcessExecTime                               0.0111048
TotalEnvSteps                            468556
policy/Entropy                               -0.308559
policy/KL                                     0.0065582
policy/KLBefore                               0
policy/LossAfter                             -0.0186305
policy/LossBefore                            -1.17796e-09
policy/Perplexity                             0.734505
policy/dLoss                                  0.0186305
---------------------------------------  ----------------
2022-04-23 14:21:57 | [train_policy] epoch #463 | Obtaining samples for iteration 463...
2022-04-23 14:21:58 | [train_policy] epoch #463 | Logging diagnostics...
2022-04-23 14:21:58 | [train_policy] epoch #463 | Optimizing policy...
2022-04-23 14:21:58 | [train_policy] epoch #463 | Computing loss before
2022-04-23 14:21:58 | [train_policy] epoch #463 | Computing KL before
2022-04-23 14:21:58 | [train_policy] epoch #463 | Optimizing
2022-04-23 14:21:58 | [train_policy] epoch #463 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:58 | [train_policy] epoch #463 | computing loss before
2022-04-23 14:21:58 | [train_policy] epoch #463 | computing gradient
2022-04-23 14:21:58 | [train_policy] epoch #463 | gradient computed
2022-04-23 14:21:58 | [train_policy] epoch #463 | computing descent direction
2022-04-23 14:21:58 | [train_policy] epoch #463 | descent direction computed
2022-04-23 14:21:58 | [train_policy] epoch #463 | backtrack iters: 1
2022-04-23 14:21:58 | [train_policy] epoch #463 | optimization finished
2022-04-23 14:21:58 | [train_policy] epoch #463 | Computing KL after
2022-04-23 14:21:58 | [train_policy] epoch #463 | Computing loss after
2022-04-23 14:21:58 | [train_policy] epoch #463 | Fitting baseline...
2022-04-23 14:21:58 | [train_policy] epoch #463 | Saving snapshot...
2022-04-23 14:21:58 | [train_policy] epoch #463 | Saved
2022-04-23 14:21:58 | [train_policy] epoch #463 | Time 166.04 s
2022-04-23 14:21:58 | [train_policy] epoch #463 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116108
Evaluation/AverageDiscountedReturn          -43.0598
Evaluation/AverageReturn                    -43.0598
Evaluation/CompletionRate                     0
Evaluation/Iteration                        463
Evaluation/MaxReturn                        -33.6013
Evaluation/MinReturn                        -64.9077
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.4848
Extras/EpisodeRewardMean                    -43.2356
LinearFeatureBaseline/ExplainedVariance     -68.8117
PolicyExecTime                                0.103336
ProcessExecTime                               0.0115142
TotalEnvSteps                            469568
policy/Entropy                               -0.303107
policy/KL                                     0.00655688
policy/KLBefore                               0
policy/LossAfter                             -0.0102191
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.73852
policy/dLoss                                  0.0102191
---------------------------------------  ----------------
2022-04-23 14:21:58 | [train_policy] epoch #464 | Obtaining samples for iteration 464...
2022-04-23 14:21:58 | [train_policy] epoch #464 | Logging diagnostics...
2022-04-23 14:21:58 | [train_policy] epoch #464 | Optimizing policy...
2022-04-23 14:21:58 | [train_policy] epoch #464 | Computing loss before
2022-04-23 14:21:58 | [train_policy] epoch #464 | Computing KL before
2022-04-23 14:21:58 | [train_policy] epoch #464 | Optimizing
2022-04-23 14:21:58 | [train_policy] epoch #464 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:58 | [train_policy] epoch #464 | computing loss before
2022-04-23 14:21:58 | [train_policy] epoch #464 | computing gradient
2022-04-23 14:21:58 | [train_policy] epoch #464 | gradient computed
2022-04-23 14:21:58 | [train_policy] epoch #464 | computing descent direction
2022-04-23 14:21:58 | [train_policy] epoch #464 | descent direction computed
2022-04-23 14:21:58 | [train_policy] epoch #464 | backtrack iters: 0
2022-04-23 14:21:58 | [train_policy] epoch #464 | optimization finished
2022-04-23 14:21:58 | [train_policy] epoch #464 | Computing KL after
2022-04-23 14:21:58 | [train_policy] epoch #464 | Computing loss after
2022-04-23 14:21:58 | [train_policy] epoch #464 | Fitting baseline...
2022-04-23 14:21:58 | [train_policy] epoch #464 | Saving snapshot...
2022-04-23 14:21:58 | [train_policy] epoch #464 | Saved
2022-04-23 14:21:58 | [train_policy] epoch #464 | Time 166.38 s
2022-04-23 14:21:58 | [train_policy] epoch #464 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116234
Evaluation/AverageDiscountedReturn          -64.5768
Evaluation/AverageReturn                    -64.5768
Evaluation/CompletionRate                     0
Evaluation/Iteration                        464
Evaluation/MaxReturn                        -30.1425
Evaluation/MinReturn                      -2062.63
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.611
Extras/EpisodeRewardMean                    -62.7337
LinearFeatureBaseline/ExplainedVariance       0.0141194
PolicyExecTime                                0.103381
ProcessExecTime                               0.0114439
TotalEnvSteps                            470580
policy/Entropy                               -0.314257
policy/KL                                     0.00905256
policy/KLBefore                               0
policy/LossAfter                             -0.0223059
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.730331
policy/dLoss                                  0.0223059
---------------------------------------  ----------------
2022-04-23 14:21:58 | [train_policy] epoch #465 | Obtaining samples for iteration 465...
2022-04-23 14:21:58 | [train_policy] epoch #465 | Logging diagnostics...
2022-04-23 14:21:58 | [train_policy] epoch #465 | Optimizing policy...
2022-04-23 14:21:58 | [train_policy] epoch #465 | Computing loss before
2022-04-23 14:21:58 | [train_policy] epoch #465 | Computing KL before
2022-04-23 14:21:58 | [train_policy] epoch #465 | Optimizing
2022-04-23 14:21:58 | [train_policy] epoch #465 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:58 | [train_policy] epoch #465 | computing loss before
2022-04-23 14:21:58 | [train_policy] epoch #465 | computing gradient
2022-04-23 14:21:58 | [train_policy] epoch #465 | gradient computed
2022-04-23 14:21:58 | [train_policy] epoch #465 | computing descent direction
2022-04-23 14:21:58 | [train_policy] epoch #465 | descent direction computed
2022-04-23 14:21:58 | [train_policy] epoch #465 | backtrack iters: 1
2022-04-23 14:21:58 | [train_policy] epoch #465 | optimization finished
2022-04-23 14:21:58 | [train_policy] epoch #465 | Computing KL after
2022-04-23 14:21:58 | [train_policy] epoch #465 | Computing loss after
2022-04-23 14:21:58 | [train_policy] epoch #465 | Fitting baseline...
2022-04-23 14:21:59 | [train_policy] epoch #465 | Saving snapshot...
2022-04-23 14:21:59 | [train_policy] epoch #465 | Saved
2022-04-23 14:21:59 | [train_policy] epoch #465 | Time 166.72 s
2022-04-23 14:21:59 | [train_policy] epoch #465 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115699
Evaluation/AverageDiscountedReturn          -64.3595
Evaluation/AverageReturn                    -64.3595
Evaluation/CompletionRate                     0
Evaluation/Iteration                        465
Evaluation/MaxReturn                        -28.3111
Evaluation/MinReturn                      -2063.06
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.638
Extras/EpisodeRewardMean                    -62.3757
LinearFeatureBaseline/ExplainedVariance       0.122936
PolicyExecTime                                0.103562
ProcessExecTime                               0.0112507
TotalEnvSteps                            471592
policy/Entropy                               -0.316212
policy/KL                                     0.00697389
policy/KLBefore                               0
policy/LossAfter                             -0.0272788
policy/LossBefore                             1.17796e-09
policy/Perplexity                             0.728905
policy/dLoss                                  0.0272788
---------------------------------------  ----------------
2022-04-23 14:21:59 | [train_policy] epoch #466 | Obtaining samples for iteration 466...
2022-04-23 14:21:59 | [train_policy] epoch #466 | Logging diagnostics...
2022-04-23 14:21:59 | [train_policy] epoch #466 | Optimizing policy...
2022-04-23 14:21:59 | [train_policy] epoch #466 | Computing loss before
2022-04-23 14:21:59 | [train_policy] epoch #466 | Computing KL before
2022-04-23 14:21:59 | [train_policy] epoch #466 | Optimizing
2022-04-23 14:21:59 | [train_policy] epoch #466 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:59 | [train_policy] epoch #466 | computing loss before
2022-04-23 14:21:59 | [train_policy] epoch #466 | computing gradient
2022-04-23 14:21:59 | [train_policy] epoch #466 | gradient computed
2022-04-23 14:21:59 | [train_policy] epoch #466 | computing descent direction
2022-04-23 14:21:59 | [train_policy] epoch #466 | descent direction computed
2022-04-23 14:21:59 | [train_policy] epoch #466 | backtrack iters: 0
2022-04-23 14:21:59 | [train_policy] epoch #466 | optimization finished
2022-04-23 14:21:59 | [train_policy] epoch #466 | Computing KL after
2022-04-23 14:21:59 | [train_policy] epoch #466 | Computing loss after
2022-04-23 14:21:59 | [train_policy] epoch #466 | Fitting baseline...
2022-04-23 14:21:59 | [train_policy] epoch #466 | Saving snapshot...
2022-04-23 14:21:59 | [train_policy] epoch #466 | Saved
2022-04-23 14:21:59 | [train_policy] epoch #466 | Time 167.06 s
2022-04-23 14:21:59 | [train_policy] epoch #466 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116072
Evaluation/AverageDiscountedReturn          -42.3253
Evaluation/AverageReturn                    -42.3253
Evaluation/CompletionRate                     0
Evaluation/Iteration                        466
Evaluation/MaxReturn                        -29.3603
Evaluation/MinReturn                        -65.1094
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.75452
Extras/EpisodeRewardMean                    -62.2537
LinearFeatureBaseline/ExplainedVariance     -30.6774
PolicyExecTime                                0.103197
ProcessExecTime                               0.0112014
TotalEnvSteps                            472604
policy/Entropy                               -0.251674
policy/KL                                     0.00968246
policy/KLBefore                               0
policy/LossAfter                             -0.0195755
policy/LossBefore                             4.33488e-08
policy/Perplexity                             0.777498
policy/dLoss                                  0.0195755
---------------------------------------  ----------------
2022-04-23 14:21:59 | [train_policy] epoch #467 | Obtaining samples for iteration 467...
2022-04-23 14:21:59 | [train_policy] epoch #467 | Logging diagnostics...
2022-04-23 14:21:59 | [train_policy] epoch #467 | Optimizing policy...
2022-04-23 14:21:59 | [train_policy] epoch #467 | Computing loss before
2022-04-23 14:21:59 | [train_policy] epoch #467 | Computing KL before
2022-04-23 14:21:59 | [train_policy] epoch #467 | Optimizing
2022-04-23 14:21:59 | [train_policy] epoch #467 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:59 | [train_policy] epoch #467 | computing loss before
2022-04-23 14:21:59 | [train_policy] epoch #467 | computing gradient
2022-04-23 14:21:59 | [train_policy] epoch #467 | gradient computed
2022-04-23 14:21:59 | [train_policy] epoch #467 | computing descent direction
2022-04-23 14:21:59 | [train_policy] epoch #467 | descent direction computed
2022-04-23 14:21:59 | [train_policy] epoch #467 | backtrack iters: 1
2022-04-23 14:21:59 | [train_policy] epoch #467 | optimization finished
2022-04-23 14:21:59 | [train_policy] epoch #467 | Computing KL after
2022-04-23 14:21:59 | [train_policy] epoch #467 | Computing loss after
2022-04-23 14:21:59 | [train_policy] epoch #467 | Fitting baseline...
2022-04-23 14:21:59 | [train_policy] epoch #467 | Saving snapshot...
2022-04-23 14:21:59 | [train_policy] epoch #467 | Saved
2022-04-23 14:21:59 | [train_policy] epoch #467 | Time 167.41 s
2022-04-23 14:21:59 | [train_policy] epoch #467 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116029
Evaluation/AverageDiscountedReturn          -64.2848
Evaluation/AverageReturn                    -64.2848
Evaluation/CompletionRate                     0
Evaluation/Iteration                        467
Evaluation/MaxReturn                        -28.0602
Evaluation/MinReturn                      -2062.51
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.652
Extras/EpisodeRewardMean                    -62.052
LinearFeatureBaseline/ExplainedVariance       0.013505
PolicyExecTime                                0.102246
ProcessExecTime                               0.0113795
TotalEnvSteps                            473616
policy/Entropy                               -0.249529
policy/KL                                     0.00714558
policy/KLBefore                               0
policy/LossAfter                             -0.0222147
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.779168
policy/dLoss                                  0.0222147
---------------------------------------  ----------------
2022-04-23 14:21:59 | [train_policy] epoch #468 | Obtaining samples for iteration 468...
2022-04-23 14:21:59 | [train_policy] epoch #468 | Logging diagnostics...
2022-04-23 14:21:59 | [train_policy] epoch #468 | Optimizing policy...
2022-04-23 14:21:59 | [train_policy] epoch #468 | Computing loss before
2022-04-23 14:21:59 | [train_policy] epoch #468 | Computing KL before
2022-04-23 14:21:59 | [train_policy] epoch #468 | Optimizing
2022-04-23 14:21:59 | [train_policy] epoch #468 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:21:59 | [train_policy] epoch #468 | computing loss before
2022-04-23 14:21:59 | [train_policy] epoch #468 | computing gradient
2022-04-23 14:21:59 | [train_policy] epoch #468 | gradient computed
2022-04-23 14:21:59 | [train_policy] epoch #468 | computing descent direction
2022-04-23 14:22:00 | [train_policy] epoch #468 | descent direction computed
2022-04-23 14:22:00 | [train_policy] epoch #468 | backtrack iters: 1
2022-04-23 14:22:00 | [train_policy] epoch #468 | optimization finished
2022-04-23 14:22:00 | [train_policy] epoch #468 | Computing KL after
2022-04-23 14:22:00 | [train_policy] epoch #468 | Computing loss after
2022-04-23 14:22:00 | [train_policy] epoch #468 | Fitting baseline...
2022-04-23 14:22:00 | [train_policy] epoch #468 | Saving snapshot...
2022-04-23 14:22:00 | [train_policy] epoch #468 | Saved
2022-04-23 14:22:00 | [train_policy] epoch #468 | Time 167.76 s
2022-04-23 14:22:00 | [train_policy] epoch #468 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.115571
Evaluation/AverageDiscountedReturn          -43.4452
Evaluation/AverageReturn                    -43.4452
Evaluation/CompletionRate                     0
Evaluation/Iteration                        468
Evaluation/MaxReturn                        -31.8421
Evaluation/MinReturn                        -65.8241
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.51577
Extras/EpisodeRewardMean                    -43.2784
LinearFeatureBaseline/ExplainedVariance     -22.0391
PolicyExecTime                                0.10311
ProcessExecTime                               0.0115533
TotalEnvSteps                            474628
policy/Entropy                               -0.262481
policy/KL                                     0.0079537
policy/KLBefore                               0
policy/LossAfter                             -0.0191681
policy/LossBefore                            -2.37947e-08
policy/Perplexity                             0.769141
policy/dLoss                                  0.0191681
---------------------------------------  ----------------
2022-04-23 14:22:00 | [train_policy] epoch #469 | Obtaining samples for iteration 469...
2022-04-23 14:22:00 | [train_policy] epoch #469 | Logging diagnostics...
2022-04-23 14:22:00 | [train_policy] epoch #469 | Optimizing policy...
2022-04-23 14:22:00 | [train_policy] epoch #469 | Computing loss before
2022-04-23 14:22:00 | [train_policy] epoch #469 | Computing KL before
2022-04-23 14:22:00 | [train_policy] epoch #469 | Optimizing
2022-04-23 14:22:00 | [train_policy] epoch #469 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:00 | [train_policy] epoch #469 | computing loss before
2022-04-23 14:22:00 | [train_policy] epoch #469 | computing gradient
2022-04-23 14:22:00 | [train_policy] epoch #469 | gradient computed
2022-04-23 14:22:00 | [train_policy] epoch #469 | computing descent direction
2022-04-23 14:22:00 | [train_policy] epoch #469 | descent direction computed
2022-04-23 14:22:00 | [train_policy] epoch #469 | backtrack iters: 0
2022-04-23 14:22:00 | [train_policy] epoch #469 | optimization finished
2022-04-23 14:22:00 | [train_policy] epoch #469 | Computing KL after
2022-04-23 14:22:00 | [train_policy] epoch #469 | Computing loss after
2022-04-23 14:22:00 | [train_policy] epoch #469 | Fitting baseline...
2022-04-23 14:22:00 | [train_policy] epoch #469 | Saving snapshot...
2022-04-23 14:22:00 | [train_policy] epoch #469 | Saved
2022-04-23 14:22:00 | [train_policy] epoch #469 | Time 168.11 s
2022-04-23 14:22:00 | [train_policy] epoch #469 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117982
Evaluation/AverageDiscountedReturn          -64.513
Evaluation/AverageReturn                    -64.513
Evaluation/CompletionRate                     0
Evaluation/Iteration                        469
Evaluation/MaxReturn                        -29.3207
Evaluation/MinReturn                      -2062.87
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.6
Extras/EpisodeRewardMean                    -62.9185
LinearFeatureBaseline/ExplainedVariance       0.0135669
PolicyExecTime                                0.108167
ProcessExecTime                               0.0115545
TotalEnvSteps                            475640
policy/Entropy                               -0.248756
policy/KL                                     0.0097457
policy/KLBefore                               0
policy/LossAfter                             -0.0333378
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.77977
policy/dLoss                                  0.0333378
---------------------------------------  ----------------
2022-04-23 14:22:00 | [train_policy] epoch #470 | Obtaining samples for iteration 470...
2022-04-23 14:22:00 | [train_policy] epoch #470 | Logging diagnostics...
2022-04-23 14:22:00 | [train_policy] epoch #470 | Optimizing policy...
2022-04-23 14:22:00 | [train_policy] epoch #470 | Computing loss before
2022-04-23 14:22:00 | [train_policy] epoch #470 | Computing KL before
2022-04-23 14:22:00 | [train_policy] epoch #470 | Optimizing
2022-04-23 14:22:00 | [train_policy] epoch #470 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:00 | [train_policy] epoch #470 | computing loss before
2022-04-23 14:22:00 | [train_policy] epoch #470 | computing gradient
2022-04-23 14:22:00 | [train_policy] epoch #470 | gradient computed
2022-04-23 14:22:00 | [train_policy] epoch #470 | computing descent direction
2022-04-23 14:22:00 | [train_policy] epoch #470 | descent direction computed
2022-04-23 14:22:00 | [train_policy] epoch #470 | backtrack iters: 0
2022-04-23 14:22:00 | [train_policy] epoch #470 | optimization finished
2022-04-23 14:22:00 | [train_policy] epoch #470 | Computing KL after
2022-04-23 14:22:00 | [train_policy] epoch #470 | Computing loss after
2022-04-23 14:22:00 | [train_policy] epoch #470 | Fitting baseline...
2022-04-23 14:22:00 | [train_policy] epoch #470 | Saving snapshot...
2022-04-23 14:22:00 | [train_policy] epoch #470 | Saved
2022-04-23 14:22:00 | [train_policy] epoch #470 | Time 168.46 s
2022-04-23 14:22:00 | [train_policy] epoch #470 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.117091
Evaluation/AverageDiscountedReturn          -42.3774
Evaluation/AverageReturn                    -42.3774
Evaluation/CompletionRate                     0
Evaluation/Iteration                        470
Evaluation/MaxReturn                        -32.8118
Evaluation/MinReturn                        -80.416
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.49833
Extras/EpisodeRewardMean                    -42.2604
LinearFeatureBaseline/ExplainedVariance     -38.4991
PolicyExecTime                                0.104589
ProcessExecTime                               0.0113497
TotalEnvSteps                            476652
policy/Entropy                               -0.243215
policy/KL                                     0.00991052
policy/KLBefore                               0
policy/LossAfter                             -0.0684085
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             0.784103
policy/dLoss                                  0.0684085
---------------------------------------  ---------------
2022-04-23 14:22:00 | [train_policy] epoch #471 | Obtaining samples for iteration 471...
2022-04-23 14:22:01 | [train_policy] epoch #471 | Logging diagnostics...
2022-04-23 14:22:01 | [train_policy] epoch #471 | Optimizing policy...
2022-04-23 14:22:01 | [train_policy] epoch #471 | Computing loss before
2022-04-23 14:22:01 | [train_policy] epoch #471 | Computing KL before
2022-04-23 14:22:01 | [train_policy] epoch #471 | Optimizing
2022-04-23 14:22:01 | [train_policy] epoch #471 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:01 | [train_policy] epoch #471 | computing loss before
2022-04-23 14:22:01 | [train_policy] epoch #471 | computing gradient
2022-04-23 14:22:01 | [train_policy] epoch #471 | gradient computed
2022-04-23 14:22:01 | [train_policy] epoch #471 | computing descent direction
2022-04-23 14:22:01 | [train_policy] epoch #471 | descent direction computed
2022-04-23 14:22:01 | [train_policy] epoch #471 | backtrack iters: 0
2022-04-23 14:22:01 | [train_policy] epoch #471 | optimization finished
2022-04-23 14:22:01 | [train_policy] epoch #471 | Computing KL after
2022-04-23 14:22:01 | [train_policy] epoch #471 | Computing loss after
2022-04-23 14:22:01 | [train_policy] epoch #471 | Fitting baseline...
2022-04-23 14:22:01 | [train_policy] epoch #471 | Saving snapshot...
2022-04-23 14:22:01 | [train_policy] epoch #471 | Saved
2022-04-23 14:22:01 | [train_policy] epoch #471 | Time 168.81 s
2022-04-23 14:22:01 | [train_policy] epoch #471 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.123124
Evaluation/AverageDiscountedReturn          -43.9863
Evaluation/AverageReturn                    -43.9863
Evaluation/CompletionRate                     0
Evaluation/Iteration                        471
Evaluation/MaxReturn                        -31.1322
Evaluation/MinReturn                        -81.0175
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.87281
Extras/EpisodeRewardMean                    -44.14
LinearFeatureBaseline/ExplainedVariance       0.871376
PolicyExecTime                                0.102108
ProcessExecTime                               0.0120842
TotalEnvSteps                            477664
policy/Entropy                               -0.228635
policy/KL                                     0.00983623
policy/KLBefore                               0
policy/LossAfter                             -0.0152419
policy/LossBefore                            -1.83761e-08
policy/Perplexity                             0.795619
policy/dLoss                                  0.0152419
---------------------------------------  ----------------
2022-04-23 14:22:01 | [train_policy] epoch #472 | Obtaining samples for iteration 472...
2022-04-23 14:22:01 | [train_policy] epoch #472 | Logging diagnostics...
2022-04-23 14:22:01 | [train_policy] epoch #472 | Optimizing policy...
2022-04-23 14:22:01 | [train_policy] epoch #472 | Computing loss before
2022-04-23 14:22:01 | [train_policy] epoch #472 | Computing KL before
2022-04-23 14:22:01 | [train_policy] epoch #472 | Optimizing
2022-04-23 14:22:01 | [train_policy] epoch #472 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:01 | [train_policy] epoch #472 | computing loss before
2022-04-23 14:22:01 | [train_policy] epoch #472 | computing gradient
2022-04-23 14:22:01 | [train_policy] epoch #472 | gradient computed
2022-04-23 14:22:01 | [train_policy] epoch #472 | computing descent direction
2022-04-23 14:22:01 | [train_policy] epoch #472 | descent direction computed
2022-04-23 14:22:01 | [train_policy] epoch #472 | backtrack iters: 1
2022-04-23 14:22:01 | [train_policy] epoch #472 | optimization finished
2022-04-23 14:22:01 | [train_policy] epoch #472 | Computing KL after
2022-04-23 14:22:01 | [train_policy] epoch #472 | Computing loss after
2022-04-23 14:22:01 | [train_policy] epoch #472 | Fitting baseline...
2022-04-23 14:22:01 | [train_policy] epoch #472 | Saving snapshot...
2022-04-23 14:22:01 | [train_policy] epoch #472 | Saved
2022-04-23 14:22:01 | [train_policy] epoch #472 | Time 169.16 s
2022-04-23 14:22:01 | [train_policy] epoch #472 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11721
Evaluation/AverageDiscountedReturn          -44.443
Evaluation/AverageReturn                    -44.443
Evaluation/CompletionRate                     0
Evaluation/Iteration                        472
Evaluation/MaxReturn                        -29.6191
Evaluation/MinReturn                        -80.7916
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.22824
Extras/EpisodeRewardMean                    -44.5601
LinearFeatureBaseline/ExplainedVariance       0.87642
PolicyExecTime                                0.10177
ProcessExecTime                               0.011153
TotalEnvSteps                            478676
policy/Entropy                               -0.236634
policy/KL                                     0.00692181
policy/KLBefore                               0
policy/LossAfter                             -0.0126225
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.78928
policy/dLoss                                  0.0126225
---------------------------------------  ----------------
2022-04-23 14:22:01 | [train_policy] epoch #473 | Obtaining samples for iteration 473...
2022-04-23 14:22:01 | [train_policy] epoch #473 | Logging diagnostics...
2022-04-23 14:22:01 | [train_policy] epoch #473 | Optimizing policy...
2022-04-23 14:22:01 | [train_policy] epoch #473 | Computing loss before
2022-04-23 14:22:01 | [train_policy] epoch #473 | Computing KL before
2022-04-23 14:22:01 | [train_policy] epoch #473 | Optimizing
2022-04-23 14:22:01 | [train_policy] epoch #473 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:01 | [train_policy] epoch #473 | computing loss before
2022-04-23 14:22:01 | [train_policy] epoch #473 | computing gradient
2022-04-23 14:22:01 | [train_policy] epoch #473 | gradient computed
2022-04-23 14:22:01 | [train_policy] epoch #473 | computing descent direction
2022-04-23 14:22:01 | [train_policy] epoch #473 | descent direction computed
2022-04-23 14:22:01 | [train_policy] epoch #473 | backtrack iters: 0
2022-04-23 14:22:01 | [train_policy] epoch #473 | optimization finished
2022-04-23 14:22:01 | [train_policy] epoch #473 | Computing KL after
2022-04-23 14:22:01 | [train_policy] epoch #473 | Computing loss after
2022-04-23 14:22:01 | [train_policy] epoch #473 | Fitting baseline...
2022-04-23 14:22:01 | [train_policy] epoch #473 | Saving snapshot...
2022-04-23 14:22:01 | [train_policy] epoch #473 | Saved
2022-04-23 14:22:01 | [train_policy] epoch #473 | Time 169.49 s
2022-04-23 14:22:01 | [train_policy] epoch #473 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116698
Evaluation/AverageDiscountedReturn          -43.2882
Evaluation/AverageReturn                    -43.2882
Evaluation/CompletionRate                     0
Evaluation/Iteration                        473
Evaluation/MaxReturn                        -31.1729
Evaluation/MinReturn                        -90.9854
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.28752
Extras/EpisodeRewardMean                    -43.4376
LinearFeatureBaseline/ExplainedVariance       0.878638
PolicyExecTime                                0.0964155
ProcessExecTime                               0.0110009
TotalEnvSteps                            479688
policy/Entropy                               -0.233293
policy/KL                                     0.00967936
policy/KLBefore                               0
policy/LossAfter                             -0.015475
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.791921
policy/dLoss                                  0.015475
---------------------------------------  ----------------
2022-04-23 14:22:01 | [train_policy] epoch #474 | Obtaining samples for iteration 474...
2022-04-23 14:22:02 | [train_policy] epoch #474 | Logging diagnostics...
2022-04-23 14:22:02 | [train_policy] epoch #474 | Optimizing policy...
2022-04-23 14:22:02 | [train_policy] epoch #474 | Computing loss before
2022-04-23 14:22:02 | [train_policy] epoch #474 | Computing KL before
2022-04-23 14:22:02 | [train_policy] epoch #474 | Optimizing
2022-04-23 14:22:02 | [train_policy] epoch #474 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:02 | [train_policy] epoch #474 | computing loss before
2022-04-23 14:22:02 | [train_policy] epoch #474 | computing gradient
2022-04-23 14:22:02 | [train_policy] epoch #474 | gradient computed
2022-04-23 14:22:02 | [train_policy] epoch #474 | computing descent direction
2022-04-23 14:22:02 | [train_policy] epoch #474 | descent direction computed
2022-04-23 14:22:02 | [train_policy] epoch #474 | backtrack iters: 1
2022-04-23 14:22:02 | [train_policy] epoch #474 | optimization finished
2022-04-23 14:22:02 | [train_policy] epoch #474 | Computing KL after
2022-04-23 14:22:02 | [train_policy] epoch #474 | Computing loss after
2022-04-23 14:22:02 | [train_policy] epoch #474 | Fitting baseline...
2022-04-23 14:22:02 | [train_policy] epoch #474 | Saving snapshot...
2022-04-23 14:22:02 | [train_policy] epoch #474 | Saved
2022-04-23 14:22:02 | [train_policy] epoch #474 | Time 169.84 s
2022-04-23 14:22:02 | [train_policy] epoch #474 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117237
Evaluation/AverageDiscountedReturn          -43.8582
Evaluation/AverageReturn                    -43.8582
Evaluation/CompletionRate                     0
Evaluation/Iteration                        474
Evaluation/MaxReturn                        -30.117
Evaluation/MinReturn                        -88.0123
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.86806
Extras/EpisodeRewardMean                    -43.6401
LinearFeatureBaseline/ExplainedVariance       0.826304
PolicyExecTime                                0.0969181
ProcessExecTime                               0.0111313
TotalEnvSteps                            480700
policy/Entropy                               -0.265582
policy/KL                                     0.00671427
policy/KLBefore                               0
policy/LossAfter                             -0.0242905
policy/LossBefore                             2.04965e-08
policy/Perplexity                             0.76676
policy/dLoss                                  0.0242905
---------------------------------------  ----------------
2022-04-23 14:22:02 | [train_policy] epoch #475 | Obtaining samples for iteration 475...
2022-04-23 14:22:02 | [train_policy] epoch #475 | Logging diagnostics...
2022-04-23 14:22:02 | [train_policy] epoch #475 | Optimizing policy...
2022-04-23 14:22:02 | [train_policy] epoch #475 | Computing loss before
2022-04-23 14:22:02 | [train_policy] epoch #475 | Computing KL before
2022-04-23 14:22:02 | [train_policy] epoch #475 | Optimizing
2022-04-23 14:22:02 | [train_policy] epoch #475 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:02 | [train_policy] epoch #475 | computing loss before
2022-04-23 14:22:02 | [train_policy] epoch #475 | computing gradient
2022-04-23 14:22:02 | [train_policy] epoch #475 | gradient computed
2022-04-23 14:22:02 | [train_policy] epoch #475 | computing descent direction
2022-04-23 14:22:02 | [train_policy] epoch #475 | descent direction computed
2022-04-23 14:22:02 | [train_policy] epoch #475 | backtrack iters: 1
2022-04-23 14:22:02 | [train_policy] epoch #475 | optimization finished
2022-04-23 14:22:02 | [train_policy] epoch #475 | Computing KL after
2022-04-23 14:22:02 | [train_policy] epoch #475 | Computing loss after
2022-04-23 14:22:02 | [train_policy] epoch #475 | Fitting baseline...
2022-04-23 14:22:02 | [train_policy] epoch #475 | Saving snapshot...
2022-04-23 14:22:02 | [train_policy] epoch #475 | Saved
2022-04-23 14:22:02 | [train_policy] epoch #475 | Time 170.19 s
2022-04-23 14:22:02 | [train_policy] epoch #475 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116968
Evaluation/AverageDiscountedReturn          -42.6377
Evaluation/AverageReturn                    -42.6377
Evaluation/CompletionRate                     0
Evaluation/Iteration                        475
Evaluation/MaxReturn                        -29.5671
Evaluation/MinReturn                        -84.4513
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.76717
Extras/EpisodeRewardMean                    -42.2615
LinearFeatureBaseline/ExplainedVariance       0.884721
PolicyExecTime                                0.100595
ProcessExecTime                               0.0118802
TotalEnvSteps                            481712
policy/Entropy                               -0.256685
policy/KL                                     0.00703401
policy/KLBefore                               0
policy/LossAfter                             -0.0212192
policy/LossBefore                            -4.35844e-09
policy/Perplexity                             0.773612
policy/dLoss                                  0.0212192
---------------------------------------  ----------------
2022-04-23 14:22:02 | [train_policy] epoch #476 | Obtaining samples for iteration 476...
2022-04-23 14:22:02 | [train_policy] epoch #476 | Logging diagnostics...
2022-04-23 14:22:02 | [train_policy] epoch #476 | Optimizing policy...
2022-04-23 14:22:02 | [train_policy] epoch #476 | Computing loss before
2022-04-23 14:22:02 | [train_policy] epoch #476 | Computing KL before
2022-04-23 14:22:02 | [train_policy] epoch #476 | Optimizing
2022-04-23 14:22:02 | [train_policy] epoch #476 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:02 | [train_policy] epoch #476 | computing loss before
2022-04-23 14:22:02 | [train_policy] epoch #476 | computing gradient
2022-04-23 14:22:02 | [train_policy] epoch #476 | gradient computed
2022-04-23 14:22:02 | [train_policy] epoch #476 | computing descent direction
2022-04-23 14:22:02 | [train_policy] epoch #476 | descent direction computed
2022-04-23 14:22:02 | [train_policy] epoch #476 | backtrack iters: 1
2022-04-23 14:22:02 | [train_policy] epoch #476 | optimization finished
2022-04-23 14:22:02 | [train_policy] epoch #476 | Computing KL after
2022-04-23 14:22:02 | [train_policy] epoch #476 | Computing loss after
2022-04-23 14:22:02 | [train_policy] epoch #476 | Fitting baseline...
2022-04-23 14:22:02 | [train_policy] epoch #476 | Saving snapshot...
2022-04-23 14:22:02 | [train_policy] epoch #476 | Saved
2022-04-23 14:22:02 | [train_policy] epoch #476 | Time 170.54 s
2022-04-23 14:22:02 | [train_policy] epoch #476 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116187
Evaluation/AverageDiscountedReturn          -43.2663
Evaluation/AverageReturn                    -43.2663
Evaluation/CompletionRate                     0
Evaluation/Iteration                        476
Evaluation/MaxReturn                        -29.7356
Evaluation/MinReturn                        -91.2901
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.27945
Extras/EpisodeRewardMean                    -42.8196
LinearFeatureBaseline/ExplainedVariance       0.84497
PolicyExecTime                                0.105572
ProcessExecTime                               0.011507
TotalEnvSteps                            482724
policy/Entropy                               -0.286144
policy/KL                                     0.00773466
policy/KLBefore                               0
policy/LossAfter                             -0.0279223
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.751155
policy/dLoss                                  0.0279224
---------------------------------------  ----------------
2022-04-23 14:22:02 | [train_policy] epoch #477 | Obtaining samples for iteration 477...
2022-04-23 14:22:03 | [train_policy] epoch #477 | Logging diagnostics...
2022-04-23 14:22:03 | [train_policy] epoch #477 | Optimizing policy...
2022-04-23 14:22:03 | [train_policy] epoch #477 | Computing loss before
2022-04-23 14:22:03 | [train_policy] epoch #477 | Computing KL before
2022-04-23 14:22:03 | [train_policy] epoch #477 | Optimizing
2022-04-23 14:22:03 | [train_policy] epoch #477 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:03 | [train_policy] epoch #477 | computing loss before
2022-04-23 14:22:03 | [train_policy] epoch #477 | computing gradient
2022-04-23 14:22:03 | [train_policy] epoch #477 | gradient computed
2022-04-23 14:22:03 | [train_policy] epoch #477 | computing descent direction
2022-04-23 14:22:03 | [train_policy] epoch #477 | descent direction computed
2022-04-23 14:22:03 | [train_policy] epoch #477 | backtrack iters: 0
2022-04-23 14:22:03 | [train_policy] epoch #477 | optimization finished
2022-04-23 14:22:03 | [train_policy] epoch #477 | Computing KL after
2022-04-23 14:22:03 | [train_policy] epoch #477 | Computing loss after
2022-04-23 14:22:03 | [train_policy] epoch #477 | Fitting baseline...
2022-04-23 14:22:03 | [train_policy] epoch #477 | Saving snapshot...
2022-04-23 14:22:03 | [train_policy] epoch #477 | Saved
2022-04-23 14:22:03 | [train_policy] epoch #477 | Time 170.89 s
2022-04-23 14:22:03 | [train_policy] epoch #477 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.123015
Evaluation/AverageDiscountedReturn         -108.296
Evaluation/AverageReturn                   -108.296
Evaluation/CompletionRate                     0
Evaluation/Iteration                        477
Evaluation/MaxReturn                        -31.8686
Evaluation/MinReturn                      -2065.08
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.05
Extras/EpisodeRewardMean                   -102.967
LinearFeatureBaseline/ExplainedVariance       0.0141818
PolicyExecTime                                0.108265
ProcessExecTime                               0.0121756
TotalEnvSteps                            483736
policy/Entropy                               -0.271975
policy/KL                                     0.00955768
policy/KLBefore                               0
policy/LossAfter                             -0.0239406
policy/LossBefore                            -9.42366e-09
policy/Perplexity                             0.761873
policy/dLoss                                  0.0239406
---------------------------------------  ----------------
2022-04-23 14:22:03 | [train_policy] epoch #478 | Obtaining samples for iteration 478...
2022-04-23 14:22:03 | [train_policy] epoch #478 | Logging diagnostics...
2022-04-23 14:22:03 | [train_policy] epoch #478 | Optimizing policy...
2022-04-23 14:22:03 | [train_policy] epoch #478 | Computing loss before
2022-04-23 14:22:03 | [train_policy] epoch #478 | Computing KL before
2022-04-23 14:22:03 | [train_policy] epoch #478 | Optimizing
2022-04-23 14:22:03 | [train_policy] epoch #478 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:03 | [train_policy] epoch #478 | computing loss before
2022-04-23 14:22:03 | [train_policy] epoch #478 | computing gradient
2022-04-23 14:22:03 | [train_policy] epoch #478 | gradient computed
2022-04-23 14:22:03 | [train_policy] epoch #478 | computing descent direction
2022-04-23 14:22:03 | [train_policy] epoch #478 | descent direction computed
2022-04-23 14:22:03 | [train_policy] epoch #478 | backtrack iters: 0
2022-04-23 14:22:03 | [train_policy] epoch #478 | optimization finished
2022-04-23 14:22:03 | [train_policy] epoch #478 | Computing KL after
2022-04-23 14:22:03 | [train_policy] epoch #478 | Computing loss after
2022-04-23 14:22:03 | [train_policy] epoch #478 | Fitting baseline...
2022-04-23 14:22:03 | [train_policy] epoch #478 | Saving snapshot...
2022-04-23 14:22:03 | [train_policy] epoch #478 | Saved
2022-04-23 14:22:03 | [train_policy] epoch #478 | Time 171.25 s
2022-04-23 14:22:03 | [train_policy] epoch #478 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.128646
Evaluation/AverageDiscountedReturn          -41.9718
Evaluation/AverageReturn                    -41.9718
Evaluation/CompletionRate                     0
Evaluation/Iteration                        478
Evaluation/MaxReturn                        -30.2231
Evaluation/MinReturn                        -66.8658
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.50068
Extras/EpisodeRewardMean                    -41.8849
LinearFeatureBaseline/ExplainedVariance    -105.202
PolicyExecTime                                0.104184
ProcessExecTime                               0.0125504
TotalEnvSteps                            484748
policy/Entropy                               -0.2778
policy/KL                                     0.00943044
policy/KLBefore                               0
policy/LossAfter                             -0.0420208
policy/LossBefore                            -2.21456e-08
policy/Perplexity                             0.757448
policy/dLoss                                  0.0420208
---------------------------------------  ----------------
2022-04-23 14:22:03 | [train_policy] epoch #479 | Obtaining samples for iteration 479...
2022-04-23 14:22:03 | [train_policy] epoch #479 | Logging diagnostics...
2022-04-23 14:22:03 | [train_policy] epoch #479 | Optimizing policy...
2022-04-23 14:22:03 | [train_policy] epoch #479 | Computing loss before
2022-04-23 14:22:03 | [train_policy] epoch #479 | Computing KL before
2022-04-23 14:22:03 | [train_policy] epoch #479 | Optimizing
2022-04-23 14:22:03 | [train_policy] epoch #479 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:03 | [train_policy] epoch #479 | computing loss before
2022-04-23 14:22:03 | [train_policy] epoch #479 | computing gradient
2022-04-23 14:22:03 | [train_policy] epoch #479 | gradient computed
2022-04-23 14:22:03 | [train_policy] epoch #479 | computing descent direction
2022-04-23 14:22:03 | [train_policy] epoch #479 | descent direction computed
2022-04-23 14:22:03 | [train_policy] epoch #479 | backtrack iters: 0
2022-04-23 14:22:03 | [train_policy] epoch #479 | optimization finished
2022-04-23 14:22:03 | [train_policy] epoch #479 | Computing KL after
2022-04-23 14:22:03 | [train_policy] epoch #479 | Computing loss after
2022-04-23 14:22:03 | [train_policy] epoch #479 | Fitting baseline...
2022-04-23 14:22:03 | [train_policy] epoch #479 | Saving snapshot...
2022-04-23 14:22:03 | [train_policy] epoch #479 | Saved
2022-04-23 14:22:03 | [train_policy] epoch #479 | Time 171.60 s
2022-04-23 14:22:03 | [train_policy] epoch #479 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116384
Evaluation/AverageDiscountedReturn          -43.9157
Evaluation/AverageReturn                    -43.9157
Evaluation/CompletionRate                     0
Evaluation/Iteration                        479
Evaluation/MaxReturn                        -30.2825
Evaluation/MinReturn                       -105.361
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.0731
Extras/EpisodeRewardMean                    -43.7496
LinearFeatureBaseline/ExplainedVariance       0.787679
PolicyExecTime                                0.101078
ProcessExecTime                               0.0113358
TotalEnvSteps                            485760
policy/Entropy                               -0.277924
policy/KL                                     0.00923624
policy/KLBefore                               0
policy/LossAfter                             -0.0198969
policy/LossBefore                             7.30334e-09
policy/Perplexity                             0.757354
policy/dLoss                                  0.0198969
---------------------------------------  ----------------
2022-04-23 14:22:03 | [train_policy] epoch #480 | Obtaining samples for iteration 480...
2022-04-23 14:22:04 | [train_policy] epoch #480 | Logging diagnostics...
2022-04-23 14:22:04 | [train_policy] epoch #480 | Optimizing policy...
2022-04-23 14:22:04 | [train_policy] epoch #480 | Computing loss before
2022-04-23 14:22:04 | [train_policy] epoch #480 | Computing KL before
2022-04-23 14:22:04 | [train_policy] epoch #480 | Optimizing
2022-04-23 14:22:04 | [train_policy] epoch #480 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:04 | [train_policy] epoch #480 | computing loss before
2022-04-23 14:22:04 | [train_policy] epoch #480 | computing gradient
2022-04-23 14:22:04 | [train_policy] epoch #480 | gradient computed
2022-04-23 14:22:04 | [train_policy] epoch #480 | computing descent direction
2022-04-23 14:22:04 | [train_policy] epoch #480 | descent direction computed
2022-04-23 14:22:04 | [train_policy] epoch #480 | backtrack iters: 1
2022-04-23 14:22:04 | [train_policy] epoch #480 | optimization finished
2022-04-23 14:22:04 | [train_policy] epoch #480 | Computing KL after
2022-04-23 14:22:04 | [train_policy] epoch #480 | Computing loss after
2022-04-23 14:22:04 | [train_policy] epoch #480 | Fitting baseline...
2022-04-23 14:22:04 | [train_policy] epoch #480 | Saving snapshot...
2022-04-23 14:22:04 | [train_policy] epoch #480 | Saved
2022-04-23 14:22:04 | [train_policy] epoch #480 | Time 171.95 s
2022-04-23 14:22:04 | [train_policy] epoch #480 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116256
Evaluation/AverageDiscountedReturn          -43.3583
Evaluation/AverageReturn                    -43.3583
Evaluation/CompletionRate                     0
Evaluation/Iteration                        480
Evaluation/MaxReturn                        -28.8035
Evaluation/MinReturn                        -64.2959
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.91796
Extras/EpisodeRewardMean                    -43.5305
LinearFeatureBaseline/ExplainedVariance       0.864237
PolicyExecTime                                0.104949
ProcessExecTime                               0.0115087
TotalEnvSteps                            486772
policy/Entropy                               -0.286811
policy/KL                                     0.00650235
policy/KLBefore                               0
policy/LossAfter                             -0.0165102
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             0.750653
policy/dLoss                                  0.0165102
---------------------------------------  ----------------
2022-04-23 14:22:04 | [train_policy] epoch #481 | Obtaining samples for iteration 481...
2022-04-23 14:22:04 | [train_policy] epoch #481 | Logging diagnostics...
2022-04-23 14:22:04 | [train_policy] epoch #481 | Optimizing policy...
2022-04-23 14:22:04 | [train_policy] epoch #481 | Computing loss before
2022-04-23 14:22:04 | [train_policy] epoch #481 | Computing KL before
2022-04-23 14:22:04 | [train_policy] epoch #481 | Optimizing
2022-04-23 14:22:04 | [train_policy] epoch #481 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:04 | [train_policy] epoch #481 | computing loss before
2022-04-23 14:22:04 | [train_policy] epoch #481 | computing gradient
2022-04-23 14:22:04 | [train_policy] epoch #481 | gradient computed
2022-04-23 14:22:04 | [train_policy] epoch #481 | computing descent direction
2022-04-23 14:22:04 | [train_policy] epoch #481 | descent direction computed
2022-04-23 14:22:04 | [train_policy] epoch #481 | backtrack iters: 1
2022-04-23 14:22:04 | [train_policy] epoch #481 | optimization finished
2022-04-23 14:22:04 | [train_policy] epoch #481 | Computing KL after
2022-04-23 14:22:04 | [train_policy] epoch #481 | Computing loss after
2022-04-23 14:22:04 | [train_policy] epoch #481 | Fitting baseline...
2022-04-23 14:22:04 | [train_policy] epoch #481 | Saving snapshot...
2022-04-23 14:22:04 | [train_policy] epoch #481 | Saved
2022-04-23 14:22:04 | [train_policy] epoch #481 | Time 172.28 s
2022-04-23 14:22:04 | [train_policy] epoch #481 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116059
Evaluation/AverageDiscountedReturn          -42.1691
Evaluation/AverageReturn                    -42.1691
Evaluation/CompletionRate                     0
Evaluation/Iteration                        481
Evaluation/MaxReturn                        -28.6878
Evaluation/MinReturn                        -68.2632
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12723
Extras/EpisodeRewardMean                    -42.3575
LinearFeatureBaseline/ExplainedVariance       0.880444
PolicyExecTime                                0.0992978
ProcessExecTime                               0.0112934
TotalEnvSteps                            487784
policy/Entropy                               -0.327216
policy/KL                                     0.0068768
policy/KLBefore                               0
policy/LossAfter                             -0.0200739
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.720928
policy/dLoss                                  0.020074
---------------------------------------  ----------------
2022-04-23 14:22:04 | [train_policy] epoch #482 | Obtaining samples for iteration 482...
2022-04-23 14:22:04 | [train_policy] epoch #482 | Logging diagnostics...
2022-04-23 14:22:04 | [train_policy] epoch #482 | Optimizing policy...
2022-04-23 14:22:04 | [train_policy] epoch #482 | Computing loss before
2022-04-23 14:22:04 | [train_policy] epoch #482 | Computing KL before
2022-04-23 14:22:04 | [train_policy] epoch #482 | Optimizing
2022-04-23 14:22:04 | [train_policy] epoch #482 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:04 | [train_policy] epoch #482 | computing loss before
2022-04-23 14:22:04 | [train_policy] epoch #482 | computing gradient
2022-04-23 14:22:04 | [train_policy] epoch #482 | gradient computed
2022-04-23 14:22:04 | [train_policy] epoch #482 | computing descent direction
2022-04-23 14:22:04 | [train_policy] epoch #482 | descent direction computed
2022-04-23 14:22:04 | [train_policy] epoch #482 | backtrack iters: 1
2022-04-23 14:22:04 | [train_policy] epoch #482 | optimization finished
2022-04-23 14:22:04 | [train_policy] epoch #482 | Computing KL after
2022-04-23 14:22:04 | [train_policy] epoch #482 | Computing loss after
2022-04-23 14:22:04 | [train_policy] epoch #482 | Fitting baseline...
2022-04-23 14:22:04 | [train_policy] epoch #482 | Saving snapshot...
2022-04-23 14:22:04 | [train_policy] epoch #482 | Saved
2022-04-23 14:22:04 | [train_policy] epoch #482 | Time 172.62 s
2022-04-23 14:22:04 | [train_policy] epoch #482 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115438
Evaluation/AverageDiscountedReturn          -45.7933
Evaluation/AverageReturn                    -45.7933
Evaluation/CompletionRate                     0
Evaluation/Iteration                        482
Evaluation/MaxReturn                        -28.4056
Evaluation/MinReturn                       -371.131
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         34.7783
Extras/EpisodeRewardMean                    -45.2346
LinearFeatureBaseline/ExplainedVariance       0.159743
PolicyExecTime                                0.101528
ProcessExecTime                               0.0112698
TotalEnvSteps                            488796
policy/Entropy                               -0.336635
policy/KL                                     0.00725486
policy/KLBefore                               0
policy/LossAfter                             -0.0342089
policy/LossBefore                            -1.17796e-08
policy/Perplexity                             0.714169
policy/dLoss                                  0.0342088
---------------------------------------  ----------------
2022-04-23 14:22:04 | [train_policy] epoch #483 | Obtaining samples for iteration 483...
2022-04-23 14:22:05 | [train_policy] epoch #483 | Logging diagnostics...
2022-04-23 14:22:05 | [train_policy] epoch #483 | Optimizing policy...
2022-04-23 14:22:05 | [train_policy] epoch #483 | Computing loss before
2022-04-23 14:22:05 | [train_policy] epoch #483 | Computing KL before
2022-04-23 14:22:05 | [train_policy] epoch #483 | Optimizing
2022-04-23 14:22:05 | [train_policy] epoch #483 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:05 | [train_policy] epoch #483 | computing loss before
2022-04-23 14:22:05 | [train_policy] epoch #483 | computing gradient
2022-04-23 14:22:05 | [train_policy] epoch #483 | gradient computed
2022-04-23 14:22:05 | [train_policy] epoch #483 | computing descent direction
2022-04-23 14:22:05 | [train_policy] epoch #483 | descent direction computed
2022-04-23 14:22:05 | [train_policy] epoch #483 | backtrack iters: 1
2022-04-23 14:22:05 | [train_policy] epoch #483 | optimization finished
2022-04-23 14:22:05 | [train_policy] epoch #483 | Computing KL after
2022-04-23 14:22:05 | [train_policy] epoch #483 | Computing loss after
2022-04-23 14:22:05 | [train_policy] epoch #483 | Fitting baseline...
2022-04-23 14:22:05 | [train_policy] epoch #483 | Saving snapshot...
2022-04-23 14:22:05 | [train_policy] epoch #483 | Saved
2022-04-23 14:22:05 | [train_policy] epoch #483 | Time 172.97 s
2022-04-23 14:22:05 | [train_policy] epoch #483 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116913
Evaluation/AverageDiscountedReturn          -43.6134
Evaluation/AverageReturn                    -43.6134
Evaluation/CompletionRate                     0
Evaluation/Iteration                        483
Evaluation/MaxReturn                        -28.958
Evaluation/MinReturn                        -65.3911
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.46202
Extras/EpisodeRewardMean                    -43.1459
LinearFeatureBaseline/ExplainedVariance       0.371897
PolicyExecTime                                0.104046
ProcessExecTime                               0.0113795
TotalEnvSteps                            489808
policy/Entropy                               -0.336856
policy/KL                                     0.0067884
policy/KLBefore                               0
policy/LossAfter                             -0.0246744
policy/LossBefore                             1.87295e-08
policy/Perplexity                             0.714012
policy/dLoss                                  0.0246745
---------------------------------------  ----------------
2022-04-23 14:22:05 | [train_policy] epoch #484 | Obtaining samples for iteration 484...
2022-04-23 14:22:05 | [train_policy] epoch #484 | Logging diagnostics...
2022-04-23 14:22:05 | [train_policy] epoch #484 | Optimizing policy...
2022-04-23 14:22:05 | [train_policy] epoch #484 | Computing loss before
2022-04-23 14:22:05 | [train_policy] epoch #484 | Computing KL before
2022-04-23 14:22:05 | [train_policy] epoch #484 | Optimizing
2022-04-23 14:22:05 | [train_policy] epoch #484 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:05 | [train_policy] epoch #484 | computing loss before
2022-04-23 14:22:05 | [train_policy] epoch #484 | computing gradient
2022-04-23 14:22:05 | [train_policy] epoch #484 | gradient computed
2022-04-23 14:22:05 | [train_policy] epoch #484 | computing descent direction
2022-04-23 14:22:05 | [train_policy] epoch #484 | descent direction computed
2022-04-23 14:22:05 | [train_policy] epoch #484 | backtrack iters: 0
2022-04-23 14:22:05 | [train_policy] epoch #484 | optimization finished
2022-04-23 14:22:05 | [train_policy] epoch #484 | Computing KL after
2022-04-23 14:22:05 | [train_policy] epoch #484 | Computing loss after
2022-04-23 14:22:05 | [train_policy] epoch #484 | Fitting baseline...
2022-04-23 14:22:05 | [train_policy] epoch #484 | Saving snapshot...
2022-04-23 14:22:05 | [train_policy] epoch #484 | Saved
2022-04-23 14:22:05 | [train_policy] epoch #484 | Time 173.31 s
2022-04-23 14:22:05 | [train_policy] epoch #484 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11664
Evaluation/AverageDiscountedReturn          -42.256
Evaluation/AverageReturn                    -42.256
Evaluation/CompletionRate                     0
Evaluation/Iteration                        484
Evaluation/MaxReturn                        -29.6733
Evaluation/MinReturn                        -63.8358
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.29917
Extras/EpisodeRewardMean                    -42.5197
LinearFeatureBaseline/ExplainedVariance       0.897433
PolicyExecTime                                0.106338
ProcessExecTime                               0.0115719
TotalEnvSteps                            490820
policy/Entropy                               -0.391516
policy/KL                                     0.00963495
policy/KLBefore                               0
policy/LossAfter                             -0.0137078
policy/LossBefore                             4.82963e-09
policy/Perplexity                             0.676031
policy/dLoss                                  0.0137078
---------------------------------------  ----------------
2022-04-23 14:22:05 | [train_policy] epoch #485 | Obtaining samples for iteration 485...
2022-04-23 14:22:05 | [train_policy] epoch #485 | Logging diagnostics...
2022-04-23 14:22:05 | [train_policy] epoch #485 | Optimizing policy...
2022-04-23 14:22:05 | [train_policy] epoch #485 | Computing loss before
2022-04-23 14:22:05 | [train_policy] epoch #485 | Computing KL before
2022-04-23 14:22:05 | [train_policy] epoch #485 | Optimizing
2022-04-23 14:22:05 | [train_policy] epoch #485 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:05 | [train_policy] epoch #485 | computing loss before
2022-04-23 14:22:05 | [train_policy] epoch #485 | computing gradient
2022-04-23 14:22:05 | [train_policy] epoch #485 | gradient computed
2022-04-23 14:22:05 | [train_policy] epoch #485 | computing descent direction
2022-04-23 14:22:05 | [train_policy] epoch #485 | descent direction computed
2022-04-23 14:22:05 | [train_policy] epoch #485 | backtrack iters: 1
2022-04-23 14:22:05 | [train_policy] epoch #485 | optimization finished
2022-04-23 14:22:05 | [train_policy] epoch #485 | Computing KL after
2022-04-23 14:22:05 | [train_policy] epoch #485 | Computing loss after
2022-04-23 14:22:05 | [train_policy] epoch #485 | Fitting baseline...
2022-04-23 14:22:05 | [train_policy] epoch #485 | Saving snapshot...
2022-04-23 14:22:05 | [train_policy] epoch #485 | Saved
2022-04-23 14:22:05 | [train_policy] epoch #485 | Time 173.66 s
2022-04-23 14:22:05 | [train_policy] epoch #485 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116518
Evaluation/AverageDiscountedReturn          -86.3827
Evaluation/AverageReturn                    -86.3827
Evaluation/CompletionRate                     0
Evaluation/Iteration                        485
Evaluation/MaxReturn                        -29.5903
Evaluation/MinReturn                      -2072.28
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.497
Extras/EpisodeRewardMean                    -82.9805
LinearFeatureBaseline/ExplainedVariance       0.0109467
PolicyExecTime                                0.101612
ProcessExecTime                               0.0115237
TotalEnvSteps                            491832
policy/Entropy                               -0.397159
policy/KL                                     0.00660416
policy/KLBefore                               0
policy/LossAfter                             -0.0226006
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.672227
policy/dLoss                                  0.0226006
---------------------------------------  ----------------
2022-04-23 14:22:05 | [train_policy] epoch #486 | Obtaining samples for iteration 486...
2022-04-23 14:22:06 | [train_policy] epoch #486 | Logging diagnostics...
2022-04-23 14:22:06 | [train_policy] epoch #486 | Optimizing policy...
2022-04-23 14:22:06 | [train_policy] epoch #486 | Computing loss before
2022-04-23 14:22:06 | [train_policy] epoch #486 | Computing KL before
2022-04-23 14:22:06 | [train_policy] epoch #486 | Optimizing
2022-04-23 14:22:06 | [train_policy] epoch #486 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:06 | [train_policy] epoch #486 | computing loss before
2022-04-23 14:22:06 | [train_policy] epoch #486 | computing gradient
2022-04-23 14:22:06 | [train_policy] epoch #486 | gradient computed
2022-04-23 14:22:06 | [train_policy] epoch #486 | computing descent direction
2022-04-23 14:22:06 | [train_policy] epoch #486 | descent direction computed
2022-04-23 14:22:06 | [train_policy] epoch #486 | backtrack iters: 1
2022-04-23 14:22:06 | [train_policy] epoch #486 | optimization finished
2022-04-23 14:22:06 | [train_policy] epoch #486 | Computing KL after
2022-04-23 14:22:06 | [train_policy] epoch #486 | Computing loss after
2022-04-23 14:22:06 | [train_policy] epoch #486 | Fitting baseline...
2022-04-23 14:22:06 | [train_policy] epoch #486 | Saving snapshot...
2022-04-23 14:22:06 | [train_policy] epoch #486 | Saved
2022-04-23 14:22:06 | [train_policy] epoch #486 | Time 174.02 s
2022-04-23 14:22:06 | [train_policy] epoch #486 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.121128
Evaluation/AverageDiscountedReturn          -64.2859
Evaluation/AverageReturn                    -64.2859
Evaluation/CompletionRate                     0
Evaluation/Iteration                        486
Evaluation/MaxReturn                        -28.8798
Evaluation/MinReturn                      -2062.46
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.579
Extras/EpisodeRewardMean                    -62.3839
LinearFeatureBaseline/ExplainedVariance       0.0916345
PolicyExecTime                                0.106671
ProcessExecTime                               0.0119407
TotalEnvSteps                            492844
policy/Entropy                               -0.410896
policy/KL                                     0.00686022
policy/KLBefore                               0
policy/LossAfter                             -0.0213618
policy/LossBefore                            -1.53134e-08
policy/Perplexity                             0.663056
policy/dLoss                                  0.0213618
---------------------------------------  ----------------
2022-04-23 14:22:06 | [train_policy] epoch #487 | Obtaining samples for iteration 487...
2022-04-23 14:22:06 | [train_policy] epoch #487 | Logging diagnostics...
2022-04-23 14:22:06 | [train_policy] epoch #487 | Optimizing policy...
2022-04-23 14:22:06 | [train_policy] epoch #487 | Computing loss before
2022-04-23 14:22:06 | [train_policy] epoch #487 | Computing KL before
2022-04-23 14:22:06 | [train_policy] epoch #487 | Optimizing
2022-04-23 14:22:06 | [train_policy] epoch #487 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:06 | [train_policy] epoch #487 | computing loss before
2022-04-23 14:22:06 | [train_policy] epoch #487 | computing gradient
2022-04-23 14:22:06 | [train_policy] epoch #487 | gradient computed
2022-04-23 14:22:06 | [train_policy] epoch #487 | computing descent direction
2022-04-23 14:22:06 | [train_policy] epoch #487 | descent direction computed
2022-04-23 14:22:06 | [train_policy] epoch #487 | backtrack iters: 0
2022-04-23 14:22:06 | [train_policy] epoch #487 | optimization finished
2022-04-23 14:22:06 | [train_policy] epoch #487 | Computing KL after
2022-04-23 14:22:06 | [train_policy] epoch #487 | Computing loss after
2022-04-23 14:22:06 | [train_policy] epoch #487 | Fitting baseline...
2022-04-23 14:22:06 | [train_policy] epoch #487 | Saving snapshot...
2022-04-23 14:22:06 | [train_policy] epoch #487 | Saved
2022-04-23 14:22:06 | [train_policy] epoch #487 | Time 174.37 s
2022-04-23 14:22:06 | [train_policy] epoch #487 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.120748
Evaluation/AverageDiscountedReturn          -66.1533
Evaluation/AverageReturn                    -66.1533
Evaluation/CompletionRate                     0
Evaluation/Iteration                        487
Evaluation/MaxReturn                        -30.4203
Evaluation/MinReturn                      -2064.41
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.694
Extras/EpisodeRewardMean                    -63.8884
LinearFeatureBaseline/ExplainedVariance       0.108308
PolicyExecTime                                0.104888
ProcessExecTime                               0.0115888
TotalEnvSteps                            493856
policy/Entropy                               -0.46628
policy/KL                                     0.00999792
policy/KLBefore                               0
policy/LossAfter                             -0.0314916
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.627332
policy/dLoss                                  0.0314916
---------------------------------------  ----------------
2022-04-23 14:22:06 | [train_policy] epoch #488 | Obtaining samples for iteration 488...
2022-04-23 14:22:06 | [train_policy] epoch #488 | Logging diagnostics...
2022-04-23 14:22:06 | [train_policy] epoch #488 | Optimizing policy...
2022-04-23 14:22:06 | [train_policy] epoch #488 | Computing loss before
2022-04-23 14:22:06 | [train_policy] epoch #488 | Computing KL before
2022-04-23 14:22:06 | [train_policy] epoch #488 | Optimizing
2022-04-23 14:22:06 | [train_policy] epoch #488 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:06 | [train_policy] epoch #488 | computing loss before
2022-04-23 14:22:06 | [train_policy] epoch #488 | computing gradient
2022-04-23 14:22:06 | [train_policy] epoch #488 | gradient computed
2022-04-23 14:22:06 | [train_policy] epoch #488 | computing descent direction
2022-04-23 14:22:06 | [train_policy] epoch #488 | descent direction computed
2022-04-23 14:22:06 | [train_policy] epoch #488 | backtrack iters: 1
2022-04-23 14:22:06 | [train_policy] epoch #488 | optimization finished
2022-04-23 14:22:06 | [train_policy] epoch #488 | Computing KL after
2022-04-23 14:22:06 | [train_policy] epoch #488 | Computing loss after
2022-04-23 14:22:06 | [train_policy] epoch #488 | Fitting baseline...
2022-04-23 14:22:06 | [train_policy] epoch #488 | Saving snapshot...
2022-04-23 14:22:07 | [train_policy] epoch #488 | Saved
2022-04-23 14:22:07 | [train_policy] epoch #488 | Time 174.72 s
2022-04-23 14:22:07 | [train_policy] epoch #488 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119492
Evaluation/AverageDiscountedReturn          -40.5724
Evaluation/AverageReturn                    -40.5724
Evaluation/CompletionRate                     0
Evaluation/Iteration                        488
Evaluation/MaxReturn                        -33.0604
Evaluation/MinReturn                        -56.5992
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.12464
Extras/EpisodeRewardMean                    -40.9835
LinearFeatureBaseline/ExplainedVariance     -63.5804
PolicyExecTime                                0.0998211
ProcessExecTime                               0.0116434
TotalEnvSteps                            494868
policy/Entropy                               -0.473254
policy/KL                                     0.00645641
policy/KLBefore                               0
policy/LossAfter                             -0.018774
policy/LossBefore                             3.00379e-08
policy/Perplexity                             0.622972
policy/dLoss                                  0.018774
---------------------------------------  ----------------
2022-04-23 14:22:07 | [train_policy] epoch #489 | Obtaining samples for iteration 489...
2022-04-23 14:22:07 | [train_policy] epoch #489 | Logging diagnostics...
2022-04-23 14:22:07 | [train_policy] epoch #489 | Optimizing policy...
2022-04-23 14:22:07 | [train_policy] epoch #489 | Computing loss before
2022-04-23 14:22:07 | [train_policy] epoch #489 | Computing KL before
2022-04-23 14:22:07 | [train_policy] epoch #489 | Optimizing
2022-04-23 14:22:07 | [train_policy] epoch #489 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:07 | [train_policy] epoch #489 | computing loss before
2022-04-23 14:22:07 | [train_policy] epoch #489 | computing gradient
2022-04-23 14:22:07 | [train_policy] epoch #489 | gradient computed
2022-04-23 14:22:07 | [train_policy] epoch #489 | computing descent direction
2022-04-23 14:22:07 | [train_policy] epoch #489 | descent direction computed
2022-04-23 14:22:07 | [train_policy] epoch #489 | backtrack iters: 1
2022-04-23 14:22:07 | [train_policy] epoch #489 | optimization finished
2022-04-23 14:22:07 | [train_policy] epoch #489 | Computing KL after
2022-04-23 14:22:07 | [train_policy] epoch #489 | Computing loss after
2022-04-23 14:22:07 | [train_policy] epoch #489 | Fitting baseline...
2022-04-23 14:22:07 | [train_policy] epoch #489 | Saving snapshot...
2022-04-23 14:22:07 | [train_policy] epoch #489 | Saved
2022-04-23 14:22:07 | [train_policy] epoch #489 | Time 175.06 s
2022-04-23 14:22:07 | [train_policy] epoch #489 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11546
Evaluation/AverageDiscountedReturn          -42.5211
Evaluation/AverageReturn                    -42.5211
Evaluation/CompletionRate                     0
Evaluation/Iteration                        489
Evaluation/MaxReturn                        -29.7625
Evaluation/MinReturn                        -61.4611
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.84824
Extras/EpisodeRewardMean                    -42.6019
LinearFeatureBaseline/ExplainedVariance       0.907012
PolicyExecTime                                0.0948517
ProcessExecTime                               0.0111258
TotalEnvSteps                            495880
policy/Entropy                               -0.507224
policy/KL                                     0.00665626
policy/KLBefore                               0
policy/LossAfter                             -0.0168229
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.602165
policy/dLoss                                  0.0168229
---------------------------------------  ----------------
2022-04-23 14:22:07 | [train_policy] epoch #490 | Obtaining samples for iteration 490...
2022-04-23 14:22:07 | [train_policy] epoch #490 | Logging diagnostics...
2022-04-23 14:22:07 | [train_policy] epoch #490 | Optimizing policy...
2022-04-23 14:22:07 | [train_policy] epoch #490 | Computing loss before
2022-04-23 14:22:07 | [train_policy] epoch #490 | Computing KL before
2022-04-23 14:22:07 | [train_policy] epoch #490 | Optimizing
2022-04-23 14:22:07 | [train_policy] epoch #490 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:07 | [train_policy] epoch #490 | computing loss before
2022-04-23 14:22:07 | [train_policy] epoch #490 | computing gradient
2022-04-23 14:22:07 | [train_policy] epoch #490 | gradient computed
2022-04-23 14:22:07 | [train_policy] epoch #490 | computing descent direction
2022-04-23 14:22:07 | [train_policy] epoch #490 | descent direction computed
2022-04-23 14:22:07 | [train_policy] epoch #490 | backtrack iters: 1
2022-04-23 14:22:07 | [train_policy] epoch #490 | optimization finished
2022-04-23 14:22:07 | [train_policy] epoch #490 | Computing KL after
2022-04-23 14:22:07 | [train_policy] epoch #490 | Computing loss after
2022-04-23 14:22:07 | [train_policy] epoch #490 | Fitting baseline...
2022-04-23 14:22:07 | [train_policy] epoch #490 | Saving snapshot...
2022-04-23 14:22:07 | [train_policy] epoch #490 | Saved
2022-04-23 14:22:07 | [train_policy] epoch #490 | Time 175.41 s
2022-04-23 14:22:07 | [train_policy] epoch #490 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.11652
Evaluation/AverageDiscountedReturn          -42.1709
Evaluation/AverageReturn                    -42.1709
Evaluation/CompletionRate                     0
Evaluation/Iteration                        490
Evaluation/MaxReturn                        -30.1734
Evaluation/MinReturn                        -79.1975
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.68947
Extras/EpisodeRewardMean                    -42.4512
LinearFeatureBaseline/ExplainedVariance       0.864786
PolicyExecTime                                0.106708
ProcessExecTime                               0.0114226
TotalEnvSteps                            496892
policy/Entropy                               -0.538566
policy/KL                                     0.0067731
policy/KLBefore                               0
policy/LossAfter                             -0.0201349
policy/LossBefore                            -8.2457e-09
policy/Perplexity                             0.583585
policy/dLoss                                  0.0201349
---------------------------------------  ---------------
2022-04-23 14:22:07 | [train_policy] epoch #491 | Obtaining samples for iteration 491...
2022-04-23 14:22:07 | [train_policy] epoch #491 | Logging diagnostics...
2022-04-23 14:22:07 | [train_policy] epoch #491 | Optimizing policy...
2022-04-23 14:22:07 | [train_policy] epoch #491 | Computing loss before
2022-04-23 14:22:07 | [train_policy] epoch #491 | Computing KL before
2022-04-23 14:22:07 | [train_policy] epoch #491 | Optimizing
2022-04-23 14:22:07 | [train_policy] epoch #491 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:07 | [train_policy] epoch #491 | computing loss before
2022-04-23 14:22:07 | [train_policy] epoch #491 | computing gradient
2022-04-23 14:22:07 | [train_policy] epoch #491 | gradient computed
2022-04-23 14:22:07 | [train_policy] epoch #491 | computing descent direction
2022-04-23 14:22:08 | [train_policy] epoch #491 | descent direction computed
2022-04-23 14:22:08 | [train_policy] epoch #491 | backtrack iters: 0
2022-04-23 14:22:08 | [train_policy] epoch #491 | optimization finished
2022-04-23 14:22:08 | [train_policy] epoch #491 | Computing KL after
2022-04-23 14:22:08 | [train_policy] epoch #491 | Computing loss after
2022-04-23 14:22:08 | [train_policy] epoch #491 | Fitting baseline...
2022-04-23 14:22:08 | [train_policy] epoch #491 | Saving snapshot...
2022-04-23 14:22:08 | [train_policy] epoch #491 | Saved
2022-04-23 14:22:08 | [train_policy] epoch #491 | Time 175.76 s
2022-04-23 14:22:08 | [train_policy] epoch #491 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117784
Evaluation/AverageDiscountedReturn          -64.534
Evaluation/AverageReturn                    -64.534
Evaluation/CompletionRate                     0
Evaluation/Iteration                        491
Evaluation/MaxReturn                        -29.4567
Evaluation/MinReturn                      -2065.48
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.882
Extras/EpisodeRewardMean                    -62.4234
LinearFeatureBaseline/ExplainedVariance       0.0134765
PolicyExecTime                                0.105099
ProcessExecTime                               0.011575
TotalEnvSteps                            497904
policy/Entropy                               -0.524122
policy/KL                                     0.00907089
policy/KLBefore                               0
policy/LossAfter                             -0.0248254
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.592075
policy/dLoss                                  0.0248254
---------------------------------------  ----------------
2022-04-23 14:22:08 | [train_policy] epoch #492 | Obtaining samples for iteration 492...
2022-04-23 14:22:08 | [train_policy] epoch #492 | Logging diagnostics...
2022-04-23 14:22:08 | [train_policy] epoch #492 | Optimizing policy...
2022-04-23 14:22:08 | [train_policy] epoch #492 | Computing loss before
2022-04-23 14:22:08 | [train_policy] epoch #492 | Computing KL before
2022-04-23 14:22:08 | [train_policy] epoch #492 | Optimizing
2022-04-23 14:22:08 | [train_policy] epoch #492 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:08 | [train_policy] epoch #492 | computing loss before
2022-04-23 14:22:08 | [train_policy] epoch #492 | computing gradient
2022-04-23 14:22:08 | [train_policy] epoch #492 | gradient computed
2022-04-23 14:22:08 | [train_policy] epoch #492 | computing descent direction
2022-04-23 14:22:08 | [train_policy] epoch #492 | descent direction computed
2022-04-23 14:22:08 | [train_policy] epoch #492 | backtrack iters: 1
2022-04-23 14:22:08 | [train_policy] epoch #492 | optimization finished
2022-04-23 14:22:08 | [train_policy] epoch #492 | Computing KL after
2022-04-23 14:22:08 | [train_policy] epoch #492 | Computing loss after
2022-04-23 14:22:08 | [train_policy] epoch #492 | Fitting baseline...
2022-04-23 14:22:08 | [train_policy] epoch #492 | Saving snapshot...
2022-04-23 14:22:08 | [train_policy] epoch #492 | Saved
2022-04-23 14:22:08 | [train_policy] epoch #492 | Time 176.12 s
2022-04-23 14:22:08 | [train_policy] epoch #492 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.118369
Evaluation/AverageDiscountedReturn          -43.3816
Evaluation/AverageReturn                    -43.3816
Evaluation/CompletionRate                     0
Evaluation/Iteration                        492
Evaluation/MaxReturn                        -29.5769
Evaluation/MinReturn                        -68.3167
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.01191
Extras/EpisodeRewardMean                    -43.3898
LinearFeatureBaseline/ExplainedVariance     -24.053
PolicyExecTime                                0.103576
ProcessExecTime                               0.0114627
TotalEnvSteps                            498916
policy/Entropy                               -0.543476
policy/KL                                     0.00703297
policy/KLBefore                               0
policy/LossAfter                             -0.0171617
policy/LossBefore                            -1.7905e-08
policy/Perplexity                             0.580726
policy/dLoss                                  0.0171617
---------------------------------------  ---------------
2022-04-23 14:22:08 | [train_policy] epoch #493 | Obtaining samples for iteration 493...
2022-04-23 14:22:08 | [train_policy] epoch #493 | Logging diagnostics...
2022-04-23 14:22:08 | [train_policy] epoch #493 | Optimizing policy...
2022-04-23 14:22:08 | [train_policy] epoch #493 | Computing loss before
2022-04-23 14:22:08 | [train_policy] epoch #493 | Computing KL before
2022-04-23 14:22:08 | [train_policy] epoch #493 | Optimizing
2022-04-23 14:22:08 | [train_policy] epoch #493 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:08 | [train_policy] epoch #493 | computing loss before
2022-04-23 14:22:08 | [train_policy] epoch #493 | computing gradient
2022-04-23 14:22:08 | [train_policy] epoch #493 | gradient computed
2022-04-23 14:22:08 | [train_policy] epoch #493 | computing descent direction
2022-04-23 14:22:08 | [train_policy] epoch #493 | descent direction computed
2022-04-23 14:22:08 | [train_policy] epoch #493 | backtrack iters: 1
2022-04-23 14:22:08 | [train_policy] epoch #493 | optimization finished
2022-04-23 14:22:08 | [train_policy] epoch #493 | Computing KL after
2022-04-23 14:22:08 | [train_policy] epoch #493 | Computing loss after
2022-04-23 14:22:08 | [train_policy] epoch #493 | Fitting baseline...
2022-04-23 14:22:08 | [train_policy] epoch #493 | Saving snapshot...
2022-04-23 14:22:08 | [train_policy] epoch #493 | Saved
2022-04-23 14:22:08 | [train_policy] epoch #493 | Time 176.48 s
2022-04-23 14:22:08 | [train_policy] epoch #493 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116119
Evaluation/AverageDiscountedReturn          -64.972
Evaluation/AverageReturn                    -64.972
Evaluation/CompletionRate                     0
Evaluation/Iteration                        493
Evaluation/MaxReturn                        -33.2309
Evaluation/MinReturn                      -2069.56
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.32
Extras/EpisodeRewardMean                    -63.1462
LinearFeatureBaseline/ExplainedVariance       0.014513
PolicyExecTime                                0.104073
ProcessExecTime                               0.0111899
TotalEnvSteps                            499928
policy/Entropy                               -0.545643
policy/KL                                     0.00702386
policy/KLBefore                               0
policy/LossAfter                             -0.0228739
policy/LossBefore                             2.77998e-08
policy/Perplexity                             0.579469
policy/dLoss                                  0.0228739
---------------------------------------  ----------------
2022-04-23 14:22:08 | [train_policy] epoch #494 | Obtaining samples for iteration 494...
2022-04-23 14:22:09 | [train_policy] epoch #494 | Logging diagnostics...
2022-04-23 14:22:09 | [train_policy] epoch #494 | Optimizing policy...
2022-04-23 14:22:09 | [train_policy] epoch #494 | Computing loss before
2022-04-23 14:22:09 | [train_policy] epoch #494 | Computing KL before
2022-04-23 14:22:09 | [train_policy] epoch #494 | Optimizing
2022-04-23 14:22:09 | [train_policy] epoch #494 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:09 | [train_policy] epoch #494 | computing loss before
2022-04-23 14:22:09 | [train_policy] epoch #494 | computing gradient
2022-04-23 14:22:09 | [train_policy] epoch #494 | gradient computed
2022-04-23 14:22:09 | [train_policy] epoch #494 | computing descent direction
2022-04-23 14:22:09 | [train_policy] epoch #494 | descent direction computed
2022-04-23 14:22:09 | [train_policy] epoch #494 | backtrack iters: 1
2022-04-23 14:22:09 | [train_policy] epoch #494 | optimization finished
2022-04-23 14:22:09 | [train_policy] epoch #494 | Computing KL after
2022-04-23 14:22:09 | [train_policy] epoch #494 | Computing loss after
2022-04-23 14:22:09 | [train_policy] epoch #494 | Fitting baseline...
2022-04-23 14:22:09 | [train_policy] epoch #494 | Saving snapshot...
2022-04-23 14:22:09 | [train_policy] epoch #494 | Saved
2022-04-23 14:22:09 | [train_policy] epoch #494 | Time 176.82 s
2022-04-23 14:22:09 | [train_policy] epoch #494 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11595
Evaluation/AverageDiscountedReturn          -65.5013
Evaluation/AverageReturn                    -65.5013
Evaluation/CompletionRate                     0
Evaluation/Iteration                        494
Evaluation/MaxReturn                        -30.3264
Evaluation/MinReturn                      -2062.28
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.491
Extras/EpisodeRewardMean                    -84.2053
LinearFeatureBaseline/ExplainedVariance       0.0731425
PolicyExecTime                                0.0979016
ProcessExecTime                               0.0110159
TotalEnvSteps                            500940
policy/Entropy                               -0.557327
policy/KL                                     0.00679876
policy/KLBefore                               0
policy/LossAfter                             -0.0225383
policy/LossBefore                            -1.53134e-09
policy/Perplexity                             0.572738
policy/dLoss                                  0.0225383
---------------------------------------  ----------------
2022-04-23 14:22:09 | [train_policy] epoch #495 | Obtaining samples for iteration 495...
2022-04-23 14:22:09 | [train_policy] epoch #495 | Logging diagnostics...
2022-04-23 14:22:09 | [train_policy] epoch #495 | Optimizing policy...
2022-04-23 14:22:09 | [train_policy] epoch #495 | Computing loss before
2022-04-23 14:22:09 | [train_policy] epoch #495 | Computing KL before
2022-04-23 14:22:09 | [train_policy] epoch #495 | Optimizing
2022-04-23 14:22:09 | [train_policy] epoch #495 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:09 | [train_policy] epoch #495 | computing loss before
2022-04-23 14:22:09 | [train_policy] epoch #495 | computing gradient
2022-04-23 14:22:09 | [train_policy] epoch #495 | gradient computed
2022-04-23 14:22:09 | [train_policy] epoch #495 | computing descent direction
2022-04-23 14:22:09 | [train_policy] epoch #495 | descent direction computed
2022-04-23 14:22:09 | [train_policy] epoch #495 | backtrack iters: 1
2022-04-23 14:22:09 | [train_policy] epoch #495 | optimization finished
2022-04-23 14:22:09 | [train_policy] epoch #495 | Computing KL after
2022-04-23 14:22:09 | [train_policy] epoch #495 | Computing loss after
2022-04-23 14:22:09 | [train_policy] epoch #495 | Fitting baseline...
2022-04-23 14:22:09 | [train_policy] epoch #495 | Saving snapshot...
2022-04-23 14:22:09 | [train_policy] epoch #495 | Saved
2022-04-23 14:22:09 | [train_policy] epoch #495 | Time 177.15 s
2022-04-23 14:22:09 | [train_policy] epoch #495 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115941
Evaluation/AverageDiscountedReturn          -63.3063
Evaluation/AverageReturn                    -63.3063
Evaluation/CompletionRate                     0
Evaluation/Iteration                        495
Evaluation/MaxReturn                        -29.5147
Evaluation/MinReturn                      -2064.44
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.891
Extras/EpisodeRewardMean                    -61.794
LinearFeatureBaseline/ExplainedVariance       0.140038
PolicyExecTime                                0.0966802
ProcessExecTime                               0.0109482
TotalEnvSteps                            501952
policy/Entropy                               -0.573046
policy/KL                                     0.00729826
policy/KLBefore                               0
policy/LossAfter                             -0.0249785
policy/LossBefore                             1.41355e-08
policy/Perplexity                             0.563806
policy/dLoss                                  0.0249785
---------------------------------------  ----------------
2022-04-23 14:22:09 | [train_policy] epoch #496 | Obtaining samples for iteration 496...
2022-04-23 14:22:09 | [train_policy] epoch #496 | Logging diagnostics...
2022-04-23 14:22:09 | [train_policy] epoch #496 | Optimizing policy...
2022-04-23 14:22:09 | [train_policy] epoch #496 | Computing loss before
2022-04-23 14:22:09 | [train_policy] epoch #496 | Computing KL before
2022-04-23 14:22:09 | [train_policy] epoch #496 | Optimizing
2022-04-23 14:22:09 | [train_policy] epoch #496 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:09 | [train_policy] epoch #496 | computing loss before
2022-04-23 14:22:09 | [train_policy] epoch #496 | computing gradient
2022-04-23 14:22:09 | [train_policy] epoch #496 | gradient computed
2022-04-23 14:22:09 | [train_policy] epoch #496 | computing descent direction
2022-04-23 14:22:09 | [train_policy] epoch #496 | descent direction computed
2022-04-23 14:22:09 | [train_policy] epoch #496 | backtrack iters: 1
2022-04-23 14:22:09 | [train_policy] epoch #496 | optimization finished
2022-04-23 14:22:09 | [train_policy] epoch #496 | Computing KL after
2022-04-23 14:22:09 | [train_policy] epoch #496 | Computing loss after
2022-04-23 14:22:09 | [train_policy] epoch #496 | Fitting baseline...
2022-04-23 14:22:09 | [train_policy] epoch #496 | Saving snapshot...
2022-04-23 14:22:09 | [train_policy] epoch #496 | Saved
2022-04-23 14:22:09 | [train_policy] epoch #496 | Time 177.51 s
2022-04-23 14:22:09 | [train_policy] epoch #496 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11702
Evaluation/AverageDiscountedReturn          -84.9743
Evaluation/AverageReturn                    -84.9743
Evaluation/CompletionRate                     0
Evaluation/Iteration                        496
Evaluation/MaxReturn                        -31.5837
Evaluation/MinReturn                      -2074.33
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        296.034
Extras/EpisodeRewardMean                    -81.2786
LinearFeatureBaseline/ExplainedVariance       0.208864
PolicyExecTime                                0.106362
ProcessExecTime                               0.0114598
TotalEnvSteps                            502964
policy/Entropy                               -0.585134
policy/KL                                     0.00660806
policy/KLBefore                               0
policy/LossAfter                             -0.0241497
policy/LossBefore                            -1.06016e-08
policy/Perplexity                             0.557031
policy/dLoss                                  0.0241497
---------------------------------------  ----------------
2022-04-23 14:22:09 | [train_policy] epoch #497 | Obtaining samples for iteration 497...
2022-04-23 14:22:10 | [train_policy] epoch #497 | Logging diagnostics...
2022-04-23 14:22:10 | [train_policy] epoch #497 | Optimizing policy...
2022-04-23 14:22:10 | [train_policy] epoch #497 | Computing loss before
2022-04-23 14:22:10 | [train_policy] epoch #497 | Computing KL before
2022-04-23 14:22:10 | [train_policy] epoch #497 | Optimizing
2022-04-23 14:22:10 | [train_policy] epoch #497 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:10 | [train_policy] epoch #497 | computing loss before
2022-04-23 14:22:10 | [train_policy] epoch #497 | computing gradient
2022-04-23 14:22:10 | [train_policy] epoch #497 | gradient computed
2022-04-23 14:22:10 | [train_policy] epoch #497 | computing descent direction
2022-04-23 14:22:10 | [train_policy] epoch #497 | descent direction computed
2022-04-23 14:22:10 | [train_policy] epoch #497 | backtrack iters: 0
2022-04-23 14:22:10 | [train_policy] epoch #497 | optimization finished
2022-04-23 14:22:10 | [train_policy] epoch #497 | Computing KL after
2022-04-23 14:22:10 | [train_policy] epoch #497 | Computing loss after
2022-04-23 14:22:10 | [train_policy] epoch #497 | Fitting baseline...
2022-04-23 14:22:10 | [train_policy] epoch #497 | Saving snapshot...
2022-04-23 14:22:10 | [train_policy] epoch #497 | Saved
2022-04-23 14:22:10 | [train_policy] epoch #497 | Time 177.86 s
2022-04-23 14:22:10 | [train_policy] epoch #497 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117065
Evaluation/AverageDiscountedReturn          -43.0326
Evaluation/AverageReturn                    -43.0326
Evaluation/CompletionRate                     0
Evaluation/Iteration                        497
Evaluation/MaxReturn                        -33.1519
Evaluation/MinReturn                        -80.9503
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.02609
Extras/EpisodeRewardMean                    -62.9611
LinearFeatureBaseline/ExplainedVariance    -111.705
PolicyExecTime                                0.107743
ProcessExecTime                               0.0113993
TotalEnvSteps                            503976
policy/Entropy                               -0.571292
policy/KL                                     0.00930554
policy/KLBefore                               0
policy/LossAfter                             -0.0300347
policy/LossBefore                            -2.63862e-08
policy/Perplexity                             0.564795
policy/dLoss                                  0.0300347
---------------------------------------  ----------------
2022-04-23 14:22:10 | [train_policy] epoch #498 | Obtaining samples for iteration 498...
2022-04-23 14:22:10 | [train_policy] epoch #498 | Logging diagnostics...
2022-04-23 14:22:10 | [train_policy] epoch #498 | Optimizing policy...
2022-04-23 14:22:10 | [train_policy] epoch #498 | Computing loss before
2022-04-23 14:22:10 | [train_policy] epoch #498 | Computing KL before
2022-04-23 14:22:10 | [train_policy] epoch #498 | Optimizing
2022-04-23 14:22:10 | [train_policy] epoch #498 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:10 | [train_policy] epoch #498 | computing loss before
2022-04-23 14:22:10 | [train_policy] epoch #498 | computing gradient
2022-04-23 14:22:10 | [train_policy] epoch #498 | gradient computed
2022-04-23 14:22:10 | [train_policy] epoch #498 | computing descent direction
2022-04-23 14:22:10 | [train_policy] epoch #498 | descent direction computed
2022-04-23 14:22:10 | [train_policy] epoch #498 | backtrack iters: 1
2022-04-23 14:22:10 | [train_policy] epoch #498 | optimization finished
2022-04-23 14:22:10 | [train_policy] epoch #498 | Computing KL after
2022-04-23 14:22:10 | [train_policy] epoch #498 | Computing loss after
2022-04-23 14:22:10 | [train_policy] epoch #498 | Fitting baseline...
2022-04-23 14:22:10 | [train_policy] epoch #498 | Saving snapshot...
2022-04-23 14:22:10 | [train_policy] epoch #498 | Saved
2022-04-23 14:22:10 | [train_policy] epoch #498 | Time 178.21 s
2022-04-23 14:22:10 | [train_policy] epoch #498 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.115663
Evaluation/AverageDiscountedReturn          -42.2031
Evaluation/AverageReturn                    -42.2031
Evaluation/CompletionRate                     0
Evaluation/Iteration                        498
Evaluation/MaxReturn                        -32.8782
Evaluation/MinReturn                        -73.1845
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.9642
Extras/EpisodeRewardMean                    -42.1351
LinearFeatureBaseline/ExplainedVariance       0.827039
PolicyExecTime                                0.105953
ProcessExecTime                               0.0113242
TotalEnvSteps                            504988
policy/Entropy                               -0.571375
policy/KL                                     0.00705011
policy/KLBefore                               0
policy/LossAfter                             -0.0214012
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.564749
policy/dLoss                                  0.0214012
---------------------------------------  ----------------
2022-04-23 14:22:10 | [train_policy] epoch #499 | Obtaining samples for iteration 499...
2022-04-23 14:22:10 | [train_policy] epoch #499 | Logging diagnostics...
2022-04-23 14:22:10 | [train_policy] epoch #499 | Optimizing policy...
2022-04-23 14:22:10 | [train_policy] epoch #499 | Computing loss before
2022-04-23 14:22:10 | [train_policy] epoch #499 | Computing KL before
2022-04-23 14:22:10 | [train_policy] epoch #499 | Optimizing
2022-04-23 14:22:10 | [train_policy] epoch #499 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:10 | [train_policy] epoch #499 | computing loss before
2022-04-23 14:22:10 | [train_policy] epoch #499 | computing gradient
2022-04-23 14:22:10 | [train_policy] epoch #499 | gradient computed
2022-04-23 14:22:10 | [train_policy] epoch #499 | computing descent direction
2022-04-23 14:22:10 | [train_policy] epoch #499 | descent direction computed
2022-04-23 14:22:10 | [train_policy] epoch #499 | backtrack iters: 1
2022-04-23 14:22:10 | [train_policy] epoch #499 | optimization finished
2022-04-23 14:22:10 | [train_policy] epoch #499 | Computing KL after
2022-04-23 14:22:10 | [train_policy] epoch #499 | Computing loss after
2022-04-23 14:22:10 | [train_policy] epoch #499 | Fitting baseline...
2022-04-23 14:22:10 | [train_policy] epoch #499 | Saving snapshot...
2022-04-23 14:22:10 | [train_policy] epoch #499 | Saved
2022-04-23 14:22:10 | [train_policy] epoch #499 | Time 178.56 s
2022-04-23 14:22:10 | [train_policy] epoch #499 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116009
Evaluation/AverageDiscountedReturn          -42.0387
Evaluation/AverageReturn                    -42.0387
Evaluation/CompletionRate                     0
Evaluation/Iteration                        499
Evaluation/MaxReturn                        -29.0938
Evaluation/MinReturn                        -97.9075
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.8658
Extras/EpisodeRewardMean                    -42.0174
LinearFeatureBaseline/ExplainedVariance       0.772022
PolicyExecTime                                0.10459
ProcessExecTime                               0.0113733
TotalEnvSteps                            506000
policy/Entropy                               -0.577382
policy/KL                                     0.00712523
policy/KLBefore                               0
policy/LossAfter                             -0.0254123
policy/LossBefore                             1.60202e-08
policy/Perplexity                             0.561366
policy/dLoss                                  0.0254123
---------------------------------------  ----------------
2022-04-23 14:22:10 | [train_policy] epoch #500 | Obtaining samples for iteration 500...
2022-04-23 14:22:11 | [train_policy] epoch #500 | Logging diagnostics...
2022-04-23 14:22:11 | [train_policy] epoch #500 | Optimizing policy...
2022-04-23 14:22:11 | [train_policy] epoch #500 | Computing loss before
2022-04-23 14:22:11 | [train_policy] epoch #500 | Computing KL before
2022-04-23 14:22:11 | [train_policy] epoch #500 | Optimizing
2022-04-23 14:22:11 | [train_policy] epoch #500 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:11 | [train_policy] epoch #500 | computing loss before
2022-04-23 14:22:11 | [train_policy] epoch #500 | computing gradient
2022-04-23 14:22:11 | [train_policy] epoch #500 | gradient computed
2022-04-23 14:22:11 | [train_policy] epoch #500 | computing descent direction
2022-04-23 14:22:11 | [train_policy] epoch #500 | descent direction computed
2022-04-23 14:22:11 | [train_policy] epoch #500 | backtrack iters: 0
2022-04-23 14:22:11 | [train_policy] epoch #500 | optimization finished
2022-04-23 14:22:11 | [train_policy] epoch #500 | Computing KL after
2022-04-23 14:22:11 | [train_policy] epoch #500 | Computing loss after
2022-04-23 14:22:11 | [train_policy] epoch #500 | Fitting baseline...
2022-04-23 14:22:11 | [train_policy] epoch #500 | Saving snapshot...
2022-04-23 14:22:11 | [train_policy] epoch #500 | Saved
2022-04-23 14:22:11 | [train_policy] epoch #500 | Time 178.91 s
2022-04-23 14:22:11 | [train_policy] epoch #500 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117144
Evaluation/AverageDiscountedReturn          -85.0212
Evaluation/AverageReturn                    -85.0212
Evaluation/CompletionRate                     0
Evaluation/Iteration                        500
Evaluation/MaxReturn                        -29.6463
Evaluation/MinReturn                      -2092
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        296.518
Extras/EpisodeRewardMean                    -81.5514
LinearFeatureBaseline/ExplainedVariance       0.0177638
PolicyExecTime                                0.108354
ProcessExecTime                               0.0118229
TotalEnvSteps                            507012
policy/Entropy                               -0.56983
policy/KL                                     0.00988496
policy/KLBefore                               0
policy/LossAfter                             -0.0240104
policy/LossBefore                             1.31931e-08
policy/Perplexity                             0.565622
policy/dLoss                                  0.0240104
---------------------------------------  ----------------
2022-04-23 14:22:11 | [train_policy] epoch #501 | Obtaining samples for iteration 501...
2022-04-23 14:22:11 | [train_policy] epoch #501 | Logging diagnostics...
2022-04-23 14:22:11 | [train_policy] epoch #501 | Optimizing policy...
2022-04-23 14:22:11 | [train_policy] epoch #501 | Computing loss before
2022-04-23 14:22:11 | [train_policy] epoch #501 | Computing KL before
2022-04-23 14:22:11 | [train_policy] epoch #501 | Optimizing
2022-04-23 14:22:11 | [train_policy] epoch #501 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:11 | [train_policy] epoch #501 | computing loss before
2022-04-23 14:22:11 | [train_policy] epoch #501 | computing gradient
2022-04-23 14:22:11 | [train_policy] epoch #501 | gradient computed
2022-04-23 14:22:11 | [train_policy] epoch #501 | computing descent direction
2022-04-23 14:22:11 | [train_policy] epoch #501 | descent direction computed
2022-04-23 14:22:11 | [train_policy] epoch #501 | backtrack iters: 1
2022-04-23 14:22:11 | [train_policy] epoch #501 | optimization finished
2022-04-23 14:22:11 | [train_policy] epoch #501 | Computing KL after
2022-04-23 14:22:11 | [train_policy] epoch #501 | Computing loss after
2022-04-23 14:22:11 | [train_policy] epoch #501 | Fitting baseline...
2022-04-23 14:22:11 | [train_policy] epoch #501 | Saving snapshot...
2022-04-23 14:22:11 | [train_policy] epoch #501 | Saved
2022-04-23 14:22:11 | [train_policy] epoch #501 | Time 179.27 s
2022-04-23 14:22:11 | [train_policy] epoch #501 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.116692
Evaluation/AverageDiscountedReturn          -41.7155
Evaluation/AverageReturn                    -41.7155
Evaluation/CompletionRate                     0
Evaluation/Iteration                        501
Evaluation/MaxReturn                        -28.973
Evaluation/MinReturn                        -66.9116
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.62713
Extras/EpisodeRewardMean                    -62.2153
LinearFeatureBaseline/ExplainedVariance     -83.6935
PolicyExecTime                                0.107442
ProcessExecTime                               0.0115798
TotalEnvSteps                            508024
policy/Entropy                               -0.576983
policy/KL                                     0.00691731
policy/KLBefore                               0
policy/LossAfter                             -0.0148453
policy/LossBefore                            -1.1544e-08
policy/Perplexity                             0.56159
policy/dLoss                                  0.0148452
---------------------------------------  ---------------
2022-04-23 14:22:11 | [train_policy] epoch #502 | Obtaining samples for iteration 502...
2022-04-23 14:22:11 | [train_policy] epoch #502 | Logging diagnostics...
2022-04-23 14:22:11 | [train_policy] epoch #502 | Optimizing policy...
2022-04-23 14:22:11 | [train_policy] epoch #502 | Computing loss before
2022-04-23 14:22:11 | [train_policy] epoch #502 | Computing KL before
2022-04-23 14:22:11 | [train_policy] epoch #502 | Optimizing
2022-04-23 14:22:11 | [train_policy] epoch #502 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:11 | [train_policy] epoch #502 | computing loss before
2022-04-23 14:22:11 | [train_policy] epoch #502 | computing gradient
2022-04-23 14:22:11 | [train_policy] epoch #502 | gradient computed
2022-04-23 14:22:11 | [train_policy] epoch #502 | computing descent direction
2022-04-23 14:22:11 | [train_policy] epoch #502 | descent direction computed
2022-04-23 14:22:11 | [train_policy] epoch #502 | backtrack iters: 1
2022-04-23 14:22:11 | [train_policy] epoch #502 | optimization finished
2022-04-23 14:22:11 | [train_policy] epoch #502 | Computing KL after
2022-04-23 14:22:11 | [train_policy] epoch #502 | Computing loss after
2022-04-23 14:22:11 | [train_policy] epoch #502 | Fitting baseline...
2022-04-23 14:22:11 | [train_policy] epoch #502 | Saving snapshot...
2022-04-23 14:22:11 | [train_policy] epoch #502 | Saved
2022-04-23 14:22:11 | [train_policy] epoch #502 | Time 179.61 s
2022-04-23 14:22:11 | [train_policy] epoch #502 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115553
Evaluation/AverageDiscountedReturn          -62.7747
Evaluation/AverageReturn                    -62.7747
Evaluation/CompletionRate                     0
Evaluation/Iteration                        502
Evaluation/MaxReturn                        -32.2616
Evaluation/MinReturn                      -2069.27
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.446
Extras/EpisodeRewardMean                    -60.9463
LinearFeatureBaseline/ExplainedVariance       0.0141828
PolicyExecTime                                0.103608
ProcessExecTime                               0.0116115
TotalEnvSteps                            509036
policy/Entropy                               -0.56435
policy/KL                                     0.00906546
policy/KLBefore                               0
policy/LossAfter                             -0.000841219
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.56873
policy/dLoss                                  0.000841213
---------------------------------------  ----------------
2022-04-23 14:22:11 | [train_policy] epoch #503 | Obtaining samples for iteration 503...
2022-04-23 14:22:12 | [train_policy] epoch #503 | Logging diagnostics...
2022-04-23 14:22:12 | [train_policy] epoch #503 | Optimizing policy...
2022-04-23 14:22:12 | [train_policy] epoch #503 | Computing loss before
2022-04-23 14:22:12 | [train_policy] epoch #503 | Computing KL before
2022-04-23 14:22:12 | [train_policy] epoch #503 | Optimizing
2022-04-23 14:22:12 | [train_policy] epoch #503 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:12 | [train_policy] epoch #503 | computing loss before
2022-04-23 14:22:12 | [train_policy] epoch #503 | computing gradient
2022-04-23 14:22:12 | [train_policy] epoch #503 | gradient computed
2022-04-23 14:22:12 | [train_policy] epoch #503 | computing descent direction
2022-04-23 14:22:12 | [train_policy] epoch #503 | descent direction computed
2022-04-23 14:22:12 | [train_policy] epoch #503 | backtrack iters: 1
2022-04-23 14:22:12 | [train_policy] epoch #503 | optimization finished
2022-04-23 14:22:12 | [train_policy] epoch #503 | Computing KL after
2022-04-23 14:22:12 | [train_policy] epoch #503 | Computing loss after
2022-04-23 14:22:12 | [train_policy] epoch #503 | Fitting baseline...
2022-04-23 14:22:12 | [train_policy] epoch #503 | Saving snapshot...
2022-04-23 14:22:12 | [train_policy] epoch #503 | Saved
2022-04-23 14:22:12 | [train_policy] epoch #503 | Time 179.97 s
2022-04-23 14:22:12 | [train_policy] epoch #503 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116755
Evaluation/AverageDiscountedReturn          -86.7987
Evaluation/AverageReturn                    -86.7987
Evaluation/CompletionRate                     0
Evaluation/Iteration                        503
Evaluation/MaxReturn                        -29.9318
Evaluation/MinReturn                      -2068.06
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.321
Extras/EpisodeRewardMean                    -83.0912
LinearFeatureBaseline/ExplainedVariance       0.168197
PolicyExecTime                                0.103851
ProcessExecTime                               0.0123103
TotalEnvSteps                            510048
policy/Entropy                               -0.55978
policy/KL                                     0.00702859
policy/KLBefore                               0
policy/LossAfter                             -0.0232395
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.571335
policy/dLoss                                  0.0232395
---------------------------------------  ----------------
2022-04-23 14:22:12 | [train_policy] epoch #504 | Obtaining samples for iteration 504...
2022-04-23 14:22:12 | [train_policy] epoch #504 | Logging diagnostics...
2022-04-23 14:22:12 | [train_policy] epoch #504 | Optimizing policy...
2022-04-23 14:22:12 | [train_policy] epoch #504 | Computing loss before
2022-04-23 14:22:12 | [train_policy] epoch #504 | Computing KL before
2022-04-23 14:22:12 | [train_policy] epoch #504 | Optimizing
2022-04-23 14:22:12 | [train_policy] epoch #504 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:12 | [train_policy] epoch #504 | computing loss before
2022-04-23 14:22:12 | [train_policy] epoch #504 | computing gradient
2022-04-23 14:22:12 | [train_policy] epoch #504 | gradient computed
2022-04-23 14:22:12 | [train_policy] epoch #504 | computing descent direction
2022-04-23 14:22:12 | [train_policy] epoch #504 | descent direction computed
2022-04-23 14:22:12 | [train_policy] epoch #504 | backtrack iters: 0
2022-04-23 14:22:12 | [train_policy] epoch #504 | optimization finished
2022-04-23 14:22:12 | [train_policy] epoch #504 | Computing KL after
2022-04-23 14:22:12 | [train_policy] epoch #504 | Computing loss after
2022-04-23 14:22:12 | [train_policy] epoch #504 | Fitting baseline...
2022-04-23 14:22:12 | [train_policy] epoch #504 | Saving snapshot...
2022-04-23 14:22:12 | [train_policy] epoch #504 | Saved
2022-04-23 14:22:12 | [train_policy] epoch #504 | Time 180.32 s
2022-04-23 14:22:12 | [train_policy] epoch #504 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116582
Evaluation/AverageDiscountedReturn          -92.7075
Evaluation/AverageReturn                    -92.7075
Evaluation/CompletionRate                     0
Evaluation/Iteration                        504
Evaluation/MaxReturn                        -29.5173
Evaluation/MinReturn                      -2199.95
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        308.478
Extras/EpisodeRewardMean                    -88.4479
LinearFeatureBaseline/ExplainedVariance       0.192698
PolicyExecTime                                0.107677
ProcessExecTime                               0.0114808
TotalEnvSteps                            511060
policy/Entropy                               -0.557532
policy/KL                                     0.00944647
policy/KLBefore                               0
policy/LossAfter                             -0.030622
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             0.57262
policy/dLoss                                  0.030622
---------------------------------------  ----------------
2022-04-23 14:22:12 | [train_policy] epoch #505 | Obtaining samples for iteration 505...
2022-04-23 14:22:12 | [train_policy] epoch #505 | Logging diagnostics...
2022-04-23 14:22:12 | [train_policy] epoch #505 | Optimizing policy...
2022-04-23 14:22:12 | [train_policy] epoch #505 | Computing loss before
2022-04-23 14:22:12 | [train_policy] epoch #505 | Computing KL before
2022-04-23 14:22:12 | [train_policy] epoch #505 | Optimizing
2022-04-23 14:22:12 | [train_policy] epoch #505 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:12 | [train_policy] epoch #505 | computing loss before
2022-04-23 14:22:12 | [train_policy] epoch #505 | computing gradient
2022-04-23 14:22:12 | [train_policy] epoch #505 | gradient computed
2022-04-23 14:22:12 | [train_policy] epoch #505 | computing descent direction
2022-04-23 14:22:12 | [train_policy] epoch #505 | descent direction computed
2022-04-23 14:22:12 | [train_policy] epoch #505 | backtrack iters: 1
2022-04-23 14:22:12 | [train_policy] epoch #505 | optimization finished
2022-04-23 14:22:12 | [train_policy] epoch #505 | Computing KL after
2022-04-23 14:22:12 | [train_policy] epoch #505 | Computing loss after
2022-04-23 14:22:12 | [train_policy] epoch #505 | Fitting baseline...
2022-04-23 14:22:12 | [train_policy] epoch #505 | Saving snapshot...
2022-04-23 14:22:12 | [train_policy] epoch #505 | Saved
2022-04-23 14:22:12 | [train_policy] epoch #505 | Time 180.67 s
2022-04-23 14:22:12 | [train_policy] epoch #505 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115963
Evaluation/AverageDiscountedReturn          -64.1517
Evaluation/AverageReturn                    -64.1517
Evaluation/CompletionRate                     0
Evaluation/Iteration                        505
Evaluation/MaxReturn                        -29.0382
Evaluation/MinReturn                      -2053.5
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.765
Extras/EpisodeRewardMean                    -62.1707
LinearFeatureBaseline/ExplainedVariance      -0.0516301
PolicyExecTime                                0.10193
ProcessExecTime                               0.0114186
TotalEnvSteps                            512072
policy/Entropy                               -0.569168
policy/KL                                     0.00680558
policy/KLBefore                               0
policy/LossAfter                             -0.0133056
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.565996
policy/dLoss                                  0.0133056
---------------------------------------  ----------------
2022-04-23 14:22:12 | [train_policy] epoch #506 | Obtaining samples for iteration 506...
2022-04-23 14:22:13 | [train_policy] epoch #506 | Logging diagnostics...
2022-04-23 14:22:13 | [train_policy] epoch #506 | Optimizing policy...
2022-04-23 14:22:13 | [train_policy] epoch #506 | Computing loss before
2022-04-23 14:22:13 | [train_policy] epoch #506 | Computing KL before
2022-04-23 14:22:13 | [train_policy] epoch #506 | Optimizing
2022-04-23 14:22:13 | [train_policy] epoch #506 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:13 | [train_policy] epoch #506 | computing loss before
2022-04-23 14:22:13 | [train_policy] epoch #506 | computing gradient
2022-04-23 14:22:13 | [train_policy] epoch #506 | gradient computed
2022-04-23 14:22:13 | [train_policy] epoch #506 | computing descent direction
2022-04-23 14:22:13 | [train_policy] epoch #506 | descent direction computed
2022-04-23 14:22:13 | [train_policy] epoch #506 | backtrack iters: 0
2022-04-23 14:22:13 | [train_policy] epoch #506 | optimization finished
2022-04-23 14:22:13 | [train_policy] epoch #506 | Computing KL after
2022-04-23 14:22:13 | [train_policy] epoch #506 | Computing loss after
2022-04-23 14:22:13 | [train_policy] epoch #506 | Fitting baseline...
2022-04-23 14:22:13 | [train_policy] epoch #506 | Saving snapshot...
2022-04-23 14:22:13 | [train_policy] epoch #506 | Saved
2022-04-23 14:22:13 | [train_policy] epoch #506 | Time 181.01 s
2022-04-23 14:22:13 | [train_policy] epoch #506 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116374
Evaluation/AverageDiscountedReturn         -109.987
Evaluation/AverageReturn                   -109.987
Evaluation/CompletionRate                     0
Evaluation/Iteration                        506
Evaluation/MaxReturn                        -30.8542
Evaluation/MinReturn                      -2133.82
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        365.873
Extras/EpisodeRewardMean                   -124.677
LinearFeatureBaseline/ExplainedVariance       0.139326
PolicyExecTime                                0.104092
ProcessExecTime                               0.0115108
TotalEnvSteps                            513084
policy/Entropy                               -0.543993
policy/KL                                     0.00961757
policy/KLBefore                               0
policy/LossAfter                             -0.0201849
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.580426
policy/dLoss                                  0.0201849
---------------------------------------  ----------------
2022-04-23 14:22:13 | [train_policy] epoch #507 | Obtaining samples for iteration 507...
2022-04-23 14:22:13 | [train_policy] epoch #507 | Logging diagnostics...
2022-04-23 14:22:13 | [train_policy] epoch #507 | Optimizing policy...
2022-04-23 14:22:13 | [train_policy] epoch #507 | Computing loss before
2022-04-23 14:22:13 | [train_policy] epoch #507 | Computing KL before
2022-04-23 14:22:13 | [train_policy] epoch #507 | Optimizing
2022-04-23 14:22:13 | [train_policy] epoch #507 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:13 | [train_policy] epoch #507 | computing loss before
2022-04-23 14:22:13 | [train_policy] epoch #507 | computing gradient
2022-04-23 14:22:13 | [train_policy] epoch #507 | gradient computed
2022-04-23 14:22:13 | [train_policy] epoch #507 | computing descent direction
2022-04-23 14:22:13 | [train_policy] epoch #507 | descent direction computed
2022-04-23 14:22:13 | [train_policy] epoch #507 | backtrack iters: 1
2022-04-23 14:22:13 | [train_policy] epoch #507 | optimization finished
2022-04-23 14:22:13 | [train_policy] epoch #507 | Computing KL after
2022-04-23 14:22:13 | [train_policy] epoch #507 | Computing loss after
2022-04-23 14:22:13 | [train_policy] epoch #507 | Fitting baseline...
2022-04-23 14:22:13 | [train_policy] epoch #507 | Saving snapshot...
2022-04-23 14:22:13 | [train_policy] epoch #507 | Saved
2022-04-23 14:22:13 | [train_policy] epoch #507 | Time 181.37 s
2022-04-23 14:22:13 | [train_policy] epoch #507 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.122117
Evaluation/AverageDiscountedReturn          -43.0375
Evaluation/AverageReturn                    -43.0375
Evaluation/CompletionRate                     0
Evaluation/Iteration                        507
Evaluation/MaxReturn                        -32.2281
Evaluation/MinReturn                        -79.979
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.91381
Extras/EpisodeRewardMean                    -43.2015
LinearFeatureBaseline/ExplainedVariance    -115.766
PolicyExecTime                                0.108443
ProcessExecTime                               0.0120771
TotalEnvSteps                            514096
policy/Entropy                               -0.536003
policy/KL                                     0.00654905
policy/KLBefore                               0
policy/LossAfter                             -0.0137905
policy/LossBefore                             1.16618e-08
policy/Perplexity                             0.585082
policy/dLoss                                  0.0137905
---------------------------------------  ----------------
2022-04-23 14:22:13 | [train_policy] epoch #508 | Obtaining samples for iteration 508...
2022-04-23 14:22:13 | [train_policy] epoch #508 | Logging diagnostics...
2022-04-23 14:22:13 | [train_policy] epoch #508 | Optimizing policy...
2022-04-23 14:22:13 | [train_policy] epoch #508 | Computing loss before
2022-04-23 14:22:13 | [train_policy] epoch #508 | Computing KL before
2022-04-23 14:22:13 | [train_policy] epoch #508 | Optimizing
2022-04-23 14:22:13 | [train_policy] epoch #508 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:13 | [train_policy] epoch #508 | computing loss before
2022-04-23 14:22:13 | [train_policy] epoch #508 | computing gradient
2022-04-23 14:22:13 | [train_policy] epoch #508 | gradient computed
2022-04-23 14:22:13 | [train_policy] epoch #508 | computing descent direction
2022-04-23 14:22:13 | [train_policy] epoch #508 | descent direction computed
2022-04-23 14:22:13 | [train_policy] epoch #508 | backtrack iters: 0
2022-04-23 14:22:13 | [train_policy] epoch #508 | optimization finished
2022-04-23 14:22:13 | [train_policy] epoch #508 | Computing KL after
2022-04-23 14:22:13 | [train_policy] epoch #508 | Computing loss after
2022-04-23 14:22:13 | [train_policy] epoch #508 | Fitting baseline...
2022-04-23 14:22:14 | [train_policy] epoch #508 | Saving snapshot...
2022-04-23 14:22:14 | [train_policy] epoch #508 | Saved
2022-04-23 14:22:14 | [train_policy] epoch #508 | Time 181.72 s
2022-04-23 14:22:14 | [train_policy] epoch #508 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118187
Evaluation/AverageDiscountedReturn          -87.9934
Evaluation/AverageReturn                    -87.9934
Evaluation/CompletionRate                     0
Evaluation/Iteration                        508
Evaluation/MaxReturn                        -33.1474
Evaluation/MinReturn                      -2123.15
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        299.541
Extras/EpisodeRewardMean                    -84.4737
LinearFeatureBaseline/ExplainedVariance       0.0119119
PolicyExecTime                                0.109976
ProcessExecTime                               0.011801
TotalEnvSteps                            515108
policy/Entropy                               -0.514381
policy/KL                                     0.00892943
policy/KLBefore                               0
policy/LossAfter                             -0.020433
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.59787
policy/dLoss                                  0.020433
---------------------------------------  ----------------
2022-04-23 14:22:14 | [train_policy] epoch #509 | Obtaining samples for iteration 509...
2022-04-23 14:22:14 | [train_policy] epoch #509 | Logging diagnostics...
2022-04-23 14:22:14 | [train_policy] epoch #509 | Optimizing policy...
2022-04-23 14:22:14 | [train_policy] epoch #509 | Computing loss before
2022-04-23 14:22:14 | [train_policy] epoch #509 | Computing KL before
2022-04-23 14:22:14 | [train_policy] epoch #509 | Optimizing
2022-04-23 14:22:14 | [train_policy] epoch #509 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:14 | [train_policy] epoch #509 | computing loss before
2022-04-23 14:22:14 | [train_policy] epoch #509 | computing gradient
2022-04-23 14:22:14 | [train_policy] epoch #509 | gradient computed
2022-04-23 14:22:14 | [train_policy] epoch #509 | computing descent direction
2022-04-23 14:22:14 | [train_policy] epoch #509 | descent direction computed
2022-04-23 14:22:14 | [train_policy] epoch #509 | backtrack iters: 0
2022-04-23 14:22:14 | [train_policy] epoch #509 | optimization finished
2022-04-23 14:22:14 | [train_policy] epoch #509 | Computing KL after
2022-04-23 14:22:14 | [train_policy] epoch #509 | Computing loss after
2022-04-23 14:22:14 | [train_policy] epoch #509 | Fitting baseline...
2022-04-23 14:22:14 | [train_policy] epoch #509 | Saving snapshot...
2022-04-23 14:22:14 | [train_policy] epoch #509 | Saved
2022-04-23 14:22:14 | [train_policy] epoch #509 | Time 182.07 s
2022-04-23 14:22:14 | [train_policy] epoch #509 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116271
Evaluation/AverageDiscountedReturn          -43.6287
Evaluation/AverageReturn                    -43.6287
Evaluation/CompletionRate                     0
Evaluation/Iteration                        509
Evaluation/MaxReturn                        -32.484
Evaluation/MinReturn                        -78.2782
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.99538
Extras/EpisodeRewardMean                    -43.5891
LinearFeatureBaseline/ExplainedVariance     -55.2171
PolicyExecTime                                0.103638
ProcessExecTime                               0.0113966
TotalEnvSteps                            516120
policy/Entropy                               -0.498603
policy/KL                                     0.00939983
policy/KLBefore                               0
policy/LossAfter                             -0.0427164
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.607379
policy/dLoss                                  0.0427165
---------------------------------------  ----------------
2022-04-23 14:22:14 | [train_policy] epoch #510 | Obtaining samples for iteration 510...
2022-04-23 14:22:14 | [train_policy] epoch #510 | Logging diagnostics...
2022-04-23 14:22:14 | [train_policy] epoch #510 | Optimizing policy...
2022-04-23 14:22:14 | [train_policy] epoch #510 | Computing loss before
2022-04-23 14:22:14 | [train_policy] epoch #510 | Computing KL before
2022-04-23 14:22:14 | [train_policy] epoch #510 | Optimizing
2022-04-23 14:22:14 | [train_policy] epoch #510 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:14 | [train_policy] epoch #510 | computing loss before
2022-04-23 14:22:14 | [train_policy] epoch #510 | computing gradient
2022-04-23 14:22:14 | [train_policy] epoch #510 | gradient computed
2022-04-23 14:22:14 | [train_policy] epoch #510 | computing descent direction
2022-04-23 14:22:14 | [train_policy] epoch #510 | descent direction computed
2022-04-23 14:22:14 | [train_policy] epoch #510 | backtrack iters: 1
2022-04-23 14:22:14 | [train_policy] epoch #510 | optimization finished
2022-04-23 14:22:14 | [train_policy] epoch #510 | Computing KL after
2022-04-23 14:22:14 | [train_policy] epoch #510 | Computing loss after
2022-04-23 14:22:14 | [train_policy] epoch #510 | Fitting baseline...
2022-04-23 14:22:14 | [train_policy] epoch #510 | Saving snapshot...
2022-04-23 14:22:14 | [train_policy] epoch #510 | Saved
2022-04-23 14:22:14 | [train_policy] epoch #510 | Time 182.42 s
2022-04-23 14:22:14 | [train_policy] epoch #510 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116395
Evaluation/AverageDiscountedReturn          -64.3458
Evaluation/AverageReturn                    -64.3458
Evaluation/CompletionRate                     0
Evaluation/Iteration                        510
Evaluation/MaxReturn                        -29.8794
Evaluation/MinReturn                      -2075.85
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.972
Extras/EpisodeRewardMean                    -62.6567
LinearFeatureBaseline/ExplainedVariance       0.0148517
PolicyExecTime                                0.102717
ProcessExecTime                               0.0114808
TotalEnvSteps                            517132
policy/Entropy                               -0.496662
policy/KL                                     0.00769793
policy/KLBefore                               0
policy/LossAfter                             -0.0219606
policy/LossBefore                             2.63862e-08
policy/Perplexity                             0.608558
policy/dLoss                                  0.0219606
---------------------------------------  ----------------
2022-04-23 14:22:14 | [train_policy] epoch #511 | Obtaining samples for iteration 511...
2022-04-23 14:22:14 | [train_policy] epoch #511 | Logging diagnostics...
2022-04-23 14:22:14 | [train_policy] epoch #511 | Optimizing policy...
2022-04-23 14:22:14 | [train_policy] epoch #511 | Computing loss before
2022-04-23 14:22:14 | [train_policy] epoch #511 | Computing KL before
2022-04-23 14:22:14 | [train_policy] epoch #511 | Optimizing
2022-04-23 14:22:14 | [train_policy] epoch #511 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:14 | [train_policy] epoch #511 | computing loss before
2022-04-23 14:22:14 | [train_policy] epoch #511 | computing gradient
2022-04-23 14:22:14 | [train_policy] epoch #511 | gradient computed
2022-04-23 14:22:14 | [train_policy] epoch #511 | computing descent direction
2022-04-23 14:22:15 | [train_policy] epoch #511 | descent direction computed
2022-04-23 14:22:15 | [train_policy] epoch #511 | backtrack iters: 1
2022-04-23 14:22:15 | [train_policy] epoch #511 | optimization finished
2022-04-23 14:22:15 | [train_policy] epoch #511 | Computing KL after
2022-04-23 14:22:15 | [train_policy] epoch #511 | Computing loss after
2022-04-23 14:22:15 | [train_policy] epoch #511 | Fitting baseline...
2022-04-23 14:22:15 | [train_policy] epoch #511 | Saving snapshot...
2022-04-23 14:22:15 | [train_policy] epoch #511 | Saved
2022-04-23 14:22:15 | [train_policy] epoch #511 | Time 182.77 s
2022-04-23 14:22:15 | [train_policy] epoch #511 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.11584
Evaluation/AverageDiscountedReturn          -43.94
Evaluation/AverageReturn                    -43.94
Evaluation/CompletionRate                     0
Evaluation/Iteration                        511
Evaluation/MaxReturn                        -33.0278
Evaluation/MinReturn                       -114.945
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.6027
Extras/EpisodeRewardMean                    -44.0383
LinearFeatureBaseline/ExplainedVariance     -33.7802
PolicyExecTime                                0.105601
ProcessExecTime                               0.0113285
TotalEnvSteps                            518144
policy/Entropy                               -0.526761
policy/KL                                     0.00708645
policy/KLBefore                               0
policy/LossAfter                             -0.0104152
policy/LossBefore                             1.7905e-08
policy/Perplexity                             0.590515
policy/dLoss                                  0.0104152
---------------------------------------  ---------------
2022-04-23 14:22:15 | [train_policy] epoch #512 | Obtaining samples for iteration 512...
2022-04-23 14:22:15 | [train_policy] epoch #512 | Logging diagnostics...
2022-04-23 14:22:15 | [train_policy] epoch #512 | Optimizing policy...
2022-04-23 14:22:15 | [train_policy] epoch #512 | Computing loss before
2022-04-23 14:22:15 | [train_policy] epoch #512 | Computing KL before
2022-04-23 14:22:15 | [train_policy] epoch #512 | Optimizing
2022-04-23 14:22:15 | [train_policy] epoch #512 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:15 | [train_policy] epoch #512 | computing loss before
2022-04-23 14:22:15 | [train_policy] epoch #512 | computing gradient
2022-04-23 14:22:15 | [train_policy] epoch #512 | gradient computed
2022-04-23 14:22:15 | [train_policy] epoch #512 | computing descent direction
2022-04-23 14:22:15 | [train_policy] epoch #512 | descent direction computed
2022-04-23 14:22:15 | [train_policy] epoch #512 | backtrack iters: 1
2022-04-23 14:22:15 | [train_policy] epoch #512 | optimization finished
2022-04-23 14:22:15 | [train_policy] epoch #512 | Computing KL after
2022-04-23 14:22:15 | [train_policy] epoch #512 | Computing loss after
2022-04-23 14:22:15 | [train_policy] epoch #512 | Fitting baseline...
2022-04-23 14:22:15 | [train_policy] epoch #512 | Saving snapshot...
2022-04-23 14:22:15 | [train_policy] epoch #512 | Saved
2022-04-23 14:22:15 | [train_policy] epoch #512 | Time 183.12 s
2022-04-23 14:22:15 | [train_policy] epoch #512 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115528
Evaluation/AverageDiscountedReturn          -44.3798
Evaluation/AverageReturn                    -44.3798
Evaluation/CompletionRate                     0
Evaluation/Iteration                        512
Evaluation/MaxReturn                        -28.8075
Evaluation/MinReturn                       -287.759
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         26.4844
Extras/EpisodeRewardMean                    -44.1272
LinearFeatureBaseline/ExplainedVariance       0.249801
PolicyExecTime                                0.104993
ProcessExecTime                               0.0112698
TotalEnvSteps                            519156
policy/Entropy                               -0.554923
policy/KL                                     0.00870778
policy/KLBefore                               0
policy/LossAfter                             -0.0230738
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             0.574117
policy/dLoss                                  0.0230738
---------------------------------------  ----------------
2022-04-23 14:22:15 | [train_policy] epoch #513 | Obtaining samples for iteration 513...
2022-04-23 14:22:15 | [train_policy] epoch #513 | Logging diagnostics...
2022-04-23 14:22:15 | [train_policy] epoch #513 | Optimizing policy...
2022-04-23 14:22:15 | [train_policy] epoch #513 | Computing loss before
2022-04-23 14:22:15 | [train_policy] epoch #513 | Computing KL before
2022-04-23 14:22:15 | [train_policy] epoch #513 | Optimizing
2022-04-23 14:22:15 | [train_policy] epoch #513 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:15 | [train_policy] epoch #513 | computing loss before
2022-04-23 14:22:15 | [train_policy] epoch #513 | computing gradient
2022-04-23 14:22:15 | [train_policy] epoch #513 | gradient computed
2022-04-23 14:22:15 | [train_policy] epoch #513 | computing descent direction
2022-04-23 14:22:15 | [train_policy] epoch #513 | descent direction computed
2022-04-23 14:22:15 | [train_policy] epoch #513 | backtrack iters: 0
2022-04-23 14:22:15 | [train_policy] epoch #513 | optimization finished
2022-04-23 14:22:15 | [train_policy] epoch #513 | Computing KL after
2022-04-23 14:22:15 | [train_policy] epoch #513 | Computing loss after
2022-04-23 14:22:15 | [train_policy] epoch #513 | Fitting baseline...
2022-04-23 14:22:15 | [train_policy] epoch #513 | Saving snapshot...
2022-04-23 14:22:15 | [train_policy] epoch #513 | Saved
2022-04-23 14:22:15 | [train_policy] epoch #513 | Time 183.47 s
2022-04-23 14:22:15 | [train_policy] epoch #513 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115798
Evaluation/AverageDiscountedReturn          -64.9104
Evaluation/AverageReturn                    -64.9104
Evaluation/CompletionRate                     0
Evaluation/Iteration                        513
Evaluation/MaxReturn                        -32.2398
Evaluation/MinReturn                      -2067.92
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.169
Extras/EpisodeRewardMean                    -62.904
LinearFeatureBaseline/ExplainedVariance       0.0326728
PolicyExecTime                                0.104409
ProcessExecTime                               0.011369
TotalEnvSteps                            520168
policy/Entropy                               -0.550408
policy/KL                                     0.00926959
policy/KLBefore                               0
policy/LossAfter                             -0.0242387
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.576715
policy/dLoss                                  0.0242387
---------------------------------------  ----------------
2022-04-23 14:22:15 | [train_policy] epoch #514 | Obtaining samples for iteration 514...
2022-04-23 14:22:16 | [train_policy] epoch #514 | Logging diagnostics...
2022-04-23 14:22:16 | [train_policy] epoch #514 | Optimizing policy...
2022-04-23 14:22:16 | [train_policy] epoch #514 | Computing loss before
2022-04-23 14:22:16 | [train_policy] epoch #514 | Computing KL before
2022-04-23 14:22:16 | [train_policy] epoch #514 | Optimizing
2022-04-23 14:22:16 | [train_policy] epoch #514 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:16 | [train_policy] epoch #514 | computing loss before
2022-04-23 14:22:16 | [train_policy] epoch #514 | computing gradient
2022-04-23 14:22:16 | [train_policy] epoch #514 | gradient computed
2022-04-23 14:22:16 | [train_policy] epoch #514 | computing descent direction
2022-04-23 14:22:16 | [train_policy] epoch #514 | descent direction computed
2022-04-23 14:22:16 | [train_policy] epoch #514 | backtrack iters: 1
2022-04-23 14:22:16 | [train_policy] epoch #514 | optimization finished
2022-04-23 14:22:16 | [train_policy] epoch #514 | Computing KL after
2022-04-23 14:22:16 | [train_policy] epoch #514 | Computing loss after
2022-04-23 14:22:16 | [train_policy] epoch #514 | Fitting baseline...
2022-04-23 14:22:16 | [train_policy] epoch #514 | Saving snapshot...
2022-04-23 14:22:16 | [train_policy] epoch #514 | Saved
2022-04-23 14:22:16 | [train_policy] epoch #514 | Time 183.83 s
2022-04-23 14:22:16 | [train_policy] epoch #514 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.116494
Evaluation/AverageDiscountedReturn          -42.0001
Evaluation/AverageReturn                    -42.0001
Evaluation/CompletionRate                     0
Evaluation/Iteration                        514
Evaluation/MaxReturn                        -32.6921
Evaluation/MinReturn                        -58.3044
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          5.98891
Extras/EpisodeRewardMean                    -42.1023
LinearFeatureBaseline/ExplainedVariance     -29.3801
PolicyExecTime                                0.107836
ProcessExecTime                               0.011342
TotalEnvSteps                            521180
policy/Entropy                               -0.568623
policy/KL                                     0.00640112
policy/KLBefore                               0
policy/LossAfter                             -0.0179171
policy/LossBefore                            -1.0366e-08
policy/Perplexity                             0.566305
policy/dLoss                                  0.0179171
---------------------------------------  ---------------
2022-04-23 14:22:16 | [train_policy] epoch #515 | Obtaining samples for iteration 515...
2022-04-23 14:22:16 | [train_policy] epoch #515 | Logging diagnostics...
2022-04-23 14:22:16 | [train_policy] epoch #515 | Optimizing policy...
2022-04-23 14:22:16 | [train_policy] epoch #515 | Computing loss before
2022-04-23 14:22:16 | [train_policy] epoch #515 | Computing KL before
2022-04-23 14:22:16 | [train_policy] epoch #515 | Optimizing
2022-04-23 14:22:16 | [train_policy] epoch #515 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:16 | [train_policy] epoch #515 | computing loss before
2022-04-23 14:22:16 | [train_policy] epoch #515 | computing gradient
2022-04-23 14:22:16 | [train_policy] epoch #515 | gradient computed
2022-04-23 14:22:16 | [train_policy] epoch #515 | computing descent direction
2022-04-23 14:22:16 | [train_policy] epoch #515 | descent direction computed
2022-04-23 14:22:16 | [train_policy] epoch #515 | backtrack iters: 0
2022-04-23 14:22:16 | [train_policy] epoch #515 | optimization finished
2022-04-23 14:22:16 | [train_policy] epoch #515 | Computing KL after
2022-04-23 14:22:16 | [train_policy] epoch #515 | Computing loss after
2022-04-23 14:22:16 | [train_policy] epoch #515 | Fitting baseline...
2022-04-23 14:22:16 | [train_policy] epoch #515 | Saving snapshot...
2022-04-23 14:22:16 | [train_policy] epoch #515 | Saved
2022-04-23 14:22:16 | [train_policy] epoch #515 | Time 184.21 s
2022-04-23 14:22:16 | [train_policy] epoch #515 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.12855
Evaluation/AverageDiscountedReturn          -64.0222
Evaluation/AverageReturn                    -64.0222
Evaluation/CompletionRate                     0
Evaluation/Iteration                        515
Evaluation/MaxReturn                        -30.5418
Evaluation/MinReturn                      -2072.33
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.694
Extras/EpisodeRewardMean                    -62.2139
LinearFeatureBaseline/ExplainedVariance       0.0109523
PolicyExecTime                                0.115098
ProcessExecTime                               0.0136652
TotalEnvSteps                            522192
policy/Entropy                               -0.541661
policy/KL                                     0.00887064
policy/KLBefore                               0
policy/LossAfter                             -0.0181077
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.581781
policy/dLoss                                  0.0181077
---------------------------------------  ----------------
2022-04-23 14:22:16 | [train_policy] epoch #516 | Obtaining samples for iteration 516...
2022-04-23 14:22:16 | [train_policy] epoch #516 | Logging diagnostics...
2022-04-23 14:22:16 | [train_policy] epoch #516 | Optimizing policy...
2022-04-23 14:22:16 | [train_policy] epoch #516 | Computing loss before
2022-04-23 14:22:16 | [train_policy] epoch #516 | Computing KL before
2022-04-23 14:22:16 | [train_policy] epoch #516 | Optimizing
2022-04-23 14:22:16 | [train_policy] epoch #516 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:16 | [train_policy] epoch #516 | computing loss before
2022-04-23 14:22:16 | [train_policy] epoch #516 | computing gradient
2022-04-23 14:22:16 | [train_policy] epoch #516 | gradient computed
2022-04-23 14:22:16 | [train_policy] epoch #516 | computing descent direction
2022-04-23 14:22:16 | [train_policy] epoch #516 | descent direction computed
2022-04-23 14:22:16 | [train_policy] epoch #516 | backtrack iters: 1
2022-04-23 14:22:16 | [train_policy] epoch #516 | optimization finished
2022-04-23 14:22:16 | [train_policy] epoch #516 | Computing KL after
2022-04-23 14:22:16 | [train_policy] epoch #516 | Computing loss after
2022-04-23 14:22:16 | [train_policy] epoch #516 | Fitting baseline...
2022-04-23 14:22:16 | [train_policy] epoch #516 | Saving snapshot...
2022-04-23 14:22:16 | [train_policy] epoch #516 | Saved
2022-04-23 14:22:16 | [train_policy] epoch #516 | Time 184.57 s
2022-04-23 14:22:16 | [train_policy] epoch #516 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.125769
Evaluation/AverageDiscountedReturn          -64.7812
Evaluation/AverageReturn                    -64.7812
Evaluation/CompletionRate                     0
Evaluation/Iteration                        516
Evaluation/MaxReturn                        -29.8233
Evaluation/MinReturn                      -2067.83
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.12
Extras/EpisodeRewardMean                    -63.4114
LinearFeatureBaseline/ExplainedVariance       0.0831322
PolicyExecTime                                0.103713
ProcessExecTime                               0.012161
TotalEnvSteps                            523204
policy/Entropy                               -0.551554
policy/KL                                     0.00650236
policy/KLBefore                               0
policy/LossAfter                             -0.0177495
policy/LossBefore                            -2.40303e-08
policy/Perplexity                             0.576054
policy/dLoss                                  0.0177494
---------------------------------------  ----------------
2022-04-23 14:22:16 | [train_policy] epoch #517 | Obtaining samples for iteration 517...
2022-04-23 14:22:17 | [train_policy] epoch #517 | Logging diagnostics...
2022-04-23 14:22:17 | [train_policy] epoch #517 | Optimizing policy...
2022-04-23 14:22:17 | [train_policy] epoch #517 | Computing loss before
2022-04-23 14:22:17 | [train_policy] epoch #517 | Computing KL before
2022-04-23 14:22:17 | [train_policy] epoch #517 | Optimizing
2022-04-23 14:22:17 | [train_policy] epoch #517 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:17 | [train_policy] epoch #517 | computing loss before
2022-04-23 14:22:17 | [train_policy] epoch #517 | computing gradient
2022-04-23 14:22:17 | [train_policy] epoch #517 | gradient computed
2022-04-23 14:22:17 | [train_policy] epoch #517 | computing descent direction
2022-04-23 14:22:17 | [train_policy] epoch #517 | descent direction computed
2022-04-23 14:22:17 | [train_policy] epoch #517 | backtrack iters: 0
2022-04-23 14:22:17 | [train_policy] epoch #517 | optimization finished
2022-04-23 14:22:17 | [train_policy] epoch #517 | Computing KL after
2022-04-23 14:22:17 | [train_policy] epoch #517 | Computing loss after
2022-04-23 14:22:17 | [train_policy] epoch #517 | Fitting baseline...
2022-04-23 14:22:17 | [train_policy] epoch #517 | Saving snapshot...
2022-04-23 14:22:17 | [train_policy] epoch #517 | Saved
2022-04-23 14:22:17 | [train_policy] epoch #517 | Time 184.93 s
2022-04-23 14:22:17 | [train_policy] epoch #517 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119213
Evaluation/AverageDiscountedReturn          -65.6134
Evaluation/AverageReturn                    -65.6134
Evaluation/CompletionRate                     0
Evaluation/Iteration                        517
Evaluation/MaxReturn                        -31.6659
Evaluation/MinReturn                      -2074.26
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.833
Extras/EpisodeRewardMean                    -64.0406
LinearFeatureBaseline/ExplainedVariance       0.0177256
PolicyExecTime                                0.0988667
ProcessExecTime                               0.0115292
TotalEnvSteps                            524216
policy/Entropy                               -0.535659
policy/KL                                     0.00938391
policy/KLBefore                               0
policy/LossAfter                             -0.0168301
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.585283
policy/dLoss                                  0.0168301
---------------------------------------  ----------------
2022-04-23 14:22:17 | [train_policy] epoch #518 | Obtaining samples for iteration 518...
2022-04-23 14:22:17 | [train_policy] epoch #518 | Logging diagnostics...
2022-04-23 14:22:17 | [train_policy] epoch #518 | Optimizing policy...
2022-04-23 14:22:17 | [train_policy] epoch #518 | Computing loss before
2022-04-23 14:22:17 | [train_policy] epoch #518 | Computing KL before
2022-04-23 14:22:17 | [train_policy] epoch #518 | Optimizing
2022-04-23 14:22:17 | [train_policy] epoch #518 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:17 | [train_policy] epoch #518 | computing loss before
2022-04-23 14:22:17 | [train_policy] epoch #518 | computing gradient
2022-04-23 14:22:17 | [train_policy] epoch #518 | gradient computed
2022-04-23 14:22:17 | [train_policy] epoch #518 | computing descent direction
2022-04-23 14:22:17 | [train_policy] epoch #518 | descent direction computed
2022-04-23 14:22:17 | [train_policy] epoch #518 | backtrack iters: 1
2022-04-23 14:22:17 | [train_policy] epoch #518 | optimization finished
2022-04-23 14:22:17 | [train_policy] epoch #518 | Computing KL after
2022-04-23 14:22:17 | [train_policy] epoch #518 | Computing loss after
2022-04-23 14:22:17 | [train_policy] epoch #518 | Fitting baseline...
2022-04-23 14:22:17 | [train_policy] epoch #518 | Saving snapshot...
2022-04-23 14:22:17 | [train_policy] epoch #518 | Saved
2022-04-23 14:22:17 | [train_policy] epoch #518 | Time 185.28 s
2022-04-23 14:22:17 | [train_policy] epoch #518 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11601
Evaluation/AverageDiscountedReturn          -41.7307
Evaluation/AverageReturn                    -41.7307
Evaluation/CompletionRate                     0
Evaluation/Iteration                        518
Evaluation/MaxReturn                        -30.429
Evaluation/MinReturn                        -69.3245
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.59831
Extras/EpisodeRewardMean                    -41.4787
LinearFeatureBaseline/ExplainedVariance     -21.4758
PolicyExecTime                                0.106269
ProcessExecTime                               0.0113044
TotalEnvSteps                            525228
policy/Entropy                               -0.502412
policy/KL                                     0.00689413
policy/KLBefore                               0
policy/LossAfter                             -0.0278309
policy/LossBefore                             1.13084e-08
policy/Perplexity                             0.605069
policy/dLoss                                  0.0278309
---------------------------------------  ----------------
2022-04-23 14:22:17 | [train_policy] epoch #519 | Obtaining samples for iteration 519...
2022-04-23 14:22:17 | [train_policy] epoch #519 | Logging diagnostics...
2022-04-23 14:22:17 | [train_policy] epoch #519 | Optimizing policy...
2022-04-23 14:22:17 | [train_policy] epoch #519 | Computing loss before
2022-04-23 14:22:17 | [train_policy] epoch #519 | Computing KL before
2022-04-23 14:22:17 | [train_policy] epoch #519 | Optimizing
2022-04-23 14:22:17 | [train_policy] epoch #519 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:17 | [train_policy] epoch #519 | computing loss before
2022-04-23 14:22:17 | [train_policy] epoch #519 | computing gradient
2022-04-23 14:22:17 | [train_policy] epoch #519 | gradient computed
2022-04-23 14:22:17 | [train_policy] epoch #519 | computing descent direction
2022-04-23 14:22:17 | [train_policy] epoch #519 | descent direction computed
2022-04-23 14:22:17 | [train_policy] epoch #519 | backtrack iters: 1
2022-04-23 14:22:17 | [train_policy] epoch #519 | optimization finished
2022-04-23 14:22:17 | [train_policy] epoch #519 | Computing KL after
2022-04-23 14:22:17 | [train_policy] epoch #519 | Computing loss after
2022-04-23 14:22:17 | [train_policy] epoch #519 | Fitting baseline...
2022-04-23 14:22:17 | [train_policy] epoch #519 | Saving snapshot...
2022-04-23 14:22:17 | [train_policy] epoch #519 | Saved
2022-04-23 14:22:17 | [train_policy] epoch #519 | Time 185.63 s
2022-04-23 14:22:17 | [train_policy] epoch #519 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116285
Evaluation/AverageDiscountedReturn          -41.8116
Evaluation/AverageReturn                    -41.8116
Evaluation/CompletionRate                     0
Evaluation/Iteration                        519
Evaluation/MaxReturn                        -29.8378
Evaluation/MinReturn                        -64.0917
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.28642
Extras/EpisodeRewardMean                    -41.9659
LinearFeatureBaseline/ExplainedVariance       0.895471
PolicyExecTime                                0.109157
ProcessExecTime                               0.0116329
TotalEnvSteps                            526240
policy/Entropy                               -0.516236
policy/KL                                     0.00667381
policy/KLBefore                               0
policy/LossAfter                             -0.0160829
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.596762
policy/dLoss                                  0.016083
---------------------------------------  ----------------
2022-04-23 14:22:17 | [train_policy] epoch #520 | Obtaining samples for iteration 520...
2022-04-23 14:22:18 | [train_policy] epoch #520 | Logging diagnostics...
2022-04-23 14:22:18 | [train_policy] epoch #520 | Optimizing policy...
2022-04-23 14:22:18 | [train_policy] epoch #520 | Computing loss before
2022-04-23 14:22:18 | [train_policy] epoch #520 | Computing KL before
2022-04-23 14:22:18 | [train_policy] epoch #520 | Optimizing
2022-04-23 14:22:18 | [train_policy] epoch #520 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:18 | [train_policy] epoch #520 | computing loss before
2022-04-23 14:22:18 | [train_policy] epoch #520 | computing gradient
2022-04-23 14:22:18 | [train_policy] epoch #520 | gradient computed
2022-04-23 14:22:18 | [train_policy] epoch #520 | computing descent direction
2022-04-23 14:22:18 | [train_policy] epoch #520 | descent direction computed
2022-04-23 14:22:18 | [train_policy] epoch #520 | backtrack iters: 0
2022-04-23 14:22:18 | [train_policy] epoch #520 | optimization finished
2022-04-23 14:22:18 | [train_policy] epoch #520 | Computing KL after
2022-04-23 14:22:18 | [train_policy] epoch #520 | Computing loss after
2022-04-23 14:22:18 | [train_policy] epoch #520 | Fitting baseline...
2022-04-23 14:22:18 | [train_policy] epoch #520 | Saving snapshot...
2022-04-23 14:22:18 | [train_policy] epoch #520 | Saved
2022-04-23 14:22:18 | [train_policy] epoch #520 | Time 185.98 s
2022-04-23 14:22:18 | [train_policy] epoch #520 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.116012
Evaluation/AverageDiscountedReturn          -87.7276
Evaluation/AverageReturn                    -87.7276
Evaluation/CompletionRate                     0
Evaluation/Iteration                        520
Evaluation/MaxReturn                        -32.9989
Evaluation/MinReturn                      -2072.45
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.581
Extras/EpisodeRewardMean                    -84.1147
LinearFeatureBaseline/ExplainedVariance       0.00953839
PolicyExecTime                                0.105931
ProcessExecTime                               0.0112848
TotalEnvSteps                            527252
policy/Entropy                               -0.507201
policy/KL                                     0.00945958
policy/KLBefore                               0
policy/LossAfter                             -0.0278093
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.602179
policy/dLoss                                  0.0278093
---------------------------------------  ---------------
2022-04-23 14:22:18 | [train_policy] epoch #521 | Obtaining samples for iteration 521...
2022-04-23 14:22:18 | [train_policy] epoch #521 | Logging diagnostics...
2022-04-23 14:22:18 | [train_policy] epoch #521 | Optimizing policy...
2022-04-23 14:22:18 | [train_policy] epoch #521 | Computing loss before
2022-04-23 14:22:18 | [train_policy] epoch #521 | Computing KL before
2022-04-23 14:22:18 | [train_policy] epoch #521 | Optimizing
2022-04-23 14:22:18 | [train_policy] epoch #521 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:18 | [train_policy] epoch #521 | computing loss before
2022-04-23 14:22:18 | [train_policy] epoch #521 | computing gradient
2022-04-23 14:22:18 | [train_policy] epoch #521 | gradient computed
2022-04-23 14:22:18 | [train_policy] epoch #521 | computing descent direction
2022-04-23 14:22:18 | [train_policy] epoch #521 | descent direction computed
2022-04-23 14:22:18 | [train_policy] epoch #521 | backtrack iters: 1
2022-04-23 14:22:18 | [train_policy] epoch #521 | optimization finished
2022-04-23 14:22:18 | [train_policy] epoch #521 | Computing KL after
2022-04-23 14:22:18 | [train_policy] epoch #521 | Computing loss after
2022-04-23 14:22:18 | [train_policy] epoch #521 | Fitting baseline...
2022-04-23 14:22:18 | [train_policy] epoch #521 | Saving snapshot...
2022-04-23 14:22:18 | [train_policy] epoch #521 | Saved
2022-04-23 14:22:18 | [train_policy] epoch #521 | Time 186.34 s
2022-04-23 14:22:18 | [train_policy] epoch #521 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.118752
Evaluation/AverageDiscountedReturn          -42.6623
Evaluation/AverageReturn                    -42.6623
Evaluation/CompletionRate                     0
Evaluation/Iteration                        521
Evaluation/MaxReturn                        -32.3875
Evaluation/MinReturn                        -63.7609
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.23617
Extras/EpisodeRewardMean                    -43.0449
LinearFeatureBaseline/ExplainedVariance     -52.1276
PolicyExecTime                                0.109949
ProcessExecTime                               0.0114884
TotalEnvSteps                            528264
policy/Entropy                               -0.533641
policy/KL                                     0.00671649
policy/KLBefore                               0
policy/LossAfter                             -0.0207942
policy/LossBefore                             2.8271e-08
policy/Perplexity                             0.586466
policy/dLoss                                  0.0207942
---------------------------------------  ---------------
2022-04-23 14:22:18 | [train_policy] epoch #522 | Obtaining samples for iteration 522...
2022-04-23 14:22:18 | [train_policy] epoch #522 | Logging diagnostics...
2022-04-23 14:22:18 | [train_policy] epoch #522 | Optimizing policy...
2022-04-23 14:22:18 | [train_policy] epoch #522 | Computing loss before
2022-04-23 14:22:18 | [train_policy] epoch #522 | Computing KL before
2022-04-23 14:22:18 | [train_policy] epoch #522 | Optimizing
2022-04-23 14:22:18 | [train_policy] epoch #522 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:18 | [train_policy] epoch #522 | computing loss before
2022-04-23 14:22:18 | [train_policy] epoch #522 | computing gradient
2022-04-23 14:22:18 | [train_policy] epoch #522 | gradient computed
2022-04-23 14:22:18 | [train_policy] epoch #522 | computing descent direction
2022-04-23 14:22:18 | [train_policy] epoch #522 | descent direction computed
2022-04-23 14:22:18 | [train_policy] epoch #522 | backtrack iters: 0
2022-04-23 14:22:18 | [train_policy] epoch #522 | optimization finished
2022-04-23 14:22:18 | [train_policy] epoch #522 | Computing KL after
2022-04-23 14:22:18 | [train_policy] epoch #522 | Computing loss after
2022-04-23 14:22:18 | [train_policy] epoch #522 | Fitting baseline...
2022-04-23 14:22:18 | [train_policy] epoch #522 | Saving snapshot...
2022-04-23 14:22:18 | [train_policy] epoch #522 | Saved
2022-04-23 14:22:18 | [train_policy] epoch #522 | Time 186.69 s
2022-04-23 14:22:18 | [train_policy] epoch #522 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117105
Evaluation/AverageDiscountedReturn          -63.7102
Evaluation/AverageReturn                    -63.7102
Evaluation/CompletionRate                     0
Evaluation/Iteration                        522
Evaluation/MaxReturn                        -31.8363
Evaluation/MinReturn                      -2065.17
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.93
Extras/EpisodeRewardMean                    -62.1126
LinearFeatureBaseline/ExplainedVariance       0.0125737
PolicyExecTime                                0.107802
ProcessExecTime                               0.0116324
TotalEnvSteps                            529276
policy/Entropy                               -0.533294
policy/KL                                     0.00909958
policy/KLBefore                               0
policy/LossAfter                             -0.030178
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.586669
policy/dLoss                                  0.030178
---------------------------------------  ----------------
2022-04-23 14:22:18 | [train_policy] epoch #523 | Obtaining samples for iteration 523...
2022-04-23 14:22:19 | [train_policy] epoch #523 | Logging diagnostics...
2022-04-23 14:22:19 | [train_policy] epoch #523 | Optimizing policy...
2022-04-23 14:22:19 | [train_policy] epoch #523 | Computing loss before
2022-04-23 14:22:19 | [train_policy] epoch #523 | Computing KL before
2022-04-23 14:22:19 | [train_policy] epoch #523 | Optimizing
2022-04-23 14:22:19 | [train_policy] epoch #523 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:19 | [train_policy] epoch #523 | computing loss before
2022-04-23 14:22:19 | [train_policy] epoch #523 | computing gradient
2022-04-23 14:22:19 | [train_policy] epoch #523 | gradient computed
2022-04-23 14:22:19 | [train_policy] epoch #523 | computing descent direction
2022-04-23 14:22:19 | [train_policy] epoch #523 | descent direction computed
2022-04-23 14:22:19 | [train_policy] epoch #523 | backtrack iters: 1
2022-04-23 14:22:19 | [train_policy] epoch #523 | optimization finished
2022-04-23 14:22:19 | [train_policy] epoch #523 | Computing KL after
2022-04-23 14:22:19 | [train_policy] epoch #523 | Computing loss after
2022-04-23 14:22:19 | [train_policy] epoch #523 | Fitting baseline...
2022-04-23 14:22:19 | [train_policy] epoch #523 | Saving snapshot...
2022-04-23 14:22:19 | [train_policy] epoch #523 | Saved
2022-04-23 14:22:19 | [train_policy] epoch #523 | Time 187.05 s
2022-04-23 14:22:19 | [train_policy] epoch #523 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11927
Evaluation/AverageDiscountedReturn          -64.5413
Evaluation/AverageReturn                    -64.5413
Evaluation/CompletionRate                     0
Evaluation/Iteration                        523
Evaluation/MaxReturn                        -32.9168
Evaluation/MinReturn                      -2064.49
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.785
Extras/EpisodeRewardMean                    -62.4762
LinearFeatureBaseline/ExplainedVariance       0.149559
PolicyExecTime                                0.103742
ProcessExecTime                               0.0115683
TotalEnvSteps                            530288
policy/Entropy                               -0.510402
policy/KL                                     0.00720743
policy/KLBefore                               0
policy/LossAfter                             -0.0230319
policy/LossBefore                             5.88979e-09
policy/Perplexity                             0.600254
policy/dLoss                                  0.0230319
---------------------------------------  ----------------
2022-04-23 14:22:19 | [train_policy] epoch #524 | Obtaining samples for iteration 524...
2022-04-23 14:22:19 | [train_policy] epoch #524 | Logging diagnostics...
2022-04-23 14:22:19 | [train_policy] epoch #524 | Optimizing policy...
2022-04-23 14:22:19 | [train_policy] epoch #524 | Computing loss before
2022-04-23 14:22:19 | [train_policy] epoch #524 | Computing KL before
2022-04-23 14:22:19 | [train_policy] epoch #524 | Optimizing
2022-04-23 14:22:19 | [train_policy] epoch #524 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:19 | [train_policy] epoch #524 | computing loss before
2022-04-23 14:22:19 | [train_policy] epoch #524 | computing gradient
2022-04-23 14:22:19 | [train_policy] epoch #524 | gradient computed
2022-04-23 14:22:19 | [train_policy] epoch #524 | computing descent direction
2022-04-23 14:22:19 | [train_policy] epoch #524 | descent direction computed
2022-04-23 14:22:19 | [train_policy] epoch #524 | backtrack iters: 0
2022-04-23 14:22:19 | [train_policy] epoch #524 | optimization finished
2022-04-23 14:22:19 | [train_policy] epoch #524 | Computing KL after
2022-04-23 14:22:19 | [train_policy] epoch #524 | Computing loss after
2022-04-23 14:22:19 | [train_policy] epoch #524 | Fitting baseline...
2022-04-23 14:22:19 | [train_policy] epoch #524 | Saving snapshot...
2022-04-23 14:22:19 | [train_policy] epoch #524 | Saved
2022-04-23 14:22:19 | [train_policy] epoch #524 | Time 187.40 s
2022-04-23 14:22:19 | [train_policy] epoch #524 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116577
Evaluation/AverageDiscountedReturn          -86.2312
Evaluation/AverageReturn                    -86.2312
Evaluation/CompletionRate                     0
Evaluation/Iteration                        524
Evaluation/MaxReturn                        -30.9941
Evaluation/MinReturn                      -2078.14
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        296.097
Extras/EpisodeRewardMean                    -82.6134
LinearFeatureBaseline/ExplainedVariance       0.183783
PolicyExecTime                                0.105253
ProcessExecTime                               0.0115387
TotalEnvSteps                            531300
policy/Entropy                               -0.527022
policy/KL                                     0.00970186
policy/KLBefore                               0
policy/LossAfter                             -0.0226388
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.59036
policy/dLoss                                  0.0226388
---------------------------------------  ----------------
2022-04-23 14:22:19 | [train_policy] epoch #525 | Obtaining samples for iteration 525...
2022-04-23 14:22:19 | [train_policy] epoch #525 | Logging diagnostics...
2022-04-23 14:22:19 | [train_policy] epoch #525 | Optimizing policy...
2022-04-23 14:22:19 | [train_policy] epoch #525 | Computing loss before
2022-04-23 14:22:19 | [train_policy] epoch #525 | Computing KL before
2022-04-23 14:22:19 | [train_policy] epoch #525 | Optimizing
2022-04-23 14:22:19 | [train_policy] epoch #525 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:19 | [train_policy] epoch #525 | computing loss before
2022-04-23 14:22:19 | [train_policy] epoch #525 | computing gradient
2022-04-23 14:22:19 | [train_policy] epoch #525 | gradient computed
2022-04-23 14:22:19 | [train_policy] epoch #525 | computing descent direction
2022-04-23 14:22:20 | [train_policy] epoch #525 | descent direction computed
2022-04-23 14:22:20 | [train_policy] epoch #525 | backtrack iters: 1
2022-04-23 14:22:20 | [train_policy] epoch #525 | optimization finished
2022-04-23 14:22:20 | [train_policy] epoch #525 | Computing KL after
2022-04-23 14:22:20 | [train_policy] epoch #525 | Computing loss after
2022-04-23 14:22:20 | [train_policy] epoch #525 | Fitting baseline...
2022-04-23 14:22:20 | [train_policy] epoch #525 | Saving snapshot...
2022-04-23 14:22:20 | [train_policy] epoch #525 | Saved
2022-04-23 14:22:20 | [train_policy] epoch #525 | Time 187.75 s
2022-04-23 14:22:20 | [train_policy] epoch #525 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.117582
Evaluation/AverageDiscountedReturn         -131.588
Evaluation/AverageReturn                   -131.588
Evaluation/CompletionRate                     0
Evaluation/Iteration                        525
Evaluation/MaxReturn                        -29.8178
Evaluation/MinReturn                      -2064.91
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        412.21
Extras/EpisodeRewardMean                   -124.576
LinearFeatureBaseline/ExplainedVariance       0.237622
PolicyExecTime                                0.108291
ProcessExecTime                               0.0114555
TotalEnvSteps                            532312
policy/Entropy                               -0.526975
policy/KL                                     0.00658545
policy/KLBefore                               0
policy/LossAfter                             -0.0256779
policy/LossBefore                             2.886e-09
policy/Perplexity                             0.590388
policy/dLoss                                  0.0256779
---------------------------------------  ---------------
2022-04-23 14:22:20 | [train_policy] epoch #526 | Obtaining samples for iteration 526...
2022-04-23 14:22:20 | [train_policy] epoch #526 | Logging diagnostics...
2022-04-23 14:22:20 | [train_policy] epoch #526 | Optimizing policy...
2022-04-23 14:22:20 | [train_policy] epoch #526 | Computing loss before
2022-04-23 14:22:20 | [train_policy] epoch #526 | Computing KL before
2022-04-23 14:22:20 | [train_policy] epoch #526 | Optimizing
2022-04-23 14:22:20 | [train_policy] epoch #526 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:20 | [train_policy] epoch #526 | computing loss before
2022-04-23 14:22:20 | [train_policy] epoch #526 | computing gradient
2022-04-23 14:22:20 | [train_policy] epoch #526 | gradient computed
2022-04-23 14:22:20 | [train_policy] epoch #526 | computing descent direction
2022-04-23 14:22:20 | [train_policy] epoch #526 | descent direction computed
2022-04-23 14:22:20 | [train_policy] epoch #526 | backtrack iters: 0
2022-04-23 14:22:20 | [train_policy] epoch #526 | optimization finished
2022-04-23 14:22:20 | [train_policy] epoch #526 | Computing KL after
2022-04-23 14:22:20 | [train_policy] epoch #526 | Computing loss after
2022-04-23 14:22:20 | [train_policy] epoch #526 | Fitting baseline...
2022-04-23 14:22:20 | [train_policy] epoch #526 | Saving snapshot...
2022-04-23 14:22:20 | [train_policy] epoch #526 | Saved
2022-04-23 14:22:20 | [train_policy] epoch #526 | Time 188.10 s
2022-04-23 14:22:20 | [train_policy] epoch #526 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118654
Evaluation/AverageDiscountedReturn          -43.1904
Evaluation/AverageReturn                    -43.1904
Evaluation/CompletionRate                     0
Evaluation/Iteration                        526
Evaluation/MaxReturn                        -29.4823
Evaluation/MinReturn                        -77.7804
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.66517
Extras/EpisodeRewardMean                    -42.7199
LinearFeatureBaseline/ExplainedVariance    -175.339
PolicyExecTime                                0.104541
ProcessExecTime                               0.0112514
TotalEnvSteps                            533324
policy/Entropy                               -0.523811
policy/KL                                     0.00993244
policy/KLBefore                               0
policy/LossAfter                             -0.043683
policy/LossBefore                             3.20404e-08
policy/Perplexity                             0.592259
policy/dLoss                                  0.043683
---------------------------------------  ----------------
2022-04-23 14:22:20 | [train_policy] epoch #527 | Obtaining samples for iteration 527...
2022-04-23 14:22:20 | [train_policy] epoch #527 | Logging diagnostics...
2022-04-23 14:22:20 | [train_policy] epoch #527 | Optimizing policy...
2022-04-23 14:22:20 | [train_policy] epoch #527 | Computing loss before
2022-04-23 14:22:20 | [train_policy] epoch #527 | Computing KL before
2022-04-23 14:22:20 | [train_policy] epoch #527 | Optimizing
2022-04-23 14:22:20 | [train_policy] epoch #527 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:20 | [train_policy] epoch #527 | computing loss before
2022-04-23 14:22:20 | [train_policy] epoch #527 | computing gradient
2022-04-23 14:22:20 | [train_policy] epoch #527 | gradient computed
2022-04-23 14:22:20 | [train_policy] epoch #527 | computing descent direction
2022-04-23 14:22:20 | [train_policy] epoch #527 | descent direction computed
2022-04-23 14:22:20 | [train_policy] epoch #527 | backtrack iters: 1
2022-04-23 14:22:20 | [train_policy] epoch #527 | optimization finished
2022-04-23 14:22:20 | [train_policy] epoch #527 | Computing KL after
2022-04-23 14:22:20 | [train_policy] epoch #527 | Computing loss after
2022-04-23 14:22:20 | [train_policy] epoch #527 | Fitting baseline...
2022-04-23 14:22:20 | [train_policy] epoch #527 | Saving snapshot...
2022-04-23 14:22:20 | [train_policy] epoch #527 | Saved
2022-04-23 14:22:20 | [train_policy] epoch #527 | Time 188.44 s
2022-04-23 14:22:20 | [train_policy] epoch #527 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118673
Evaluation/AverageDiscountedReturn          -42.8577
Evaluation/AverageReturn                    -42.8577
Evaluation/CompletionRate                     0
Evaluation/Iteration                        527
Evaluation/MaxReturn                        -27.8962
Evaluation/MinReturn                        -79.2552
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.2356
Extras/EpisodeRewardMean                    -42.7316
LinearFeatureBaseline/ExplainedVariance       0.833081
PolicyExecTime                                0.0984232
ProcessExecTime                               0.011214
TotalEnvSteps                            534336
policy/Entropy                               -0.521595
policy/KL                                     0.00661801
policy/KLBefore                               0
policy/LossAfter                             -0.0171606
policy/LossBefore                            -7.06774e-10
policy/Perplexity                             0.593573
policy/dLoss                                  0.0171606
---------------------------------------  ----------------
2022-04-23 14:22:20 | [train_policy] epoch #528 | Obtaining samples for iteration 528...
2022-04-23 14:22:21 | [train_policy] epoch #528 | Logging diagnostics...
2022-04-23 14:22:21 | [train_policy] epoch #528 | Optimizing policy...
2022-04-23 14:22:21 | [train_policy] epoch #528 | Computing loss before
2022-04-23 14:22:21 | [train_policy] epoch #528 | Computing KL before
2022-04-23 14:22:21 | [train_policy] epoch #528 | Optimizing
2022-04-23 14:22:21 | [train_policy] epoch #528 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:21 | [train_policy] epoch #528 | computing loss before
2022-04-23 14:22:21 | [train_policy] epoch #528 | computing gradient
2022-04-23 14:22:21 | [train_policy] epoch #528 | gradient computed
2022-04-23 14:22:21 | [train_policy] epoch #528 | computing descent direction
2022-04-23 14:22:21 | [train_policy] epoch #528 | descent direction computed
2022-04-23 14:22:21 | [train_policy] epoch #528 | backtrack iters: 0
2022-04-23 14:22:21 | [train_policy] epoch #528 | optimization finished
2022-04-23 14:22:21 | [train_policy] epoch #528 | Computing KL after
2022-04-23 14:22:21 | [train_policy] epoch #528 | Computing loss after
2022-04-23 14:22:21 | [train_policy] epoch #528 | Fitting baseline...
2022-04-23 14:22:21 | [train_policy] epoch #528 | Saving snapshot...
2022-04-23 14:22:21 | [train_policy] epoch #528 | Saved
2022-04-23 14:22:21 | [train_policy] epoch #528 | Time 188.80 s
2022-04-23 14:22:21 | [train_policy] epoch #528 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.12569
Evaluation/AverageDiscountedReturn          -42.6011
Evaluation/AverageReturn                    -42.6011
Evaluation/CompletionRate                     0
Evaluation/Iteration                        528
Evaluation/MaxReturn                        -29.6821
Evaluation/MinReturn                        -86.7646
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.38777
Extras/EpisodeRewardMean                    -42.5117
LinearFeatureBaseline/ExplainedVariance       0.823417
PolicyExecTime                                0.11259
ProcessExecTime                               0.0121562
TotalEnvSteps                            535348
policy/Entropy                               -0.492113
policy/KL                                     0.0096199
policy/KLBefore                               0
policy/LossAfter                             -0.01996
policy/LossBefore                             6.00758e-09
policy/Perplexity                             0.611333
policy/dLoss                                  0.01996
---------------------------------------  ----------------
2022-04-23 14:22:21 | [train_policy] epoch #529 | Obtaining samples for iteration 529...
2022-04-23 14:22:21 | [train_policy] epoch #529 | Logging diagnostics...
2022-04-23 14:22:21 | [train_policy] epoch #529 | Optimizing policy...
2022-04-23 14:22:21 | [train_policy] epoch #529 | Computing loss before
2022-04-23 14:22:21 | [train_policy] epoch #529 | Computing KL before
2022-04-23 14:22:21 | [train_policy] epoch #529 | Optimizing
2022-04-23 14:22:21 | [train_policy] epoch #529 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:21 | [train_policy] epoch #529 | computing loss before
2022-04-23 14:22:21 | [train_policy] epoch #529 | computing gradient
2022-04-23 14:22:21 | [train_policy] epoch #529 | gradient computed
2022-04-23 14:22:21 | [train_policy] epoch #529 | computing descent direction
2022-04-23 14:22:21 | [train_policy] epoch #529 | descent direction computed
2022-04-23 14:22:21 | [train_policy] epoch #529 | backtrack iters: 0
2022-04-23 14:22:21 | [train_policy] epoch #529 | optimization finished
2022-04-23 14:22:21 | [train_policy] epoch #529 | Computing KL after
2022-04-23 14:22:21 | [train_policy] epoch #529 | Computing loss after
2022-04-23 14:22:21 | [train_policy] epoch #529 | Fitting baseline...
2022-04-23 14:22:21 | [train_policy] epoch #529 | Saving snapshot...
2022-04-23 14:22:21 | [train_policy] epoch #529 | Saved
2022-04-23 14:22:21 | [train_policy] epoch #529 | Time 189.16 s
2022-04-23 14:22:21 | [train_policy] epoch #529 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119359
Evaluation/AverageDiscountedReturn          -85.8586
Evaluation/AverageReturn                    -85.8586
Evaluation/CompletionRate                     0
Evaluation/Iteration                        529
Evaluation/MaxReturn                        -28.3142
Evaluation/MinReturn                      -2065.29
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.154
Extras/EpisodeRewardMean                    -82.3505
LinearFeatureBaseline/ExplainedVariance       0.0139835
PolicyExecTime                                0.104851
ProcessExecTime                               0.0113132
TotalEnvSteps                            536360
policy/Entropy                               -0.468448
policy/KL                                     0.00955295
policy/KLBefore                               0
policy/LossAfter                             -0.0259463
policy/LossBefore                            -1.41355e-09
policy/Perplexity                             0.625973
policy/dLoss                                  0.0259463
---------------------------------------  ----------------
2022-04-23 14:22:21 | [train_policy] epoch #530 | Obtaining samples for iteration 530...
2022-04-23 14:22:21 | [train_policy] epoch #530 | Logging diagnostics...
2022-04-23 14:22:21 | [train_policy] epoch #530 | Optimizing policy...
2022-04-23 14:22:21 | [train_policy] epoch #530 | Computing loss before
2022-04-23 14:22:21 | [train_policy] epoch #530 | Computing KL before
2022-04-23 14:22:21 | [train_policy] epoch #530 | Optimizing
2022-04-23 14:22:21 | [train_policy] epoch #530 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:21 | [train_policy] epoch #530 | computing loss before
2022-04-23 14:22:21 | [train_policy] epoch #530 | computing gradient
2022-04-23 14:22:21 | [train_policy] epoch #530 | gradient computed
2022-04-23 14:22:21 | [train_policy] epoch #530 | computing descent direction
2022-04-23 14:22:21 | [train_policy] epoch #530 | descent direction computed
2022-04-23 14:22:21 | [train_policy] epoch #530 | backtrack iters: 0
2022-04-23 14:22:21 | [train_policy] epoch #530 | optimization finished
2022-04-23 14:22:21 | [train_policy] epoch #530 | Computing KL after
2022-04-23 14:22:21 | [train_policy] epoch #530 | Computing loss after
2022-04-23 14:22:21 | [train_policy] epoch #530 | Fitting baseline...
2022-04-23 14:22:21 | [train_policy] epoch #530 | Saving snapshot...
2022-04-23 14:22:21 | [train_policy] epoch #530 | Saved
2022-04-23 14:22:21 | [train_policy] epoch #530 | Time 189.52 s
2022-04-23 14:22:21 | [train_policy] epoch #530 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.130092
Evaluation/AverageDiscountedReturn          -86.5935
Evaluation/AverageReturn                    -86.5935
Evaluation/CompletionRate                     0
Evaluation/Iteration                        530
Evaluation/MaxReturn                        -28.0582
Evaluation/MinReturn                      -2066.45
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.196
Extras/EpisodeRewardMean                    -83.3558
LinearFeatureBaseline/ExplainedVariance       0.165777
PolicyExecTime                                0.110205
ProcessExecTime                               0.0125539
TotalEnvSteps                            537372
policy/Entropy                               -0.444307
policy/KL                                     0.00875493
policy/KLBefore                               0
policy/LossAfter                             -0.0415429
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.641269
policy/dLoss                                  0.0415429
---------------------------------------  ----------------
2022-04-23 14:22:21 | [train_policy] epoch #531 | Obtaining samples for iteration 531...
2022-04-23 14:22:22 | [train_policy] epoch #531 | Logging diagnostics...
2022-04-23 14:22:22 | [train_policy] epoch #531 | Optimizing policy...
2022-04-23 14:22:22 | [train_policy] epoch #531 | Computing loss before
2022-04-23 14:22:22 | [train_policy] epoch #531 | Computing KL before
2022-04-23 14:22:22 | [train_policy] epoch #531 | Optimizing
2022-04-23 14:22:22 | [train_policy] epoch #531 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:22 | [train_policy] epoch #531 | computing loss before
2022-04-23 14:22:22 | [train_policy] epoch #531 | computing gradient
2022-04-23 14:22:22 | [train_policy] epoch #531 | gradient computed
2022-04-23 14:22:22 | [train_policy] epoch #531 | computing descent direction
2022-04-23 14:22:22 | [train_policy] epoch #531 | descent direction computed
2022-04-23 14:22:22 | [train_policy] epoch #531 | backtrack iters: 1
2022-04-23 14:22:22 | [train_policy] epoch #531 | optimization finished
2022-04-23 14:22:22 | [train_policy] epoch #531 | Computing KL after
2022-04-23 14:22:22 | [train_policy] epoch #531 | Computing loss after
2022-04-23 14:22:22 | [train_policy] epoch #531 | Fitting baseline...
2022-04-23 14:22:22 | [train_policy] epoch #531 | Saving snapshot...
2022-04-23 14:22:22 | [train_policy] epoch #531 | Saved
2022-04-23 14:22:22 | [train_policy] epoch #531 | Time 189.86 s
2022-04-23 14:22:22 | [train_policy] epoch #531 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.116164
Evaluation/AverageDiscountedReturn         -107.598
Evaluation/AverageReturn                   -107.598
Evaluation/CompletionRate                     0
Evaluation/Iteration                        531
Evaluation/MaxReturn                        -30.2034
Evaluation/MinReturn                      -2067.21
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.537
Extras/EpisodeRewardMean                   -102.518
LinearFeatureBaseline/ExplainedVariance       0.249471
PolicyExecTime                                0.1054
ProcessExecTime                               0.011426
TotalEnvSteps                            538384
policy/Entropy                               -0.488126
policy/KL                                     0.00702114
policy/KLBefore                               0
policy/LossAfter                             -0.0242196
policy/LossBefore                             2.3088e-08
policy/Perplexity                             0.613776
policy/dLoss                                  0.0242196
---------------------------------------  ---------------
2022-04-23 14:22:22 | [train_policy] epoch #532 | Obtaining samples for iteration 532...
2022-04-23 14:22:22 | [train_policy] epoch #532 | Logging diagnostics...
2022-04-23 14:22:22 | [train_policy] epoch #532 | Optimizing policy...
2022-04-23 14:22:22 | [train_policy] epoch #532 | Computing loss before
2022-04-23 14:22:22 | [train_policy] epoch #532 | Computing KL before
2022-04-23 14:22:22 | [train_policy] epoch #532 | Optimizing
2022-04-23 14:22:22 | [train_policy] epoch #532 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:22 | [train_policy] epoch #532 | computing loss before
2022-04-23 14:22:22 | [train_policy] epoch #532 | computing gradient
2022-04-23 14:22:22 | [train_policy] epoch #532 | gradient computed
2022-04-23 14:22:22 | [train_policy] epoch #532 | computing descent direction
2022-04-23 14:22:22 | [train_policy] epoch #532 | descent direction computed
2022-04-23 14:22:22 | [train_policy] epoch #532 | backtrack iters: 1
2022-04-23 14:22:22 | [train_policy] epoch #532 | optimization finished
2022-04-23 14:22:22 | [train_policy] epoch #532 | Computing KL after
2022-04-23 14:22:22 | [train_policy] epoch #532 | Computing loss after
2022-04-23 14:22:22 | [train_policy] epoch #532 | Fitting baseline...
2022-04-23 14:22:22 | [train_policy] epoch #532 | Saving snapshot...
2022-04-23 14:22:22 | [train_policy] epoch #532 | Saved
2022-04-23 14:22:22 | [train_policy] epoch #532 | Time 190.21 s
2022-04-23 14:22:22 | [train_policy] epoch #532 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118814
Evaluation/AverageDiscountedReturn          -64.9533
Evaluation/AverageReturn                    -64.9533
Evaluation/CompletionRate                     0
Evaluation/Iteration                        532
Evaluation/MaxReturn                        -30.133
Evaluation/MinReturn                      -2064.04
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.755
Extras/EpisodeRewardMean                    -63.3155
LinearFeatureBaseline/ExplainedVariance      -0.17967
PolicyExecTime                                0.102053
ProcessExecTime                               0.0116658
TotalEnvSteps                            539396
policy/Entropy                               -0.490709
policy/KL                                     0.00645989
policy/KLBefore                               0
policy/LossAfter                             -0.0194164
policy/LossBefore                            -3.41608e-09
policy/Perplexity                             0.612192
policy/dLoss                                  0.0194164
---------------------------------------  ----------------
2022-04-23 14:22:22 | [train_policy] epoch #533 | Obtaining samples for iteration 533...
2022-04-23 14:22:22 | [train_policy] epoch #533 | Logging diagnostics...
2022-04-23 14:22:22 | [train_policy] epoch #533 | Optimizing policy...
2022-04-23 14:22:22 | [train_policy] epoch #533 | Computing loss before
2022-04-23 14:22:22 | [train_policy] epoch #533 | Computing KL before
2022-04-23 14:22:22 | [train_policy] epoch #533 | Optimizing
2022-04-23 14:22:22 | [train_policy] epoch #533 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:22 | [train_policy] epoch #533 | computing loss before
2022-04-23 14:22:22 | [train_policy] epoch #533 | computing gradient
2022-04-23 14:22:22 | [train_policy] epoch #533 | gradient computed
2022-04-23 14:22:22 | [train_policy] epoch #533 | computing descent direction
2022-04-23 14:22:22 | [train_policy] epoch #533 | descent direction computed
2022-04-23 14:22:22 | [train_policy] epoch #533 | backtrack iters: 0
2022-04-23 14:22:22 | [train_policy] epoch #533 | optimization finished
2022-04-23 14:22:22 | [train_policy] epoch #533 | Computing KL after
2022-04-23 14:22:22 | [train_policy] epoch #533 | Computing loss after
2022-04-23 14:22:22 | [train_policy] epoch #533 | Fitting baseline...
2022-04-23 14:22:22 | [train_policy] epoch #533 | Saving snapshot...
2022-04-23 14:22:22 | [train_policy] epoch #533 | Saved
2022-04-23 14:22:22 | [train_policy] epoch #533 | Time 190.56 s
2022-04-23 14:22:22 | [train_policy] epoch #533 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119275
Evaluation/AverageDiscountedReturn          -42.721
Evaluation/AverageReturn                    -42.721
Evaluation/CompletionRate                     0
Evaluation/Iteration                        533
Evaluation/MaxReturn                        -27.9509
Evaluation/MinReturn                        -72.9083
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.08254
Extras/EpisodeRewardMean                    -63.0367
LinearFeatureBaseline/ExplainedVariance     -33.8711
PolicyExecTime                                0.102585
ProcessExecTime                               0.0132232
TotalEnvSteps                            540408
policy/Entropy                               -0.456658
policy/KL                                     0.00901519
policy/KLBefore                               0
policy/LossAfter                             -0.0220579
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.633397
policy/dLoss                                  0.0220579
---------------------------------------  ----------------
2022-04-23 14:22:22 | [train_policy] epoch #534 | Obtaining samples for iteration 534...
2022-04-23 14:22:23 | [train_policy] epoch #534 | Logging diagnostics...
2022-04-23 14:22:23 | [train_policy] epoch #534 | Optimizing policy...
2022-04-23 14:22:23 | [train_policy] epoch #534 | Computing loss before
2022-04-23 14:22:23 | [train_policy] epoch #534 | Computing KL before
2022-04-23 14:22:23 | [train_policy] epoch #534 | Optimizing
2022-04-23 14:22:23 | [train_policy] epoch #534 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:23 | [train_policy] epoch #534 | computing loss before
2022-04-23 14:22:23 | [train_policy] epoch #534 | computing gradient
2022-04-23 14:22:23 | [train_policy] epoch #534 | gradient computed
2022-04-23 14:22:23 | [train_policy] epoch #534 | computing descent direction
2022-04-23 14:22:23 | [train_policy] epoch #534 | descent direction computed
2022-04-23 14:22:23 | [train_policy] epoch #534 | backtrack iters: 0
2022-04-23 14:22:23 | [train_policy] epoch #534 | optimization finished
2022-04-23 14:22:23 | [train_policy] epoch #534 | Computing KL after
2022-04-23 14:22:23 | [train_policy] epoch #534 | Computing loss after
2022-04-23 14:22:23 | [train_policy] epoch #534 | Fitting baseline...
2022-04-23 14:22:23 | [train_policy] epoch #534 | Saving snapshot...
2022-04-23 14:22:23 | [train_policy] epoch #534 | Saved
2022-04-23 14:22:23 | [train_policy] epoch #534 | Time 190.88 s
2022-04-23 14:22:23 | [train_policy] epoch #534 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.117704
Evaluation/AverageDiscountedReturn          -62.8126
Evaluation/AverageReturn                    -62.8126
Evaluation/CompletionRate                     0
Evaluation/Iteration                        534
Evaluation/MaxReturn                        -28.3675
Evaluation/MinReturn                      -2067.44
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.259
Extras/EpisodeRewardMean                    -61.2902
LinearFeatureBaseline/ExplainedVariance       0.0142252
PolicyExecTime                                0.094255
ProcessExecTime                               0.0110309
TotalEnvSteps                            541420
policy/Entropy                               -0.452554
policy/KL                                     0.00870211
policy/KLBefore                               0
policy/LossAfter                             -0.0281876
policy/LossBefore                            -2.45015e-08
policy/Perplexity                             0.636002
policy/dLoss                                  0.0281876
---------------------------------------  ----------------
2022-04-23 14:22:23 | [train_policy] epoch #535 | Obtaining samples for iteration 535...
2022-04-23 14:22:23 | [train_policy] epoch #535 | Logging diagnostics...
2022-04-23 14:22:23 | [train_policy] epoch #535 | Optimizing policy...
2022-04-23 14:22:23 | [train_policy] epoch #535 | Computing loss before
2022-04-23 14:22:23 | [train_policy] epoch #535 | Computing KL before
2022-04-23 14:22:23 | [train_policy] epoch #535 | Optimizing
2022-04-23 14:22:23 | [train_policy] epoch #535 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:23 | [train_policy] epoch #535 | computing loss before
2022-04-23 14:22:23 | [train_policy] epoch #535 | computing gradient
2022-04-23 14:22:23 | [train_policy] epoch #535 | gradient computed
2022-04-23 14:22:23 | [train_policy] epoch #535 | computing descent direction
2022-04-23 14:22:23 | [train_policy] epoch #535 | descent direction computed
2022-04-23 14:22:23 | [train_policy] epoch #535 | backtrack iters: 1
2022-04-23 14:22:23 | [train_policy] epoch #535 | optimization finished
2022-04-23 14:22:23 | [train_policy] epoch #535 | Computing KL after
2022-04-23 14:22:23 | [train_policy] epoch #535 | Computing loss after
2022-04-23 14:22:23 | [train_policy] epoch #535 | Fitting baseline...
2022-04-23 14:22:23 | [train_policy] epoch #535 | Saving snapshot...
2022-04-23 14:22:23 | [train_policy] epoch #535 | Saved
2022-04-23 14:22:23 | [train_policy] epoch #535 | Time 191.22 s
2022-04-23 14:22:23 | [train_policy] epoch #535 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118952
Evaluation/AverageDiscountedReturn          -43.3199
Evaluation/AverageReturn                    -43.3199
Evaluation/CompletionRate                     0
Evaluation/Iteration                        535
Evaluation/MaxReturn                        -28.4113
Evaluation/MinReturn                        -71.3069
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25162
Extras/EpisodeRewardMean                    -43.1154
LinearFeatureBaseline/ExplainedVariance     -61.3686
PolicyExecTime                                0.1007
ProcessExecTime                               0.0116403
TotalEnvSteps                            542432
policy/Entropy                               -0.461853
policy/KL                                     0.00654483
policy/KLBefore                               0
policy/LossAfter                             -0.015254
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.630115
policy/dLoss                                  0.015254
---------------------------------------  ----------------
2022-04-23 14:22:23 | [train_policy] epoch #536 | Obtaining samples for iteration 536...
2022-04-23 14:22:23 | [train_policy] epoch #536 | Logging diagnostics...
2022-04-23 14:22:23 | [train_policy] epoch #536 | Optimizing policy...
2022-04-23 14:22:23 | [train_policy] epoch #536 | Computing loss before
2022-04-23 14:22:23 | [train_policy] epoch #536 | Computing KL before
2022-04-23 14:22:23 | [train_policy] epoch #536 | Optimizing
2022-04-23 14:22:23 | [train_policy] epoch #536 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:23 | [train_policy] epoch #536 | computing loss before
2022-04-23 14:22:23 | [train_policy] epoch #536 | computing gradient
2022-04-23 14:22:23 | [train_policy] epoch #536 | gradient computed
2022-04-23 14:22:23 | [train_policy] epoch #536 | computing descent direction
2022-04-23 14:22:23 | [train_policy] epoch #536 | descent direction computed
2022-04-23 14:22:23 | [train_policy] epoch #536 | backtrack iters: 1
2022-04-23 14:22:23 | [train_policy] epoch #536 | optimization finished
2022-04-23 14:22:23 | [train_policy] epoch #536 | Computing KL after
2022-04-23 14:22:23 | [train_policy] epoch #536 | Computing loss after
2022-04-23 14:22:23 | [train_policy] epoch #536 | Fitting baseline...
2022-04-23 14:22:23 | [train_policy] epoch #536 | Saving snapshot...
2022-04-23 14:22:23 | [train_policy] epoch #536 | Saved
2022-04-23 14:22:23 | [train_policy] epoch #536 | Time 191.56 s
2022-04-23 14:22:23 | [train_policy] epoch #536 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.115873
Evaluation/AverageDiscountedReturn          -43.34
Evaluation/AverageReturn                    -43.34
Evaluation/CompletionRate                     0
Evaluation/Iteration                        536
Evaluation/MaxReturn                        -29.1584
Evaluation/MinReturn                        -65.9496
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.19539
Extras/EpisodeRewardMean                    -43.5199
LinearFeatureBaseline/ExplainedVariance       0.843491
PolicyExecTime                                0.110021
ProcessExecTime                               0.0112083
TotalEnvSteps                            543444
policy/Entropy                               -0.488979
policy/KL                                     0.00686612
policy/KLBefore                               0
policy/LossAfter                             -0.0208631
policy/LossBefore                             1.7905e-08
policy/Perplexity                             0.613252
policy/dLoss                                  0.0208631
---------------------------------------  ---------------
2022-04-23 14:22:23 | [train_policy] epoch #537 | Obtaining samples for iteration 537...
2022-04-23 14:22:24 | [train_policy] epoch #537 | Logging diagnostics...
2022-04-23 14:22:24 | [train_policy] epoch #537 | Optimizing policy...
2022-04-23 14:22:24 | [train_policy] epoch #537 | Computing loss before
2022-04-23 14:22:24 | [train_policy] epoch #537 | Computing KL before
2022-04-23 14:22:24 | [train_policy] epoch #537 | Optimizing
2022-04-23 14:22:24 | [train_policy] epoch #537 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:24 | [train_policy] epoch #537 | computing loss before
2022-04-23 14:22:24 | [train_policy] epoch #537 | computing gradient
2022-04-23 14:22:24 | [train_policy] epoch #537 | gradient computed
2022-04-23 14:22:24 | [train_policy] epoch #537 | computing descent direction
2022-04-23 14:22:24 | [train_policy] epoch #537 | descent direction computed
2022-04-23 14:22:24 | [train_policy] epoch #537 | backtrack iters: 1
2022-04-23 14:22:24 | [train_policy] epoch #537 | optimization finished
2022-04-23 14:22:24 | [train_policy] epoch #537 | Computing KL after
2022-04-23 14:22:24 | [train_policy] epoch #537 | Computing loss after
2022-04-23 14:22:24 | [train_policy] epoch #537 | Fitting baseline...
2022-04-23 14:22:24 | [train_policy] epoch #537 | Saving snapshot...
2022-04-23 14:22:24 | [train_policy] epoch #537 | Saved
2022-04-23 14:22:24 | [train_policy] epoch #537 | Time 191.90 s
2022-04-23 14:22:24 | [train_policy] epoch #537 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115641
Evaluation/AverageDiscountedReturn          -64.2341
Evaluation/AverageReturn                    -64.2341
Evaluation/CompletionRate                     0
Evaluation/Iteration                        537
Evaluation/MaxReturn                        -33.3003
Evaluation/MinReturn                      -2065.71
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.971
Extras/EpisodeRewardMean                    -62.5896
LinearFeatureBaseline/ExplainedVariance       0.0130009
PolicyExecTime                                0.106098
ProcessExecTime                               0.0112357
TotalEnvSteps                            544456
policy/Entropy                               -0.458942
policy/KL                                     0.0064197
policy/KLBefore                               0
policy/LossAfter                             -0.0231865
policy/LossBefore                            -8.01011e-09
policy/Perplexity                             0.631952
policy/dLoss                                  0.0231865
---------------------------------------  ----------------
2022-04-23 14:22:24 | [train_policy] epoch #538 | Obtaining samples for iteration 538...
2022-04-23 14:22:24 | [train_policy] epoch #538 | Logging diagnostics...
2022-04-23 14:22:24 | [train_policy] epoch #538 | Optimizing policy...
2022-04-23 14:22:24 | [train_policy] epoch #538 | Computing loss before
2022-04-23 14:22:24 | [train_policy] epoch #538 | Computing KL before
2022-04-23 14:22:24 | [train_policy] epoch #538 | Optimizing
2022-04-23 14:22:24 | [train_policy] epoch #538 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:24 | [train_policy] epoch #538 | computing loss before
2022-04-23 14:22:24 | [train_policy] epoch #538 | computing gradient
2022-04-23 14:22:24 | [train_policy] epoch #538 | gradient computed
2022-04-23 14:22:24 | [train_policy] epoch #538 | computing descent direction
2022-04-23 14:22:24 | [train_policy] epoch #538 | descent direction computed
2022-04-23 14:22:24 | [train_policy] epoch #538 | backtrack iters: 0
2022-04-23 14:22:24 | [train_policy] epoch #538 | optimization finished
2022-04-23 14:22:24 | [train_policy] epoch #538 | Computing KL after
2022-04-23 14:22:24 | [train_policy] epoch #538 | Computing loss after
2022-04-23 14:22:24 | [train_policy] epoch #538 | Fitting baseline...
2022-04-23 14:22:24 | [train_policy] epoch #538 | Saving snapshot...
2022-04-23 14:22:24 | [train_policy] epoch #538 | Saved
2022-04-23 14:22:24 | [train_policy] epoch #538 | Time 192.25 s
2022-04-23 14:22:24 | [train_policy] epoch #538 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117983
Evaluation/AverageDiscountedReturn          -86.3858
Evaluation/AverageReturn                    -86.3858
Evaluation/CompletionRate                     0
Evaluation/Iteration                        538
Evaluation/MaxReturn                        -32.0342
Evaluation/MinReturn                      -2069.69
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.604
Extras/EpisodeRewardMean                    -82.6502
LinearFeatureBaseline/ExplainedVariance       0.173851
PolicyExecTime                                0.110141
ProcessExecTime                               0.011488
TotalEnvSteps                            545468
policy/Entropy                               -0.410356
policy/KL                                     0.00949368
policy/KLBefore                               0
policy/LossAfter                             -0.0220608
policy/LossBefore                            -2.51494e-08
policy/Perplexity                             0.663414
policy/dLoss                                  0.0220608
---------------------------------------  ----------------
2022-04-23 14:22:24 | [train_policy] epoch #539 | Obtaining samples for iteration 539...
2022-04-23 14:22:24 | [train_policy] epoch #539 | Logging diagnostics...
2022-04-23 14:22:24 | [train_policy] epoch #539 | Optimizing policy...
2022-04-23 14:22:24 | [train_policy] epoch #539 | Computing loss before
2022-04-23 14:22:24 | [train_policy] epoch #539 | Computing KL before
2022-04-23 14:22:24 | [train_policy] epoch #539 | Optimizing
2022-04-23 14:22:24 | [train_policy] epoch #539 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:24 | [train_policy] epoch #539 | computing loss before
2022-04-23 14:22:24 | [train_policy] epoch #539 | computing gradient
2022-04-23 14:22:24 | [train_policy] epoch #539 | gradient computed
2022-04-23 14:22:24 | [train_policy] epoch #539 | computing descent direction
2022-04-23 14:22:24 | [train_policy] epoch #539 | descent direction computed
2022-04-23 14:22:24 | [train_policy] epoch #539 | backtrack iters: 0
2022-04-23 14:22:24 | [train_policy] epoch #539 | optimization finished
2022-04-23 14:22:24 | [train_policy] epoch #539 | Computing KL after
2022-04-23 14:22:24 | [train_policy] epoch #539 | Computing loss after
2022-04-23 14:22:24 | [train_policy] epoch #539 | Fitting baseline...
2022-04-23 14:22:24 | [train_policy] epoch #539 | Saving snapshot...
2022-04-23 14:22:24 | [train_policy] epoch #539 | Saved
2022-04-23 14:22:24 | [train_policy] epoch #539 | Time 192.60 s
2022-04-23 14:22:24 | [train_policy] epoch #539 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.118238
Evaluation/AverageDiscountedReturn          -64.6524
Evaluation/AverageReturn                    -64.6524
Evaluation/CompletionRate                     0
Evaluation/Iteration                        539
Evaluation/MaxReturn                        -31.6541
Evaluation/MinReturn                      -2068.32
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.17
Extras/EpisodeRewardMean                    -63.2917
LinearFeatureBaseline/ExplainedVariance       0.101063
PolicyExecTime                                0.108927
ProcessExecTime                               0.0112314
TotalEnvSteps                            546480
policy/Entropy                               -0.394606
policy/KL                                     0.00927875
policy/KLBefore                               0
policy/LossAfter                             -0.0161078
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.673945
policy/dLoss                                  0.0161078
---------------------------------------  ---------------
2022-04-23 14:22:24 | [train_policy] epoch #540 | Obtaining samples for iteration 540...
2022-04-23 14:22:25 | [train_policy] epoch #540 | Logging diagnostics...
2022-04-23 14:22:25 | [train_policy] epoch #540 | Optimizing policy...
2022-04-23 14:22:25 | [train_policy] epoch #540 | Computing loss before
2022-04-23 14:22:25 | [train_policy] epoch #540 | Computing KL before
2022-04-23 14:22:25 | [train_policy] epoch #540 | Optimizing
2022-04-23 14:22:25 | [train_policy] epoch #540 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:25 | [train_policy] epoch #540 | computing loss before
2022-04-23 14:22:25 | [train_policy] epoch #540 | computing gradient
2022-04-23 14:22:25 | [train_policy] epoch #540 | gradient computed
2022-04-23 14:22:25 | [train_policy] epoch #540 | computing descent direction
2022-04-23 14:22:25 | [train_policy] epoch #540 | descent direction computed
2022-04-23 14:22:25 | [train_policy] epoch #540 | backtrack iters: 1
2022-04-23 14:22:25 | [train_policy] epoch #540 | optimization finished
2022-04-23 14:22:25 | [train_policy] epoch #540 | Computing KL after
2022-04-23 14:22:25 | [train_policy] epoch #540 | Computing loss after
2022-04-23 14:22:25 | [train_policy] epoch #540 | Fitting baseline...
2022-04-23 14:22:25 | [train_policy] epoch #540 | Saving snapshot...
2022-04-23 14:22:25 | [train_policy] epoch #540 | Saved
2022-04-23 14:22:25 | [train_policy] epoch #540 | Time 192.96 s
2022-04-23 14:22:25 | [train_policy] epoch #540 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.118745
Evaluation/AverageDiscountedReturn          -65.352
Evaluation/AverageReturn                    -65.352
Evaluation/CompletionRate                     0
Evaluation/Iteration                        540
Evaluation/MaxReturn                        -33.1613
Evaluation/MinReturn                      -2065.1
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.843
Extras/EpisodeRewardMean                    -63.6997
LinearFeatureBaseline/ExplainedVariance       0.118952
PolicyExecTime                                0.11591
ProcessExecTime                               0.0114739
TotalEnvSteps                            547492
policy/Entropy                               -0.418394
policy/KL                                     0.00688722
policy/KLBefore                               0
policy/LossAfter                             -0.0124569
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.658103
policy/dLoss                                  0.0124569
---------------------------------------  ----------------
2022-04-23 14:22:25 | [train_policy] epoch #541 | Obtaining samples for iteration 541...
2022-04-23 14:22:25 | [train_policy] epoch #541 | Logging diagnostics...
2022-04-23 14:22:25 | [train_policy] epoch #541 | Optimizing policy...
2022-04-23 14:22:25 | [train_policy] epoch #541 | Computing loss before
2022-04-23 14:22:25 | [train_policy] epoch #541 | Computing KL before
2022-04-23 14:22:25 | [train_policy] epoch #541 | Optimizing
2022-04-23 14:22:25 | [train_policy] epoch #541 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:25 | [train_policy] epoch #541 | computing loss before
2022-04-23 14:22:25 | [train_policy] epoch #541 | computing gradient
2022-04-23 14:22:25 | [train_policy] epoch #541 | gradient computed
2022-04-23 14:22:25 | [train_policy] epoch #541 | computing descent direction
2022-04-23 14:22:25 | [train_policy] epoch #541 | descent direction computed
2022-04-23 14:22:25 | [train_policy] epoch #541 | backtrack iters: 0
2022-04-23 14:22:25 | [train_policy] epoch #541 | optimization finished
2022-04-23 14:22:25 | [train_policy] epoch #541 | Computing KL after
2022-04-23 14:22:25 | [train_policy] epoch #541 | Computing loss after
2022-04-23 14:22:25 | [train_policy] epoch #541 | Fitting baseline...
2022-04-23 14:22:25 | [train_policy] epoch #541 | Saving snapshot...
2022-04-23 14:22:25 | [train_policy] epoch #541 | Saved
2022-04-23 14:22:25 | [train_policy] epoch #541 | Time 193.31 s
2022-04-23 14:22:25 | [train_policy] epoch #541 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118327
Evaluation/AverageDiscountedReturn          -87.7558
Evaluation/AverageReturn                    -87.7558
Evaluation/CompletionRate                     0
Evaluation/Iteration                        541
Evaluation/MaxReturn                        -30.3753
Evaluation/MinReturn                      -2065.15
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.88
Extras/EpisodeRewardMean                    -84.0501
LinearFeatureBaseline/ExplainedVariance       0.132758
PolicyExecTime                                0.11208
ProcessExecTime                               0.0113432
TotalEnvSteps                            548504
policy/Entropy                               -0.422242
policy/KL                                     0.0095395
policy/KLBefore                               0
policy/LossAfter                             -0.0314928
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.655575
policy/dLoss                                  0.0314928
---------------------------------------  ----------------
2022-04-23 14:22:25 | [train_policy] epoch #542 | Obtaining samples for iteration 542...
2022-04-23 14:22:25 | [train_policy] epoch #542 | Logging diagnostics...
2022-04-23 14:22:25 | [train_policy] epoch #542 | Optimizing policy...
2022-04-23 14:22:25 | [train_policy] epoch #542 | Computing loss before
2022-04-23 14:22:25 | [train_policy] epoch #542 | Computing KL before
2022-04-23 14:22:25 | [train_policy] epoch #542 | Optimizing
2022-04-23 14:22:25 | [train_policy] epoch #542 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:25 | [train_policy] epoch #542 | computing loss before
2022-04-23 14:22:25 | [train_policy] epoch #542 | computing gradient
2022-04-23 14:22:25 | [train_policy] epoch #542 | gradient computed
2022-04-23 14:22:25 | [train_policy] epoch #542 | computing descent direction
2022-04-23 14:22:25 | [train_policy] epoch #542 | descent direction computed
2022-04-23 14:22:25 | [train_policy] epoch #542 | backtrack iters: 0
2022-04-23 14:22:25 | [train_policy] epoch #542 | optimization finished
2022-04-23 14:22:25 | [train_policy] epoch #542 | Computing KL after
2022-04-23 14:22:25 | [train_policy] epoch #542 | Computing loss after
2022-04-23 14:22:25 | [train_policy] epoch #542 | Fitting baseline...
2022-04-23 14:22:25 | [train_policy] epoch #542 | Saving snapshot...
2022-04-23 14:22:25 | [train_policy] epoch #542 | Saved
2022-04-23 14:22:25 | [train_policy] epoch #542 | Time 193.66 s
2022-04-23 14:22:25 | [train_policy] epoch #542 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118328
Evaluation/AverageDiscountedReturn          -65.9946
Evaluation/AverageReturn                    -65.9946
Evaluation/CompletionRate                     0
Evaluation/Iteration                        542
Evaluation/MaxReturn                        -30.497
Evaluation/MinReturn                      -2063.7
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.628
Extras/EpisodeRewardMean                    -84.4158
LinearFeatureBaseline/ExplainedVariance       0.0566503
PolicyExecTime                                0.111638
ProcessExecTime                               0.01191
TotalEnvSteps                            549516
policy/Entropy                               -0.419665
policy/KL                                     0.00977945
policy/KLBefore                               0
policy/LossAfter                             -0.0208014
policy/LossBefore                             1.01304e-08
policy/Perplexity                             0.657267
policy/dLoss                                  0.0208014
---------------------------------------  ----------------
2022-04-23 14:22:25 | [train_policy] epoch #543 | Obtaining samples for iteration 543...
2022-04-23 14:22:26 | [train_policy] epoch #543 | Logging diagnostics...
2022-04-23 14:22:26 | [train_policy] epoch #543 | Optimizing policy...
2022-04-23 14:22:26 | [train_policy] epoch #543 | Computing loss before
2022-04-23 14:22:26 | [train_policy] epoch #543 | Computing KL before
2022-04-23 14:22:26 | [train_policy] epoch #543 | Optimizing
2022-04-23 14:22:26 | [train_policy] epoch #543 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:26 | [train_policy] epoch #543 | computing loss before
2022-04-23 14:22:26 | [train_policy] epoch #543 | computing gradient
2022-04-23 14:22:26 | [train_policy] epoch #543 | gradient computed
2022-04-23 14:22:26 | [train_policy] epoch #543 | computing descent direction
2022-04-23 14:22:26 | [train_policy] epoch #543 | descent direction computed
2022-04-23 14:22:26 | [train_policy] epoch #543 | backtrack iters: 1
2022-04-23 14:22:26 | [train_policy] epoch #543 | optimization finished
2022-04-23 14:22:26 | [train_policy] epoch #543 | Computing KL after
2022-04-23 14:22:26 | [train_policy] epoch #543 | Computing loss after
2022-04-23 14:22:26 | [train_policy] epoch #543 | Fitting baseline...
2022-04-23 14:22:26 | [train_policy] epoch #543 | Saving snapshot...
2022-04-23 14:22:26 | [train_policy] epoch #543 | Saved
2022-04-23 14:22:26 | [train_policy] epoch #543 | Time 194.03 s
2022-04-23 14:22:26 | [train_policy] epoch #543 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.121136
Evaluation/AverageDiscountedReturn          -64.1543
Evaluation/AverageReturn                    -64.1543
Evaluation/CompletionRate                     0
Evaluation/Iteration                        543
Evaluation/MaxReturn                        -30.4289
Evaluation/MinReturn                      -2063.18
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.66
Extras/EpisodeRewardMean                    -82.8922
LinearFeatureBaseline/ExplainedVariance       0.142607
PolicyExecTime                                0.109066
ProcessExecTime                               0.0122285
TotalEnvSteps                            550528
policy/Entropy                               -0.412846
policy/KL                                     0.00657088
policy/KLBefore                               0
policy/LossAfter                             -0.024997
policy/LossBefore                            -1.17796e-08
policy/Perplexity                             0.661764
policy/dLoss                                  0.024997
---------------------------------------  ----------------
2022-04-23 14:22:26 | [train_policy] epoch #544 | Obtaining samples for iteration 544...
2022-04-23 14:22:26 | [train_policy] epoch #544 | Logging diagnostics...
2022-04-23 14:22:26 | [train_policy] epoch #544 | Optimizing policy...
2022-04-23 14:22:26 | [train_policy] epoch #544 | Computing loss before
2022-04-23 14:22:26 | [train_policy] epoch #544 | Computing KL before
2022-04-23 14:22:26 | [train_policy] epoch #544 | Optimizing
2022-04-23 14:22:26 | [train_policy] epoch #544 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:26 | [train_policy] epoch #544 | computing loss before
2022-04-23 14:22:26 | [train_policy] epoch #544 | computing gradient
2022-04-23 14:22:26 | [train_policy] epoch #544 | gradient computed
2022-04-23 14:22:26 | [train_policy] epoch #544 | computing descent direction
2022-04-23 14:22:26 | [train_policy] epoch #544 | descent direction computed
2022-04-23 14:22:26 | [train_policy] epoch #544 | backtrack iters: 1
2022-04-23 14:22:26 | [train_policy] epoch #544 | optimization finished
2022-04-23 14:22:26 | [train_policy] epoch #544 | Computing KL after
2022-04-23 14:22:26 | [train_policy] epoch #544 | Computing loss after
2022-04-23 14:22:26 | [train_policy] epoch #544 | Fitting baseline...
2022-04-23 14:22:26 | [train_policy] epoch #544 | Saving snapshot...
2022-04-23 14:22:26 | [train_policy] epoch #544 | Saved
2022-04-23 14:22:26 | [train_policy] epoch #544 | Time 194.39 s
2022-04-23 14:22:26 | [train_policy] epoch #544 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.120256
Evaluation/AverageDiscountedReturn          -65.0075
Evaluation/AverageReturn                    -65.0075
Evaluation/CompletionRate                     0
Evaluation/Iteration                        544
Evaluation/MaxReturn                        -32.1396
Evaluation/MinReturn                      -2064.29
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.815
Extras/EpisodeRewardMean                    -63.4876
LinearFeatureBaseline/ExplainedVariance       0.0990083
PolicyExecTime                                0.110046
ProcessExecTime                               0.0123916
TotalEnvSteps                            551540
policy/Entropy                               -0.428651
policy/KL                                     0.00636489
policy/KLBefore                               0
policy/LossAfter                             -0.0233607
policy/LossBefore                             1.13084e-08
policy/Perplexity                             0.651387
policy/dLoss                                  0.0233607
---------------------------------------  ----------------
2022-04-23 14:22:26 | [train_policy] epoch #545 | Obtaining samples for iteration 545...
2022-04-23 14:22:26 | [train_policy] epoch #545 | Logging diagnostics...
2022-04-23 14:22:26 | [train_policy] epoch #545 | Optimizing policy...
2022-04-23 14:22:26 | [train_policy] epoch #545 | Computing loss before
2022-04-23 14:22:26 | [train_policy] epoch #545 | Computing KL before
2022-04-23 14:22:26 | [train_policy] epoch #545 | Optimizing
2022-04-23 14:22:26 | [train_policy] epoch #545 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:26 | [train_policy] epoch #545 | computing loss before
2022-04-23 14:22:26 | [train_policy] epoch #545 | computing gradient
2022-04-23 14:22:26 | [train_policy] epoch #545 | gradient computed
2022-04-23 14:22:26 | [train_policy] epoch #545 | computing descent direction
2022-04-23 14:22:27 | [train_policy] epoch #545 | descent direction computed
2022-04-23 14:22:27 | [train_policy] epoch #545 | backtrack iters: 1
2022-04-23 14:22:27 | [train_policy] epoch #545 | optimization finished
2022-04-23 14:22:27 | [train_policy] epoch #545 | Computing KL after
2022-04-23 14:22:27 | [train_policy] epoch #545 | Computing loss after
2022-04-23 14:22:27 | [train_policy] epoch #545 | Fitting baseline...
2022-04-23 14:22:27 | [train_policy] epoch #545 | Saving snapshot...
2022-04-23 14:22:27 | [train_policy] epoch #545 | Saved
2022-04-23 14:22:27 | [train_policy] epoch #545 | Time 194.75 s
2022-04-23 14:22:27 | [train_policy] epoch #545 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.117389
Evaluation/AverageDiscountedReturn          -44.586
Evaluation/AverageReturn                    -44.586
Evaluation/CompletionRate                     0
Evaluation/Iteration                        545
Evaluation/MaxReturn                        -33.0908
Evaluation/MinReturn                       -128.208
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         12.3245
Extras/EpisodeRewardMean                    -44.4176
LinearFeatureBaseline/ExplainedVariance     -15.1911
PolicyExecTime                                0.104813
ProcessExecTime                               0.0120537
TotalEnvSteps                            552552
policy/Entropy                               -0.472258
policy/KL                                     0.00667782
policy/KLBefore                               0
policy/LossAfter                             -0.0201496
policy/LossBefore                             1.2133e-08
policy/Perplexity                             0.623592
policy/dLoss                                  0.0201496
---------------------------------------  ---------------
2022-04-23 14:22:27 | [train_policy] epoch #546 | Obtaining samples for iteration 546...
2022-04-23 14:22:27 | [train_policy] epoch #546 | Logging diagnostics...
2022-04-23 14:22:27 | [train_policy] epoch #546 | Optimizing policy...
2022-04-23 14:22:27 | [train_policy] epoch #546 | Computing loss before
2022-04-23 14:22:27 | [train_policy] epoch #546 | Computing KL before
2022-04-23 14:22:27 | [train_policy] epoch #546 | Optimizing
2022-04-23 14:22:27 | [train_policy] epoch #546 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:27 | [train_policy] epoch #546 | computing loss before
2022-04-23 14:22:27 | [train_policy] epoch #546 | computing gradient
2022-04-23 14:22:27 | [train_policy] epoch #546 | gradient computed
2022-04-23 14:22:27 | [train_policy] epoch #546 | computing descent direction
2022-04-23 14:22:27 | [train_policy] epoch #546 | descent direction computed
2022-04-23 14:22:27 | [train_policy] epoch #546 | backtrack iters: 1
2022-04-23 14:22:27 | [train_policy] epoch #546 | optimization finished
2022-04-23 14:22:27 | [train_policy] epoch #546 | Computing KL after
2022-04-23 14:22:27 | [train_policy] epoch #546 | Computing loss after
2022-04-23 14:22:27 | [train_policy] epoch #546 | Fitting baseline...
2022-04-23 14:22:27 | [train_policy] epoch #546 | Saving snapshot...
2022-04-23 14:22:27 | [train_policy] epoch #546 | Saved
2022-04-23 14:22:27 | [train_policy] epoch #546 | Time 195.09 s
2022-04-23 14:22:27 | [train_policy] epoch #546 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117022
Evaluation/AverageDiscountedReturn          -51.5504
Evaluation/AverageReturn                    -51.5504
Evaluation/CompletionRate                     0
Evaluation/Iteration                        546
Evaluation/MaxReturn                        -33.1315
Evaluation/MinReturn                       -606.439
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         60.1034
Extras/EpisodeRewardMean                    -51.0173
LinearFeatureBaseline/ExplainedVariance       0.375855
PolicyExecTime                                0.105419
ProcessExecTime                               0.0115671
TotalEnvSteps                            553564
policy/Entropy                               -0.496276
policy/KL                                     0.00665754
policy/KLBefore                               0
policy/LossAfter                             -0.0291683
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.608794
policy/dLoss                                  0.0291683
---------------------------------------  ----------------
2022-04-23 14:22:27 | [train_policy] epoch #547 | Obtaining samples for iteration 547...
2022-04-23 14:22:27 | [train_policy] epoch #547 | Logging diagnostics...
2022-04-23 14:22:27 | [train_policy] epoch #547 | Optimizing policy...
2022-04-23 14:22:27 | [train_policy] epoch #547 | Computing loss before
2022-04-23 14:22:27 | [train_policy] epoch #547 | Computing KL before
2022-04-23 14:22:27 | [train_policy] epoch #547 | Optimizing
2022-04-23 14:22:27 | [train_policy] epoch #547 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:27 | [train_policy] epoch #547 | computing loss before
2022-04-23 14:22:27 | [train_policy] epoch #547 | computing gradient
2022-04-23 14:22:27 | [train_policy] epoch #547 | gradient computed
2022-04-23 14:22:27 | [train_policy] epoch #547 | computing descent direction
2022-04-23 14:22:27 | [train_policy] epoch #547 | descent direction computed
2022-04-23 14:22:27 | [train_policy] epoch #547 | backtrack iters: 1
2022-04-23 14:22:27 | [train_policy] epoch #547 | optimization finished
2022-04-23 14:22:27 | [train_policy] epoch #547 | Computing KL after
2022-04-23 14:22:27 | [train_policy] epoch #547 | Computing loss after
2022-04-23 14:22:27 | [train_policy] epoch #547 | Fitting baseline...
2022-04-23 14:22:27 | [train_policy] epoch #547 | Saving snapshot...
2022-04-23 14:22:27 | [train_policy] epoch #547 | Saved
2022-04-23 14:22:27 | [train_policy] epoch #547 | Time 195.43 s
2022-04-23 14:22:27 | [train_policy] epoch #547 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116003
Evaluation/AverageDiscountedReturn          -43.5978
Evaluation/AverageReturn                    -43.5978
Evaluation/CompletionRate                     0
Evaluation/Iteration                        547
Evaluation/MaxReturn                        -33.2931
Evaluation/MinReturn                        -69.6907
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.32074
Extras/EpisodeRewardMean                    -43.3388
LinearFeatureBaseline/ExplainedVariance     -13.2151
PolicyExecTime                                0.0989928
ProcessExecTime                               0.0112824
TotalEnvSteps                            554576
policy/Entropy                               -0.504728
policy/KL                                     0.00648256
policy/KLBefore                               0
policy/LossAfter                             -0.0450899
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.60367
policy/dLoss                                  0.0450899
---------------------------------------  ----------------
2022-04-23 14:22:27 | [train_policy] epoch #548 | Obtaining samples for iteration 548...
2022-04-23 14:22:27 | [train_policy] epoch #548 | Logging diagnostics...
2022-04-23 14:22:27 | [train_policy] epoch #548 | Optimizing policy...
2022-04-23 14:22:27 | [train_policy] epoch #548 | Computing loss before
2022-04-23 14:22:27 | [train_policy] epoch #548 | Computing KL before
2022-04-23 14:22:27 | [train_policy] epoch #548 | Optimizing
2022-04-23 14:22:27 | [train_policy] epoch #548 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:27 | [train_policy] epoch #548 | computing loss before
2022-04-23 14:22:28 | [train_policy] epoch #548 | computing gradient
2022-04-23 14:22:28 | [train_policy] epoch #548 | gradient computed
2022-04-23 14:22:28 | [train_policy] epoch #548 | computing descent direction
2022-04-23 14:22:28 | [train_policy] epoch #548 | descent direction computed
2022-04-23 14:22:28 | [train_policy] epoch #548 | backtrack iters: 0
2022-04-23 14:22:28 | [train_policy] epoch #548 | optimization finished
2022-04-23 14:22:28 | [train_policy] epoch #548 | Computing KL after
2022-04-23 14:22:28 | [train_policy] epoch #548 | Computing loss after
2022-04-23 14:22:28 | [train_policy] epoch #548 | Fitting baseline...
2022-04-23 14:22:28 | [train_policy] epoch #548 | Saving snapshot...
2022-04-23 14:22:28 | [train_policy] epoch #548 | Saved
2022-04-23 14:22:28 | [train_policy] epoch #548 | Time 195.79 s
2022-04-23 14:22:28 | [train_policy] epoch #548 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116654
Evaluation/AverageDiscountedReturn          -43.4752
Evaluation/AverageReturn                    -43.4752
Evaluation/CompletionRate                     0
Evaluation/Iteration                        548
Evaluation/MaxReturn                        -32.1144
Evaluation/MinReturn                        -67.4455
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.51627
Extras/EpisodeRewardMean                    -43.5348
LinearFeatureBaseline/ExplainedVariance       0.835015
PolicyExecTime                                0.114446
ProcessExecTime                               0.0115829
TotalEnvSteps                            555588
policy/Entropy                               -0.543053
policy/KL                                     0.00997579
policy/KLBefore                               0
policy/LossAfter                             -0.0284119
policy/LossBefore                            -2.41481e-09
policy/Perplexity                             0.580972
policy/dLoss                                  0.0284119
---------------------------------------  ----------------
2022-04-23 14:22:28 | [train_policy] epoch #549 | Obtaining samples for iteration 549...
2022-04-23 14:22:28 | [train_policy] epoch #549 | Logging diagnostics...
2022-04-23 14:22:28 | [train_policy] epoch #549 | Optimizing policy...
2022-04-23 14:22:28 | [train_policy] epoch #549 | Computing loss before
2022-04-23 14:22:28 | [train_policy] epoch #549 | Computing KL before
2022-04-23 14:22:28 | [train_policy] epoch #549 | Optimizing
2022-04-23 14:22:28 | [train_policy] epoch #549 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:28 | [train_policy] epoch #549 | computing loss before
2022-04-23 14:22:28 | [train_policy] epoch #549 | computing gradient
2022-04-23 14:22:28 | [train_policy] epoch #549 | gradient computed
2022-04-23 14:22:28 | [train_policy] epoch #549 | computing descent direction
2022-04-23 14:22:28 | [train_policy] epoch #549 | descent direction computed
2022-04-23 14:22:28 | [train_policy] epoch #549 | backtrack iters: 1
2022-04-23 14:22:28 | [train_policy] epoch #549 | optimization finished
2022-04-23 14:22:28 | [train_policy] epoch #549 | Computing KL after
2022-04-23 14:22:28 | [train_policy] epoch #549 | Computing loss after
2022-04-23 14:22:28 | [train_policy] epoch #549 | Fitting baseline...
2022-04-23 14:22:28 | [train_policy] epoch #549 | Saving snapshot...
2022-04-23 14:22:28 | [train_policy] epoch #549 | Saved
2022-04-23 14:22:28 | [train_policy] epoch #549 | Time 196.14 s
2022-04-23 14:22:28 | [train_policy] epoch #549 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115748
Evaluation/AverageDiscountedReturn          -65.3134
Evaluation/AverageReturn                    -65.3134
Evaluation/CompletionRate                     0
Evaluation/Iteration                        549
Evaluation/MaxReturn                        -27.9644
Evaluation/MinReturn                      -2062.56
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.547
Extras/EpisodeRewardMean                    -63.3865
LinearFeatureBaseline/ExplainedVariance       0.0133726
PolicyExecTime                                0.102019
ProcessExecTime                               0.0110211
TotalEnvSteps                            556600
policy/Entropy                               -0.544588
policy/KL                                     0.00698625
policy/KLBefore                               0
policy/LossAfter                             -0.0238068
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.580081
policy/dLoss                                  0.0238068
---------------------------------------  ----------------
2022-04-23 14:22:28 | [train_policy] epoch #550 | Obtaining samples for iteration 550...
2022-04-23 14:22:28 | [train_policy] epoch #550 | Logging diagnostics...
2022-04-23 14:22:28 | [train_policy] epoch #550 | Optimizing policy...
2022-04-23 14:22:28 | [train_policy] epoch #550 | Computing loss before
2022-04-23 14:22:28 | [train_policy] epoch #550 | Computing KL before
2022-04-23 14:22:28 | [train_policy] epoch #550 | Optimizing
2022-04-23 14:22:28 | [train_policy] epoch #550 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:28 | [train_policy] epoch #550 | computing loss before
2022-04-23 14:22:28 | [train_policy] epoch #550 | computing gradient
2022-04-23 14:22:28 | [train_policy] epoch #550 | gradient computed
2022-04-23 14:22:28 | [train_policy] epoch #550 | computing descent direction
2022-04-23 14:22:28 | [train_policy] epoch #550 | descent direction computed
2022-04-23 14:22:28 | [train_policy] epoch #550 | backtrack iters: 1
2022-04-23 14:22:28 | [train_policy] epoch #550 | optimization finished
2022-04-23 14:22:28 | [train_policy] epoch #550 | Computing KL after
2022-04-23 14:22:28 | [train_policy] epoch #550 | Computing loss after
2022-04-23 14:22:28 | [train_policy] epoch #550 | Fitting baseline...
2022-04-23 14:22:28 | [train_policy] epoch #550 | Saving snapshot...
2022-04-23 14:22:28 | [train_policy] epoch #550 | Saved
2022-04-23 14:22:28 | [train_policy] epoch #550 | Time 196.50 s
2022-04-23 14:22:28 | [train_policy] epoch #550 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119756
Evaluation/AverageDiscountedReturn          -41.574
Evaluation/AverageReturn                    -41.574
Evaluation/CompletionRate                     0
Evaluation/Iteration                        550
Evaluation/MaxReturn                        -33.1245
Evaluation/MinReturn                        -76.0165
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83721
Extras/EpisodeRewardMean                    -41.6014
LinearFeatureBaseline/ExplainedVariance     -25.4788
PolicyExecTime                                0.112267
ProcessExecTime                               0.0116656
TotalEnvSteps                            557612
policy/Entropy                               -0.546262
policy/KL                                     0.00692937
policy/KLBefore                               0
policy/LossAfter                             -0.0157878
policy/LossBefore                             2.26168e-08
policy/Perplexity                             0.579111
policy/dLoss                                  0.0157878
---------------------------------------  ----------------
2022-04-23 14:22:28 | [train_policy] epoch #551 | Obtaining samples for iteration 551...
2022-04-23 14:22:29 | [train_policy] epoch #551 | Logging diagnostics...
2022-04-23 14:22:29 | [train_policy] epoch #551 | Optimizing policy...
2022-04-23 14:22:29 | [train_policy] epoch #551 | Computing loss before
2022-04-23 14:22:29 | [train_policy] epoch #551 | Computing KL before
2022-04-23 14:22:29 | [train_policy] epoch #551 | Optimizing
2022-04-23 14:22:29 | [train_policy] epoch #551 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:29 | [train_policy] epoch #551 | computing loss before
2022-04-23 14:22:29 | [train_policy] epoch #551 | computing gradient
2022-04-23 14:22:29 | [train_policy] epoch #551 | gradient computed
2022-04-23 14:22:29 | [train_policy] epoch #551 | computing descent direction
2022-04-23 14:22:29 | [train_policy] epoch #551 | descent direction computed
2022-04-23 14:22:29 | [train_policy] epoch #551 | backtrack iters: 0
2022-04-23 14:22:29 | [train_policy] epoch #551 | optimization finished
2022-04-23 14:22:29 | [train_policy] epoch #551 | Computing KL after
2022-04-23 14:22:29 | [train_policy] epoch #551 | Computing loss after
2022-04-23 14:22:29 | [train_policy] epoch #551 | Fitting baseline...
2022-04-23 14:22:29 | [train_policy] epoch #551 | Saving snapshot...
2022-04-23 14:22:29 | [train_policy] epoch #551 | Saved
2022-04-23 14:22:29 | [train_policy] epoch #551 | Time 196.84 s
2022-04-23 14:22:29 | [train_policy] epoch #551 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118565
Evaluation/AverageDiscountedReturn          -43.6502
Evaluation/AverageReturn                    -43.6502
Evaluation/CompletionRate                     0
Evaluation/Iteration                        551
Evaluation/MaxReturn                        -28.6696
Evaluation/MinReturn                        -77.7484
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.0133
Extras/EpisodeRewardMean                    -43.3124
LinearFeatureBaseline/ExplainedVariance       0.826701
PolicyExecTime                                0.104399
ProcessExecTime                               0.0115409
TotalEnvSteps                            558624
policy/Entropy                               -0.577126
policy/KL                                     0.00876558
policy/KLBefore                               0
policy/LossAfter                             -0.0386553
policy/LossBefore                             1.76694e-08
policy/Perplexity                             0.56151
policy/dLoss                                  0.0386553
---------------------------------------  ----------------
2022-04-23 14:22:29 | [train_policy] epoch #552 | Obtaining samples for iteration 552...
2022-04-23 14:22:29 | [train_policy] epoch #552 | Logging diagnostics...
2022-04-23 14:22:29 | [train_policy] epoch #552 | Optimizing policy...
2022-04-23 14:22:29 | [train_policy] epoch #552 | Computing loss before
2022-04-23 14:22:29 | [train_policy] epoch #552 | Computing KL before
2022-04-23 14:22:29 | [train_policy] epoch #552 | Optimizing
2022-04-23 14:22:29 | [train_policy] epoch #552 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:29 | [train_policy] epoch #552 | computing loss before
2022-04-23 14:22:29 | [train_policy] epoch #552 | computing gradient
2022-04-23 14:22:29 | [train_policy] epoch #552 | gradient computed
2022-04-23 14:22:29 | [train_policy] epoch #552 | computing descent direction
2022-04-23 14:22:29 | [train_policy] epoch #552 | descent direction computed
2022-04-23 14:22:29 | [train_policy] epoch #552 | backtrack iters: 1
2022-04-23 14:22:29 | [train_policy] epoch #552 | optimization finished
2022-04-23 14:22:29 | [train_policy] epoch #552 | Computing KL after
2022-04-23 14:22:29 | [train_policy] epoch #552 | Computing loss after
2022-04-23 14:22:29 | [train_policy] epoch #552 | Fitting baseline...
2022-04-23 14:22:29 | [train_policy] epoch #552 | Saving snapshot...
2022-04-23 14:22:29 | [train_policy] epoch #552 | Saved
2022-04-23 14:22:29 | [train_policy] epoch #552 | Time 197.17 s
2022-04-23 14:22:29 | [train_policy] epoch #552 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118556
Evaluation/AverageDiscountedReturn          -86.1364
Evaluation/AverageReturn                    -86.1364
Evaluation/CompletionRate                     0
Evaluation/Iteration                        552
Evaluation/MaxReturn                        -29.1089
Evaluation/MinReturn                      -2064.99
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.868
Extras/EpisodeRewardMean                    -82.6665
LinearFeatureBaseline/ExplainedVariance       0.0115159
PolicyExecTime                                0.102877
ProcessExecTime                               0.0114622
TotalEnvSteps                            559636
policy/Entropy                               -0.616928
policy/KL                                     0.00696335
policy/KLBefore                               0
policy/LossAfter                             -0.022003
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.539599
policy/dLoss                                  0.022003
---------------------------------------  ----------------
2022-04-23 14:22:29 | [train_policy] epoch #553 | Obtaining samples for iteration 553...
2022-04-23 14:22:29 | [train_policy] epoch #553 | Logging diagnostics...
2022-04-23 14:22:29 | [train_policy] epoch #553 | Optimizing policy...
2022-04-23 14:22:29 | [train_policy] epoch #553 | Computing loss before
2022-04-23 14:22:29 | [train_policy] epoch #553 | Computing KL before
2022-04-23 14:22:29 | [train_policy] epoch #553 | Optimizing
2022-04-23 14:22:29 | [train_policy] epoch #553 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:29 | [train_policy] epoch #553 | computing loss before
2022-04-23 14:22:29 | [train_policy] epoch #553 | computing gradient
2022-04-23 14:22:29 | [train_policy] epoch #553 | gradient computed
2022-04-23 14:22:29 | [train_policy] epoch #553 | computing descent direction
2022-04-23 14:22:29 | [train_policy] epoch #553 | descent direction computed
2022-04-23 14:22:29 | [train_policy] epoch #553 | backtrack iters: 0
2022-04-23 14:22:29 | [train_policy] epoch #553 | optimization finished
2022-04-23 14:22:29 | [train_policy] epoch #553 | Computing KL after
2022-04-23 14:22:29 | [train_policy] epoch #553 | Computing loss after
2022-04-23 14:22:29 | [train_policy] epoch #553 | Fitting baseline...
2022-04-23 14:22:29 | [train_policy] epoch #553 | Saving snapshot...
2022-04-23 14:22:29 | [train_policy] epoch #553 | Saved
2022-04-23 14:22:29 | [train_policy] epoch #553 | Time 197.52 s
2022-04-23 14:22:29 | [train_policy] epoch #553 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119649
Evaluation/AverageDiscountedReturn          -41.7031
Evaluation/AverageReturn                    -41.7031
Evaluation/CompletionRate                     0
Evaluation/Iteration                        553
Evaluation/MaxReturn                        -33.0838
Evaluation/MinReturn                        -89.5287
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.82387
Extras/EpisodeRewardMean                    -41.8185
LinearFeatureBaseline/ExplainedVariance     -98.869
PolicyExecTime                                0.108126
ProcessExecTime                               0.0116446
TotalEnvSteps                            560648
policy/Entropy                               -0.633509
policy/KL                                     0.00976094
policy/KLBefore                               0
policy/LossAfter                             -0.0223123
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             0.530726
policy/dLoss                                  0.0223123
---------------------------------------  ----------------
2022-04-23 14:22:29 | [train_policy] epoch #554 | Obtaining samples for iteration 554...
2022-04-23 14:22:30 | [train_policy] epoch #554 | Logging diagnostics...
2022-04-23 14:22:30 | [train_policy] epoch #554 | Optimizing policy...
2022-04-23 14:22:30 | [train_policy] epoch #554 | Computing loss before
2022-04-23 14:22:30 | [train_policy] epoch #554 | Computing KL before
2022-04-23 14:22:30 | [train_policy] epoch #554 | Optimizing
2022-04-23 14:22:30 | [train_policy] epoch #554 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:30 | [train_policy] epoch #554 | computing loss before
2022-04-23 14:22:30 | [train_policy] epoch #554 | computing gradient
2022-04-23 14:22:30 | [train_policy] epoch #554 | gradient computed
2022-04-23 14:22:30 | [train_policy] epoch #554 | computing descent direction
2022-04-23 14:22:30 | [train_policy] epoch #554 | descent direction computed
2022-04-23 14:22:30 | [train_policy] epoch #554 | backtrack iters: 1
2022-04-23 14:22:30 | [train_policy] epoch #554 | optimization finished
2022-04-23 14:22:30 | [train_policy] epoch #554 | Computing KL after
2022-04-23 14:22:30 | [train_policy] epoch #554 | Computing loss after
2022-04-23 14:22:30 | [train_policy] epoch #554 | Fitting baseline...
2022-04-23 14:22:30 | [train_policy] epoch #554 | Saving snapshot...
2022-04-23 14:22:30 | [train_policy] epoch #554 | Saved
2022-04-23 14:22:30 | [train_policy] epoch #554 | Time 197.87 s
2022-04-23 14:22:30 | [train_policy] epoch #554 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116329
Evaluation/AverageDiscountedReturn          -41.8464
Evaluation/AverageReturn                    -41.8464
Evaluation/CompletionRate                     0
Evaluation/Iteration                        554
Evaluation/MaxReturn                        -29.5817
Evaluation/MinReturn                        -65.8316
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83038
Extras/EpisodeRewardMean                    -41.7247
LinearFeatureBaseline/ExplainedVariance       0.866769
PolicyExecTime                                0.10689
ProcessExecTime                               0.0113356
TotalEnvSteps                            561660
policy/Entropy                               -0.638211
policy/KL                                     0.00669333
policy/KLBefore                               0
policy/LossAfter                             -0.0142115
policy/LossBefore                            -3.88726e-09
policy/Perplexity                             0.528237
policy/dLoss                                  0.0142115
---------------------------------------  ----------------
2022-04-23 14:22:30 | [train_policy] epoch #555 | Obtaining samples for iteration 555...
2022-04-23 14:22:30 | [train_policy] epoch #555 | Logging diagnostics...
2022-04-23 14:22:30 | [train_policy] epoch #555 | Optimizing policy...
2022-04-23 14:22:30 | [train_policy] epoch #555 | Computing loss before
2022-04-23 14:22:30 | [train_policy] epoch #555 | Computing KL before
2022-04-23 14:22:30 | [train_policy] epoch #555 | Optimizing
2022-04-23 14:22:30 | [train_policy] epoch #555 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:30 | [train_policy] epoch #555 | computing loss before
2022-04-23 14:22:30 | [train_policy] epoch #555 | computing gradient
2022-04-23 14:22:30 | [train_policy] epoch #555 | gradient computed
2022-04-23 14:22:30 | [train_policy] epoch #555 | computing descent direction
2022-04-23 14:22:30 | [train_policy] epoch #555 | descent direction computed
2022-04-23 14:22:30 | [train_policy] epoch #555 | backtrack iters: 0
2022-04-23 14:22:30 | [train_policy] epoch #555 | optimization finished
2022-04-23 14:22:30 | [train_policy] epoch #555 | Computing KL after
2022-04-23 14:22:30 | [train_policy] epoch #555 | Computing loss after
2022-04-23 14:22:30 | [train_policy] epoch #555 | Fitting baseline...
2022-04-23 14:22:30 | [train_policy] epoch #555 | Saving snapshot...
2022-04-23 14:22:30 | [train_policy] epoch #555 | Saved
2022-04-23 14:22:30 | [train_policy] epoch #555 | Time 198.20 s
2022-04-23 14:22:30 | [train_policy] epoch #555 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119281
Evaluation/AverageDiscountedReturn          -65.3079
Evaluation/AverageReturn                    -65.3079
Evaluation/CompletionRate                     0
Evaluation/Iteration                        555
Evaluation/MaxReturn                        -30.0355
Evaluation/MinReturn                      -2065.14
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.823
Extras/EpisodeRewardMean                    -63.4209
LinearFeatureBaseline/ExplainedVariance       0.0107314
PolicyExecTime                                0.0979505
ProcessExecTime                               0.0117021
TotalEnvSteps                            562672
policy/Entropy                               -0.62668
policy/KL                                     0.00980846
policy/KLBefore                               0
policy/LossAfter                             -0.0268285
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.534363
policy/dLoss                                  0.0268285
---------------------------------------  ----------------
2022-04-23 14:22:30 | [train_policy] epoch #556 | Obtaining samples for iteration 556...
2022-04-23 14:22:30 | [train_policy] epoch #556 | Logging diagnostics...
2022-04-23 14:22:30 | [train_policy] epoch #556 | Optimizing policy...
2022-04-23 14:22:30 | [train_policy] epoch #556 | Computing loss before
2022-04-23 14:22:30 | [train_policy] epoch #556 | Computing KL before
2022-04-23 14:22:30 | [train_policy] epoch #556 | Optimizing
2022-04-23 14:22:30 | [train_policy] epoch #556 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:30 | [train_policy] epoch #556 | computing loss before
2022-04-23 14:22:30 | [train_policy] epoch #556 | computing gradient
2022-04-23 14:22:30 | [train_policy] epoch #556 | gradient computed
2022-04-23 14:22:30 | [train_policy] epoch #556 | computing descent direction
2022-04-23 14:22:30 | [train_policy] epoch #556 | descent direction computed
2022-04-23 14:22:30 | [train_policy] epoch #556 | backtrack iters: 1
2022-04-23 14:22:30 | [train_policy] epoch #556 | optimization finished
2022-04-23 14:22:30 | [train_policy] epoch #556 | Computing KL after
2022-04-23 14:22:30 | [train_policy] epoch #556 | Computing loss after
2022-04-23 14:22:30 | [train_policy] epoch #556 | Fitting baseline...
2022-04-23 14:22:30 | [train_policy] epoch #556 | Saving snapshot...
2022-04-23 14:22:30 | [train_policy] epoch #556 | Saved
2022-04-23 14:22:30 | [train_policy] epoch #556 | Time 198.54 s
2022-04-23 14:22:30 | [train_policy] epoch #556 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117541
Evaluation/AverageDiscountedReturn          -42.6459
Evaluation/AverageReturn                    -42.6459
Evaluation/CompletionRate                     0
Evaluation/Iteration                        556
Evaluation/MaxReturn                        -29.5202
Evaluation/MinReturn                        -69.0393
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.80307
Extras/EpisodeRewardMean                    -42.6339
LinearFeatureBaseline/ExplainedVariance     -30.8861
PolicyExecTime                                0.107614
ProcessExecTime                               0.0118144
TotalEnvSteps                            563684
policy/Entropy                               -0.626475
policy/KL                                     0.00650131
policy/KLBefore                               0
policy/LossAfter                             -0.0223067
policy/LossBefore                            -1.93185e-08
policy/Perplexity                             0.534473
policy/dLoss                                  0.0223066
---------------------------------------  ----------------
2022-04-23 14:22:30 | [train_policy] epoch #557 | Obtaining samples for iteration 557...
2022-04-23 14:22:31 | [train_policy] epoch #557 | Logging diagnostics...
2022-04-23 14:22:31 | [train_policy] epoch #557 | Optimizing policy...
2022-04-23 14:22:31 | [train_policy] epoch #557 | Computing loss before
2022-04-23 14:22:31 | [train_policy] epoch #557 | Computing KL before
2022-04-23 14:22:31 | [train_policy] epoch #557 | Optimizing
2022-04-23 14:22:31 | [train_policy] epoch #557 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:31 | [train_policy] epoch #557 | computing loss before
2022-04-23 14:22:31 | [train_policy] epoch #557 | computing gradient
2022-04-23 14:22:31 | [train_policy] epoch #557 | gradient computed
2022-04-23 14:22:31 | [train_policy] epoch #557 | computing descent direction
2022-04-23 14:22:31 | [train_policy] epoch #557 | descent direction computed
2022-04-23 14:22:31 | [train_policy] epoch #557 | backtrack iters: 1
2022-04-23 14:22:31 | [train_policy] epoch #557 | optimization finished
2022-04-23 14:22:31 | [train_policy] epoch #557 | Computing KL after
2022-04-23 14:22:31 | [train_policy] epoch #557 | Computing loss after
2022-04-23 14:22:31 | [train_policy] epoch #557 | Fitting baseline...
2022-04-23 14:22:31 | [train_policy] epoch #557 | Saving snapshot...
2022-04-23 14:22:31 | [train_policy] epoch #557 | Saved
2022-04-23 14:22:31 | [train_policy] epoch #557 | Time 198.86 s
2022-04-23 14:22:31 | [train_policy] epoch #557 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118837
Evaluation/AverageDiscountedReturn         -108.411
Evaluation/AverageReturn                   -108.411
Evaluation/CompletionRate                     0
Evaluation/Iteration                        557
Evaluation/MaxReturn                        -28.3077
Evaluation/MinReturn                      -2067.85
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.681
Extras/EpisodeRewardMean                   -103.17
LinearFeatureBaseline/ExplainedVariance       0.0100699
PolicyExecTime                                0.0963974
ProcessExecTime                               0.0113082
TotalEnvSteps                            564696
policy/Entropy                               -0.598567
policy/KL                                     0.0065812
policy/KLBefore                               0
policy/LossAfter                             -0.0185693
policy/LossBefore                            -1.46067e-08
policy/Perplexity                             0.549599
policy/dLoss                                  0.0185693
---------------------------------------  ----------------
2022-04-23 14:22:31 | [train_policy] epoch #558 | Obtaining samples for iteration 558...
2022-04-23 14:22:31 | [train_policy] epoch #558 | Logging diagnostics...
2022-04-23 14:22:31 | [train_policy] epoch #558 | Optimizing policy...
2022-04-23 14:22:31 | [train_policy] epoch #558 | Computing loss before
2022-04-23 14:22:31 | [train_policy] epoch #558 | Computing KL before
2022-04-23 14:22:31 | [train_policy] epoch #558 | Optimizing
2022-04-23 14:22:31 | [train_policy] epoch #558 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:31 | [train_policy] epoch #558 | computing loss before
2022-04-23 14:22:31 | [train_policy] epoch #558 | computing gradient
2022-04-23 14:22:31 | [train_policy] epoch #558 | gradient computed
2022-04-23 14:22:31 | [train_policy] epoch #558 | computing descent direction
2022-04-23 14:22:31 | [train_policy] epoch #558 | descent direction computed
2022-04-23 14:22:31 | [train_policy] epoch #558 | backtrack iters: 0
2022-04-23 14:22:31 | [train_policy] epoch #558 | optimization finished
2022-04-23 14:22:31 | [train_policy] epoch #558 | Computing KL after
2022-04-23 14:22:31 | [train_policy] epoch #558 | Computing loss after
2022-04-23 14:22:31 | [train_policy] epoch #558 | Fitting baseline...
2022-04-23 14:22:31 | [train_policy] epoch #558 | Saving snapshot...
2022-04-23 14:22:31 | [train_policy] epoch #558 | Saved
2022-04-23 14:22:31 | [train_policy] epoch #558 | Time 199.22 s
2022-04-23 14:22:31 | [train_policy] epoch #558 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.116755
Evaluation/AverageDiscountedReturn         -107.488
Evaluation/AverageReturn                   -107.488
Evaluation/CompletionRate                     0
Evaluation/Iteration                        558
Evaluation/MaxReturn                        -29.1359
Evaluation/MinReturn                      -2065.19
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.423
Extras/EpisodeRewardMean                   -102.347
LinearFeatureBaseline/ExplainedVariance       0.261024
PolicyExecTime                                0.117449
ProcessExecTime                               0.0120647
TotalEnvSteps                            565708
policy/Entropy                               -0.588473
policy/KL                                     0.00919607
policy/KLBefore                               0
policy/LossAfter                             -0.0189241
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             0.555174
policy/dLoss                                  0.0189241
---------------------------------------  ----------------
2022-04-23 14:22:31 | [train_policy] epoch #559 | Obtaining samples for iteration 559...
2022-04-23 14:22:31 | [train_policy] epoch #559 | Logging diagnostics...
2022-04-23 14:22:31 | [train_policy] epoch #559 | Optimizing policy...
2022-04-23 14:22:31 | [train_policy] epoch #559 | Computing loss before
2022-04-23 14:22:31 | [train_policy] epoch #559 | Computing KL before
2022-04-23 14:22:31 | [train_policy] epoch #559 | Optimizing
2022-04-23 14:22:31 | [train_policy] epoch #559 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:31 | [train_policy] epoch #559 | computing loss before
2022-04-23 14:22:31 | [train_policy] epoch #559 | computing gradient
2022-04-23 14:22:31 | [train_policy] epoch #559 | gradient computed
2022-04-23 14:22:31 | [train_policy] epoch #559 | computing descent direction
2022-04-23 14:22:31 | [train_policy] epoch #559 | descent direction computed
2022-04-23 14:22:31 | [train_policy] epoch #559 | backtrack iters: 1
2022-04-23 14:22:31 | [train_policy] epoch #559 | optimization finished
2022-04-23 14:22:31 | [train_policy] epoch #559 | Computing KL after
2022-04-23 14:22:31 | [train_policy] epoch #559 | Computing loss after
2022-04-23 14:22:31 | [train_policy] epoch #559 | Fitting baseline...
2022-04-23 14:22:31 | [train_policy] epoch #559 | Saving snapshot...
2022-04-23 14:22:31 | [train_policy] epoch #559 | Saved
2022-04-23 14:22:31 | [train_policy] epoch #559 | Time 199.58 s
2022-04-23 14:22:31 | [train_policy] epoch #559 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.123535
Evaluation/AverageDiscountedReturn          -43.6183
Evaluation/AverageReturn                    -43.6183
Evaluation/CompletionRate                     0
Evaluation/Iteration                        559
Evaluation/MaxReturn                        -32.8818
Evaluation/MinReturn                        -75.7773
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.73171
Extras/EpisodeRewardMean                    -63.7328
LinearFeatureBaseline/ExplainedVariance    -153.929
PolicyExecTime                                0.113714
ProcessExecTime                               0.0122266
TotalEnvSteps                            566720
policy/Entropy                               -0.545589
policy/KL                                     0.0073258
policy/KLBefore                               0
policy/LossAfter                             -0.0241636
policy/LossBefore                             2.87422e-08
policy/Perplexity                             0.5795
policy/dLoss                                  0.0241636
---------------------------------------  ----------------
2022-04-23 14:22:31 | [train_policy] epoch #560 | Obtaining samples for iteration 560...
2022-04-23 14:22:32 | [train_policy] epoch #560 | Logging diagnostics...
2022-04-23 14:22:32 | [train_policy] epoch #560 | Optimizing policy...
2022-04-23 14:22:32 | [train_policy] epoch #560 | Computing loss before
2022-04-23 14:22:32 | [train_policy] epoch #560 | Computing KL before
2022-04-23 14:22:32 | [train_policy] epoch #560 | Optimizing
2022-04-23 14:22:32 | [train_policy] epoch #560 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:32 | [train_policy] epoch #560 | computing loss before
2022-04-23 14:22:32 | [train_policy] epoch #560 | computing gradient
2022-04-23 14:22:32 | [train_policy] epoch #560 | gradient computed
2022-04-23 14:22:32 | [train_policy] epoch #560 | computing descent direction
2022-04-23 14:22:32 | [train_policy] epoch #560 | descent direction computed
2022-04-23 14:22:32 | [train_policy] epoch #560 | backtrack iters: 1
2022-04-23 14:22:32 | [train_policy] epoch #560 | optimization finished
2022-04-23 14:22:32 | [train_policy] epoch #560 | Computing KL after
2022-04-23 14:22:32 | [train_policy] epoch #560 | Computing loss after
2022-04-23 14:22:32 | [train_policy] epoch #560 | Fitting baseline...
2022-04-23 14:22:32 | [train_policy] epoch #560 | Saving snapshot...
2022-04-23 14:22:32 | [train_policy] epoch #560 | Saved
2022-04-23 14:22:32 | [train_policy] epoch #560 | Time 199.96 s
2022-04-23 14:22:32 | [train_policy] epoch #560 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.126092
Evaluation/AverageDiscountedReturn          -86.0655
Evaluation/AverageReturn                    -86.0655
Evaluation/CompletionRate                     0
Evaluation/Iteration                        560
Evaluation/MaxReturn                        -32.5533
Evaluation/MinReturn                      -2064.18
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.903
Extras/EpisodeRewardMean                    -83.0027
LinearFeatureBaseline/ExplainedVariance       0.011477
PolicyExecTime                                0.11927
ProcessExecTime                               0.0122771
TotalEnvSteps                            567732
policy/Entropy                               -0.559799
policy/KL                                     0.00670184
policy/KLBefore                               0
policy/LossAfter                             -0.0145758
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.571324
policy/dLoss                                  0.0145758
---------------------------------------  ----------------
2022-04-23 14:22:32 | [train_policy] epoch #561 | Obtaining samples for iteration 561...
2022-04-23 14:22:32 | [train_policy] epoch #561 | Logging diagnostics...
2022-04-23 14:22:32 | [train_policy] epoch #561 | Optimizing policy...
2022-04-23 14:22:32 | [train_policy] epoch #561 | Computing loss before
2022-04-23 14:22:32 | [train_policy] epoch #561 | Computing KL before
2022-04-23 14:22:32 | [train_policy] epoch #561 | Optimizing
2022-04-23 14:22:32 | [train_policy] epoch #561 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:32 | [train_policy] epoch #561 | computing loss before
2022-04-23 14:22:32 | [train_policy] epoch #561 | computing gradient
2022-04-23 14:22:32 | [train_policy] epoch #561 | gradient computed
2022-04-23 14:22:32 | [train_policy] epoch #561 | computing descent direction
2022-04-23 14:22:32 | [train_policy] epoch #561 | descent direction computed
2022-04-23 14:22:32 | [train_policy] epoch #561 | backtrack iters: 1
2022-04-23 14:22:32 | [train_policy] epoch #561 | optimization finished
2022-04-23 14:22:32 | [train_policy] epoch #561 | Computing KL after
2022-04-23 14:22:32 | [train_policy] epoch #561 | Computing loss after
2022-04-23 14:22:32 | [train_policy] epoch #561 | Fitting baseline...
2022-04-23 14:22:32 | [train_policy] epoch #561 | Saving snapshot...
2022-04-23 14:22:32 | [train_policy] epoch #561 | Saved
2022-04-23 14:22:32 | [train_policy] epoch #561 | Time 200.31 s
2022-04-23 14:22:32 | [train_policy] epoch #561 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.122888
Evaluation/AverageDiscountedReturn          -63.1239
Evaluation/AverageReturn                    -63.1239
Evaluation/CompletionRate                     0
Evaluation/Iteration                        561
Evaluation/MaxReturn                        -28.7675
Evaluation/MinReturn                      -2063.95
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.87
Extras/EpisodeRewardMean                    -61.5653
LinearFeatureBaseline/ExplainedVariance      -0.054524
PolicyExecTime                                0.107799
ProcessExecTime                               0.0118248
TotalEnvSteps                            568744
policy/Entropy                               -0.602611
policy/KL                                     0.00705354
policy/KLBefore                               0
policy/LossAfter                             -0.0157804
policy/LossBefore                             1.10728e-08
policy/Perplexity                             0.547381
policy/dLoss                                  0.0157804
---------------------------------------  ----------------
2022-04-23 14:22:32 | [train_policy] epoch #562 | Obtaining samples for iteration 562...
2022-04-23 14:22:32 | [train_policy] epoch #562 | Logging diagnostics...
2022-04-23 14:22:32 | [train_policy] epoch #562 | Optimizing policy...
2022-04-23 14:22:32 | [train_policy] epoch #562 | Computing loss before
2022-04-23 14:22:32 | [train_policy] epoch #562 | Computing KL before
2022-04-23 14:22:32 | [train_policy] epoch #562 | Optimizing
2022-04-23 14:22:32 | [train_policy] epoch #562 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:32 | [train_policy] epoch #562 | computing loss before
2022-04-23 14:22:32 | [train_policy] epoch #562 | computing gradient
2022-04-23 14:22:32 | [train_policy] epoch #562 | gradient computed
2022-04-23 14:22:32 | [train_policy] epoch #562 | computing descent direction
2022-04-23 14:22:32 | [train_policy] epoch #562 | descent direction computed
2022-04-23 14:22:32 | [train_policy] epoch #562 | backtrack iters: 1
2022-04-23 14:22:32 | [train_policy] epoch #562 | optimization finished
2022-04-23 14:22:32 | [train_policy] epoch #562 | Computing KL after
2022-04-23 14:22:32 | [train_policy] epoch #562 | Computing loss after
2022-04-23 14:22:32 | [train_policy] epoch #562 | Fitting baseline...
2022-04-23 14:22:32 | [train_policy] epoch #562 | Saving snapshot...
2022-04-23 14:22:32 | [train_policy] epoch #562 | Saved
2022-04-23 14:22:32 | [train_policy] epoch #562 | Time 200.67 s
2022-04-23 14:22:32 | [train_policy] epoch #562 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.124014
Evaluation/AverageDiscountedReturn          -42.3441
Evaluation/AverageReturn                    -42.3441
Evaluation/CompletionRate                     0
Evaluation/Iteration                        562
Evaluation/MaxReturn                        -29.5238
Evaluation/MinReturn                        -60.2376
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.46422
Extras/EpisodeRewardMean                    -42.1611
LinearFeatureBaseline/ExplainedVariance     -28.0476
PolicyExecTime                                0.106192
ProcessExecTime                               0.0121584
TotalEnvSteps                            569756
policy/Entropy                               -0.58643
policy/KL                                     0.00684353
policy/KLBefore                               0
policy/LossAfter                             -0.0118767
policy/LossBefore                             2.23812e-08
policy/Perplexity                             0.55631
policy/dLoss                                  0.0118768
---------------------------------------  ----------------
2022-04-23 14:22:32 | [train_policy] epoch #563 | Obtaining samples for iteration 563...
2022-04-23 14:22:33 | [train_policy] epoch #563 | Logging diagnostics...
2022-04-23 14:22:33 | [train_policy] epoch #563 | Optimizing policy...
2022-04-23 14:22:33 | [train_policy] epoch #563 | Computing loss before
2022-04-23 14:22:33 | [train_policy] epoch #563 | Computing KL before
2022-04-23 14:22:33 | [train_policy] epoch #563 | Optimizing
2022-04-23 14:22:33 | [train_policy] epoch #563 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:33 | [train_policy] epoch #563 | computing loss before
2022-04-23 14:22:33 | [train_policy] epoch #563 | computing gradient
2022-04-23 14:22:33 | [train_policy] epoch #563 | gradient computed
2022-04-23 14:22:33 | [train_policy] epoch #563 | computing descent direction
2022-04-23 14:22:33 | [train_policy] epoch #563 | descent direction computed
2022-04-23 14:22:33 | [train_policy] epoch #563 | backtrack iters: 1
2022-04-23 14:22:33 | [train_policy] epoch #563 | optimization finished
2022-04-23 14:22:33 | [train_policy] epoch #563 | Computing KL after
2022-04-23 14:22:33 | [train_policy] epoch #563 | Computing loss after
2022-04-23 14:22:33 | [train_policy] epoch #563 | Fitting baseline...
2022-04-23 14:22:33 | [train_policy] epoch #563 | Saving snapshot...
2022-04-23 14:22:33 | [train_policy] epoch #563 | Saved
2022-04-23 14:22:33 | [train_policy] epoch #563 | Time 201.03 s
2022-04-23 14:22:33 | [train_policy] epoch #563 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.126296
Evaluation/AverageDiscountedReturn          -41.79
Evaluation/AverageReturn                    -41.79
Evaluation/CompletionRate                     0
Evaluation/Iteration                        563
Evaluation/MaxReturn                        -32.5021
Evaluation/MinReturn                        -76.984
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.60554
Extras/EpisodeRewardMean                    -41.6621
LinearFeatureBaseline/ExplainedVariance       0.90113
PolicyExecTime                                0.102
ProcessExecTime                               0.0122898
TotalEnvSteps                            570768
policy/Entropy                               -0.617423
policy/KL                                     0.00647255
policy/KLBefore                               0
policy/LossAfter                             -0.0152231
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.539332
policy/dLoss                                  0.0152231
---------------------------------------  ----------------
2022-04-23 14:22:33 | [train_policy] epoch #564 | Obtaining samples for iteration 564...
2022-04-23 14:22:33 | [train_policy] epoch #564 | Logging diagnostics...
2022-04-23 14:22:33 | [train_policy] epoch #564 | Optimizing policy...
2022-04-23 14:22:33 | [train_policy] epoch #564 | Computing loss before
2022-04-23 14:22:33 | [train_policy] epoch #564 | Computing KL before
2022-04-23 14:22:33 | [train_policy] epoch #564 | Optimizing
2022-04-23 14:22:33 | [train_policy] epoch #564 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:33 | [train_policy] epoch #564 | computing loss before
2022-04-23 14:22:33 | [train_policy] epoch #564 | computing gradient
2022-04-23 14:22:33 | [train_policy] epoch #564 | gradient computed
2022-04-23 14:22:33 | [train_policy] epoch #564 | computing descent direction
2022-04-23 14:22:33 | [train_policy] epoch #564 | descent direction computed
2022-04-23 14:22:33 | [train_policy] epoch #564 | backtrack iters: 1
2022-04-23 14:22:33 | [train_policy] epoch #564 | optimization finished
2022-04-23 14:22:33 | [train_policy] epoch #564 | Computing KL after
2022-04-23 14:22:33 | [train_policy] epoch #564 | Computing loss after
2022-04-23 14:22:33 | [train_policy] epoch #564 | Fitting baseline...
2022-04-23 14:22:33 | [train_policy] epoch #564 | Saving snapshot...
2022-04-23 14:22:33 | [train_policy] epoch #564 | Saved
2022-04-23 14:22:33 | [train_policy] epoch #564 | Time 201.36 s
2022-04-23 14:22:33 | [train_policy] epoch #564 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115424
Evaluation/AverageDiscountedReturn          -86.0797
Evaluation/AverageReturn                    -86.0797
Evaluation/CompletionRate                     0
Evaluation/Iteration                        564
Evaluation/MaxReturn                        -32.1618
Evaluation/MinReturn                      -2065
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.074
Extras/EpisodeRewardMean                    -82.459
LinearFeatureBaseline/ExplainedVariance       0.00998224
PolicyExecTime                                0.0957878
ProcessExecTime                               0.0108695
TotalEnvSteps                            571780
policy/Entropy                               -0.608476
policy/KL                                     0.00643129
policy/KLBefore                               0
policy/LossAfter                             -0.0264852
policy/LossBefore                             6.12538e-09
policy/Perplexity                             0.544179
policy/dLoss                                  0.0264852
---------------------------------------  ----------------
2022-04-23 14:22:33 | [train_policy] epoch #565 | Obtaining samples for iteration 565...
2022-04-23 14:22:33 | [train_policy] epoch #565 | Logging diagnostics...
2022-04-23 14:22:33 | [train_policy] epoch #565 | Optimizing policy...
2022-04-23 14:22:33 | [train_policy] epoch #565 | Computing loss before
2022-04-23 14:22:33 | [train_policy] epoch #565 | Computing KL before
2022-04-23 14:22:33 | [train_policy] epoch #565 | Optimizing
2022-04-23 14:22:33 | [train_policy] epoch #565 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:33 | [train_policy] epoch #565 | computing loss before
2022-04-23 14:22:33 | [train_policy] epoch #565 | computing gradient
2022-04-23 14:22:33 | [train_policy] epoch #565 | gradient computed
2022-04-23 14:22:33 | [train_policy] epoch #565 | computing descent direction
2022-04-23 14:22:33 | [train_policy] epoch #565 | descent direction computed
2022-04-23 14:22:33 | [train_policy] epoch #565 | backtrack iters: 1
2022-04-23 14:22:33 | [train_policy] epoch #565 | optimization finished
2022-04-23 14:22:33 | [train_policy] epoch #565 | Computing KL after
2022-04-23 14:22:33 | [train_policy] epoch #565 | Computing loss after
2022-04-23 14:22:33 | [train_policy] epoch #565 | Fitting baseline...
2022-04-23 14:22:33 | [train_policy] epoch #565 | Saving snapshot...
2022-04-23 14:22:33 | [train_policy] epoch #565 | Saved
2022-04-23 14:22:33 | [train_policy] epoch #565 | Time 201.70 s
2022-04-23 14:22:33 | [train_policy] epoch #565 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116078
Evaluation/AverageDiscountedReturn          -65.2188
Evaluation/AverageReturn                    -65.2188
Evaluation/CompletionRate                     0
Evaluation/Iteration                        565
Evaluation/MaxReturn                        -32.0118
Evaluation/MinReturn                      -2065.79
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.851
Extras/EpisodeRewardMean                    -63.569
LinearFeatureBaseline/ExplainedVariance      -0.0125069
PolicyExecTime                                0.108043
ProcessExecTime                               0.0113575
TotalEnvSteps                            572792
policy/Entropy                               -0.592758
policy/KL                                     0.0075041
policy/KLBefore                               0
policy/LossAfter                             -0.0167203
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.5528
policy/dLoss                                  0.0167203
---------------------------------------  ----------------
2022-04-23 14:22:33 | [train_policy] epoch #566 | Obtaining samples for iteration 566...
2022-04-23 14:22:34 | [train_policy] epoch #566 | Logging diagnostics...
2022-04-23 14:22:34 | [train_policy] epoch #566 | Optimizing policy...
2022-04-23 14:22:34 | [train_policy] epoch #566 | Computing loss before
2022-04-23 14:22:34 | [train_policy] epoch #566 | Computing KL before
2022-04-23 14:22:34 | [train_policy] epoch #566 | Optimizing
2022-04-23 14:22:34 | [train_policy] epoch #566 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:34 | [train_policy] epoch #566 | computing loss before
2022-04-23 14:22:34 | [train_policy] epoch #566 | computing gradient
2022-04-23 14:22:34 | [train_policy] epoch #566 | gradient computed
2022-04-23 14:22:34 | [train_policy] epoch #566 | computing descent direction
2022-04-23 14:22:34 | [train_policy] epoch #566 | descent direction computed
2022-04-23 14:22:34 | [train_policy] epoch #566 | backtrack iters: 0
2022-04-23 14:22:34 | [train_policy] epoch #566 | optimization finished
2022-04-23 14:22:34 | [train_policy] epoch #566 | Computing KL after
2022-04-23 14:22:34 | [train_policy] epoch #566 | Computing loss after
2022-04-23 14:22:34 | [train_policy] epoch #566 | Fitting baseline...
2022-04-23 14:22:34 | [train_policy] epoch #566 | Saving snapshot...
2022-04-23 14:22:34 | [train_policy] epoch #566 | Saved
2022-04-23 14:22:34 | [train_policy] epoch #566 | Time 202.03 s
2022-04-23 14:22:34 | [train_policy] epoch #566 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115742
Evaluation/AverageDiscountedReturn          -63.9627
Evaluation/AverageReturn                    -63.9627
Evaluation/CompletionRate                     0
Evaluation/Iteration                        566
Evaluation/MaxReturn                        -33.4611
Evaluation/MinReturn                      -2064.35
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.826
Extras/EpisodeRewardMean                    -62.2507
LinearFeatureBaseline/ExplainedVariance       0.126288
PolicyExecTime                                0.0988123
ProcessExecTime                               0.0109107
TotalEnvSteps                            573804
policy/Entropy                               -0.538638
policy/KL                                     0.00962729
policy/KLBefore                               0
policy/LossAfter                             -0.0181456
policy/LossBefore                             2.07321e-08
policy/Perplexity                             0.583542
policy/dLoss                                  0.0181456
---------------------------------------  ----------------
2022-04-23 14:22:34 | [train_policy] epoch #567 | Obtaining samples for iteration 567...
2022-04-23 14:22:34 | [train_policy] epoch #567 | Logging diagnostics...
2022-04-23 14:22:34 | [train_policy] epoch #567 | Optimizing policy...
2022-04-23 14:22:34 | [train_policy] epoch #567 | Computing loss before
2022-04-23 14:22:34 | [train_policy] epoch #567 | Computing KL before
2022-04-23 14:22:34 | [train_policy] epoch #567 | Optimizing
2022-04-23 14:22:34 | [train_policy] epoch #567 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:34 | [train_policy] epoch #567 | computing loss before
2022-04-23 14:22:34 | [train_policy] epoch #567 | computing gradient
2022-04-23 14:22:34 | [train_policy] epoch #567 | gradient computed
2022-04-23 14:22:34 | [train_policy] epoch #567 | computing descent direction
2022-04-23 14:22:34 | [train_policy] epoch #567 | descent direction computed
2022-04-23 14:22:34 | [train_policy] epoch #567 | backtrack iters: 1
2022-04-23 14:22:34 | [train_policy] epoch #567 | optimization finished
2022-04-23 14:22:34 | [train_policy] epoch #567 | Computing KL after
2022-04-23 14:22:34 | [train_policy] epoch #567 | Computing loss after
2022-04-23 14:22:34 | [train_policy] epoch #567 | Fitting baseline...
2022-04-23 14:22:34 | [train_policy] epoch #567 | Saving snapshot...
2022-04-23 14:22:34 | [train_policy] epoch #567 | Saved
2022-04-23 14:22:34 | [train_policy] epoch #567 | Time 202.38 s
2022-04-23 14:22:34 | [train_policy] epoch #567 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116465
Evaluation/AverageDiscountedReturn          -65.7904
Evaluation/AverageReturn                    -65.7904
Evaluation/CompletionRate                     0
Evaluation/Iteration                        567
Evaluation/MaxReturn                        -28.6286
Evaluation/MinReturn                      -2066.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.301
Extras/EpisodeRewardMean                    -63.803
LinearFeatureBaseline/ExplainedVariance       0.113831
PolicyExecTime                                0.0984459
ProcessExecTime                               0.0109584
TotalEnvSteps                            574816
policy/Entropy                               -0.540543
policy/KL                                     0.007593
policy/KLBefore                               0
policy/LossAfter                             -0.0280296
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.582432
policy/dLoss                                  0.0280296
---------------------------------------  ----------------
2022-04-23 14:22:34 | [train_policy] epoch #568 | Obtaining samples for iteration 568...
2022-04-23 14:22:34 | [train_policy] epoch #568 | Logging diagnostics...
2022-04-23 14:22:34 | [train_policy] epoch #568 | Optimizing policy...
2022-04-23 14:22:34 | [train_policy] epoch #568 | Computing loss before
2022-04-23 14:22:34 | [train_policy] epoch #568 | Computing KL before
2022-04-23 14:22:34 | [train_policy] epoch #568 | Optimizing
2022-04-23 14:22:34 | [train_policy] epoch #568 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:34 | [train_policy] epoch #568 | computing loss before
2022-04-23 14:22:34 | [train_policy] epoch #568 | computing gradient
2022-04-23 14:22:34 | [train_policy] epoch #568 | gradient computed
2022-04-23 14:22:34 | [train_policy] epoch #568 | computing descent direction
2022-04-23 14:22:34 | [train_policy] epoch #568 | descent direction computed
2022-04-23 14:22:34 | [train_policy] epoch #568 | backtrack iters: 1
2022-04-23 14:22:34 | [train_policy] epoch #568 | optimization finished
2022-04-23 14:22:34 | [train_policy] epoch #568 | Computing KL after
2022-04-23 14:22:34 | [train_policy] epoch #568 | Computing loss after
2022-04-23 14:22:34 | [train_policy] epoch #568 | Fitting baseline...
2022-04-23 14:22:34 | [train_policy] epoch #568 | Saving snapshot...
2022-04-23 14:22:35 | [train_policy] epoch #568 | Saved
2022-04-23 14:22:35 | [train_policy] epoch #568 | Time 202.72 s
2022-04-23 14:22:35 | [train_policy] epoch #568 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115688
Evaluation/AverageDiscountedReturn          -42.8138
Evaluation/AverageReturn                    -42.8138
Evaluation/CompletionRate                     0
Evaluation/Iteration                        568
Evaluation/MaxReturn                        -28.9463
Evaluation/MinReturn                        -69.1519
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.15373
Extras/EpisodeRewardMean                    -63.3742
LinearFeatureBaseline/ExplainedVariance     -23.4708
PolicyExecTime                                0.100671
ProcessExecTime                               0.0109763
TotalEnvSteps                            575828
policy/Entropy                               -0.503823
policy/KL                                     0.00688016
policy/KLBefore                               0
policy/LossAfter                             -0.0146683
policy/LossBefore                            -2.85066e-08
policy/Perplexity                             0.604216
policy/dLoss                                  0.0146683
---------------------------------------  ----------------
2022-04-23 14:22:35 | [train_policy] epoch #569 | Obtaining samples for iteration 569...
2022-04-23 14:22:35 | [train_policy] epoch #569 | Logging diagnostics...
2022-04-23 14:22:35 | [train_policy] epoch #569 | Optimizing policy...
2022-04-23 14:22:35 | [train_policy] epoch #569 | Computing loss before
2022-04-23 14:22:35 | [train_policy] epoch #569 | Computing KL before
2022-04-23 14:22:35 | [train_policy] epoch #569 | Optimizing
2022-04-23 14:22:35 | [train_policy] epoch #569 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:35 | [train_policy] epoch #569 | computing loss before
2022-04-23 14:22:35 | [train_policy] epoch #569 | computing gradient
2022-04-23 14:22:35 | [train_policy] epoch #569 | gradient computed
2022-04-23 14:22:35 | [train_policy] epoch #569 | computing descent direction
2022-04-23 14:22:35 | [train_policy] epoch #569 | descent direction computed
2022-04-23 14:22:35 | [train_policy] epoch #569 | backtrack iters: 0
2022-04-23 14:22:35 | [train_policy] epoch #569 | optimization finished
2022-04-23 14:22:35 | [train_policy] epoch #569 | Computing KL after
2022-04-23 14:22:35 | [train_policy] epoch #569 | Computing loss after
2022-04-23 14:22:35 | [train_policy] epoch #569 | Fitting baseline...
2022-04-23 14:22:35 | [train_policy] epoch #569 | Saving snapshot...
2022-04-23 14:22:35 | [train_policy] epoch #569 | Saved
2022-04-23 14:22:35 | [train_policy] epoch #569 | Time 203.07 s
2022-04-23 14:22:35 | [train_policy] epoch #569 | EpochTime 0.35 s
---------------------------------------  ---------------
EnvExecTime                                   0.118136
Evaluation/AverageDiscountedReturn          -71.4242
Evaluation/AverageReturn                    -71.4242
Evaluation/CompletionRate                     0
Evaluation/Iteration                        569
Evaluation/MaxReturn                        -31.5326
Evaluation/MinReturn                      -2310.96
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        235.743
Extras/EpisodeRewardMean                    -68.9851
LinearFeatureBaseline/ExplainedVariance       0.0125976
PolicyExecTime                                0.107818
ProcessExecTime                               0.0112631
TotalEnvSteps                            576840
policy/Entropy                               -0.481951
policy/KL                                     0.00973441
policy/KLBefore                               0
policy/LossAfter                             -0.0249528
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.617578
policy/dLoss                                  0.0249528
---------------------------------------  ---------------
2022-04-23 14:22:35 | [train_policy] epoch #570 | Obtaining samples for iteration 570...
2022-04-23 14:22:35 | [train_policy] epoch #570 | Logging diagnostics...
2022-04-23 14:22:35 | [train_policy] epoch #570 | Optimizing policy...
2022-04-23 14:22:35 | [train_policy] epoch #570 | Computing loss before
2022-04-23 14:22:35 | [train_policy] epoch #570 | Computing KL before
2022-04-23 14:22:35 | [train_policy] epoch #570 | Optimizing
2022-04-23 14:22:35 | [train_policy] epoch #570 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:35 | [train_policy] epoch #570 | computing loss before
2022-04-23 14:22:35 | [train_policy] epoch #570 | computing gradient
2022-04-23 14:22:35 | [train_policy] epoch #570 | gradient computed
2022-04-23 14:22:35 | [train_policy] epoch #570 | computing descent direction
2022-04-23 14:22:35 | [train_policy] epoch #570 | descent direction computed
2022-04-23 14:22:35 | [train_policy] epoch #570 | backtrack iters: 1
2022-04-23 14:22:35 | [train_policy] epoch #570 | optimization finished
2022-04-23 14:22:35 | [train_policy] epoch #570 | Computing KL after
2022-04-23 14:22:35 | [train_policy] epoch #570 | Computing loss after
2022-04-23 14:22:35 | [train_policy] epoch #570 | Fitting baseline...
2022-04-23 14:22:35 | [train_policy] epoch #570 | Saving snapshot...
2022-04-23 14:22:35 | [train_policy] epoch #570 | Saved
2022-04-23 14:22:35 | [train_policy] epoch #570 | Time 203.44 s
2022-04-23 14:22:35 | [train_policy] epoch #570 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.122788
Evaluation/AverageDiscountedReturn          -42.3464
Evaluation/AverageReturn                    -42.3464
Evaluation/CompletionRate                     0
Evaluation/Iteration                        570
Evaluation/MaxReturn                        -30.5333
Evaluation/MinReturn                        -81.0874
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.56752
Extras/EpisodeRewardMean                    -42.794
LinearFeatureBaseline/ExplainedVariance     -32.3887
PolicyExecTime                                0.113358
ProcessExecTime                               0.012444
TotalEnvSteps                            577852
policy/Entropy                               -0.477599
policy/KL                                     0.00655164
policy/KLBefore                               0
policy/LossAfter                             -0.014183
policy/LossBefore                             2.36769e-08
policy/Perplexity                             0.620271
policy/dLoss                                  0.014183
---------------------------------------  ----------------
2022-04-23 14:22:35 | [train_policy] epoch #571 | Obtaining samples for iteration 571...
2022-04-23 14:22:36 | [train_policy] epoch #571 | Logging diagnostics...
2022-04-23 14:22:36 | [train_policy] epoch #571 | Optimizing policy...
2022-04-23 14:22:36 | [train_policy] epoch #571 | Computing loss before
2022-04-23 14:22:36 | [train_policy] epoch #571 | Computing KL before
2022-04-23 14:22:36 | [train_policy] epoch #571 | Optimizing
2022-04-23 14:22:36 | [train_policy] epoch #571 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:36 | [train_policy] epoch #571 | computing loss before
2022-04-23 14:22:36 | [train_policy] epoch #571 | computing gradient
2022-04-23 14:22:36 | [train_policy] epoch #571 | gradient computed
2022-04-23 14:22:36 | [train_policy] epoch #571 | computing descent direction
2022-04-23 14:22:36 | [train_policy] epoch #571 | descent direction computed
2022-04-23 14:22:36 | [train_policy] epoch #571 | backtrack iters: 0
2022-04-23 14:22:36 | [train_policy] epoch #571 | optimization finished
2022-04-23 14:22:36 | [train_policy] epoch #571 | Computing KL after
2022-04-23 14:22:36 | [train_policy] epoch #571 | Computing loss after
2022-04-23 14:22:36 | [train_policy] epoch #571 | Fitting baseline...
2022-04-23 14:22:36 | [train_policy] epoch #571 | Saving snapshot...
2022-04-23 14:22:36 | [train_policy] epoch #571 | Saved
2022-04-23 14:22:36 | [train_policy] epoch #571 | Time 203.81 s
2022-04-23 14:22:36 | [train_policy] epoch #571 | EpochTime 0.36 s
---------------------------------------  ---------------
EnvExecTime                                   0.119874
Evaluation/AverageDiscountedReturn          -42.4305
Evaluation/AverageReturn                    -42.4305
Evaluation/CompletionRate                     0
Evaluation/Iteration                        571
Evaluation/MaxReturn                        -29.923
Evaluation/MinReturn                        -61.5637
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.42867
Extras/EpisodeRewardMean                    -42.942
LinearFeatureBaseline/ExplainedVariance       0.833476
PolicyExecTime                                0.120193
ProcessExecTime                               0.012274
TotalEnvSteps                            578864
policy/Entropy                               -0.540734
policy/KL                                     0.00975257
policy/KLBefore                               0
policy/LossAfter                             -0.0196956
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.58232
policy/dLoss                                  0.0196956
---------------------------------------  ---------------
2022-04-23 14:22:36 | [train_policy] epoch #572 | Obtaining samples for iteration 572...
2022-04-23 14:22:36 | [train_policy] epoch #572 | Logging diagnostics...
2022-04-23 14:22:36 | [train_policy] epoch #572 | Optimizing policy...
2022-04-23 14:22:36 | [train_policy] epoch #572 | Computing loss before
2022-04-23 14:22:36 | [train_policy] epoch #572 | Computing KL before
2022-04-23 14:22:36 | [train_policy] epoch #572 | Optimizing
2022-04-23 14:22:36 | [train_policy] epoch #572 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:36 | [train_policy] epoch #572 | computing loss before
2022-04-23 14:22:36 | [train_policy] epoch #572 | computing gradient
2022-04-23 14:22:36 | [train_policy] epoch #572 | gradient computed
2022-04-23 14:22:36 | [train_policy] epoch #572 | computing descent direction
2022-04-23 14:22:36 | [train_policy] epoch #572 | descent direction computed
2022-04-23 14:22:36 | [train_policy] epoch #572 | backtrack iters: 0
2022-04-23 14:22:36 | [train_policy] epoch #572 | optimization finished
2022-04-23 14:22:36 | [train_policy] epoch #572 | Computing KL after
2022-04-23 14:22:36 | [train_policy] epoch #572 | Computing loss after
2022-04-23 14:22:36 | [train_policy] epoch #572 | Fitting baseline...
2022-04-23 14:22:36 | [train_policy] epoch #572 | Saving snapshot...
2022-04-23 14:22:36 | [train_policy] epoch #572 | Saved
2022-04-23 14:22:36 | [train_policy] epoch #572 | Time 204.16 s
2022-04-23 14:22:36 | [train_policy] epoch #572 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117603
Evaluation/AverageDiscountedReturn          -41.2732
Evaluation/AverageReturn                    -41.2732
Evaluation/CompletionRate                     0
Evaluation/Iteration                        572
Evaluation/MaxReturn                        -29.0652
Evaluation/MinReturn                        -75.4289
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85268
Extras/EpisodeRewardMean                    -41.3646
LinearFeatureBaseline/ExplainedVariance       0.872064
PolicyExecTime                                0.113842
ProcessExecTime                               0.011656
TotalEnvSteps                            579876
policy/Entropy                               -0.596187
policy/KL                                     0.00991319
policy/KLBefore                               0
policy/LossAfter                             -0.0237089
policy/LossBefore                            -1.60202e-08
policy/Perplexity                             0.550908
policy/dLoss                                  0.0237088
---------------------------------------  ----------------
2022-04-23 14:22:36 | [train_policy] epoch #573 | Obtaining samples for iteration 573...
2022-04-23 14:22:36 | [train_policy] epoch #573 | Logging diagnostics...
2022-04-23 14:22:36 | [train_policy] epoch #573 | Optimizing policy...
2022-04-23 14:22:36 | [train_policy] epoch #573 | Computing loss before
2022-04-23 14:22:36 | [train_policy] epoch #573 | Computing KL before
2022-04-23 14:22:36 | [train_policy] epoch #573 | Optimizing
2022-04-23 14:22:36 | [train_policy] epoch #573 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:36 | [train_policy] epoch #573 | computing loss before
2022-04-23 14:22:36 | [train_policy] epoch #573 | computing gradient
2022-04-23 14:22:36 | [train_policy] epoch #573 | gradient computed
2022-04-23 14:22:36 | [train_policy] epoch #573 | computing descent direction
2022-04-23 14:22:36 | [train_policy] epoch #573 | descent direction computed
2022-04-23 14:22:36 | [train_policy] epoch #573 | backtrack iters: 0
2022-04-23 14:22:36 | [train_policy] epoch #573 | optimization finished
2022-04-23 14:22:36 | [train_policy] epoch #573 | Computing KL after
2022-04-23 14:22:36 | [train_policy] epoch #573 | Computing loss after
2022-04-23 14:22:36 | [train_policy] epoch #573 | Fitting baseline...
2022-04-23 14:22:36 | [train_policy] epoch #573 | Saving snapshot...
2022-04-23 14:22:36 | [train_policy] epoch #573 | Saved
2022-04-23 14:22:36 | [train_policy] epoch #573 | Time 204.53 s
2022-04-23 14:22:36 | [train_policy] epoch #573 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.121564
Evaluation/AverageDiscountedReturn          -42.4675
Evaluation/AverageReturn                    -42.4675
Evaluation/CompletionRate                     0
Evaluation/Iteration                        573
Evaluation/MaxReturn                        -29.7542
Evaluation/MinReturn                        -74.2801
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.502
Extras/EpisodeRewardMean                    -42.187
LinearFeatureBaseline/ExplainedVariance       0.856983
PolicyExecTime                                0.113002
ProcessExecTime                               0.0119913
TotalEnvSteps                            580888
policy/Entropy                               -0.601279
policy/KL                                     0.00989995
policy/KLBefore                               0
policy/LossAfter                             -0.0164068
policy/LossBefore                            -2.94489e-09
policy/Perplexity                             0.54811
policy/dLoss                                  0.0164068
---------------------------------------  ----------------
2022-04-23 14:22:36 | [train_policy] epoch #574 | Obtaining samples for iteration 574...
2022-04-23 14:22:37 | [train_policy] epoch #574 | Logging diagnostics...
2022-04-23 14:22:37 | [train_policy] epoch #574 | Optimizing policy...
2022-04-23 14:22:37 | [train_policy] epoch #574 | Computing loss before
2022-04-23 14:22:37 | [train_policy] epoch #574 | Computing KL before
2022-04-23 14:22:37 | [train_policy] epoch #574 | Optimizing
2022-04-23 14:22:37 | [train_policy] epoch #574 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:37 | [train_policy] epoch #574 | computing loss before
2022-04-23 14:22:37 | [train_policy] epoch #574 | computing gradient
2022-04-23 14:22:37 | [train_policy] epoch #574 | gradient computed
2022-04-23 14:22:37 | [train_policy] epoch #574 | computing descent direction
2022-04-23 14:22:37 | [train_policy] epoch #574 | descent direction computed
2022-04-23 14:22:37 | [train_policy] epoch #574 | backtrack iters: 1
2022-04-23 14:22:37 | [train_policy] epoch #574 | optimization finished
2022-04-23 14:22:37 | [train_policy] epoch #574 | Computing KL after
2022-04-23 14:22:37 | [train_policy] epoch #574 | Computing loss after
2022-04-23 14:22:37 | [train_policy] epoch #574 | Fitting baseline...
2022-04-23 14:22:37 | [train_policy] epoch #574 | Saving snapshot...
2022-04-23 14:22:37 | [train_policy] epoch #574 | Saved
2022-04-23 14:22:37 | [train_policy] epoch #574 | Time 204.89 s
2022-04-23 14:22:37 | [train_policy] epoch #574 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.121278
Evaluation/AverageDiscountedReturn          -40.2784
Evaluation/AverageReturn                    -40.2784
Evaluation/CompletionRate                     0
Evaluation/Iteration                        574
Evaluation/MaxReturn                        -29.5535
Evaluation/MinReturn                        -57.4258
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.5004
Extras/EpisodeRewardMean                    -40.2501
LinearFeatureBaseline/ExplainedVariance       0.8979
PolicyExecTime                                0.113612
ProcessExecTime                               0.0119779
TotalEnvSteps                            581900
policy/Entropy                               -0.578743
policy/KL                                     0.00657775
policy/KLBefore                               0
policy/LossAfter                             -0.0133326
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.560603
policy/dLoss                                  0.0133326
---------------------------------------  ----------------
2022-04-23 14:22:37 | [train_policy] epoch #575 | Obtaining samples for iteration 575...
2022-04-23 14:22:37 | [train_policy] epoch #575 | Logging diagnostics...
2022-04-23 14:22:37 | [train_policy] epoch #575 | Optimizing policy...
2022-04-23 14:22:37 | [train_policy] epoch #575 | Computing loss before
2022-04-23 14:22:37 | [train_policy] epoch #575 | Computing KL before
2022-04-23 14:22:37 | [train_policy] epoch #575 | Optimizing
2022-04-23 14:22:37 | [train_policy] epoch #575 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:37 | [train_policy] epoch #575 | computing loss before
2022-04-23 14:22:37 | [train_policy] epoch #575 | computing gradient
2022-04-23 14:22:37 | [train_policy] epoch #575 | gradient computed
2022-04-23 14:22:37 | [train_policy] epoch #575 | computing descent direction
2022-04-23 14:22:37 | [train_policy] epoch #575 | descent direction computed
2022-04-23 14:22:37 | [train_policy] epoch #575 | backtrack iters: 1
2022-04-23 14:22:37 | [train_policy] epoch #575 | optimization finished
2022-04-23 14:22:37 | [train_policy] epoch #575 | Computing KL after
2022-04-23 14:22:37 | [train_policy] epoch #575 | Computing loss after
2022-04-23 14:22:37 | [train_policy] epoch #575 | Fitting baseline...
2022-04-23 14:22:37 | [train_policy] epoch #575 | Saving snapshot...
2022-04-23 14:22:37 | [train_policy] epoch #575 | Saved
2022-04-23 14:22:37 | [train_policy] epoch #575 | Time 205.24 s
2022-04-23 14:22:37 | [train_policy] epoch #575 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.124692
Evaluation/AverageDiscountedReturn          -84.7097
Evaluation/AverageReturn                    -84.7097
Evaluation/CompletionRate                     0
Evaluation/Iteration                        575
Evaluation/MaxReturn                        -29.8402
Evaluation/MinReturn                      -2070.08
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.885
Extras/EpisodeRewardMean                    -81.0406
LinearFeatureBaseline/ExplainedVariance       0.00923946
PolicyExecTime                                0.0976129
ProcessExecTime                               0.0116918
TotalEnvSteps                            582912
policy/Entropy                               -0.554514
policy/KL                                     0.00670529
policy/KLBefore                               0
policy/LossAfter                             -0.0240249
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.574351
policy/dLoss                                  0.0240249
---------------------------------------  ----------------
2022-04-23 14:22:37 | [train_policy] epoch #576 | Obtaining samples for iteration 576...
2022-04-23 14:22:37 | [train_policy] epoch #576 | Logging diagnostics...
2022-04-23 14:22:37 | [train_policy] epoch #576 | Optimizing policy...
2022-04-23 14:22:37 | [train_policy] epoch #576 | Computing loss before
2022-04-23 14:22:37 | [train_policy] epoch #576 | Computing KL before
2022-04-23 14:22:37 | [train_policy] epoch #576 | Optimizing
2022-04-23 14:22:37 | [train_policy] epoch #576 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:37 | [train_policy] epoch #576 | computing loss before
2022-04-23 14:22:37 | [train_policy] epoch #576 | computing gradient
2022-04-23 14:22:37 | [train_policy] epoch #576 | gradient computed
2022-04-23 14:22:37 | [train_policy] epoch #576 | computing descent direction
2022-04-23 14:22:37 | [train_policy] epoch #576 | descent direction computed
2022-04-23 14:22:37 | [train_policy] epoch #576 | backtrack iters: 1
2022-04-23 14:22:37 | [train_policy] epoch #576 | optimization finished
2022-04-23 14:22:37 | [train_policy] epoch #576 | Computing KL after
2022-04-23 14:22:37 | [train_policy] epoch #576 | Computing loss after
2022-04-23 14:22:37 | [train_policy] epoch #576 | Fitting baseline...
2022-04-23 14:22:37 | [train_policy] epoch #576 | Saving snapshot...
2022-04-23 14:22:37 | [train_policy] epoch #576 | Saved
2022-04-23 14:22:37 | [train_policy] epoch #576 | Time 205.59 s
2022-04-23 14:22:37 | [train_policy] epoch #576 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116051
Evaluation/AverageDiscountedReturn          -85.8457
Evaluation/AverageReturn                    -85.8457
Evaluation/CompletionRate                     0
Evaluation/Iteration                        576
Evaluation/MaxReturn                        -30.0319
Evaluation/MinReturn                      -2066.62
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.225
Extras/EpisodeRewardMean                    -82.1807
LinearFeatureBaseline/ExplainedVariance       0.126428
PolicyExecTime                                0.105383
ProcessExecTime                               0.0113342
TotalEnvSteps                            583924
policy/Entropy                               -0.571726
policy/KL                                     0.00656254
policy/KLBefore                               0
policy/LossAfter                             -0.0152373
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.56455
policy/dLoss                                  0.0152373
---------------------------------------  ----------------
2022-04-23 14:22:37 | [train_policy] epoch #577 | Obtaining samples for iteration 577...
2022-04-23 14:22:38 | [train_policy] epoch #577 | Logging diagnostics...
2022-04-23 14:22:38 | [train_policy] epoch #577 | Optimizing policy...
2022-04-23 14:22:38 | [train_policy] epoch #577 | Computing loss before
2022-04-23 14:22:38 | [train_policy] epoch #577 | Computing KL before
2022-04-23 14:22:38 | [train_policy] epoch #577 | Optimizing
2022-04-23 14:22:38 | [train_policy] epoch #577 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:38 | [train_policy] epoch #577 | computing loss before
2022-04-23 14:22:38 | [train_policy] epoch #577 | computing gradient
2022-04-23 14:22:38 | [train_policy] epoch #577 | gradient computed
2022-04-23 14:22:38 | [train_policy] epoch #577 | computing descent direction
2022-04-23 14:22:38 | [train_policy] epoch #577 | descent direction computed
2022-04-23 14:22:38 | [train_policy] epoch #577 | backtrack iters: 1
2022-04-23 14:22:38 | [train_policy] epoch #577 | optimization finished
2022-04-23 14:22:38 | [train_policy] epoch #577 | Computing KL after
2022-04-23 14:22:38 | [train_policy] epoch #577 | Computing loss after
2022-04-23 14:22:38 | [train_policy] epoch #577 | Fitting baseline...
2022-04-23 14:22:38 | [train_policy] epoch #577 | Saving snapshot...
2022-04-23 14:22:38 | [train_policy] epoch #577 | Saved
2022-04-23 14:22:38 | [train_policy] epoch #577 | Time 205.94 s
2022-04-23 14:22:38 | [train_policy] epoch #577 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116103
Evaluation/AverageDiscountedReturn          -64.1808
Evaluation/AverageReturn                    -64.1808
Evaluation/CompletionRate                     0
Evaluation/Iteration                        577
Evaluation/MaxReturn                        -29.906
Evaluation/MinReturn                      -2076.6
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        211.099
Extras/EpisodeRewardMean                    -62.4206
LinearFeatureBaseline/ExplainedVariance       0.0323856
PolicyExecTime                                0.0986993
ProcessExecTime                               0.0115173
TotalEnvSteps                            584936
policy/Entropy                               -0.578257
policy/KL                                     0.00647188
policy/KLBefore                               0
policy/LossAfter                             -0.019947
policy/LossBefore                            -7.18554e-09
policy/Perplexity                             0.560875
policy/dLoss                                  0.019947
---------------------------------------  ----------------
2022-04-23 14:22:38 | [train_policy] epoch #578 | Obtaining samples for iteration 578...
2022-04-23 14:22:38 | [train_policy] epoch #578 | Logging diagnostics...
2022-04-23 14:22:38 | [train_policy] epoch #578 | Optimizing policy...
2022-04-23 14:22:38 | [train_policy] epoch #578 | Computing loss before
2022-04-23 14:22:38 | [train_policy] epoch #578 | Computing KL before
2022-04-23 14:22:38 | [train_policy] epoch #578 | Optimizing
2022-04-23 14:22:38 | [train_policy] epoch #578 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:38 | [train_policy] epoch #578 | computing loss before
2022-04-23 14:22:38 | [train_policy] epoch #578 | computing gradient
2022-04-23 14:22:38 | [train_policy] epoch #578 | gradient computed
2022-04-23 14:22:38 | [train_policy] epoch #578 | computing descent direction
2022-04-23 14:22:38 | [train_policy] epoch #578 | descent direction computed
2022-04-23 14:22:38 | [train_policy] epoch #578 | backtrack iters: 0
2022-04-23 14:22:38 | [train_policy] epoch #578 | optimization finished
2022-04-23 14:22:38 | [train_policy] epoch #578 | Computing KL after
2022-04-23 14:22:38 | [train_policy] epoch #578 | Computing loss after
2022-04-23 14:22:38 | [train_policy] epoch #578 | Fitting baseline...
2022-04-23 14:22:38 | [train_policy] epoch #578 | Saving snapshot...
2022-04-23 14:22:38 | [train_policy] epoch #578 | Saved
2022-04-23 14:22:38 | [train_policy] epoch #578 | Time 206.30 s
2022-04-23 14:22:38 | [train_policy] epoch #578 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.122704
Evaluation/AverageDiscountedReturn          -41.6644
Evaluation/AverageReturn                    -41.6644
Evaluation/CompletionRate                     0
Evaluation/Iteration                        578
Evaluation/MaxReturn                        -30.5614
Evaluation/MinReturn                        -74.6805
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.92407
Extras/EpisodeRewardMean                    -41.7269
LinearFeatureBaseline/ExplainedVariance     -20.6735
PolicyExecTime                                0.106917
ProcessExecTime                               0.0123219
TotalEnvSteps                            585948
policy/Entropy                               -0.585213
policy/KL                                     0.0099471
policy/KLBefore                               0
policy/LossAfter                             -0.0325115
policy/LossBefore                            -6.47877e-09
policy/Perplexity                             0.556987
policy/dLoss                                  0.0325115
---------------------------------------  ----------------
2022-04-23 14:22:38 | [train_policy] epoch #579 | Obtaining samples for iteration 579...
2022-04-23 14:22:38 | [train_policy] epoch #579 | Logging diagnostics...
2022-04-23 14:22:38 | [train_policy] epoch #579 | Optimizing policy...
2022-04-23 14:22:38 | [train_policy] epoch #579 | Computing loss before
2022-04-23 14:22:38 | [train_policy] epoch #579 | Computing KL before
2022-04-23 14:22:38 | [train_policy] epoch #579 | Optimizing
2022-04-23 14:22:38 | [train_policy] epoch #579 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:38 | [train_policy] epoch #579 | computing loss before
2022-04-23 14:22:38 | [train_policy] epoch #579 | computing gradient
2022-04-23 14:22:38 | [train_policy] epoch #579 | gradient computed
2022-04-23 14:22:38 | [train_policy] epoch #579 | computing descent direction
2022-04-23 14:22:38 | [train_policy] epoch #579 | descent direction computed
2022-04-23 14:22:38 | [train_policy] epoch #579 | backtrack iters: 0
2022-04-23 14:22:38 | [train_policy] epoch #579 | optimization finished
2022-04-23 14:22:38 | [train_policy] epoch #579 | Computing KL after
2022-04-23 14:22:38 | [train_policy] epoch #579 | Computing loss after
2022-04-23 14:22:38 | [train_policy] epoch #579 | Fitting baseline...
2022-04-23 14:22:38 | [train_policy] epoch #579 | Saving snapshot...
2022-04-23 14:22:38 | [train_policy] epoch #579 | Saved
2022-04-23 14:22:38 | [train_policy] epoch #579 | Time 206.64 s
2022-04-23 14:22:38 | [train_policy] epoch #579 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118468
Evaluation/AverageDiscountedReturn          -41.8945
Evaluation/AverageReturn                    -41.8945
Evaluation/CompletionRate                     0
Evaluation/Iteration                        579
Evaluation/MaxReturn                        -30.0528
Evaluation/MinReturn                        -69.5328
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.74163
Extras/EpisodeRewardMean                    -42.0232
LinearFeatureBaseline/ExplainedVariance       0.89396
PolicyExecTime                                0.101944
ProcessExecTime                               0.0115876
TotalEnvSteps                            586960
policy/Entropy                               -0.594218
policy/KL                                     0.00875673
policy/KLBefore                               0
policy/LossAfter                             -0.0162945
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.551994
policy/dLoss                                  0.0162945
---------------------------------------  ----------------
2022-04-23 14:22:38 | [train_policy] epoch #580 | Obtaining samples for iteration 580...
2022-04-23 14:22:39 | [train_policy] epoch #580 | Logging diagnostics...
2022-04-23 14:22:39 | [train_policy] epoch #580 | Optimizing policy...
2022-04-23 14:22:39 | [train_policy] epoch #580 | Computing loss before
2022-04-23 14:22:39 | [train_policy] epoch #580 | Computing KL before
2022-04-23 14:22:39 | [train_policy] epoch #580 | Optimizing
2022-04-23 14:22:39 | [train_policy] epoch #580 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:39 | [train_policy] epoch #580 | computing loss before
2022-04-23 14:22:39 | [train_policy] epoch #580 | computing gradient
2022-04-23 14:22:39 | [train_policy] epoch #580 | gradient computed
2022-04-23 14:22:39 | [train_policy] epoch #580 | computing descent direction
2022-04-23 14:22:39 | [train_policy] epoch #580 | descent direction computed
2022-04-23 14:22:39 | [train_policy] epoch #580 | backtrack iters: 1
2022-04-23 14:22:39 | [train_policy] epoch #580 | optimization finished
2022-04-23 14:22:39 | [train_policy] epoch #580 | Computing KL after
2022-04-23 14:22:39 | [train_policy] epoch #580 | Computing loss after
2022-04-23 14:22:39 | [train_policy] epoch #580 | Fitting baseline...
2022-04-23 14:22:39 | [train_policy] epoch #580 | Saving snapshot...
2022-04-23 14:22:39 | [train_policy] epoch #580 | Saved
2022-04-23 14:22:39 | [train_policy] epoch #580 | Time 206.97 s
2022-04-23 14:22:39 | [train_policy] epoch #580 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118347
Evaluation/AverageDiscountedReturn          -64.9011
Evaluation/AverageReturn                    -64.9011
Evaluation/CompletionRate                     0
Evaluation/Iteration                        580
Evaluation/MaxReturn                        -30.2868
Evaluation/MinReturn                      -2064.74
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.818
Extras/EpisodeRewardMean                    -62.7097
LinearFeatureBaseline/ExplainedVariance       0.0124656
PolicyExecTime                                0.10151
ProcessExecTime                               0.0111227
TotalEnvSteps                            587972
policy/Entropy                               -0.602069
policy/KL                                     0.00740027
policy/KLBefore                               0
policy/LossAfter                             -0.0252804
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.547677
policy/dLoss                                  0.0252804
---------------------------------------  ----------------
2022-04-23 14:22:39 | [train_policy] epoch #581 | Obtaining samples for iteration 581...
2022-04-23 14:22:39 | [train_policy] epoch #581 | Logging diagnostics...
2022-04-23 14:22:39 | [train_policy] epoch #581 | Optimizing policy...
2022-04-23 14:22:39 | [train_policy] epoch #581 | Computing loss before
2022-04-23 14:22:39 | [train_policy] epoch #581 | Computing KL before
2022-04-23 14:22:39 | [train_policy] epoch #581 | Optimizing
2022-04-23 14:22:39 | [train_policy] epoch #581 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:39 | [train_policy] epoch #581 | computing loss before
2022-04-23 14:22:39 | [train_policy] epoch #581 | computing gradient
2022-04-23 14:22:39 | [train_policy] epoch #581 | gradient computed
2022-04-23 14:22:39 | [train_policy] epoch #581 | computing descent direction
2022-04-23 14:22:39 | [train_policy] epoch #581 | descent direction computed
2022-04-23 14:22:39 | [train_policy] epoch #581 | backtrack iters: 1
2022-04-23 14:22:39 | [train_policy] epoch #581 | optimization finished
2022-04-23 14:22:39 | [train_policy] epoch #581 | Computing KL after
2022-04-23 14:22:39 | [train_policy] epoch #581 | Computing loss after
2022-04-23 14:22:39 | [train_policy] epoch #581 | Fitting baseline...
2022-04-23 14:22:39 | [train_policy] epoch #581 | Saving snapshot...
2022-04-23 14:22:39 | [train_policy] epoch #581 | Saved
2022-04-23 14:22:39 | [train_policy] epoch #581 | Time 207.32 s
2022-04-23 14:22:39 | [train_policy] epoch #581 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117455
Evaluation/AverageDiscountedReturn          -42.8733
Evaluation/AverageReturn                    -42.8733
Evaluation/CompletionRate                     0
Evaluation/Iteration                        581
Evaluation/MaxReturn                        -31.2426
Evaluation/MinReturn                        -73.375
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.70567
Extras/EpisodeRewardMean                    -42.5657
LinearFeatureBaseline/ExplainedVariance     -14.4784
PolicyExecTime                                0.104845
ProcessExecTime                               0.011193
TotalEnvSteps                            588984
policy/Entropy                               -0.670391
policy/KL                                     0.00679331
policy/KLBefore                               0
policy/LossAfter                             -0.017451
policy/LossBefore                             1.22508e-08
policy/Perplexity                             0.511508
policy/dLoss                                  0.0174511
---------------------------------------  ----------------
2022-04-23 14:22:39 | [train_policy] epoch #582 | Obtaining samples for iteration 582...
2022-04-23 14:22:39 | [train_policy] epoch #582 | Logging diagnostics...
2022-04-23 14:22:39 | [train_policy] epoch #582 | Optimizing policy...
2022-04-23 14:22:39 | [train_policy] epoch #582 | Computing loss before
2022-04-23 14:22:39 | [train_policy] epoch #582 | Computing KL before
2022-04-23 14:22:39 | [train_policy] epoch #582 | Optimizing
2022-04-23 14:22:39 | [train_policy] epoch #582 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:39 | [train_policy] epoch #582 | computing loss before
2022-04-23 14:22:39 | [train_policy] epoch #582 | computing gradient
2022-04-23 14:22:39 | [train_policy] epoch #582 | gradient computed
2022-04-23 14:22:39 | [train_policy] epoch #582 | computing descent direction
2022-04-23 14:22:39 | [train_policy] epoch #582 | descent direction computed
2022-04-23 14:22:39 | [train_policy] epoch #582 | backtrack iters: 0
2022-04-23 14:22:39 | [train_policy] epoch #582 | optimization finished
2022-04-23 14:22:39 | [train_policy] epoch #582 | Computing KL after
2022-04-23 14:22:39 | [train_policy] epoch #582 | Computing loss after
2022-04-23 14:22:39 | [train_policy] epoch #582 | Fitting baseline...
2022-04-23 14:22:39 | [train_policy] epoch #582 | Saving snapshot...
2022-04-23 14:22:39 | [train_policy] epoch #582 | Saved
2022-04-23 14:22:39 | [train_policy] epoch #582 | Time 207.67 s
2022-04-23 14:22:39 | [train_policy] epoch #582 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117176
Evaluation/AverageDiscountedReturn          -86.4746
Evaluation/AverageReturn                    -86.4746
Evaluation/CompletionRate                     0
Evaluation/Iteration                        582
Evaluation/MaxReturn                        -33.2167
Evaluation/MinReturn                      -2067.22
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.301
Extras/EpisodeRewardMean                    -83.2104
LinearFeatureBaseline/ExplainedVariance       0.0110217
PolicyExecTime                                0.118202
ProcessExecTime                               0.011204
TotalEnvSteps                            589996
policy/Entropy                               -0.675984
policy/KL                                     0.00970572
policy/KLBefore                               0
policy/LossAfter                             -0.0238127
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.508656
policy/dLoss                                  0.0238127
---------------------------------------  ----------------
2022-04-23 14:22:39 | [train_policy] epoch #583 | Obtaining samples for iteration 583...
2022-04-23 14:22:40 | [train_policy] epoch #583 | Logging diagnostics...
2022-04-23 14:22:40 | [train_policy] epoch #583 | Optimizing policy...
2022-04-23 14:22:40 | [train_policy] epoch #583 | Computing loss before
2022-04-23 14:22:40 | [train_policy] epoch #583 | Computing KL before
2022-04-23 14:22:40 | [train_policy] epoch #583 | Optimizing
2022-04-23 14:22:40 | [train_policy] epoch #583 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:40 | [train_policy] epoch #583 | computing loss before
2022-04-23 14:22:40 | [train_policy] epoch #583 | computing gradient
2022-04-23 14:22:40 | [train_policy] epoch #583 | gradient computed
2022-04-23 14:22:40 | [train_policy] epoch #583 | computing descent direction
2022-04-23 14:22:40 | [train_policy] epoch #583 | descent direction computed
2022-04-23 14:22:40 | [train_policy] epoch #583 | backtrack iters: 0
2022-04-23 14:22:40 | [train_policy] epoch #583 | optimization finished
2022-04-23 14:22:40 | [train_policy] epoch #583 | Computing KL after
2022-04-23 14:22:40 | [train_policy] epoch #583 | Computing loss after
2022-04-23 14:22:40 | [train_policy] epoch #583 | Fitting baseline...
2022-04-23 14:22:40 | [train_policy] epoch #583 | Saving snapshot...
2022-04-23 14:22:40 | [train_policy] epoch #583 | Saved
2022-04-23 14:22:40 | [train_policy] epoch #583 | Time 208.03 s
2022-04-23 14:22:40 | [train_policy] epoch #583 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119736
Evaluation/AverageDiscountedReturn          -63.5518
Evaluation/AverageReturn                    -63.5518
Evaluation/CompletionRate                     0
Evaluation/Iteration                        583
Evaluation/MaxReturn                        -31.7749
Evaluation/MinReturn                      -2067.98
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.226
Extras/EpisodeRewardMean                    -62.0688
LinearFeatureBaseline/ExplainedVariance       0.14718
PolicyExecTime                                0.10299
ProcessExecTime                               0.0116673
TotalEnvSteps                            591008
policy/Entropy                               -0.600942
policy/KL                                     0.00927478
policy/KLBefore                               0
policy/LossAfter                             -0.0230411
policy/LossBefore                            -1.34287e-08
policy/Perplexity                             0.548295
policy/dLoss                                  0.0230411
---------------------------------------  ----------------
2022-04-23 14:22:40 | [train_policy] epoch #584 | Obtaining samples for iteration 584...
2022-04-23 14:22:40 | [train_policy] epoch #584 | Logging diagnostics...
2022-04-23 14:22:40 | [train_policy] epoch #584 | Optimizing policy...
2022-04-23 14:22:40 | [train_policy] epoch #584 | Computing loss before
2022-04-23 14:22:40 | [train_policy] epoch #584 | Computing KL before
2022-04-23 14:22:40 | [train_policy] epoch #584 | Optimizing
2022-04-23 14:22:40 | [train_policy] epoch #584 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:40 | [train_policy] epoch #584 | computing loss before
2022-04-23 14:22:40 | [train_policy] epoch #584 | computing gradient
2022-04-23 14:22:40 | [train_policy] epoch #584 | gradient computed
2022-04-23 14:22:40 | [train_policy] epoch #584 | computing descent direction
2022-04-23 14:22:40 | [train_policy] epoch #584 | descent direction computed
2022-04-23 14:22:40 | [train_policy] epoch #584 | backtrack iters: 1
2022-04-23 14:22:40 | [train_policy] epoch #584 | optimization finished
2022-04-23 14:22:40 | [train_policy] epoch #584 | Computing KL after
2022-04-23 14:22:40 | [train_policy] epoch #584 | Computing loss after
2022-04-23 14:22:40 | [train_policy] epoch #584 | Fitting baseline...
2022-04-23 14:22:40 | [train_policy] epoch #584 | Saving snapshot...
2022-04-23 14:22:40 | [train_policy] epoch #584 | Saved
2022-04-23 14:22:40 | [train_policy] epoch #584 | Time 208.39 s
2022-04-23 14:22:40 | [train_policy] epoch #584 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.122918
Evaluation/AverageDiscountedReturn          -86.4003
Evaluation/AverageReturn                    -86.4003
Evaluation/CompletionRate                     0
Evaluation/Iteration                        584
Evaluation/MaxReturn                        -31.1652
Evaluation/MinReturn                      -2069.84
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        295.329
Extras/EpisodeRewardMean                    -82.6184
LinearFeatureBaseline/ExplainedVariance       0.198972
PolicyExecTime                                0.102467
ProcessExecTime                               0.0117686
TotalEnvSteps                            592020
policy/Entropy                               -0.598452
policy/KL                                     0.00639063
policy/KLBefore                               0
policy/LossAfter                             -0.0218555
policy/LossBefore                            -1.06016e-09
policy/Perplexity                             0.549662
policy/dLoss                                  0.0218555
---------------------------------------  ----------------
2022-04-23 14:22:40 | [train_policy] epoch #585 | Obtaining samples for iteration 585...
2022-04-23 14:22:40 | [train_policy] epoch #585 | Logging diagnostics...
2022-04-23 14:22:40 | [train_policy] epoch #585 | Optimizing policy...
2022-04-23 14:22:40 | [train_policy] epoch #585 | Computing loss before
2022-04-23 14:22:40 | [train_policy] epoch #585 | Computing KL before
2022-04-23 14:22:40 | [train_policy] epoch #585 | Optimizing
2022-04-23 14:22:40 | [train_policy] epoch #585 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:40 | [train_policy] epoch #585 | computing loss before
2022-04-23 14:22:40 | [train_policy] epoch #585 | computing gradient
2022-04-23 14:22:40 | [train_policy] epoch #585 | gradient computed
2022-04-23 14:22:40 | [train_policy] epoch #585 | computing descent direction
2022-04-23 14:22:41 | [train_policy] epoch #585 | descent direction computed
2022-04-23 14:22:41 | [train_policy] epoch #585 | backtrack iters: 0
2022-04-23 14:22:41 | [train_policy] epoch #585 | optimization finished
2022-04-23 14:22:41 | [train_policy] epoch #585 | Computing KL after
2022-04-23 14:22:41 | [train_policy] epoch #585 | Computing loss after
2022-04-23 14:22:41 | [train_policy] epoch #585 | Fitting baseline...
2022-04-23 14:22:41 | [train_policy] epoch #585 | Saving snapshot...
2022-04-23 14:22:41 | [train_policy] epoch #585 | Saved
2022-04-23 14:22:41 | [train_policy] epoch #585 | Time 208.74 s
2022-04-23 14:22:41 | [train_policy] epoch #585 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.121528
Evaluation/AverageDiscountedReturn         -106.376
Evaluation/AverageReturn                   -106.376
Evaluation/CompletionRate                     0
Evaluation/Iteration                        585
Evaluation/MaxReturn                        -31.1057
Evaluation/MinReturn                      -2066.38
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        359.505
Extras/EpisodeRewardMean                   -121.677
LinearFeatureBaseline/ExplainedVariance       0.271943
PolicyExecTime                                0.104869
ProcessExecTime                               0.012027
TotalEnvSteps                            593032
policy/Entropy                               -0.59502
policy/KL                                     0.00979274
policy/KLBefore                               0
policy/LossAfter                             -0.0274722
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.551551
policy/dLoss                                  0.0274722
---------------------------------------  ----------------
2022-04-23 14:22:41 | [train_policy] epoch #586 | Obtaining samples for iteration 586...
2022-04-23 14:22:41 | [train_policy] epoch #586 | Logging diagnostics...
2022-04-23 14:22:41 | [train_policy] epoch #586 | Optimizing policy...
2022-04-23 14:22:41 | [train_policy] epoch #586 | Computing loss before
2022-04-23 14:22:41 | [train_policy] epoch #586 | Computing KL before
2022-04-23 14:22:41 | [train_policy] epoch #586 | Optimizing
2022-04-23 14:22:41 | [train_policy] epoch #586 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:41 | [train_policy] epoch #586 | computing loss before
2022-04-23 14:22:41 | [train_policy] epoch #586 | computing gradient
2022-04-23 14:22:41 | [train_policy] epoch #586 | gradient computed
2022-04-23 14:22:41 | [train_policy] epoch #586 | computing descent direction
2022-04-23 14:22:41 | [train_policy] epoch #586 | descent direction computed
2022-04-23 14:22:41 | [train_policy] epoch #586 | backtrack iters: 1
2022-04-23 14:22:41 | [train_policy] epoch #586 | optimization finished
2022-04-23 14:22:41 | [train_policy] epoch #586 | Computing KL after
2022-04-23 14:22:41 | [train_policy] epoch #586 | Computing loss after
2022-04-23 14:22:41 | [train_policy] epoch #586 | Fitting baseline...
2022-04-23 14:22:41 | [train_policy] epoch #586 | Saving snapshot...
2022-04-23 14:22:41 | [train_policy] epoch #586 | Saved
2022-04-23 14:22:41 | [train_policy] epoch #586 | Time 209.09 s
2022-04-23 14:22:41 | [train_policy] epoch #586 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118875
Evaluation/AverageDiscountedReturn          -63.773
Evaluation/AverageReturn                    -63.773
Evaluation/CompletionRate                     0
Evaluation/Iteration                        586
Evaluation/MaxReturn                        -30.9951
Evaluation/MinReturn                      -2065.55
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.995
Extras/EpisodeRewardMean                    -61.8654
LinearFeatureBaseline/ExplainedVariance      -0.229618
PolicyExecTime                                0.109251
ProcessExecTime                               0.0114067
TotalEnvSteps                            594044
policy/Entropy                               -0.597044
policy/KL                                     0.00665905
policy/KLBefore                               0
policy/LossAfter                             -0.00950474
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.550436
policy/dLoss                                  0.00950475
---------------------------------------  ----------------
2022-04-23 14:22:41 | [train_policy] epoch #587 | Obtaining samples for iteration 587...
2022-04-23 14:22:41 | [train_policy] epoch #587 | Logging diagnostics...
2022-04-23 14:22:41 | [train_policy] epoch #587 | Optimizing policy...
2022-04-23 14:22:41 | [train_policy] epoch #587 | Computing loss before
2022-04-23 14:22:41 | [train_policy] epoch #587 | Computing KL before
2022-04-23 14:22:41 | [train_policy] epoch #587 | Optimizing
2022-04-23 14:22:41 | [train_policy] epoch #587 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:41 | [train_policy] epoch #587 | computing loss before
2022-04-23 14:22:41 | [train_policy] epoch #587 | computing gradient
2022-04-23 14:22:41 | [train_policy] epoch #587 | gradient computed
2022-04-23 14:22:41 | [train_policy] epoch #587 | computing descent direction
2022-04-23 14:22:41 | [train_policy] epoch #587 | descent direction computed
2022-04-23 14:22:41 | [train_policy] epoch #587 | backtrack iters: 1
2022-04-23 14:22:41 | [train_policy] epoch #587 | optimization finished
2022-04-23 14:22:41 | [train_policy] epoch #587 | Computing KL after
2022-04-23 14:22:41 | [train_policy] epoch #587 | Computing loss after
2022-04-23 14:22:41 | [train_policy] epoch #587 | Fitting baseline...
2022-04-23 14:22:41 | [train_policy] epoch #587 | Saving snapshot...
2022-04-23 14:22:41 | [train_policy] epoch #587 | Saved
2022-04-23 14:22:41 | [train_policy] epoch #587 | Time 209.44 s
2022-04-23 14:22:41 | [train_policy] epoch #587 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117283
Evaluation/AverageDiscountedReturn          -43.0199
Evaluation/AverageReturn                    -43.0199
Evaluation/CompletionRate                     0
Evaluation/Iteration                        587
Evaluation/MaxReturn                        -31.0212
Evaluation/MinReturn                        -64.6986
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.76641
Extras/EpisodeRewardMean                    -43.0089
LinearFeatureBaseline/ExplainedVariance     -33.5872
PolicyExecTime                                0.108521
ProcessExecTime                               0.0116191
TotalEnvSteps                            595056
policy/Entropy                               -0.660048
policy/KL                                     0.00688756
policy/KLBefore                               0
policy/LossAfter                             -0.0189225
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.516827
policy/dLoss                                  0.0189225
---------------------------------------  ----------------
2022-04-23 14:22:41 | [train_policy] epoch #588 | Obtaining samples for iteration 588...
2022-04-23 14:22:41 | [train_policy] epoch #588 | Logging diagnostics...
2022-04-23 14:22:41 | [train_policy] epoch #588 | Optimizing policy...
2022-04-23 14:22:41 | [train_policy] epoch #588 | Computing loss before
2022-04-23 14:22:41 | [train_policy] epoch #588 | Computing KL before
2022-04-23 14:22:41 | [train_policy] epoch #588 | Optimizing
2022-04-23 14:22:42 | [train_policy] epoch #588 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:42 | [train_policy] epoch #588 | computing loss before
2022-04-23 14:22:42 | [train_policy] epoch #588 | computing gradient
2022-04-23 14:22:42 | [train_policy] epoch #588 | gradient computed
2022-04-23 14:22:42 | [train_policy] epoch #588 | computing descent direction
2022-04-23 14:22:42 | [train_policy] epoch #588 | descent direction computed
2022-04-23 14:22:42 | [train_policy] epoch #588 | backtrack iters: 1
2022-04-23 14:22:42 | [train_policy] epoch #588 | optimization finished
2022-04-23 14:22:42 | [train_policy] epoch #588 | Computing KL after
2022-04-23 14:22:42 | [train_policy] epoch #588 | Computing loss after
2022-04-23 14:22:42 | [train_policy] epoch #588 | Fitting baseline...
2022-04-23 14:22:42 | [train_policy] epoch #588 | Saving snapshot...
2022-04-23 14:22:42 | [train_policy] epoch #588 | Saved
2022-04-23 14:22:42 | [train_policy] epoch #588 | Time 209.79 s
2022-04-23 14:22:42 | [train_policy] epoch #588 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.11639
Evaluation/AverageDiscountedReturn          -43.2006
Evaluation/AverageReturn                    -43.2006
Evaluation/CompletionRate                     0
Evaluation/Iteration                        588
Evaluation/MaxReturn                        -32.9177
Evaluation/MinReturn                        -63.9207
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.88471
Extras/EpisodeRewardMean                    -42.988
LinearFeatureBaseline/ExplainedVariance       0.860236
PolicyExecTime                                0.111685
ProcessExecTime                               0.0112829
TotalEnvSteps                            596068
policy/Entropy                               -0.702985
policy/KL                                     0.00660391
policy/KLBefore                               0
policy/LossAfter                             -0.0197049
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.495105
policy/dLoss                                  0.0197049
---------------------------------------  ----------------
2022-04-23 14:22:42 | [train_policy] epoch #589 | Obtaining samples for iteration 589...
2022-04-23 14:22:42 | [train_policy] epoch #589 | Logging diagnostics...
2022-04-23 14:22:42 | [train_policy] epoch #589 | Optimizing policy...
2022-04-23 14:22:42 | [train_policy] epoch #589 | Computing loss before
2022-04-23 14:22:42 | [train_policy] epoch #589 | Computing KL before
2022-04-23 14:22:42 | [train_policy] epoch #589 | Optimizing
2022-04-23 14:22:42 | [train_policy] epoch #589 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:42 | [train_policy] epoch #589 | computing loss before
2022-04-23 14:22:42 | [train_policy] epoch #589 | computing gradient
2022-04-23 14:22:42 | [train_policy] epoch #589 | gradient computed
2022-04-23 14:22:42 | [train_policy] epoch #589 | computing descent direction
2022-04-23 14:22:42 | [train_policy] epoch #589 | descent direction computed
2022-04-23 14:22:42 | [train_policy] epoch #589 | backtrack iters: 1
2022-04-23 14:22:42 | [train_policy] epoch #589 | optimization finished
2022-04-23 14:22:42 | [train_policy] epoch #589 | Computing KL after
2022-04-23 14:22:42 | [train_policy] epoch #589 | Computing loss after
2022-04-23 14:22:42 | [train_policy] epoch #589 | Fitting baseline...
2022-04-23 14:22:42 | [train_policy] epoch #589 | Saving snapshot...
2022-04-23 14:22:42 | [train_policy] epoch #589 | Saved
2022-04-23 14:22:42 | [train_policy] epoch #589 | Time 210.13 s
2022-04-23 14:22:42 | [train_policy] epoch #589 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116105
Evaluation/AverageDiscountedReturn          -42.2991
Evaluation/AverageReturn                    -42.2991
Evaluation/CompletionRate                     0
Evaluation/Iteration                        589
Evaluation/MaxReturn                        -31.7903
Evaluation/MinReturn                        -67.6702
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.16364
Extras/EpisodeRewardMean                    -42.2659
LinearFeatureBaseline/ExplainedVariance       0.897429
PolicyExecTime                                0.101954
ProcessExecTime                               0.0109668
TotalEnvSteps                            597080
policy/Entropy                               -0.71256
policy/KL                                     0.0067091
policy/KLBefore                               0
policy/LossAfter                             -0.0143904
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.490387
policy/dLoss                                  0.0143904
---------------------------------------  ----------------
2022-04-23 14:22:42 | [train_policy] epoch #590 | Obtaining samples for iteration 590...
2022-04-23 14:22:42 | [train_policy] epoch #590 | Logging diagnostics...
2022-04-23 14:22:42 | [train_policy] epoch #590 | Optimizing policy...
2022-04-23 14:22:42 | [train_policy] epoch #590 | Computing loss before
2022-04-23 14:22:42 | [train_policy] epoch #590 | Computing KL before
2022-04-23 14:22:42 | [train_policy] epoch #590 | Optimizing
2022-04-23 14:22:42 | [train_policy] epoch #590 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:42 | [train_policy] epoch #590 | computing loss before
2022-04-23 14:22:42 | [train_policy] epoch #590 | computing gradient
2022-04-23 14:22:42 | [train_policy] epoch #590 | gradient computed
2022-04-23 14:22:42 | [train_policy] epoch #590 | computing descent direction
2022-04-23 14:22:42 | [train_policy] epoch #590 | descent direction computed
2022-04-23 14:22:42 | [train_policy] epoch #590 | backtrack iters: 2
2022-04-23 14:22:42 | [train_policy] epoch #590 | optimization finished
2022-04-23 14:22:42 | [train_policy] epoch #590 | Computing KL after
2022-04-23 14:22:42 | [train_policy] epoch #590 | Computing loss after
2022-04-23 14:22:42 | [train_policy] epoch #590 | Fitting baseline...
2022-04-23 14:22:42 | [train_policy] epoch #590 | Saving snapshot...
2022-04-23 14:22:42 | [train_policy] epoch #590 | Saved
2022-04-23 14:22:42 | [train_policy] epoch #590 | Time 210.48 s
2022-04-23 14:22:42 | [train_policy] epoch #590 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117186
Evaluation/AverageDiscountedReturn          -63.2951
Evaluation/AverageReturn                    -63.2951
Evaluation/CompletionRate                     0
Evaluation/Iteration                        590
Evaluation/MaxReturn                        -32.0353
Evaluation/MinReturn                      -2063.41
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.846
Extras/EpisodeRewardMean                    -62.0795
LinearFeatureBaseline/ExplainedVariance       0.0134912
PolicyExecTime                                0.0958557
ProcessExecTime                               0.0109804
TotalEnvSteps                            598092
policy/Entropy                               -0.714085
policy/KL                                     0.00570925
policy/KLBefore                               0
policy/LossAfter                             -0.0186875
policy/LossBefore                             1.22508e-08
policy/Perplexity                             0.48964
policy/dLoss                                  0.0186876
---------------------------------------  ----------------
2022-04-23 14:22:42 | [train_policy] epoch #591 | Obtaining samples for iteration 591...
2022-04-23 14:22:43 | [train_policy] epoch #591 | Logging diagnostics...
2022-04-23 14:22:43 | [train_policy] epoch #591 | Optimizing policy...
2022-04-23 14:22:43 | [train_policy] epoch #591 | Computing loss before
2022-04-23 14:22:43 | [train_policy] epoch #591 | Computing KL before
2022-04-23 14:22:43 | [train_policy] epoch #591 | Optimizing
2022-04-23 14:22:43 | [train_policy] epoch #591 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:43 | [train_policy] epoch #591 | computing loss before
2022-04-23 14:22:43 | [train_policy] epoch #591 | computing gradient
2022-04-23 14:22:43 | [train_policy] epoch #591 | gradient computed
2022-04-23 14:22:43 | [train_policy] epoch #591 | computing descent direction
2022-04-23 14:22:43 | [train_policy] epoch #591 | descent direction computed
2022-04-23 14:22:43 | [train_policy] epoch #591 | backtrack iters: 0
2022-04-23 14:22:43 | [train_policy] epoch #591 | optimization finished
2022-04-23 14:22:43 | [train_policy] epoch #591 | Computing KL after
2022-04-23 14:22:43 | [train_policy] epoch #591 | Computing loss after
2022-04-23 14:22:43 | [train_policy] epoch #591 | Fitting baseline...
2022-04-23 14:22:43 | [train_policy] epoch #591 | Saving snapshot...
2022-04-23 14:22:43 | [train_policy] epoch #591 | Saved
2022-04-23 14:22:43 | [train_policy] epoch #591 | Time 210.82 s
2022-04-23 14:22:43 | [train_policy] epoch #591 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.115702
Evaluation/AverageDiscountedReturn          -64.0122
Evaluation/AverageReturn                    -64.0122
Evaluation/CompletionRate                     0
Evaluation/Iteration                        591
Evaluation/MaxReturn                        -30.5565
Evaluation/MinReturn                      -2065.85
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        210.004
Extras/EpisodeRewardMean                    -62.2124
LinearFeatureBaseline/ExplainedVariance       0.0730113
PolicyExecTime                                0.0982192
ProcessExecTime                               0.0108621
TotalEnvSteps                            599104
policy/Entropy                               -0.675343
policy/KL                                     0.00992648
policy/KLBefore                               0
policy/LossAfter                             -0.0214694
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             0.508982
policy/dLoss                                  0.0214694
---------------------------------------  ----------------
2022-04-23 14:22:43 | [train_policy] epoch #592 | Obtaining samples for iteration 592...
2022-04-23 14:22:43 | [train_policy] epoch #592 | Logging diagnostics...
2022-04-23 14:22:43 | [train_policy] epoch #592 | Optimizing policy...
2022-04-23 14:22:43 | [train_policy] epoch #592 | Computing loss before
2022-04-23 14:22:43 | [train_policy] epoch #592 | Computing KL before
2022-04-23 14:22:43 | [train_policy] epoch #592 | Optimizing
2022-04-23 14:22:43 | [train_policy] epoch #592 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:43 | [train_policy] epoch #592 | computing loss before
2022-04-23 14:22:43 | [train_policy] epoch #592 | computing gradient
2022-04-23 14:22:43 | [train_policy] epoch #592 | gradient computed
2022-04-23 14:22:43 | [train_policy] epoch #592 | computing descent direction
2022-04-23 14:22:43 | [train_policy] epoch #592 | descent direction computed
2022-04-23 14:22:43 | [train_policy] epoch #592 | backtrack iters: 0
2022-04-23 14:22:43 | [train_policy] epoch #592 | optimization finished
2022-04-23 14:22:43 | [train_policy] epoch #592 | Computing KL after
2022-04-23 14:22:43 | [train_policy] epoch #592 | Computing loss after
2022-04-23 14:22:43 | [train_policy] epoch #592 | Fitting baseline...
2022-04-23 14:22:43 | [train_policy] epoch #592 | Saving snapshot...
2022-04-23 14:22:43 | [train_policy] epoch #592 | Saved
2022-04-23 14:22:43 | [train_policy] epoch #592 | Time 211.16 s
2022-04-23 14:22:43 | [train_policy] epoch #592 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.115471
Evaluation/AverageDiscountedReturn          -85.5302
Evaluation/AverageReturn                    -85.5302
Evaluation/CompletionRate                     0
Evaluation/Iteration                        592
Evaluation/MaxReturn                        -31.0514
Evaluation/MinReturn                      -2063.61
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        294.934
Extras/EpisodeRewardMean                    -82.0258
LinearFeatureBaseline/ExplainedVariance       0.170156
PolicyExecTime                                0.100319
ProcessExecTime                               0.0108626
TotalEnvSteps                            600116
policy/Entropy                               -0.661327
policy/KL                                     0.00976791
policy/KLBefore                               0
policy/LossAfter                             -0.0292173
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             0.516166
policy/dLoss                                  0.0292173
---------------------------------------  ---------------
2022-04-23 14:22:43 | [train_policy] epoch #593 | Obtaining samples for iteration 593...
2022-04-23 14:22:43 | [train_policy] epoch #593 | Logging diagnostics...
2022-04-23 14:22:43 | [train_policy] epoch #593 | Optimizing policy...
2022-04-23 14:22:43 | [train_policy] epoch #593 | Computing loss before
2022-04-23 14:22:43 | [train_policy] epoch #593 | Computing KL before
2022-04-23 14:22:43 | [train_policy] epoch #593 | Optimizing
2022-04-23 14:22:43 | [train_policy] epoch #593 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:43 | [train_policy] epoch #593 | computing loss before
2022-04-23 14:22:43 | [train_policy] epoch #593 | computing gradient
2022-04-23 14:22:43 | [train_policy] epoch #593 | gradient computed
2022-04-23 14:22:43 | [train_policy] epoch #593 | computing descent direction
2022-04-23 14:22:43 | [train_policy] epoch #593 | descent direction computed
2022-04-23 14:22:43 | [train_policy] epoch #593 | backtrack iters: 1
2022-04-23 14:22:43 | [train_policy] epoch #593 | optimization finished
2022-04-23 14:22:43 | [train_policy] epoch #593 | Computing KL after
2022-04-23 14:22:43 | [train_policy] epoch #593 | Computing loss after
2022-04-23 14:22:43 | [train_policy] epoch #593 | Fitting baseline...
2022-04-23 14:22:43 | [train_policy] epoch #593 | Saving snapshot...
2022-04-23 14:22:43 | [train_policy] epoch #593 | Saved
2022-04-23 14:22:43 | [train_policy] epoch #593 | Time 211.50 s
2022-04-23 14:22:43 | [train_policy] epoch #593 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116306
Evaluation/AverageDiscountedReturn          -64.2109
Evaluation/AverageReturn                    -64.2109
Evaluation/CompletionRate                     0
Evaluation/Iteration                        593
Evaluation/MaxReturn                        -32.1853
Evaluation/MinReturn                      -2048.38
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.138
Extras/EpisodeRewardMean                    -62.0609
LinearFeatureBaseline/ExplainedVariance      -0.251573
PolicyExecTime                                0.0985105
ProcessExecTime                               0.010962
TotalEnvSteps                            601128
policy/Entropy                               -0.682347
policy/KL                                     0.00656686
policy/KLBefore                               0
policy/LossAfter                             -0.0162349
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.505429
policy/dLoss                                  0.0162349
---------------------------------------  ----------------
2022-04-23 14:22:43 | [train_policy] epoch #594 | Obtaining samples for iteration 594...
2022-04-23 14:22:44 | [train_policy] epoch #594 | Logging diagnostics...
2022-04-23 14:22:44 | [train_policy] epoch #594 | Optimizing policy...
2022-04-23 14:22:44 | [train_policy] epoch #594 | Computing loss before
2022-04-23 14:22:44 | [train_policy] epoch #594 | Computing KL before
2022-04-23 14:22:44 | [train_policy] epoch #594 | Optimizing
2022-04-23 14:22:44 | [train_policy] epoch #594 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:44 | [train_policy] epoch #594 | computing loss before
2022-04-23 14:22:44 | [train_policy] epoch #594 | computing gradient
2022-04-23 14:22:44 | [train_policy] epoch #594 | gradient computed
2022-04-23 14:22:44 | [train_policy] epoch #594 | computing descent direction
2022-04-23 14:22:44 | [train_policy] epoch #594 | descent direction computed
2022-04-23 14:22:44 | [train_policy] epoch #594 | backtrack iters: 1
2022-04-23 14:22:44 | [train_policy] epoch #594 | optimization finished
2022-04-23 14:22:44 | [train_policy] epoch #594 | Computing KL after
2022-04-23 14:22:44 | [train_policy] epoch #594 | Computing loss after
2022-04-23 14:22:44 | [train_policy] epoch #594 | Fitting baseline...
2022-04-23 14:22:44 | [train_policy] epoch #594 | Saving snapshot...
2022-04-23 14:22:44 | [train_policy] epoch #594 | Saved
2022-04-23 14:22:44 | [train_policy] epoch #594 | Time 211.88 s
2022-04-23 14:22:44 | [train_policy] epoch #594 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                   0.123231
Evaluation/AverageDiscountedReturn          -64.5819
Evaluation/AverageReturn                    -64.5819
Evaluation/CompletionRate                     0
Evaluation/Iteration                        594
Evaluation/MaxReturn                        -31.087
Evaluation/MinReturn                      -2062.81
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.59
Extras/EpisodeRewardMean                    -62.8917
LinearFeatureBaseline/ExplainedVariance       0.100511
PolicyExecTime                                0.107348
ProcessExecTime                               0.0122614
TotalEnvSteps                            602140
policy/Entropy                               -0.651111
policy/KL                                     0.00646349
policy/KLBefore                               0
policy/LossAfter                             -0.0161211
policy/LossBefore                             1.0366e-08
policy/Perplexity                             0.521466
policy/dLoss                                  0.0161211
---------------------------------------  ---------------
2022-04-23 14:22:44 | [train_policy] epoch #595 | Obtaining samples for iteration 595...
2022-04-23 14:22:44 | [train_policy] epoch #595 | Logging diagnostics...
2022-04-23 14:22:44 | [train_policy] epoch #595 | Optimizing policy...
2022-04-23 14:22:44 | [train_policy] epoch #595 | Computing loss before
2022-04-23 14:22:44 | [train_policy] epoch #595 | Computing KL before
2022-04-23 14:22:44 | [train_policy] epoch #595 | Optimizing
2022-04-23 14:22:44 | [train_policy] epoch #595 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:44 | [train_policy] epoch #595 | computing loss before
2022-04-23 14:22:44 | [train_policy] epoch #595 | computing gradient
2022-04-23 14:22:44 | [train_policy] epoch #595 | gradient computed
2022-04-23 14:22:44 | [train_policy] epoch #595 | computing descent direction
2022-04-23 14:22:44 | [train_policy] epoch #595 | descent direction computed
2022-04-23 14:22:44 | [train_policy] epoch #595 | backtrack iters: 0
2022-04-23 14:22:44 | [train_policy] epoch #595 | optimization finished
2022-04-23 14:22:44 | [train_policy] epoch #595 | Computing KL after
2022-04-23 14:22:44 | [train_policy] epoch #595 | Computing loss after
2022-04-23 14:22:44 | [train_policy] epoch #595 | Fitting baseline...
2022-04-23 14:22:44 | [train_policy] epoch #595 | Saving snapshot...
2022-04-23 14:22:44 | [train_policy] epoch #595 | Saved
2022-04-23 14:22:44 | [train_policy] epoch #595 | Time 212.23 s
2022-04-23 14:22:44 | [train_policy] epoch #595 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118185
Evaluation/AverageDiscountedReturn          -65.2604
Evaluation/AverageReturn                    -65.2604
Evaluation/CompletionRate                     0
Evaluation/Iteration                        595
Evaluation/MaxReturn                        -30.9426
Evaluation/MinReturn                      -2063.09
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.612
Extras/EpisodeRewardMean                    -63.3578
LinearFeatureBaseline/ExplainedVariance       0.070793
PolicyExecTime                                0.105831
ProcessExecTime                               0.011632
TotalEnvSteps                            603152
policy/Entropy                               -0.586597
policy/KL                                     0.0089074
policy/KLBefore                               0
policy/LossAfter                             -0.523062
policy/LossBefore                            -6.83215e-09
policy/Perplexity                             0.556217
policy/dLoss                                  0.523062
---------------------------------------  ----------------
2022-04-23 14:22:44 | [train_policy] epoch #596 | Obtaining samples for iteration 596...
2022-04-23 14:22:44 | [train_policy] epoch #596 | Logging diagnostics...
2022-04-23 14:22:44 | [train_policy] epoch #596 | Optimizing policy...
2022-04-23 14:22:44 | [train_policy] epoch #596 | Computing loss before
2022-04-23 14:22:44 | [train_policy] epoch #596 | Computing KL before
2022-04-23 14:22:44 | [train_policy] epoch #596 | Optimizing
2022-04-23 14:22:44 | [train_policy] epoch #596 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:44 | [train_policy] epoch #596 | computing loss before
2022-04-23 14:22:44 | [train_policy] epoch #596 | computing gradient
2022-04-23 14:22:44 | [train_policy] epoch #596 | gradient computed
2022-04-23 14:22:44 | [train_policy] epoch #596 | computing descent direction
2022-04-23 14:22:44 | [train_policy] epoch #596 | descent direction computed
2022-04-23 14:22:44 | [train_policy] epoch #596 | backtrack iters: 1
2022-04-23 14:22:44 | [train_policy] epoch #596 | optimization finished
2022-04-23 14:22:44 | [train_policy] epoch #596 | Computing KL after
2022-04-23 14:22:44 | [train_policy] epoch #596 | Computing loss after
2022-04-23 14:22:44 | [train_policy] epoch #596 | Fitting baseline...
2022-04-23 14:22:44 | [train_policy] epoch #596 | Saving snapshot...
2022-04-23 14:22:44 | [train_policy] epoch #596 | Saved
2022-04-23 14:22:44 | [train_policy] epoch #596 | Time 212.58 s
2022-04-23 14:22:44 | [train_policy] epoch #596 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116996
Evaluation/AverageDiscountedReturn          -40.889
Evaluation/AverageReturn                    -40.889
Evaluation/CompletionRate                     0
Evaluation/Iteration                        596
Evaluation/MaxReturn                        -31.8668
Evaluation/MinReturn                        -75.5314
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.58509
Extras/EpisodeRewardMean                    -41.1373
LinearFeatureBaseline/ExplainedVariance     -14.9137
PolicyExecTime                                0.106359
ProcessExecTime                               0.0113063
TotalEnvSteps                            604164
policy/Entropy                               -0.593269
policy/KL                                     0.00703765
policy/KLBefore                               0
policy/LossAfter                             -0.0228152
policy/LossBefore                             1.69626e-08
policy/Perplexity                             0.552518
policy/dLoss                                  0.0228152
---------------------------------------  ----------------
2022-04-23 14:22:44 | [train_policy] epoch #597 | Obtaining samples for iteration 597...
2022-04-23 14:22:45 | [train_policy] epoch #597 | Logging diagnostics...
2022-04-23 14:22:45 | [train_policy] epoch #597 | Optimizing policy...
2022-04-23 14:22:45 | [train_policy] epoch #597 | Computing loss before
2022-04-23 14:22:45 | [train_policy] epoch #597 | Computing KL before
2022-04-23 14:22:45 | [train_policy] epoch #597 | Optimizing
2022-04-23 14:22:45 | [train_policy] epoch #597 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:45 | [train_policy] epoch #597 | computing loss before
2022-04-23 14:22:45 | [train_policy] epoch #597 | computing gradient
2022-04-23 14:22:45 | [train_policy] epoch #597 | gradient computed
2022-04-23 14:22:45 | [train_policy] epoch #597 | computing descent direction
2022-04-23 14:22:45 | [train_policy] epoch #597 | descent direction computed
2022-04-23 14:22:45 | [train_policy] epoch #597 | backtrack iters: 0
2022-04-23 14:22:45 | [train_policy] epoch #597 | optimization finished
2022-04-23 14:22:45 | [train_policy] epoch #597 | Computing KL after
2022-04-23 14:22:45 | [train_policy] epoch #597 | Computing loss after
2022-04-23 14:22:45 | [train_policy] epoch #597 | Fitting baseline...
2022-04-23 14:22:45 | [train_policy] epoch #597 | Saving snapshot...
2022-04-23 14:22:45 | [train_policy] epoch #597 | Saved
2022-04-23 14:22:45 | [train_policy] epoch #597 | Time 212.95 s
2022-04-23 14:22:45 | [train_policy] epoch #597 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.118298
Evaluation/AverageDiscountedReturn          -65.4168
Evaluation/AverageReturn                    -65.4168
Evaluation/CompletionRate                     0
Evaluation/Iteration                        597
Evaluation/MaxReturn                        -33.2756
Evaluation/MinReturn                      -2063.32
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.614
Extras/EpisodeRewardMean                    -63.6731
LinearFeatureBaseline/ExplainedVariance       0.0109812
PolicyExecTime                                0.106145
ProcessExecTime                               0.0117462
TotalEnvSteps                            605176
policy/Entropy                               -0.606379
policy/KL                                     0.00942882
policy/KLBefore                               0
policy/LossAfter                             -0.0232005
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.545322
policy/dLoss                                  0.0232005
---------------------------------------  ----------------
2022-04-23 14:22:45 | [train_policy] epoch #598 | Obtaining samples for iteration 598...
2022-04-23 14:22:45 | [train_policy] epoch #598 | Logging diagnostics...
2022-04-23 14:22:45 | [train_policy] epoch #598 | Optimizing policy...
2022-04-23 14:22:45 | [train_policy] epoch #598 | Computing loss before
2022-04-23 14:22:45 | [train_policy] epoch #598 | Computing KL before
2022-04-23 14:22:45 | [train_policy] epoch #598 | Optimizing
2022-04-23 14:22:45 | [train_policy] epoch #598 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:45 | [train_policy] epoch #598 | computing loss before
2022-04-23 14:22:45 | [train_policy] epoch #598 | computing gradient
2022-04-23 14:22:45 | [train_policy] epoch #598 | gradient computed
2022-04-23 14:22:45 | [train_policy] epoch #598 | computing descent direction
2022-04-23 14:22:45 | [train_policy] epoch #598 | descent direction computed
2022-04-23 14:22:45 | [train_policy] epoch #598 | backtrack iters: 1
2022-04-23 14:22:45 | [train_policy] epoch #598 | optimization finished
2022-04-23 14:22:45 | [train_policy] epoch #598 | Computing KL after
2022-04-23 14:22:45 | [train_policy] epoch #598 | Computing loss after
2022-04-23 14:22:45 | [train_policy] epoch #598 | Fitting baseline...
2022-04-23 14:22:45 | [train_policy] epoch #598 | Saving snapshot...
2022-04-23 14:22:45 | [train_policy] epoch #598 | Saved
2022-04-23 14:22:45 | [train_policy] epoch #598 | Time 213.30 s
2022-04-23 14:22:45 | [train_policy] epoch #598 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118278
Evaluation/AverageDiscountedReturn          -42.154
Evaluation/AverageReturn                    -42.154
Evaluation/CompletionRate                     0
Evaluation/Iteration                        598
Evaluation/MaxReturn                        -32.0229
Evaluation/MinReturn                        -73.0283
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.71682
Extras/EpisodeRewardMean                    -41.9712
LinearFeatureBaseline/ExplainedVariance     -20.5628
PolicyExecTime                                0.103655
ProcessExecTime                               0.0114343
TotalEnvSteps                            606188
policy/Entropy                               -0.642431
policy/KL                                     0.00649008
policy/KLBefore                               0
policy/LossAfter                             -0.0279755
policy/LossBefore                            -4.42912e-08
policy/Perplexity                             0.526012
policy/dLoss                                  0.0279755
---------------------------------------  ----------------
2022-04-23 14:22:45 | [train_policy] epoch #599 | Obtaining samples for iteration 599...
2022-04-23 14:22:45 | [train_policy] epoch #599 | Logging diagnostics...
2022-04-23 14:22:45 | [train_policy] epoch #599 | Optimizing policy...
2022-04-23 14:22:45 | [train_policy] epoch #599 | Computing loss before
2022-04-23 14:22:45 | [train_policy] epoch #599 | Computing KL before
2022-04-23 14:22:45 | [train_policy] epoch #599 | Optimizing
2022-04-23 14:22:45 | [train_policy] epoch #599 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:45 | [train_policy] epoch #599 | computing loss before
2022-04-23 14:22:45 | [train_policy] epoch #599 | computing gradient
2022-04-23 14:22:45 | [train_policy] epoch #599 | gradient computed
2022-04-23 14:22:45 | [train_policy] epoch #599 | computing descent direction
2022-04-23 14:22:45 | [train_policy] epoch #599 | descent direction computed
2022-04-23 14:22:45 | [train_policy] epoch #599 | backtrack iters: 1
2022-04-23 14:22:45 | [train_policy] epoch #599 | optimization finished
2022-04-23 14:22:45 | [train_policy] epoch #599 | Computing KL after
2022-04-23 14:22:45 | [train_policy] epoch #599 | Computing loss after
2022-04-23 14:22:45 | [train_policy] epoch #599 | Fitting baseline...
2022-04-23 14:22:45 | [train_policy] epoch #599 | Saving snapshot...
2022-04-23 14:22:45 | [train_policy] epoch #599 | Saved
2022-04-23 14:22:45 | [train_policy] epoch #599 | Time 213.63 s
2022-04-23 14:22:45 | [train_policy] epoch #599 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.116569
Evaluation/AverageDiscountedReturn          -42.0024
Evaluation/AverageReturn                    -42.0024
Evaluation/CompletionRate                     0
Evaluation/Iteration                        599
Evaluation/MaxReturn                        -32.709
Evaluation/MinReturn                        -75.6659
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.6158
Extras/EpisodeRewardMean                    -42.3865
LinearFeatureBaseline/ExplainedVariance       0.849475
PolicyExecTime                                0.0939991
ProcessExecTime                               0.011085
TotalEnvSteps                            607200
policy/Entropy                               -0.649068
policy/KL                                     0.00642291
policy/KLBefore                               0
policy/LossAfter                             -0.0152612
policy/LossBefore                             5.5364e-09
policy/Perplexity                             0.522533
policy/dLoss                                  0.0152612
---------------------------------------  ---------------
2022-04-23 14:22:45 | [train_policy] epoch #600 | Obtaining samples for iteration 600...
2022-04-23 14:22:46 | [train_policy] epoch #600 | Logging diagnostics...
2022-04-23 14:22:46 | [train_policy] epoch #600 | Optimizing policy...
2022-04-23 14:22:46 | [train_policy] epoch #600 | Computing loss before
2022-04-23 14:22:46 | [train_policy] epoch #600 | Computing KL before
2022-04-23 14:22:46 | [train_policy] epoch #600 | Optimizing
2022-04-23 14:22:46 | [train_policy] epoch #600 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:46 | [train_policy] epoch #600 | computing loss before
2022-04-23 14:22:46 | [train_policy] epoch #600 | computing gradient
2022-04-23 14:22:46 | [train_policy] epoch #600 | gradient computed
2022-04-23 14:22:46 | [train_policy] epoch #600 | computing descent direction
2022-04-23 14:22:46 | [train_policy] epoch #600 | descent direction computed
2022-04-23 14:22:46 | [train_policy] epoch #600 | backtrack iters: 1
2022-04-23 14:22:46 | [train_policy] epoch #600 | optimization finished
2022-04-23 14:22:46 | [train_policy] epoch #600 | Computing KL after
2022-04-23 14:22:46 | [train_policy] epoch #600 | Computing loss after
2022-04-23 14:22:46 | [train_policy] epoch #600 | Fitting baseline...
2022-04-23 14:22:46 | [train_policy] epoch #600 | Saving snapshot...
2022-04-23 14:22:46 | [train_policy] epoch #600 | Saved
2022-04-23 14:22:46 | [train_policy] epoch #600 | Time 213.97 s
2022-04-23 14:22:46 | [train_policy] epoch #600 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116209
Evaluation/AverageDiscountedReturn          -42.957
Evaluation/AverageReturn                    -42.957
Evaluation/CompletionRate                     0
Evaluation/Iteration                        600
Evaluation/MaxReturn                        -30.3989
Evaluation/MinReturn                        -74.0736
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.19021
Extras/EpisodeRewardMean                    -42.9525
LinearFeatureBaseline/ExplainedVariance       0.896138
PolicyExecTime                                0.0954504
ProcessExecTime                               0.0110803
TotalEnvSteps                            608212
policy/Entropy                               -0.660222
policy/KL                                     0.00653805
policy/KLBefore                               0
policy/LossAfter                             -0.0211939
policy/LossBefore                             2.00253e-09
policy/Perplexity                             0.516736
policy/dLoss                                  0.0211939
---------------------------------------  ----------------
2022-04-23 14:22:46 | [train_policy] epoch #601 | Obtaining samples for iteration 601...
2022-04-23 14:22:46 | [train_policy] epoch #601 | Logging diagnostics...
2022-04-23 14:22:46 | [train_policy] epoch #601 | Optimizing policy...
2022-04-23 14:22:46 | [train_policy] epoch #601 | Computing loss before
2022-04-23 14:22:46 | [train_policy] epoch #601 | Computing KL before
2022-04-23 14:22:46 | [train_policy] epoch #601 | Optimizing
2022-04-23 14:22:46 | [train_policy] epoch #601 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:46 | [train_policy] epoch #601 | computing loss before
2022-04-23 14:22:46 | [train_policy] epoch #601 | computing gradient
2022-04-23 14:22:46 | [train_policy] epoch #601 | gradient computed
2022-04-23 14:22:46 | [train_policy] epoch #601 | computing descent direction
2022-04-23 14:22:46 | [train_policy] epoch #601 | descent direction computed
2022-04-23 14:22:46 | [train_policy] epoch #601 | backtrack iters: 1
2022-04-23 14:22:46 | [train_policy] epoch #601 | optimization finished
2022-04-23 14:22:46 | [train_policy] epoch #601 | Computing KL after
2022-04-23 14:22:46 | [train_policy] epoch #601 | Computing loss after
2022-04-23 14:22:46 | [train_policy] epoch #601 | Fitting baseline...
2022-04-23 14:22:46 | [train_policy] epoch #601 | Saving snapshot...
2022-04-23 14:22:46 | [train_policy] epoch #601 | Saved
2022-04-23 14:22:46 | [train_policy] epoch #601 | Time 214.33 s
2022-04-23 14:22:46 | [train_policy] epoch #601 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116559
Evaluation/AverageDiscountedReturn          -42.4182
Evaluation/AverageReturn                    -42.4182
Evaluation/CompletionRate                     0
Evaluation/Iteration                        601
Evaluation/MaxReturn                        -31.7178
Evaluation/MinReturn                        -74.684
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.85665
Extras/EpisodeRewardMean                    -42.6686
LinearFeatureBaseline/ExplainedVariance       0.88001
PolicyExecTime                                0.104002
ProcessExecTime                               0.0110967
TotalEnvSteps                            609224
policy/Entropy                               -0.68635
policy/KL                                     0.00645539
policy/KLBefore                               0
policy/LossAfter                             -0.0158696
policy/LossBefore                             9.77705e-09
policy/Perplexity                             0.50341
policy/dLoss                                  0.0158696
---------------------------------------  ----------------
2022-04-23 14:22:46 | [train_policy] epoch #602 | Obtaining samples for iteration 602...
2022-04-23 14:22:46 | [train_policy] epoch #602 | Logging diagnostics...
2022-04-23 14:22:46 | [train_policy] epoch #602 | Optimizing policy...
2022-04-23 14:22:46 | [train_policy] epoch #602 | Computing loss before
2022-04-23 14:22:46 | [train_policy] epoch #602 | Computing KL before
2022-04-23 14:22:46 | [train_policy] epoch #602 | Optimizing
2022-04-23 14:22:46 | [train_policy] epoch #602 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:46 | [train_policy] epoch #602 | computing loss before
2022-04-23 14:22:46 | [train_policy] epoch #602 | computing gradient
2022-04-23 14:22:46 | [train_policy] epoch #602 | gradient computed
2022-04-23 14:22:46 | [train_policy] epoch #602 | computing descent direction
2022-04-23 14:22:46 | [train_policy] epoch #602 | descent direction computed
2022-04-23 14:22:46 | [train_policy] epoch #602 | backtrack iters: 1
2022-04-23 14:22:46 | [train_policy] epoch #602 | optimization finished
2022-04-23 14:22:46 | [train_policy] epoch #602 | Computing KL after
2022-04-23 14:22:46 | [train_policy] epoch #602 | Computing loss after
2022-04-23 14:22:46 | [train_policy] epoch #602 | Fitting baseline...
2022-04-23 14:22:46 | [train_policy] epoch #602 | Saving snapshot...
2022-04-23 14:22:46 | [train_policy] epoch #602 | Saved
2022-04-23 14:22:46 | [train_policy] epoch #602 | Time 214.68 s
2022-04-23 14:22:46 | [train_policy] epoch #602 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.117788
Evaluation/AverageDiscountedReturn          -42.3887
Evaluation/AverageReturn                    -42.3887
Evaluation/CompletionRate                     0
Evaluation/Iteration                        602
Evaluation/MaxReturn                        -31.3231
Evaluation/MinReturn                        -63.8458
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.74444
Extras/EpisodeRewardMean                    -42.1995
LinearFeatureBaseline/ExplainedVariance       0.888068
PolicyExecTime                                0.10446
ProcessExecTime                               0.011533
TotalEnvSteps                            610236
policy/Entropy                               -0.73103
policy/KL                                     0.00659568
policy/KLBefore                               0
policy/LossAfter                             -0.0167584
policy/LossBefore                            -4.18175e-09
policy/Perplexity                             0.481413
policy/dLoss                                  0.0167584
---------------------------------------  ----------------
2022-04-23 14:22:46 | [train_policy] epoch #603 | Obtaining samples for iteration 603...
2022-04-23 14:22:47 | [train_policy] epoch #603 | Logging diagnostics...
2022-04-23 14:22:47 | [train_policy] epoch #603 | Optimizing policy...
2022-04-23 14:22:47 | [train_policy] epoch #603 | Computing loss before
2022-04-23 14:22:47 | [train_policy] epoch #603 | Computing KL before
2022-04-23 14:22:47 | [train_policy] epoch #603 | Optimizing
2022-04-23 14:22:47 | [train_policy] epoch #603 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:47 | [train_policy] epoch #603 | computing loss before
2022-04-23 14:22:47 | [train_policy] epoch #603 | computing gradient
2022-04-23 14:22:47 | [train_policy] epoch #603 | gradient computed
2022-04-23 14:22:47 | [train_policy] epoch #603 | computing descent direction
2022-04-23 14:22:47 | [train_policy] epoch #603 | descent direction computed
2022-04-23 14:22:47 | [train_policy] epoch #603 | backtrack iters: 1
2022-04-23 14:22:47 | [train_policy] epoch #603 | optimization finished
2022-04-23 14:22:47 | [train_policy] epoch #603 | Computing KL after
2022-04-23 14:22:47 | [train_policy] epoch #603 | Computing loss after
2022-04-23 14:22:47 | [train_policy] epoch #603 | Fitting baseline...
2022-04-23 14:22:47 | [train_policy] epoch #603 | Saving snapshot...
2022-04-23 14:22:47 | [train_policy] epoch #603 | Saved
2022-04-23 14:22:47 | [train_policy] epoch #603 | Time 215.03 s
2022-04-23 14:22:47 | [train_policy] epoch #603 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118239
Evaluation/AverageDiscountedReturn          -42.646
Evaluation/AverageReturn                    -42.646
Evaluation/CompletionRate                     0
Evaluation/Iteration                        603
Evaluation/MaxReturn                        -32.2503
Evaluation/MinReturn                        -76.3583
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.35862
Extras/EpisodeRewardMean                    -43.0621
LinearFeatureBaseline/ExplainedVariance       0.868934
PolicyExecTime                                0.103571
ProcessExecTime                               0.0116618
TotalEnvSteps                            611248
policy/Entropy                               -0.767506
policy/KL                                     0.00696165
policy/KLBefore                               0
policy/LossAfter                             -0.0239645
policy/LossBefore                             1.69626e-08
policy/Perplexity                             0.464169
policy/dLoss                                  0.0239645
---------------------------------------  ----------------
2022-04-23 14:22:47 | [train_policy] epoch #604 | Obtaining samples for iteration 604...
2022-04-23 14:22:47 | [train_policy] epoch #604 | Logging diagnostics...
2022-04-23 14:22:47 | [train_policy] epoch #604 | Optimizing policy...
2022-04-23 14:22:47 | [train_policy] epoch #604 | Computing loss before
2022-04-23 14:22:47 | [train_policy] epoch #604 | Computing KL before
2022-04-23 14:22:47 | [train_policy] epoch #604 | Optimizing
2022-04-23 14:22:47 | [train_policy] epoch #604 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:47 | [train_policy] epoch #604 | computing loss before
2022-04-23 14:22:47 | [train_policy] epoch #604 | computing gradient
2022-04-23 14:22:47 | [train_policy] epoch #604 | gradient computed
2022-04-23 14:22:47 | [train_policy] epoch #604 | computing descent direction
2022-04-23 14:22:47 | [train_policy] epoch #604 | descent direction computed
2022-04-23 14:22:47 | [train_policy] epoch #604 | backtrack iters: 0
2022-04-23 14:22:47 | [train_policy] epoch #604 | optimization finished
2022-04-23 14:22:47 | [train_policy] epoch #604 | Computing KL after
2022-04-23 14:22:47 | [train_policy] epoch #604 | Computing loss after
2022-04-23 14:22:47 | [train_policy] epoch #604 | Fitting baseline...
2022-04-23 14:22:47 | [train_policy] epoch #604 | Saving snapshot...
2022-04-23 14:22:47 | [train_policy] epoch #604 | Saved
2022-04-23 14:22:47 | [train_policy] epoch #604 | Time 215.39 s
2022-04-23 14:22:47 | [train_policy] epoch #604 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118474
Evaluation/AverageDiscountedReturn          -41.2576
Evaluation/AverageReturn                    -41.2576
Evaluation/CompletionRate                     0
Evaluation/Iteration                        604
Evaluation/MaxReturn                        -33.8325
Evaluation/MinReturn                        -73.8854
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.74858
Extras/EpisodeRewardMean                    -41.5499
LinearFeatureBaseline/ExplainedVariance       0.875492
PolicyExecTime                                0.109171
ProcessExecTime                               0.0119853
TotalEnvSteps                            612260
policy/Entropy                               -0.740565
policy/KL                                     0.00995636
policy/KLBefore                               0
policy/LossAfter                             -0.0220566
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.476844
policy/dLoss                                  0.0220567
---------------------------------------  ----------------
2022-04-23 14:22:47 | [train_policy] epoch #605 | Obtaining samples for iteration 605...
2022-04-23 14:22:47 | [train_policy] epoch #605 | Logging diagnostics...
2022-04-23 14:22:47 | [train_policy] epoch #605 | Optimizing policy...
2022-04-23 14:22:47 | [train_policy] epoch #605 | Computing loss before
2022-04-23 14:22:47 | [train_policy] epoch #605 | Computing KL before
2022-04-23 14:22:47 | [train_policy] epoch #605 | Optimizing
2022-04-23 14:22:47 | [train_policy] epoch #605 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:47 | [train_policy] epoch #605 | computing loss before
2022-04-23 14:22:47 | [train_policy] epoch #605 | computing gradient
2022-04-23 14:22:47 | [train_policy] epoch #605 | gradient computed
2022-04-23 14:22:47 | [train_policy] epoch #605 | computing descent direction
2022-04-23 14:22:47 | [train_policy] epoch #605 | descent direction computed
2022-04-23 14:22:48 | [train_policy] epoch #605 | backtrack iters: 1
2022-04-23 14:22:48 | [train_policy] epoch #605 | optimization finished
2022-04-23 14:22:48 | [train_policy] epoch #605 | Computing KL after
2022-04-23 14:22:48 | [train_policy] epoch #605 | Computing loss after
2022-04-23 14:22:48 | [train_policy] epoch #605 | Fitting baseline...
2022-04-23 14:22:48 | [train_policy] epoch #605 | Saving snapshot...
2022-04-23 14:22:48 | [train_policy] epoch #605 | Saved
2022-04-23 14:22:48 | [train_policy] epoch #605 | Time 215.74 s
2022-04-23 14:22:48 | [train_policy] epoch #605 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116328
Evaluation/AverageDiscountedReturn          -42.8561
Evaluation/AverageReturn                    -42.8561
Evaluation/CompletionRate                     0
Evaluation/Iteration                        605
Evaluation/MaxReturn                        -30.8864
Evaluation/MinReturn                        -73.6059
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.88424
Extras/EpisodeRewardMean                    -42.3821
LinearFeatureBaseline/ExplainedVariance       0.869043
PolicyExecTime                                0.0985663
ProcessExecTime                               0.0114899
TotalEnvSteps                            613272
policy/Entropy                               -0.802771
policy/KL                                     0.00660926
policy/KLBefore                               0
policy/LossAfter                             -0.0126351
policy/LossBefore                             1.64914e-09
policy/Perplexity                             0.448086
policy/dLoss                                  0.0126351
---------------------------------------  ----------------
2022-04-23 14:22:48 | [train_policy] epoch #606 | Obtaining samples for iteration 606...
2022-04-23 14:22:48 | [train_policy] epoch #606 | Logging diagnostics...
2022-04-23 14:22:48 | [train_policy] epoch #606 | Optimizing policy...
2022-04-23 14:22:48 | [train_policy] epoch #606 | Computing loss before
2022-04-23 14:22:48 | [train_policy] epoch #606 | Computing KL before
2022-04-23 14:22:48 | [train_policy] epoch #606 | Optimizing
2022-04-23 14:22:48 | [train_policy] epoch #606 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:48 | [train_policy] epoch #606 | computing loss before
2022-04-23 14:22:48 | [train_policy] epoch #606 | computing gradient
2022-04-23 14:22:48 | [train_policy] epoch #606 | gradient computed
2022-04-23 14:22:48 | [train_policy] epoch #606 | computing descent direction
2022-04-23 14:22:48 | [train_policy] epoch #606 | descent direction computed
2022-04-23 14:22:48 | [train_policy] epoch #606 | backtrack iters: 1
2022-04-23 14:22:48 | [train_policy] epoch #606 | optimization finished
2022-04-23 14:22:48 | [train_policy] epoch #606 | Computing KL after
2022-04-23 14:22:48 | [train_policy] epoch #606 | Computing loss after
2022-04-23 14:22:48 | [train_policy] epoch #606 | Fitting baseline...
2022-04-23 14:22:48 | [train_policy] epoch #606 | Saving snapshot...
2022-04-23 14:22:48 | [train_policy] epoch #606 | Saved
2022-04-23 14:22:48 | [train_policy] epoch #606 | Time 216.08 s
2022-04-23 14:22:48 | [train_policy] epoch #606 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11751
Evaluation/AverageDiscountedReturn          -40.8379
Evaluation/AverageReturn                    -40.8379
Evaluation/CompletionRate                     0
Evaluation/Iteration                        606
Evaluation/MaxReturn                        -32.0745
Evaluation/MinReturn                        -64.0985
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.5921
Extras/EpisodeRewardMean                    -41.372
LinearFeatureBaseline/ExplainedVariance       0.861807
PolicyExecTime                                0.0935853
ProcessExecTime                               0.0111268
TotalEnvSteps                            614284
policy/Entropy                               -0.827433
policy/KL                                     0.0066032
policy/KLBefore                               0
policy/LossAfter                             -0.0171606
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.43717
policy/dLoss                                  0.0171606
---------------------------------------  ----------------
2022-04-23 14:22:48 | [train_policy] epoch #607 | Obtaining samples for iteration 607...
2022-04-23 14:22:48 | [train_policy] epoch #607 | Logging diagnostics...
2022-04-23 14:22:48 | [train_policy] epoch #607 | Optimizing policy...
2022-04-23 14:22:48 | [train_policy] epoch #607 | Computing loss before
2022-04-23 14:22:48 | [train_policy] epoch #607 | Computing KL before
2022-04-23 14:22:48 | [train_policy] epoch #607 | Optimizing
2022-04-23 14:22:48 | [train_policy] epoch #607 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:48 | [train_policy] epoch #607 | computing loss before
2022-04-23 14:22:48 | [train_policy] epoch #607 | computing gradient
2022-04-23 14:22:48 | [train_policy] epoch #607 | gradient computed
2022-04-23 14:22:48 | [train_policy] epoch #607 | computing descent direction
2022-04-23 14:22:48 | [train_policy] epoch #607 | descent direction computed
2022-04-23 14:22:48 | [train_policy] epoch #607 | backtrack iters: 0
2022-04-23 14:22:48 | [train_policy] epoch #607 | optimization finished
2022-04-23 14:22:48 | [train_policy] epoch #607 | Computing KL after
2022-04-23 14:22:48 | [train_policy] epoch #607 | Computing loss after
2022-04-23 14:22:48 | [train_policy] epoch #607 | Fitting baseline...
2022-04-23 14:22:48 | [train_policy] epoch #607 | Saving snapshot...
2022-04-23 14:22:48 | [train_policy] epoch #607 | Saved
2022-04-23 14:22:48 | [train_policy] epoch #607 | Time 216.43 s
2022-04-23 14:22:48 | [train_policy] epoch #607 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11788
Evaluation/AverageDiscountedReturn          -41.6082
Evaluation/AverageReturn                    -41.6082
Evaluation/CompletionRate                     0
Evaluation/Iteration                        607
Evaluation/MaxReturn                        -31.3831
Evaluation/MinReturn                        -74.2203
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.38064
Extras/EpisodeRewardMean                    -41.9037
LinearFeatureBaseline/ExplainedVariance       0.875905
PolicyExecTime                                0.101004
ProcessExecTime                               0.0112507
TotalEnvSteps                            615296
policy/Entropy                               -0.825194
policy/KL                                     0.00980865
policy/KLBefore                               0
policy/LossAfter                             -0.0179294
policy/LossBefore                             4.47624e-09
policy/Perplexity                             0.43815
policy/dLoss                                  0.0179294
---------------------------------------  ----------------
2022-04-23 14:22:48 | [train_policy] epoch #608 | Obtaining samples for iteration 608...
2022-04-23 14:22:48 | [train_policy] epoch #608 | Logging diagnostics...
2022-04-23 14:22:48 | [train_policy] epoch #608 | Optimizing policy...
2022-04-23 14:22:48 | [train_policy] epoch #608 | Computing loss before
2022-04-23 14:22:48 | [train_policy] epoch #608 | Computing KL before
2022-04-23 14:22:48 | [train_policy] epoch #608 | Optimizing
2022-04-23 14:22:48 | [train_policy] epoch #608 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:48 | [train_policy] epoch #608 | computing loss before
2022-04-23 14:22:48 | [train_policy] epoch #608 | computing gradient
2022-04-23 14:22:48 | [train_policy] epoch #608 | gradient computed
2022-04-23 14:22:48 | [train_policy] epoch #608 | computing descent direction
2022-04-23 14:22:49 | [train_policy] epoch #608 | descent direction computed
2022-04-23 14:22:49 | [train_policy] epoch #608 | backtrack iters: 0
2022-04-23 14:22:49 | [train_policy] epoch #608 | optimization finished
2022-04-23 14:22:49 | [train_policy] epoch #608 | Computing KL after
2022-04-23 14:22:49 | [train_policy] epoch #608 | Computing loss after
2022-04-23 14:22:49 | [train_policy] epoch #608 | Fitting baseline...
2022-04-23 14:22:49 | [train_policy] epoch #608 | Saving snapshot...
2022-04-23 14:22:49 | [train_policy] epoch #608 | Saved
2022-04-23 14:22:49 | [train_policy] epoch #608 | Time 216.78 s
2022-04-23 14:22:49 | [train_policy] epoch #608 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116223
Evaluation/AverageDiscountedReturn          -42.1738
Evaluation/AverageReturn                    -42.1738
Evaluation/CompletionRate                     0
Evaluation/Iteration                        608
Evaluation/MaxReturn                        -33.3523
Evaluation/MinReturn                        -63.282
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.74965
Extras/EpisodeRewardMean                    -42.2396
LinearFeatureBaseline/ExplainedVariance       0.863587
PolicyExecTime                                0.106829
ProcessExecTime                               0.0112967
TotalEnvSteps                            616308
policy/Entropy                               -0.842774
policy/KL                                     0.009905
policy/KLBefore                               0
policy/LossAfter                             -0.0309126
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.430514
policy/dLoss                                  0.0309126
---------------------------------------  ----------------
2022-04-23 14:22:49 | [train_policy] epoch #609 | Obtaining samples for iteration 609...
2022-04-23 14:22:49 | [train_policy] epoch #609 | Logging diagnostics...
2022-04-23 14:22:49 | [train_policy] epoch #609 | Optimizing policy...
2022-04-23 14:22:49 | [train_policy] epoch #609 | Computing loss before
2022-04-23 14:22:49 | [train_policy] epoch #609 | Computing KL before
2022-04-23 14:22:49 | [train_policy] epoch #609 | Optimizing
2022-04-23 14:22:49 | [train_policy] epoch #609 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:49 | [train_policy] epoch #609 | computing loss before
2022-04-23 14:22:49 | [train_policy] epoch #609 | computing gradient
2022-04-23 14:22:49 | [train_policy] epoch #609 | gradient computed
2022-04-23 14:22:49 | [train_policy] epoch #609 | computing descent direction
2022-04-23 14:22:49 | [train_policy] epoch #609 | descent direction computed
2022-04-23 14:22:49 | [train_policy] epoch #609 | backtrack iters: 1
2022-04-23 14:22:49 | [train_policy] epoch #609 | optimization finished
2022-04-23 14:22:49 | [train_policy] epoch #609 | Computing KL after
2022-04-23 14:22:49 | [train_policy] epoch #609 | Computing loss after
2022-04-23 14:22:49 | [train_policy] epoch #609 | Fitting baseline...
2022-04-23 14:22:49 | [train_policy] epoch #609 | Saving snapshot...
2022-04-23 14:22:49 | [train_policy] epoch #609 | Saved
2022-04-23 14:22:49 | [train_policy] epoch #609 | Time 217.14 s
2022-04-23 14:22:49 | [train_policy] epoch #609 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.116081
Evaluation/AverageDiscountedReturn          -44.1048
Evaluation/AverageReturn                    -44.1048
Evaluation/CompletionRate                     0
Evaluation/Iteration                        609
Evaluation/MaxReturn                        -32.1458
Evaluation/MinReturn                        -64.0935
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.57193
Extras/EpisodeRewardMean                    -43.9166
LinearFeatureBaseline/ExplainedVariance       0.876193
PolicyExecTime                                0.112434
ProcessExecTime                               0.0112653
TotalEnvSteps                            617320
policy/Entropy                               -0.910856
policy/KL                                     0.00680881
policy/KLBefore                               0
policy/LossAfter                             -0.0172744
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.40218
policy/dLoss                                  0.0172744
---------------------------------------  ----------------
2022-04-23 14:22:49 | [train_policy] epoch #610 | Obtaining samples for iteration 610...
2022-04-23 14:22:49 | [train_policy] epoch #610 | Logging diagnostics...
2022-04-23 14:22:49 | [train_policy] epoch #610 | Optimizing policy...
2022-04-23 14:22:49 | [train_policy] epoch #610 | Computing loss before
2022-04-23 14:22:49 | [train_policy] epoch #610 | Computing KL before
2022-04-23 14:22:49 | [train_policy] epoch #610 | Optimizing
2022-04-23 14:22:49 | [train_policy] epoch #610 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:49 | [train_policy] epoch #610 | computing loss before
2022-04-23 14:22:49 | [train_policy] epoch #610 | computing gradient
2022-04-23 14:22:49 | [train_policy] epoch #610 | gradient computed
2022-04-23 14:22:49 | [train_policy] epoch #610 | computing descent direction
2022-04-23 14:22:49 | [train_policy] epoch #610 | descent direction computed
2022-04-23 14:22:49 | [train_policy] epoch #610 | backtrack iters: 0
2022-04-23 14:22:49 | [train_policy] epoch #610 | optimization finished
2022-04-23 14:22:49 | [train_policy] epoch #610 | Computing KL after
2022-04-23 14:22:49 | [train_policy] epoch #610 | Computing loss after
2022-04-23 14:22:49 | [train_policy] epoch #610 | Fitting baseline...
2022-04-23 14:22:49 | [train_policy] epoch #610 | Saving snapshot...
2022-04-23 14:22:49 | [train_policy] epoch #610 | Saved
2022-04-23 14:22:49 | [train_policy] epoch #610 | Time 217.48 s
2022-04-23 14:22:49 | [train_policy] epoch #610 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11603
Evaluation/AverageDiscountedReturn          -42.3293
Evaluation/AverageReturn                    -42.3293
Evaluation/CompletionRate                     0
Evaluation/Iteration                        610
Evaluation/MaxReturn                        -32.8156
Evaluation/MinReturn                        -73.75
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.61003
Extras/EpisodeRewardMean                    -42.5956
LinearFeatureBaseline/ExplainedVariance       0.854752
PolicyExecTime                                0.103215
ProcessExecTime                               0.0109844
TotalEnvSteps                            618332
policy/Entropy                               -0.898957
policy/KL                                     0.00964773
policy/KLBefore                               0
policy/LossAfter                             -0.0218879
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             0.406994
policy/dLoss                                  0.0218879
---------------------------------------  ----------------
2022-04-23 14:22:49 | [train_policy] epoch #611 | Obtaining samples for iteration 611...
2022-04-23 14:22:50 | [train_policy] epoch #611 | Logging diagnostics...
2022-04-23 14:22:50 | [train_policy] epoch #611 | Optimizing policy...
2022-04-23 14:22:50 | [train_policy] epoch #611 | Computing loss before
2022-04-23 14:22:50 | [train_policy] epoch #611 | Computing KL before
2022-04-23 14:22:50 | [train_policy] epoch #611 | Optimizing
2022-04-23 14:22:50 | [train_policy] epoch #611 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:50 | [train_policy] epoch #611 | computing loss before
2022-04-23 14:22:50 | [train_policy] epoch #611 | computing gradient
2022-04-23 14:22:50 | [train_policy] epoch #611 | gradient computed
2022-04-23 14:22:50 | [train_policy] epoch #611 | computing descent direction
2022-04-23 14:22:50 | [train_policy] epoch #611 | descent direction computed
2022-04-23 14:22:50 | [train_policy] epoch #611 | backtrack iters: 0
2022-04-23 14:22:50 | [train_policy] epoch #611 | optimization finished
2022-04-23 14:22:50 | [train_policy] epoch #611 | Computing KL after
2022-04-23 14:22:50 | [train_policy] epoch #611 | Computing loss after
2022-04-23 14:22:50 | [train_policy] epoch #611 | Fitting baseline...
2022-04-23 14:22:50 | [train_policy] epoch #611 | Saving snapshot...
2022-04-23 14:22:50 | [train_policy] epoch #611 | Saved
2022-04-23 14:22:50 | [train_policy] epoch #611 | Time 217.85 s
2022-04-23 14:22:50 | [train_policy] epoch #611 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.119374
Evaluation/AverageDiscountedReturn          -41.726
Evaluation/AverageReturn                    -41.726
Evaluation/CompletionRate                     0
Evaluation/Iteration                        611
Evaluation/MaxReturn                        -33.3663
Evaluation/MinReturn                        -63.2766
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.45047
Extras/EpisodeRewardMean                    -41.4695
LinearFeatureBaseline/ExplainedVariance       0.901879
PolicyExecTime                                0.127105
ProcessExecTime                               0.0115218
TotalEnvSteps                            619344
policy/Entropy                               -0.889786
policy/KL                                     0.00918186
policy/KLBefore                               0
policy/LossAfter                             -0.020504
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.410744
policy/dLoss                                  0.020504
---------------------------------------  ----------------
2022-04-23 14:22:50 | [train_policy] epoch #612 | Obtaining samples for iteration 612...
2022-04-23 14:22:50 | [train_policy] epoch #612 | Logging diagnostics...
2022-04-23 14:22:50 | [train_policy] epoch #612 | Optimizing policy...
2022-04-23 14:22:50 | [train_policy] epoch #612 | Computing loss before
2022-04-23 14:22:50 | [train_policy] epoch #612 | Computing KL before
2022-04-23 14:22:50 | [train_policy] epoch #612 | Optimizing
2022-04-23 14:22:50 | [train_policy] epoch #612 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:50 | [train_policy] epoch #612 | computing loss before
2022-04-23 14:22:50 | [train_policy] epoch #612 | computing gradient
2022-04-23 14:22:50 | [train_policy] epoch #612 | gradient computed
2022-04-23 14:22:50 | [train_policy] epoch #612 | computing descent direction
2022-04-23 14:22:50 | [train_policy] epoch #612 | descent direction computed
2022-04-23 14:22:50 | [train_policy] epoch #612 | backtrack iters: 0
2022-04-23 14:22:50 | [train_policy] epoch #612 | optimization finished
2022-04-23 14:22:50 | [train_policy] epoch #612 | Computing KL after
2022-04-23 14:22:50 | [train_policy] epoch #612 | Computing loss after
2022-04-23 14:22:50 | [train_policy] epoch #612 | Fitting baseline...
2022-04-23 14:22:50 | [train_policy] epoch #612 | Saving snapshot...
2022-04-23 14:22:50 | [train_policy] epoch #612 | Saved
2022-04-23 14:22:50 | [train_policy] epoch #612 | Time 218.21 s
2022-04-23 14:22:50 | [train_policy] epoch #612 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.119255
Evaluation/AverageDiscountedReturn          -42.5214
Evaluation/AverageReturn                    -42.5214
Evaluation/CompletionRate                     0
Evaluation/Iteration                        612
Evaluation/MaxReturn                        -33.217
Evaluation/MinReturn                        -72.8172
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.59011
Extras/EpisodeRewardMean                    -42.2664
LinearFeatureBaseline/ExplainedVariance       0.873735
PolicyExecTime                                0.101359
ProcessExecTime                               0.0111504
TotalEnvSteps                            620356
policy/Entropy                               -0.912076
policy/KL                                     0.00980133
policy/KLBefore                               0
policy/LossAfter                             -0.037518
policy/LossBefore                            -1.69626e-08
policy/Perplexity                             0.401689
policy/dLoss                                  0.037518
---------------------------------------  ----------------
2022-04-23 14:22:50 | [train_policy] epoch #613 | Obtaining samples for iteration 613...
2022-04-23 14:22:50 | [train_policy] epoch #613 | Logging diagnostics...
2022-04-23 14:22:50 | [train_policy] epoch #613 | Optimizing policy...
2022-04-23 14:22:50 | [train_policy] epoch #613 | Computing loss before
2022-04-23 14:22:50 | [train_policy] epoch #613 | Computing KL before
2022-04-23 14:22:50 | [train_policy] epoch #613 | Optimizing
2022-04-23 14:22:50 | [train_policy] epoch #613 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:50 | [train_policy] epoch #613 | computing loss before
2022-04-23 14:22:50 | [train_policy] epoch #613 | computing gradient
2022-04-23 14:22:50 | [train_policy] epoch #613 | gradient computed
2022-04-23 14:22:50 | [train_policy] epoch #613 | computing descent direction
2022-04-23 14:22:50 | [train_policy] epoch #613 | descent direction computed
2022-04-23 14:22:50 | [train_policy] epoch #613 | backtrack iters: 1
2022-04-23 14:22:50 | [train_policy] epoch #613 | optimization finished
2022-04-23 14:22:50 | [train_policy] epoch #613 | Computing KL after
2022-04-23 14:22:50 | [train_policy] epoch #613 | Computing loss after
2022-04-23 14:22:50 | [train_policy] epoch #613 | Fitting baseline...
2022-04-23 14:22:50 | [train_policy] epoch #613 | Saving snapshot...
2022-04-23 14:22:50 | [train_policy] epoch #613 | Saved
2022-04-23 14:22:50 | [train_policy] epoch #613 | Time 218.58 s
2022-04-23 14:22:50 | [train_policy] epoch #613 | EpochTime 0.37 s
---------------------------------------  ---------------
EnvExecTime                                   0.120111
Evaluation/AverageDiscountedReturn          -41.5055
Evaluation/AverageReturn                    -41.5055
Evaluation/CompletionRate                     0
Evaluation/Iteration                        613
Evaluation/MaxReturn                        -32.4714
Evaluation/MinReturn                        -64.0603
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.45082
Extras/EpisodeRewardMean                    -41.4951
LinearFeatureBaseline/ExplainedVariance       0.884815
PolicyExecTime                                0.12152
ProcessExecTime                               0.0111611
TotalEnvSteps                            621368
policy/Entropy                               -0.953632
policy/KL                                     0.00672357
policy/KLBefore                               0
policy/LossAfter                             -0.0121563
policy/LossBefore                            -2.191e-08
policy/Perplexity                             0.385339
policy/dLoss                                  0.0121563
---------------------------------------  ---------------
2022-04-23 14:22:50 | [train_policy] epoch #614 | Obtaining samples for iteration 614...
2022-04-23 14:22:51 | [train_policy] epoch #614 | Logging diagnostics...
2022-04-23 14:22:51 | [train_policy] epoch #614 | Optimizing policy...
2022-04-23 14:22:51 | [train_policy] epoch #614 | Computing loss before
2022-04-23 14:22:51 | [train_policy] epoch #614 | Computing KL before
2022-04-23 14:22:51 | [train_policy] epoch #614 | Optimizing
2022-04-23 14:22:51 | [train_policy] epoch #614 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:51 | [train_policy] epoch #614 | computing loss before
2022-04-23 14:22:51 | [train_policy] epoch #614 | computing gradient
2022-04-23 14:22:51 | [train_policy] epoch #614 | gradient computed
2022-04-23 14:22:51 | [train_policy] epoch #614 | computing descent direction
2022-04-23 14:22:51 | [train_policy] epoch #614 | descent direction computed
2022-04-23 14:22:51 | [train_policy] epoch #614 | backtrack iters: 0
2022-04-23 14:22:51 | [train_policy] epoch #614 | optimization finished
2022-04-23 14:22:51 | [train_policy] epoch #614 | Computing KL after
2022-04-23 14:22:51 | [train_policy] epoch #614 | Computing loss after
2022-04-23 14:22:51 | [train_policy] epoch #614 | Fitting baseline...
2022-04-23 14:22:51 | [train_policy] epoch #614 | Saving snapshot...
2022-04-23 14:22:51 | [train_policy] epoch #614 | Saved
2022-04-23 14:22:51 | [train_policy] epoch #614 | Time 218.95 s
2022-04-23 14:22:51 | [train_policy] epoch #614 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.123109
Evaluation/AverageDiscountedReturn          -42.5253
Evaluation/AverageReturn                    -42.5253
Evaluation/CompletionRate                     0
Evaluation/Iteration                        614
Evaluation/MaxReturn                        -33.3701
Evaluation/MinReturn                        -64.356
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.53032
Extras/EpisodeRewardMean                    -42.4132
LinearFeatureBaseline/ExplainedVariance       0.890933
PolicyExecTime                                0.113529
ProcessExecTime                               0.0120454
TotalEnvSteps                            622380
policy/Entropy                               -0.929823
policy/KL                                     0.00890108
policy/KLBefore                               0
policy/LossAfter                             -0.0140836
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.394624
policy/dLoss                                  0.0140837
---------------------------------------  ----------------
2022-04-23 14:22:51 | [train_policy] epoch #615 | Obtaining samples for iteration 615...
2022-04-23 14:22:51 | [train_policy] epoch #615 | Logging diagnostics...
2022-04-23 14:22:51 | [train_policy] epoch #615 | Optimizing policy...
2022-04-23 14:22:51 | [train_policy] epoch #615 | Computing loss before
2022-04-23 14:22:51 | [train_policy] epoch #615 | Computing KL before
2022-04-23 14:22:51 | [train_policy] epoch #615 | Optimizing
2022-04-23 14:22:51 | [train_policy] epoch #615 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:51 | [train_policy] epoch #615 | computing loss before
2022-04-23 14:22:51 | [train_policy] epoch #615 | computing gradient
2022-04-23 14:22:51 | [train_policy] epoch #615 | gradient computed
2022-04-23 14:22:51 | [train_policy] epoch #615 | computing descent direction
2022-04-23 14:22:51 | [train_policy] epoch #615 | descent direction computed
2022-04-23 14:22:51 | [train_policy] epoch #615 | backtrack iters: 1
2022-04-23 14:22:51 | [train_policy] epoch #615 | optimization finished
2022-04-23 14:22:51 | [train_policy] epoch #615 | Computing KL after
2022-04-23 14:22:51 | [train_policy] epoch #615 | Computing loss after
2022-04-23 14:22:51 | [train_policy] epoch #615 | Fitting baseline...
2022-04-23 14:22:51 | [train_policy] epoch #615 | Saving snapshot...
2022-04-23 14:22:51 | [train_policy] epoch #615 | Saved
2022-04-23 14:22:51 | [train_policy] epoch #615 | Time 219.29 s
2022-04-23 14:22:51 | [train_policy] epoch #615 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11699
Evaluation/AverageDiscountedReturn          -41.9209
Evaluation/AverageReturn                    -41.9209
Evaluation/CompletionRate                     0
Evaluation/Iteration                        615
Evaluation/MaxReturn                        -32.0849
Evaluation/MinReturn                        -72.1244
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.03582
Extras/EpisodeRewardMean                    -41.7515
LinearFeatureBaseline/ExplainedVariance       0.893235
PolicyExecTime                                0.0992899
ProcessExecTime                               0.0110428
TotalEnvSteps                            623392
policy/Entropy                               -0.988931
policy/KL                                     0.00658905
policy/KLBefore                               0
policy/LossAfter                             -0.0164401
policy/LossBefore                            -6.77326e-09
policy/Perplexity                             0.371974
policy/dLoss                                  0.0164401
---------------------------------------  ----------------
2022-04-23 14:22:51 | [train_policy] epoch #616 | Obtaining samples for iteration 616...
2022-04-23 14:22:51 | [train_policy] epoch #616 | Logging diagnostics...
2022-04-23 14:22:51 | [train_policy] epoch #616 | Optimizing policy...
2022-04-23 14:22:51 | [train_policy] epoch #616 | Computing loss before
2022-04-23 14:22:51 | [train_policy] epoch #616 | Computing KL before
2022-04-23 14:22:51 | [train_policy] epoch #616 | Optimizing
2022-04-23 14:22:51 | [train_policy] epoch #616 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:51 | [train_policy] epoch #616 | computing loss before
2022-04-23 14:22:51 | [train_policy] epoch #616 | computing gradient
2022-04-23 14:22:51 | [train_policy] epoch #616 | gradient computed
2022-04-23 14:22:51 | [train_policy] epoch #616 | computing descent direction
2022-04-23 14:22:51 | [train_policy] epoch #616 | descent direction computed
2022-04-23 14:22:51 | [train_policy] epoch #616 | backtrack iters: 0
2022-04-23 14:22:51 | [train_policy] epoch #616 | optimization finished
2022-04-23 14:22:51 | [train_policy] epoch #616 | Computing KL after
2022-04-23 14:22:51 | [train_policy] epoch #616 | Computing loss after
2022-04-23 14:22:51 | [train_policy] epoch #616 | Fitting baseline...
2022-04-23 14:22:51 | [train_policy] epoch #616 | Saving snapshot...
2022-04-23 14:22:51 | [train_policy] epoch #616 | Saved
2022-04-23 14:22:51 | [train_policy] epoch #616 | Time 219.64 s
2022-04-23 14:22:51 | [train_policy] epoch #616 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116507
Evaluation/AverageDiscountedReturn          -43.1193
Evaluation/AverageReturn                    -43.1193
Evaluation/CompletionRate                     0
Evaluation/Iteration                        616
Evaluation/MaxReturn                        -31.8586
Evaluation/MinReturn                        -75.2058
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.43306
Extras/EpisodeRewardMean                    -43.274
LinearFeatureBaseline/ExplainedVariance       0.823351
PolicyExecTime                                0.108274
ProcessExecTime                               0.0115657
TotalEnvSteps                            624404
policy/Entropy                               -0.989992
policy/KL                                     0.00947286
policy/KLBefore                               0
policy/LossAfter                             -0.0206306
policy/LossBefore                            -1.57846e-08
policy/Perplexity                             0.37158
policy/dLoss                                  0.0206306
---------------------------------------  ----------------
2022-04-23 14:22:51 | [train_policy] epoch #617 | Obtaining samples for iteration 617...
2022-04-23 14:22:52 | [train_policy] epoch #617 | Logging diagnostics...
2022-04-23 14:22:52 | [train_policy] epoch #617 | Optimizing policy...
2022-04-23 14:22:52 | [train_policy] epoch #617 | Computing loss before
2022-04-23 14:22:52 | [train_policy] epoch #617 | Computing KL before
2022-04-23 14:22:52 | [train_policy] epoch #617 | Optimizing
2022-04-23 14:22:52 | [train_policy] epoch #617 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:52 | [train_policy] epoch #617 | computing loss before
2022-04-23 14:22:52 | [train_policy] epoch #617 | computing gradient
2022-04-23 14:22:52 | [train_policy] epoch #617 | gradient computed
2022-04-23 14:22:52 | [train_policy] epoch #617 | computing descent direction
2022-04-23 14:22:52 | [train_policy] epoch #617 | descent direction computed
2022-04-23 14:22:52 | [train_policy] epoch #617 | backtrack iters: 0
2022-04-23 14:22:52 | [train_policy] epoch #617 | optimization finished
2022-04-23 14:22:52 | [train_policy] epoch #617 | Computing KL after
2022-04-23 14:22:52 | [train_policy] epoch #617 | Computing loss after
2022-04-23 14:22:52 | [train_policy] epoch #617 | Fitting baseline...
2022-04-23 14:22:52 | [train_policy] epoch #617 | Saving snapshot...
2022-04-23 14:22:52 | [train_policy] epoch #617 | Saved
2022-04-23 14:22:52 | [train_policy] epoch #617 | Time 219.98 s
2022-04-23 14:22:52 | [train_policy] epoch #617 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117839
Evaluation/AverageDiscountedReturn          -41.6236
Evaluation/AverageReturn                    -41.6236
Evaluation/CompletionRate                     0
Evaluation/Iteration                        617
Evaluation/MaxReturn                        -32.5241
Evaluation/MinReturn                        -65.0097
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.22501
Extras/EpisodeRewardMean                    -41.5943
LinearFeatureBaseline/ExplainedVariance       0.877741
PolicyExecTime                                0.106388
ProcessExecTime                               0.0124521
TotalEnvSteps                            625416
policy/Entropy                               -0.953126
policy/KL                                     0.00965197
policy/KLBefore                               0
policy/LossAfter                             -0.0186917
policy/LossBefore                             6.36097e-09
policy/Perplexity                             0.385534
policy/dLoss                                  0.0186917
---------------------------------------  ----------------
2022-04-23 14:22:52 | [train_policy] epoch #618 | Obtaining samples for iteration 618...
2022-04-23 14:22:52 | [train_policy] epoch #618 | Logging diagnostics...
2022-04-23 14:22:52 | [train_policy] epoch #618 | Optimizing policy...
2022-04-23 14:22:52 | [train_policy] epoch #618 | Computing loss before
2022-04-23 14:22:52 | [train_policy] epoch #618 | Computing KL before
2022-04-23 14:22:52 | [train_policy] epoch #618 | Optimizing
2022-04-23 14:22:52 | [train_policy] epoch #618 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:52 | [train_policy] epoch #618 | computing loss before
2022-04-23 14:22:52 | [train_policy] epoch #618 | computing gradient
2022-04-23 14:22:52 | [train_policy] epoch #618 | gradient computed
2022-04-23 14:22:52 | [train_policy] epoch #618 | computing descent direction
2022-04-23 14:22:52 | [train_policy] epoch #618 | descent direction computed
2022-04-23 14:22:52 | [train_policy] epoch #618 | backtrack iters: 1
2022-04-23 14:22:52 | [train_policy] epoch #618 | optimization finished
2022-04-23 14:22:52 | [train_policy] epoch #618 | Computing KL after
2022-04-23 14:22:52 | [train_policy] epoch #618 | Computing loss after
2022-04-23 14:22:52 | [train_policy] epoch #618 | Fitting baseline...
2022-04-23 14:22:52 | [train_policy] epoch #618 | Saving snapshot...
2022-04-23 14:22:52 | [train_policy] epoch #618 | Saved
2022-04-23 14:22:52 | [train_policy] epoch #618 | Time 220.33 s
2022-04-23 14:22:52 | [train_policy] epoch #618 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118123
Evaluation/AverageDiscountedReturn          -42.4976
Evaluation/AverageReturn                    -42.4976
Evaluation/CompletionRate                     0
Evaluation/Iteration                        618
Evaluation/MaxReturn                        -33.0099
Evaluation/MinReturn                        -64.0149
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.68266
Extras/EpisodeRewardMean                    -42.3841
LinearFeatureBaseline/ExplainedVariance       0.891333
PolicyExecTime                                0.106313
ProcessExecTime                               0.0122588
TotalEnvSteps                            626428
policy/Entropy                               -0.990282
policy/KL                                     0.006875
policy/KLBefore                               0
policy/LossAfter                             -0.0151457
policy/LossBefore                            -9.65925e-09
policy/Perplexity                             0.371472
policy/dLoss                                  0.0151457
---------------------------------------  ----------------
2022-04-23 14:22:52 | [train_policy] epoch #619 | Obtaining samples for iteration 619...
2022-04-23 14:22:52 | [train_policy] epoch #619 | Logging diagnostics...
2022-04-23 14:22:52 | [train_policy] epoch #619 | Optimizing policy...
2022-04-23 14:22:52 | [train_policy] epoch #619 | Computing loss before
2022-04-23 14:22:52 | [train_policy] epoch #619 | Computing KL before
2022-04-23 14:22:52 | [train_policy] epoch #619 | Optimizing
2022-04-23 14:22:52 | [train_policy] epoch #619 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:52 | [train_policy] epoch #619 | computing loss before
2022-04-23 14:22:52 | [train_policy] epoch #619 | computing gradient
2022-04-23 14:22:52 | [train_policy] epoch #619 | gradient computed
2022-04-23 14:22:52 | [train_policy] epoch #619 | computing descent direction
2022-04-23 14:22:52 | [train_policy] epoch #619 | descent direction computed
2022-04-23 14:22:52 | [train_policy] epoch #619 | backtrack iters: 1
2022-04-23 14:22:52 | [train_policy] epoch #619 | optimization finished
2022-04-23 14:22:52 | [train_policy] epoch #619 | Computing KL after
2022-04-23 14:22:52 | [train_policy] epoch #619 | Computing loss after
2022-04-23 14:22:52 | [train_policy] epoch #619 | Fitting baseline...
2022-04-23 14:22:52 | [train_policy] epoch #619 | Saving snapshot...
2022-04-23 14:22:52 | [train_policy] epoch #619 | Saved
2022-04-23 14:22:52 | [train_policy] epoch #619 | Time 220.67 s
2022-04-23 14:22:52 | [train_policy] epoch #619 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117675
Evaluation/AverageDiscountedReturn          -43.6879
Evaluation/AverageReturn                    -43.6879
Evaluation/CompletionRate                     0
Evaluation/Iteration                        619
Evaluation/MaxReturn                        -32.4868
Evaluation/MinReturn                        -71.9968
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.62737
Extras/EpisodeRewardMean                    -43.564
LinearFeatureBaseline/ExplainedVariance       0.887343
PolicyExecTime                                0.102121
ProcessExecTime                               0.0115488
TotalEnvSteps                            627440
policy/Entropy                               -1.03439
policy/KL                                     0.00663234
policy/KLBefore                               0
policy/LossAfter                             -0.0134938
policy/LossBefore                            -7.77452e-09
policy/Perplexity                             0.355443
policy/dLoss                                  0.0134938
---------------------------------------  ----------------
2022-04-23 14:22:52 | [train_policy] epoch #620 | Obtaining samples for iteration 620...
2022-04-23 14:22:53 | [train_policy] epoch #620 | Logging diagnostics...
2022-04-23 14:22:53 | [train_policy] epoch #620 | Optimizing policy...
2022-04-23 14:22:53 | [train_policy] epoch #620 | Computing loss before
2022-04-23 14:22:53 | [train_policy] epoch #620 | Computing KL before
2022-04-23 14:22:53 | [train_policy] epoch #620 | Optimizing
2022-04-23 14:22:53 | [train_policy] epoch #620 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:53 | [train_policy] epoch #620 | computing loss before
2022-04-23 14:22:53 | [train_policy] epoch #620 | computing gradient
2022-04-23 14:22:53 | [train_policy] epoch #620 | gradient computed
2022-04-23 14:22:53 | [train_policy] epoch #620 | computing descent direction
2022-04-23 14:22:53 | [train_policy] epoch #620 | descent direction computed
2022-04-23 14:22:53 | [train_policy] epoch #620 | backtrack iters: 1
2022-04-23 14:22:53 | [train_policy] epoch #620 | optimization finished
2022-04-23 14:22:53 | [train_policy] epoch #620 | Computing KL after
2022-04-23 14:22:53 | [train_policy] epoch #620 | Computing loss after
2022-04-23 14:22:53 | [train_policy] epoch #620 | Fitting baseline...
2022-04-23 14:22:53 | [train_policy] epoch #620 | Saving snapshot...
2022-04-23 14:22:53 | [train_policy] epoch #620 | Saved
2022-04-23 14:22:53 | [train_policy] epoch #620 | Time 221.00 s
2022-04-23 14:22:53 | [train_policy] epoch #620 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119205
Evaluation/AverageDiscountedReturn          -42.3757
Evaluation/AverageReturn                    -42.3757
Evaluation/CompletionRate                     0
Evaluation/Iteration                        620
Evaluation/MaxReturn                        -31.2202
Evaluation/MinReturn                        -74.3859
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.20826
Extras/EpisodeRewardMean                    -42.2707
LinearFeatureBaseline/ExplainedVariance       0.85406
PolicyExecTime                                0.099601
ProcessExecTime                               0.0112755
TotalEnvSteps                            628452
policy/Entropy                               -1.07684
policy/KL                                     0.0069374
policy/KLBefore                               0
policy/LossAfter                             -0.0149591
policy/LossBefore                            -1.93185e-08
policy/Perplexity                             0.340671
policy/dLoss                                  0.0149591
---------------------------------------  ----------------
2022-04-23 14:22:53 | [train_policy] epoch #621 | Obtaining samples for iteration 621...
2022-04-23 14:22:53 | [train_policy] epoch #621 | Logging diagnostics...
2022-04-23 14:22:53 | [train_policy] epoch #621 | Optimizing policy...
2022-04-23 14:22:53 | [train_policy] epoch #621 | Computing loss before
2022-04-23 14:22:53 | [train_policy] epoch #621 | Computing KL before
2022-04-23 14:22:53 | [train_policy] epoch #621 | Optimizing
2022-04-23 14:22:53 | [train_policy] epoch #621 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:53 | [train_policy] epoch #621 | computing loss before
2022-04-23 14:22:53 | [train_policy] epoch #621 | computing gradient
2022-04-23 14:22:53 | [train_policy] epoch #621 | gradient computed
2022-04-23 14:22:53 | [train_policy] epoch #621 | computing descent direction
2022-04-23 14:22:53 | [train_policy] epoch #621 | descent direction computed
2022-04-23 14:22:53 | [train_policy] epoch #621 | backtrack iters: 0
2022-04-23 14:22:53 | [train_policy] epoch #621 | optimization finished
2022-04-23 14:22:53 | [train_policy] epoch #621 | Computing KL after
2022-04-23 14:22:53 | [train_policy] epoch #621 | Computing loss after
2022-04-23 14:22:53 | [train_policy] epoch #621 | Fitting baseline...
2022-04-23 14:22:53 | [train_policy] epoch #621 | Saving snapshot...
2022-04-23 14:22:53 | [train_policy] epoch #621 | Saved
2022-04-23 14:22:53 | [train_policy] epoch #621 | Time 221.34 s
2022-04-23 14:22:53 | [train_policy] epoch #621 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119396
Evaluation/AverageDiscountedReturn          -41.1526
Evaluation/AverageReturn                    -41.1526
Evaluation/CompletionRate                     0
Evaluation/Iteration                        621
Evaluation/MaxReturn                        -32.1838
Evaluation/MinReturn                        -73.0682
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.46415
Extras/EpisodeRewardMean                    -41.5052
LinearFeatureBaseline/ExplainedVariance       0.889468
PolicyExecTime                                0.102296
ProcessExecTime                               0.0111687
TotalEnvSteps                            629464
policy/Entropy                               -1.05401
policy/KL                                     0.00924894
policy/KLBefore                               0
policy/LossAfter                             -0.01331
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.348536
policy/dLoss                                  0.01331
---------------------------------------  ----------------
2022-04-23 14:22:53 | [train_policy] epoch #622 | Obtaining samples for iteration 622...
2022-04-23 14:22:53 | [train_policy] epoch #622 | Logging diagnostics...
2022-04-23 14:22:53 | [train_policy] epoch #622 | Optimizing policy...
2022-04-23 14:22:53 | [train_policy] epoch #622 | Computing loss before
2022-04-23 14:22:53 | [train_policy] epoch #622 | Computing KL before
2022-04-23 14:22:53 | [train_policy] epoch #622 | Optimizing
2022-04-23 14:22:53 | [train_policy] epoch #622 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:53 | [train_policy] epoch #622 | computing loss before
2022-04-23 14:22:53 | [train_policy] epoch #622 | computing gradient
2022-04-23 14:22:53 | [train_policy] epoch #622 | gradient computed
2022-04-23 14:22:53 | [train_policy] epoch #622 | computing descent direction
2022-04-23 14:22:53 | [train_policy] epoch #622 | descent direction computed
2022-04-23 14:22:53 | [train_policy] epoch #622 | backtrack iters: 1
2022-04-23 14:22:53 | [train_policy] epoch #622 | optimization finished
2022-04-23 14:22:53 | [train_policy] epoch #622 | Computing KL after
2022-04-23 14:22:53 | [train_policy] epoch #622 | Computing loss after
2022-04-23 14:22:53 | [train_policy] epoch #622 | Fitting baseline...
2022-04-23 14:22:53 | [train_policy] epoch #622 | Saving snapshot...
2022-04-23 14:22:53 | [train_policy] epoch #622 | Saved
2022-04-23 14:22:53 | [train_policy] epoch #622 | Time 221.68 s
2022-04-23 14:22:53 | [train_policy] epoch #622 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.120413
Evaluation/AverageDiscountedReturn          -40.8132
Evaluation/AverageReturn                    -40.8132
Evaluation/CompletionRate                     0
Evaluation/Iteration                        622
Evaluation/MaxReturn                        -32.5949
Evaluation/MinReturn                        -64.0743
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.0849
Extras/EpisodeRewardMean                    -40.8477
LinearFeatureBaseline/ExplainedVariance       0.922548
PolicyExecTime                                0.110087
ProcessExecTime                               0.011126
TotalEnvSteps                            630476
policy/Entropy                               -1.0718
policy/KL                                     0.00668761
policy/KLBefore                               0
policy/LossAfter                             -0.0124932
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.342392
policy/dLoss                                  0.0124932
---------------------------------------  ----------------
2022-04-23 14:22:53 | [train_policy] epoch #623 | Obtaining samples for iteration 623...
2022-04-23 14:22:54 | [train_policy] epoch #623 | Logging diagnostics...
2022-04-23 14:22:54 | [train_policy] epoch #623 | Optimizing policy...
2022-04-23 14:22:54 | [train_policy] epoch #623 | Computing loss before
2022-04-23 14:22:54 | [train_policy] epoch #623 | Computing KL before
2022-04-23 14:22:54 | [train_policy] epoch #623 | Optimizing
2022-04-23 14:22:54 | [train_policy] epoch #623 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:54 | [train_policy] epoch #623 | computing loss before
2022-04-23 14:22:54 | [train_policy] epoch #623 | computing gradient
2022-04-23 14:22:54 | [train_policy] epoch #623 | gradient computed
2022-04-23 14:22:54 | [train_policy] epoch #623 | computing descent direction
2022-04-23 14:22:54 | [train_policy] epoch #623 | descent direction computed
2022-04-23 14:22:54 | [train_policy] epoch #623 | backtrack iters: 0
2022-04-23 14:22:54 | [train_policy] epoch #623 | optimization finished
2022-04-23 14:22:54 | [train_policy] epoch #623 | Computing KL after
2022-04-23 14:22:54 | [train_policy] epoch #623 | Computing loss after
2022-04-23 14:22:54 | [train_policy] epoch #623 | Fitting baseline...
2022-04-23 14:22:54 | [train_policy] epoch #623 | Saving snapshot...
2022-04-23 14:22:54 | [train_policy] epoch #623 | Saved
2022-04-23 14:22:54 | [train_policy] epoch #623 | Time 222.02 s
2022-04-23 14:22:54 | [train_policy] epoch #623 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11912
Evaluation/AverageDiscountedReturn          -41.0266
Evaluation/AverageReturn                    -41.0266
Evaluation/CompletionRate                     0
Evaluation/Iteration                        623
Evaluation/MaxReturn                        -32.4526
Evaluation/MinReturn                        -72.703
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.36334
Extras/EpisodeRewardMean                    -41.3274
LinearFeatureBaseline/ExplainedVariance       0.891497
PolicyExecTime                                0.105193
ProcessExecTime                               0.0116062
TotalEnvSteps                            631488
policy/Entropy                               -1.01183
policy/KL                                     0.00950456
policy/KLBefore                               0
policy/LossAfter                             -0.0215225
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.363552
policy/dLoss                                  0.0215225
---------------------------------------  ----------------
2022-04-23 14:22:54 | [train_policy] epoch #624 | Obtaining samples for iteration 624...
2022-04-23 14:22:54 | [train_policy] epoch #624 | Logging diagnostics...
2022-04-23 14:22:54 | [train_policy] epoch #624 | Optimizing policy...
2022-04-23 14:22:54 | [train_policy] epoch #624 | Computing loss before
2022-04-23 14:22:54 | [train_policy] epoch #624 | Computing KL before
2022-04-23 14:22:54 | [train_policy] epoch #624 | Optimizing
2022-04-23 14:22:54 | [train_policy] epoch #624 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:54 | [train_policy] epoch #624 | computing loss before
2022-04-23 14:22:54 | [train_policy] epoch #624 | computing gradient
2022-04-23 14:22:54 | [train_policy] epoch #624 | gradient computed
2022-04-23 14:22:54 | [train_policy] epoch #624 | computing descent direction
2022-04-23 14:22:54 | [train_policy] epoch #624 | descent direction computed
2022-04-23 14:22:54 | [train_policy] epoch #624 | backtrack iters: 1
2022-04-23 14:22:54 | [train_policy] epoch #624 | optimization finished
2022-04-23 14:22:54 | [train_policy] epoch #624 | Computing KL after
2022-04-23 14:22:54 | [train_policy] epoch #624 | Computing loss after
2022-04-23 14:22:54 | [train_policy] epoch #624 | Fitting baseline...
2022-04-23 14:22:54 | [train_policy] epoch #624 | Saving snapshot...
2022-04-23 14:22:54 | [train_policy] epoch #624 | Saved
2022-04-23 14:22:54 | [train_policy] epoch #624 | Time 222.37 s
2022-04-23 14:22:54 | [train_policy] epoch #624 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119181
Evaluation/AverageDiscountedReturn          -40.9072
Evaluation/AverageReturn                    -40.9072
Evaluation/CompletionRate                     0
Evaluation/Iteration                        624
Evaluation/MaxReturn                        -31.0905
Evaluation/MinReturn                        -64.1609
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.25541
Extras/EpisodeRewardMean                    -40.7234
LinearFeatureBaseline/ExplainedVariance       0.870733
PolicyExecTime                                0.100742
ProcessExecTime                               0.0112431
TotalEnvSteps                            632500
policy/Entropy                               -1.08142
policy/KL                                     0.00693469
policy/KLBefore                               0
policy/LossAfter                             -0.0112248
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.339112
policy/dLoss                                  0.0112248
---------------------------------------  ----------------
2022-04-23 14:22:54 | [train_policy] epoch #625 | Obtaining samples for iteration 625...
2022-04-23 14:22:54 | [train_policy] epoch #625 | Logging diagnostics...
2022-04-23 14:22:54 | [train_policy] epoch #625 | Optimizing policy...
2022-04-23 14:22:54 | [train_policy] epoch #625 | Computing loss before
2022-04-23 14:22:54 | [train_policy] epoch #625 | Computing KL before
2022-04-23 14:22:54 | [train_policy] epoch #625 | Optimizing
2022-04-23 14:22:54 | [train_policy] epoch #625 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:54 | [train_policy] epoch #625 | computing loss before
2022-04-23 14:22:54 | [train_policy] epoch #625 | computing gradient
2022-04-23 14:22:54 | [train_policy] epoch #625 | gradient computed
2022-04-23 14:22:54 | [train_policy] epoch #625 | computing descent direction
2022-04-23 14:22:54 | [train_policy] epoch #625 | descent direction computed
2022-04-23 14:22:54 | [train_policy] epoch #625 | backtrack iters: 0
2022-04-23 14:22:54 | [train_policy] epoch #625 | optimization finished
2022-04-23 14:22:54 | [train_policy] epoch #625 | Computing KL after
2022-04-23 14:22:54 | [train_policy] epoch #625 | Computing loss after
2022-04-23 14:22:54 | [train_policy] epoch #625 | Fitting baseline...
2022-04-23 14:22:54 | [train_policy] epoch #625 | Saving snapshot...
2022-04-23 14:22:54 | [train_policy] epoch #625 | Saved
2022-04-23 14:22:54 | [train_policy] epoch #625 | Time 222.71 s
2022-04-23 14:22:54 | [train_policy] epoch #625 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119386
Evaluation/AverageDiscountedReturn          -42.0889
Evaluation/AverageReturn                    -42.0889
Evaluation/CompletionRate                     0
Evaluation/Iteration                        625
Evaluation/MaxReturn                        -33.782
Evaluation/MinReturn                        -73.3688
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.13718
Extras/EpisodeRewardMean                    -41.6751
LinearFeatureBaseline/ExplainedVariance       0.872723
PolicyExecTime                                0.0993445
ProcessExecTime                               0.0114665
TotalEnvSteps                            633512
policy/Entropy                               -1.05018
policy/KL                                     0.00902794
policy/KLBefore                               0
policy/LossAfter                             -0.0203194
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.349874
policy/dLoss                                  0.0203194
---------------------------------------  ----------------
2022-04-23 14:22:55 | [train_policy] epoch #626 | Obtaining samples for iteration 626...
2022-04-23 14:22:55 | [train_policy] epoch #626 | Logging diagnostics...
2022-04-23 14:22:55 | [train_policy] epoch #626 | Optimizing policy...
2022-04-23 14:22:55 | [train_policy] epoch #626 | Computing loss before
2022-04-23 14:22:55 | [train_policy] epoch #626 | Computing KL before
2022-04-23 14:22:55 | [train_policy] epoch #626 | Optimizing
2022-04-23 14:22:55 | [train_policy] epoch #626 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:55 | [train_policy] epoch #626 | computing loss before
2022-04-23 14:22:55 | [train_policy] epoch #626 | computing gradient
2022-04-23 14:22:55 | [train_policy] epoch #626 | gradient computed
2022-04-23 14:22:55 | [train_policy] epoch #626 | computing descent direction
2022-04-23 14:22:55 | [train_policy] epoch #626 | descent direction computed
2022-04-23 14:22:55 | [train_policy] epoch #626 | backtrack iters: 1
2022-04-23 14:22:55 | [train_policy] epoch #626 | optimization finished
2022-04-23 14:22:55 | [train_policy] epoch #626 | Computing KL after
2022-04-23 14:22:55 | [train_policy] epoch #626 | Computing loss after
2022-04-23 14:22:55 | [train_policy] epoch #626 | Fitting baseline...
2022-04-23 14:22:55 | [train_policy] epoch #626 | Saving snapshot...
2022-04-23 14:22:55 | [train_policy] epoch #626 | Saved
2022-04-23 14:22:55 | [train_policy] epoch #626 | Time 223.05 s
2022-04-23 14:22:55 | [train_policy] epoch #626 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119708
Evaluation/AverageDiscountedReturn          -41.8269
Evaluation/AverageReturn                    -41.8269
Evaluation/CompletionRate                     0
Evaluation/Iteration                        626
Evaluation/MaxReturn                        -32.0243
Evaluation/MinReturn                        -77.0722
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.46466
Extras/EpisodeRewardMean                    -41.5028
LinearFeatureBaseline/ExplainedVariance       0.865418
PolicyExecTime                                0.101376
ProcessExecTime                               0.0115318
TotalEnvSteps                            634524
policy/Entropy                               -1.04725
policy/KL                                     0.0066972
policy/KLBefore                               0
policy/LossAfter                             -0.0221436
policy/LossBefore                             4.12285e-09
policy/Perplexity                             0.350901
policy/dLoss                                  0.0221436
---------------------------------------  ----------------
2022-04-23 14:22:55 | [train_policy] epoch #627 | Obtaining samples for iteration 627...
2022-04-23 14:22:55 | [train_policy] epoch #627 | Logging diagnostics...
2022-04-23 14:22:55 | [train_policy] epoch #627 | Optimizing policy...
2022-04-23 14:22:55 | [train_policy] epoch #627 | Computing loss before
2022-04-23 14:22:55 | [train_policy] epoch #627 | Computing KL before
2022-04-23 14:22:55 | [train_policy] epoch #627 | Optimizing
2022-04-23 14:22:55 | [train_policy] epoch #627 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:55 | [train_policy] epoch #627 | computing loss before
2022-04-23 14:22:55 | [train_policy] epoch #627 | computing gradient
2022-04-23 14:22:55 | [train_policy] epoch #627 | gradient computed
2022-04-23 14:22:55 | [train_policy] epoch #627 | computing descent direction
2022-04-23 14:22:55 | [train_policy] epoch #627 | descent direction computed
2022-04-23 14:22:55 | [train_policy] epoch #627 | backtrack iters: 1
2022-04-23 14:22:55 | [train_policy] epoch #627 | optimization finished
2022-04-23 14:22:55 | [train_policy] epoch #627 | Computing KL after
2022-04-23 14:22:55 | [train_policy] epoch #627 | Computing loss after
2022-04-23 14:22:55 | [train_policy] epoch #627 | Fitting baseline...
2022-04-23 14:22:55 | [train_policy] epoch #627 | Saving snapshot...
2022-04-23 14:22:55 | [train_policy] epoch #627 | Saved
2022-04-23 14:22:55 | [train_policy] epoch #627 | Time 223.39 s
2022-04-23 14:22:55 | [train_policy] epoch #627 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119721
Evaluation/AverageDiscountedReturn          -42.6225
Evaluation/AverageReturn                    -42.6225
Evaluation/CompletionRate                     0
Evaluation/Iteration                        627
Evaluation/MaxReturn                        -32.9011
Evaluation/MinReturn                        -74.5274
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.36344
Extras/EpisodeRewardMean                    -42.8482
LinearFeatureBaseline/ExplainedVariance       0.885456
PolicyExecTime                                0.100949
ProcessExecTime                               0.0113657
TotalEnvSteps                            635536
policy/Entropy                               -1.08857
policy/KL                                     0.00711774
policy/KLBefore                               0
policy/LossAfter                             -0.0245879
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.336697
policy/dLoss                                  0.0245879
---------------------------------------  ----------------
2022-04-23 14:22:55 | [train_policy] epoch #628 | Obtaining samples for iteration 628...
2022-04-23 14:22:55 | [train_policy] epoch #628 | Logging diagnostics...
2022-04-23 14:22:55 | [train_policy] epoch #628 | Optimizing policy...
2022-04-23 14:22:55 | [train_policy] epoch #628 | Computing loss before
2022-04-23 14:22:55 | [train_policy] epoch #628 | Computing KL before
2022-04-23 14:22:55 | [train_policy] epoch #628 | Optimizing
2022-04-23 14:22:55 | [train_policy] epoch #628 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:55 | [train_policy] epoch #628 | computing loss before
2022-04-23 14:22:55 | [train_policy] epoch #628 | computing gradient
2022-04-23 14:22:55 | [train_policy] epoch #628 | gradient computed
2022-04-23 14:22:55 | [train_policy] epoch #628 | computing descent direction
2022-04-23 14:22:56 | [train_policy] epoch #628 | descent direction computed
2022-04-23 14:22:56 | [train_policy] epoch #628 | backtrack iters: 1
2022-04-23 14:22:56 | [train_policy] epoch #628 | optimization finished
2022-04-23 14:22:56 | [train_policy] epoch #628 | Computing KL after
2022-04-23 14:22:56 | [train_policy] epoch #628 | Computing loss after
2022-04-23 14:22:56 | [train_policy] epoch #628 | Fitting baseline...
2022-04-23 14:22:56 | [train_policy] epoch #628 | Saving snapshot...
2022-04-23 14:22:56 | [train_policy] epoch #628 | Saved
2022-04-23 14:22:56 | [train_policy] epoch #628 | Time 223.74 s
2022-04-23 14:22:56 | [train_policy] epoch #628 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117099
Evaluation/AverageDiscountedReturn          -40.8181
Evaluation/AverageReturn                    -40.8181
Evaluation/CompletionRate                     0
Evaluation/Iteration                        628
Evaluation/MaxReturn                        -32.4016
Evaluation/MinReturn                        -63.8103
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.54515
Extras/EpisodeRewardMean                    -40.5863
LinearFeatureBaseline/ExplainedVariance       0.876716
PolicyExecTime                                0.104729
ProcessExecTime                               0.0118613
TotalEnvSteps                            636548
policy/Entropy                               -1.10468
policy/KL                                     0.00646242
policy/KLBefore                               0
policy/LossAfter                             -0.0103826
policy/LossBefore                             4.82963e-09
policy/Perplexity                             0.331317
policy/dLoss                                  0.0103826
---------------------------------------  ----------------
2022-04-23 14:22:56 | [train_policy] epoch #629 | Obtaining samples for iteration 629...
2022-04-23 14:22:56 | [train_policy] epoch #629 | Logging diagnostics...
2022-04-23 14:22:56 | [train_policy] epoch #629 | Optimizing policy...
2022-04-23 14:22:56 | [train_policy] epoch #629 | Computing loss before
2022-04-23 14:22:56 | [train_policy] epoch #629 | Computing KL before
2022-04-23 14:22:56 | [train_policy] epoch #629 | Optimizing
2022-04-23 14:22:56 | [train_policy] epoch #629 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:56 | [train_policy] epoch #629 | computing loss before
2022-04-23 14:22:56 | [train_policy] epoch #629 | computing gradient
2022-04-23 14:22:56 | [train_policy] epoch #629 | gradient computed
2022-04-23 14:22:56 | [train_policy] epoch #629 | computing descent direction
2022-04-23 14:22:56 | [train_policy] epoch #629 | descent direction computed
2022-04-23 14:22:56 | [train_policy] epoch #629 | backtrack iters: 1
2022-04-23 14:22:56 | [train_policy] epoch #629 | optimization finished
2022-04-23 14:22:56 | [train_policy] epoch #629 | Computing KL after
2022-04-23 14:22:56 | [train_policy] epoch #629 | Computing loss after
2022-04-23 14:22:56 | [train_policy] epoch #629 | Fitting baseline...
2022-04-23 14:22:56 | [train_policy] epoch #629 | Saving snapshot...
2022-04-23 14:22:56 | [train_policy] epoch #629 | Saved
2022-04-23 14:22:56 | [train_policy] epoch #629 | Time 224.11 s
2022-04-23 14:22:56 | [train_policy] epoch #629 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.131459
Evaluation/AverageDiscountedReturn          -42.3856
Evaluation/AverageReturn                    -42.3856
Evaluation/CompletionRate                     0
Evaluation/Iteration                        629
Evaluation/MaxReturn                        -33.0871
Evaluation/MinReturn                        -73.0728
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77871
Extras/EpisodeRewardMean                    -42.0197
LinearFeatureBaseline/ExplainedVariance       0.846805
PolicyExecTime                                0.108955
ProcessExecTime                               0.0131345
TotalEnvSteps                            637560
policy/Entropy                               -1.15515
policy/KL                                     0.00648781
policy/KLBefore                               0
policy/LossAfter                             -0.0165426
policy/LossBefore                            -1.59024e-09
policy/Perplexity                             0.315009
policy/dLoss                                  0.0165426
---------------------------------------  ----------------
2022-04-23 14:22:56 | [train_policy] epoch #630 | Obtaining samples for iteration 630...
2022-04-23 14:22:56 | [train_policy] epoch #630 | Logging diagnostics...
2022-04-23 14:22:56 | [train_policy] epoch #630 | Optimizing policy...
2022-04-23 14:22:56 | [train_policy] epoch #630 | Computing loss before
2022-04-23 14:22:56 | [train_policy] epoch #630 | Computing KL before
2022-04-23 14:22:56 | [train_policy] epoch #630 | Optimizing
2022-04-23 14:22:56 | [train_policy] epoch #630 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:56 | [train_policy] epoch #630 | computing loss before
2022-04-23 14:22:56 | [train_policy] epoch #630 | computing gradient
2022-04-23 14:22:56 | [train_policy] epoch #630 | gradient computed
2022-04-23 14:22:56 | [train_policy] epoch #630 | computing descent direction
2022-04-23 14:22:56 | [train_policy] epoch #630 | descent direction computed
2022-04-23 14:22:56 | [train_policy] epoch #630 | backtrack iters: 1
2022-04-23 14:22:56 | [train_policy] epoch #630 | optimization finished
2022-04-23 14:22:56 | [train_policy] epoch #630 | Computing KL after
2022-04-23 14:22:56 | [train_policy] epoch #630 | Computing loss after
2022-04-23 14:22:56 | [train_policy] epoch #630 | Fitting baseline...
2022-04-23 14:22:56 | [train_policy] epoch #630 | Saving snapshot...
2022-04-23 14:22:56 | [train_policy] epoch #630 | Saved
2022-04-23 14:22:56 | [train_policy] epoch #630 | Time 224.49 s
2022-04-23 14:22:56 | [train_policy] epoch #630 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.120596
Evaluation/AverageDiscountedReturn          -41.303
Evaluation/AverageReturn                    -41.303
Evaluation/CompletionRate                     0
Evaluation/Iteration                        630
Evaluation/MaxReturn                        -31.9855
Evaluation/MinReturn                        -73.3882
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.81276
Extras/EpisodeRewardMean                    -41.3144
LinearFeatureBaseline/ExplainedVariance       0.877462
PolicyExecTime                                0.115699
ProcessExecTime                               0.0126257
TotalEnvSteps                            638572
policy/Entropy                               -1.12144
policy/KL                                     0.00658043
policy/KLBefore                               0
policy/LossAfter                             -0.0145878
policy/LossBefore                            -8.59909e-09
policy/Perplexity                             0.325811
policy/dLoss                                  0.0145878
---------------------------------------  ----------------
2022-04-23 14:22:56 | [train_policy] epoch #631 | Obtaining samples for iteration 631...
2022-04-23 14:22:57 | [train_policy] epoch #631 | Logging diagnostics...
2022-04-23 14:22:57 | [train_policy] epoch #631 | Optimizing policy...
2022-04-23 14:22:57 | [train_policy] epoch #631 | Computing loss before
2022-04-23 14:22:57 | [train_policy] epoch #631 | Computing KL before
2022-04-23 14:22:57 | [train_policy] epoch #631 | Optimizing
2022-04-23 14:22:57 | [train_policy] epoch #631 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:57 | [train_policy] epoch #631 | computing loss before
2022-04-23 14:22:57 | [train_policy] epoch #631 | computing gradient
2022-04-23 14:22:57 | [train_policy] epoch #631 | gradient computed
2022-04-23 14:22:57 | [train_policy] epoch #631 | computing descent direction
2022-04-23 14:22:57 | [train_policy] epoch #631 | descent direction computed
2022-04-23 14:22:57 | [train_policy] epoch #631 | backtrack iters: 1
2022-04-23 14:22:57 | [train_policy] epoch #631 | optimization finished
2022-04-23 14:22:57 | [train_policy] epoch #631 | Computing KL after
2022-04-23 14:22:57 | [train_policy] epoch #631 | Computing loss after
2022-04-23 14:22:57 | [train_policy] epoch #631 | Fitting baseline...
2022-04-23 14:22:57 | [train_policy] epoch #631 | Saving snapshot...
2022-04-23 14:22:57 | [train_policy] epoch #631 | Saved
2022-04-23 14:22:57 | [train_policy] epoch #631 | Time 224.84 s
2022-04-23 14:22:57 | [train_policy] epoch #631 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.116108
Evaluation/AverageDiscountedReturn          -42.4158
Evaluation/AverageReturn                    -42.4158
Evaluation/CompletionRate                     0
Evaluation/Iteration                        631
Evaluation/MaxReturn                        -31.9024
Evaluation/MinReturn                        -73.9692
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.53493
Extras/EpisodeRewardMean                    -42.4664
LinearFeatureBaseline/ExplainedVariance       0.854294
PolicyExecTime                                0.109582
ProcessExecTime                               0.0113497
TotalEnvSteps                            639584
policy/Entropy                               -1.16711
policy/KL                                     0.00668865
policy/KLBefore                               0
policy/LossAfter                             -0.0194888
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.311264
policy/dLoss                                  0.0194888
---------------------------------------  ----------------
2022-04-23 14:22:57 | [train_policy] epoch #632 | Obtaining samples for iteration 632...
2022-04-23 14:22:57 | [train_policy] epoch #632 | Logging diagnostics...
2022-04-23 14:22:57 | [train_policy] epoch #632 | Optimizing policy...
2022-04-23 14:22:57 | [train_policy] epoch #632 | Computing loss before
2022-04-23 14:22:57 | [train_policy] epoch #632 | Computing KL before
2022-04-23 14:22:57 | [train_policy] epoch #632 | Optimizing
2022-04-23 14:22:57 | [train_policy] epoch #632 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:57 | [train_policy] epoch #632 | computing loss before
2022-04-23 14:22:57 | [train_policy] epoch #632 | computing gradient
2022-04-23 14:22:57 | [train_policy] epoch #632 | gradient computed
2022-04-23 14:22:57 | [train_policy] epoch #632 | computing descent direction
2022-04-23 14:22:57 | [train_policy] epoch #632 | descent direction computed
2022-04-23 14:22:57 | [train_policy] epoch #632 | backtrack iters: 0
2022-04-23 14:22:57 | [train_policy] epoch #632 | optimization finished
2022-04-23 14:22:57 | [train_policy] epoch #632 | Computing KL after
2022-04-23 14:22:57 | [train_policy] epoch #632 | Computing loss after
2022-04-23 14:22:57 | [train_policy] epoch #632 | Fitting baseline...
2022-04-23 14:22:57 | [train_policy] epoch #632 | Saving snapshot...
2022-04-23 14:22:57 | [train_policy] epoch #632 | Saved
2022-04-23 14:22:57 | [train_policy] epoch #632 | Time 225.22 s
2022-04-23 14:22:57 | [train_policy] epoch #632 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.118974
Evaluation/AverageDiscountedReturn          -41.7529
Evaluation/AverageReturn                    -41.7529
Evaluation/CompletionRate                     0
Evaluation/Iteration                        632
Evaluation/MaxReturn                        -32.7969
Evaluation/MinReturn                        -73.3707
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.23189
Extras/EpisodeRewardMean                    -41.9917
LinearFeatureBaseline/ExplainedVariance       0.874453
PolicyExecTime                                0.140948
ProcessExecTime                               0.011379
TotalEnvSteps                            640596
policy/Entropy                               -1.12869
policy/KL                                     0.00984128
policy/KLBefore                               0
policy/LossAfter                             -0.0238078
policy/LossBefore                            -1.41355e-08
policy/Perplexity                             0.323456
policy/dLoss                                  0.0238078
---------------------------------------  ----------------
2022-04-23 14:22:57 | [train_policy] epoch #633 | Obtaining samples for iteration 633...
2022-04-23 14:22:57 | [train_policy] epoch #633 | Logging diagnostics...
2022-04-23 14:22:57 | [train_policy] epoch #633 | Optimizing policy...
2022-04-23 14:22:57 | [train_policy] epoch #633 | Computing loss before
2022-04-23 14:22:57 | [train_policy] epoch #633 | Computing KL before
2022-04-23 14:22:57 | [train_policy] epoch #633 | Optimizing
2022-04-23 14:22:57 | [train_policy] epoch #633 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:57 | [train_policy] epoch #633 | computing loss before
2022-04-23 14:22:57 | [train_policy] epoch #633 | computing gradient
2022-04-23 14:22:57 | [train_policy] epoch #633 | gradient computed
2022-04-23 14:22:57 | [train_policy] epoch #633 | computing descent direction
2022-04-23 14:22:57 | [train_policy] epoch #633 | descent direction computed
2022-04-23 14:22:57 | [train_policy] epoch #633 | backtrack iters: 0
2022-04-23 14:22:57 | [train_policy] epoch #633 | optimization finished
2022-04-23 14:22:57 | [train_policy] epoch #633 | Computing KL after
2022-04-23 14:22:57 | [train_policy] epoch #633 | Computing loss after
2022-04-23 14:22:57 | [train_policy] epoch #633 | Fitting baseline...
2022-04-23 14:22:57 | [train_policy] epoch #633 | Saving snapshot...
2022-04-23 14:22:57 | [train_policy] epoch #633 | Saved
2022-04-23 14:22:57 | [train_policy] epoch #633 | Time 225.57 s
2022-04-23 14:22:57 | [train_policy] epoch #633 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.123363
Evaluation/AverageDiscountedReturn          -41.1525
Evaluation/AverageReturn                    -41.1525
Evaluation/CompletionRate                     0
Evaluation/Iteration                        633
Evaluation/MaxReturn                        -30.5365
Evaluation/MinReturn                        -73.6379
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.75882
Extras/EpisodeRewardMean                    -41.8829
LinearFeatureBaseline/ExplainedVariance       0.881555
PolicyExecTime                                0.104767
ProcessExecTime                               0.0122011
TotalEnvSteps                            641608
policy/Entropy                               -1.11213
policy/KL                                     0.00977888
policy/KLBefore                               0
policy/LossAfter                             -0.0121631
policy/LossBefore                            -2.02609e-08
policy/Perplexity                             0.328858
policy/dLoss                                  0.0121631
---------------------------------------  ----------------
2022-04-23 14:22:57 | [train_policy] epoch #634 | Obtaining samples for iteration 634...
2022-04-23 14:22:58 | [train_policy] epoch #634 | Logging diagnostics...
2022-04-23 14:22:58 | [train_policy] epoch #634 | Optimizing policy...
2022-04-23 14:22:58 | [train_policy] epoch #634 | Computing loss before
2022-04-23 14:22:58 | [train_policy] epoch #634 | Computing KL before
2022-04-23 14:22:58 | [train_policy] epoch #634 | Optimizing
2022-04-23 14:22:58 | [train_policy] epoch #634 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:58 | [train_policy] epoch #634 | computing loss before
2022-04-23 14:22:58 | [train_policy] epoch #634 | computing gradient
2022-04-23 14:22:58 | [train_policy] epoch #634 | gradient computed
2022-04-23 14:22:58 | [train_policy] epoch #634 | computing descent direction
2022-04-23 14:22:58 | [train_policy] epoch #634 | descent direction computed
2022-04-23 14:22:58 | [train_policy] epoch #634 | backtrack iters: 0
2022-04-23 14:22:58 | [train_policy] epoch #634 | optimization finished
2022-04-23 14:22:58 | [train_policy] epoch #634 | Computing KL after
2022-04-23 14:22:58 | [train_policy] epoch #634 | Computing loss after
2022-04-23 14:22:58 | [train_policy] epoch #634 | Fitting baseline...
2022-04-23 14:22:58 | [train_policy] epoch #634 | Saving snapshot...
2022-04-23 14:22:58 | [train_policy] epoch #634 | Saved
2022-04-23 14:22:58 | [train_policy] epoch #634 | Time 225.91 s
2022-04-23 14:22:58 | [train_policy] epoch #634 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.120269
Evaluation/AverageDiscountedReturn          -41.3911
Evaluation/AverageReturn                    -41.3911
Evaluation/CompletionRate                     0
Evaluation/Iteration                        634
Evaluation/MaxReturn                        -32.7947
Evaluation/MinReturn                        -73.373
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.54274
Extras/EpisodeRewardMean                    -41.2353
LinearFeatureBaseline/ExplainedVariance       0.876088
PolicyExecTime                                0.0973775
ProcessExecTime                               0.0113573
TotalEnvSteps                            642620
policy/Entropy                               -1.13158
policy/KL                                     0.00971756
policy/KLBefore                               0
policy/LossAfter                             -0.0200914
policy/LossBefore                            -5.4186e-09
policy/Perplexity                             0.322523
policy/dLoss                                  0.0200914
---------------------------------------  ---------------
2022-04-23 14:22:58 | [train_policy] epoch #635 | Obtaining samples for iteration 635...
2022-04-23 14:22:58 | [train_policy] epoch #635 | Logging diagnostics...
2022-04-23 14:22:58 | [train_policy] epoch #635 | Optimizing policy...
2022-04-23 14:22:58 | [train_policy] epoch #635 | Computing loss before
2022-04-23 14:22:58 | [train_policy] epoch #635 | Computing KL before
2022-04-23 14:22:58 | [train_policy] epoch #635 | Optimizing
2022-04-23 14:22:58 | [train_policy] epoch #635 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:58 | [train_policy] epoch #635 | computing loss before
2022-04-23 14:22:58 | [train_policy] epoch #635 | computing gradient
2022-04-23 14:22:58 | [train_policy] epoch #635 | gradient computed
2022-04-23 14:22:58 | [train_policy] epoch #635 | computing descent direction
2022-04-23 14:22:58 | [train_policy] epoch #635 | descent direction computed
2022-04-23 14:22:58 | [train_policy] epoch #635 | backtrack iters: 1
2022-04-23 14:22:58 | [train_policy] epoch #635 | optimization finished
2022-04-23 14:22:58 | [train_policy] epoch #635 | Computing KL after
2022-04-23 14:22:58 | [train_policy] epoch #635 | Computing loss after
2022-04-23 14:22:58 | [train_policy] epoch #635 | Fitting baseline...
2022-04-23 14:22:58 | [train_policy] epoch #635 | Saving snapshot...
2022-04-23 14:22:58 | [train_policy] epoch #635 | Saved
2022-04-23 14:22:58 | [train_policy] epoch #635 | Time 226.26 s
2022-04-23 14:22:58 | [train_policy] epoch #635 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118091
Evaluation/AverageDiscountedReturn          -41.7177
Evaluation/AverageReturn                    -41.7177
Evaluation/CompletionRate                     0
Evaluation/Iteration                        635
Evaluation/MaxReturn                        -31.0766
Evaluation/MinReturn                        -74.6793
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.96906
Extras/EpisodeRewardMean                    -41.7656
LinearFeatureBaseline/ExplainedVariance       0.860434
PolicyExecTime                                0.104442
ProcessExecTime                               0.0115287
TotalEnvSteps                            643632
policy/Entropy                               -1.15973
policy/KL                                     0.00678554
policy/KLBefore                               0
policy/LossAfter                             -0.0213907
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.313569
policy/dLoss                                  0.0213907
---------------------------------------  ----------------
2022-04-23 14:22:58 | [train_policy] epoch #636 | Obtaining samples for iteration 636...
2022-04-23 14:22:58 | [train_policy] epoch #636 | Logging diagnostics...
2022-04-23 14:22:58 | [train_policy] epoch #636 | Optimizing policy...
2022-04-23 14:22:58 | [train_policy] epoch #636 | Computing loss before
2022-04-23 14:22:58 | [train_policy] epoch #636 | Computing KL before
2022-04-23 14:22:58 | [train_policy] epoch #636 | Optimizing
2022-04-23 14:22:58 | [train_policy] epoch #636 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:58 | [train_policy] epoch #636 | computing loss before
2022-04-23 14:22:58 | [train_policy] epoch #636 | computing gradient
2022-04-23 14:22:58 | [train_policy] epoch #636 | gradient computed
2022-04-23 14:22:58 | [train_policy] epoch #636 | computing descent direction
2022-04-23 14:22:58 | [train_policy] epoch #636 | descent direction computed
2022-04-23 14:22:58 | [train_policy] epoch #636 | backtrack iters: 1
2022-04-23 14:22:58 | [train_policy] epoch #636 | optimization finished
2022-04-23 14:22:58 | [train_policy] epoch #636 | Computing KL after
2022-04-23 14:22:58 | [train_policy] epoch #636 | Computing loss after
2022-04-23 14:22:58 | [train_policy] epoch #636 | Fitting baseline...
2022-04-23 14:22:58 | [train_policy] epoch #636 | Saving snapshot...
2022-04-23 14:22:58 | [train_policy] epoch #636 | Saved
2022-04-23 14:22:58 | [train_policy] epoch #636 | Time 226.62 s
2022-04-23 14:22:58 | [train_policy] epoch #636 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.124583
Evaluation/AverageDiscountedReturn          -41.5429
Evaluation/AverageReturn                    -41.5429
Evaluation/CompletionRate                     0
Evaluation/Iteration                        636
Evaluation/MaxReturn                        -32.7694
Evaluation/MinReturn                        -74.4857
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.2487
Extras/EpisodeRewardMean                    -41.3512
LinearFeatureBaseline/ExplainedVariance       0.836919
PolicyExecTime                                0.108409
ProcessExecTime                               0.0121672
TotalEnvSteps                            644644
policy/Entropy                               -1.1782
policy/KL                                     0.00682533
policy/KLBefore                               0
policy/LossAfter                             -0.0169574
policy/LossBefore                            -1.38999e-08
policy/Perplexity                             0.307831
policy/dLoss                                  0.0169574
---------------------------------------  ----------------
2022-04-23 14:22:58 | [train_policy] epoch #637 | Obtaining samples for iteration 637...
2022-04-23 14:22:59 | [train_policy] epoch #637 | Logging diagnostics...
2022-04-23 14:22:59 | [train_policy] epoch #637 | Optimizing policy...
2022-04-23 14:22:59 | [train_policy] epoch #637 | Computing loss before
2022-04-23 14:22:59 | [train_policy] epoch #637 | Computing KL before
2022-04-23 14:22:59 | [train_policy] epoch #637 | Optimizing
2022-04-23 14:22:59 | [train_policy] epoch #637 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:59 | [train_policy] epoch #637 | computing loss before
2022-04-23 14:22:59 | [train_policy] epoch #637 | computing gradient
2022-04-23 14:22:59 | [train_policy] epoch #637 | gradient computed
2022-04-23 14:22:59 | [train_policy] epoch #637 | computing descent direction
2022-04-23 14:22:59 | [train_policy] epoch #637 | descent direction computed
2022-04-23 14:22:59 | [train_policy] epoch #637 | backtrack iters: 0
2022-04-23 14:22:59 | [train_policy] epoch #637 | optimization finished
2022-04-23 14:22:59 | [train_policy] epoch #637 | Computing KL after
2022-04-23 14:22:59 | [train_policy] epoch #637 | Computing loss after
2022-04-23 14:22:59 | [train_policy] epoch #637 | Fitting baseline...
2022-04-23 14:22:59 | [train_policy] epoch #637 | Saving snapshot...
2022-04-23 14:22:59 | [train_policy] epoch #637 | Saved
2022-04-23 14:22:59 | [train_policy] epoch #637 | Time 226.97 s
2022-04-23 14:22:59 | [train_policy] epoch #637 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.131716
Evaluation/AverageDiscountedReturn          -40.5958
Evaluation/AverageReturn                    -40.5958
Evaluation/CompletionRate                     0
Evaluation/Iteration                        637
Evaluation/MaxReturn                        -31.7652
Evaluation/MinReturn                        -64.8298
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.56798
Extras/EpisodeRewardMean                    -40.4788
LinearFeatureBaseline/ExplainedVariance       0.883511
PolicyExecTime                                0.105455
ProcessExecTime                               0.0130913
TotalEnvSteps                            645656
policy/Entropy                               -1.10046
policy/KL                                     0.00940637
policy/KLBefore                               0
policy/LossAfter                             -0.0156511
policy/LossBefore                            -9.18807e-09
policy/Perplexity                             0.332716
policy/dLoss                                  0.0156511
---------------------------------------  ----------------
2022-04-23 14:22:59 | [train_policy] epoch #638 | Obtaining samples for iteration 638...
2022-04-23 14:22:59 | [train_policy] epoch #638 | Logging diagnostics...
2022-04-23 14:22:59 | [train_policy] epoch #638 | Optimizing policy...
2022-04-23 14:22:59 | [train_policy] epoch #638 | Computing loss before
2022-04-23 14:22:59 | [train_policy] epoch #638 | Computing KL before
2022-04-23 14:22:59 | [train_policy] epoch #638 | Optimizing
2022-04-23 14:22:59 | [train_policy] epoch #638 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:59 | [train_policy] epoch #638 | computing loss before
2022-04-23 14:22:59 | [train_policy] epoch #638 | computing gradient
2022-04-23 14:22:59 | [train_policy] epoch #638 | gradient computed
2022-04-23 14:22:59 | [train_policy] epoch #638 | computing descent direction
2022-04-23 14:22:59 | [train_policy] epoch #638 | descent direction computed
2022-04-23 14:22:59 | [train_policy] epoch #638 | backtrack iters: 0
2022-04-23 14:22:59 | [train_policy] epoch #638 | optimization finished
2022-04-23 14:22:59 | [train_policy] epoch #638 | Computing KL after
2022-04-23 14:22:59 | [train_policy] epoch #638 | Computing loss after
2022-04-23 14:22:59 | [train_policy] epoch #638 | Fitting baseline...
2022-04-23 14:22:59 | [train_policy] epoch #638 | Saving snapshot...
2022-04-23 14:22:59 | [train_policy] epoch #638 | Saved
2022-04-23 14:22:59 | [train_policy] epoch #638 | Time 227.31 s
2022-04-23 14:22:59 | [train_policy] epoch #638 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116287
Evaluation/AverageDiscountedReturn          -42.0723
Evaluation/AverageReturn                    -42.0723
Evaluation/CompletionRate                     0
Evaluation/Iteration                        638
Evaluation/MaxReturn                        -30.8543
Evaluation/MinReturn                        -73.4158
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.21754
Extras/EpisodeRewardMean                    -42.1408
LinearFeatureBaseline/ExplainedVariance       0.863243
PolicyExecTime                                0.112218
ProcessExecTime                               0.0115051
TotalEnvSteps                            646668
policy/Entropy                               -1.05351
policy/KL                                     0.00944176
policy/KLBefore                               0
policy/LossAfter                             -0.0186322
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             0.348712
policy/dLoss                                  0.0186322
---------------------------------------  ----------------
2022-04-23 14:22:59 | [train_policy] epoch #639 | Obtaining samples for iteration 639...
2022-04-23 14:22:59 | [train_policy] epoch #639 | Logging diagnostics...
2022-04-23 14:22:59 | [train_policy] epoch #639 | Optimizing policy...
2022-04-23 14:22:59 | [train_policy] epoch #639 | Computing loss before
2022-04-23 14:22:59 | [train_policy] epoch #639 | Computing KL before
2022-04-23 14:22:59 | [train_policy] epoch #639 | Optimizing
2022-04-23 14:22:59 | [train_policy] epoch #639 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:22:59 | [train_policy] epoch #639 | computing loss before
2022-04-23 14:22:59 | [train_policy] epoch #639 | computing gradient
2022-04-23 14:22:59 | [train_policy] epoch #639 | gradient computed
2022-04-23 14:22:59 | [train_policy] epoch #639 | computing descent direction
2022-04-23 14:22:59 | [train_policy] epoch #639 | descent direction computed
2022-04-23 14:22:59 | [train_policy] epoch #639 | backtrack iters: 0
2022-04-23 14:22:59 | [train_policy] epoch #639 | optimization finished
2022-04-23 14:22:59 | [train_policy] epoch #639 | Computing KL after
2022-04-23 14:22:59 | [train_policy] epoch #639 | Computing loss after
2022-04-23 14:22:59 | [train_policy] epoch #639 | Fitting baseline...
2022-04-23 14:22:59 | [train_policy] epoch #639 | Saving snapshot...
2022-04-23 14:22:59 | [train_policy] epoch #639 | Saved
2022-04-23 14:22:59 | [train_policy] epoch #639 | Time 227.64 s
2022-04-23 14:22:59 | [train_policy] epoch #639 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.11609
Evaluation/AverageDiscountedReturn          -40.8637
Evaluation/AverageReturn                    -40.8637
Evaluation/CompletionRate                     0
Evaluation/Iteration                        639
Evaluation/MaxReturn                        -31.1506
Evaluation/MinReturn                        -65.1723
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85362
Extras/EpisodeRewardMean                    -41.0002
LinearFeatureBaseline/ExplainedVariance       0.886917
PolicyExecTime                                0.106278
ProcessExecTime                               0.0112414
TotalEnvSteps                            647680
policy/Entropy                               -1.02365
policy/KL                                     0.00960054
policy/KLBefore                               0
policy/LossAfter                             -0.0182661
policy/LossBefore                            -5.5364e-09
policy/Perplexity                             0.359283
policy/dLoss                                  0.0182661
---------------------------------------  ---------------
2022-04-23 14:22:59 | [train_policy] epoch #640 | Obtaining samples for iteration 640...
2022-04-23 14:23:00 | [train_policy] epoch #640 | Logging diagnostics...
2022-04-23 14:23:00 | [train_policy] epoch #640 | Optimizing policy...
2022-04-23 14:23:00 | [train_policy] epoch #640 | Computing loss before
2022-04-23 14:23:00 | [train_policy] epoch #640 | Computing KL before
2022-04-23 14:23:00 | [train_policy] epoch #640 | Optimizing
2022-04-23 14:23:00 | [train_policy] epoch #640 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:00 | [train_policy] epoch #640 | computing loss before
2022-04-23 14:23:00 | [train_policy] epoch #640 | computing gradient
2022-04-23 14:23:00 | [train_policy] epoch #640 | gradient computed
2022-04-23 14:23:00 | [train_policy] epoch #640 | computing descent direction
2022-04-23 14:23:00 | [train_policy] epoch #640 | descent direction computed
2022-04-23 14:23:00 | [train_policy] epoch #640 | backtrack iters: 0
2022-04-23 14:23:00 | [train_policy] epoch #640 | optimization finished
2022-04-23 14:23:00 | [train_policy] epoch #640 | Computing KL after
2022-04-23 14:23:00 | [train_policy] epoch #640 | Computing loss after
2022-04-23 14:23:00 | [train_policy] epoch #640 | Fitting baseline...
2022-04-23 14:23:00 | [train_policy] epoch #640 | Saving snapshot...
2022-04-23 14:23:00 | [train_policy] epoch #640 | Saved
2022-04-23 14:23:00 | [train_policy] epoch #640 | Time 227.98 s
2022-04-23 14:23:00 | [train_policy] epoch #640 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.120098
Evaluation/AverageDiscountedReturn          -40.8316
Evaluation/AverageReturn                    -40.8316
Evaluation/CompletionRate                     0
Evaluation/Iteration                        640
Evaluation/MaxReturn                        -32.8149
Evaluation/MinReturn                        -65.1858
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.35387
Extras/EpisodeRewardMean                    -40.6188
LinearFeatureBaseline/ExplainedVariance       0.894198
PolicyExecTime                                0.108447
ProcessExecTime                               0.0113974
TotalEnvSteps                            648692
policy/Entropy                               -1.05179
policy/KL                                     0.0090003
policy/KLBefore                               0
policy/LossAfter                             -0.0151697
policy/LossBefore                            -2.94489e-09
policy/Perplexity                             0.349311
policy/dLoss                                  0.0151697
---------------------------------------  ----------------
2022-04-23 14:23:00 | [train_policy] epoch #641 | Obtaining samples for iteration 641...
2022-04-23 14:23:00 | [train_policy] epoch #641 | Logging diagnostics...
2022-04-23 14:23:00 | [train_policy] epoch #641 | Optimizing policy...
2022-04-23 14:23:00 | [train_policy] epoch #641 | Computing loss before
2022-04-23 14:23:00 | [train_policy] epoch #641 | Computing KL before
2022-04-23 14:23:00 | [train_policy] epoch #641 | Optimizing
2022-04-23 14:23:00 | [train_policy] epoch #641 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:00 | [train_policy] epoch #641 | computing loss before
2022-04-23 14:23:00 | [train_policy] epoch #641 | computing gradient
2022-04-23 14:23:00 | [train_policy] epoch #641 | gradient computed
2022-04-23 14:23:00 | [train_policy] epoch #641 | computing descent direction
2022-04-23 14:23:00 | [train_policy] epoch #641 | descent direction computed
2022-04-23 14:23:00 | [train_policy] epoch #641 | backtrack iters: 0
2022-04-23 14:23:00 | [train_policy] epoch #641 | optimization finished
2022-04-23 14:23:00 | [train_policy] epoch #641 | Computing KL after
2022-04-23 14:23:00 | [train_policy] epoch #641 | Computing loss after
2022-04-23 14:23:00 | [train_policy] epoch #641 | Fitting baseline...
2022-04-23 14:23:00 | [train_policy] epoch #641 | Saving snapshot...
2022-04-23 14:23:00 | [train_policy] epoch #641 | Saved
2022-04-23 14:23:00 | [train_policy] epoch #641 | Time 228.31 s
2022-04-23 14:23:00 | [train_policy] epoch #641 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119833
Evaluation/AverageDiscountedReturn          -74.4266
Evaluation/AverageReturn                    -74.4266
Evaluation/CompletionRate                     0
Evaluation/Iteration                        641
Evaluation/MaxReturn                        -32.8146
Evaluation/MinReturn                      -2059.31
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        232.686
Extras/EpisodeRewardMean                    -71.8368
LinearFeatureBaseline/ExplainedVariance       0.0153294
PolicyExecTime                                0.0948198
ProcessExecTime                               0.0112507
TotalEnvSteps                            649704
policy/Entropy                               -1.04272
policy/KL                                     0.00982269
policy/KLBefore                               0
policy/LossAfter                             -0.0306048
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.352495
policy/dLoss                                  0.0306048
---------------------------------------  ----------------
2022-04-23 14:23:00 | [train_policy] epoch #642 | Obtaining samples for iteration 642...
2022-04-23 14:23:00 | [train_policy] epoch #642 | Logging diagnostics...
2022-04-23 14:23:00 | [train_policy] epoch #642 | Optimizing policy...
2022-04-23 14:23:00 | [train_policy] epoch #642 | Computing loss before
2022-04-23 14:23:00 | [train_policy] epoch #642 | Computing KL before
2022-04-23 14:23:00 | [train_policy] epoch #642 | Optimizing
2022-04-23 14:23:00 | [train_policy] epoch #642 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:00 | [train_policy] epoch #642 | computing loss before
2022-04-23 14:23:00 | [train_policy] epoch #642 | computing gradient
2022-04-23 14:23:00 | [train_policy] epoch #642 | gradient computed
2022-04-23 14:23:00 | [train_policy] epoch #642 | computing descent direction
2022-04-23 14:23:00 | [train_policy] epoch #642 | descent direction computed
2022-04-23 14:23:00 | [train_policy] epoch #642 | backtrack iters: 0
2022-04-23 14:23:00 | [train_policy] epoch #642 | optimization finished
2022-04-23 14:23:00 | [train_policy] epoch #642 | Computing KL after
2022-04-23 14:23:00 | [train_policy] epoch #642 | Computing loss after
2022-04-23 14:23:00 | [train_policy] epoch #642 | Fitting baseline...
2022-04-23 14:23:00 | [train_policy] epoch #642 | Saving snapshot...
2022-04-23 14:23:00 | [train_policy] epoch #642 | Saved
2022-04-23 14:23:00 | [train_policy] epoch #642 | Time 228.64 s
2022-04-23 14:23:00 | [train_policy] epoch #642 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119581
Evaluation/AverageDiscountedReturn          -41.2609
Evaluation/AverageReturn                    -41.2609
Evaluation/CompletionRate                     0
Evaluation/Iteration                        642
Evaluation/MaxReturn                        -32.253
Evaluation/MinReturn                        -74.233
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.83355
Extras/EpisodeRewardMean                    -41.3402
LinearFeatureBaseline/ExplainedVariance     -34.7635
PolicyExecTime                                0.0993736
ProcessExecTime                               0.0112426
TotalEnvSteps                            650716
policy/Entropy                               -1.02109
policy/KL                                     0.00949851
policy/KLBefore                               0
policy/LossAfter                             -0.0600793
policy/LossBefore                            -3.53387e-09
policy/Perplexity                             0.360203
policy/dLoss                                  0.0600793
---------------------------------------  ----------------
2022-04-23 14:23:00 | [train_policy] epoch #643 | Obtaining samples for iteration 643...
2022-04-23 14:23:01 | [train_policy] epoch #643 | Logging diagnostics...
2022-04-23 14:23:01 | [train_policy] epoch #643 | Optimizing policy...
2022-04-23 14:23:01 | [train_policy] epoch #643 | Computing loss before
2022-04-23 14:23:01 | [train_policy] epoch #643 | Computing KL before
2022-04-23 14:23:01 | [train_policy] epoch #643 | Optimizing
2022-04-23 14:23:01 | [train_policy] epoch #643 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:01 | [train_policy] epoch #643 | computing loss before
2022-04-23 14:23:01 | [train_policy] epoch #643 | computing gradient
2022-04-23 14:23:01 | [train_policy] epoch #643 | gradient computed
2022-04-23 14:23:01 | [train_policy] epoch #643 | computing descent direction
2022-04-23 14:23:01 | [train_policy] epoch #643 | descent direction computed
2022-04-23 14:23:01 | [train_policy] epoch #643 | backtrack iters: 1
2022-04-23 14:23:01 | [train_policy] epoch #643 | optimization finished
2022-04-23 14:23:01 | [train_policy] epoch #643 | Computing KL after
2022-04-23 14:23:01 | [train_policy] epoch #643 | Computing loss after
2022-04-23 14:23:01 | [train_policy] epoch #643 | Fitting baseline...
2022-04-23 14:23:01 | [train_policy] epoch #643 | Saving snapshot...
2022-04-23 14:23:01 | [train_policy] epoch #643 | Saved
2022-04-23 14:23:01 | [train_policy] epoch #643 | Time 228.96 s
2022-04-23 14:23:01 | [train_policy] epoch #643 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118595
Evaluation/AverageDiscountedReturn          -40.7609
Evaluation/AverageReturn                    -40.7609
Evaluation/CompletionRate                     0
Evaluation/Iteration                        643
Evaluation/MaxReturn                        -32.2957
Evaluation/MinReturn                        -83.2766
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.36933
Extras/EpisodeRewardMean                    -41.0797
LinearFeatureBaseline/ExplainedVariance       0.795087
PolicyExecTime                                0.0948672
ProcessExecTime                               0.0113561
TotalEnvSteps                            651728
policy/Entropy                               -1.03009
policy/KL                                     0.00648065
policy/KLBefore                               0
policy/LossAfter                             -0.0158872
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.356976
policy/dLoss                                  0.0158872
---------------------------------------  ----------------
2022-04-23 14:23:01 | [train_policy] epoch #644 | Obtaining samples for iteration 644...
2022-04-23 14:23:01 | [train_policy] epoch #644 | Logging diagnostics...
2022-04-23 14:23:01 | [train_policy] epoch #644 | Optimizing policy...
2022-04-23 14:23:01 | [train_policy] epoch #644 | Computing loss before
2022-04-23 14:23:01 | [train_policy] epoch #644 | Computing KL before
2022-04-23 14:23:01 | [train_policy] epoch #644 | Optimizing
2022-04-23 14:23:01 | [train_policy] epoch #644 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:01 | [train_policy] epoch #644 | computing loss before
2022-04-23 14:23:01 | [train_policy] epoch #644 | computing gradient
2022-04-23 14:23:01 | [train_policy] epoch #644 | gradient computed
2022-04-23 14:23:01 | [train_policy] epoch #644 | computing descent direction
2022-04-23 14:23:01 | [train_policy] epoch #644 | descent direction computed
2022-04-23 14:23:01 | [train_policy] epoch #644 | backtrack iters: 1
2022-04-23 14:23:01 | [train_policy] epoch #644 | optimization finished
2022-04-23 14:23:01 | [train_policy] epoch #644 | Computing KL after
2022-04-23 14:23:01 | [train_policy] epoch #644 | Computing loss after
2022-04-23 14:23:01 | [train_policy] epoch #644 | Fitting baseline...
2022-04-23 14:23:01 | [train_policy] epoch #644 | Saving snapshot...
2022-04-23 14:23:01 | [train_policy] epoch #644 | Saved
2022-04-23 14:23:01 | [train_policy] epoch #644 | Time 229.30 s
2022-04-23 14:23:01 | [train_policy] epoch #644 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.121138
Evaluation/AverageDiscountedReturn          -42.0838
Evaluation/AverageReturn                    -42.0838
Evaluation/CompletionRate                     0
Evaluation/Iteration                        644
Evaluation/MaxReturn                        -32.1578
Evaluation/MinReturn                        -73.8054
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.82606
Extras/EpisodeRewardMean                    -42.0113
LinearFeatureBaseline/ExplainedVariance       0.877648
PolicyExecTime                                0.100416
ProcessExecTime                               0.0114071
TotalEnvSteps                            652740
policy/Entropy                               -1.07265
policy/KL                                     0.00651846
policy/KLBefore                               0
policy/LossAfter                             -0.0188536
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.342101
policy/dLoss                                  0.0188536
---------------------------------------  ----------------
2022-04-23 14:23:01 | [train_policy] epoch #645 | Obtaining samples for iteration 645...
2022-04-23 14:23:01 | [train_policy] epoch #645 | Logging diagnostics...
2022-04-23 14:23:01 | [train_policy] epoch #645 | Optimizing policy...
2022-04-23 14:23:01 | [train_policy] epoch #645 | Computing loss before
2022-04-23 14:23:01 | [train_policy] epoch #645 | Computing KL before
2022-04-23 14:23:01 | [train_policy] epoch #645 | Optimizing
2022-04-23 14:23:01 | [train_policy] epoch #645 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:01 | [train_policy] epoch #645 | computing loss before
2022-04-23 14:23:01 | [train_policy] epoch #645 | computing gradient
2022-04-23 14:23:01 | [train_policy] epoch #645 | gradient computed
2022-04-23 14:23:01 | [train_policy] epoch #645 | computing descent direction
2022-04-23 14:23:01 | [train_policy] epoch #645 | descent direction computed
2022-04-23 14:23:01 | [train_policy] epoch #645 | backtrack iters: 1
2022-04-23 14:23:01 | [train_policy] epoch #645 | optimization finished
2022-04-23 14:23:01 | [train_policy] epoch #645 | Computing KL after
2022-04-23 14:23:01 | [train_policy] epoch #645 | Computing loss after
2022-04-23 14:23:01 | [train_policy] epoch #645 | Fitting baseline...
2022-04-23 14:23:01 | [train_policy] epoch #645 | Saving snapshot...
2022-04-23 14:23:01 | [train_policy] epoch #645 | Saved
2022-04-23 14:23:01 | [train_policy] epoch #645 | Time 229.64 s
2022-04-23 14:23:01 | [train_policy] epoch #645 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119046
Evaluation/AverageDiscountedReturn          -41.6172
Evaluation/AverageReturn                    -41.6172
Evaluation/CompletionRate                     0
Evaluation/Iteration                        645
Evaluation/MaxReturn                        -30.0555
Evaluation/MinReturn                        -65.4937
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.06424
Extras/EpisodeRewardMean                    -41.7001
LinearFeatureBaseline/ExplainedVariance       0.882378
PolicyExecTime                                0.0980909
ProcessExecTime                               0.0115936
TotalEnvSteps                            653752
policy/Entropy                               -1.11582
policy/KL                                     0.00660216
policy/KLBefore                               0
policy/LossAfter                             -0.0136599
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.327647
policy/dLoss                                  0.0136599
---------------------------------------  ----------------
2022-04-23 14:23:01 | [train_policy] epoch #646 | Obtaining samples for iteration 646...
2022-04-23 14:23:02 | [train_policy] epoch #646 | Logging diagnostics...
2022-04-23 14:23:02 | [train_policy] epoch #646 | Optimizing policy...
2022-04-23 14:23:02 | [train_policy] epoch #646 | Computing loss before
2022-04-23 14:23:02 | [train_policy] epoch #646 | Computing KL before
2022-04-23 14:23:02 | [train_policy] epoch #646 | Optimizing
2022-04-23 14:23:02 | [train_policy] epoch #646 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:02 | [train_policy] epoch #646 | computing loss before
2022-04-23 14:23:02 | [train_policy] epoch #646 | computing gradient
2022-04-23 14:23:02 | [train_policy] epoch #646 | gradient computed
2022-04-23 14:23:02 | [train_policy] epoch #646 | computing descent direction
2022-04-23 14:23:02 | [train_policy] epoch #646 | descent direction computed
2022-04-23 14:23:02 | [train_policy] epoch #646 | backtrack iters: 1
2022-04-23 14:23:02 | [train_policy] epoch #646 | optimization finished
2022-04-23 14:23:02 | [train_policy] epoch #646 | Computing KL after
2022-04-23 14:23:02 | [train_policy] epoch #646 | Computing loss after
2022-04-23 14:23:02 | [train_policy] epoch #646 | Fitting baseline...
2022-04-23 14:23:02 | [train_policy] epoch #646 | Saving snapshot...
2022-04-23 14:23:02 | [train_policy] epoch #646 | Saved
2022-04-23 14:23:02 | [train_policy] epoch #646 | Time 229.96 s
2022-04-23 14:23:02 | [train_policy] epoch #646 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119149
Evaluation/AverageDiscountedReturn          -41.4206
Evaluation/AverageReturn                    -41.4206
Evaluation/CompletionRate                     0
Evaluation/Iteration                        646
Evaluation/MaxReturn                        -31.5024
Evaluation/MinReturn                        -76.2174
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.89635
Extras/EpisodeRewardMean                    -41.4765
LinearFeatureBaseline/ExplainedVariance       0.875607
PolicyExecTime                                0.0943971
ProcessExecTime                               0.0111353
TotalEnvSteps                            654764
policy/Entropy                               -1.16754
policy/KL                                     0.00659128
policy/KLBefore                               0
policy/LossAfter                             -0.0208493
policy/LossBefore                            -6.24317e-09
policy/Perplexity                             0.311131
policy/dLoss                                  0.0208493
---------------------------------------  ----------------
2022-04-23 14:23:02 | [train_policy] epoch #647 | Obtaining samples for iteration 647...
2022-04-23 14:23:02 | [train_policy] epoch #647 | Logging diagnostics...
2022-04-23 14:23:02 | [train_policy] epoch #647 | Optimizing policy...
2022-04-23 14:23:02 | [train_policy] epoch #647 | Computing loss before
2022-04-23 14:23:02 | [train_policy] epoch #647 | Computing KL before
2022-04-23 14:23:02 | [train_policy] epoch #647 | Optimizing
2022-04-23 14:23:02 | [train_policy] epoch #647 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:02 | [train_policy] epoch #647 | computing loss before
2022-04-23 14:23:02 | [train_policy] epoch #647 | computing gradient
2022-04-23 14:23:02 | [train_policy] epoch #647 | gradient computed
2022-04-23 14:23:02 | [train_policy] epoch #647 | computing descent direction
2022-04-23 14:23:02 | [train_policy] epoch #647 | descent direction computed
2022-04-23 14:23:02 | [train_policy] epoch #647 | backtrack iters: 1
2022-04-23 14:23:02 | [train_policy] epoch #647 | optimization finished
2022-04-23 14:23:02 | [train_policy] epoch #647 | Computing KL after
2022-04-23 14:23:02 | [train_policy] epoch #647 | Computing loss after
2022-04-23 14:23:02 | [train_policy] epoch #647 | Fitting baseline...
2022-04-23 14:23:02 | [train_policy] epoch #647 | Saving snapshot...
2022-04-23 14:23:02 | [train_policy] epoch #647 | Saved
2022-04-23 14:23:02 | [train_policy] epoch #647 | Time 230.29 s
2022-04-23 14:23:02 | [train_policy] epoch #647 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119218
Evaluation/AverageDiscountedReturn          -40.4264
Evaluation/AverageReturn                    -40.4264
Evaluation/CompletionRate                     0
Evaluation/Iteration                        647
Evaluation/MaxReturn                        -32.9253
Evaluation/MinReturn                        -58.7728
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.46465
Extras/EpisodeRewardMean                    -40.7609
LinearFeatureBaseline/ExplainedVariance       0.902593
PolicyExecTime                                0.0945644
ProcessExecTime                               0.0112543
TotalEnvSteps                            655776
policy/Entropy                               -1.18088
policy/KL                                     0.0065934
policy/KLBefore                               0
policy/LossAfter                             -0.0268926
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.307007
policy/dLoss                                  0.0268926
---------------------------------------  ----------------
2022-04-23 14:23:02 | [train_policy] epoch #648 | Obtaining samples for iteration 648...
2022-04-23 14:23:02 | [train_policy] epoch #648 | Logging diagnostics...
2022-04-23 14:23:02 | [train_policy] epoch #648 | Optimizing policy...
2022-04-23 14:23:02 | [train_policy] epoch #648 | Computing loss before
2022-04-23 14:23:02 | [train_policy] epoch #648 | Computing KL before
2022-04-23 14:23:02 | [train_policy] epoch #648 | Optimizing
2022-04-23 14:23:02 | [train_policy] epoch #648 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:02 | [train_policy] epoch #648 | computing loss before
2022-04-23 14:23:02 | [train_policy] epoch #648 | computing gradient
2022-04-23 14:23:02 | [train_policy] epoch #648 | gradient computed
2022-04-23 14:23:02 | [train_policy] epoch #648 | computing descent direction
2022-04-23 14:23:02 | [train_policy] epoch #648 | descent direction computed
2022-04-23 14:23:02 | [train_policy] epoch #648 | backtrack iters: 0
2022-04-23 14:23:02 | [train_policy] epoch #648 | optimization finished
2022-04-23 14:23:02 | [train_policy] epoch #648 | Computing KL after
2022-04-23 14:23:02 | [train_policy] epoch #648 | Computing loss after
2022-04-23 14:23:02 | [train_policy] epoch #648 | Fitting baseline...
2022-04-23 14:23:02 | [train_policy] epoch #648 | Saving snapshot...
2022-04-23 14:23:02 | [train_policy] epoch #648 | Saved
2022-04-23 14:23:02 | [train_policy] epoch #648 | Time 230.61 s
2022-04-23 14:23:02 | [train_policy] epoch #648 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120231
Evaluation/AverageDiscountedReturn          -40.8746
Evaluation/AverageReturn                    -40.8746
Evaluation/CompletionRate                     0
Evaluation/Iteration                        648
Evaluation/MaxReturn                        -30.4736
Evaluation/MinReturn                        -65.9333
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.07642
Extras/EpisodeRewardMean                    -41.047
LinearFeatureBaseline/ExplainedVariance       0.881879
PolicyExecTime                                0.0937736
ProcessExecTime                               0.0111151
TotalEnvSteps                            656788
policy/Entropy                               -1.17388
policy/KL                                     0.00993375
policy/KLBefore                               0
policy/LossAfter                             -0.0188048
policy/LossBefore                             1.53134e-08
policy/Perplexity                             0.309166
policy/dLoss                                  0.0188048
---------------------------------------  ----------------
2022-04-23 14:23:02 | [train_policy] epoch #649 | Obtaining samples for iteration 649...
2022-04-23 14:23:03 | [train_policy] epoch #649 | Logging diagnostics...
2022-04-23 14:23:03 | [train_policy] epoch #649 | Optimizing policy...
2022-04-23 14:23:03 | [train_policy] epoch #649 | Computing loss before
2022-04-23 14:23:03 | [train_policy] epoch #649 | Computing KL before
2022-04-23 14:23:03 | [train_policy] epoch #649 | Optimizing
2022-04-23 14:23:03 | [train_policy] epoch #649 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:03 | [train_policy] epoch #649 | computing loss before
2022-04-23 14:23:03 | [train_policy] epoch #649 | computing gradient
2022-04-23 14:23:03 | [train_policy] epoch #649 | gradient computed
2022-04-23 14:23:03 | [train_policy] epoch #649 | computing descent direction
2022-04-23 14:23:03 | [train_policy] epoch #649 | descent direction computed
2022-04-23 14:23:03 | [train_policy] epoch #649 | backtrack iters: 1
2022-04-23 14:23:03 | [train_policy] epoch #649 | optimization finished
2022-04-23 14:23:03 | [train_policy] epoch #649 | Computing KL after
2022-04-23 14:23:03 | [train_policy] epoch #649 | Computing loss after
2022-04-23 14:23:03 | [train_policy] epoch #649 | Fitting baseline...
2022-04-23 14:23:03 | [train_policy] epoch #649 | Saving snapshot...
2022-04-23 14:23:03 | [train_policy] epoch #649 | Saved
2022-04-23 14:23:03 | [train_policy] epoch #649 | Time 230.93 s
2022-04-23 14:23:03 | [train_policy] epoch #649 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118995
Evaluation/AverageDiscountedReturn          -41.9689
Evaluation/AverageReturn                    -41.9689
Evaluation/CompletionRate                     0
Evaluation/Iteration                        649
Evaluation/MaxReturn                        -30.0886
Evaluation/MinReturn                        -61.5754
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.53285
Extras/EpisodeRewardMean                    -41.6809
LinearFeatureBaseline/ExplainedVariance       0.892006
PolicyExecTime                                0.0931025
ProcessExecTime                               0.0113435
TotalEnvSteps                            657800
policy/Entropy                               -1.19294
policy/KL                                     0.00653901
policy/KLBefore                               0
policy/LossAfter                             -0.0156427
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.303329
policy/dLoss                                  0.0156427
---------------------------------------  ----------------
2022-04-23 14:23:03 | [train_policy] epoch #650 | Obtaining samples for iteration 650...
2022-04-23 14:23:03 | [train_policy] epoch #650 | Logging diagnostics...
2022-04-23 14:23:03 | [train_policy] epoch #650 | Optimizing policy...
2022-04-23 14:23:03 | [train_policy] epoch #650 | Computing loss before
2022-04-23 14:23:03 | [train_policy] epoch #650 | Computing KL before
2022-04-23 14:23:03 | [train_policy] epoch #650 | Optimizing
2022-04-23 14:23:03 | [train_policy] epoch #650 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:03 | [train_policy] epoch #650 | computing loss before
2022-04-23 14:23:03 | [train_policy] epoch #650 | computing gradient
2022-04-23 14:23:03 | [train_policy] epoch #650 | gradient computed
2022-04-23 14:23:03 | [train_policy] epoch #650 | computing descent direction
2022-04-23 14:23:03 | [train_policy] epoch #650 | descent direction computed
2022-04-23 14:23:03 | [train_policy] epoch #650 | backtrack iters: 1
2022-04-23 14:23:03 | [train_policy] epoch #650 | optimization finished
2022-04-23 14:23:03 | [train_policy] epoch #650 | Computing KL after
2022-04-23 14:23:03 | [train_policy] epoch #650 | Computing loss after
2022-04-23 14:23:03 | [train_policy] epoch #650 | Fitting baseline...
2022-04-23 14:23:03 | [train_policy] epoch #650 | Saving snapshot...
2022-04-23 14:23:03 | [train_policy] epoch #650 | Saved
2022-04-23 14:23:03 | [train_policy] epoch #650 | Time 231.26 s
2022-04-23 14:23:03 | [train_policy] epoch #650 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119668
Evaluation/AverageDiscountedReturn          -61.7291
Evaluation/AverageReturn                    -61.7291
Evaluation/CompletionRate                     0
Evaluation/Iteration                        650
Evaluation/MaxReturn                        -31.2485
Evaluation/MinReturn                      -1914.19
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        194.341
Extras/EpisodeRewardMean                    -60.2167
LinearFeatureBaseline/ExplainedVariance       0.0139576
PolicyExecTime                                0.0925772
ProcessExecTime                               0.0112004
TotalEnvSteps                            658812
policy/Entropy                               -1.23717
policy/KL                                     0.00640873
policy/KLBefore                               0
policy/LossAfter                             -0.0187165
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.290205
policy/dLoss                                  0.0187165
---------------------------------------  ----------------
2022-04-23 14:23:03 | [train_policy] epoch #651 | Obtaining samples for iteration 651...
2022-04-23 14:23:03 | [train_policy] epoch #651 | Logging diagnostics...
2022-04-23 14:23:03 | [train_policy] epoch #651 | Optimizing policy...
2022-04-23 14:23:03 | [train_policy] epoch #651 | Computing loss before
2022-04-23 14:23:03 | [train_policy] epoch #651 | Computing KL before
2022-04-23 14:23:03 | [train_policy] epoch #651 | Optimizing
2022-04-23 14:23:03 | [train_policy] epoch #651 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:03 | [train_policy] epoch #651 | computing loss before
2022-04-23 14:23:03 | [train_policy] epoch #651 | computing gradient
2022-04-23 14:23:03 | [train_policy] epoch #651 | gradient computed
2022-04-23 14:23:03 | [train_policy] epoch #651 | computing descent direction
2022-04-23 14:23:03 | [train_policy] epoch #651 | descent direction computed
2022-04-23 14:23:03 | [train_policy] epoch #651 | backtrack iters: 0
2022-04-23 14:23:03 | [train_policy] epoch #651 | optimization finished
2022-04-23 14:23:03 | [train_policy] epoch #651 | Computing KL after
2022-04-23 14:23:03 | [train_policy] epoch #651 | Computing loss after
2022-04-23 14:23:03 | [train_policy] epoch #651 | Fitting baseline...
2022-04-23 14:23:03 | [train_policy] epoch #651 | Saving snapshot...
2022-04-23 14:23:03 | [train_policy] epoch #651 | Saved
2022-04-23 14:23:03 | [train_policy] epoch #651 | Time 231.63 s
2022-04-23 14:23:03 | [train_policy] epoch #651 | EpochTime 0.37 s
---------------------------------------  ----------------
EnvExecTime                                   0.128113
Evaluation/AverageDiscountedReturn          -58.8156
Evaluation/AverageReturn                    -58.8156
Evaluation/CompletionRate                     0
Evaluation/Iteration                        651
Evaluation/MaxReturn                        -30.3112
Evaluation/MinReturn                       -956.496
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        122.426
Extras/EpisodeRewardMean                    -57.2987
LinearFeatureBaseline/ExplainedVariance       0.0910217
PolicyExecTime                                0.121298
ProcessExecTime                               0.0127766
TotalEnvSteps                            659824
policy/Entropy                               -1.21185
policy/KL                                     0.00972449
policy/KLBefore                               0
policy/LossAfter                             -0.0545638
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.297645
policy/dLoss                                  0.0545638
---------------------------------------  ----------------
2022-04-23 14:23:03 | [train_policy] epoch #652 | Obtaining samples for iteration 652...
2022-04-23 14:23:04 | [train_policy] epoch #652 | Logging diagnostics...
2022-04-23 14:23:04 | [train_policy] epoch #652 | Optimizing policy...
2022-04-23 14:23:04 | [train_policy] epoch #652 | Computing loss before
2022-04-23 14:23:04 | [train_policy] epoch #652 | Computing KL before
2022-04-23 14:23:04 | [train_policy] epoch #652 | Optimizing
2022-04-23 14:23:04 | [train_policy] epoch #652 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:04 | [train_policy] epoch #652 | computing loss before
2022-04-23 14:23:04 | [train_policy] epoch #652 | computing gradient
2022-04-23 14:23:04 | [train_policy] epoch #652 | gradient computed
2022-04-23 14:23:04 | [train_policy] epoch #652 | computing descent direction
2022-04-23 14:23:04 | [train_policy] epoch #652 | descent direction computed
2022-04-23 14:23:04 | [train_policy] epoch #652 | backtrack iters: 0
2022-04-23 14:23:04 | [train_policy] epoch #652 | optimization finished
2022-04-23 14:23:04 | [train_policy] epoch #652 | Computing KL after
2022-04-23 14:23:04 | [train_policy] epoch #652 | Computing loss after
2022-04-23 14:23:04 | [train_policy] epoch #652 | Fitting baseline...
2022-04-23 14:23:04 | [train_policy] epoch #652 | Saving snapshot...
2022-04-23 14:23:04 | [train_policy] epoch #652 | Saved
2022-04-23 14:23:04 | [train_policy] epoch #652 | Time 231.98 s
2022-04-23 14:23:04 | [train_policy] epoch #652 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117469
Evaluation/AverageDiscountedReturn          -64.1148
Evaluation/AverageReturn                    -64.1148
Evaluation/CompletionRate                     0
Evaluation/Iteration                        652
Evaluation/MaxReturn                        -32.1368
Evaluation/MinReturn                      -2062.58
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.671
Extras/EpisodeRewardMean                    -62.0412
LinearFeatureBaseline/ExplainedVariance       0.0987715
PolicyExecTime                                0.112511
ProcessExecTime                               0.0114388
TotalEnvSteps                            660836
policy/Entropy                               -1.22511
policy/KL                                     0.00975059
policy/KLBefore                               0
policy/LossAfter                             -0.026149
policy/LossBefore                            -1.41355e-08
policy/Perplexity                             0.293724
policy/dLoss                                  0.026149
---------------------------------------  ----------------
2022-04-23 14:23:04 | [train_policy] epoch #653 | Obtaining samples for iteration 653...
2022-04-23 14:23:04 | [train_policy] epoch #653 | Logging diagnostics...
2022-04-23 14:23:04 | [train_policy] epoch #653 | Optimizing policy...
2022-04-23 14:23:04 | [train_policy] epoch #653 | Computing loss before
2022-04-23 14:23:04 | [train_policy] epoch #653 | Computing KL before
2022-04-23 14:23:04 | [train_policy] epoch #653 | Optimizing
2022-04-23 14:23:04 | [train_policy] epoch #653 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:04 | [train_policy] epoch #653 | computing loss before
2022-04-23 14:23:04 | [train_policy] epoch #653 | computing gradient
2022-04-23 14:23:04 | [train_policy] epoch #653 | gradient computed
2022-04-23 14:23:04 | [train_policy] epoch #653 | computing descent direction
2022-04-23 14:23:04 | [train_policy] epoch #653 | descent direction computed
2022-04-23 14:23:04 | [train_policy] epoch #653 | backtrack iters: 1
2022-04-23 14:23:04 | [train_policy] epoch #653 | optimization finished
2022-04-23 14:23:04 | [train_policy] epoch #653 | Computing KL after
2022-04-23 14:23:04 | [train_policy] epoch #653 | Computing loss after
2022-04-23 14:23:04 | [train_policy] epoch #653 | Fitting baseline...
2022-04-23 14:23:04 | [train_policy] epoch #653 | Saving snapshot...
2022-04-23 14:23:04 | [train_policy] epoch #653 | Saved
2022-04-23 14:23:04 | [train_policy] epoch #653 | Time 232.33 s
2022-04-23 14:23:04 | [train_policy] epoch #653 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.117215
Evaluation/AverageDiscountedReturn          -41.6165
Evaluation/AverageReturn                    -41.6165
Evaluation/CompletionRate                     0
Evaluation/Iteration                        653
Evaluation/MaxReturn                        -30.0968
Evaluation/MinReturn                        -74.5438
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.13682
Extras/EpisodeRewardMean                    -41.557
LinearFeatureBaseline/ExplainedVariance     -27.3045
PolicyExecTime                                0.112361
ProcessExecTime                               0.011385
TotalEnvSteps                            661848
policy/Entropy                               -1.25538
policy/KL                                     0.00723594
policy/KLBefore                               0
policy/LossAfter                             -0.0142993
policy/LossBefore                             2.63862e-08
policy/Perplexity                             0.284967
policy/dLoss                                  0.0142993
---------------------------------------  ----------------
2022-04-23 14:23:04 | [train_policy] epoch #654 | Obtaining samples for iteration 654...
2022-04-23 14:23:04 | [train_policy] epoch #654 | Logging diagnostics...
2022-04-23 14:23:04 | [train_policy] epoch #654 | Optimizing policy...
2022-04-23 14:23:04 | [train_policy] epoch #654 | Computing loss before
2022-04-23 14:23:04 | [train_policy] epoch #654 | Computing KL before
2022-04-23 14:23:04 | [train_policy] epoch #654 | Optimizing
2022-04-23 14:23:04 | [train_policy] epoch #654 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:04 | [train_policy] epoch #654 | computing loss before
2022-04-23 14:23:04 | [train_policy] epoch #654 | computing gradient
2022-04-23 14:23:04 | [train_policy] epoch #654 | gradient computed
2022-04-23 14:23:04 | [train_policy] epoch #654 | computing descent direction
2022-04-23 14:23:04 | [train_policy] epoch #654 | descent direction computed
2022-04-23 14:23:04 | [train_policy] epoch #654 | backtrack iters: 1
2022-04-23 14:23:04 | [train_policy] epoch #654 | optimization finished
2022-04-23 14:23:04 | [train_policy] epoch #654 | Computing KL after
2022-04-23 14:23:04 | [train_policy] epoch #654 | Computing loss after
2022-04-23 14:23:04 | [train_policy] epoch #654 | Fitting baseline...
2022-04-23 14:23:04 | [train_policy] epoch #654 | Saving snapshot...
2022-04-23 14:23:04 | [train_policy] epoch #654 | Saved
2022-04-23 14:23:04 | [train_policy] epoch #654 | Time 232.67 s
2022-04-23 14:23:04 | [train_policy] epoch #654 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117235
Evaluation/AverageDiscountedReturn          -41.3707
Evaluation/AverageReturn                    -41.3707
Evaluation/CompletionRate                     0
Evaluation/Iteration                        654
Evaluation/MaxReturn                        -30.1654
Evaluation/MinReturn                        -73.1496
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.65837
Extras/EpisodeRewardMean                    -42.1119
LinearFeatureBaseline/ExplainedVariance       0.875273
PolicyExecTime                                0.104186
ProcessExecTime                               0.0114951
TotalEnvSteps                            662860
policy/Entropy                               -1.30971
policy/KL                                     0.0065722
policy/KLBefore                               0
policy/LossAfter                             -0.0112074
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.269898
policy/dLoss                                  0.0112074
---------------------------------------  ----------------
2022-04-23 14:23:04 | [train_policy] epoch #655 | Obtaining samples for iteration 655...
2022-04-23 14:23:05 | [train_policy] epoch #655 | Logging diagnostics...
2022-04-23 14:23:05 | [train_policy] epoch #655 | Optimizing policy...
2022-04-23 14:23:05 | [train_policy] epoch #655 | Computing loss before
2022-04-23 14:23:05 | [train_policy] epoch #655 | Computing KL before
2022-04-23 14:23:05 | [train_policy] epoch #655 | Optimizing
2022-04-23 14:23:05 | [train_policy] epoch #655 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:05 | [train_policy] epoch #655 | computing loss before
2022-04-23 14:23:05 | [train_policy] epoch #655 | computing gradient
2022-04-23 14:23:05 | [train_policy] epoch #655 | gradient computed
2022-04-23 14:23:05 | [train_policy] epoch #655 | computing descent direction
2022-04-23 14:23:05 | [train_policy] epoch #655 | descent direction computed
2022-04-23 14:23:05 | [train_policy] epoch #655 | backtrack iters: 0
2022-04-23 14:23:05 | [train_policy] epoch #655 | optimization finished
2022-04-23 14:23:05 | [train_policy] epoch #655 | Computing KL after
2022-04-23 14:23:05 | [train_policy] epoch #655 | Computing loss after
2022-04-23 14:23:05 | [train_policy] epoch #655 | Fitting baseline...
2022-04-23 14:23:05 | [train_policy] epoch #655 | Saving snapshot...
2022-04-23 14:23:05 | [train_policy] epoch #655 | Saved
2022-04-23 14:23:05 | [train_policy] epoch #655 | Time 233.00 s
2022-04-23 14:23:05 | [train_policy] epoch #655 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120767
Evaluation/AverageDiscountedReturn          -42.1041
Evaluation/AverageReturn                    -42.1041
Evaluation/CompletionRate                     0
Evaluation/Iteration                        655
Evaluation/MaxReturn                        -30.1208
Evaluation/MinReturn                        -73.6048
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.35444
Extras/EpisodeRewardMean                    -42.1048
LinearFeatureBaseline/ExplainedVariance       0.872085
PolicyExecTime                                0.0979512
ProcessExecTime                               0.0116634
TotalEnvSteps                            663872
policy/Entropy                               -1.28377
policy/KL                                     0.00954456
policy/KLBefore                               0
policy/LossAfter                             -0.018133
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.27699
policy/dLoss                                  0.018133
---------------------------------------  ----------------
2022-04-23 14:23:05 | [train_policy] epoch #656 | Obtaining samples for iteration 656...
2022-04-23 14:23:05 | [train_policy] epoch #656 | Logging diagnostics...
2022-04-23 14:23:05 | [train_policy] epoch #656 | Optimizing policy...
2022-04-23 14:23:05 | [train_policy] epoch #656 | Computing loss before
2022-04-23 14:23:05 | [train_policy] epoch #656 | Computing KL before
2022-04-23 14:23:05 | [train_policy] epoch #656 | Optimizing
2022-04-23 14:23:05 | [train_policy] epoch #656 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:05 | [train_policy] epoch #656 | computing loss before
2022-04-23 14:23:05 | [train_policy] epoch #656 | computing gradient
2022-04-23 14:23:05 | [train_policy] epoch #656 | gradient computed
2022-04-23 14:23:05 | [train_policy] epoch #656 | computing descent direction
2022-04-23 14:23:05 | [train_policy] epoch #656 | descent direction computed
2022-04-23 14:23:05 | [train_policy] epoch #656 | backtrack iters: 1
2022-04-23 14:23:05 | [train_policy] epoch #656 | optimization finished
2022-04-23 14:23:05 | [train_policy] epoch #656 | Computing KL after
2022-04-23 14:23:05 | [train_policy] epoch #656 | Computing loss after
2022-04-23 14:23:05 | [train_policy] epoch #656 | Fitting baseline...
2022-04-23 14:23:05 | [train_policy] epoch #656 | Saving snapshot...
2022-04-23 14:23:05 | [train_policy] epoch #656 | Saved
2022-04-23 14:23:05 | [train_policy] epoch #656 | Time 233.33 s
2022-04-23 14:23:05 | [train_policy] epoch #656 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119237
Evaluation/AverageDiscountedReturn          -41.2602
Evaluation/AverageReturn                    -41.2602
Evaluation/CompletionRate                     0
Evaluation/Iteration                        656
Evaluation/MaxReturn                        -29.4137
Evaluation/MinReturn                        -64.3488
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.59476
Extras/EpisodeRewardMean                    -41.1051
LinearFeatureBaseline/ExplainedVariance       0.879029
PolicyExecTime                                0.0946794
ProcessExecTime                               0.0110898
TotalEnvSteps                            664884
policy/Entropy                               -1.30421
policy/KL                                     0.00687131
policy/KLBefore                               0
policy/LossAfter                             -0.0163407
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.271388
policy/dLoss                                  0.0163407
---------------------------------------  ----------------
2022-04-23 14:23:05 | [train_policy] epoch #657 | Obtaining samples for iteration 657...
2022-04-23 14:23:05 | [train_policy] epoch #657 | Logging diagnostics...
2022-04-23 14:23:05 | [train_policy] epoch #657 | Optimizing policy...
2022-04-23 14:23:05 | [train_policy] epoch #657 | Computing loss before
2022-04-23 14:23:05 | [train_policy] epoch #657 | Computing KL before
2022-04-23 14:23:05 | [train_policy] epoch #657 | Optimizing
2022-04-23 14:23:05 | [train_policy] epoch #657 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:05 | [train_policy] epoch #657 | computing loss before
2022-04-23 14:23:05 | [train_policy] epoch #657 | computing gradient
2022-04-23 14:23:05 | [train_policy] epoch #657 | gradient computed
2022-04-23 14:23:05 | [train_policy] epoch #657 | computing descent direction
2022-04-23 14:23:05 | [train_policy] epoch #657 | descent direction computed
2022-04-23 14:23:05 | [train_policy] epoch #657 | backtrack iters: 1
2022-04-23 14:23:05 | [train_policy] epoch #657 | optimization finished
2022-04-23 14:23:05 | [train_policy] epoch #657 | Computing KL after
2022-04-23 14:23:05 | [train_policy] epoch #657 | Computing loss after
2022-04-23 14:23:05 | [train_policy] epoch #657 | Fitting baseline...
2022-04-23 14:23:05 | [train_policy] epoch #657 | Saving snapshot...
2022-04-23 14:23:05 | [train_policy] epoch #657 | Saved
2022-04-23 14:23:05 | [train_policy] epoch #657 | Time 233.69 s
2022-04-23 14:23:05 | [train_policy] epoch #657 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.118677
Evaluation/AverageDiscountedReturn          -41.3906
Evaluation/AverageReturn                    -41.3906
Evaluation/CompletionRate                     0
Evaluation/Iteration                        657
Evaluation/MaxReturn                        -30.39
Evaluation/MinReturn                        -63.9379
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05908
Extras/EpisodeRewardMean                    -41.5587
LinearFeatureBaseline/ExplainedVariance       0.893538
PolicyExecTime                                0.0933537
ProcessExecTime                               0.0111661
TotalEnvSteps                            665896
policy/Entropy                               -1.31467
policy/KL                                     0.00643121
policy/KLBefore                               0
policy/LossAfter                             -0.0124576
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.268564
policy/dLoss                                  0.0124576
---------------------------------------  ----------------
2022-04-23 14:23:05 | [train_policy] epoch #658 | Obtaining samples for iteration 658...
2022-04-23 14:23:06 | [train_policy] epoch #658 | Logging diagnostics...
2022-04-23 14:23:06 | [train_policy] epoch #658 | Optimizing policy...
2022-04-23 14:23:06 | [train_policy] epoch #658 | Computing loss before
2022-04-23 14:23:06 | [train_policy] epoch #658 | Computing KL before
2022-04-23 14:23:06 | [train_policy] epoch #658 | Optimizing
2022-04-23 14:23:06 | [train_policy] epoch #658 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:06 | [train_policy] epoch #658 | computing loss before
2022-04-23 14:23:06 | [train_policy] epoch #658 | computing gradient
2022-04-23 14:23:06 | [train_policy] epoch #658 | gradient computed
2022-04-23 14:23:06 | [train_policy] epoch #658 | computing descent direction
2022-04-23 14:23:06 | [train_policy] epoch #658 | descent direction computed
2022-04-23 14:23:06 | [train_policy] epoch #658 | backtrack iters: 0
2022-04-23 14:23:06 | [train_policy] epoch #658 | optimization finished
2022-04-23 14:23:06 | [train_policy] epoch #658 | Computing KL after
2022-04-23 14:23:06 | [train_policy] epoch #658 | Computing loss after
2022-04-23 14:23:06 | [train_policy] epoch #658 | Fitting baseline...
2022-04-23 14:23:06 | [train_policy] epoch #658 | Saving snapshot...
2022-04-23 14:23:06 | [train_policy] epoch #658 | Saved
2022-04-23 14:23:06 | [train_policy] epoch #658 | Time 234.02 s
2022-04-23 14:23:06 | [train_policy] epoch #658 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117922
Evaluation/AverageDiscountedReturn          -41.7712
Evaluation/AverageReturn                    -41.7712
Evaluation/CompletionRate                     0
Evaluation/Iteration                        658
Evaluation/MaxReturn                        -29.7206
Evaluation/MinReturn                        -72.9284
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.17392
Extras/EpisodeRewardMean                    -41.7569
LinearFeatureBaseline/ExplainedVariance       0.870738
PolicyExecTime                                0.0961204
ProcessExecTime                               0.011251
TotalEnvSteps                            666908
policy/Entropy                               -1.28871
policy/KL                                     0.00982933
policy/KLBefore                               0
policy/LossAfter                             -0.0130174
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.275627
policy/dLoss                                  0.0130174
---------------------------------------  ----------------
2022-04-23 14:23:06 | [train_policy] epoch #659 | Obtaining samples for iteration 659...
2022-04-23 14:23:06 | [train_policy] epoch #659 | Logging diagnostics...
2022-04-23 14:23:06 | [train_policy] epoch #659 | Optimizing policy...
2022-04-23 14:23:06 | [train_policy] epoch #659 | Computing loss before
2022-04-23 14:23:06 | [train_policy] epoch #659 | Computing KL before
2022-04-23 14:23:06 | [train_policy] epoch #659 | Optimizing
2022-04-23 14:23:06 | [train_policy] epoch #659 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:06 | [train_policy] epoch #659 | computing loss before
2022-04-23 14:23:06 | [train_policy] epoch #659 | computing gradient
2022-04-23 14:23:06 | [train_policy] epoch #659 | gradient computed
2022-04-23 14:23:06 | [train_policy] epoch #659 | computing descent direction
2022-04-23 14:23:06 | [train_policy] epoch #659 | descent direction computed
2022-04-23 14:23:06 | [train_policy] epoch #659 | backtrack iters: 1
2022-04-23 14:23:06 | [train_policy] epoch #659 | optimization finished
2022-04-23 14:23:06 | [train_policy] epoch #659 | Computing KL after
2022-04-23 14:23:06 | [train_policy] epoch #659 | Computing loss after
2022-04-23 14:23:06 | [train_policy] epoch #659 | Fitting baseline...
2022-04-23 14:23:06 | [train_policy] epoch #659 | Saving snapshot...
2022-04-23 14:23:06 | [train_policy] epoch #659 | Saved
2022-04-23 14:23:06 | [train_policy] epoch #659 | Time 234.36 s
2022-04-23 14:23:06 | [train_policy] epoch #659 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118215
Evaluation/AverageDiscountedReturn          -41.9533
Evaluation/AverageReturn                    -41.9533
Evaluation/CompletionRate                     0
Evaluation/Iteration                        659
Evaluation/MaxReturn                        -32.7926
Evaluation/MinReturn                        -72.8034
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05788
Extras/EpisodeRewardMean                    -41.6979
LinearFeatureBaseline/ExplainedVariance       0.826273
PolicyExecTime                                0.100369
ProcessExecTime                               0.011359
TotalEnvSteps                            667920
policy/Entropy                               -1.30486
policy/KL                                     0.00666646
policy/KLBefore                               0
policy/LossAfter                             -0.0146548
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.27121
policy/dLoss                                  0.0146548
---------------------------------------  ----------------
2022-04-23 14:23:06 | [train_policy] epoch #660 | Obtaining samples for iteration 660...
2022-04-23 14:23:06 | [train_policy] epoch #660 | Logging diagnostics...
2022-04-23 14:23:06 | [train_policy] epoch #660 | Optimizing policy...
2022-04-23 14:23:06 | [train_policy] epoch #660 | Computing loss before
2022-04-23 14:23:06 | [train_policy] epoch #660 | Computing KL before
2022-04-23 14:23:06 | [train_policy] epoch #660 | Optimizing
2022-04-23 14:23:06 | [train_policy] epoch #660 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:06 | [train_policy] epoch #660 | computing loss before
2022-04-23 14:23:06 | [train_policy] epoch #660 | computing gradient
2022-04-23 14:23:06 | [train_policy] epoch #660 | gradient computed
2022-04-23 14:23:06 | [train_policy] epoch #660 | computing descent direction
2022-04-23 14:23:06 | [train_policy] epoch #660 | descent direction computed
2022-04-23 14:23:06 | [train_policy] epoch #660 | backtrack iters: 0
2022-04-23 14:23:06 | [train_policy] epoch #660 | optimization finished
2022-04-23 14:23:06 | [train_policy] epoch #660 | Computing KL after
2022-04-23 14:23:06 | [train_policy] epoch #660 | Computing loss after
2022-04-23 14:23:06 | [train_policy] epoch #660 | Fitting baseline...
2022-04-23 14:23:06 | [train_policy] epoch #660 | Saving snapshot...
2022-04-23 14:23:06 | [train_policy] epoch #660 | Saved
2022-04-23 14:23:06 | [train_policy] epoch #660 | Time 234.69 s
2022-04-23 14:23:06 | [train_policy] epoch #660 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119272
Evaluation/AverageDiscountedReturn          -41.5405
Evaluation/AverageReturn                    -41.5405
Evaluation/CompletionRate                     0
Evaluation/Iteration                        660
Evaluation/MaxReturn                        -30.0821
Evaluation/MinReturn                        -72.6381
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.85285
Extras/EpisodeRewardMean                    -41.5991
LinearFeatureBaseline/ExplainedVariance       0.863129
PolicyExecTime                                0.102232
ProcessExecTime                               0.0113983
TotalEnvSteps                            668932
policy/Entropy                               -1.25821
policy/KL                                     0.00951377
policy/KLBefore                               0
policy/LossAfter                             -0.0162415
policy/LossBefore                             1.80227e-08
policy/Perplexity                             0.284161
policy/dLoss                                  0.0162415
---------------------------------------  ----------------
2022-04-23 14:23:06 | [train_policy] epoch #661 | Obtaining samples for iteration 661...
2022-04-23 14:23:07 | [train_policy] epoch #661 | Logging diagnostics...
2022-04-23 14:23:07 | [train_policy] epoch #661 | Optimizing policy...
2022-04-23 14:23:07 | [train_policy] epoch #661 | Computing loss before
2022-04-23 14:23:07 | [train_policy] epoch #661 | Computing KL before
2022-04-23 14:23:07 | [train_policy] epoch #661 | Optimizing
2022-04-23 14:23:07 | [train_policy] epoch #661 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:07 | [train_policy] epoch #661 | computing loss before
2022-04-23 14:23:07 | [train_policy] epoch #661 | computing gradient
2022-04-23 14:23:07 | [train_policy] epoch #661 | gradient computed
2022-04-23 14:23:07 | [train_policy] epoch #661 | computing descent direction
2022-04-23 14:23:07 | [train_policy] epoch #661 | descent direction computed
2022-04-23 14:23:07 | [train_policy] epoch #661 | backtrack iters: 1
2022-04-23 14:23:07 | [train_policy] epoch #661 | optimization finished
2022-04-23 14:23:07 | [train_policy] epoch #661 | Computing KL after
2022-04-23 14:23:07 | [train_policy] epoch #661 | Computing loss after
2022-04-23 14:23:07 | [train_policy] epoch #661 | Fitting baseline...
2022-04-23 14:23:07 | [train_policy] epoch #661 | Saving snapshot...
2022-04-23 14:23:07 | [train_policy] epoch #661 | Saved
2022-04-23 14:23:07 | [train_policy] epoch #661 | Time 235.03 s
2022-04-23 14:23:07 | [train_policy] epoch #661 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119624
Evaluation/AverageDiscountedReturn          -39.573
Evaluation/AverageReturn                    -39.573
Evaluation/CompletionRate                     0
Evaluation/Iteration                        661
Evaluation/MaxReturn                        -29.9213
Evaluation/MinReturn                        -64.6939
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.96521
Extras/EpisodeRewardMean                    -39.236
LinearFeatureBaseline/ExplainedVariance       0.887695
PolicyExecTime                                0.0993528
ProcessExecTime                               0.0111778
TotalEnvSteps                            669944
policy/Entropy                               -1.26482
policy/KL                                     0.00640431
policy/KLBefore                               0
policy/LossAfter                             -0.0435779
policy/LossBefore                             3.67523e-08
policy/Perplexity                             0.28229
policy/dLoss                                  0.0435779
---------------------------------------  ----------------
2022-04-23 14:23:07 | [train_policy] epoch #662 | Obtaining samples for iteration 662...
2022-04-23 14:23:07 | [train_policy] epoch #662 | Logging diagnostics...
2022-04-23 14:23:07 | [train_policy] epoch #662 | Optimizing policy...
2022-04-23 14:23:07 | [train_policy] epoch #662 | Computing loss before
2022-04-23 14:23:07 | [train_policy] epoch #662 | Computing KL before
2022-04-23 14:23:07 | [train_policy] epoch #662 | Optimizing
2022-04-23 14:23:07 | [train_policy] epoch #662 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:07 | [train_policy] epoch #662 | computing loss before
2022-04-23 14:23:07 | [train_policy] epoch #662 | computing gradient
2022-04-23 14:23:07 | [train_policy] epoch #662 | gradient computed
2022-04-23 14:23:07 | [train_policy] epoch #662 | computing descent direction
2022-04-23 14:23:07 | [train_policy] epoch #662 | descent direction computed
2022-04-23 14:23:07 | [train_policy] epoch #662 | backtrack iters: 1
2022-04-23 14:23:07 | [train_policy] epoch #662 | optimization finished
2022-04-23 14:23:07 | [train_policy] epoch #662 | Computing KL after
2022-04-23 14:23:07 | [train_policy] epoch #662 | Computing loss after
2022-04-23 14:23:07 | [train_policy] epoch #662 | Fitting baseline...
2022-04-23 14:23:07 | [train_policy] epoch #662 | Saving snapshot...
2022-04-23 14:23:07 | [train_policy] epoch #662 | Saved
2022-04-23 14:23:07 | [train_policy] epoch #662 | Time 235.37 s
2022-04-23 14:23:07 | [train_policy] epoch #662 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11913
Evaluation/AverageDiscountedReturn          -40.6811
Evaluation/AverageReturn                    -40.6811
Evaluation/CompletionRate                     0
Evaluation/Iteration                        662
Evaluation/MaxReturn                        -31.2064
Evaluation/MinReturn                        -63.942
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.01627
Extras/EpisodeRewardMean                    -40.6589
LinearFeatureBaseline/ExplainedVariance       0.912647
PolicyExecTime                                0.101097
ProcessExecTime                               0.0111442
TotalEnvSteps                            670956
policy/Entropy                               -1.28787
policy/KL                                     0.00669603
policy/KLBefore                               0
policy/LossAfter                             -0.00923482
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.275859
policy/dLoss                                  0.00923483
---------------------------------------  ----------------
2022-04-23 14:23:07 | [train_policy] epoch #663 | Obtaining samples for iteration 663...
2022-04-23 14:23:07 | [train_policy] epoch #663 | Logging diagnostics...
2022-04-23 14:23:07 | [train_policy] epoch #663 | Optimizing policy...
2022-04-23 14:23:07 | [train_policy] epoch #663 | Computing loss before
2022-04-23 14:23:07 | [train_policy] epoch #663 | Computing KL before
2022-04-23 14:23:07 | [train_policy] epoch #663 | Optimizing
2022-04-23 14:23:07 | [train_policy] epoch #663 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:07 | [train_policy] epoch #663 | computing loss before
2022-04-23 14:23:07 | [train_policy] epoch #663 | computing gradient
2022-04-23 14:23:07 | [train_policy] epoch #663 | gradient computed
2022-04-23 14:23:07 | [train_policy] epoch #663 | computing descent direction
2022-04-23 14:23:07 | [train_policy] epoch #663 | descent direction computed
2022-04-23 14:23:07 | [train_policy] epoch #663 | backtrack iters: 0
2022-04-23 14:23:07 | [train_policy] epoch #663 | optimization finished
2022-04-23 14:23:07 | [train_policy] epoch #663 | Computing KL after
2022-04-23 14:23:07 | [train_policy] epoch #663 | Computing loss after
2022-04-23 14:23:07 | [train_policy] epoch #663 | Fitting baseline...
2022-04-23 14:23:07 | [train_policy] epoch #663 | Saving snapshot...
2022-04-23 14:23:07 | [train_policy] epoch #663 | Saved
2022-04-23 14:23:07 | [train_policy] epoch #663 | Time 235.69 s
2022-04-23 14:23:07 | [train_policy] epoch #663 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119452
Evaluation/AverageDiscountedReturn          -40.9522
Evaluation/AverageReturn                    -40.9522
Evaluation/CompletionRate                     0
Evaluation/Iteration                        663
Evaluation/MaxReturn                        -30.977
Evaluation/MinReturn                        -79.2222
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.84331
Extras/EpisodeRewardMean                    -40.7149
LinearFeatureBaseline/ExplainedVariance       0.879863
PolicyExecTime                                0.0941987
ProcessExecTime                               0.0113063
TotalEnvSteps                            671968
policy/Entropy                               -1.24046
policy/KL                                     0.0099015
policy/KLBefore                               0
policy/LossAfter                             -0.0224997
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.289252
policy/dLoss                                  0.0224997
---------------------------------------  ----------------
2022-04-23 14:23:07 | [train_policy] epoch #664 | Obtaining samples for iteration 664...
2022-04-23 14:23:08 | [train_policy] epoch #664 | Logging diagnostics...
2022-04-23 14:23:08 | [train_policy] epoch #664 | Optimizing policy...
2022-04-23 14:23:08 | [train_policy] epoch #664 | Computing loss before
2022-04-23 14:23:08 | [train_policy] epoch #664 | Computing KL before
2022-04-23 14:23:08 | [train_policy] epoch #664 | Optimizing
2022-04-23 14:23:08 | [train_policy] epoch #664 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:08 | [train_policy] epoch #664 | computing loss before
2022-04-23 14:23:08 | [train_policy] epoch #664 | computing gradient
2022-04-23 14:23:08 | [train_policy] epoch #664 | gradient computed
2022-04-23 14:23:08 | [train_policy] epoch #664 | computing descent direction
2022-04-23 14:23:08 | [train_policy] epoch #664 | descent direction computed
2022-04-23 14:23:08 | [train_policy] epoch #664 | backtrack iters: 1
2022-04-23 14:23:08 | [train_policy] epoch #664 | optimization finished
2022-04-23 14:23:08 | [train_policy] epoch #664 | Computing KL after
2022-04-23 14:23:08 | [train_policy] epoch #664 | Computing loss after
2022-04-23 14:23:08 | [train_policy] epoch #664 | Fitting baseline...
2022-04-23 14:23:08 | [train_policy] epoch #664 | Saving snapshot...
2022-04-23 14:23:08 | [train_policy] epoch #664 | Saved
2022-04-23 14:23:08 | [train_policy] epoch #664 | Time 236.02 s
2022-04-23 14:23:08 | [train_policy] epoch #664 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119311
Evaluation/AverageDiscountedReturn          -41.6097
Evaluation/AverageReturn                    -41.6097
Evaluation/CompletionRate                     0
Evaluation/Iteration                        664
Evaluation/MaxReturn                        -29.9929
Evaluation/MinReturn                        -72.8465
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.92999
Extras/EpisodeRewardMean                    -41.3385
LinearFeatureBaseline/ExplainedVariance       0.88037
PolicyExecTime                                0.0993097
ProcessExecTime                               0.0111609
TotalEnvSteps                            672980
policy/Entropy                               -1.2282
policy/KL                                     0.00676215
policy/KLBefore                               0
policy/LossAfter                             -0.0107313
policy/LossBefore                            -3.06269e-09
policy/Perplexity                             0.292819
policy/dLoss                                  0.0107313
---------------------------------------  ----------------
2022-04-23 14:23:08 | [train_policy] epoch #665 | Obtaining samples for iteration 665...
2022-04-23 14:23:08 | [train_policy] epoch #665 | Logging diagnostics...
2022-04-23 14:23:08 | [train_policy] epoch #665 | Optimizing policy...
2022-04-23 14:23:08 | [train_policy] epoch #665 | Computing loss before
2022-04-23 14:23:08 | [train_policy] epoch #665 | Computing KL before
2022-04-23 14:23:08 | [train_policy] epoch #665 | Optimizing
2022-04-23 14:23:08 | [train_policy] epoch #665 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:08 | [train_policy] epoch #665 | computing loss before
2022-04-23 14:23:08 | [train_policy] epoch #665 | computing gradient
2022-04-23 14:23:08 | [train_policy] epoch #665 | gradient computed
2022-04-23 14:23:08 | [train_policy] epoch #665 | computing descent direction
2022-04-23 14:23:08 | [train_policy] epoch #665 | descent direction computed
2022-04-23 14:23:08 | [train_policy] epoch #665 | backtrack iters: 1
2022-04-23 14:23:08 | [train_policy] epoch #665 | optimization finished
2022-04-23 14:23:08 | [train_policy] epoch #665 | Computing KL after
2022-04-23 14:23:08 | [train_policy] epoch #665 | Computing loss after
2022-04-23 14:23:08 | [train_policy] epoch #665 | Fitting baseline...
2022-04-23 14:23:08 | [train_policy] epoch #665 | Saving snapshot...
2022-04-23 14:23:08 | [train_policy] epoch #665 | Saved
2022-04-23 14:23:08 | [train_policy] epoch #665 | Time 236.35 s
2022-04-23 14:23:08 | [train_policy] epoch #665 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119405
Evaluation/AverageDiscountedReturn          -42.0094
Evaluation/AverageReturn                    -42.0094
Evaluation/CompletionRate                     0
Evaluation/Iteration                        665
Evaluation/MaxReturn                        -30.1482
Evaluation/MinReturn                        -57.3185
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.18544
Extras/EpisodeRewardMean                    -41.863
LinearFeatureBaseline/ExplainedVariance       0.860491
PolicyExecTime                                0.0969515
ProcessExecTime                               0.0115325
TotalEnvSteps                            673992
policy/Entropy                               -1.21879
policy/KL                                     0.00655596
policy/KLBefore                               0
policy/LossAfter                             -0.0149475
policy/LossBefore                             6.83215e-09
policy/Perplexity                             0.295588
policy/dLoss                                  0.0149475
---------------------------------------  ----------------
2022-04-23 14:23:08 | [train_policy] epoch #666 | Obtaining samples for iteration 666...
2022-04-23 14:23:08 | [train_policy] epoch #666 | Logging diagnostics...
2022-04-23 14:23:08 | [train_policy] epoch #666 | Optimizing policy...
2022-04-23 14:23:08 | [train_policy] epoch #666 | Computing loss before
2022-04-23 14:23:08 | [train_policy] epoch #666 | Computing KL before
2022-04-23 14:23:08 | [train_policy] epoch #666 | Optimizing
2022-04-23 14:23:08 | [train_policy] epoch #666 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:08 | [train_policy] epoch #666 | computing loss before
2022-04-23 14:23:08 | [train_policy] epoch #666 | computing gradient
2022-04-23 14:23:08 | [train_policy] epoch #666 | gradient computed
2022-04-23 14:23:08 | [train_policy] epoch #666 | computing descent direction
2022-04-23 14:23:08 | [train_policy] epoch #666 | descent direction computed
2022-04-23 14:23:08 | [train_policy] epoch #666 | backtrack iters: 1
2022-04-23 14:23:08 | [train_policy] epoch #666 | optimization finished
2022-04-23 14:23:08 | [train_policy] epoch #666 | Computing KL after
2022-04-23 14:23:08 | [train_policy] epoch #666 | Computing loss after
2022-04-23 14:23:08 | [train_policy] epoch #666 | Fitting baseline...
2022-04-23 14:23:08 | [train_policy] epoch #666 | Saving snapshot...
2022-04-23 14:23:08 | [train_policy] epoch #666 | Saved
2022-04-23 14:23:08 | [train_policy] epoch #666 | Time 236.69 s
2022-04-23 14:23:08 | [train_policy] epoch #666 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119555
Evaluation/AverageDiscountedReturn          -42.7561
Evaluation/AverageReturn                    -42.7561
Evaluation/CompletionRate                     0
Evaluation/Iteration                        666
Evaluation/MaxReturn                        -29.6184
Evaluation/MinReturn                        -73.1497
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.45473
Extras/EpisodeRewardMean                    -42.65
LinearFeatureBaseline/ExplainedVariance       0.874053
PolicyExecTime                                0.0980022
ProcessExecTime                               0.0113168
TotalEnvSteps                            675004
policy/Entropy                               -1.20196
policy/KL                                     0.00654308
policy/KLBefore                               0
policy/LossAfter                             -0.0123608
policy/LossBefore                             1.17796e-08
policy/Perplexity                             0.300605
policy/dLoss                                  0.0123608
---------------------------------------  ----------------
2022-04-23 14:23:08 | [train_policy] epoch #667 | Obtaining samples for iteration 667...
2022-04-23 14:23:09 | [train_policy] epoch #667 | Logging diagnostics...
2022-04-23 14:23:09 | [train_policy] epoch #667 | Optimizing policy...
2022-04-23 14:23:09 | [train_policy] epoch #667 | Computing loss before
2022-04-23 14:23:09 | [train_policy] epoch #667 | Computing KL before
2022-04-23 14:23:09 | [train_policy] epoch #667 | Optimizing
2022-04-23 14:23:09 | [train_policy] epoch #667 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:09 | [train_policy] epoch #667 | computing loss before
2022-04-23 14:23:09 | [train_policy] epoch #667 | computing gradient
2022-04-23 14:23:09 | [train_policy] epoch #667 | gradient computed
2022-04-23 14:23:09 | [train_policy] epoch #667 | computing descent direction
2022-04-23 14:23:09 | [train_policy] epoch #667 | descent direction computed
2022-04-23 14:23:09 | [train_policy] epoch #667 | backtrack iters: 1
2022-04-23 14:23:09 | [train_policy] epoch #667 | optimization finished
2022-04-23 14:23:09 | [train_policy] epoch #667 | Computing KL after
2022-04-23 14:23:09 | [train_policy] epoch #667 | Computing loss after
2022-04-23 14:23:09 | [train_policy] epoch #667 | Fitting baseline...
2022-04-23 14:23:09 | [train_policy] epoch #667 | Saving snapshot...
2022-04-23 14:23:09 | [train_policy] epoch #667 | Saved
2022-04-23 14:23:09 | [train_policy] epoch #667 | Time 237.02 s
2022-04-23 14:23:09 | [train_policy] epoch #667 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.118771
Evaluation/AverageDiscountedReturn          -41.2382
Evaluation/AverageReturn                    -41.2382
Evaluation/CompletionRate                     0
Evaluation/Iteration                        667
Evaluation/MaxReturn                        -29.561
Evaluation/MinReturn                        -73.6219
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.71016
Extras/EpisodeRewardMean                    -41.2669
LinearFeatureBaseline/ExplainedVariance       0.869951
PolicyExecTime                                0.100261
ProcessExecTime                               0.0112932
TotalEnvSteps                            676016
policy/Entropy                               -1.26418
policy/KL                                     0.00674263
policy/KLBefore                               0
policy/LossAfter                             -0.0179151
policy/LossBefore                             5.4186e-09
policy/Perplexity                             0.282471
policy/dLoss                                  0.0179151
---------------------------------------  ---------------
2022-04-23 14:23:09 | [train_policy] epoch #668 | Obtaining samples for iteration 668...
2022-04-23 14:23:09 | [train_policy] epoch #668 | Logging diagnostics...
2022-04-23 14:23:09 | [train_policy] epoch #668 | Optimizing policy...
2022-04-23 14:23:09 | [train_policy] epoch #668 | Computing loss before
2022-04-23 14:23:09 | [train_policy] epoch #668 | Computing KL before
2022-04-23 14:23:09 | [train_policy] epoch #668 | Optimizing
2022-04-23 14:23:09 | [train_policy] epoch #668 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:09 | [train_policy] epoch #668 | computing loss before
2022-04-23 14:23:09 | [train_policy] epoch #668 | computing gradient
2022-04-23 14:23:09 | [train_policy] epoch #668 | gradient computed
2022-04-23 14:23:09 | [train_policy] epoch #668 | computing descent direction
2022-04-23 14:23:09 | [train_policy] epoch #668 | descent direction computed
2022-04-23 14:23:09 | [train_policy] epoch #668 | backtrack iters: 1
2022-04-23 14:23:09 | [train_policy] epoch #668 | optimization finished
2022-04-23 14:23:09 | [train_policy] epoch #668 | Computing KL after
2022-04-23 14:23:09 | [train_policy] epoch #668 | Computing loss after
2022-04-23 14:23:09 | [train_policy] epoch #668 | Fitting baseline...
2022-04-23 14:23:09 | [train_policy] epoch #668 | Saving snapshot...
2022-04-23 14:23:09 | [train_policy] epoch #668 | Saved
2022-04-23 14:23:09 | [train_policy] epoch #668 | Time 237.35 s
2022-04-23 14:23:09 | [train_policy] epoch #668 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.119196
Evaluation/AverageDiscountedReturn          -41.3142
Evaluation/AverageReturn                    -41.3142
Evaluation/CompletionRate                     0
Evaluation/Iteration                        668
Evaluation/MaxReturn                        -32.0024
Evaluation/MinReturn                        -72.8726
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.34554
Extras/EpisodeRewardMean                    -41.2408
LinearFeatureBaseline/ExplainedVariance       0.87169
PolicyExecTime                                0.100308
ProcessExecTime                               0.0113745
TotalEnvSteps                            677028
policy/Entropy                               -1.27733
policy/KL                                     0.00646457
policy/KLBefore                               0
policy/LossAfter                             -0.0182182
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.278779
policy/dLoss                                  0.0182182
---------------------------------------  ---------------
2022-04-23 14:23:09 | [train_policy] epoch #669 | Obtaining samples for iteration 669...
2022-04-23 14:23:09 | [train_policy] epoch #669 | Logging diagnostics...
2022-04-23 14:23:09 | [train_policy] epoch #669 | Optimizing policy...
2022-04-23 14:23:09 | [train_policy] epoch #669 | Computing loss before
2022-04-23 14:23:09 | [train_policy] epoch #669 | Computing KL before
2022-04-23 14:23:09 | [train_policy] epoch #669 | Optimizing
2022-04-23 14:23:09 | [train_policy] epoch #669 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:09 | [train_policy] epoch #669 | computing loss before
2022-04-23 14:23:09 | [train_policy] epoch #669 | computing gradient
2022-04-23 14:23:09 | [train_policy] epoch #669 | gradient computed
2022-04-23 14:23:09 | [train_policy] epoch #669 | computing descent direction
2022-04-23 14:23:09 | [train_policy] epoch #669 | descent direction computed
2022-04-23 14:23:09 | [train_policy] epoch #669 | backtrack iters: 1
2022-04-23 14:23:09 | [train_policy] epoch #669 | optimization finished
2022-04-23 14:23:09 | [train_policy] epoch #669 | Computing KL after
2022-04-23 14:23:09 | [train_policy] epoch #669 | Computing loss after
2022-04-23 14:23:09 | [train_policy] epoch #669 | Fitting baseline...
2022-04-23 14:23:09 | [train_policy] epoch #669 | Saving snapshot...
2022-04-23 14:23:09 | [train_policy] epoch #669 | Saved
2022-04-23 14:23:09 | [train_policy] epoch #669 | Time 237.67 s
2022-04-23 14:23:09 | [train_policy] epoch #669 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118547
Evaluation/AverageDiscountedReturn          -40.6951
Evaluation/AverageReturn                    -40.6951
Evaluation/CompletionRate                     0
Evaluation/Iteration                        669
Evaluation/MaxReturn                        -29.4373
Evaluation/MinReturn                        -74.9392
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.25115
Extras/EpisodeRewardMean                    -40.611
LinearFeatureBaseline/ExplainedVariance       0.867628
PolicyExecTime                                0.0927615
ProcessExecTime                               0.0111232
TotalEnvSteps                            678040
policy/Entropy                               -1.34448
policy/KL                                     0.00674155
policy/KLBefore                               0
policy/LossAfter                             -0.0149896
policy/LossBefore                             1.50779e-08
policy/Perplexity                             0.260675
policy/dLoss                                  0.0149896
---------------------------------------  ----------------
2022-04-23 14:23:09 | [train_policy] epoch #670 | Obtaining samples for iteration 670...
2022-04-23 14:23:10 | [train_policy] epoch #670 | Logging diagnostics...
2022-04-23 14:23:10 | [train_policy] epoch #670 | Optimizing policy...
2022-04-23 14:23:10 | [train_policy] epoch #670 | Computing loss before
2022-04-23 14:23:10 | [train_policy] epoch #670 | Computing KL before
2022-04-23 14:23:10 | [train_policy] epoch #670 | Optimizing
2022-04-23 14:23:10 | [train_policy] epoch #670 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:10 | [train_policy] epoch #670 | computing loss before
2022-04-23 14:23:10 | [train_policy] epoch #670 | computing gradient
2022-04-23 14:23:10 | [train_policy] epoch #670 | gradient computed
2022-04-23 14:23:10 | [train_policy] epoch #670 | computing descent direction
2022-04-23 14:23:10 | [train_policy] epoch #670 | descent direction computed
2022-04-23 14:23:10 | [train_policy] epoch #670 | backtrack iters: 0
2022-04-23 14:23:10 | [train_policy] epoch #670 | optimization finished
2022-04-23 14:23:10 | [train_policy] epoch #670 | Computing KL after
2022-04-23 14:23:10 | [train_policy] epoch #670 | Computing loss after
2022-04-23 14:23:10 | [train_policy] epoch #670 | Fitting baseline...
2022-04-23 14:23:10 | [train_policy] epoch #670 | Saving snapshot...
2022-04-23 14:23:10 | [train_policy] epoch #670 | Saved
2022-04-23 14:23:10 | [train_policy] epoch #670 | Time 238.00 s
2022-04-23 14:23:10 | [train_policy] epoch #670 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119229
Evaluation/AverageDiscountedReturn          -41.6021
Evaluation/AverageReturn                    -41.6021
Evaluation/CompletionRate                     0
Evaluation/Iteration                        670
Evaluation/MaxReturn                        -29.2525
Evaluation/MinReturn                        -72.7157
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.27621
Extras/EpisodeRewardMean                    -41.4406
LinearFeatureBaseline/ExplainedVariance       0.890932
PolicyExecTime                                0.0960913
ProcessExecTime                               0.0114422
TotalEnvSteps                            679052
policy/Entropy                               -1.34723
policy/KL                                     0.00981603
policy/KLBefore                               0
policy/LossAfter                             -0.0174749
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.25996
policy/dLoss                                  0.0174749
---------------------------------------  ----------------
2022-04-23 14:23:10 | [train_policy] epoch #671 | Obtaining samples for iteration 671...
2022-04-23 14:23:10 | [train_policy] epoch #671 | Logging diagnostics...
2022-04-23 14:23:10 | [train_policy] epoch #671 | Optimizing policy...
2022-04-23 14:23:10 | [train_policy] epoch #671 | Computing loss before
2022-04-23 14:23:10 | [train_policy] epoch #671 | Computing KL before
2022-04-23 14:23:10 | [train_policy] epoch #671 | Optimizing
2022-04-23 14:23:10 | [train_policy] epoch #671 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:10 | [train_policy] epoch #671 | computing loss before
2022-04-23 14:23:10 | [train_policy] epoch #671 | computing gradient
2022-04-23 14:23:10 | [train_policy] epoch #671 | gradient computed
2022-04-23 14:23:10 | [train_policy] epoch #671 | computing descent direction
2022-04-23 14:23:10 | [train_policy] epoch #671 | descent direction computed
2022-04-23 14:23:10 | [train_policy] epoch #671 | backtrack iters: 0
2022-04-23 14:23:10 | [train_policy] epoch #671 | optimization finished
2022-04-23 14:23:10 | [train_policy] epoch #671 | Computing KL after
2022-04-23 14:23:10 | [train_policy] epoch #671 | Computing loss after
2022-04-23 14:23:10 | [train_policy] epoch #671 | Fitting baseline...
2022-04-23 14:23:10 | [train_policy] epoch #671 | Saving snapshot...
2022-04-23 14:23:10 | [train_policy] epoch #671 | Saved
2022-04-23 14:23:10 | [train_policy] epoch #671 | Time 238.32 s
2022-04-23 14:23:10 | [train_policy] epoch #671 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118722
Evaluation/AverageDiscountedReturn          -42.6165
Evaluation/AverageReturn                    -42.6165
Evaluation/CompletionRate                     0
Evaluation/Iteration                        671
Evaluation/MaxReturn                        -30.9766
Evaluation/MinReturn                        -73.6174
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.71917
Extras/EpisodeRewardMean                    -42.698
LinearFeatureBaseline/ExplainedVariance       0.869679
PolicyExecTime                                0.0950234
ProcessExecTime                               0.0112593
TotalEnvSteps                            680064
policy/Entropy                               -1.32241
policy/KL                                     0.00991426
policy/KLBefore                               0
policy/LossAfter                             -0.0209101
policy/LossBefore                            -3.06269e-09
policy/Perplexity                             0.266492
policy/dLoss                                  0.0209101
---------------------------------------  ----------------
2022-04-23 14:23:10 | [train_policy] epoch #672 | Obtaining samples for iteration 672...
2022-04-23 14:23:10 | [train_policy] epoch #672 | Logging diagnostics...
2022-04-23 14:23:10 | [train_policy] epoch #672 | Optimizing policy...
2022-04-23 14:23:10 | [train_policy] epoch #672 | Computing loss before
2022-04-23 14:23:10 | [train_policy] epoch #672 | Computing KL before
2022-04-23 14:23:10 | [train_policy] epoch #672 | Optimizing
2022-04-23 14:23:10 | [train_policy] epoch #672 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:10 | [train_policy] epoch #672 | computing loss before
2022-04-23 14:23:10 | [train_policy] epoch #672 | computing gradient
2022-04-23 14:23:10 | [train_policy] epoch #672 | gradient computed
2022-04-23 14:23:10 | [train_policy] epoch #672 | computing descent direction
2022-04-23 14:23:10 | [train_policy] epoch #672 | descent direction computed
2022-04-23 14:23:10 | [train_policy] epoch #672 | backtrack iters: 1
2022-04-23 14:23:10 | [train_policy] epoch #672 | optimization finished
2022-04-23 14:23:10 | [train_policy] epoch #672 | Computing KL after
2022-04-23 14:23:10 | [train_policy] epoch #672 | Computing loss after
2022-04-23 14:23:10 | [train_policy] epoch #672 | Fitting baseline...
2022-04-23 14:23:10 | [train_policy] epoch #672 | Saving snapshot...
2022-04-23 14:23:10 | [train_policy] epoch #672 | Saved
2022-04-23 14:23:10 | [train_policy] epoch #672 | Time 238.65 s
2022-04-23 14:23:10 | [train_policy] epoch #672 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120214
Evaluation/AverageDiscountedReturn          -41.6614
Evaluation/AverageReturn                    -41.6614
Evaluation/CompletionRate                     0
Evaluation/Iteration                        672
Evaluation/MaxReturn                        -30.107
Evaluation/MinReturn                        -72.9209
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.79587
Extras/EpisodeRewardMean                    -41.4077
LinearFeatureBaseline/ExplainedVariance       0.892091
PolicyExecTime                                0.0955906
ProcessExecTime                               0.0113068
TotalEnvSteps                            681076
policy/Entropy                               -1.32258
policy/KL                                     0.00659868
policy/KLBefore                               0
policy/LossAfter                             -0.0203612
policy/LossBefore                            -9.42366e-09
policy/Perplexity                             0.266448
policy/dLoss                                  0.0203612
---------------------------------------  ----------------
2022-04-23 14:23:10 | [train_policy] epoch #673 | Obtaining samples for iteration 673...
2022-04-23 14:23:11 | [train_policy] epoch #673 | Logging diagnostics...
2022-04-23 14:23:11 | [train_policy] epoch #673 | Optimizing policy...
2022-04-23 14:23:11 | [train_policy] epoch #673 | Computing loss before
2022-04-23 14:23:11 | [train_policy] epoch #673 | Computing KL before
2022-04-23 14:23:11 | [train_policy] epoch #673 | Optimizing
2022-04-23 14:23:11 | [train_policy] epoch #673 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:11 | [train_policy] epoch #673 | computing loss before
2022-04-23 14:23:11 | [train_policy] epoch #673 | computing gradient
2022-04-23 14:23:11 | [train_policy] epoch #673 | gradient computed
2022-04-23 14:23:11 | [train_policy] epoch #673 | computing descent direction
2022-04-23 14:23:11 | [train_policy] epoch #673 | descent direction computed
2022-04-23 14:23:11 | [train_policy] epoch #673 | backtrack iters: 0
2022-04-23 14:23:11 | [train_policy] epoch #673 | optimization finished
2022-04-23 14:23:11 | [train_policy] epoch #673 | Computing KL after
2022-04-23 14:23:11 | [train_policy] epoch #673 | Computing loss after
2022-04-23 14:23:11 | [train_policy] epoch #673 | Fitting baseline...
2022-04-23 14:23:11 | [train_policy] epoch #673 | Saving snapshot...
2022-04-23 14:23:11 | [train_policy] epoch #673 | Saved
2022-04-23 14:23:11 | [train_policy] epoch #673 | Time 238.98 s
2022-04-23 14:23:11 | [train_policy] epoch #673 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.119
Evaluation/AverageDiscountedReturn          -41.4946
Evaluation/AverageReturn                    -41.4946
Evaluation/CompletionRate                     0
Evaluation/Iteration                        673
Evaluation/MaxReturn                        -30.0504
Evaluation/MinReturn                        -64.0935
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43021
Extras/EpisodeRewardMean                    -41.5001
LinearFeatureBaseline/ExplainedVariance       0.881127
PolicyExecTime                                0.100936
ProcessExecTime                               0.0111787
TotalEnvSteps                            682088
policy/Entropy                               -1.26019
policy/KL                                     0.00951878
policy/KLBefore                               0
policy/LossAfter                             -0.0249381
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.2836
policy/dLoss                                  0.0249381
---------------------------------------  ---------------
2022-04-23 14:23:11 | [train_policy] epoch #674 | Obtaining samples for iteration 674...
2022-04-23 14:23:11 | [train_policy] epoch #674 | Logging diagnostics...
2022-04-23 14:23:11 | [train_policy] epoch #674 | Optimizing policy...
2022-04-23 14:23:11 | [train_policy] epoch #674 | Computing loss before
2022-04-23 14:23:11 | [train_policy] epoch #674 | Computing KL before
2022-04-23 14:23:11 | [train_policy] epoch #674 | Optimizing
2022-04-23 14:23:11 | [train_policy] epoch #674 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:11 | [train_policy] epoch #674 | computing loss before
2022-04-23 14:23:11 | [train_policy] epoch #674 | computing gradient
2022-04-23 14:23:11 | [train_policy] epoch #674 | gradient computed
2022-04-23 14:23:11 | [train_policy] epoch #674 | computing descent direction
2022-04-23 14:23:11 | [train_policy] epoch #674 | descent direction computed
2022-04-23 14:23:11 | [train_policy] epoch #674 | backtrack iters: 1
2022-04-23 14:23:11 | [train_policy] epoch #674 | optimization finished
2022-04-23 14:23:11 | [train_policy] epoch #674 | Computing KL after
2022-04-23 14:23:11 | [train_policy] epoch #674 | Computing loss after
2022-04-23 14:23:11 | [train_policy] epoch #674 | Fitting baseline...
2022-04-23 14:23:11 | [train_policy] epoch #674 | Saving snapshot...
2022-04-23 14:23:11 | [train_policy] epoch #674 | Saved
2022-04-23 14:23:11 | [train_policy] epoch #674 | Time 239.32 s
2022-04-23 14:23:11 | [train_policy] epoch #674 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119025
Evaluation/AverageDiscountedReturn          -41.2927
Evaluation/AverageReturn                    -41.2927
Evaluation/CompletionRate                     0
Evaluation/Iteration                        674
Evaluation/MaxReturn                        -28.7583
Evaluation/MinReturn                        -64.3479
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.76607
Extras/EpisodeRewardMean                    -41.3878
LinearFeatureBaseline/ExplainedVariance       0.878159
PolicyExecTime                                0.110822
ProcessExecTime                               0.0113757
TotalEnvSteps                            683100
policy/Entropy                               -1.2499
policy/KL                                     0.00672521
policy/KLBefore                               0
policy/LossAfter                             -0.0125674
policy/LossBefore                            -1.43711e-08
policy/Perplexity                             0.286533
policy/dLoss                                  0.0125674
---------------------------------------  ----------------
2022-04-23 14:23:11 | [train_policy] epoch #675 | Obtaining samples for iteration 675...
2022-04-23 14:23:11 | [train_policy] epoch #675 | Logging diagnostics...
2022-04-23 14:23:11 | [train_policy] epoch #675 | Optimizing policy...
2022-04-23 14:23:11 | [train_policy] epoch #675 | Computing loss before
2022-04-23 14:23:11 | [train_policy] epoch #675 | Computing KL before
2022-04-23 14:23:11 | [train_policy] epoch #675 | Optimizing
2022-04-23 14:23:11 | [train_policy] epoch #675 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:11 | [train_policy] epoch #675 | computing loss before
2022-04-23 14:23:11 | [train_policy] epoch #675 | computing gradient
2022-04-23 14:23:11 | [train_policy] epoch #675 | gradient computed
2022-04-23 14:23:11 | [train_policy] epoch #675 | computing descent direction
2022-04-23 14:23:11 | [train_policy] epoch #675 | descent direction computed
2022-04-23 14:23:11 | [train_policy] epoch #675 | backtrack iters: 1
2022-04-23 14:23:11 | [train_policy] epoch #675 | optimization finished
2022-04-23 14:23:11 | [train_policy] epoch #675 | Computing KL after
2022-04-23 14:23:11 | [train_policy] epoch #675 | Computing loss after
2022-04-23 14:23:11 | [train_policy] epoch #675 | Fitting baseline...
2022-04-23 14:23:11 | [train_policy] epoch #675 | Saving snapshot...
2022-04-23 14:23:11 | [train_policy] epoch #675 | Saved
2022-04-23 14:23:11 | [train_policy] epoch #675 | Time 239.68 s
2022-04-23 14:23:11 | [train_policy] epoch #675 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.126424
Evaluation/AverageDiscountedReturn          -40.9378
Evaluation/AverageReturn                    -40.9378
Evaluation/CompletionRate                     0
Evaluation/Iteration                        675
Evaluation/MaxReturn                        -29.2971
Evaluation/MinReturn                        -72.5114
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.93525
Extras/EpisodeRewardMean                    -40.7964
LinearFeatureBaseline/ExplainedVariance       0.868471
PolicyExecTime                                0.111401
ProcessExecTime                               0.012486
TotalEnvSteps                            684112
policy/Entropy                               -1.29448
policy/KL                                     0.00668807
policy/KLBefore                               0
policy/LossAfter                             -0.0140418
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.274039
policy/dLoss                                  0.0140418
---------------------------------------  ----------------
2022-04-23 14:23:11 | [train_policy] epoch #676 | Obtaining samples for iteration 676...
2022-04-23 14:23:12 | [train_policy] epoch #676 | Logging diagnostics...
2022-04-23 14:23:12 | [train_policy] epoch #676 | Optimizing policy...
2022-04-23 14:23:12 | [train_policy] epoch #676 | Computing loss before
2022-04-23 14:23:12 | [train_policy] epoch #676 | Computing KL before
2022-04-23 14:23:12 | [train_policy] epoch #676 | Optimizing
2022-04-23 14:23:12 | [train_policy] epoch #676 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:12 | [train_policy] epoch #676 | computing loss before
2022-04-23 14:23:12 | [train_policy] epoch #676 | computing gradient
2022-04-23 14:23:12 | [train_policy] epoch #676 | gradient computed
2022-04-23 14:23:12 | [train_policy] epoch #676 | computing descent direction
2022-04-23 14:23:12 | [train_policy] epoch #676 | descent direction computed
2022-04-23 14:23:12 | [train_policy] epoch #676 | backtrack iters: 0
2022-04-23 14:23:12 | [train_policy] epoch #676 | optimization finished
2022-04-23 14:23:12 | [train_policy] epoch #676 | Computing KL after
2022-04-23 14:23:12 | [train_policy] epoch #676 | Computing loss after
2022-04-23 14:23:12 | [train_policy] epoch #676 | Fitting baseline...
2022-04-23 14:23:12 | [train_policy] epoch #676 | Saving snapshot...
2022-04-23 14:23:12 | [train_policy] epoch #676 | Saved
2022-04-23 14:23:12 | [train_policy] epoch #676 | Time 240.04 s
2022-04-23 14:23:12 | [train_policy] epoch #676 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.125949
Evaluation/AverageDiscountedReturn          -40.6691
Evaluation/AverageReturn                    -40.6691
Evaluation/CompletionRate                     0
Evaluation/Iteration                        676
Evaluation/MaxReturn                        -28.9752
Evaluation/MinReturn                        -60.885
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.67444
Extras/EpisodeRewardMean                    -40.4745
LinearFeatureBaseline/ExplainedVariance       0.910835
PolicyExecTime                                0.115588
ProcessExecTime                               0.0121193
TotalEnvSteps                            685124
policy/Entropy                               -1.27493
policy/KL                                     0.00992266
policy/KLBefore                               0
policy/LossAfter                             -0.0240814
policy/LossBefore                            -2.12032e-09
policy/Perplexity                             0.279449
policy/dLoss                                  0.0240814
---------------------------------------  ----------------
2022-04-23 14:23:12 | [train_policy] epoch #677 | Obtaining samples for iteration 677...
2022-04-23 14:23:12 | [train_policy] epoch #677 | Logging diagnostics...
2022-04-23 14:23:12 | [train_policy] epoch #677 | Optimizing policy...
2022-04-23 14:23:12 | [train_policy] epoch #677 | Computing loss before
2022-04-23 14:23:12 | [train_policy] epoch #677 | Computing KL before
2022-04-23 14:23:12 | [train_policy] epoch #677 | Optimizing
2022-04-23 14:23:12 | [train_policy] epoch #677 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:12 | [train_policy] epoch #677 | computing loss before
2022-04-23 14:23:12 | [train_policy] epoch #677 | computing gradient
2022-04-23 14:23:12 | [train_policy] epoch #677 | gradient computed
2022-04-23 14:23:12 | [train_policy] epoch #677 | computing descent direction
2022-04-23 14:23:12 | [train_policy] epoch #677 | descent direction computed
2022-04-23 14:23:12 | [train_policy] epoch #677 | backtrack iters: 1
2022-04-23 14:23:12 | [train_policy] epoch #677 | optimization finished
2022-04-23 14:23:12 | [train_policy] epoch #677 | Computing KL after
2022-04-23 14:23:12 | [train_policy] epoch #677 | Computing loss after
2022-04-23 14:23:12 | [train_policy] epoch #677 | Fitting baseline...
2022-04-23 14:23:12 | [train_policy] epoch #677 | Saving snapshot...
2022-04-23 14:23:12 | [train_policy] epoch #677 | Saved
2022-04-23 14:23:12 | [train_policy] epoch #677 | Time 240.37 s
2022-04-23 14:23:12 | [train_policy] epoch #677 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.121808
Evaluation/AverageDiscountedReturn          -41.3843
Evaluation/AverageReturn                    -41.3843
Evaluation/CompletionRate                     0
Evaluation/Iteration                        677
Evaluation/MaxReturn                        -29.0158
Evaluation/MinReturn                        -67.4554
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.01888
Extras/EpisodeRewardMean                    -41.2583
LinearFeatureBaseline/ExplainedVariance       0.88404
PolicyExecTime                                0.0957718
ProcessExecTime                               0.0119798
TotalEnvSteps                            686136
policy/Entropy                               -1.31481
policy/KL                                     0.00686118
policy/KLBefore                               0
policy/LossAfter                             -0.0152785
policy/LossBefore                             9.18807e-09
policy/Perplexity                             0.268527
policy/dLoss                                  0.0152785
---------------------------------------  ----------------
2022-04-23 14:23:12 | [train_policy] epoch #678 | Obtaining samples for iteration 678...
2022-04-23 14:23:12 | [train_policy] epoch #678 | Logging diagnostics...
2022-04-23 14:23:12 | [train_policy] epoch #678 | Optimizing policy...
2022-04-23 14:23:12 | [train_policy] epoch #678 | Computing loss before
2022-04-23 14:23:12 | [train_policy] epoch #678 | Computing KL before
2022-04-23 14:23:12 | [train_policy] epoch #678 | Optimizing
2022-04-23 14:23:12 | [train_policy] epoch #678 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:12 | [train_policy] epoch #678 | computing loss before
2022-04-23 14:23:12 | [train_policy] epoch #678 | computing gradient
2022-04-23 14:23:12 | [train_policy] epoch #678 | gradient computed
2022-04-23 14:23:12 | [train_policy] epoch #678 | computing descent direction
2022-04-23 14:23:12 | [train_policy] epoch #678 | descent direction computed
2022-04-23 14:23:12 | [train_policy] epoch #678 | backtrack iters: 1
2022-04-23 14:23:12 | [train_policy] epoch #678 | optimization finished
2022-04-23 14:23:12 | [train_policy] epoch #678 | Computing KL after
2022-04-23 14:23:12 | [train_policy] epoch #678 | Computing loss after
2022-04-23 14:23:12 | [train_policy] epoch #678 | Fitting baseline...
2022-04-23 14:23:12 | [train_policy] epoch #678 | Saving snapshot...
2022-04-23 14:23:12 | [train_policy] epoch #678 | Saved
2022-04-23 14:23:12 | [train_policy] epoch #678 | Time 240.70 s
2022-04-23 14:23:12 | [train_policy] epoch #678 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119171
Evaluation/AverageDiscountedReturn          -41.777
Evaluation/AverageReturn                    -41.777
Evaluation/CompletionRate                     0
Evaluation/Iteration                        678
Evaluation/MaxReturn                        -29.3884
Evaluation/MinReturn                        -74.0007
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.63385
Extras/EpisodeRewardMean                    -41.9038
LinearFeatureBaseline/ExplainedVariance       0.871918
PolicyExecTime                                0.0954292
ProcessExecTime                               0.0115504
TotalEnvSteps                            687148
policy/Entropy                               -1.33252
policy/KL                                     0.00675178
policy/KLBefore                               0
policy/LossAfter                             -0.0190025
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.263811
policy/dLoss                                  0.0190025
---------------------------------------  ----------------
2022-04-23 14:23:12 | [train_policy] epoch #679 | Obtaining samples for iteration 679...
2022-04-23 14:23:13 | [train_policy] epoch #679 | Logging diagnostics...
2022-04-23 14:23:13 | [train_policy] epoch #679 | Optimizing policy...
2022-04-23 14:23:13 | [train_policy] epoch #679 | Computing loss before
2022-04-23 14:23:13 | [train_policy] epoch #679 | Computing KL before
2022-04-23 14:23:13 | [train_policy] epoch #679 | Optimizing
2022-04-23 14:23:13 | [train_policy] epoch #679 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:13 | [train_policy] epoch #679 | computing loss before
2022-04-23 14:23:13 | [train_policy] epoch #679 | computing gradient
2022-04-23 14:23:13 | [train_policy] epoch #679 | gradient computed
2022-04-23 14:23:13 | [train_policy] epoch #679 | computing descent direction
2022-04-23 14:23:13 | [train_policy] epoch #679 | descent direction computed
2022-04-23 14:23:13 | [train_policy] epoch #679 | backtrack iters: 1
2022-04-23 14:23:13 | [train_policy] epoch #679 | optimization finished
2022-04-23 14:23:13 | [train_policy] epoch #679 | Computing KL after
2022-04-23 14:23:13 | [train_policy] epoch #679 | Computing loss after
2022-04-23 14:23:13 | [train_policy] epoch #679 | Fitting baseline...
2022-04-23 14:23:13 | [train_policy] epoch #679 | Saving snapshot...
2022-04-23 14:23:13 | [train_policy] epoch #679 | Saved
2022-04-23 14:23:13 | [train_policy] epoch #679 | Time 241.03 s
2022-04-23 14:23:13 | [train_policy] epoch #679 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118476
Evaluation/AverageDiscountedReturn          -42.2268
Evaluation/AverageReturn                    -42.2268
Evaluation/CompletionRate                     0
Evaluation/Iteration                        679
Evaluation/MaxReturn                        -29.3346
Evaluation/MinReturn                        -72.3486
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.18673
Extras/EpisodeRewardMean                    -42.1351
LinearFeatureBaseline/ExplainedVariance       0.876526
PolicyExecTime                                0.0937028
ProcessExecTime                               0.0110598
TotalEnvSteps                            688160
policy/Entropy                               -1.32215
policy/KL                                     0.00645968
policy/KLBefore                               0
policy/LossAfter                             -0.0134245
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.266563
policy/dLoss                                  0.0134245
---------------------------------------  ----------------
2022-04-23 14:23:13 | [train_policy] epoch #680 | Obtaining samples for iteration 680...
2022-04-23 14:23:13 | [train_policy] epoch #680 | Logging diagnostics...
2022-04-23 14:23:13 | [train_policy] epoch #680 | Optimizing policy...
2022-04-23 14:23:13 | [train_policy] epoch #680 | Computing loss before
2022-04-23 14:23:13 | [train_policy] epoch #680 | Computing KL before
2022-04-23 14:23:13 | [train_policy] epoch #680 | Optimizing
2022-04-23 14:23:13 | [train_policy] epoch #680 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:13 | [train_policy] epoch #680 | computing loss before
2022-04-23 14:23:13 | [train_policy] epoch #680 | computing gradient
2022-04-23 14:23:13 | [train_policy] epoch #680 | gradient computed
2022-04-23 14:23:13 | [train_policy] epoch #680 | computing descent direction
2022-04-23 14:23:13 | [train_policy] epoch #680 | descent direction computed
2022-04-23 14:23:13 | [train_policy] epoch #680 | backtrack iters: 0
2022-04-23 14:23:13 | [train_policy] epoch #680 | optimization finished
2022-04-23 14:23:13 | [train_policy] epoch #680 | Computing KL after
2022-04-23 14:23:13 | [train_policy] epoch #680 | Computing loss after
2022-04-23 14:23:13 | [train_policy] epoch #680 | Fitting baseline...
2022-04-23 14:23:13 | [train_policy] epoch #680 | Saving snapshot...
2022-04-23 14:23:13 | [train_policy] epoch #680 | Saved
2022-04-23 14:23:13 | [train_policy] epoch #680 | Time 241.37 s
2022-04-23 14:23:13 | [train_policy] epoch #680 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.125243
Evaluation/AverageDiscountedReturn          -41.7308
Evaluation/AverageReturn                    -41.7308
Evaluation/CompletionRate                     0
Evaluation/Iteration                        680
Evaluation/MaxReturn                        -30.4749
Evaluation/MinReturn                        -72.4178
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.55558
Extras/EpisodeRewardMean                    -41.5119
LinearFeatureBaseline/ExplainedVariance       0.880129
PolicyExecTime                                0.0972455
ProcessExecTime                               0.0119944
TotalEnvSteps                            689172
policy/Entropy                               -1.27667
policy/KL                                     0.00924253
policy/KLBefore                               0
policy/LossAfter                             -0.0200774
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             0.278965
policy/dLoss                                  0.0200774
---------------------------------------  ----------------
2022-04-23 14:23:13 | [train_policy] epoch #681 | Obtaining samples for iteration 681...
2022-04-23 14:23:13 | [train_policy] epoch #681 | Logging diagnostics...
2022-04-23 14:23:13 | [train_policy] epoch #681 | Optimizing policy...
2022-04-23 14:23:13 | [train_policy] epoch #681 | Computing loss before
2022-04-23 14:23:13 | [train_policy] epoch #681 | Computing KL before
2022-04-23 14:23:13 | [train_policy] epoch #681 | Optimizing
2022-04-23 14:23:13 | [train_policy] epoch #681 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:13 | [train_policy] epoch #681 | computing loss before
2022-04-23 14:23:13 | [train_policy] epoch #681 | computing gradient
2022-04-23 14:23:13 | [train_policy] epoch #681 | gradient computed
2022-04-23 14:23:13 | [train_policy] epoch #681 | computing descent direction
2022-04-23 14:23:13 | [train_policy] epoch #681 | descent direction computed
2022-04-23 14:23:13 | [train_policy] epoch #681 | backtrack iters: 1
2022-04-23 14:23:13 | [train_policy] epoch #681 | optimization finished
2022-04-23 14:23:13 | [train_policy] epoch #681 | Computing KL after
2022-04-23 14:23:13 | [train_policy] epoch #681 | Computing loss after
2022-04-23 14:23:13 | [train_policy] epoch #681 | Fitting baseline...
2022-04-23 14:23:13 | [train_policy] epoch #681 | Saving snapshot...
2022-04-23 14:23:13 | [train_policy] epoch #681 | Saved
2022-04-23 14:23:13 | [train_policy] epoch #681 | Time 241.69 s
2022-04-23 14:23:13 | [train_policy] epoch #681 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118855
Evaluation/AverageDiscountedReturn          -39.9123
Evaluation/AverageReturn                    -39.9123
Evaluation/CompletionRate                     0
Evaluation/Iteration                        681
Evaluation/MaxReturn                        -30.8164
Evaluation/MinReturn                        -71.6379
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.37114
Extras/EpisodeRewardMean                    -40.0542
LinearFeatureBaseline/ExplainedVariance       0.879085
PolicyExecTime                                0.0949121
ProcessExecTime                               0.0112569
TotalEnvSteps                            690184
policy/Entropy                               -1.24843
policy/KL                                     0.00643774
policy/KLBefore                               0
policy/LossAfter                             -0.02097
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.286954
policy/dLoss                                  0.02097
---------------------------------------  ----------------
2022-04-23 14:23:13 | [train_policy] epoch #682 | Obtaining samples for iteration 682...
2022-04-23 14:23:14 | [train_policy] epoch #682 | Logging diagnostics...
2022-04-23 14:23:14 | [train_policy] epoch #682 | Optimizing policy...
2022-04-23 14:23:14 | [train_policy] epoch #682 | Computing loss before
2022-04-23 14:23:14 | [train_policy] epoch #682 | Computing KL before
2022-04-23 14:23:14 | [train_policy] epoch #682 | Optimizing
2022-04-23 14:23:14 | [train_policy] epoch #682 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:14 | [train_policy] epoch #682 | computing loss before
2022-04-23 14:23:14 | [train_policy] epoch #682 | computing gradient
2022-04-23 14:23:14 | [train_policy] epoch #682 | gradient computed
2022-04-23 14:23:14 | [train_policy] epoch #682 | computing descent direction
2022-04-23 14:23:14 | [train_policy] epoch #682 | descent direction computed
2022-04-23 14:23:14 | [train_policy] epoch #682 | backtrack iters: 0
2022-04-23 14:23:14 | [train_policy] epoch #682 | optimization finished
2022-04-23 14:23:14 | [train_policy] epoch #682 | Computing KL after
2022-04-23 14:23:14 | [train_policy] epoch #682 | Computing loss after
2022-04-23 14:23:14 | [train_policy] epoch #682 | Fitting baseline...
2022-04-23 14:23:14 | [train_policy] epoch #682 | Saving snapshot...
2022-04-23 14:23:14 | [train_policy] epoch #682 | Saved
2022-04-23 14:23:14 | [train_policy] epoch #682 | Time 242.02 s
2022-04-23 14:23:14 | [train_policy] epoch #682 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.119459
Evaluation/AverageDiscountedReturn          -42.2073
Evaluation/AverageReturn                    -42.2073
Evaluation/CompletionRate                     0
Evaluation/Iteration                        682
Evaluation/MaxReturn                        -32.494
Evaluation/MinReturn                        -72.292
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.62917
Extras/EpisodeRewardMean                    -42.2685
LinearFeatureBaseline/ExplainedVariance       0.870725
PolicyExecTime                                0.0960507
ProcessExecTime                               0.0111966
TotalEnvSteps                            691196
policy/Entropy                               -1.23463
policy/KL                                     0.00938003
policy/KLBefore                               0
policy/LossAfter                             -0.0124259
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.290942
policy/dLoss                                  0.0124259
---------------------------------------  ---------------
2022-04-23 14:23:14 | [train_policy] epoch #683 | Obtaining samples for iteration 683...
2022-04-23 14:23:14 | [train_policy] epoch #683 | Logging diagnostics...
2022-04-23 14:23:14 | [train_policy] epoch #683 | Optimizing policy...
2022-04-23 14:23:14 | [train_policy] epoch #683 | Computing loss before
2022-04-23 14:23:14 | [train_policy] epoch #683 | Computing KL before
2022-04-23 14:23:14 | [train_policy] epoch #683 | Optimizing
2022-04-23 14:23:14 | [train_policy] epoch #683 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:14 | [train_policy] epoch #683 | computing loss before
2022-04-23 14:23:14 | [train_policy] epoch #683 | computing gradient
2022-04-23 14:23:14 | [train_policy] epoch #683 | gradient computed
2022-04-23 14:23:14 | [train_policy] epoch #683 | computing descent direction
2022-04-23 14:23:14 | [train_policy] epoch #683 | descent direction computed
2022-04-23 14:23:14 | [train_policy] epoch #683 | backtrack iters: 1
2022-04-23 14:23:14 | [train_policy] epoch #683 | optimization finished
2022-04-23 14:23:14 | [train_policy] epoch #683 | Computing KL after
2022-04-23 14:23:14 | [train_policy] epoch #683 | Computing loss after
2022-04-23 14:23:14 | [train_policy] epoch #683 | Fitting baseline...
2022-04-23 14:23:14 | [train_policy] epoch #683 | Saving snapshot...
2022-04-23 14:23:14 | [train_policy] epoch #683 | Saved
2022-04-23 14:23:14 | [train_policy] epoch #683 | Time 242.34 s
2022-04-23 14:23:14 | [train_policy] epoch #683 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119624
Evaluation/AverageDiscountedReturn          -41.3784
Evaluation/AverageReturn                    -41.3784
Evaluation/CompletionRate                     0
Evaluation/Iteration                        683
Evaluation/MaxReturn                        -30.0338
Evaluation/MinReturn                        -72.9027
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.53463
Extras/EpisodeRewardMean                    -41.6338
LinearFeatureBaseline/ExplainedVariance       0.879835
PolicyExecTime                                0.0961919
ProcessExecTime                               0.0113618
TotalEnvSteps                            692208
policy/Entropy                               -1.2582
policy/KL                                     0.00668468
policy/KLBefore                               0
policy/LossAfter                             -0.0178066
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             0.284164
policy/dLoss                                  0.0178066
---------------------------------------  ----------------
2022-04-23 14:23:14 | [train_policy] epoch #684 | Obtaining samples for iteration 684...
2022-04-23 14:23:14 | [train_policy] epoch #684 | Logging diagnostics...
2022-04-23 14:23:14 | [train_policy] epoch #684 | Optimizing policy...
2022-04-23 14:23:14 | [train_policy] epoch #684 | Computing loss before
2022-04-23 14:23:14 | [train_policy] epoch #684 | Computing KL before
2022-04-23 14:23:14 | [train_policy] epoch #684 | Optimizing
2022-04-23 14:23:14 | [train_policy] epoch #684 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:14 | [train_policy] epoch #684 | computing loss before
2022-04-23 14:23:14 | [train_policy] epoch #684 | computing gradient
2022-04-23 14:23:14 | [train_policy] epoch #684 | gradient computed
2022-04-23 14:23:14 | [train_policy] epoch #684 | computing descent direction
2022-04-23 14:23:14 | [train_policy] epoch #684 | descent direction computed
2022-04-23 14:23:14 | [train_policy] epoch #684 | backtrack iters: 1
2022-04-23 14:23:14 | [train_policy] epoch #684 | optimization finished
2022-04-23 14:23:14 | [train_policy] epoch #684 | Computing KL after
2022-04-23 14:23:14 | [train_policy] epoch #684 | Computing loss after
2022-04-23 14:23:14 | [train_policy] epoch #684 | Fitting baseline...
2022-04-23 14:23:14 | [train_policy] epoch #684 | Saving snapshot...
2022-04-23 14:23:14 | [train_policy] epoch #684 | Saved
2022-04-23 14:23:14 | [train_policy] epoch #684 | Time 242.67 s
2022-04-23 14:23:14 | [train_policy] epoch #684 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118951
Evaluation/AverageDiscountedReturn          -39.953
Evaluation/AverageReturn                    -39.953
Evaluation/CompletionRate                     0
Evaluation/Iteration                        684
Evaluation/MaxReturn                        -31.9877
Evaluation/MinReturn                        -72.236
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.81551
Extras/EpisodeRewardMean                    -40.1124
LinearFeatureBaseline/ExplainedVariance       0.902117
PolicyExecTime                                0.0946581
ProcessExecTime                               0.0111518
TotalEnvSteps                            693220
policy/Entropy                               -1.31944
policy/KL                                     0.00653621
policy/KLBefore                               0
policy/LossAfter                             -0.0137424
policy/LossBefore                             1.10728e-08
policy/Perplexity                             0.267284
policy/dLoss                                  0.0137424
---------------------------------------  ----------------
2022-04-23 14:23:14 | [train_policy] epoch #685 | Obtaining samples for iteration 685...
2022-04-23 14:23:15 | [train_policy] epoch #685 | Logging diagnostics...
2022-04-23 14:23:15 | [train_policy] epoch #685 | Optimizing policy...
2022-04-23 14:23:15 | [train_policy] epoch #685 | Computing loss before
2022-04-23 14:23:15 | [train_policy] epoch #685 | Computing KL before
2022-04-23 14:23:15 | [train_policy] epoch #685 | Optimizing
2022-04-23 14:23:15 | [train_policy] epoch #685 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:15 | [train_policy] epoch #685 | computing loss before
2022-04-23 14:23:15 | [train_policy] epoch #685 | computing gradient
2022-04-23 14:23:15 | [train_policy] epoch #685 | gradient computed
2022-04-23 14:23:15 | [train_policy] epoch #685 | computing descent direction
2022-04-23 14:23:15 | [train_policy] epoch #685 | descent direction computed
2022-04-23 14:23:15 | [train_policy] epoch #685 | backtrack iters: 1
2022-04-23 14:23:15 | [train_policy] epoch #685 | optimization finished
2022-04-23 14:23:15 | [train_policy] epoch #685 | Computing KL after
2022-04-23 14:23:15 | [train_policy] epoch #685 | Computing loss after
2022-04-23 14:23:15 | [train_policy] epoch #685 | Fitting baseline...
2022-04-23 14:23:15 | [train_policy] epoch #685 | Saving snapshot...
2022-04-23 14:23:15 | [train_policy] epoch #685 | Saved
2022-04-23 14:23:15 | [train_policy] epoch #685 | Time 242.99 s
2022-04-23 14:23:15 | [train_policy] epoch #685 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.118765
Evaluation/AverageDiscountedReturn          -41.7146
Evaluation/AverageReturn                    -41.7146
Evaluation/CompletionRate                     0
Evaluation/Iteration                        685
Evaluation/MaxReturn                        -28.9627
Evaluation/MinReturn                        -63.9645
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.66784
Extras/EpisodeRewardMean                    -41.3306
LinearFeatureBaseline/ExplainedVariance       0.883515
PolicyExecTime                                0.091043
ProcessExecTime                               0.0110481
TotalEnvSteps                            694232
policy/Entropy                               -1.34689
policy/KL                                     0.00647984
policy/KLBefore                               0
policy/LossAfter                             -0.0118585
policy/LossBefore                             1.93185e-08
policy/Perplexity                             0.260047
policy/dLoss                                  0.0118585
---------------------------------------  ----------------
2022-04-23 14:23:15 | [train_policy] epoch #686 | Obtaining samples for iteration 686...
2022-04-23 14:23:15 | [train_policy] epoch #686 | Logging diagnostics...
2022-04-23 14:23:15 | [train_policy] epoch #686 | Optimizing policy...
2022-04-23 14:23:15 | [train_policy] epoch #686 | Computing loss before
2022-04-23 14:23:15 | [train_policy] epoch #686 | Computing KL before
2022-04-23 14:23:15 | [train_policy] epoch #686 | Optimizing
2022-04-23 14:23:15 | [train_policy] epoch #686 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:15 | [train_policy] epoch #686 | computing loss before
2022-04-23 14:23:15 | [train_policy] epoch #686 | computing gradient
2022-04-23 14:23:15 | [train_policy] epoch #686 | gradient computed
2022-04-23 14:23:15 | [train_policy] epoch #686 | computing descent direction
2022-04-23 14:23:15 | [train_policy] epoch #686 | descent direction computed
2022-04-23 14:23:15 | [train_policy] epoch #686 | backtrack iters: 1
2022-04-23 14:23:15 | [train_policy] epoch #686 | optimization finished
2022-04-23 14:23:15 | [train_policy] epoch #686 | Computing KL after
2022-04-23 14:23:15 | [train_policy] epoch #686 | Computing loss after
2022-04-23 14:23:15 | [train_policy] epoch #686 | Fitting baseline...
2022-04-23 14:23:15 | [train_policy] epoch #686 | Saving snapshot...
2022-04-23 14:23:15 | [train_policy] epoch #686 | Saved
2022-04-23 14:23:15 | [train_policy] epoch #686 | Time 243.32 s
2022-04-23 14:23:15 | [train_policy] epoch #686 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.11877
Evaluation/AverageDiscountedReturn          -41.6332
Evaluation/AverageReturn                    -41.6332
Evaluation/CompletionRate                     0
Evaluation/Iteration                        686
Evaluation/MaxReturn                        -29.9427
Evaluation/MinReturn                        -63.9427
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05011
Extras/EpisodeRewardMean                    -41.7366
LinearFeatureBaseline/ExplainedVariance       0.880333
PolicyExecTime                                0.0968499
ProcessExecTime                               0.0109975
TotalEnvSteps                            695244
policy/Entropy                               -1.34341
policy/KL                                     0.00650236
policy/KLBefore                               0
policy/LossAfter                             -0.0118828
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.260954
policy/dLoss                                  0.0118828
---------------------------------------  ---------------
2022-04-23 14:23:15 | [train_policy] epoch #687 | Obtaining samples for iteration 687...
2022-04-23 14:23:15 | [train_policy] epoch #687 | Logging diagnostics...
2022-04-23 14:23:15 | [train_policy] epoch #687 | Optimizing policy...
2022-04-23 14:23:15 | [train_policy] epoch #687 | Computing loss before
2022-04-23 14:23:15 | [train_policy] epoch #687 | Computing KL before
2022-04-23 14:23:15 | [train_policy] epoch #687 | Optimizing
2022-04-23 14:23:15 | [train_policy] epoch #687 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:15 | [train_policy] epoch #687 | computing loss before
2022-04-23 14:23:15 | [train_policy] epoch #687 | computing gradient
2022-04-23 14:23:15 | [train_policy] epoch #687 | gradient computed
2022-04-23 14:23:15 | [train_policy] epoch #687 | computing descent direction
2022-04-23 14:23:15 | [train_policy] epoch #687 | descent direction computed
2022-04-23 14:23:15 | [train_policy] epoch #687 | backtrack iters: 1
2022-04-23 14:23:15 | [train_policy] epoch #687 | optimization finished
2022-04-23 14:23:15 | [train_policy] epoch #687 | Computing KL after
2022-04-23 14:23:15 | [train_policy] epoch #687 | Computing loss after
2022-04-23 14:23:15 | [train_policy] epoch #687 | Fitting baseline...
2022-04-23 14:23:15 | [train_policy] epoch #687 | Saving snapshot...
2022-04-23 14:23:15 | [train_policy] epoch #687 | Saved
2022-04-23 14:23:15 | [train_policy] epoch #687 | Time 243.64 s
2022-04-23 14:23:15 | [train_policy] epoch #687 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.118009
Evaluation/AverageDiscountedReturn          -41.485
Evaluation/AverageReturn                    -41.485
Evaluation/CompletionRate                     0
Evaluation/Iteration                        687
Evaluation/MaxReturn                        -31.4486
Evaluation/MinReturn                        -72.235
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.42338
Extras/EpisodeRewardMean                    -41.6042
LinearFeatureBaseline/ExplainedVariance       0.895942
PolicyExecTime                                0.0922337
ProcessExecTime                               0.0110147
TotalEnvSteps                            696256
policy/Entropy                               -1.35266
policy/KL                                     0.00668314
policy/KLBefore                               0
policy/LossAfter                             -0.0148269
policy/LossBefore                            -3.7459e-08
policy/Perplexity                             0.258551
policy/dLoss                                  0.0148268
---------------------------------------  ---------------
2022-04-23 14:23:15 | [train_policy] epoch #688 | Obtaining samples for iteration 688...
2022-04-23 14:23:16 | [train_policy] epoch #688 | Logging diagnostics...
2022-04-23 14:23:16 | [train_policy] epoch #688 | Optimizing policy...
2022-04-23 14:23:16 | [train_policy] epoch #688 | Computing loss before
2022-04-23 14:23:16 | [train_policy] epoch #688 | Computing KL before
2022-04-23 14:23:16 | [train_policy] epoch #688 | Optimizing
2022-04-23 14:23:16 | [train_policy] epoch #688 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:16 | [train_policy] epoch #688 | computing loss before
2022-04-23 14:23:16 | [train_policy] epoch #688 | computing gradient
2022-04-23 14:23:16 | [train_policy] epoch #688 | gradient computed
2022-04-23 14:23:16 | [train_policy] epoch #688 | computing descent direction
2022-04-23 14:23:16 | [train_policy] epoch #688 | descent direction computed
2022-04-23 14:23:16 | [train_policy] epoch #688 | backtrack iters: 1
2022-04-23 14:23:16 | [train_policy] epoch #688 | optimization finished
2022-04-23 14:23:16 | [train_policy] epoch #688 | Computing KL after
2022-04-23 14:23:16 | [train_policy] epoch #688 | Computing loss after
2022-04-23 14:23:16 | [train_policy] epoch #688 | Fitting baseline...
2022-04-23 14:23:16 | [train_policy] epoch #688 | Saving snapshot...
2022-04-23 14:23:16 | [train_policy] epoch #688 | Saved
2022-04-23 14:23:16 | [train_policy] epoch #688 | Time 243.96 s
2022-04-23 14:23:16 | [train_policy] epoch #688 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119063
Evaluation/AverageDiscountedReturn          -42.3148
Evaluation/AverageReturn                    -42.3148
Evaluation/CompletionRate                     0
Evaluation/Iteration                        688
Evaluation/MaxReturn                        -30.196
Evaluation/MinReturn                        -72.632
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.82714
Extras/EpisodeRewardMean                    -42.0269
LinearFeatureBaseline/ExplainedVariance       0.860175
PolicyExecTime                                0.0961862
ProcessExecTime                               0.0111473
TotalEnvSteps                            697268
policy/Entropy                               -1.4226
policy/KL                                     0.0067541
policy/KLBefore                               0
policy/LossAfter                             -0.0197331
policy/LossBefore                             1.74338e-08
policy/Perplexity                             0.241086
policy/dLoss                                  0.0197331
---------------------------------------  ----------------
2022-04-23 14:23:16 | [train_policy] epoch #689 | Obtaining samples for iteration 689...
2022-04-23 14:23:16 | [train_policy] epoch #689 | Logging diagnostics...
2022-04-23 14:23:16 | [train_policy] epoch #689 | Optimizing policy...
2022-04-23 14:23:16 | [train_policy] epoch #689 | Computing loss before
2022-04-23 14:23:16 | [train_policy] epoch #689 | Computing KL before
2022-04-23 14:23:16 | [train_policy] epoch #689 | Optimizing
2022-04-23 14:23:16 | [train_policy] epoch #689 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:16 | [train_policy] epoch #689 | computing loss before
2022-04-23 14:23:16 | [train_policy] epoch #689 | computing gradient
2022-04-23 14:23:16 | [train_policy] epoch #689 | gradient computed
2022-04-23 14:23:16 | [train_policy] epoch #689 | computing descent direction
2022-04-23 14:23:16 | [train_policy] epoch #689 | descent direction computed
2022-04-23 14:23:16 | [train_policy] epoch #689 | backtrack iters: 0
2022-04-23 14:23:16 | [train_policy] epoch #689 | optimization finished
2022-04-23 14:23:16 | [train_policy] epoch #689 | Computing KL after
2022-04-23 14:23:16 | [train_policy] epoch #689 | Computing loss after
2022-04-23 14:23:16 | [train_policy] epoch #689 | Fitting baseline...
2022-04-23 14:23:16 | [train_policy] epoch #689 | Saving snapshot...
2022-04-23 14:23:16 | [train_policy] epoch #689 | Saved
2022-04-23 14:23:16 | [train_policy] epoch #689 | Time 244.28 s
2022-04-23 14:23:16 | [train_policy] epoch #689 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118114
Evaluation/AverageDiscountedReturn          -39.4634
Evaluation/AverageReturn                    -39.4634
Evaluation/CompletionRate                     0
Evaluation/Iteration                        689
Evaluation/MaxReturn                        -29.2118
Evaluation/MinReturn                        -57.4402
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.21745
Extras/EpisodeRewardMean                    -39.4782
LinearFeatureBaseline/ExplainedVariance       0.910562
PolicyExecTime                                0.0926919
ProcessExecTime                               0.0110686
TotalEnvSteps                            698280
policy/Entropy                               -1.42549
policy/KL                                     0.00935027
policy/KLBefore                               0
policy/LossAfter                             -0.0216898
policy/LossBefore                            -1.69626e-08
policy/Perplexity                             0.240391
policy/dLoss                                  0.0216898
---------------------------------------  ----------------
2022-04-23 14:23:16 | [train_policy] epoch #690 | Obtaining samples for iteration 690...
2022-04-23 14:23:16 | [train_policy] epoch #690 | Logging diagnostics...
2022-04-23 14:23:16 | [train_policy] epoch #690 | Optimizing policy...
2022-04-23 14:23:16 | [train_policy] epoch #690 | Computing loss before
2022-04-23 14:23:16 | [train_policy] epoch #690 | Computing KL before
2022-04-23 14:23:16 | [train_policy] epoch #690 | Optimizing
2022-04-23 14:23:16 | [train_policy] epoch #690 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:16 | [train_policy] epoch #690 | computing loss before
2022-04-23 14:23:16 | [train_policy] epoch #690 | computing gradient
2022-04-23 14:23:16 | [train_policy] epoch #690 | gradient computed
2022-04-23 14:23:16 | [train_policy] epoch #690 | computing descent direction
2022-04-23 14:23:16 | [train_policy] epoch #690 | descent direction computed
2022-04-23 14:23:16 | [train_policy] epoch #690 | backtrack iters: 0
2022-04-23 14:23:16 | [train_policy] epoch #690 | optimization finished
2022-04-23 14:23:16 | [train_policy] epoch #690 | Computing KL after
2022-04-23 14:23:16 | [train_policy] epoch #690 | Computing loss after
2022-04-23 14:23:16 | [train_policy] epoch #690 | Fitting baseline...
2022-04-23 14:23:16 | [train_policy] epoch #690 | Saving snapshot...
2022-04-23 14:23:16 | [train_policy] epoch #690 | Saved
2022-04-23 14:23:16 | [train_policy] epoch #690 | Time 244.61 s
2022-04-23 14:23:16 | [train_policy] epoch #690 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118426
Evaluation/AverageDiscountedReturn          -41.8361
Evaluation/AverageReturn                    -41.8361
Evaluation/CompletionRate                     0
Evaluation/Iteration                        690
Evaluation/MaxReturn                        -29.7872
Evaluation/MinReturn                        -63.9543
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.36347
Extras/EpisodeRewardMean                    -41.7684
LinearFeatureBaseline/ExplainedVariance       0.879784
PolicyExecTime                                0.0928864
ProcessExecTime                               0.0112917
TotalEnvSteps                            699292
policy/Entropy                               -1.40004
policy/KL                                     0.00992767
policy/KLBefore                               0
policy/LossAfter                             -0.0176654
policy/LossBefore                             1.41355e-08
policy/Perplexity                             0.246587
policy/dLoss                                  0.0176654
---------------------------------------  ----------------
2022-04-23 14:23:16 | [train_policy] epoch #691 | Obtaining samples for iteration 691...
2022-04-23 14:23:17 | [train_policy] epoch #691 | Logging diagnostics...
2022-04-23 14:23:17 | [train_policy] epoch #691 | Optimizing policy...
2022-04-23 14:23:17 | [train_policy] epoch #691 | Computing loss before
2022-04-23 14:23:17 | [train_policy] epoch #691 | Computing KL before
2022-04-23 14:23:17 | [train_policy] epoch #691 | Optimizing
2022-04-23 14:23:17 | [train_policy] epoch #691 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:17 | [train_policy] epoch #691 | computing loss before
2022-04-23 14:23:17 | [train_policy] epoch #691 | computing gradient
2022-04-23 14:23:17 | [train_policy] epoch #691 | gradient computed
2022-04-23 14:23:17 | [train_policy] epoch #691 | computing descent direction
2022-04-23 14:23:17 | [train_policy] epoch #691 | descent direction computed
2022-04-23 14:23:17 | [train_policy] epoch #691 | backtrack iters: 1
2022-04-23 14:23:17 | [train_policy] epoch #691 | optimization finished
2022-04-23 14:23:17 | [train_policy] epoch #691 | Computing KL after
2022-04-23 14:23:17 | [train_policy] epoch #691 | Computing loss after
2022-04-23 14:23:17 | [train_policy] epoch #691 | Fitting baseline...
2022-04-23 14:23:17 | [train_policy] epoch #691 | Saving snapshot...
2022-04-23 14:23:17 | [train_policy] epoch #691 | Saved
2022-04-23 14:23:17 | [train_policy] epoch #691 | Time 244.93 s
2022-04-23 14:23:17 | [train_policy] epoch #691 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118099
Evaluation/AverageDiscountedReturn          -40.7288
Evaluation/AverageReturn                    -40.7288
Evaluation/CompletionRate                     0
Evaluation/Iteration                        691
Evaluation/MaxReturn                        -29.366
Evaluation/MinReturn                        -60.8043
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85971
Extras/EpisodeRewardMean                    -40.9498
LinearFeatureBaseline/ExplainedVariance       0.893188
PolicyExecTime                                0.0921259
ProcessExecTime                               0.0112209
TotalEnvSteps                            700304
policy/Entropy                               -1.41747
policy/KL                                     0.00651503
policy/KLBefore                               0
policy/LossAfter                             -0.0102004
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.242326
policy/dLoss                                  0.0102004
---------------------------------------  ----------------
2022-04-23 14:23:17 | [train_policy] epoch #692 | Obtaining samples for iteration 692...
2022-04-23 14:23:17 | [train_policy] epoch #692 | Logging diagnostics...
2022-04-23 14:23:17 | [train_policy] epoch #692 | Optimizing policy...
2022-04-23 14:23:17 | [train_policy] epoch #692 | Computing loss before
2022-04-23 14:23:17 | [train_policy] epoch #692 | Computing KL before
2022-04-23 14:23:17 | [train_policy] epoch #692 | Optimizing
2022-04-23 14:23:17 | [train_policy] epoch #692 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:17 | [train_policy] epoch #692 | computing loss before
2022-04-23 14:23:17 | [train_policy] epoch #692 | computing gradient
2022-04-23 14:23:17 | [train_policy] epoch #692 | gradient computed
2022-04-23 14:23:17 | [train_policy] epoch #692 | computing descent direction
2022-04-23 14:23:17 | [train_policy] epoch #692 | descent direction computed
2022-04-23 14:23:17 | [train_policy] epoch #692 | backtrack iters: 0
2022-04-23 14:23:17 | [train_policy] epoch #692 | optimization finished
2022-04-23 14:23:17 | [train_policy] epoch #692 | Computing KL after
2022-04-23 14:23:17 | [train_policy] epoch #692 | Computing loss after
2022-04-23 14:23:17 | [train_policy] epoch #692 | Fitting baseline...
2022-04-23 14:23:17 | [train_policy] epoch #692 | Saving snapshot...
2022-04-23 14:23:17 | [train_policy] epoch #692 | Saved
2022-04-23 14:23:17 | [train_policy] epoch #692 | Time 245.25 s
2022-04-23 14:23:17 | [train_policy] epoch #692 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.118576
Evaluation/AverageDiscountedReturn          -42.9074
Evaluation/AverageReturn                    -42.9074
Evaluation/CompletionRate                     0
Evaluation/Iteration                        692
Evaluation/MaxReturn                        -29.382
Evaluation/MinReturn                        -64.1853
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.60232
Extras/EpisodeRewardMean                    -42.4739
LinearFeatureBaseline/ExplainedVariance       0.901786
PolicyExecTime                                0.092103
ProcessExecTime                               0.0110273
TotalEnvSteps                            701316
policy/Entropy                               -1.43396
policy/KL                                     0.00975049
policy/KLBefore                               0
policy/LossAfter                             -0.0140874
policy/LossBefore                             1.70804e-08
policy/Perplexity                             0.238363
policy/dLoss                                  0.0140874
---------------------------------------  ----------------
2022-04-23 14:23:17 | [train_policy] epoch #693 | Obtaining samples for iteration 693...
2022-04-23 14:23:17 | [train_policy] epoch #693 | Logging diagnostics...
2022-04-23 14:23:17 | [train_policy] epoch #693 | Optimizing policy...
2022-04-23 14:23:17 | [train_policy] epoch #693 | Computing loss before
2022-04-23 14:23:17 | [train_policy] epoch #693 | Computing KL before
2022-04-23 14:23:17 | [train_policy] epoch #693 | Optimizing
2022-04-23 14:23:17 | [train_policy] epoch #693 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:17 | [train_policy] epoch #693 | computing loss before
2022-04-23 14:23:17 | [train_policy] epoch #693 | computing gradient
2022-04-23 14:23:17 | [train_policy] epoch #693 | gradient computed
2022-04-23 14:23:17 | [train_policy] epoch #693 | computing descent direction
2022-04-23 14:23:17 | [train_policy] epoch #693 | descent direction computed
2022-04-23 14:23:17 | [train_policy] epoch #693 | backtrack iters: 0
2022-04-23 14:23:17 | [train_policy] epoch #693 | optimization finished
2022-04-23 14:23:17 | [train_policy] epoch #693 | Computing KL after
2022-04-23 14:23:17 | [train_policy] epoch #693 | Computing loss after
2022-04-23 14:23:17 | [train_policy] epoch #693 | Fitting baseline...
2022-04-23 14:23:17 | [train_policy] epoch #693 | Saving snapshot...
2022-04-23 14:23:17 | [train_policy] epoch #693 | Saved
2022-04-23 14:23:17 | [train_policy] epoch #693 | Time 245.58 s
2022-04-23 14:23:17 | [train_policy] epoch #693 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.1198
Evaluation/AverageDiscountedReturn          -39.9488
Evaluation/AverageReturn                    -39.9488
Evaluation/CompletionRate                     0
Evaluation/Iteration                        693
Evaluation/MaxReturn                        -29.6463
Evaluation/MinReturn                        -73.5746
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.57744
Extras/EpisodeRewardMean                    -40.1627
LinearFeatureBaseline/ExplainedVariance       0.867754
PolicyExecTime                                0.0980825
ProcessExecTime                               0.0117977
TotalEnvSteps                            702328
policy/Entropy                               -1.42281
policy/KL                                     0.00973848
policy/KLBefore                               0
policy/LossAfter                             -0.0173008
policy/LossBefore                            -1.48423e-08
policy/Perplexity                             0.241035
policy/dLoss                                  0.0173008
---------------------------------------  ----------------
2022-04-23 14:23:17 | [train_policy] epoch #694 | Obtaining samples for iteration 694...
2022-04-23 14:23:18 | [train_policy] epoch #694 | Logging diagnostics...
2022-04-23 14:23:18 | [train_policy] epoch #694 | Optimizing policy...
2022-04-23 14:23:18 | [train_policy] epoch #694 | Computing loss before
2022-04-23 14:23:18 | [train_policy] epoch #694 | Computing KL before
2022-04-23 14:23:18 | [train_policy] epoch #694 | Optimizing
2022-04-23 14:23:18 | [train_policy] epoch #694 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:18 | [train_policy] epoch #694 | computing loss before
2022-04-23 14:23:18 | [train_policy] epoch #694 | computing gradient
2022-04-23 14:23:18 | [train_policy] epoch #694 | gradient computed
2022-04-23 14:23:18 | [train_policy] epoch #694 | computing descent direction
2022-04-23 14:23:18 | [train_policy] epoch #694 | descent direction computed
2022-04-23 14:23:18 | [train_policy] epoch #694 | backtrack iters: 1
2022-04-23 14:23:18 | [train_policy] epoch #694 | optimization finished
2022-04-23 14:23:18 | [train_policy] epoch #694 | Computing KL after
2022-04-23 14:23:18 | [train_policy] epoch #694 | Computing loss after
2022-04-23 14:23:18 | [train_policy] epoch #694 | Fitting baseline...
2022-04-23 14:23:18 | [train_policy] epoch #694 | Saving snapshot...
2022-04-23 14:23:18 | [train_policy] epoch #694 | Saved
2022-04-23 14:23:18 | [train_policy] epoch #694 | Time 245.90 s
2022-04-23 14:23:18 | [train_policy] epoch #694 | EpochTime 0.31 s
---------------------------------------  ---------------
EnvExecTime                                   0.117843
Evaluation/AverageDiscountedReturn          -43.259
Evaluation/AverageReturn                    -43.259
Evaluation/CompletionRate                     0
Evaluation/Iteration                        694
Evaluation/MaxReturn                        -30.0958
Evaluation/MinReturn                        -72.1536
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1216
Extras/EpisodeRewardMean                    -43.0117
LinearFeatureBaseline/ExplainedVariance       0.868085
PolicyExecTime                                0.0911424
ProcessExecTime                               0.0111287
TotalEnvSteps                            703340
policy/Entropy                               -1.47437
policy/KL                                     0.0066916
policy/KLBefore                               0
policy/LossAfter                             -0.0139301
policy/LossBefore                             8.3635e-09
policy/Perplexity                             0.228923
policy/dLoss                                  0.0139301
---------------------------------------  ---------------
2022-04-23 14:23:18 | [train_policy] epoch #695 | Obtaining samples for iteration 695...
2022-04-23 14:23:18 | [train_policy] epoch #695 | Logging diagnostics...
2022-04-23 14:23:18 | [train_policy] epoch #695 | Optimizing policy...
2022-04-23 14:23:18 | [train_policy] epoch #695 | Computing loss before
2022-04-23 14:23:18 | [train_policy] epoch #695 | Computing KL before
2022-04-23 14:23:18 | [train_policy] epoch #695 | Optimizing
2022-04-23 14:23:18 | [train_policy] epoch #695 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:18 | [train_policy] epoch #695 | computing loss before
2022-04-23 14:23:18 | [train_policy] epoch #695 | computing gradient
2022-04-23 14:23:18 | [train_policy] epoch #695 | gradient computed
2022-04-23 14:23:18 | [train_policy] epoch #695 | computing descent direction
2022-04-23 14:23:18 | [train_policy] epoch #695 | descent direction computed
2022-04-23 14:23:18 | [train_policy] epoch #695 | backtrack iters: 1
2022-04-23 14:23:18 | [train_policy] epoch #695 | optimization finished
2022-04-23 14:23:18 | [train_policy] epoch #695 | Computing KL after
2022-04-23 14:23:18 | [train_policy] epoch #695 | Computing loss after
2022-04-23 14:23:18 | [train_policy] epoch #695 | Fitting baseline...
2022-04-23 14:23:18 | [train_policy] epoch #695 | Saving snapshot...
2022-04-23 14:23:18 | [train_policy] epoch #695 | Saved
2022-04-23 14:23:18 | [train_policy] epoch #695 | Time 246.22 s
2022-04-23 14:23:18 | [train_policy] epoch #695 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.117463
Evaluation/AverageDiscountedReturn          -41.3827
Evaluation/AverageReturn                    -41.3827
Evaluation/CompletionRate                     0
Evaluation/Iteration                        695
Evaluation/MaxReturn                        -32.1099
Evaluation/MinReturn                        -63.7871
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.47164
Extras/EpisodeRewardMean                    -41.6061
LinearFeatureBaseline/ExplainedVariance       0.892642
PolicyExecTime                                0.0920365
ProcessExecTime                               0.0112336
TotalEnvSteps                            704352
policy/Entropy                               -1.52766
policy/KL                                     0.00664486
policy/KLBefore                               0
policy/LossAfter                             -0.0169867
policy/LossBefore                            -3.91082e-08
policy/Perplexity                             0.217043
policy/dLoss                                  0.0169867
---------------------------------------  ----------------
2022-04-23 14:23:18 | [train_policy] epoch #696 | Obtaining samples for iteration 696...
2022-04-23 14:23:18 | [train_policy] epoch #696 | Logging diagnostics...
2022-04-23 14:23:18 | [train_policy] epoch #696 | Optimizing policy...
2022-04-23 14:23:18 | [train_policy] epoch #696 | Computing loss before
2022-04-23 14:23:18 | [train_policy] epoch #696 | Computing KL before
2022-04-23 14:23:18 | [train_policy] epoch #696 | Optimizing
2022-04-23 14:23:18 | [train_policy] epoch #696 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:18 | [train_policy] epoch #696 | computing loss before
2022-04-23 14:23:18 | [train_policy] epoch #696 | computing gradient
2022-04-23 14:23:18 | [train_policy] epoch #696 | gradient computed
2022-04-23 14:23:18 | [train_policy] epoch #696 | computing descent direction
2022-04-23 14:23:18 | [train_policy] epoch #696 | descent direction computed
2022-04-23 14:23:18 | [train_policy] epoch #696 | backtrack iters: 0
2022-04-23 14:23:18 | [train_policy] epoch #696 | optimization finished
2022-04-23 14:23:18 | [train_policy] epoch #696 | Computing KL after
2022-04-23 14:23:18 | [train_policy] epoch #696 | Computing loss after
2022-04-23 14:23:18 | [train_policy] epoch #696 | Fitting baseline...
2022-04-23 14:23:18 | [train_policy] epoch #696 | Saving snapshot...
2022-04-23 14:23:18 | [train_policy] epoch #696 | Saved
2022-04-23 14:23:18 | [train_policy] epoch #696 | Time 246.54 s
2022-04-23 14:23:18 | [train_policy] epoch #696 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118215
Evaluation/AverageDiscountedReturn          -41.3049
Evaluation/AverageReturn                    -41.3049
Evaluation/CompletionRate                     0
Evaluation/Iteration                        696
Evaluation/MaxReturn                        -30.1642
Evaluation/MinReturn                        -63.8255
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.47503
Extras/EpisodeRewardMean                    -41.0667
LinearFeatureBaseline/ExplainedVariance       0.906847
PolicyExecTime                                0.0947242
ProcessExecTime                               0.0112686
TotalEnvSteps                            705364
policy/Entropy                               -1.44597
policy/KL                                     0.00940098
policy/KLBefore                               0
policy/LossAfter                             -0.0195468
policy/LossBefore                             1.41355e-08
policy/Perplexity                             0.235518
policy/dLoss                                  0.0195469
---------------------------------------  ----------------
2022-04-23 14:23:18 | [train_policy] epoch #697 | Obtaining samples for iteration 697...
2022-04-23 14:23:19 | [train_policy] epoch #697 | Logging diagnostics...
2022-04-23 14:23:19 | [train_policy] epoch #697 | Optimizing policy...
2022-04-23 14:23:19 | [train_policy] epoch #697 | Computing loss before
2022-04-23 14:23:19 | [train_policy] epoch #697 | Computing KL before
2022-04-23 14:23:19 | [train_policy] epoch #697 | Optimizing
2022-04-23 14:23:19 | [train_policy] epoch #697 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:19 | [train_policy] epoch #697 | computing loss before
2022-04-23 14:23:19 | [train_policy] epoch #697 | computing gradient
2022-04-23 14:23:19 | [train_policy] epoch #697 | gradient computed
2022-04-23 14:23:19 | [train_policy] epoch #697 | computing descent direction
2022-04-23 14:23:19 | [train_policy] epoch #697 | descent direction computed
2022-04-23 14:23:19 | [train_policy] epoch #697 | backtrack iters: 0
2022-04-23 14:23:19 | [train_policy] epoch #697 | optimization finished
2022-04-23 14:23:19 | [train_policy] epoch #697 | Computing KL after
2022-04-23 14:23:19 | [train_policy] epoch #697 | Computing loss after
2022-04-23 14:23:19 | [train_policy] epoch #697 | Fitting baseline...
2022-04-23 14:23:19 | [train_policy] epoch #697 | Saving snapshot...
2022-04-23 14:23:19 | [train_policy] epoch #697 | Saved
2022-04-23 14:23:19 | [train_policy] epoch #697 | Time 246.86 s
2022-04-23 14:23:19 | [train_policy] epoch #697 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.117881
Evaluation/AverageDiscountedReturn          -41.5237
Evaluation/AverageReturn                    -41.5237
Evaluation/CompletionRate                     0
Evaluation/Iteration                        697
Evaluation/MaxReturn                        -29.9647
Evaluation/MinReturn                        -64.6819
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.573
Extras/EpisodeRewardMean                    -41.2991
LinearFeatureBaseline/ExplainedVariance       0.825406
PolicyExecTime                                0.0910642
ProcessExecTime                               0.0112498
TotalEnvSteps                            706376
policy/Entropy                               -1.46993
policy/KL                                     0.00971322
policy/KLBefore                               0
policy/LossAfter                             -0.0234239
policy/LossBefore                            -9.07027e-09
policy/Perplexity                             0.229942
policy/dLoss                                  0.0234239
---------------------------------------  ----------------
2022-04-23 14:23:19 | [train_policy] epoch #698 | Obtaining samples for iteration 698...
2022-04-23 14:23:19 | [train_policy] epoch #698 | Logging diagnostics...
2022-04-23 14:23:19 | [train_policy] epoch #698 | Optimizing policy...
2022-04-23 14:23:19 | [train_policy] epoch #698 | Computing loss before
2022-04-23 14:23:19 | [train_policy] epoch #698 | Computing KL before
2022-04-23 14:23:19 | [train_policy] epoch #698 | Optimizing
2022-04-23 14:23:19 | [train_policy] epoch #698 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:19 | [train_policy] epoch #698 | computing loss before
2022-04-23 14:23:19 | [train_policy] epoch #698 | computing gradient
2022-04-23 14:23:19 | [train_policy] epoch #698 | gradient computed
2022-04-23 14:23:19 | [train_policy] epoch #698 | computing descent direction
2022-04-23 14:23:19 | [train_policy] epoch #698 | descent direction computed
2022-04-23 14:23:19 | [train_policy] epoch #698 | backtrack iters: 1
2022-04-23 14:23:19 | [train_policy] epoch #698 | optimization finished
2022-04-23 14:23:19 | [train_policy] epoch #698 | Computing KL after
2022-04-23 14:23:19 | [train_policy] epoch #698 | Computing loss after
2022-04-23 14:23:19 | [train_policy] epoch #698 | Fitting baseline...
2022-04-23 14:23:19 | [train_policy] epoch #698 | Saving snapshot...
2022-04-23 14:23:19 | [train_policy] epoch #698 | Saved
2022-04-23 14:23:19 | [train_policy] epoch #698 | Time 247.19 s
2022-04-23 14:23:19 | [train_policy] epoch #698 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.11822
Evaluation/AverageDiscountedReturn          -41.4692
Evaluation/AverageReturn                    -41.4692
Evaluation/CompletionRate                     0
Evaluation/Iteration                        698
Evaluation/MaxReturn                        -30.0974
Evaluation/MinReturn                        -64.8069
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.17542
Extras/EpisodeRewardMean                    -41.5255
LinearFeatureBaseline/ExplainedVariance       0.872536
PolicyExecTime                                0.0985024
ProcessExecTime                               0.0112667
TotalEnvSteps                            707388
policy/Entropy                               -1.46226
policy/KL                                     0.00650485
policy/KLBefore                               0
policy/LossAfter                             -0.00791469
policy/LossBefore                            -1.56668e-08
policy/Perplexity                             0.231713
policy/dLoss                                  0.00791467
---------------------------------------  ----------------
2022-04-23 14:23:19 | [train_policy] epoch #699 | Obtaining samples for iteration 699...
2022-04-23 14:23:19 | [train_policy] epoch #699 | Logging diagnostics...
2022-04-23 14:23:19 | [train_policy] epoch #699 | Optimizing policy...
2022-04-23 14:23:19 | [train_policy] epoch #699 | Computing loss before
2022-04-23 14:23:19 | [train_policy] epoch #699 | Computing KL before
2022-04-23 14:23:19 | [train_policy] epoch #699 | Optimizing
2022-04-23 14:23:19 | [train_policy] epoch #699 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:19 | [train_policy] epoch #699 | computing loss before
2022-04-23 14:23:19 | [train_policy] epoch #699 | computing gradient
2022-04-23 14:23:19 | [train_policy] epoch #699 | gradient computed
2022-04-23 14:23:19 | [train_policy] epoch #699 | computing descent direction
2022-04-23 14:23:19 | [train_policy] epoch #699 | descent direction computed
2022-04-23 14:23:19 | [train_policy] epoch #699 | backtrack iters: 1
2022-04-23 14:23:19 | [train_policy] epoch #699 | optimization finished
2022-04-23 14:23:19 | [train_policy] epoch #699 | Computing KL after
2022-04-23 14:23:19 | [train_policy] epoch #699 | Computing loss after
2022-04-23 14:23:19 | [train_policy] epoch #699 | Fitting baseline...
2022-04-23 14:23:19 | [train_policy] epoch #699 | Saving snapshot...
2022-04-23 14:23:19 | [train_policy] epoch #699 | Saved
2022-04-23 14:23:19 | [train_policy] epoch #699 | Time 247.51 s
2022-04-23 14:23:19 | [train_policy] epoch #699 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.117618
Evaluation/AverageDiscountedReturn          -41.3944
Evaluation/AverageReturn                    -41.3944
Evaluation/CompletionRate                     0
Evaluation/Iteration                        699
Evaluation/MaxReturn                        -30.7417
Evaluation/MinReturn                        -72.3409
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.46274
Extras/EpisodeRewardMean                    -41.1839
LinearFeatureBaseline/ExplainedVariance       0.87476
PolicyExecTime                                0.0924799
ProcessExecTime                               0.0111489
TotalEnvSteps                            708400
policy/Entropy                               -1.47122
policy/KL                                     0.00642224
policy/KLBefore                               0
policy/LossAfter                             -0.0151792
policy/LossBefore                            -1.14262e-08
policy/Perplexity                             0.229646
policy/dLoss                                  0.0151792
---------------------------------------  ----------------
2022-04-23 14:23:19 | [train_policy] epoch #700 | Obtaining samples for iteration 700...
2022-04-23 14:23:20 | [train_policy] epoch #700 | Logging diagnostics...
2022-04-23 14:23:20 | [train_policy] epoch #700 | Optimizing policy...
2022-04-23 14:23:20 | [train_policy] epoch #700 | Computing loss before
2022-04-23 14:23:20 | [train_policy] epoch #700 | Computing KL before
2022-04-23 14:23:20 | [train_policy] epoch #700 | Optimizing
2022-04-23 14:23:20 | [train_policy] epoch #700 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:20 | [train_policy] epoch #700 | computing loss before
2022-04-23 14:23:20 | [train_policy] epoch #700 | computing gradient
2022-04-23 14:23:20 | [train_policy] epoch #700 | gradient computed
2022-04-23 14:23:20 | [train_policy] epoch #700 | computing descent direction
2022-04-23 14:23:20 | [train_policy] epoch #700 | descent direction computed
2022-04-23 14:23:20 | [train_policy] epoch #700 | backtrack iters: 1
2022-04-23 14:23:20 | [train_policy] epoch #700 | optimization finished
2022-04-23 14:23:20 | [train_policy] epoch #700 | Computing KL after
2022-04-23 14:23:20 | [train_policy] epoch #700 | Computing loss after
2022-04-23 14:23:20 | [train_policy] epoch #700 | Fitting baseline...
2022-04-23 14:23:20 | [train_policy] epoch #700 | Saving snapshot...
2022-04-23 14:23:20 | [train_policy] epoch #700 | Saved
2022-04-23 14:23:20 | [train_policy] epoch #700 | Time 247.83 s
2022-04-23 14:23:20 | [train_policy] epoch #700 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.118528
Evaluation/AverageDiscountedReturn          -40.1328
Evaluation/AverageReturn                    -40.1328
Evaluation/CompletionRate                     0
Evaluation/Iteration                        700
Evaluation/MaxReturn                        -31.5004
Evaluation/MinReturn                        -72.4967
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.38601
Extras/EpisodeRewardMean                    -40.1168
LinearFeatureBaseline/ExplainedVariance       0.883138
PolicyExecTime                                0.0914364
ProcessExecTime                               0.0109942
TotalEnvSteps                            709412
policy/Entropy                               -1.51238
policy/KL                                     0.00678938
policy/KLBefore                               0
policy/LossAfter                             -0.0157813
policy/LossBefore                             1.5549e-08
policy/Perplexity                             0.220385
policy/dLoss                                  0.0157813
---------------------------------------  ---------------
2022-04-23 14:23:20 | [train_policy] epoch #701 | Obtaining samples for iteration 701...
2022-04-23 14:23:20 | [train_policy] epoch #701 | Logging diagnostics...
2022-04-23 14:23:20 | [train_policy] epoch #701 | Optimizing policy...
2022-04-23 14:23:20 | [train_policy] epoch #701 | Computing loss before
2022-04-23 14:23:20 | [train_policy] epoch #701 | Computing KL before
2022-04-23 14:23:20 | [train_policy] epoch #701 | Optimizing
2022-04-23 14:23:20 | [train_policy] epoch #701 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:20 | [train_policy] epoch #701 | computing loss before
2022-04-23 14:23:20 | [train_policy] epoch #701 | computing gradient
2022-04-23 14:23:20 | [train_policy] epoch #701 | gradient computed
2022-04-23 14:23:20 | [train_policy] epoch #701 | computing descent direction
2022-04-23 14:23:20 | [train_policy] epoch #701 | descent direction computed
2022-04-23 14:23:20 | [train_policy] epoch #701 | backtrack iters: 0
2022-04-23 14:23:20 | [train_policy] epoch #701 | optimization finished
2022-04-23 14:23:20 | [train_policy] epoch #701 | Computing KL after
2022-04-23 14:23:20 | [train_policy] epoch #701 | Computing loss after
2022-04-23 14:23:20 | [train_policy] epoch #701 | Fitting baseline...
2022-04-23 14:23:20 | [train_policy] epoch #701 | Saving snapshot...
2022-04-23 14:23:20 | [train_policy] epoch #701 | Saved
2022-04-23 14:23:20 | [train_policy] epoch #701 | Time 248.15 s
2022-04-23 14:23:20 | [train_policy] epoch #701 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.117557
Evaluation/AverageDiscountedReturn          -41.0257
Evaluation/AverageReturn                    -41.0257
Evaluation/CompletionRate                     0
Evaluation/Iteration                        701
Evaluation/MaxReturn                        -30.67
Evaluation/MinReturn                        -72.0629
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.34187
Extras/EpisodeRewardMean                    -41.2415
LinearFeatureBaseline/ExplainedVariance       0.841492
PolicyExecTime                                0.0913243
ProcessExecTime                               0.0110004
TotalEnvSteps                            710424
policy/Entropy                               -1.51257
policy/KL                                     0.00989033
policy/KLBefore                               0
policy/LossAfter                             -0.0624162
policy/LossBefore                            -0
policy/Perplexity                             0.220342
policy/dLoss                                  0.0624162
---------------------------------------  ---------------
2022-04-23 14:23:20 | [train_policy] epoch #702 | Obtaining samples for iteration 702...
2022-04-23 14:23:20 | [train_policy] epoch #702 | Logging diagnostics...
2022-04-23 14:23:20 | [train_policy] epoch #702 | Optimizing policy...
2022-04-23 14:23:20 | [train_policy] epoch #702 | Computing loss before
2022-04-23 14:23:20 | [train_policy] epoch #702 | Computing KL before
2022-04-23 14:23:20 | [train_policy] epoch #702 | Optimizing
2022-04-23 14:23:20 | [train_policy] epoch #702 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:20 | [train_policy] epoch #702 | computing loss before
2022-04-23 14:23:20 | [train_policy] epoch #702 | computing gradient
2022-04-23 14:23:20 | [train_policy] epoch #702 | gradient computed
2022-04-23 14:23:20 | [train_policy] epoch #702 | computing descent direction
2022-04-23 14:23:20 | [train_policy] epoch #702 | descent direction computed
2022-04-23 14:23:20 | [train_policy] epoch #702 | backtrack iters: 0
2022-04-23 14:23:20 | [train_policy] epoch #702 | optimization finished
2022-04-23 14:23:20 | [train_policy] epoch #702 | Computing KL after
2022-04-23 14:23:20 | [train_policy] epoch #702 | Computing loss after
2022-04-23 14:23:20 | [train_policy] epoch #702 | Fitting baseline...
2022-04-23 14:23:20 | [train_policy] epoch #702 | Saving snapshot...
2022-04-23 14:23:20 | [train_policy] epoch #702 | Saved
2022-04-23 14:23:20 | [train_policy] epoch #702 | Time 248.48 s
2022-04-23 14:23:20 | [train_policy] epoch #702 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120264
Evaluation/AverageDiscountedReturn          -41.5231
Evaluation/AverageReturn                    -41.5231
Evaluation/CompletionRate                     0
Evaluation/Iteration                        702
Evaluation/MaxReturn                        -29.6207
Evaluation/MinReturn                        -73.1251
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.08222
Extras/EpisodeRewardMean                    -41.5043
LinearFeatureBaseline/ExplainedVariance       0.884469
PolicyExecTime                                0.0942974
ProcessExecTime                               0.0116744
TotalEnvSteps                            711436
policy/Entropy                               -1.49117
policy/KL                                     0.00973266
policy/KLBefore                               0
policy/LossAfter                             -0.0129497
policy/LossBefore                             6.47877e-09
policy/Perplexity                             0.22511
policy/dLoss                                  0.0129498
---------------------------------------  ----------------
2022-04-23 14:23:20 | [train_policy] epoch #703 | Obtaining samples for iteration 703...
2022-04-23 14:23:21 | [train_policy] epoch #703 | Logging diagnostics...
2022-04-23 14:23:21 | [train_policy] epoch #703 | Optimizing policy...
2022-04-23 14:23:21 | [train_policy] epoch #703 | Computing loss before
2022-04-23 14:23:21 | [train_policy] epoch #703 | Computing KL before
2022-04-23 14:23:21 | [train_policy] epoch #703 | Optimizing
2022-04-23 14:23:21 | [train_policy] epoch #703 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:21 | [train_policy] epoch #703 | computing loss before
2022-04-23 14:23:21 | [train_policy] epoch #703 | computing gradient
2022-04-23 14:23:21 | [train_policy] epoch #703 | gradient computed
2022-04-23 14:23:21 | [train_policy] epoch #703 | computing descent direction
2022-04-23 14:23:21 | [train_policy] epoch #703 | descent direction computed
2022-04-23 14:23:21 | [train_policy] epoch #703 | backtrack iters: 1
2022-04-23 14:23:21 | [train_policy] epoch #703 | optimization finished
2022-04-23 14:23:21 | [train_policy] epoch #703 | Computing KL after
2022-04-23 14:23:21 | [train_policy] epoch #703 | Computing loss after
2022-04-23 14:23:21 | [train_policy] epoch #703 | Fitting baseline...
2022-04-23 14:23:21 | [train_policy] epoch #703 | Saving snapshot...
2022-04-23 14:23:21 | [train_policy] epoch #703 | Saved
2022-04-23 14:23:21 | [train_policy] epoch #703 | Time 248.81 s
2022-04-23 14:23:21 | [train_policy] epoch #703 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118012
Evaluation/AverageDiscountedReturn          -41.3792
Evaluation/AverageReturn                    -41.3792
Evaluation/CompletionRate                     0
Evaluation/Iteration                        703
Evaluation/MaxReturn                        -32.6903
Evaluation/MinReturn                        -64.5981
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.49375
Extras/EpisodeRewardMean                    -41.6353
LinearFeatureBaseline/ExplainedVariance       0.897698
PolicyExecTime                                0.0947623
ProcessExecTime                               0.011338
TotalEnvSteps                            712448
policy/Entropy                               -1.52249
policy/KL                                     0.00655506
policy/KLBefore                               0
policy/LossAfter                             -0.0169952
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.218168
policy/dLoss                                  0.0169952
---------------------------------------  ----------------
2022-04-23 14:23:21 | [train_policy] epoch #704 | Obtaining samples for iteration 704...
2022-04-23 14:23:21 | [train_policy] epoch #704 | Logging diagnostics...
2022-04-23 14:23:21 | [train_policy] epoch #704 | Optimizing policy...
2022-04-23 14:23:21 | [train_policy] epoch #704 | Computing loss before
2022-04-23 14:23:21 | [train_policy] epoch #704 | Computing KL before
2022-04-23 14:23:21 | [train_policy] epoch #704 | Optimizing
2022-04-23 14:23:21 | [train_policy] epoch #704 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:21 | [train_policy] epoch #704 | computing loss before
2022-04-23 14:23:21 | [train_policy] epoch #704 | computing gradient
2022-04-23 14:23:21 | [train_policy] epoch #704 | gradient computed
2022-04-23 14:23:21 | [train_policy] epoch #704 | computing descent direction
2022-04-23 14:23:21 | [train_policy] epoch #704 | descent direction computed
2022-04-23 14:23:21 | [train_policy] epoch #704 | backtrack iters: 1
2022-04-23 14:23:21 | [train_policy] epoch #704 | optimization finished
2022-04-23 14:23:21 | [train_policy] epoch #704 | Computing KL after
2022-04-23 14:23:21 | [train_policy] epoch #704 | Computing loss after
2022-04-23 14:23:21 | [train_policy] epoch #704 | Fitting baseline...
2022-04-23 14:23:21 | [train_policy] epoch #704 | Saving snapshot...
2022-04-23 14:23:21 | [train_policy] epoch #704 | Saved
2022-04-23 14:23:21 | [train_policy] epoch #704 | Time 249.13 s
2022-04-23 14:23:21 | [train_policy] epoch #704 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118327
Evaluation/AverageDiscountedReturn          -40.8303
Evaluation/AverageReturn                    -40.8303
Evaluation/CompletionRate                     0
Evaluation/Iteration                        704
Evaluation/MaxReturn                        -30.0733
Evaluation/MinReturn                        -64.2812
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5242
Extras/EpisodeRewardMean                    -40.9692
LinearFeatureBaseline/ExplainedVariance       0.885394
PolicyExecTime                                0.0955796
ProcessExecTime                               0.0112205
TotalEnvSteps                            713460
policy/Entropy                               -1.54734
policy/KL                                     0.00746849
policy/KLBefore                               0
policy/LossAfter                             -0.0197781
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.212813
policy/dLoss                                  0.0197781
---------------------------------------  ----------------
2022-04-23 14:23:21 | [train_policy] epoch #705 | Obtaining samples for iteration 705...
2022-04-23 14:23:21 | [train_policy] epoch #705 | Logging diagnostics...
2022-04-23 14:23:21 | [train_policy] epoch #705 | Optimizing policy...
2022-04-23 14:23:21 | [train_policy] epoch #705 | Computing loss before
2022-04-23 14:23:21 | [train_policy] epoch #705 | Computing KL before
2022-04-23 14:23:21 | [train_policy] epoch #705 | Optimizing
2022-04-23 14:23:21 | [train_policy] epoch #705 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:21 | [train_policy] epoch #705 | computing loss before
2022-04-23 14:23:21 | [train_policy] epoch #705 | computing gradient
2022-04-23 14:23:21 | [train_policy] epoch #705 | gradient computed
2022-04-23 14:23:21 | [train_policy] epoch #705 | computing descent direction
2022-04-23 14:23:21 | [train_policy] epoch #705 | descent direction computed
2022-04-23 14:23:21 | [train_policy] epoch #705 | backtrack iters: 0
2022-04-23 14:23:21 | [train_policy] epoch #705 | optimization finished
2022-04-23 14:23:21 | [train_policy] epoch #705 | Computing KL after
2022-04-23 14:23:21 | [train_policy] epoch #705 | Computing loss after
2022-04-23 14:23:21 | [train_policy] epoch #705 | Fitting baseline...
2022-04-23 14:23:21 | [train_policy] epoch #705 | Saving snapshot...
2022-04-23 14:23:21 | [train_policy] epoch #705 | Saved
2022-04-23 14:23:21 | [train_policy] epoch #705 | Time 249.45 s
2022-04-23 14:23:21 | [train_policy] epoch #705 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119368
Evaluation/AverageDiscountedReturn          -41.6483
Evaluation/AverageReturn                    -41.6483
Evaluation/CompletionRate                     0
Evaluation/Iteration                        705
Evaluation/MaxReturn                        -29.1092
Evaluation/MinReturn                        -72.3017
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.9426
Extras/EpisodeRewardMean                    -41.5524
LinearFeatureBaseline/ExplainedVariance       0.8736
PolicyExecTime                                0.0941939
ProcessExecTime                               0.0112672
TotalEnvSteps                            714472
policy/Entropy                               -1.54849
policy/KL                                     0.00973893
policy/KLBefore                               0
policy/LossAfter                             -0.0221159
policy/LossBefore                            -1.41355e-08
policy/Perplexity                             0.212569
policy/dLoss                                  0.0221159
---------------------------------------  ----------------
2022-04-23 14:23:21 | [train_policy] epoch #706 | Obtaining samples for iteration 706...
2022-04-23 14:23:21 | [train_policy] epoch #706 | Logging diagnostics...
2022-04-23 14:23:21 | [train_policy] epoch #706 | Optimizing policy...
2022-04-23 14:23:21 | [train_policy] epoch #706 | Computing loss before
2022-04-23 14:23:21 | [train_policy] epoch #706 | Computing KL before
2022-04-23 14:23:21 | [train_policy] epoch #706 | Optimizing
2022-04-23 14:23:21 | [train_policy] epoch #706 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:21 | [train_policy] epoch #706 | computing loss before
2022-04-23 14:23:21 | [train_policy] epoch #706 | computing gradient
2022-04-23 14:23:22 | [train_policy] epoch #706 | gradient computed
2022-04-23 14:23:22 | [train_policy] epoch #706 | computing descent direction
2022-04-23 14:23:22 | [train_policy] epoch #706 | descent direction computed
2022-04-23 14:23:22 | [train_policy] epoch #706 | backtrack iters: 1
2022-04-23 14:23:22 | [train_policy] epoch #706 | optimization finished
2022-04-23 14:23:22 | [train_policy] epoch #706 | Computing KL after
2022-04-23 14:23:22 | [train_policy] epoch #706 | Computing loss after
2022-04-23 14:23:22 | [train_policy] epoch #706 | Fitting baseline...
2022-04-23 14:23:22 | [train_policy] epoch #706 | Saving snapshot...
2022-04-23 14:23:22 | [train_policy] epoch #706 | Saved
2022-04-23 14:23:22 | [train_policy] epoch #706 | Time 249.78 s
2022-04-23 14:23:22 | [train_policy] epoch #706 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118223
Evaluation/AverageDiscountedReturn          -40.0536
Evaluation/AverageReturn                    -40.0536
Evaluation/CompletionRate                     0
Evaluation/Iteration                        706
Evaluation/MaxReturn                        -29.7224
Evaluation/MinReturn                        -60.8782
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.88495
Extras/EpisodeRewardMean                    -40.0407
LinearFeatureBaseline/ExplainedVariance       0.874017
PolicyExecTime                                0.0950761
ProcessExecTime                               0.0113142
TotalEnvSteps                            715484
policy/Entropy                               -1.54041
policy/KL                                     0.00641866
policy/KLBefore                               0
policy/LossAfter                             -0.0183999
policy/LossBefore                             6.12538e-09
policy/Perplexity                             0.214293
policy/dLoss                                  0.0183999
---------------------------------------  ----------------
2022-04-23 14:23:22 | [train_policy] epoch #707 | Obtaining samples for iteration 707...
2022-04-23 14:23:22 | [train_policy] epoch #707 | Logging diagnostics...
2022-04-23 14:23:22 | [train_policy] epoch #707 | Optimizing policy...
2022-04-23 14:23:22 | [train_policy] epoch #707 | Computing loss before
2022-04-23 14:23:22 | [train_policy] epoch #707 | Computing KL before
2022-04-23 14:23:22 | [train_policy] epoch #707 | Optimizing
2022-04-23 14:23:22 | [train_policy] epoch #707 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:22 | [train_policy] epoch #707 | computing loss before
2022-04-23 14:23:22 | [train_policy] epoch #707 | computing gradient
2022-04-23 14:23:22 | [train_policy] epoch #707 | gradient computed
2022-04-23 14:23:22 | [train_policy] epoch #707 | computing descent direction
2022-04-23 14:23:22 | [train_policy] epoch #707 | descent direction computed
2022-04-23 14:23:22 | [train_policy] epoch #707 | backtrack iters: 1
2022-04-23 14:23:22 | [train_policy] epoch #707 | optimization finished
2022-04-23 14:23:22 | [train_policy] epoch #707 | Computing KL after
2022-04-23 14:23:22 | [train_policy] epoch #707 | Computing loss after
2022-04-23 14:23:22 | [train_policy] epoch #707 | Fitting baseline...
2022-04-23 14:23:22 | [train_policy] epoch #707 | Saving snapshot...
2022-04-23 14:23:22 | [train_policy] epoch #707 | Saved
2022-04-23 14:23:22 | [train_policy] epoch #707 | Time 250.10 s
2022-04-23 14:23:22 | [train_policy] epoch #707 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.117738
Evaluation/AverageDiscountedReturn          -40.3667
Evaluation/AverageReturn                    -40.3667
Evaluation/CompletionRate                     0
Evaluation/Iteration                        707
Evaluation/MaxReturn                        -29.7577
Evaluation/MinReturn                        -72.2235
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.52828
Extras/EpisodeRewardMean                    -40.4151
LinearFeatureBaseline/ExplainedVariance       0.883893
PolicyExecTime                                0.0935385
ProcessExecTime                               0.0111959
TotalEnvSteps                            716496
policy/Entropy                               -1.61052
policy/KL                                     0.0066675
policy/KLBefore                               0
policy/LossAfter                             -0.0126173
policy/LossBefore                            -1.42533e-08
policy/Perplexity                             0.199784
policy/dLoss                                  0.0126173
---------------------------------------  ----------------
2022-04-23 14:23:22 | [train_policy] epoch #708 | Obtaining samples for iteration 708...
2022-04-23 14:23:22 | [train_policy] epoch #708 | Logging diagnostics...
2022-04-23 14:23:22 | [train_policy] epoch #708 | Optimizing policy...
2022-04-23 14:23:22 | [train_policy] epoch #708 | Computing loss before
2022-04-23 14:23:22 | [train_policy] epoch #708 | Computing KL before
2022-04-23 14:23:22 | [train_policy] epoch #708 | Optimizing
2022-04-23 14:23:22 | [train_policy] epoch #708 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:22 | [train_policy] epoch #708 | computing loss before
2022-04-23 14:23:22 | [train_policy] epoch #708 | computing gradient
2022-04-23 14:23:22 | [train_policy] epoch #708 | gradient computed
2022-04-23 14:23:22 | [train_policy] epoch #708 | computing descent direction
2022-04-23 14:23:22 | [train_policy] epoch #708 | descent direction computed
2022-04-23 14:23:22 | [train_policy] epoch #708 | backtrack iters: 0
2022-04-23 14:23:22 | [train_policy] epoch #708 | optimization finished
2022-04-23 14:23:22 | [train_policy] epoch #708 | Computing KL after
2022-04-23 14:23:22 | [train_policy] epoch #708 | Computing loss after
2022-04-23 14:23:22 | [train_policy] epoch #708 | Fitting baseline...
2022-04-23 14:23:22 | [train_policy] epoch #708 | Saving snapshot...
2022-04-23 14:23:22 | [train_policy] epoch #708 | Saved
2022-04-23 14:23:22 | [train_policy] epoch #708 | Time 250.42 s
2022-04-23 14:23:22 | [train_policy] epoch #708 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118128
Evaluation/AverageDiscountedReturn          -41.3344
Evaluation/AverageReturn                    -41.3344
Evaluation/CompletionRate                     0
Evaluation/Iteration                        708
Evaluation/MaxReturn                        -29.5143
Evaluation/MinReturn                        -74.4424
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.08834
Extras/EpisodeRewardMean                    -41.3524
LinearFeatureBaseline/ExplainedVariance       0.866891
PolicyExecTime                                0.0924573
ProcessExecTime                               0.0112948
TotalEnvSteps                            717508
policy/Entropy                               -1.5743
policy/KL                                     0.00945757
policy/KLBefore                               0
policy/LossAfter                             -0.0225546
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.207152
policy/dLoss                                  0.0225547
---------------------------------------  ----------------
2022-04-23 14:23:22 | [train_policy] epoch #709 | Obtaining samples for iteration 709...
2022-04-23 14:23:22 | [train_policy] epoch #709 | Logging diagnostics...
2022-04-23 14:23:22 | [train_policy] epoch #709 | Optimizing policy...
2022-04-23 14:23:22 | [train_policy] epoch #709 | Computing loss before
2022-04-23 14:23:22 | [train_policy] epoch #709 | Computing KL before
2022-04-23 14:23:22 | [train_policy] epoch #709 | Optimizing
2022-04-23 14:23:22 | [train_policy] epoch #709 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:22 | [train_policy] epoch #709 | computing loss before
2022-04-23 14:23:22 | [train_policy] epoch #709 | computing gradient
2022-04-23 14:23:22 | [train_policy] epoch #709 | gradient computed
2022-04-23 14:23:22 | [train_policy] epoch #709 | computing descent direction
2022-04-23 14:23:23 | [train_policy] epoch #709 | descent direction computed
2022-04-23 14:23:23 | [train_policy] epoch #709 | backtrack iters: 0
2022-04-23 14:23:23 | [train_policy] epoch #709 | optimization finished
2022-04-23 14:23:23 | [train_policy] epoch #709 | Computing KL after
2022-04-23 14:23:23 | [train_policy] epoch #709 | Computing loss after
2022-04-23 14:23:23 | [train_policy] epoch #709 | Fitting baseline...
2022-04-23 14:23:23 | [train_policy] epoch #709 | Saving snapshot...
2022-04-23 14:23:23 | [train_policy] epoch #709 | Saved
2022-04-23 14:23:23 | [train_policy] epoch #709 | Time 250.75 s
2022-04-23 14:23:23 | [train_policy] epoch #709 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.117818
Evaluation/AverageDiscountedReturn          -41.5256
Evaluation/AverageReturn                    -41.5256
Evaluation/CompletionRate                     0
Evaluation/Iteration                        709
Evaluation/MaxReturn                        -30.0837
Evaluation/MinReturn                        -72.7679
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.56974
Extras/EpisodeRewardMean                    -41.341
LinearFeatureBaseline/ExplainedVariance       0.841186
PolicyExecTime                                0.0924928
ProcessExecTime                               0.0111818
TotalEnvSteps                            718520
policy/Entropy                               -1.52475
policy/KL                                     0.00992178
policy/KLBefore                               0
policy/LossAfter                             -0.0144482
policy/LossBefore                             2.59151e-08
policy/Perplexity                             0.217676
policy/dLoss                                  0.0144482
---------------------------------------  ----------------
2022-04-23 14:23:23 | [train_policy] epoch #710 | Obtaining samples for iteration 710...
2022-04-23 14:23:23 | [train_policy] epoch #710 | Logging diagnostics...
2022-04-23 14:23:23 | [train_policy] epoch #710 | Optimizing policy...
2022-04-23 14:23:23 | [train_policy] epoch #710 | Computing loss before
2022-04-23 14:23:23 | [train_policy] epoch #710 | Computing KL before
2022-04-23 14:23:23 | [train_policy] epoch #710 | Optimizing
2022-04-23 14:23:23 | [train_policy] epoch #710 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:23 | [train_policy] epoch #710 | computing loss before
2022-04-23 14:23:23 | [train_policy] epoch #710 | computing gradient
2022-04-23 14:23:23 | [train_policy] epoch #710 | gradient computed
2022-04-23 14:23:23 | [train_policy] epoch #710 | computing descent direction
2022-04-23 14:23:23 | [train_policy] epoch #710 | descent direction computed
2022-04-23 14:23:23 | [train_policy] epoch #710 | backtrack iters: 1
2022-04-23 14:23:23 | [train_policy] epoch #710 | optimization finished
2022-04-23 14:23:23 | [train_policy] epoch #710 | Computing KL after
2022-04-23 14:23:23 | [train_policy] epoch #710 | Computing loss after
2022-04-23 14:23:23 | [train_policy] epoch #710 | Fitting baseline...
2022-04-23 14:23:23 | [train_policy] epoch #710 | Saving snapshot...
2022-04-23 14:23:23 | [train_policy] epoch #710 | Saved
2022-04-23 14:23:23 | [train_policy] epoch #710 | Time 251.07 s
2022-04-23 14:23:23 | [train_policy] epoch #710 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118484
Evaluation/AverageDiscountedReturn          -41.9418
Evaluation/AverageReturn                    -41.9418
Evaluation/CompletionRate                     0
Evaluation/Iteration                        710
Evaluation/MaxReturn                        -29.9486
Evaluation/MinReturn                        -71.7726
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.33843
Extras/EpisodeRewardMean                    -41.7781
LinearFeatureBaseline/ExplainedVariance       0.871162
PolicyExecTime                                0.0932832
ProcessExecTime                               0.0110726
TotalEnvSteps                            719532
policy/Entropy                               -1.50392
policy/KL                                     0.00640029
policy/KLBefore                               0
policy/LossAfter                             -0.0133123
policy/LossBefore                            -2.12032e-08
policy/Perplexity                             0.222257
policy/dLoss                                  0.0133122
---------------------------------------  ----------------
2022-04-23 14:23:23 | [train_policy] epoch #711 | Obtaining samples for iteration 711...
2022-04-23 14:23:23 | [train_policy] epoch #711 | Logging diagnostics...
2022-04-23 14:23:23 | [train_policy] epoch #711 | Optimizing policy...
2022-04-23 14:23:23 | [train_policy] epoch #711 | Computing loss before
2022-04-23 14:23:23 | [train_policy] epoch #711 | Computing KL before
2022-04-23 14:23:23 | [train_policy] epoch #711 | Optimizing
2022-04-23 14:23:23 | [train_policy] epoch #711 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:23 | [train_policy] epoch #711 | computing loss before
2022-04-23 14:23:23 | [train_policy] epoch #711 | computing gradient
2022-04-23 14:23:23 | [train_policy] epoch #711 | gradient computed
2022-04-23 14:23:23 | [train_policy] epoch #711 | computing descent direction
2022-04-23 14:23:23 | [train_policy] epoch #711 | descent direction computed
2022-04-23 14:23:23 | [train_policy] epoch #711 | backtrack iters: 0
2022-04-23 14:23:23 | [train_policy] epoch #711 | optimization finished
2022-04-23 14:23:23 | [train_policy] epoch #711 | Computing KL after
2022-04-23 14:23:23 | [train_policy] epoch #711 | Computing loss after
2022-04-23 14:23:23 | [train_policy] epoch #711 | Fitting baseline...
2022-04-23 14:23:23 | [train_policy] epoch #711 | Saving snapshot...
2022-04-23 14:23:23 | [train_policy] epoch #711 | Saved
2022-04-23 14:23:23 | [train_policy] epoch #711 | Time 251.39 s
2022-04-23 14:23:23 | [train_policy] epoch #711 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.118381
Evaluation/AverageDiscountedReturn          -40.6601
Evaluation/AverageReturn                    -40.6601
Evaluation/CompletionRate                     0
Evaluation/Iteration                        711
Evaluation/MaxReturn                        -28.5708
Evaluation/MinReturn                        -71.9056
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.0858
Extras/EpisodeRewardMean                    -40.5246
LinearFeatureBaseline/ExplainedVariance       0.874672
PolicyExecTime                                0.0920756
ProcessExecTime                               0.0110743
TotalEnvSteps                            720544
policy/Entropy                               -1.50834
policy/KL                                     0.00966602
policy/KLBefore                               0
policy/LossAfter                             -0.0211683
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.221278
policy/dLoss                                  0.0211683
---------------------------------------  ----------------
2022-04-23 14:23:23 | [train_policy] epoch #712 | Obtaining samples for iteration 712...
2022-04-23 14:23:23 | [train_policy] epoch #712 | Logging diagnostics...
2022-04-23 14:23:23 | [train_policy] epoch #712 | Optimizing policy...
2022-04-23 14:23:23 | [train_policy] epoch #712 | Computing loss before
2022-04-23 14:23:23 | [train_policy] epoch #712 | Computing KL before
2022-04-23 14:23:23 | [train_policy] epoch #712 | Optimizing
2022-04-23 14:23:23 | [train_policy] epoch #712 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:23 | [train_policy] epoch #712 | computing loss before
2022-04-23 14:23:23 | [train_policy] epoch #712 | computing gradient
2022-04-23 14:23:23 | [train_policy] epoch #712 | gradient computed
2022-04-23 14:23:23 | [train_policy] epoch #712 | computing descent direction
2022-04-23 14:23:23 | [train_policy] epoch #712 | descent direction computed
2022-04-23 14:23:23 | [train_policy] epoch #712 | backtrack iters: 1
2022-04-23 14:23:23 | [train_policy] epoch #712 | optimization finished
2022-04-23 14:23:23 | [train_policy] epoch #712 | Computing KL after
2022-04-23 14:23:23 | [train_policy] epoch #712 | Computing loss after
2022-04-23 14:23:23 | [train_policy] epoch #712 | Fitting baseline...
2022-04-23 14:23:23 | [train_policy] epoch #712 | Saving snapshot...
2022-04-23 14:23:23 | [train_policy] epoch #712 | Saved
2022-04-23 14:23:23 | [train_policy] epoch #712 | Time 251.71 s
2022-04-23 14:23:23 | [train_policy] epoch #712 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.11747
Evaluation/AverageDiscountedReturn          -41.3551
Evaluation/AverageReturn                    -41.3551
Evaluation/CompletionRate                     0
Evaluation/Iteration                        712
Evaluation/MaxReturn                        -28.6784
Evaluation/MinReturn                        -72.9008
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.5933
Extras/EpisodeRewardMean                    -41.4929
LinearFeatureBaseline/ExplainedVariance       0.855352
PolicyExecTime                                0.0905814
ProcessExecTime                               0.0111651
TotalEnvSteps                            721556
policy/Entropy                               -1.5417
policy/KL                                     0.00655557
policy/KLBefore                               0
policy/LossAfter                             -0.0167442
policy/LossBefore                             5.30081e-09
policy/Perplexity                             0.214016
policy/dLoss                                  0.0167442
---------------------------------------  ----------------
2022-04-23 14:23:23 | [train_policy] epoch #713 | Obtaining samples for iteration 713...
2022-04-23 14:23:24 | [train_policy] epoch #713 | Logging diagnostics...
2022-04-23 14:23:24 | [train_policy] epoch #713 | Optimizing policy...
2022-04-23 14:23:24 | [train_policy] epoch #713 | Computing loss before
2022-04-23 14:23:24 | [train_policy] epoch #713 | Computing KL before
2022-04-23 14:23:24 | [train_policy] epoch #713 | Optimizing
2022-04-23 14:23:24 | [train_policy] epoch #713 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:24 | [train_policy] epoch #713 | computing loss before
2022-04-23 14:23:24 | [train_policy] epoch #713 | computing gradient
2022-04-23 14:23:24 | [train_policy] epoch #713 | gradient computed
2022-04-23 14:23:24 | [train_policy] epoch #713 | computing descent direction
2022-04-23 14:23:24 | [train_policy] epoch #713 | descent direction computed
2022-04-23 14:23:24 | [train_policy] epoch #713 | backtrack iters: 0
2022-04-23 14:23:24 | [train_policy] epoch #713 | optimization finished
2022-04-23 14:23:24 | [train_policy] epoch #713 | Computing KL after
2022-04-23 14:23:24 | [train_policy] epoch #713 | Computing loss after
2022-04-23 14:23:24 | [train_policy] epoch #713 | Fitting baseline...
2022-04-23 14:23:24 | [train_policy] epoch #713 | Saving snapshot...
2022-04-23 14:23:24 | [train_policy] epoch #713 | Saved
2022-04-23 14:23:24 | [train_policy] epoch #713 | Time 252.03 s
2022-04-23 14:23:24 | [train_policy] epoch #713 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119087
Evaluation/AverageDiscountedReturn          -41.2048
Evaluation/AverageReturn                    -41.2048
Evaluation/CompletionRate                     0
Evaluation/Iteration                        713
Evaluation/MaxReturn                        -28.8402
Evaluation/MinReturn                        -71.9942
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.38914
Extras/EpisodeRewardMean                    -41.1567
LinearFeatureBaseline/ExplainedVariance       0.880503
PolicyExecTime                                0.0928547
ProcessExecTime                               0.0111887
TotalEnvSteps                            722568
policy/Entropy                               -1.52342
policy/KL                                     0.0097314
policy/KLBefore                               0
policy/LossAfter                             -0.0208812
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.217965
policy/dLoss                                  0.0208812
---------------------------------------  ----------------
2022-04-23 14:23:24 | [train_policy] epoch #714 | Obtaining samples for iteration 714...
2022-04-23 14:23:24 | [train_policy] epoch #714 | Logging diagnostics...
2022-04-23 14:23:24 | [train_policy] epoch #714 | Optimizing policy...
2022-04-23 14:23:24 | [train_policy] epoch #714 | Computing loss before
2022-04-23 14:23:24 | [train_policy] epoch #714 | Computing KL before
2022-04-23 14:23:24 | [train_policy] epoch #714 | Optimizing
2022-04-23 14:23:24 | [train_policy] epoch #714 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:24 | [train_policy] epoch #714 | computing loss before
2022-04-23 14:23:24 | [train_policy] epoch #714 | computing gradient
2022-04-23 14:23:24 | [train_policy] epoch #714 | gradient computed
2022-04-23 14:23:24 | [train_policy] epoch #714 | computing descent direction
2022-04-23 14:23:24 | [train_policy] epoch #714 | descent direction computed
2022-04-23 14:23:24 | [train_policy] epoch #714 | backtrack iters: 0
2022-04-23 14:23:24 | [train_policy] epoch #714 | optimization finished
2022-04-23 14:23:24 | [train_policy] epoch #714 | Computing KL after
2022-04-23 14:23:24 | [train_policy] epoch #714 | Computing loss after
2022-04-23 14:23:24 | [train_policy] epoch #714 | Fitting baseline...
2022-04-23 14:23:24 | [train_policy] epoch #714 | Saving snapshot...
2022-04-23 14:23:24 | [train_policy] epoch #714 | Saved
2022-04-23 14:23:24 | [train_policy] epoch #714 | Time 252.35 s
2022-04-23 14:23:24 | [train_policy] epoch #714 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119364
Evaluation/AverageDiscountedReturn          -41.7592
Evaluation/AverageReturn                    -41.7592
Evaluation/CompletionRate                     0
Evaluation/Iteration                        714
Evaluation/MaxReturn                        -28.8784
Evaluation/MinReturn                        -72.4822
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.81702
Extras/EpisodeRewardMean                    -41.4297
LinearFeatureBaseline/ExplainedVariance       0.838156
PolicyExecTime                                0.093441
ProcessExecTime                               0.0111725
TotalEnvSteps                            723580
policy/Entropy                               -1.51089
policy/KL                                     0.00987853
policy/KLBefore                               0
policy/LossAfter                             -0.0137862
policy/LossBefore                            -1.27219e-08
policy/Perplexity                             0.220714
policy/dLoss                                  0.0137862
---------------------------------------  ----------------
2022-04-23 14:23:24 | [train_policy] epoch #715 | Obtaining samples for iteration 715...
2022-04-23 14:23:24 | [train_policy] epoch #715 | Logging diagnostics...
2022-04-23 14:23:24 | [train_policy] epoch #715 | Optimizing policy...
2022-04-23 14:23:24 | [train_policy] epoch #715 | Computing loss before
2022-04-23 14:23:24 | [train_policy] epoch #715 | Computing KL before
2022-04-23 14:23:24 | [train_policy] epoch #715 | Optimizing
2022-04-23 14:23:24 | [train_policy] epoch #715 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:24 | [train_policy] epoch #715 | computing loss before
2022-04-23 14:23:24 | [train_policy] epoch #715 | computing gradient
2022-04-23 14:23:24 | [train_policy] epoch #715 | gradient computed
2022-04-23 14:23:24 | [train_policy] epoch #715 | computing descent direction
2022-04-23 14:23:24 | [train_policy] epoch #715 | descent direction computed
2022-04-23 14:23:24 | [train_policy] epoch #715 | backtrack iters: 1
2022-04-23 14:23:24 | [train_policy] epoch #715 | optimization finished
2022-04-23 14:23:24 | [train_policy] epoch #715 | Computing KL after
2022-04-23 14:23:24 | [train_policy] epoch #715 | Computing loss after
2022-04-23 14:23:24 | [train_policy] epoch #715 | Fitting baseline...
2022-04-23 14:23:24 | [train_policy] epoch #715 | Saving snapshot...
2022-04-23 14:23:24 | [train_policy] epoch #715 | Saved
2022-04-23 14:23:24 | [train_policy] epoch #715 | Time 252.68 s
2022-04-23 14:23:24 | [train_policy] epoch #715 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118514
Evaluation/AverageDiscountedReturn          -41.9206
Evaluation/AverageReturn                    -41.9206
Evaluation/CompletionRate                     0
Evaluation/Iteration                        715
Evaluation/MaxReturn                        -30.5835
Evaluation/MinReturn                        -64.4606
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.4986
Extras/EpisodeRewardMean                    -41.6736
LinearFeatureBaseline/ExplainedVariance       0.893026
PolicyExecTime                                0.0945773
ProcessExecTime                               0.0111699
TotalEnvSteps                            724592
policy/Entropy                               -1.50377
policy/KL                                     0.00642535
policy/KLBefore                               0
policy/LossAfter                             -0.0145736
policy/LossBefore                             1.90829e-08
policy/Perplexity                             0.22229
policy/dLoss                                  0.0145736
---------------------------------------  ----------------
2022-04-23 14:23:24 | [train_policy] epoch #716 | Obtaining samples for iteration 716...
2022-04-23 14:23:25 | [train_policy] epoch #716 | Logging diagnostics...
2022-04-23 14:23:25 | [train_policy] epoch #716 | Optimizing policy...
2022-04-23 14:23:25 | [train_policy] epoch #716 | Computing loss before
2022-04-23 14:23:25 | [train_policy] epoch #716 | Computing KL before
2022-04-23 14:23:25 | [train_policy] epoch #716 | Optimizing
2022-04-23 14:23:25 | [train_policy] epoch #716 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:25 | [train_policy] epoch #716 | computing loss before
2022-04-23 14:23:25 | [train_policy] epoch #716 | computing gradient
2022-04-23 14:23:25 | [train_policy] epoch #716 | gradient computed
2022-04-23 14:23:25 | [train_policy] epoch #716 | computing descent direction
2022-04-23 14:23:25 | [train_policy] epoch #716 | descent direction computed
2022-04-23 14:23:25 | [train_policy] epoch #716 | backtrack iters: 1
2022-04-23 14:23:25 | [train_policy] epoch #716 | optimization finished
2022-04-23 14:23:25 | [train_policy] epoch #716 | Computing KL after
2022-04-23 14:23:25 | [train_policy] epoch #716 | Computing loss after
2022-04-23 14:23:25 | [train_policy] epoch #716 | Fitting baseline...
2022-04-23 14:23:25 | [train_policy] epoch #716 | Saving snapshot...
2022-04-23 14:23:25 | [train_policy] epoch #716 | Saved
2022-04-23 14:23:25 | [train_policy] epoch #716 | Time 253.00 s
2022-04-23 14:23:25 | [train_policy] epoch #716 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118477
Evaluation/AverageDiscountedReturn          -40.7957
Evaluation/AverageReturn                    -40.7957
Evaluation/CompletionRate                     0
Evaluation/Iteration                        716
Evaluation/MaxReturn                        -29.0698
Evaluation/MinReturn                        -72.2015
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.39581
Extras/EpisodeRewardMean                    -40.7424
LinearFeatureBaseline/ExplainedVariance       0.878361
PolicyExecTime                                0.0935092
ProcessExecTime                               0.0111568
TotalEnvSteps                            725604
policy/Entropy                               -1.53216
policy/KL                                     0.00659211
policy/KLBefore                               0
policy/LossAfter                             -0.013908
policy/LossBefore                            -6.12538e-09
policy/Perplexity                             0.216069
policy/dLoss                                  0.013908
---------------------------------------  ----------------
2022-04-23 14:23:25 | [train_policy] epoch #717 | Obtaining samples for iteration 717...
2022-04-23 14:23:25 | [train_policy] epoch #717 | Logging diagnostics...
2022-04-23 14:23:25 | [train_policy] epoch #717 | Optimizing policy...
2022-04-23 14:23:25 | [train_policy] epoch #717 | Computing loss before
2022-04-23 14:23:25 | [train_policy] epoch #717 | Computing KL before
2022-04-23 14:23:25 | [train_policy] epoch #717 | Optimizing
2022-04-23 14:23:25 | [train_policy] epoch #717 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:25 | [train_policy] epoch #717 | computing loss before
2022-04-23 14:23:25 | [train_policy] epoch #717 | computing gradient
2022-04-23 14:23:25 | [train_policy] epoch #717 | gradient computed
2022-04-23 14:23:25 | [train_policy] epoch #717 | computing descent direction
2022-04-23 14:23:25 | [train_policy] epoch #717 | descent direction computed
2022-04-23 14:23:25 | [train_policy] epoch #717 | backtrack iters: 0
2022-04-23 14:23:25 | [train_policy] epoch #717 | optimization finished
2022-04-23 14:23:25 | [train_policy] epoch #717 | Computing KL after
2022-04-23 14:23:25 | [train_policy] epoch #717 | Computing loss after
2022-04-23 14:23:25 | [train_policy] epoch #717 | Fitting baseline...
2022-04-23 14:23:25 | [train_policy] epoch #717 | Saving snapshot...
2022-04-23 14:23:25 | [train_policy] epoch #717 | Saved
2022-04-23 14:23:25 | [train_policy] epoch #717 | Time 253.33 s
2022-04-23 14:23:25 | [train_policy] epoch #717 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118879
Evaluation/AverageDiscountedReturn          -40.3881
Evaluation/AverageReturn                    -40.3881
Evaluation/CompletionRate                     0
Evaluation/Iteration                        717
Evaluation/MaxReturn                        -28.699
Evaluation/MinReturn                        -64.6361
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12561
Extras/EpisodeRewardMean                    -40.5204
LinearFeatureBaseline/ExplainedVariance       0.882972
PolicyExecTime                                0.0984137
ProcessExecTime                               0.0111468
TotalEnvSteps                            726616
policy/Entropy                               -1.55884
policy/KL                                     0.00958064
policy/KLBefore                               0
policy/LossAfter                             -0.0163054
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.21038
policy/dLoss                                  0.0163054
---------------------------------------  ----------------
2022-04-23 14:23:25 | [train_policy] epoch #718 | Obtaining samples for iteration 718...
2022-04-23 14:23:25 | [train_policy] epoch #718 | Logging diagnostics...
2022-04-23 14:23:25 | [train_policy] epoch #718 | Optimizing policy...
2022-04-23 14:23:25 | [train_policy] epoch #718 | Computing loss before
2022-04-23 14:23:25 | [train_policy] epoch #718 | Computing KL before
2022-04-23 14:23:25 | [train_policy] epoch #718 | Optimizing
2022-04-23 14:23:25 | [train_policy] epoch #718 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:25 | [train_policy] epoch #718 | computing loss before
2022-04-23 14:23:25 | [train_policy] epoch #718 | computing gradient
2022-04-23 14:23:25 | [train_policy] epoch #718 | gradient computed
2022-04-23 14:23:25 | [train_policy] epoch #718 | computing descent direction
2022-04-23 14:23:25 | [train_policy] epoch #718 | descent direction computed
2022-04-23 14:23:25 | [train_policy] epoch #718 | backtrack iters: 1
2022-04-23 14:23:25 | [train_policy] epoch #718 | optimization finished
2022-04-23 14:23:25 | [train_policy] epoch #718 | Computing KL after
2022-04-23 14:23:25 | [train_policy] epoch #718 | Computing loss after
2022-04-23 14:23:25 | [train_policy] epoch #718 | Fitting baseline...
2022-04-23 14:23:25 | [train_policy] epoch #718 | Saving snapshot...
2022-04-23 14:23:25 | [train_policy] epoch #718 | Saved
2022-04-23 14:23:25 | [train_policy] epoch #718 | Time 253.66 s
2022-04-23 14:23:25 | [train_policy] epoch #718 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118657
Evaluation/AverageDiscountedReturn          -41.7824
Evaluation/AverageReturn                    -41.7824
Evaluation/CompletionRate                     0
Evaluation/Iteration                        718
Evaluation/MaxReturn                        -28.5852
Evaluation/MinReturn                        -72.2232
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.60731
Extras/EpisodeRewardMean                    -41.7721
LinearFeatureBaseline/ExplainedVariance       0.85306
PolicyExecTime                                0.0970163
ProcessExecTime                               0.0112872
TotalEnvSteps                            727628
policy/Entropy                               -1.59844
policy/KL                                     0.0068728
policy/KLBefore                               0
policy/LossAfter                             -0.0144331
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.202211
policy/dLoss                                  0.0144331
---------------------------------------  ----------------
2022-04-23 14:23:25 | [train_policy] epoch #719 | Obtaining samples for iteration 719...
2022-04-23 14:23:26 | [train_policy] epoch #719 | Logging diagnostics...
2022-04-23 14:23:26 | [train_policy] epoch #719 | Optimizing policy...
2022-04-23 14:23:26 | [train_policy] epoch #719 | Computing loss before
2022-04-23 14:23:26 | [train_policy] epoch #719 | Computing KL before
2022-04-23 14:23:26 | [train_policy] epoch #719 | Optimizing
2022-04-23 14:23:26 | [train_policy] epoch #719 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:26 | [train_policy] epoch #719 | computing loss before
2022-04-23 14:23:26 | [train_policy] epoch #719 | computing gradient
2022-04-23 14:23:26 | [train_policy] epoch #719 | gradient computed
2022-04-23 14:23:26 | [train_policy] epoch #719 | computing descent direction
2022-04-23 14:23:26 | [train_policy] epoch #719 | descent direction computed
2022-04-23 14:23:26 | [train_policy] epoch #719 | backtrack iters: 1
2022-04-23 14:23:26 | [train_policy] epoch #719 | optimization finished
2022-04-23 14:23:26 | [train_policy] epoch #719 | Computing KL after
2022-04-23 14:23:26 | [train_policy] epoch #719 | Computing loss after
2022-04-23 14:23:26 | [train_policy] epoch #719 | Fitting baseline...
2022-04-23 14:23:26 | [train_policy] epoch #719 | Saving snapshot...
2022-04-23 14:23:26 | [train_policy] epoch #719 | Saved
2022-04-23 14:23:26 | [train_policy] epoch #719 | Time 253.98 s
2022-04-23 14:23:26 | [train_policy] epoch #719 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118678
Evaluation/AverageDiscountedReturn          -41.4783
Evaluation/AverageReturn                    -41.4783
Evaluation/CompletionRate                     0
Evaluation/Iteration                        719
Evaluation/MaxReturn                        -28.702
Evaluation/MinReturn                        -72.9573
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.6837
Extras/EpisodeRewardMean                    -41.364
LinearFeatureBaseline/ExplainedVariance       0.850242
PolicyExecTime                                0.0918
ProcessExecTime                               0.0111156
TotalEnvSteps                            728640
policy/Entropy                               -1.60595
policy/KL                                     0.00644735
policy/KLBefore                               0
policy/LossAfter                             -0.0167602
policy/LossBefore                             8.95248e-09
policy/Perplexity                             0.200698
policy/dLoss                                  0.0167602
---------------------------------------  ----------------
2022-04-23 14:23:26 | [train_policy] epoch #720 | Obtaining samples for iteration 720...
2022-04-23 14:23:26 | [train_policy] epoch #720 | Logging diagnostics...
2022-04-23 14:23:26 | [train_policy] epoch #720 | Optimizing policy...
2022-04-23 14:23:26 | [train_policy] epoch #720 | Computing loss before
2022-04-23 14:23:26 | [train_policy] epoch #720 | Computing KL before
2022-04-23 14:23:26 | [train_policy] epoch #720 | Optimizing
2022-04-23 14:23:26 | [train_policy] epoch #720 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:26 | [train_policy] epoch #720 | computing loss before
2022-04-23 14:23:26 | [train_policy] epoch #720 | computing gradient
2022-04-23 14:23:26 | [train_policy] epoch #720 | gradient computed
2022-04-23 14:23:26 | [train_policy] epoch #720 | computing descent direction
2022-04-23 14:23:26 | [train_policy] epoch #720 | descent direction computed
2022-04-23 14:23:26 | [train_policy] epoch #720 | backtrack iters: 1
2022-04-23 14:23:26 | [train_policy] epoch #720 | optimization finished
2022-04-23 14:23:26 | [train_policy] epoch #720 | Computing KL after
2022-04-23 14:23:26 | [train_policy] epoch #720 | Computing loss after
2022-04-23 14:23:26 | [train_policy] epoch #720 | Fitting baseline...
2022-04-23 14:23:26 | [train_policy] epoch #720 | Saving snapshot...
2022-04-23 14:23:26 | [train_policy] epoch #720 | Saved
2022-04-23 14:23:26 | [train_policy] epoch #720 | Time 254.30 s
2022-04-23 14:23:26 | [train_policy] epoch #720 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.11733
Evaluation/AverageDiscountedReturn          -41.3839
Evaluation/AverageReturn                    -41.3839
Evaluation/CompletionRate                     0
Evaluation/Iteration                        720
Evaluation/MaxReturn                        -28.6164
Evaluation/MinReturn                        -73.376
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.93299
Extras/EpisodeRewardMean                    -41.5234
LinearFeatureBaseline/ExplainedVariance       0.863458
PolicyExecTime                                0.0910408
ProcessExecTime                               0.0110288
TotalEnvSteps                            729652
policy/Entropy                               -1.68325
policy/KL                                     0.00673108
policy/KLBefore                               0
policy/LossAfter                             -0.0130031
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.18577
policy/dLoss                                  0.0130031
---------------------------------------  ----------------
2022-04-23 14:23:26 | [train_policy] epoch #721 | Obtaining samples for iteration 721...
2022-04-23 14:23:26 | [train_policy] epoch #721 | Logging diagnostics...
2022-04-23 14:23:26 | [train_policy] epoch #721 | Optimizing policy...
2022-04-23 14:23:26 | [train_policy] epoch #721 | Computing loss before
2022-04-23 14:23:26 | [train_policy] epoch #721 | Computing KL before
2022-04-23 14:23:26 | [train_policy] epoch #721 | Optimizing
2022-04-23 14:23:26 | [train_policy] epoch #721 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:26 | [train_policy] epoch #721 | computing loss before
2022-04-23 14:23:26 | [train_policy] epoch #721 | computing gradient
2022-04-23 14:23:26 | [train_policy] epoch #721 | gradient computed
2022-04-23 14:23:26 | [train_policy] epoch #721 | computing descent direction
2022-04-23 14:23:26 | [train_policy] epoch #721 | descent direction computed
2022-04-23 14:23:26 | [train_policy] epoch #721 | backtrack iters: 1
2022-04-23 14:23:26 | [train_policy] epoch #721 | optimization finished
2022-04-23 14:23:26 | [train_policy] epoch #721 | Computing KL after
2022-04-23 14:23:26 | [train_policy] epoch #721 | Computing loss after
2022-04-23 14:23:26 | [train_policy] epoch #721 | Fitting baseline...
2022-04-23 14:23:26 | [train_policy] epoch #721 | Saving snapshot...
2022-04-23 14:23:26 | [train_policy] epoch #721 | Saved
2022-04-23 14:23:26 | [train_policy] epoch #721 | Time 254.63 s
2022-04-23 14:23:26 | [train_policy] epoch #721 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.117932
Evaluation/AverageDiscountedReturn          -41.4779
Evaluation/AverageReturn                    -41.4779
Evaluation/CompletionRate                     0
Evaluation/Iteration                        721
Evaluation/MaxReturn                        -29.1805
Evaluation/MinReturn                        -64.2068
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.98221
Extras/EpisodeRewardMean                    -41.5406
LinearFeatureBaseline/ExplainedVariance       0.881036
PolicyExecTime                                0.0921204
ProcessExecTime                               0.0110295
TotalEnvSteps                            730664
policy/Entropy                               -1.67772
policy/KL                                     0.00649404
policy/KLBefore                               0
policy/LossAfter                             -0.0196987
policy/LossBefore                            -8.2457e-10
policy/Perplexity                             0.1868
policy/dLoss                                  0.0196987
---------------------------------------  ---------------
2022-04-23 14:23:26 | [train_policy] epoch #722 | Obtaining samples for iteration 722...
2022-04-23 14:23:27 | [train_policy] epoch #722 | Logging diagnostics...
2022-04-23 14:23:27 | [train_policy] epoch #722 | Optimizing policy...
2022-04-23 14:23:27 | [train_policy] epoch #722 | Computing loss before
2022-04-23 14:23:27 | [train_policy] epoch #722 | Computing KL before
2022-04-23 14:23:27 | [train_policy] epoch #722 | Optimizing
2022-04-23 14:23:27 | [train_policy] epoch #722 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:27 | [train_policy] epoch #722 | computing loss before
2022-04-23 14:23:27 | [train_policy] epoch #722 | computing gradient
2022-04-23 14:23:27 | [train_policy] epoch #722 | gradient computed
2022-04-23 14:23:27 | [train_policy] epoch #722 | computing descent direction
2022-04-23 14:23:27 | [train_policy] epoch #722 | descent direction computed
2022-04-23 14:23:27 | [train_policy] epoch #722 | backtrack iters: 1
2022-04-23 14:23:27 | [train_policy] epoch #722 | optimization finished
2022-04-23 14:23:27 | [train_policy] epoch #722 | Computing KL after
2022-04-23 14:23:27 | [train_policy] epoch #722 | Computing loss after
2022-04-23 14:23:27 | [train_policy] epoch #722 | Fitting baseline...
2022-04-23 14:23:27 | [train_policy] epoch #722 | Saving snapshot...
2022-04-23 14:23:27 | [train_policy] epoch #722 | Saved
2022-04-23 14:23:27 | [train_policy] epoch #722 | Time 254.96 s
2022-04-23 14:23:27 | [train_policy] epoch #722 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118848
Evaluation/AverageDiscountedReturn          -39.9359
Evaluation/AverageReturn                    -39.9359
Evaluation/CompletionRate                     0
Evaluation/Iteration                        722
Evaluation/MaxReturn                        -27.6697
Evaluation/MinReturn                        -72.9858
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.96909
Extras/EpisodeRewardMean                    -39.8768
LinearFeatureBaseline/ExplainedVariance       0.878904
PolicyExecTime                                0.0962763
ProcessExecTime                               0.0111854
TotalEnvSteps                            731676
policy/Entropy                               -1.72441
policy/KL                                     0.00648028
policy/KLBefore                               0
policy/LossAfter                             -0.014661
policy/LossBefore                            -1.08372e-08
policy/Perplexity                             0.178278
policy/dLoss                                  0.014661
---------------------------------------  ----------------
2022-04-23 14:23:27 | [train_policy] epoch #723 | Obtaining samples for iteration 723...
2022-04-23 14:23:27 | [train_policy] epoch #723 | Logging diagnostics...
2022-04-23 14:23:27 | [train_policy] epoch #723 | Optimizing policy...
2022-04-23 14:23:27 | [train_policy] epoch #723 | Computing loss before
2022-04-23 14:23:27 | [train_policy] epoch #723 | Computing KL before
2022-04-23 14:23:27 | [train_policy] epoch #723 | Optimizing
2022-04-23 14:23:27 | [train_policy] epoch #723 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:27 | [train_policy] epoch #723 | computing loss before
2022-04-23 14:23:27 | [train_policy] epoch #723 | computing gradient
2022-04-23 14:23:27 | [train_policy] epoch #723 | gradient computed
2022-04-23 14:23:27 | [train_policy] epoch #723 | computing descent direction
2022-04-23 14:23:27 | [train_policy] epoch #723 | descent direction computed
2022-04-23 14:23:27 | [train_policy] epoch #723 | backtrack iters: 1
2022-04-23 14:23:27 | [train_policy] epoch #723 | optimization finished
2022-04-23 14:23:27 | [train_policy] epoch #723 | Computing KL after
2022-04-23 14:23:27 | [train_policy] epoch #723 | Computing loss after
2022-04-23 14:23:27 | [train_policy] epoch #723 | Fitting baseline...
2022-04-23 14:23:27 | [train_policy] epoch #723 | Saving snapshot...
2022-04-23 14:23:27 | [train_policy] epoch #723 | Saved
2022-04-23 14:23:27 | [train_policy] epoch #723 | Time 255.30 s
2022-04-23 14:23:27 | [train_policy] epoch #723 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117121
Evaluation/AverageDiscountedReturn          -41.4836
Evaluation/AverageReturn                    -41.4836
Evaluation/CompletionRate                     0
Evaluation/Iteration                        723
Evaluation/MaxReturn                        -27.6647
Evaluation/MinReturn                        -64.2293
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.72757
Extras/EpisodeRewardMean                    -41.6986
LinearFeatureBaseline/ExplainedVariance       0.87501
PolicyExecTime                                0.101837
ProcessExecTime                               0.011713
TotalEnvSteps                            732688
policy/Entropy                               -1.74687
policy/KL                                     0.00663194
policy/KLBefore                               0
policy/LossAfter                             -0.0136216
policy/LossBefore                            -1.57846e-08
policy/Perplexity                             0.174318
policy/dLoss                                  0.0136216
---------------------------------------  ----------------
2022-04-23 14:23:27 | [train_policy] epoch #724 | Obtaining samples for iteration 724...
2022-04-23 14:23:27 | [train_policy] epoch #724 | Logging diagnostics...
2022-04-23 14:23:27 | [train_policy] epoch #724 | Optimizing policy...
2022-04-23 14:23:27 | [train_policy] epoch #724 | Computing loss before
2022-04-23 14:23:27 | [train_policy] epoch #724 | Computing KL before
2022-04-23 14:23:27 | [train_policy] epoch #724 | Optimizing
2022-04-23 14:23:27 | [train_policy] epoch #724 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:27 | [train_policy] epoch #724 | computing loss before
2022-04-23 14:23:27 | [train_policy] epoch #724 | computing gradient
2022-04-23 14:23:27 | [train_policy] epoch #724 | gradient computed
2022-04-23 14:23:27 | [train_policy] epoch #724 | computing descent direction
2022-04-23 14:23:27 | [train_policy] epoch #724 | descent direction computed
2022-04-23 14:23:27 | [train_policy] epoch #724 | backtrack iters: 0
2022-04-23 14:23:27 | [train_policy] epoch #724 | optimization finished
2022-04-23 14:23:27 | [train_policy] epoch #724 | Computing KL after
2022-04-23 14:23:27 | [train_policy] epoch #724 | Computing loss after
2022-04-23 14:23:27 | [train_policy] epoch #724 | Fitting baseline...
2022-04-23 14:23:27 | [train_policy] epoch #724 | Saving snapshot...
2022-04-23 14:23:27 | [train_policy] epoch #724 | Saved
2022-04-23 14:23:27 | [train_policy] epoch #724 | Time 255.63 s
2022-04-23 14:23:27 | [train_policy] epoch #724 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119659
Evaluation/AverageDiscountedReturn          -41.6806
Evaluation/AverageReturn                    -41.6806
Evaluation/CompletionRate                     0
Evaluation/Iteration                        724
Evaluation/MaxReturn                        -27.9363
Evaluation/MinReturn                        -72.7192
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.73401
Extras/EpisodeRewardMean                    -41.9192
LinearFeatureBaseline/ExplainedVariance       0.857524
PolicyExecTime                                0.098973
ProcessExecTime                               0.0116897
TotalEnvSteps                            733700
policy/Entropy                               -1.77231
policy/KL                                     0.00987863
policy/KLBefore                               0
policy/LossAfter                             -0.0133813
policy/LossBefore                             1.27219e-08
policy/Perplexity                             0.16994
policy/dLoss                                  0.0133813
---------------------------------------  ----------------
2022-04-23 14:23:27 | [train_policy] epoch #725 | Obtaining samples for iteration 725...
2022-04-23 14:23:28 | [train_policy] epoch #725 | Logging diagnostics...
2022-04-23 14:23:28 | [train_policy] epoch #725 | Optimizing policy...
2022-04-23 14:23:28 | [train_policy] epoch #725 | Computing loss before
2022-04-23 14:23:28 | [train_policy] epoch #725 | Computing KL before
2022-04-23 14:23:28 | [train_policy] epoch #725 | Optimizing
2022-04-23 14:23:28 | [train_policy] epoch #725 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:28 | [train_policy] epoch #725 | computing loss before
2022-04-23 14:23:28 | [train_policy] epoch #725 | computing gradient
2022-04-23 14:23:28 | [train_policy] epoch #725 | gradient computed
2022-04-23 14:23:28 | [train_policy] epoch #725 | computing descent direction
2022-04-23 14:23:28 | [train_policy] epoch #725 | descent direction computed
2022-04-23 14:23:28 | [train_policy] epoch #725 | backtrack iters: 1
2022-04-23 14:23:28 | [train_policy] epoch #725 | optimization finished
2022-04-23 14:23:28 | [train_policy] epoch #725 | Computing KL after
2022-04-23 14:23:28 | [train_policy] epoch #725 | Computing loss after
2022-04-23 14:23:28 | [train_policy] epoch #725 | Fitting baseline...
2022-04-23 14:23:28 | [train_policy] epoch #725 | Saving snapshot...
2022-04-23 14:23:28 | [train_policy] epoch #725 | Saved
2022-04-23 14:23:28 | [train_policy] epoch #725 | Time 255.96 s
2022-04-23 14:23:28 | [train_policy] epoch #725 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120018
Evaluation/AverageDiscountedReturn          -42.3349
Evaluation/AverageReturn                    -42.3349
Evaluation/CompletionRate                     0
Evaluation/Iteration                        725
Evaluation/MaxReturn                        -28.1782
Evaluation/MinReturn                        -73.1017
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.97898
Extras/EpisodeRewardMean                    -41.7216
LinearFeatureBaseline/ExplainedVariance       0.811727
PolicyExecTime                                0.0993178
ProcessExecTime                               0.0118504
TotalEnvSteps                            734712
policy/Entropy                               -1.77238
policy/KL                                     0.00654029
policy/KLBefore                               0
policy/LossAfter                             -0.011579
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.169928
policy/dLoss                                  0.011579
---------------------------------------  ----------------
2022-04-23 14:23:28 | [train_policy] epoch #726 | Obtaining samples for iteration 726...
2022-04-23 14:23:28 | [train_policy] epoch #726 | Logging diagnostics...
2022-04-23 14:23:28 | [train_policy] epoch #726 | Optimizing policy...
2022-04-23 14:23:28 | [train_policy] epoch #726 | Computing loss before
2022-04-23 14:23:28 | [train_policy] epoch #726 | Computing KL before
2022-04-23 14:23:28 | [train_policy] epoch #726 | Optimizing
2022-04-23 14:23:28 | [train_policy] epoch #726 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:28 | [train_policy] epoch #726 | computing loss before
2022-04-23 14:23:28 | [train_policy] epoch #726 | computing gradient
2022-04-23 14:23:28 | [train_policy] epoch #726 | gradient computed
2022-04-23 14:23:28 | [train_policy] epoch #726 | computing descent direction
2022-04-23 14:23:28 | [train_policy] epoch #726 | descent direction computed
2022-04-23 14:23:28 | [train_policy] epoch #726 | backtrack iters: 1
2022-04-23 14:23:28 | [train_policy] epoch #726 | optimization finished
2022-04-23 14:23:28 | [train_policy] epoch #726 | Computing KL after
2022-04-23 14:23:28 | [train_policy] epoch #726 | Computing loss after
2022-04-23 14:23:28 | [train_policy] epoch #726 | Fitting baseline...
2022-04-23 14:23:28 | [train_policy] epoch #726 | Saving snapshot...
2022-04-23 14:23:28 | [train_policy] epoch #726 | Saved
2022-04-23 14:23:28 | [train_policy] epoch #726 | Time 256.29 s
2022-04-23 14:23:28 | [train_policy] epoch #726 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118079
Evaluation/AverageDiscountedReturn          -39.3208
Evaluation/AverageReturn                    -39.3208
Evaluation/CompletionRate                     0
Evaluation/Iteration                        726
Evaluation/MaxReturn                        -28.6091
Evaluation/MinReturn                        -63.9487
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.99363
Extras/EpisodeRewardMean                    -39.6095
LinearFeatureBaseline/ExplainedVariance       0.890534
PolicyExecTime                                0.0910141
ProcessExecTime                               0.0112376
TotalEnvSteps                            735724
policy/Entropy                               -1.78806
policy/KL                                     0.00660582
policy/KLBefore                               0
policy/LossAfter                             -0.0106618
policy/LossBefore                            -2.35591e-09
policy/Perplexity                             0.167285
policy/dLoss                                  0.0106618
---------------------------------------  ----------------
2022-04-23 14:23:28 | [train_policy] epoch #727 | Obtaining samples for iteration 727...
2022-04-23 14:23:28 | [train_policy] epoch #727 | Logging diagnostics...
2022-04-23 14:23:28 | [train_policy] epoch #727 | Optimizing policy...
2022-04-23 14:23:28 | [train_policy] epoch #727 | Computing loss before
2022-04-23 14:23:28 | [train_policy] epoch #727 | Computing KL before
2022-04-23 14:23:28 | [train_policy] epoch #727 | Optimizing
2022-04-23 14:23:28 | [train_policy] epoch #727 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:28 | [train_policy] epoch #727 | computing loss before
2022-04-23 14:23:28 | [train_policy] epoch #727 | computing gradient
2022-04-23 14:23:28 | [train_policy] epoch #727 | gradient computed
2022-04-23 14:23:28 | [train_policy] epoch #727 | computing descent direction
2022-04-23 14:23:28 | [train_policy] epoch #727 | descent direction computed
2022-04-23 14:23:28 | [train_policy] epoch #727 | backtrack iters: 1
2022-04-23 14:23:28 | [train_policy] epoch #727 | optimization finished
2022-04-23 14:23:28 | [train_policy] epoch #727 | Computing KL after
2022-04-23 14:23:28 | [train_policy] epoch #727 | Computing loss after
2022-04-23 14:23:28 | [train_policy] epoch #727 | Fitting baseline...
2022-04-23 14:23:28 | [train_policy] epoch #727 | Saving snapshot...
2022-04-23 14:23:28 | [train_policy] epoch #727 | Saved
2022-04-23 14:23:28 | [train_policy] epoch #727 | Time 256.61 s
2022-04-23 14:23:28 | [train_policy] epoch #727 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120822
Evaluation/AverageDiscountedReturn          -41.1808
Evaluation/AverageReturn                    -41.1808
Evaluation/CompletionRate                     0
Evaluation/Iteration                        727
Evaluation/MaxReturn                        -28.6795
Evaluation/MinReturn                        -71.9847
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.67128
Extras/EpisodeRewardMean                    -40.8811
LinearFeatureBaseline/ExplainedVariance       0.888853
PolicyExecTime                                0.0929613
ProcessExecTime                               0.0112681
TotalEnvSteps                            736736
policy/Entropy                               -1.79255
policy/KL                                     0.00700836
policy/KLBefore                               0
policy/LossAfter                             -0.0158379
policy/LossBefore                            -3.47497e-09
policy/Perplexity                             0.166535
policy/dLoss                                  0.0158379
---------------------------------------  ----------------
2022-04-23 14:23:28 | [train_policy] epoch #728 | Obtaining samples for iteration 728...
2022-04-23 14:23:29 | [train_policy] epoch #728 | Logging diagnostics...
2022-04-23 14:23:29 | [train_policy] epoch #728 | Optimizing policy...
2022-04-23 14:23:29 | [train_policy] epoch #728 | Computing loss before
2022-04-23 14:23:29 | [train_policy] epoch #728 | Computing KL before
2022-04-23 14:23:29 | [train_policy] epoch #728 | Optimizing
2022-04-23 14:23:29 | [train_policy] epoch #728 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:29 | [train_policy] epoch #728 | computing loss before
2022-04-23 14:23:29 | [train_policy] epoch #728 | computing gradient
2022-04-23 14:23:29 | [train_policy] epoch #728 | gradient computed
2022-04-23 14:23:29 | [train_policy] epoch #728 | computing descent direction
2022-04-23 14:23:29 | [train_policy] epoch #728 | descent direction computed
2022-04-23 14:23:29 | [train_policy] epoch #728 | backtrack iters: 0
2022-04-23 14:23:29 | [train_policy] epoch #728 | optimization finished
2022-04-23 14:23:29 | [train_policy] epoch #728 | Computing KL after
2022-04-23 14:23:29 | [train_policy] epoch #728 | Computing loss after
2022-04-23 14:23:29 | [train_policy] epoch #728 | Fitting baseline...
2022-04-23 14:23:29 | [train_policy] epoch #728 | Saving snapshot...
2022-04-23 14:23:29 | [train_policy] epoch #728 | Saved
2022-04-23 14:23:29 | [train_policy] epoch #728 | Time 256.94 s
2022-04-23 14:23:29 | [train_policy] epoch #728 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119109
Evaluation/AverageDiscountedReturn          -42.123
Evaluation/AverageReturn                    -42.123
Evaluation/CompletionRate                     0
Evaluation/Iteration                        728
Evaluation/MaxReturn                        -28.2712
Evaluation/MinReturn                        -72.6529
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.25169
Extras/EpisodeRewardMean                    -42.3141
LinearFeatureBaseline/ExplainedVariance       0.78186
PolicyExecTime                                0.0948534
ProcessExecTime                               0.0111318
TotalEnvSteps                            737748
policy/Entropy                               -1.7568
policy/KL                                     0.00963126
policy/KLBefore                               0
policy/LossAfter                             -0.0218201
policy/LossBefore                             2.35591e-10
policy/Perplexity                             0.172596
policy/dLoss                                  0.0218201
---------------------------------------  ----------------
2022-04-23 14:23:29 | [train_policy] epoch #729 | Obtaining samples for iteration 729...
2022-04-23 14:23:29 | [train_policy] epoch #729 | Logging diagnostics...
2022-04-23 14:23:29 | [train_policy] epoch #729 | Optimizing policy...
2022-04-23 14:23:29 | [train_policy] epoch #729 | Computing loss before
2022-04-23 14:23:29 | [train_policy] epoch #729 | Computing KL before
2022-04-23 14:23:29 | [train_policy] epoch #729 | Optimizing
2022-04-23 14:23:29 | [train_policy] epoch #729 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:29 | [train_policy] epoch #729 | computing loss before
2022-04-23 14:23:29 | [train_policy] epoch #729 | computing gradient
2022-04-23 14:23:29 | [train_policy] epoch #729 | gradient computed
2022-04-23 14:23:29 | [train_policy] epoch #729 | computing descent direction
2022-04-23 14:23:29 | [train_policy] epoch #729 | descent direction computed
2022-04-23 14:23:29 | [train_policy] epoch #729 | backtrack iters: 1
2022-04-23 14:23:29 | [train_policy] epoch #729 | optimization finished
2022-04-23 14:23:29 | [train_policy] epoch #729 | Computing KL after
2022-04-23 14:23:29 | [train_policy] epoch #729 | Computing loss after
2022-04-23 14:23:29 | [train_policy] epoch #729 | Fitting baseline...
2022-04-23 14:23:29 | [train_policy] epoch #729 | Saving snapshot...
2022-04-23 14:23:29 | [train_policy] epoch #729 | Saved
2022-04-23 14:23:29 | [train_policy] epoch #729 | Time 257.28 s
2022-04-23 14:23:29 | [train_policy] epoch #729 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11994
Evaluation/AverageDiscountedReturn          -41.3549
Evaluation/AverageReturn                    -41.3549
Evaluation/CompletionRate                     0
Evaluation/Iteration                        729
Evaluation/MaxReturn                        -30.1177
Evaluation/MinReturn                        -72.7063
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.1531
Extras/EpisodeRewardMean                    -40.9582
LinearFeatureBaseline/ExplainedVariance       0.896303
PolicyExecTime                                0.0986559
ProcessExecTime                               0.0113351
TotalEnvSteps                            738760
policy/Entropy                               -1.78116
policy/KL                                     0.00666917
policy/KLBefore                               0
policy/LossAfter                             -0.0158847
policy/LossBefore                             4.35844e-09
policy/Perplexity                             0.168442
policy/dLoss                                  0.0158847
---------------------------------------  ----------------
2022-04-23 14:23:29 | [train_policy] epoch #730 | Obtaining samples for iteration 730...
2022-04-23 14:23:29 | [train_policy] epoch #730 | Logging diagnostics...
2022-04-23 14:23:29 | [train_policy] epoch #730 | Optimizing policy...
2022-04-23 14:23:29 | [train_policy] epoch #730 | Computing loss before
2022-04-23 14:23:29 | [train_policy] epoch #730 | Computing KL before
2022-04-23 14:23:29 | [train_policy] epoch #730 | Optimizing
2022-04-23 14:23:29 | [train_policy] epoch #730 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:29 | [train_policy] epoch #730 | computing loss before
2022-04-23 14:23:29 | [train_policy] epoch #730 | computing gradient
2022-04-23 14:23:29 | [train_policy] epoch #730 | gradient computed
2022-04-23 14:23:29 | [train_policy] epoch #730 | computing descent direction
2022-04-23 14:23:29 | [train_policy] epoch #730 | descent direction computed
2022-04-23 14:23:29 | [train_policy] epoch #730 | backtrack iters: 0
2022-04-23 14:23:29 | [train_policy] epoch #730 | optimization finished
2022-04-23 14:23:29 | [train_policy] epoch #730 | Computing KL after
2022-04-23 14:23:29 | [train_policy] epoch #730 | Computing loss after
2022-04-23 14:23:29 | [train_policy] epoch #730 | Fitting baseline...
2022-04-23 14:23:29 | [train_policy] epoch #730 | Saving snapshot...
2022-04-23 14:23:29 | [train_policy] epoch #730 | Saved
2022-04-23 14:23:29 | [train_policy] epoch #730 | Time 257.61 s
2022-04-23 14:23:29 | [train_policy] epoch #730 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118883
Evaluation/AverageDiscountedReturn          -42.2371
Evaluation/AverageReturn                    -42.2371
Evaluation/CompletionRate                     0
Evaluation/Iteration                        730
Evaluation/MaxReturn                        -28.4805
Evaluation/MinReturn                        -72.2873
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.47473
Extras/EpisodeRewardMean                    -42.3188
LinearFeatureBaseline/ExplainedVariance       0.879782
PolicyExecTime                                0.0992167
ProcessExecTime                               0.0110929
TotalEnvSteps                            739772
policy/Entropy                               -1.76606
policy/KL                                     0.00965558
policy/KLBefore                               0
policy/LossAfter                             -0.0188826
policy/LossBefore                            -1.29575e-08
policy/Perplexity                             0.171006
policy/dLoss                                  0.0188826
---------------------------------------  ----------------
2022-04-23 14:23:29 | [train_policy] epoch #731 | Obtaining samples for iteration 731...
2022-04-23 14:23:30 | [train_policy] epoch #731 | Logging diagnostics...
2022-04-23 14:23:30 | [train_policy] epoch #731 | Optimizing policy...
2022-04-23 14:23:30 | [train_policy] epoch #731 | Computing loss before
2022-04-23 14:23:30 | [train_policy] epoch #731 | Computing KL before
2022-04-23 14:23:30 | [train_policy] epoch #731 | Optimizing
2022-04-23 14:23:30 | [train_policy] epoch #731 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:30 | [train_policy] epoch #731 | computing loss before
2022-04-23 14:23:30 | [train_policy] epoch #731 | computing gradient
2022-04-23 14:23:30 | [train_policy] epoch #731 | gradient computed
2022-04-23 14:23:30 | [train_policy] epoch #731 | computing descent direction
2022-04-23 14:23:30 | [train_policy] epoch #731 | descent direction computed
2022-04-23 14:23:30 | [train_policy] epoch #731 | backtrack iters: 1
2022-04-23 14:23:30 | [train_policy] epoch #731 | optimization finished
2022-04-23 14:23:30 | [train_policy] epoch #731 | Computing KL after
2022-04-23 14:23:30 | [train_policy] epoch #731 | Computing loss after
2022-04-23 14:23:30 | [train_policy] epoch #731 | Fitting baseline...
2022-04-23 14:23:30 | [train_policy] epoch #731 | Saving snapshot...
2022-04-23 14:23:30 | [train_policy] epoch #731 | Saved
2022-04-23 14:23:30 | [train_policy] epoch #731 | Time 257.96 s
2022-04-23 14:23:30 | [train_policy] epoch #731 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118469
Evaluation/AverageDiscountedReturn          -39.4936
Evaluation/AverageReturn                    -39.4936
Evaluation/CompletionRate                     0
Evaluation/Iteration                        731
Evaluation/MaxReturn                        -28.7003
Evaluation/MinReturn                        -71.9494
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.9619
Extras/EpisodeRewardMean                    -39.2855
LinearFeatureBaseline/ExplainedVariance       0.870838
PolicyExecTime                                0.107584
ProcessExecTime                               0.0111122
TotalEnvSteps                            740784
policy/Entropy                               -1.74809
policy/KL                                     0.00645144
policy/KLBefore                               0
policy/LossAfter                             -0.0125927
policy/LossBefore                            -1.36643e-08
policy/Perplexity                             0.174106
policy/dLoss                                  0.0125927
---------------------------------------  ----------------
2022-04-23 14:23:30 | [train_policy] epoch #732 | Obtaining samples for iteration 732...
2022-04-23 14:23:30 | [train_policy] epoch #732 | Logging diagnostics...
2022-04-23 14:23:30 | [train_policy] epoch #732 | Optimizing policy...
2022-04-23 14:23:30 | [train_policy] epoch #732 | Computing loss before
2022-04-23 14:23:30 | [train_policy] epoch #732 | Computing KL before
2022-04-23 14:23:30 | [train_policy] epoch #732 | Optimizing
2022-04-23 14:23:30 | [train_policy] epoch #732 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:30 | [train_policy] epoch #732 | computing loss before
2022-04-23 14:23:30 | [train_policy] epoch #732 | computing gradient
2022-04-23 14:23:30 | [train_policy] epoch #732 | gradient computed
2022-04-23 14:23:30 | [train_policy] epoch #732 | computing descent direction
2022-04-23 14:23:30 | [train_policy] epoch #732 | descent direction computed
2022-04-23 14:23:30 | [train_policy] epoch #732 | backtrack iters: 0
2022-04-23 14:23:30 | [train_policy] epoch #732 | optimization finished
2022-04-23 14:23:30 | [train_policy] epoch #732 | Computing KL after
2022-04-23 14:23:30 | [train_policy] epoch #732 | Computing loss after
2022-04-23 14:23:30 | [train_policy] epoch #732 | Fitting baseline...
2022-04-23 14:23:30 | [train_policy] epoch #732 | Saving snapshot...
2022-04-23 14:23:30 | [train_policy] epoch #732 | Saved
2022-04-23 14:23:30 | [train_policy] epoch #732 | Time 258.30 s
2022-04-23 14:23:30 | [train_policy] epoch #732 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119623
Evaluation/AverageDiscountedReturn          -41.3416
Evaluation/AverageReturn                    -41.3416
Evaluation/CompletionRate                     0
Evaluation/Iteration                        732
Evaluation/MaxReturn                        -32.1231
Evaluation/MinReturn                        -73.6364
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.86761
Extras/EpisodeRewardMean                    -41.1783
LinearFeatureBaseline/ExplainedVariance       0.878668
PolicyExecTime                                0.101323
ProcessExecTime                               0.0117784
TotalEnvSteps                            741796
policy/Entropy                               -1.78074
policy/KL                                     0.00986633
policy/KLBefore                               0
policy/LossAfter                             -0.0410271
policy/LossBefore                            -1.46067e-08
policy/Perplexity                             0.168513
policy/dLoss                                  0.0410271
---------------------------------------  ----------------
2022-04-23 14:23:30 | [train_policy] epoch #733 | Obtaining samples for iteration 733...
2022-04-23 14:23:30 | [train_policy] epoch #733 | Logging diagnostics...
2022-04-23 14:23:30 | [train_policy] epoch #733 | Optimizing policy...
2022-04-23 14:23:30 | [train_policy] epoch #733 | Computing loss before
2022-04-23 14:23:30 | [train_policy] epoch #733 | Computing KL before
2022-04-23 14:23:30 | [train_policy] epoch #733 | Optimizing
2022-04-23 14:23:30 | [train_policy] epoch #733 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:30 | [train_policy] epoch #733 | computing loss before
2022-04-23 14:23:30 | [train_policy] epoch #733 | computing gradient
2022-04-23 14:23:30 | [train_policy] epoch #733 | gradient computed
2022-04-23 14:23:30 | [train_policy] epoch #733 | computing descent direction
2022-04-23 14:23:30 | [train_policy] epoch #733 | descent direction computed
2022-04-23 14:23:30 | [train_policy] epoch #733 | backtrack iters: 0
2022-04-23 14:23:30 | [train_policy] epoch #733 | optimization finished
2022-04-23 14:23:30 | [train_policy] epoch #733 | Computing KL after
2022-04-23 14:23:30 | [train_policy] epoch #733 | Computing loss after
2022-04-23 14:23:30 | [train_policy] epoch #733 | Fitting baseline...
2022-04-23 14:23:30 | [train_policy] epoch #733 | Saving snapshot...
2022-04-23 14:23:30 | [train_policy] epoch #733 | Saved
2022-04-23 14:23:30 | [train_policy] epoch #733 | Time 258.63 s
2022-04-23 14:23:30 | [train_policy] epoch #733 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.12015
Evaluation/AverageDiscountedReturn          -42.1659
Evaluation/AverageReturn                    -42.1659
Evaluation/CompletionRate                     0
Evaluation/Iteration                        733
Evaluation/MaxReturn                        -28.2767
Evaluation/MinReturn                        -72.2267
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.34964
Extras/EpisodeRewardMean                    -42.062
LinearFeatureBaseline/ExplainedVariance       0.848457
PolicyExecTime                                0.101265
ProcessExecTime                               0.0116222
TotalEnvSteps                            742808
policy/Entropy                               -1.76843
policy/KL                                     0.00978323
policy/KLBefore                               0
policy/LossAfter                             -0.0183127
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.1706
policy/dLoss                                  0.0183127
---------------------------------------  ----------------
2022-04-23 14:23:30 | [train_policy] epoch #734 | Obtaining samples for iteration 734...
2022-04-23 14:23:31 | [train_policy] epoch #734 | Logging diagnostics...
2022-04-23 14:23:31 | [train_policy] epoch #734 | Optimizing policy...
2022-04-23 14:23:31 | [train_policy] epoch #734 | Computing loss before
2022-04-23 14:23:31 | [train_policy] epoch #734 | Computing KL before
2022-04-23 14:23:31 | [train_policy] epoch #734 | Optimizing
2022-04-23 14:23:31 | [train_policy] epoch #734 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:31 | [train_policy] epoch #734 | computing loss before
2022-04-23 14:23:31 | [train_policy] epoch #734 | computing gradient
2022-04-23 14:23:31 | [train_policy] epoch #734 | gradient computed
2022-04-23 14:23:31 | [train_policy] epoch #734 | computing descent direction
2022-04-23 14:23:31 | [train_policy] epoch #734 | descent direction computed
2022-04-23 14:23:31 | [train_policy] epoch #734 | backtrack iters: 1
2022-04-23 14:23:31 | [train_policy] epoch #734 | optimization finished
2022-04-23 14:23:31 | [train_policy] epoch #734 | Computing KL after
2022-04-23 14:23:31 | [train_policy] epoch #734 | Computing loss after
2022-04-23 14:23:31 | [train_policy] epoch #734 | Fitting baseline...
2022-04-23 14:23:31 | [train_policy] epoch #734 | Saving snapshot...
2022-04-23 14:23:31 | [train_policy] epoch #734 | Saved
2022-04-23 14:23:31 | [train_policy] epoch #734 | Time 258.96 s
2022-04-23 14:23:31 | [train_policy] epoch #734 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118884
Evaluation/AverageDiscountedReturn          -42.1511
Evaluation/AverageReturn                    -42.1511
Evaluation/CompletionRate                     0
Evaluation/Iteration                        734
Evaluation/MaxReturn                        -28.8261
Evaluation/MinReturn                        -73.7409
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.2257
Extras/EpisodeRewardMean                    -42.5519
LinearFeatureBaseline/ExplainedVariance       0.847961
PolicyExecTime                                0.0921695
ProcessExecTime                               0.0112848
TotalEnvSteps                            743820
policy/Entropy                               -1.79852
policy/KL                                     0.00683253
policy/KLBefore                               0
policy/LossAfter                             -0.0196525
policy/LossBefore                            -7.42113e-09
policy/Perplexity                             0.165544
policy/dLoss                                  0.0196525
---------------------------------------  ----------------
2022-04-23 14:23:31 | [train_policy] epoch #735 | Obtaining samples for iteration 735...
2022-04-23 14:23:31 | [train_policy] epoch #735 | Logging diagnostics...
2022-04-23 14:23:31 | [train_policy] epoch #735 | Optimizing policy...
2022-04-23 14:23:31 | [train_policy] epoch #735 | Computing loss before
2022-04-23 14:23:31 | [train_policy] epoch #735 | Computing KL before
2022-04-23 14:23:31 | [train_policy] epoch #735 | Optimizing
2022-04-23 14:23:31 | [train_policy] epoch #735 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:31 | [train_policy] epoch #735 | computing loss before
2022-04-23 14:23:31 | [train_policy] epoch #735 | computing gradient
2022-04-23 14:23:31 | [train_policy] epoch #735 | gradient computed
2022-04-23 14:23:31 | [train_policy] epoch #735 | computing descent direction
2022-04-23 14:23:31 | [train_policy] epoch #735 | descent direction computed
2022-04-23 14:23:31 | [train_policy] epoch #735 | backtrack iters: 1
2022-04-23 14:23:31 | [train_policy] epoch #735 | optimization finished
2022-04-23 14:23:31 | [train_policy] epoch #735 | Computing KL after
2022-04-23 14:23:31 | [train_policy] epoch #735 | Computing loss after
2022-04-23 14:23:31 | [train_policy] epoch #735 | Fitting baseline...
2022-04-23 14:23:31 | [train_policy] epoch #735 | Saving snapshot...
2022-04-23 14:23:31 | [train_policy] epoch #735 | Saved
2022-04-23 14:23:31 | [train_policy] epoch #735 | Time 259.28 s
2022-04-23 14:23:31 | [train_policy] epoch #735 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118857
Evaluation/AverageDiscountedReturn          -40.0118
Evaluation/AverageReturn                    -40.0118
Evaluation/CompletionRate                     0
Evaluation/Iteration                        735
Evaluation/MaxReturn                        -28.6895
Evaluation/MinReturn                        -60.5022
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.10291
Extras/EpisodeRewardMean                    -39.9325
LinearFeatureBaseline/ExplainedVariance       0.857501
PolicyExecTime                                0.0918202
ProcessExecTime                               0.0112188
TotalEnvSteps                            744832
policy/Entropy                               -1.78897
policy/KL                                     0.00744765
policy/KLBefore                               0
policy/LossAfter                             -0.0232241
policy/LossBefore                            -4.47624e-09
policy/Perplexity                             0.167133
policy/dLoss                                  0.0232241
---------------------------------------  ----------------
2022-04-23 14:23:31 | [train_policy] epoch #736 | Obtaining samples for iteration 736...
2022-04-23 14:23:31 | [train_policy] epoch #736 | Logging diagnostics...
2022-04-23 14:23:31 | [train_policy] epoch #736 | Optimizing policy...
2022-04-23 14:23:31 | [train_policy] epoch #736 | Computing loss before
2022-04-23 14:23:31 | [train_policy] epoch #736 | Computing KL before
2022-04-23 14:23:31 | [train_policy] epoch #736 | Optimizing
2022-04-23 14:23:31 | [train_policy] epoch #736 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:31 | [train_policy] epoch #736 | computing loss before
2022-04-23 14:23:31 | [train_policy] epoch #736 | computing gradient
2022-04-23 14:23:31 | [train_policy] epoch #736 | gradient computed
2022-04-23 14:23:31 | [train_policy] epoch #736 | computing descent direction
2022-04-23 14:23:31 | [train_policy] epoch #736 | descent direction computed
2022-04-23 14:23:31 | [train_policy] epoch #736 | backtrack iters: 1
2022-04-23 14:23:31 | [train_policy] epoch #736 | optimization finished
2022-04-23 14:23:31 | [train_policy] epoch #736 | Computing KL after
2022-04-23 14:23:31 | [train_policy] epoch #736 | Computing loss after
2022-04-23 14:23:31 | [train_policy] epoch #736 | Fitting baseline...
2022-04-23 14:23:31 | [train_policy] epoch #736 | Saving snapshot...
2022-04-23 14:23:31 | [train_policy] epoch #736 | Saved
2022-04-23 14:23:31 | [train_policy] epoch #736 | Time 259.63 s
2022-04-23 14:23:31 | [train_policy] epoch #736 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11961
Evaluation/AverageDiscountedReturn          -41.9684
Evaluation/AverageReturn                    -41.9684
Evaluation/CompletionRate                     0
Evaluation/Iteration                        736
Evaluation/MaxReturn                        -29.0675
Evaluation/MinReturn                        -72.9882
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.22402
Extras/EpisodeRewardMean                    -41.5567
LinearFeatureBaseline/ExplainedVariance       0.866029
PolicyExecTime                                0.102457
ProcessExecTime                               0.0118506
TotalEnvSteps                            745844
policy/Entropy                               -1.78893
policy/KL                                     0.00671862
policy/KLBefore                               0
policy/LossAfter                             -0.020226
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.167139
policy/dLoss                                  0.020226
---------------------------------------  ----------------
2022-04-23 14:23:31 | [train_policy] epoch #737 | Obtaining samples for iteration 737...
2022-04-23 14:23:32 | [train_policy] epoch #737 | Logging diagnostics...
2022-04-23 14:23:32 | [train_policy] epoch #737 | Optimizing policy...
2022-04-23 14:23:32 | [train_policy] epoch #737 | Computing loss before
2022-04-23 14:23:32 | [train_policy] epoch #737 | Computing KL before
2022-04-23 14:23:32 | [train_policy] epoch #737 | Optimizing
2022-04-23 14:23:32 | [train_policy] epoch #737 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:32 | [train_policy] epoch #737 | computing loss before
2022-04-23 14:23:32 | [train_policy] epoch #737 | computing gradient
2022-04-23 14:23:32 | [train_policy] epoch #737 | gradient computed
2022-04-23 14:23:32 | [train_policy] epoch #737 | computing descent direction
2022-04-23 14:23:32 | [train_policy] epoch #737 | descent direction computed
2022-04-23 14:23:32 | [train_policy] epoch #737 | backtrack iters: 0
2022-04-23 14:23:32 | [train_policy] epoch #737 | optimization finished
2022-04-23 14:23:32 | [train_policy] epoch #737 | Computing KL after
2022-04-23 14:23:32 | [train_policy] epoch #737 | Computing loss after
2022-04-23 14:23:32 | [train_policy] epoch #737 | Fitting baseline...
2022-04-23 14:23:32 | [train_policy] epoch #737 | Saving snapshot...
2022-04-23 14:23:32 | [train_policy] epoch #737 | Saved
2022-04-23 14:23:32 | [train_policy] epoch #737 | Time 259.98 s
2022-04-23 14:23:32 | [train_policy] epoch #737 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.124689
Evaluation/AverageDiscountedReturn          -39.7225
Evaluation/AverageReturn                    -39.7225
Evaluation/CompletionRate                     0
Evaluation/Iteration                        737
Evaluation/MaxReturn                        -28.737
Evaluation/MinReturn                        -72.4262
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.6204
Extras/EpisodeRewardMean                    -40.1835
LinearFeatureBaseline/ExplainedVariance       0.883947
PolicyExecTime                                0.113111
ProcessExecTime                               0.0123739
TotalEnvSteps                            746856
policy/Entropy                               -1.81799
policy/KL                                     0.00982769
policy/KLBefore                               0
policy/LossAfter                             -0.0146165
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.162351
policy/dLoss                                  0.0146165
---------------------------------------  ----------------
2022-04-23 14:23:32 | [train_policy] epoch #738 | Obtaining samples for iteration 738...
2022-04-23 14:23:32 | [train_policy] epoch #738 | Logging diagnostics...
2022-04-23 14:23:32 | [train_policy] epoch #738 | Optimizing policy...
2022-04-23 14:23:32 | [train_policy] epoch #738 | Computing loss before
2022-04-23 14:23:32 | [train_policy] epoch #738 | Computing KL before
2022-04-23 14:23:32 | [train_policy] epoch #738 | Optimizing
2022-04-23 14:23:32 | [train_policy] epoch #738 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:32 | [train_policy] epoch #738 | computing loss before
2022-04-23 14:23:32 | [train_policy] epoch #738 | computing gradient
2022-04-23 14:23:32 | [train_policy] epoch #738 | gradient computed
2022-04-23 14:23:32 | [train_policy] epoch #738 | computing descent direction
2022-04-23 14:23:32 | [train_policy] epoch #738 | descent direction computed
2022-04-23 14:23:32 | [train_policy] epoch #738 | backtrack iters: 1
2022-04-23 14:23:32 | [train_policy] epoch #738 | optimization finished
2022-04-23 14:23:32 | [train_policy] epoch #738 | Computing KL after
2022-04-23 14:23:32 | [train_policy] epoch #738 | Computing loss after
2022-04-23 14:23:32 | [train_policy] epoch #738 | Fitting baseline...
2022-04-23 14:23:32 | [train_policy] epoch #738 | Saving snapshot...
2022-04-23 14:23:32 | [train_policy] epoch #738 | Saved
2022-04-23 14:23:32 | [train_policy] epoch #738 | Time 260.32 s
2022-04-23 14:23:32 | [train_policy] epoch #738 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.115564
Evaluation/AverageDiscountedReturn          -40.7086
Evaluation/AverageReturn                    -40.7086
Evaluation/CompletionRate                     0
Evaluation/Iteration                        738
Evaluation/MaxReturn                        -28.9015
Evaluation/MinReturn                        -63.8164
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.18759
Extras/EpisodeRewardMean                    -41.1403
LinearFeatureBaseline/ExplainedVariance       0.888023
PolicyExecTime                                0.106894
ProcessExecTime                               0.0115237
TotalEnvSteps                            747868
policy/Entropy                               -1.87732
policy/KL                                     0.00672636
policy/KLBefore                               0
policy/LossAfter                             -0.0103114
policy/LossBefore                            -0
policy/Perplexity                             0.153
policy/dLoss                                  0.0103114
---------------------------------------  ---------------
2022-04-23 14:23:32 | [train_policy] epoch #739 | Obtaining samples for iteration 739...
2022-04-23 14:23:32 | [train_policy] epoch #739 | Logging diagnostics...
2022-04-23 14:23:32 | [train_policy] epoch #739 | Optimizing policy...
2022-04-23 14:23:32 | [train_policy] epoch #739 | Computing loss before
2022-04-23 14:23:32 | [train_policy] epoch #739 | Computing KL before
2022-04-23 14:23:32 | [train_policy] epoch #739 | Optimizing
2022-04-23 14:23:32 | [train_policy] epoch #739 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:32 | [train_policy] epoch #739 | computing loss before
2022-04-23 14:23:32 | [train_policy] epoch #739 | computing gradient
2022-04-23 14:23:32 | [train_policy] epoch #739 | gradient computed
2022-04-23 14:23:32 | [train_policy] epoch #739 | computing descent direction
2022-04-23 14:23:32 | [train_policy] epoch #739 | descent direction computed
2022-04-23 14:23:32 | [train_policy] epoch #739 | backtrack iters: 0
2022-04-23 14:23:32 | [train_policy] epoch #739 | optimization finished
2022-04-23 14:23:32 | [train_policy] epoch #739 | Computing KL after
2022-04-23 14:23:32 | [train_policy] epoch #739 | Computing loss after
2022-04-23 14:23:32 | [train_policy] epoch #739 | Fitting baseline...
2022-04-23 14:23:32 | [train_policy] epoch #739 | Saving snapshot...
2022-04-23 14:23:32 | [train_policy] epoch #739 | Saved
2022-04-23 14:23:32 | [train_policy] epoch #739 | Time 260.65 s
2022-04-23 14:23:32 | [train_policy] epoch #739 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.118382
Evaluation/AverageDiscountedReturn          -40.6726
Evaluation/AverageReturn                    -40.6726
Evaluation/CompletionRate                     0
Evaluation/Iteration                        739
Evaluation/MaxReturn                        -29.3589
Evaluation/MinReturn                        -72.5106
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.77332
Extras/EpisodeRewardMean                    -40.6578
LinearFeatureBaseline/ExplainedVariance       0.877257
PolicyExecTime                                0.094322
ProcessExecTime                               0.0114622
TotalEnvSteps                            748880
policy/Entropy                               -1.83047
policy/KL                                     0.00933626
policy/KLBefore                               0
policy/LossAfter                             -0.0163157
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.160339
policy/dLoss                                  0.0163157
---------------------------------------  ---------------
2022-04-23 14:23:32 | [train_policy] epoch #740 | Obtaining samples for iteration 740...
2022-04-23 14:23:33 | [train_policy] epoch #740 | Logging diagnostics...
2022-04-23 14:23:33 | [train_policy] epoch #740 | Optimizing policy...
2022-04-23 14:23:33 | [train_policy] epoch #740 | Computing loss before
2022-04-23 14:23:33 | [train_policy] epoch #740 | Computing KL before
2022-04-23 14:23:33 | [train_policy] epoch #740 | Optimizing
2022-04-23 14:23:33 | [train_policy] epoch #740 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:33 | [train_policy] epoch #740 | computing loss before
2022-04-23 14:23:33 | [train_policy] epoch #740 | computing gradient
2022-04-23 14:23:33 | [train_policy] epoch #740 | gradient computed
2022-04-23 14:23:33 | [train_policy] epoch #740 | computing descent direction
2022-04-23 14:23:33 | [train_policy] epoch #740 | descent direction computed
2022-04-23 14:23:33 | [train_policy] epoch #740 | backtrack iters: 1
2022-04-23 14:23:33 | [train_policy] epoch #740 | optimization finished
2022-04-23 14:23:33 | [train_policy] epoch #740 | Computing KL after
2022-04-23 14:23:33 | [train_policy] epoch #740 | Computing loss after
2022-04-23 14:23:33 | [train_policy] epoch #740 | Fitting baseline...
2022-04-23 14:23:33 | [train_policy] epoch #740 | Saving snapshot...
2022-04-23 14:23:33 | [train_policy] epoch #740 | Saved
2022-04-23 14:23:33 | [train_policy] epoch #740 | Time 260.98 s
2022-04-23 14:23:33 | [train_policy] epoch #740 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11554
Evaluation/AverageDiscountedReturn          -40.1682
Evaluation/AverageReturn                    -40.1682
Evaluation/CompletionRate                     0
Evaluation/Iteration                        740
Evaluation/MaxReturn                        -28.1491
Evaluation/MinReturn                        -64.3154
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.51148
Extras/EpisodeRewardMean                    -40.2651
LinearFeatureBaseline/ExplainedVariance       0.900471
PolicyExecTime                                0.106138
ProcessExecTime                               0.0112789
TotalEnvSteps                            749892
policy/Entropy                               -1.84142
policy/KL                                     0.00668869
policy/KLBefore                               0
policy/LossAfter                             -0.016305
policy/LossBefore                             1.08961e-08
policy/Perplexity                             0.158593
policy/dLoss                                  0.016305
---------------------------------------  ----------------
2022-04-23 14:23:33 | [train_policy] epoch #741 | Obtaining samples for iteration 741...
2022-04-23 14:23:33 | [train_policy] epoch #741 | Logging diagnostics...
2022-04-23 14:23:33 | [train_policy] epoch #741 | Optimizing policy...
2022-04-23 14:23:33 | [train_policy] epoch #741 | Computing loss before
2022-04-23 14:23:33 | [train_policy] epoch #741 | Computing KL before
2022-04-23 14:23:33 | [train_policy] epoch #741 | Optimizing
2022-04-23 14:23:33 | [train_policy] epoch #741 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:33 | [train_policy] epoch #741 | computing loss before
2022-04-23 14:23:33 | [train_policy] epoch #741 | computing gradient
2022-04-23 14:23:33 | [train_policy] epoch #741 | gradient computed
2022-04-23 14:23:33 | [train_policy] epoch #741 | computing descent direction
2022-04-23 14:23:33 | [train_policy] epoch #741 | descent direction computed
2022-04-23 14:23:33 | [train_policy] epoch #741 | backtrack iters: 1
2022-04-23 14:23:33 | [train_policy] epoch #741 | optimization finished
2022-04-23 14:23:33 | [train_policy] epoch #741 | Computing KL after
2022-04-23 14:23:33 | [train_policy] epoch #741 | Computing loss after
2022-04-23 14:23:33 | [train_policy] epoch #741 | Fitting baseline...
2022-04-23 14:23:33 | [train_policy] epoch #741 | Saving snapshot...
2022-04-23 14:23:33 | [train_policy] epoch #741 | Saved
2022-04-23 14:23:33 | [train_policy] epoch #741 | Time 261.32 s
2022-04-23 14:23:33 | [train_policy] epoch #741 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119555
Evaluation/AverageDiscountedReturn          -40.7963
Evaluation/AverageReturn                    -40.7963
Evaluation/CompletionRate                     0
Evaluation/Iteration                        741
Evaluation/MaxReturn                        -28.9403
Evaluation/MinReturn                        -72.2187
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.38601
Extras/EpisodeRewardMean                    -40.7193
LinearFeatureBaseline/ExplainedVariance       0.866036
PolicyExecTime                                0.099561
ProcessExecTime                               0.0113893
TotalEnvSteps                            750904
policy/Entropy                               -1.91805
policy/KL                                     0.00668529
policy/KLBefore                               0
policy/LossAfter                             -0.00991906
policy/LossBefore                            -8.48129e-09
policy/Perplexity                             0.146893
policy/dLoss                                  0.00991905
---------------------------------------  ----------------
2022-04-23 14:23:33 | [train_policy] epoch #742 | Obtaining samples for iteration 742...
2022-04-23 14:23:33 | [train_policy] epoch #742 | Logging diagnostics...
2022-04-23 14:23:33 | [train_policy] epoch #742 | Optimizing policy...
2022-04-23 14:23:33 | [train_policy] epoch #742 | Computing loss before
2022-04-23 14:23:33 | [train_policy] epoch #742 | Computing KL before
2022-04-23 14:23:33 | [train_policy] epoch #742 | Optimizing
2022-04-23 14:23:33 | [train_policy] epoch #742 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:33 | [train_policy] epoch #742 | computing loss before
2022-04-23 14:23:33 | [train_policy] epoch #742 | computing gradient
2022-04-23 14:23:33 | [train_policy] epoch #742 | gradient computed
2022-04-23 14:23:33 | [train_policy] epoch #742 | computing descent direction
2022-04-23 14:23:33 | [train_policy] epoch #742 | descent direction computed
2022-04-23 14:23:33 | [train_policy] epoch #742 | backtrack iters: 0
2022-04-23 14:23:33 | [train_policy] epoch #742 | optimization finished
2022-04-23 14:23:33 | [train_policy] epoch #742 | Computing KL after
2022-04-23 14:23:33 | [train_policy] epoch #742 | Computing loss after
2022-04-23 14:23:33 | [train_policy] epoch #742 | Fitting baseline...
2022-04-23 14:23:33 | [train_policy] epoch #742 | Saving snapshot...
2022-04-23 14:23:33 | [train_policy] epoch #742 | Saved
2022-04-23 14:23:33 | [train_policy] epoch #742 | Time 261.64 s
2022-04-23 14:23:33 | [train_policy] epoch #742 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119065
Evaluation/AverageDiscountedReturn          -41.8759
Evaluation/AverageReturn                    -41.8759
Evaluation/CompletionRate                     0
Evaluation/Iteration                        742
Evaluation/MaxReturn                        -28.4054
Evaluation/MinReturn                        -73.1448
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.246
Extras/EpisodeRewardMean                    -41.5814
LinearFeatureBaseline/ExplainedVariance       0.87389
PolicyExecTime                                0.0924933
ProcessExecTime                               0.0110872
TotalEnvSteps                            751916
policy/Entropy                               -1.87484
policy/KL                                     0.00950121
policy/KLBefore                               0
policy/LossAfter                             -0.0198169
policy/LossBefore                             9.65925e-09
policy/Perplexity                             0.15338
policy/dLoss                                  0.0198169
---------------------------------------  ----------------
2022-04-23 14:23:33 | [train_policy] epoch #743 | Obtaining samples for iteration 743...
2022-04-23 14:23:34 | [train_policy] epoch #743 | Logging diagnostics...
2022-04-23 14:23:34 | [train_policy] epoch #743 | Optimizing policy...
2022-04-23 14:23:34 | [train_policy] epoch #743 | Computing loss before
2022-04-23 14:23:34 | [train_policy] epoch #743 | Computing KL before
2022-04-23 14:23:34 | [train_policy] epoch #743 | Optimizing
2022-04-23 14:23:34 | [train_policy] epoch #743 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:34 | [train_policy] epoch #743 | computing loss before
2022-04-23 14:23:34 | [train_policy] epoch #743 | computing gradient
2022-04-23 14:23:34 | [train_policy] epoch #743 | gradient computed
2022-04-23 14:23:34 | [train_policy] epoch #743 | computing descent direction
2022-04-23 14:23:34 | [train_policy] epoch #743 | descent direction computed
2022-04-23 14:23:34 | [train_policy] epoch #743 | backtrack iters: 1
2022-04-23 14:23:34 | [train_policy] epoch #743 | optimization finished
2022-04-23 14:23:34 | [train_policy] epoch #743 | Computing KL after
2022-04-23 14:23:34 | [train_policy] epoch #743 | Computing loss after
2022-04-23 14:23:34 | [train_policy] epoch #743 | Fitting baseline...
2022-04-23 14:23:34 | [train_policy] epoch #743 | Saving snapshot...
2022-04-23 14:23:34 | [train_policy] epoch #743 | Saved
2022-04-23 14:23:34 | [train_policy] epoch #743 | Time 261.97 s
2022-04-23 14:23:34 | [train_policy] epoch #743 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118407
Evaluation/AverageDiscountedReturn          -40.5415
Evaluation/AverageReturn                    -40.5415
Evaluation/CompletionRate                     0
Evaluation/Iteration                        743
Evaluation/MaxReturn                        -28.8318
Evaluation/MinReturn                        -72.1597
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.65468
Extras/EpisodeRewardMean                    -40.5193
LinearFeatureBaseline/ExplainedVariance       0.905288
PolicyExecTime                                0.0967824
ProcessExecTime                               0.0112104
TotalEnvSteps                            752928
policy/Entropy                               -1.87846
policy/KL                                     0.00665177
policy/KLBefore                               0
policy/LossAfter                             -0.0163283
policy/LossBefore                            -7.98066e-09
policy/Perplexity                             0.152826
policy/dLoss                                  0.0163283
---------------------------------------  ----------------
2022-04-23 14:23:34 | [train_policy] epoch #744 | Obtaining samples for iteration 744...
2022-04-23 14:23:34 | [train_policy] epoch #744 | Logging diagnostics...
2022-04-23 14:23:34 | [train_policy] epoch #744 | Optimizing policy...
2022-04-23 14:23:34 | [train_policy] epoch #744 | Computing loss before
2022-04-23 14:23:34 | [train_policy] epoch #744 | Computing KL before
2022-04-23 14:23:34 | [train_policy] epoch #744 | Optimizing
2022-04-23 14:23:34 | [train_policy] epoch #744 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:34 | [train_policy] epoch #744 | computing loss before
2022-04-23 14:23:34 | [train_policy] epoch #744 | computing gradient
2022-04-23 14:23:34 | [train_policy] epoch #744 | gradient computed
2022-04-23 14:23:34 | [train_policy] epoch #744 | computing descent direction
2022-04-23 14:23:34 | [train_policy] epoch #744 | descent direction computed
2022-04-23 14:23:34 | [train_policy] epoch #744 | backtrack iters: 1
2022-04-23 14:23:34 | [train_policy] epoch #744 | optimization finished
2022-04-23 14:23:34 | [train_policy] epoch #744 | Computing KL after
2022-04-23 14:23:34 | [train_policy] epoch #744 | Computing loss after
2022-04-23 14:23:34 | [train_policy] epoch #744 | Fitting baseline...
2022-04-23 14:23:34 | [train_policy] epoch #744 | Saving snapshot...
2022-04-23 14:23:34 | [train_policy] epoch #744 | Saved
2022-04-23 14:23:34 | [train_policy] epoch #744 | Time 262.30 s
2022-04-23 14:23:34 | [train_policy] epoch #744 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.118798
Evaluation/AverageDiscountedReturn          -40.9991
Evaluation/AverageReturn                    -40.9991
Evaluation/CompletionRate                     0
Evaluation/Iteration                        744
Evaluation/MaxReturn                        -29.175
Evaluation/MinReturn                        -63.6961
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.80208
Extras/EpisodeRewardMean                    -41.0414
LinearFeatureBaseline/ExplainedVariance       0.90453
PolicyExecTime                                0.0950558
ProcessExecTime                               0.0112946
TotalEnvSteps                            753940
policy/Entropy                               -1.91714
policy/KL                                     0.00667054
policy/KLBefore                               0
policy/LossAfter                             -0.0122729
policy/LossBefore                            -5.4186e-09
policy/Perplexity                             0.147028
policy/dLoss                                  0.0122729
---------------------------------------  ---------------
2022-04-23 14:23:34 | [train_policy] epoch #745 | Obtaining samples for iteration 745...
2022-04-23 14:23:34 | [train_policy] epoch #745 | Logging diagnostics...
2022-04-23 14:23:34 | [train_policy] epoch #745 | Optimizing policy...
2022-04-23 14:23:34 | [train_policy] epoch #745 | Computing loss before
2022-04-23 14:23:34 | [train_policy] epoch #745 | Computing KL before
2022-04-23 14:23:34 | [train_policy] epoch #745 | Optimizing
2022-04-23 14:23:34 | [train_policy] epoch #745 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:34 | [train_policy] epoch #745 | computing loss before
2022-04-23 14:23:34 | [train_policy] epoch #745 | computing gradient
2022-04-23 14:23:34 | [train_policy] epoch #745 | gradient computed
2022-04-23 14:23:34 | [train_policy] epoch #745 | computing descent direction
2022-04-23 14:23:34 | [train_policy] epoch #745 | descent direction computed
2022-04-23 14:23:34 | [train_policy] epoch #745 | backtrack iters: 0
2022-04-23 14:23:34 | [train_policy] epoch #745 | optimization finished
2022-04-23 14:23:34 | [train_policy] epoch #745 | Computing KL after
2022-04-23 14:23:34 | [train_policy] epoch #745 | Computing loss after
2022-04-23 14:23:34 | [train_policy] epoch #745 | Fitting baseline...
2022-04-23 14:23:34 | [train_policy] epoch #745 | Saving snapshot...
2022-04-23 14:23:34 | [train_policy] epoch #745 | Saved
2022-04-23 14:23:34 | [train_policy] epoch #745 | Time 262.63 s
2022-04-23 14:23:34 | [train_policy] epoch #745 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118834
Evaluation/AverageDiscountedReturn          -39.1494
Evaluation/AverageReturn                    -39.1494
Evaluation/CompletionRate                     0
Evaluation/Iteration                        745
Evaluation/MaxReturn                        -28.6422
Evaluation/MinReturn                        -72.675
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.50223
Extras/EpisodeRewardMean                    -39.0383
LinearFeatureBaseline/ExplainedVariance       0.890266
PolicyExecTime                                0.0971301
ProcessExecTime                               0.01144
TotalEnvSteps                            754952
policy/Entropy                               -1.87645
policy/KL                                     0.00938852
policy/KLBefore                               0
policy/LossAfter                             -0.0160404
policy/LossBefore                            -1.66092e-08
policy/Perplexity                             0.153133
policy/dLoss                                  0.0160404
---------------------------------------  ----------------
2022-04-23 14:23:34 | [train_policy] epoch #746 | Obtaining samples for iteration 746...
2022-04-23 14:23:35 | [train_policy] epoch #746 | Logging diagnostics...
2022-04-23 14:23:35 | [train_policy] epoch #746 | Optimizing policy...
2022-04-23 14:23:35 | [train_policy] epoch #746 | Computing loss before
2022-04-23 14:23:35 | [train_policy] epoch #746 | Computing KL before
2022-04-23 14:23:35 | [train_policy] epoch #746 | Optimizing
2022-04-23 14:23:35 | [train_policy] epoch #746 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:35 | [train_policy] epoch #746 | computing loss before
2022-04-23 14:23:35 | [train_policy] epoch #746 | computing gradient
2022-04-23 14:23:35 | [train_policy] epoch #746 | gradient computed
2022-04-23 14:23:35 | [train_policy] epoch #746 | computing descent direction
2022-04-23 14:23:35 | [train_policy] epoch #746 | descent direction computed
2022-04-23 14:23:35 | [train_policy] epoch #746 | backtrack iters: 0
2022-04-23 14:23:35 | [train_policy] epoch #746 | optimization finished
2022-04-23 14:23:35 | [train_policy] epoch #746 | Computing KL after
2022-04-23 14:23:35 | [train_policy] epoch #746 | Computing loss after
2022-04-23 14:23:35 | [train_policy] epoch #746 | Fitting baseline...
2022-04-23 14:23:35 | [train_policy] epoch #746 | Saving snapshot...
2022-04-23 14:23:35 | [train_policy] epoch #746 | Saved
2022-04-23 14:23:35 | [train_policy] epoch #746 | Time 262.97 s
2022-04-23 14:23:35 | [train_policy] epoch #746 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.124748
Evaluation/AverageDiscountedReturn          -40.2133
Evaluation/AverageReturn                    -40.2133
Evaluation/CompletionRate                     0
Evaluation/Iteration                        746
Evaluation/MaxReturn                        -28.6851
Evaluation/MinReturn                        -72.2553
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.55828
Extras/EpisodeRewardMean                    -40.3099
LinearFeatureBaseline/ExplainedVariance       0.879086
PolicyExecTime                                0.100909
ProcessExecTime                               0.0119295
TotalEnvSteps                            755964
policy/Entropy                               -1.83669
policy/KL                                     0.00945364
policy/KLBefore                               0
policy/LossAfter                             -0.0195579
policy/LossBefore                             1.1544e-08
policy/Perplexity                             0.159344
policy/dLoss                                  0.0195579
---------------------------------------  ---------------
2022-04-23 14:23:35 | [train_policy] epoch #747 | Obtaining samples for iteration 747...
2022-04-23 14:23:35 | [train_policy] epoch #747 | Logging diagnostics...
2022-04-23 14:23:35 | [train_policy] epoch #747 | Optimizing policy...
2022-04-23 14:23:35 | [train_policy] epoch #747 | Computing loss before
2022-04-23 14:23:35 | [train_policy] epoch #747 | Computing KL before
2022-04-23 14:23:35 | [train_policy] epoch #747 | Optimizing
2022-04-23 14:23:35 | [train_policy] epoch #747 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:35 | [train_policy] epoch #747 | computing loss before
2022-04-23 14:23:35 | [train_policy] epoch #747 | computing gradient
2022-04-23 14:23:35 | [train_policy] epoch #747 | gradient computed
2022-04-23 14:23:35 | [train_policy] epoch #747 | computing descent direction
2022-04-23 14:23:35 | [train_policy] epoch #747 | descent direction computed
2022-04-23 14:23:35 | [train_policy] epoch #747 | backtrack iters: 0
2022-04-23 14:23:35 | [train_policy] epoch #747 | optimization finished
2022-04-23 14:23:35 | [train_policy] epoch #747 | Computing KL after
2022-04-23 14:23:35 | [train_policy] epoch #747 | Computing loss after
2022-04-23 14:23:35 | [train_policy] epoch #747 | Fitting baseline...
2022-04-23 14:23:35 | [train_policy] epoch #747 | Saving snapshot...
2022-04-23 14:23:35 | [train_policy] epoch #747 | Saved
2022-04-23 14:23:35 | [train_policy] epoch #747 | Time 263.31 s
2022-04-23 14:23:35 | [train_policy] epoch #747 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.1154
Evaluation/AverageDiscountedReturn          -42.0135
Evaluation/AverageReturn                    -42.0135
Evaluation/CompletionRate                     0
Evaluation/Iteration                        747
Evaluation/MaxReturn                        -30.0521
Evaluation/MinReturn                        -72.4942
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.48757
Extras/EpisodeRewardMean                    -42.169
LinearFeatureBaseline/ExplainedVariance       0.861974
PolicyExecTime                                0.105588
ProcessExecTime                               0.0111878
TotalEnvSteps                            756976
policy/Entropy                               -1.84534
policy/KL                                     0.00969303
policy/KLBefore                               0
policy/LossAfter                             -0.0209281
policy/LossBefore                            -7.53893e-09
policy/Perplexity                             0.157971
policy/dLoss                                  0.0209281
---------------------------------------  ----------------
2022-04-23 14:23:35 | [train_policy] epoch #748 | Obtaining samples for iteration 748...
2022-04-23 14:23:35 | [train_policy] epoch #748 | Logging diagnostics...
2022-04-23 14:23:35 | [train_policy] epoch #748 | Optimizing policy...
2022-04-23 14:23:35 | [train_policy] epoch #748 | Computing loss before
2022-04-23 14:23:35 | [train_policy] epoch #748 | Computing KL before
2022-04-23 14:23:35 | [train_policy] epoch #748 | Optimizing
2022-04-23 14:23:35 | [train_policy] epoch #748 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:35 | [train_policy] epoch #748 | computing loss before
2022-04-23 14:23:35 | [train_policy] epoch #748 | computing gradient
2022-04-23 14:23:35 | [train_policy] epoch #748 | gradient computed
2022-04-23 14:23:35 | [train_policy] epoch #748 | computing descent direction
2022-04-23 14:23:35 | [train_policy] epoch #748 | descent direction computed
2022-04-23 14:23:35 | [train_policy] epoch #748 | backtrack iters: 1
2022-04-23 14:23:35 | [train_policy] epoch #748 | optimization finished
2022-04-23 14:23:35 | [train_policy] epoch #748 | Computing KL after
2022-04-23 14:23:35 | [train_policy] epoch #748 | Computing loss after
2022-04-23 14:23:35 | [train_policy] epoch #748 | Fitting baseline...
2022-04-23 14:23:35 | [train_policy] epoch #748 | Saving snapshot...
2022-04-23 14:23:35 | [train_policy] epoch #748 | Saved
2022-04-23 14:23:35 | [train_policy] epoch #748 | Time 263.64 s
2022-04-23 14:23:35 | [train_policy] epoch #748 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115532
Evaluation/AverageDiscountedReturn          -42.5892
Evaluation/AverageReturn                    -42.5892
Evaluation/CompletionRate                     0
Evaluation/Iteration                        748
Evaluation/MaxReturn                        -28.8615
Evaluation/MinReturn                        -72.1062
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.73116
Extras/EpisodeRewardMean                    -42.9489
LinearFeatureBaseline/ExplainedVariance       0.872139
PolicyExecTime                                0.106472
ProcessExecTime                               0.011574
TotalEnvSteps                            757988
policy/Entropy                               -1.82583
policy/KL                                     0.00647031
policy/KLBefore                               0
policy/LossAfter                             -0.0170266
policy/LossBefore                            -1.17796e-09
policy/Perplexity                             0.161085
policy/dLoss                                  0.0170266
---------------------------------------  ----------------
2022-04-23 14:23:35 | [train_policy] epoch #749 | Obtaining samples for iteration 749...
2022-04-23 14:23:36 | [train_policy] epoch #749 | Logging diagnostics...
2022-04-23 14:23:36 | [train_policy] epoch #749 | Optimizing policy...
2022-04-23 14:23:36 | [train_policy] epoch #749 | Computing loss before
2022-04-23 14:23:36 | [train_policy] epoch #749 | Computing KL before
2022-04-23 14:23:36 | [train_policy] epoch #749 | Optimizing
2022-04-23 14:23:36 | [train_policy] epoch #749 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:36 | [train_policy] epoch #749 | computing loss before
2022-04-23 14:23:36 | [train_policy] epoch #749 | computing gradient
2022-04-23 14:23:36 | [train_policy] epoch #749 | gradient computed
2022-04-23 14:23:36 | [train_policy] epoch #749 | computing descent direction
2022-04-23 14:23:36 | [train_policy] epoch #749 | descent direction computed
2022-04-23 14:23:36 | [train_policy] epoch #749 | backtrack iters: 1
2022-04-23 14:23:36 | [train_policy] epoch #749 | optimization finished
2022-04-23 14:23:36 | [train_policy] epoch #749 | Computing KL after
2022-04-23 14:23:36 | [train_policy] epoch #749 | Computing loss after
2022-04-23 14:23:36 | [train_policy] epoch #749 | Fitting baseline...
2022-04-23 14:23:36 | [train_policy] epoch #749 | Saving snapshot...
2022-04-23 14:23:36 | [train_policy] epoch #749 | Saved
2022-04-23 14:23:36 | [train_policy] epoch #749 | Time 263.98 s
2022-04-23 14:23:36 | [train_policy] epoch #749 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116046
Evaluation/AverageDiscountedReturn          -40.9766
Evaluation/AverageReturn                    -40.9766
Evaluation/CompletionRate                     0
Evaluation/Iteration                        749
Evaluation/MaxReturn                        -28.892
Evaluation/MinReturn                        -64.0985
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.32024
Extras/EpisodeRewardMean                    -40.8298
LinearFeatureBaseline/ExplainedVariance       0.870607
PolicyExecTime                                0.101415
ProcessExecTime                               0.011616
TotalEnvSteps                            759000
policy/Entropy                               -1.82189
policy/KL                                     0.00679608
policy/KLBefore                               0
policy/LossAfter                             -0.017461
policy/LossBefore                            -1.04249e-08
policy/Perplexity                             0.161719
policy/dLoss                                  0.017461
---------------------------------------  ----------------
2022-04-23 14:23:36 | [train_policy] epoch #750 | Obtaining samples for iteration 750...
2022-04-23 14:23:36 | [train_policy] epoch #750 | Logging diagnostics...
2022-04-23 14:23:36 | [train_policy] epoch #750 | Optimizing policy...
2022-04-23 14:23:36 | [train_policy] epoch #750 | Computing loss before
2022-04-23 14:23:36 | [train_policy] epoch #750 | Computing KL before
2022-04-23 14:23:36 | [train_policy] epoch #750 | Optimizing
2022-04-23 14:23:36 | [train_policy] epoch #750 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:36 | [train_policy] epoch #750 | computing loss before
2022-04-23 14:23:36 | [train_policy] epoch #750 | computing gradient
2022-04-23 14:23:36 | [train_policy] epoch #750 | gradient computed
2022-04-23 14:23:36 | [train_policy] epoch #750 | computing descent direction
2022-04-23 14:23:36 | [train_policy] epoch #750 | descent direction computed
2022-04-23 14:23:36 | [train_policy] epoch #750 | backtrack iters: 0
2022-04-23 14:23:36 | [train_policy] epoch #750 | optimization finished
2022-04-23 14:23:36 | [train_policy] epoch #750 | Computing KL after
2022-04-23 14:23:36 | [train_policy] epoch #750 | Computing loss after
2022-04-23 14:23:36 | [train_policy] epoch #750 | Fitting baseline...
2022-04-23 14:23:36 | [train_policy] epoch #750 | Saving snapshot...
2022-04-23 14:23:36 | [train_policy] epoch #750 | Saved
2022-04-23 14:23:36 | [train_policy] epoch #750 | Time 264.30 s
2022-04-23 14:23:36 | [train_policy] epoch #750 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119525
Evaluation/AverageDiscountedReturn          -40.8526
Evaluation/AverageReturn                    -40.8526
Evaluation/CompletionRate                     0
Evaluation/Iteration                        750
Evaluation/MaxReturn                        -28.7335
Evaluation/MinReturn                        -64.5877
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.4654
Extras/EpisodeRewardMean                    -40.8052
LinearFeatureBaseline/ExplainedVariance       0.88593
PolicyExecTime                                0.0955818
ProcessExecTime                               0.0111711
TotalEnvSteps                            760012
policy/Entropy                               -1.81322
policy/KL                                     0.00980428
policy/KLBefore                               0
policy/LossAfter                             -0.014095
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.163128
policy/dLoss                                  0.014095
---------------------------------------  ----------------
2022-04-23 14:23:36 | [train_policy] epoch #751 | Obtaining samples for iteration 751...
2022-04-23 14:23:36 | [train_policy] epoch #751 | Logging diagnostics...
2022-04-23 14:23:36 | [train_policy] epoch #751 | Optimizing policy...
2022-04-23 14:23:36 | [train_policy] epoch #751 | Computing loss before
2022-04-23 14:23:36 | [train_policy] epoch #751 | Computing KL before
2022-04-23 14:23:36 | [train_policy] epoch #751 | Optimizing
2022-04-23 14:23:36 | [train_policy] epoch #751 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:36 | [train_policy] epoch #751 | computing loss before
2022-04-23 14:23:36 | [train_policy] epoch #751 | computing gradient
2022-04-23 14:23:36 | [train_policy] epoch #751 | gradient computed
2022-04-23 14:23:36 | [train_policy] epoch #751 | computing descent direction
2022-04-23 14:23:36 | [train_policy] epoch #751 | descent direction computed
2022-04-23 14:23:36 | [train_policy] epoch #751 | backtrack iters: 1
2022-04-23 14:23:36 | [train_policy] epoch #751 | optimization finished
2022-04-23 14:23:36 | [train_policy] epoch #751 | Computing KL after
2022-04-23 14:23:36 | [train_policy] epoch #751 | Computing loss after
2022-04-23 14:23:36 | [train_policy] epoch #751 | Fitting baseline...
2022-04-23 14:23:36 | [train_policy] epoch #751 | Saving snapshot...
2022-04-23 14:23:36 | [train_policy] epoch #751 | Saved
2022-04-23 14:23:36 | [train_policy] epoch #751 | Time 264.63 s
2022-04-23 14:23:36 | [train_policy] epoch #751 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118907
Evaluation/AverageDiscountedReturn          -40.1089
Evaluation/AverageReturn                    -40.1089
Evaluation/CompletionRate                     0
Evaluation/Iteration                        751
Evaluation/MaxReturn                        -27.7492
Evaluation/MinReturn                        -71.9907
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.87179
Extras/EpisodeRewardMean                    -40.2789
LinearFeatureBaseline/ExplainedVariance       0.893753
PolicyExecTime                                0.0975671
ProcessExecTime                               0.0113161
TotalEnvSteps                            761024
policy/Entropy                               -1.80201
policy/KL                                     0.00643226
policy/KLBefore                               0
policy/LossAfter                             -0.0166883
policy/LossBefore                             1.49601e-08
policy/Perplexity                             0.164967
policy/dLoss                                  0.0166883
---------------------------------------  ----------------
2022-04-23 14:23:36 | [train_policy] epoch #752 | Obtaining samples for iteration 752...
2022-04-23 14:23:37 | [train_policy] epoch #752 | Logging diagnostics...
2022-04-23 14:23:37 | [train_policy] epoch #752 | Optimizing policy...
2022-04-23 14:23:37 | [train_policy] epoch #752 | Computing loss before
2022-04-23 14:23:37 | [train_policy] epoch #752 | Computing KL before
2022-04-23 14:23:37 | [train_policy] epoch #752 | Optimizing
2022-04-23 14:23:37 | [train_policy] epoch #752 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:37 | [train_policy] epoch #752 | computing loss before
2022-04-23 14:23:37 | [train_policy] epoch #752 | computing gradient
2022-04-23 14:23:37 | [train_policy] epoch #752 | gradient computed
2022-04-23 14:23:37 | [train_policy] epoch #752 | computing descent direction
2022-04-23 14:23:37 | [train_policy] epoch #752 | descent direction computed
2022-04-23 14:23:37 | [train_policy] epoch #752 | backtrack iters: 1
2022-04-23 14:23:37 | [train_policy] epoch #752 | optimization finished
2022-04-23 14:23:37 | [train_policy] epoch #752 | Computing KL after
2022-04-23 14:23:37 | [train_policy] epoch #752 | Computing loss after
2022-04-23 14:23:37 | [train_policy] epoch #752 | Fitting baseline...
2022-04-23 14:23:37 | [train_policy] epoch #752 | Saving snapshot...
2022-04-23 14:23:37 | [train_policy] epoch #752 | Saved
2022-04-23 14:23:37 | [train_policy] epoch #752 | Time 264.96 s
2022-04-23 14:23:37 | [train_policy] epoch #752 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119026
Evaluation/AverageDiscountedReturn          -41.6075
Evaluation/AverageReturn                    -41.6075
Evaluation/CompletionRate                     0
Evaluation/Iteration                        752
Evaluation/MaxReturn                        -28.0301
Evaluation/MinReturn                        -72.0933
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.08468
Extras/EpisodeRewardMean                    -41.5249
LinearFeatureBaseline/ExplainedVariance       0.872608
PolicyExecTime                                0.0969582
ProcessExecTime                               0.0115278
TotalEnvSteps                            762036
policy/Entropy                               -1.80419
policy/KL                                     0.00641268
policy/KLBefore                               0
policy/LossAfter                             -0.020924
policy/LossBefore                             7.77452e-09
policy/Perplexity                             0.164608
policy/dLoss                                  0.020924
---------------------------------------  ----------------
2022-04-23 14:23:37 | [train_policy] epoch #753 | Obtaining samples for iteration 753...
2022-04-23 14:23:37 | [train_policy] epoch #753 | Logging diagnostics...
2022-04-23 14:23:37 | [train_policy] epoch #753 | Optimizing policy...
2022-04-23 14:23:37 | [train_policy] epoch #753 | Computing loss before
2022-04-23 14:23:37 | [train_policy] epoch #753 | Computing KL before
2022-04-23 14:23:37 | [train_policy] epoch #753 | Optimizing
2022-04-23 14:23:37 | [train_policy] epoch #753 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:37 | [train_policy] epoch #753 | computing loss before
2022-04-23 14:23:37 | [train_policy] epoch #753 | computing gradient
2022-04-23 14:23:37 | [train_policy] epoch #753 | gradient computed
2022-04-23 14:23:37 | [train_policy] epoch #753 | computing descent direction
2022-04-23 14:23:37 | [train_policy] epoch #753 | descent direction computed
2022-04-23 14:23:37 | [train_policy] epoch #753 | backtrack iters: 0
2022-04-23 14:23:37 | [train_policy] epoch #753 | optimization finished
2022-04-23 14:23:37 | [train_policy] epoch #753 | Computing KL after
2022-04-23 14:23:37 | [train_policy] epoch #753 | Computing loss after
2022-04-23 14:23:37 | [train_policy] epoch #753 | Fitting baseline...
2022-04-23 14:23:37 | [train_policy] epoch #753 | Saving snapshot...
2022-04-23 14:23:37 | [train_policy] epoch #753 | Saved
2022-04-23 14:23:37 | [train_policy] epoch #753 | Time 265.29 s
2022-04-23 14:23:37 | [train_policy] epoch #753 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119159
Evaluation/AverageDiscountedReturn          -40.6491
Evaluation/AverageReturn                    -40.6491
Evaluation/CompletionRate                     0
Evaluation/Iteration                        753
Evaluation/MaxReturn                        -29.2764
Evaluation/MinReturn                        -63.8145
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.98585
Extras/EpisodeRewardMean                    -40.5276
LinearFeatureBaseline/ExplainedVariance       0.898101
PolicyExecTime                                0.0950758
ProcessExecTime                               0.0113881
TotalEnvSteps                            763048
policy/Entropy                               -1.81024
policy/KL                                     0.0099572
policy/KLBefore                               0
policy/LossAfter                             -0.0201826
policy/LossBefore                             3.29828e-09
policy/Perplexity                             0.163614
policy/dLoss                                  0.0201826
---------------------------------------  ----------------
2022-04-23 14:23:37 | [train_policy] epoch #754 | Obtaining samples for iteration 754...
2022-04-23 14:23:37 | [train_policy] epoch #754 | Logging diagnostics...
2022-04-23 14:23:37 | [train_policy] epoch #754 | Optimizing policy...
2022-04-23 14:23:37 | [train_policy] epoch #754 | Computing loss before
2022-04-23 14:23:37 | [train_policy] epoch #754 | Computing KL before
2022-04-23 14:23:37 | [train_policy] epoch #754 | Optimizing
2022-04-23 14:23:37 | [train_policy] epoch #754 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:37 | [train_policy] epoch #754 | computing loss before
2022-04-23 14:23:37 | [train_policy] epoch #754 | computing gradient
2022-04-23 14:23:37 | [train_policy] epoch #754 | gradient computed
2022-04-23 14:23:37 | [train_policy] epoch #754 | computing descent direction
2022-04-23 14:23:37 | [train_policy] epoch #754 | descent direction computed
2022-04-23 14:23:37 | [train_policy] epoch #754 | backtrack iters: 1
2022-04-23 14:23:37 | [train_policy] epoch #754 | optimization finished
2022-04-23 14:23:37 | [train_policy] epoch #754 | Computing KL after
2022-04-23 14:23:37 | [train_policy] epoch #754 | Computing loss after
2022-04-23 14:23:37 | [train_policy] epoch #754 | Fitting baseline...
2022-04-23 14:23:37 | [train_policy] epoch #754 | Saving snapshot...
2022-04-23 14:23:37 | [train_policy] epoch #754 | Saved
2022-04-23 14:23:37 | [train_policy] epoch #754 | Time 265.63 s
2022-04-23 14:23:37 | [train_policy] epoch #754 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119351
Evaluation/AverageDiscountedReturn          -41.0415
Evaluation/AverageReturn                    -41.0415
Evaluation/CompletionRate                     0
Evaluation/Iteration                        754
Evaluation/MaxReturn                        -28.4445
Evaluation/MinReturn                        -72.0018
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.69553
Extras/EpisodeRewardMean                    -41.0827
LinearFeatureBaseline/ExplainedVariance       0.878123
PolicyExecTime                                0.101077
ProcessExecTime                               0.0114784
TotalEnvSteps                            764060
policy/Entropy                               -1.81742
policy/KL                                     0.00654284
policy/KLBefore                               0
policy/LossAfter                             -0.0119324
policy/LossBefore                            -3.06269e-09
policy/Perplexity                             0.162444
policy/dLoss                                  0.0119324
---------------------------------------  ----------------
2022-04-23 14:23:37 | [train_policy] epoch #755 | Obtaining samples for iteration 755...
2022-04-23 14:23:38 | [train_policy] epoch #755 | Logging diagnostics...
2022-04-23 14:23:38 | [train_policy] epoch #755 | Optimizing policy...
2022-04-23 14:23:38 | [train_policy] epoch #755 | Computing loss before
2022-04-23 14:23:38 | [train_policy] epoch #755 | Computing KL before
2022-04-23 14:23:38 | [train_policy] epoch #755 | Optimizing
2022-04-23 14:23:38 | [train_policy] epoch #755 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:38 | [train_policy] epoch #755 | computing loss before
2022-04-23 14:23:38 | [train_policy] epoch #755 | computing gradient
2022-04-23 14:23:38 | [train_policy] epoch #755 | gradient computed
2022-04-23 14:23:38 | [train_policy] epoch #755 | computing descent direction
2022-04-23 14:23:38 | [train_policy] epoch #755 | descent direction computed
2022-04-23 14:23:38 | [train_policy] epoch #755 | backtrack iters: 0
2022-04-23 14:23:38 | [train_policy] epoch #755 | optimization finished
2022-04-23 14:23:38 | [train_policy] epoch #755 | Computing KL after
2022-04-23 14:23:38 | [train_policy] epoch #755 | Computing loss after
2022-04-23 14:23:38 | [train_policy] epoch #755 | Fitting baseline...
2022-04-23 14:23:38 | [train_policy] epoch #755 | Saving snapshot...
2022-04-23 14:23:38 | [train_policy] epoch #755 | Saved
2022-04-23 14:23:38 | [train_policy] epoch #755 | Time 265.97 s
2022-04-23 14:23:38 | [train_policy] epoch #755 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11962
Evaluation/AverageDiscountedReturn          -41.6803
Evaluation/AverageReturn                    -41.6803
Evaluation/CompletionRate                     0
Evaluation/Iteration                        755
Evaluation/MaxReturn                        -29.14
Evaluation/MinReturn                        -71.615
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.22644
Extras/EpisodeRewardMean                    -41.5372
LinearFeatureBaseline/ExplainedVariance       0.889524
PolicyExecTime                                0.0975919
ProcessExecTime                               0.0113933
TotalEnvSteps                            765072
policy/Entropy                               -1.82903
policy/KL                                     0.00988667
policy/KLBefore                               0
policy/LossAfter                             -0.0153174
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.16057
policy/dLoss                                  0.0153174
---------------------------------------  ----------------
2022-04-23 14:23:38 | [train_policy] epoch #756 | Obtaining samples for iteration 756...
2022-04-23 14:23:38 | [train_policy] epoch #756 | Logging diagnostics...
2022-04-23 14:23:38 | [train_policy] epoch #756 | Optimizing policy...
2022-04-23 14:23:38 | [train_policy] epoch #756 | Computing loss before
2022-04-23 14:23:38 | [train_policy] epoch #756 | Computing KL before
2022-04-23 14:23:38 | [train_policy] epoch #756 | Optimizing
2022-04-23 14:23:38 | [train_policy] epoch #756 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:38 | [train_policy] epoch #756 | computing loss before
2022-04-23 14:23:38 | [train_policy] epoch #756 | computing gradient
2022-04-23 14:23:38 | [train_policy] epoch #756 | gradient computed
2022-04-23 14:23:38 | [train_policy] epoch #756 | computing descent direction
2022-04-23 14:23:38 | [train_policy] epoch #756 | descent direction computed
2022-04-23 14:23:38 | [train_policy] epoch #756 | backtrack iters: 1
2022-04-23 14:23:38 | [train_policy] epoch #756 | optimization finished
2022-04-23 14:23:38 | [train_policy] epoch #756 | Computing KL after
2022-04-23 14:23:38 | [train_policy] epoch #756 | Computing loss after
2022-04-23 14:23:38 | [train_policy] epoch #756 | Fitting baseline...
2022-04-23 14:23:38 | [train_policy] epoch #756 | Saving snapshot...
2022-04-23 14:23:38 | [train_policy] epoch #756 | Saved
2022-04-23 14:23:38 | [train_policy] epoch #756 | Time 266.29 s
2022-04-23 14:23:38 | [train_policy] epoch #756 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118364
Evaluation/AverageDiscountedReturn          -42.1477
Evaluation/AverageReturn                    -42.1477
Evaluation/CompletionRate                     0
Evaluation/Iteration                        756
Evaluation/MaxReturn                        -29.3807
Evaluation/MinReturn                        -65.2633
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.4889
Extras/EpisodeRewardMean                    -42.2235
LinearFeatureBaseline/ExplainedVariance       0.865934
PolicyExecTime                                0.0933182
ProcessExecTime                               0.0110593
TotalEnvSteps                            766084
policy/Entropy                               -1.79356
policy/KL                                     0.00640885
policy/KLBefore                               0
policy/LossAfter                             -0.0196775
policy/LossBefore                            -1.20152e-08
policy/Perplexity                             0.166367
policy/dLoss                                  0.0196775
---------------------------------------  ----------------
2022-04-23 14:23:38 | [train_policy] epoch #757 | Obtaining samples for iteration 757...
2022-04-23 14:23:38 | [train_policy] epoch #757 | Logging diagnostics...
2022-04-23 14:23:38 | [train_policy] epoch #757 | Optimizing policy...
2022-04-23 14:23:38 | [train_policy] epoch #757 | Computing loss before
2022-04-23 14:23:38 | [train_policy] epoch #757 | Computing KL before
2022-04-23 14:23:38 | [train_policy] epoch #757 | Optimizing
2022-04-23 14:23:38 | [train_policy] epoch #757 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:38 | [train_policy] epoch #757 | computing loss before
2022-04-23 14:23:38 | [train_policy] epoch #757 | computing gradient
2022-04-23 14:23:38 | [train_policy] epoch #757 | gradient computed
2022-04-23 14:23:38 | [train_policy] epoch #757 | computing descent direction
2022-04-23 14:23:38 | [train_policy] epoch #757 | descent direction computed
2022-04-23 14:23:38 | [train_policy] epoch #757 | backtrack iters: 1
2022-04-23 14:23:38 | [train_policy] epoch #757 | optimization finished
2022-04-23 14:23:38 | [train_policy] epoch #757 | Computing KL after
2022-04-23 14:23:38 | [train_policy] epoch #757 | Computing loss after
2022-04-23 14:23:38 | [train_policy] epoch #757 | Fitting baseline...
2022-04-23 14:23:38 | [train_policy] epoch #757 | Saving snapshot...
2022-04-23 14:23:38 | [train_policy] epoch #757 | Saved
2022-04-23 14:23:38 | [train_policy] epoch #757 | Time 266.63 s
2022-04-23 14:23:38 | [train_policy] epoch #757 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118266
Evaluation/AverageDiscountedReturn          -63.1499
Evaluation/AverageReturn                    -63.1499
Evaluation/CompletionRate                     0
Evaluation/Iteration                        757
Evaluation/MaxReturn                        -29.1537
Evaluation/MinReturn                      -2046.72
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.086
Extras/EpisodeRewardMean                    -61.709
LinearFeatureBaseline/ExplainedVariance       0.00970903
PolicyExecTime                                0.097683
ProcessExecTime                               0.0110886
TotalEnvSteps                            767096
policy/Entropy                               -1.78511
policy/KL                                     0.00640859
policy/KLBefore                               0
policy/LossAfter                             -0.0222384
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.167778
policy/dLoss                                  0.0222384
---------------------------------------  ----------------
2022-04-23 14:23:38 | [train_policy] epoch #758 | Obtaining samples for iteration 758...
2022-04-23 14:23:39 | [train_policy] epoch #758 | Logging diagnostics...
2022-04-23 14:23:39 | [train_policy] epoch #758 | Optimizing policy...
2022-04-23 14:23:39 | [train_policy] epoch #758 | Computing loss before
2022-04-23 14:23:39 | [train_policy] epoch #758 | Computing KL before
2022-04-23 14:23:39 | [train_policy] epoch #758 | Optimizing
2022-04-23 14:23:39 | [train_policy] epoch #758 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:39 | [train_policy] epoch #758 | computing loss before
2022-04-23 14:23:39 | [train_policy] epoch #758 | computing gradient
2022-04-23 14:23:39 | [train_policy] epoch #758 | gradient computed
2022-04-23 14:23:39 | [train_policy] epoch #758 | computing descent direction
2022-04-23 14:23:39 | [train_policy] epoch #758 | descent direction computed
2022-04-23 14:23:39 | [train_policy] epoch #758 | backtrack iters: 1
2022-04-23 14:23:39 | [train_policy] epoch #758 | optimization finished
2022-04-23 14:23:39 | [train_policy] epoch #758 | Computing KL after
2022-04-23 14:23:39 | [train_policy] epoch #758 | Computing loss after
2022-04-23 14:23:39 | [train_policy] epoch #758 | Fitting baseline...
2022-04-23 14:23:39 | [train_policy] epoch #758 | Saving snapshot...
2022-04-23 14:23:39 | [train_policy] epoch #758 | Saved
2022-04-23 14:23:39 | [train_policy] epoch #758 | Time 266.97 s
2022-04-23 14:23:39 | [train_policy] epoch #758 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.118658
Evaluation/AverageDiscountedReturn          -40.1642
Evaluation/AverageReturn                    -40.1642
Evaluation/CompletionRate                     0
Evaluation/Iteration                        758
Evaluation/MaxReturn                        -28.8325
Evaluation/MinReturn                        -72.1573
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.748
Extras/EpisodeRewardMean                    -40.1388
LinearFeatureBaseline/ExplainedVariance     -60.78
PolicyExecTime                                0.0993094
ProcessExecTime                               0.0116076
TotalEnvSteps                            768108
policy/Entropy                               -1.799
policy/KL                                     0.00663809
policy/KLBefore                               0
policy/LossAfter                             -0.0182288
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.165465
policy/dLoss                                  0.0182288
---------------------------------------  ---------------
2022-04-23 14:23:39 | [train_policy] epoch #759 | Obtaining samples for iteration 759...
2022-04-23 14:23:39 | [train_policy] epoch #759 | Logging diagnostics...
2022-04-23 14:23:39 | [train_policy] epoch #759 | Optimizing policy...
2022-04-23 14:23:39 | [train_policy] epoch #759 | Computing loss before
2022-04-23 14:23:39 | [train_policy] epoch #759 | Computing KL before
2022-04-23 14:23:39 | [train_policy] epoch #759 | Optimizing
2022-04-23 14:23:39 | [train_policy] epoch #759 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:39 | [train_policy] epoch #759 | computing loss before
2022-04-23 14:23:39 | [train_policy] epoch #759 | computing gradient
2022-04-23 14:23:39 | [train_policy] epoch #759 | gradient computed
2022-04-23 14:23:39 | [train_policy] epoch #759 | computing descent direction
2022-04-23 14:23:39 | [train_policy] epoch #759 | descent direction computed
2022-04-23 14:23:39 | [train_policy] epoch #759 | backtrack iters: 0
2022-04-23 14:23:39 | [train_policy] epoch #759 | optimization finished
2022-04-23 14:23:39 | [train_policy] epoch #759 | Computing KL after
2022-04-23 14:23:39 | [train_policy] epoch #759 | Computing loss after
2022-04-23 14:23:39 | [train_policy] epoch #759 | Fitting baseline...
2022-04-23 14:23:39 | [train_policy] epoch #759 | Saving snapshot...
2022-04-23 14:23:39 | [train_policy] epoch #759 | Saved
2022-04-23 14:23:39 | [train_policy] epoch #759 | Time 267.31 s
2022-04-23 14:23:39 | [train_policy] epoch #759 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118521
Evaluation/AverageDiscountedReturn          -39.8919
Evaluation/AverageReturn                    -39.8919
Evaluation/CompletionRate                     0
Evaluation/Iteration                        759
Evaluation/MaxReturn                        -32.1608
Evaluation/MinReturn                        -64.3423
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.30628
Extras/EpisodeRewardMean                    -40.2964
LinearFeatureBaseline/ExplainedVariance       0.900213
PolicyExecTime                                0.0995033
ProcessExecTime                               0.0111892
TotalEnvSteps                            769120
policy/Entropy                               -1.81846
policy/KL                                     0.00976864
policy/KLBefore                               0
policy/LossAfter                             -0.0168055
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.162275
policy/dLoss                                  0.0168055
---------------------------------------  ----------------
2022-04-23 14:23:39 | [train_policy] epoch #760 | Obtaining samples for iteration 760...
2022-04-23 14:23:39 | [train_policy] epoch #760 | Logging diagnostics...
2022-04-23 14:23:39 | [train_policy] epoch #760 | Optimizing policy...
2022-04-23 14:23:39 | [train_policy] epoch #760 | Computing loss before
2022-04-23 14:23:39 | [train_policy] epoch #760 | Computing KL before
2022-04-23 14:23:39 | [train_policy] epoch #760 | Optimizing
2022-04-23 14:23:39 | [train_policy] epoch #760 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:39 | [train_policy] epoch #760 | computing loss before
2022-04-23 14:23:39 | [train_policy] epoch #760 | computing gradient
2022-04-23 14:23:39 | [train_policy] epoch #760 | gradient computed
2022-04-23 14:23:39 | [train_policy] epoch #760 | computing descent direction
2022-04-23 14:23:39 | [train_policy] epoch #760 | descent direction computed
2022-04-23 14:23:39 | [train_policy] epoch #760 | backtrack iters: 0
2022-04-23 14:23:39 | [train_policy] epoch #760 | optimization finished
2022-04-23 14:23:39 | [train_policy] epoch #760 | Computing KL after
2022-04-23 14:23:39 | [train_policy] epoch #760 | Computing loss after
2022-04-23 14:23:39 | [train_policy] epoch #760 | Fitting baseline...
2022-04-23 14:23:39 | [train_policy] epoch #760 | Saving snapshot...
2022-04-23 14:23:39 | [train_policy] epoch #760 | Saved
2022-04-23 14:23:39 | [train_policy] epoch #760 | Time 267.65 s
2022-04-23 14:23:39 | [train_policy] epoch #760 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119326
Evaluation/AverageDiscountedReturn          -40.2907
Evaluation/AverageReturn                    -40.2907
Evaluation/CompletionRate                     0
Evaluation/Iteration                        760
Evaluation/MaxReturn                        -29.2601
Evaluation/MinReturn                        -71.306
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.84643
Extras/EpisodeRewardMean                    -40.7836
LinearFeatureBaseline/ExplainedVariance       0.86311
PolicyExecTime                                0.102235
ProcessExecTime                               0.0110588
TotalEnvSteps                            770132
policy/Entropy                               -1.83708
policy/KL                                     0.00992337
policy/KLBefore                               0
policy/LossAfter                             -0.0292873
policy/LossBefore                            -1.34287e-08
policy/Perplexity                             0.159282
policy/dLoss                                  0.0292873
---------------------------------------  ----------------
2022-04-23 14:23:39 | [train_policy] epoch #761 | Obtaining samples for iteration 761...
2022-04-23 14:23:40 | [train_policy] epoch #761 | Logging diagnostics...
2022-04-23 14:23:40 | [train_policy] epoch #761 | Optimizing policy...
2022-04-23 14:23:40 | [train_policy] epoch #761 | Computing loss before
2022-04-23 14:23:40 | [train_policy] epoch #761 | Computing KL before
2022-04-23 14:23:40 | [train_policy] epoch #761 | Optimizing
2022-04-23 14:23:40 | [train_policy] epoch #761 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:40 | [train_policy] epoch #761 | computing loss before
2022-04-23 14:23:40 | [train_policy] epoch #761 | computing gradient
2022-04-23 14:23:40 | [train_policy] epoch #761 | gradient computed
2022-04-23 14:23:40 | [train_policy] epoch #761 | computing descent direction
2022-04-23 14:23:40 | [train_policy] epoch #761 | descent direction computed
2022-04-23 14:23:40 | [train_policy] epoch #761 | backtrack iters: 0
2022-04-23 14:23:40 | [train_policy] epoch #761 | optimization finished
2022-04-23 14:23:40 | [train_policy] epoch #761 | Computing KL after
2022-04-23 14:23:40 | [train_policy] epoch #761 | Computing loss after
2022-04-23 14:23:40 | [train_policy] epoch #761 | Fitting baseline...
2022-04-23 14:23:40 | [train_policy] epoch #761 | Saving snapshot...
2022-04-23 14:23:40 | [train_policy] epoch #761 | Saved
2022-04-23 14:23:40 | [train_policy] epoch #761 | Time 267.98 s
2022-04-23 14:23:40 | [train_policy] epoch #761 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117819
Evaluation/AverageDiscountedReturn          -40.9482
Evaluation/AverageReturn                    -40.9482
Evaluation/CompletionRate                     0
Evaluation/Iteration                        761
Evaluation/MaxReturn                        -30.0853
Evaluation/MinReturn                        -63.8022
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.51349
Extras/EpisodeRewardMean                    -40.6235
LinearFeatureBaseline/ExplainedVariance       0.878316
PolicyExecTime                                0.0970716
ProcessExecTime                               0.011065
TotalEnvSteps                            771144
policy/Entropy                               -1.83747
policy/KL                                     0.00961743
policy/KLBefore                               0
policy/LossAfter                             -0.0198784
policy/LossBefore                            -1.83761e-08
policy/Perplexity                             0.15922
policy/dLoss                                  0.0198784
---------------------------------------  ----------------
2022-04-23 14:23:40 | [train_policy] epoch #762 | Obtaining samples for iteration 762...
2022-04-23 14:23:40 | [train_policy] epoch #762 | Logging diagnostics...
2022-04-23 14:23:40 | [train_policy] epoch #762 | Optimizing policy...
2022-04-23 14:23:40 | [train_policy] epoch #762 | Computing loss before
2022-04-23 14:23:40 | [train_policy] epoch #762 | Computing KL before
2022-04-23 14:23:40 | [train_policy] epoch #762 | Optimizing
2022-04-23 14:23:40 | [train_policy] epoch #762 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:40 | [train_policy] epoch #762 | computing loss before
2022-04-23 14:23:40 | [train_policy] epoch #762 | computing gradient
2022-04-23 14:23:40 | [train_policy] epoch #762 | gradient computed
2022-04-23 14:23:40 | [train_policy] epoch #762 | computing descent direction
2022-04-23 14:23:40 | [train_policy] epoch #762 | descent direction computed
2022-04-23 14:23:40 | [train_policy] epoch #762 | backtrack iters: 1
2022-04-23 14:23:40 | [train_policy] epoch #762 | optimization finished
2022-04-23 14:23:40 | [train_policy] epoch #762 | Computing KL after
2022-04-23 14:23:40 | [train_policy] epoch #762 | Computing loss after
2022-04-23 14:23:40 | [train_policy] epoch #762 | Fitting baseline...
2022-04-23 14:23:40 | [train_policy] epoch #762 | Saving snapshot...
2022-04-23 14:23:40 | [train_policy] epoch #762 | Saved
2022-04-23 14:23:40 | [train_policy] epoch #762 | Time 268.31 s
2022-04-23 14:23:40 | [train_policy] epoch #762 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118845
Evaluation/AverageDiscountedReturn          -41.2494
Evaluation/AverageReturn                    -41.2494
Evaluation/CompletionRate                     0
Evaluation/Iteration                        762
Evaluation/MaxReturn                        -30.2402
Evaluation/MinReturn                        -73.4437
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.42712
Extras/EpisodeRewardMean                    -41.2309
LinearFeatureBaseline/ExplainedVariance       0.832159
PolicyExecTime                                0.0959461
ProcessExecTime                               0.0112305
TotalEnvSteps                            772156
policy/Entropy                               -1.83947
policy/KL                                     0.00647141
policy/KLBefore                               0
policy/LossAfter                             -0.0218254
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.158902
policy/dLoss                                  0.0218254
---------------------------------------  ----------------
2022-04-23 14:23:40 | [train_policy] epoch #763 | Obtaining samples for iteration 763...
2022-04-23 14:23:40 | [train_policy] epoch #763 | Logging diagnostics...
2022-04-23 14:23:40 | [train_policy] epoch #763 | Optimizing policy...
2022-04-23 14:23:40 | [train_policy] epoch #763 | Computing loss before
2022-04-23 14:23:40 | [train_policy] epoch #763 | Computing KL before
2022-04-23 14:23:40 | [train_policy] epoch #763 | Optimizing
2022-04-23 14:23:40 | [train_policy] epoch #763 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:40 | [train_policy] epoch #763 | computing loss before
2022-04-23 14:23:40 | [train_policy] epoch #763 | computing gradient
2022-04-23 14:23:40 | [train_policy] epoch #763 | gradient computed
2022-04-23 14:23:40 | [train_policy] epoch #763 | computing descent direction
2022-04-23 14:23:40 | [train_policy] epoch #763 | descent direction computed
2022-04-23 14:23:40 | [train_policy] epoch #763 | backtrack iters: 1
2022-04-23 14:23:40 | [train_policy] epoch #763 | optimization finished
2022-04-23 14:23:40 | [train_policy] epoch #763 | Computing KL after
2022-04-23 14:23:40 | [train_policy] epoch #763 | Computing loss after
2022-04-23 14:23:40 | [train_policy] epoch #763 | Fitting baseline...
2022-04-23 14:23:40 | [train_policy] epoch #763 | Saving snapshot...
2022-04-23 14:23:40 | [train_policy] epoch #763 | Saved
2022-04-23 14:23:40 | [train_policy] epoch #763 | Time 268.65 s
2022-04-23 14:23:40 | [train_policy] epoch #763 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.11854
Evaluation/AverageDiscountedReturn          -40.3321
Evaluation/AverageReturn                    -40.3321
Evaluation/CompletionRate                     0
Evaluation/Iteration                        763
Evaluation/MaxReturn                        -29.615
Evaluation/MinReturn                        -65.7749
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.09505
Extras/EpisodeRewardMean                    -40.4263
LinearFeatureBaseline/ExplainedVariance       0.857436
PolicyExecTime                                0.0982883
ProcessExecTime                               0.0113187
TotalEnvSteps                            773168
policy/Entropy                               -1.83445
policy/KL                                     0.00659187
policy/KLBefore                               0
policy/LossAfter                             -0.01289
policy/LossBefore                             1.6727e-08
policy/Perplexity                             0.159701
policy/dLoss                                  0.01289
---------------------------------------  ---------------
2022-04-23 14:23:40 | [train_policy] epoch #764 | Obtaining samples for iteration 764...
2022-04-23 14:23:41 | [train_policy] epoch #764 | Logging diagnostics...
2022-04-23 14:23:41 | [train_policy] epoch #764 | Optimizing policy...
2022-04-23 14:23:41 | [train_policy] epoch #764 | Computing loss before
2022-04-23 14:23:41 | [train_policy] epoch #764 | Computing KL before
2022-04-23 14:23:41 | [train_policy] epoch #764 | Optimizing
2022-04-23 14:23:41 | [train_policy] epoch #764 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:41 | [train_policy] epoch #764 | computing loss before
2022-04-23 14:23:41 | [train_policy] epoch #764 | computing gradient
2022-04-23 14:23:41 | [train_policy] epoch #764 | gradient computed
2022-04-23 14:23:41 | [train_policy] epoch #764 | computing descent direction
2022-04-23 14:23:41 | [train_policy] epoch #764 | descent direction computed
2022-04-23 14:23:41 | [train_policy] epoch #764 | backtrack iters: 1
2022-04-23 14:23:41 | [train_policy] epoch #764 | optimization finished
2022-04-23 14:23:41 | [train_policy] epoch #764 | Computing KL after
2022-04-23 14:23:41 | [train_policy] epoch #764 | Computing loss after
2022-04-23 14:23:41 | [train_policy] epoch #764 | Fitting baseline...
2022-04-23 14:23:41 | [train_policy] epoch #764 | Saving snapshot...
2022-04-23 14:23:41 | [train_policy] epoch #764 | Saved
2022-04-23 14:23:41 | [train_policy] epoch #764 | Time 268.98 s
2022-04-23 14:23:41 | [train_policy] epoch #764 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118502
Evaluation/AverageDiscountedReturn          -41.7119
Evaluation/AverageReturn                    -41.7119
Evaluation/CompletionRate                     0
Evaluation/Iteration                        764
Evaluation/MaxReturn                        -29.1109
Evaluation/MinReturn                        -72.0284
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.8841
Extras/EpisodeRewardMean                    -41.7421
LinearFeatureBaseline/ExplainedVariance       0.870819
PolicyExecTime                                0.0935633
ProcessExecTime                               0.0110948
TotalEnvSteps                            774180
policy/Entropy                               -1.82772
policy/KL                                     0.00649876
policy/KLBefore                               0
policy/LossAfter                             -0.0161425
policy/LossBefore                            -9.42366e-09
policy/Perplexity                             0.16078
policy/dLoss                                  0.0161424
---------------------------------------  ----------------
2022-04-23 14:23:41 | [train_policy] epoch #765 | Obtaining samples for iteration 765...
2022-04-23 14:23:41 | [train_policy] epoch #765 | Logging diagnostics...
2022-04-23 14:23:41 | [train_policy] epoch #765 | Optimizing policy...
2022-04-23 14:23:41 | [train_policy] epoch #765 | Computing loss before
2022-04-23 14:23:41 | [train_policy] epoch #765 | Computing KL before
2022-04-23 14:23:41 | [train_policy] epoch #765 | Optimizing
2022-04-23 14:23:41 | [train_policy] epoch #765 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:41 | [train_policy] epoch #765 | computing loss before
2022-04-23 14:23:41 | [train_policy] epoch #765 | computing gradient
2022-04-23 14:23:41 | [train_policy] epoch #765 | gradient computed
2022-04-23 14:23:41 | [train_policy] epoch #765 | computing descent direction
2022-04-23 14:23:41 | [train_policy] epoch #765 | descent direction computed
2022-04-23 14:23:41 | [train_policy] epoch #765 | backtrack iters: 0
2022-04-23 14:23:41 | [train_policy] epoch #765 | optimization finished
2022-04-23 14:23:41 | [train_policy] epoch #765 | Computing KL after
2022-04-23 14:23:41 | [train_policy] epoch #765 | Computing loss after
2022-04-23 14:23:41 | [train_policy] epoch #765 | Fitting baseline...
2022-04-23 14:23:41 | [train_policy] epoch #765 | Saving snapshot...
2022-04-23 14:23:41 | [train_policy] epoch #765 | Saved
2022-04-23 14:23:41 | [train_policy] epoch #765 | Time 269.31 s
2022-04-23 14:23:41 | [train_policy] epoch #765 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119981
Evaluation/AverageDiscountedReturn          -40.6824
Evaluation/AverageReturn                    -40.6824
Evaluation/CompletionRate                     0
Evaluation/Iteration                        765
Evaluation/MaxReturn                        -31.0819
Evaluation/MinReturn                        -63.5921
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.79679
Extras/EpisodeRewardMean                    -40.8944
LinearFeatureBaseline/ExplainedVariance       0.870459
PolicyExecTime                                0.0949621
ProcessExecTime                               0.0110989
TotalEnvSteps                            775192
policy/Entropy                               -1.83754
policy/KL                                     0.00974937
policy/KLBefore                               0
policy/LossAfter                             -0.0165468
policy/LossBefore                             1.64914e-09
policy/Perplexity                             0.159209
policy/dLoss                                  0.0165468
---------------------------------------  ----------------
2022-04-23 14:23:41 | [train_policy] epoch #766 | Obtaining samples for iteration 766...
2022-04-23 14:23:41 | [train_policy] epoch #766 | Logging diagnostics...
2022-04-23 14:23:41 | [train_policy] epoch #766 | Optimizing policy...
2022-04-23 14:23:41 | [train_policy] epoch #766 | Computing loss before
2022-04-23 14:23:41 | [train_policy] epoch #766 | Computing KL before
2022-04-23 14:23:41 | [train_policy] epoch #766 | Optimizing
2022-04-23 14:23:41 | [train_policy] epoch #766 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:41 | [train_policy] epoch #766 | computing loss before
2022-04-23 14:23:41 | [train_policy] epoch #766 | computing gradient
2022-04-23 14:23:41 | [train_policy] epoch #766 | gradient computed
2022-04-23 14:23:41 | [train_policy] epoch #766 | computing descent direction
2022-04-23 14:23:41 | [train_policy] epoch #766 | descent direction computed
2022-04-23 14:23:41 | [train_policy] epoch #766 | backtrack iters: 1
2022-04-23 14:23:41 | [train_policy] epoch #766 | optimization finished
2022-04-23 14:23:41 | [train_policy] epoch #766 | Computing KL after
2022-04-23 14:23:41 | [train_policy] epoch #766 | Computing loss after
2022-04-23 14:23:41 | [train_policy] epoch #766 | Fitting baseline...
2022-04-23 14:23:41 | [train_policy] epoch #766 | Saving snapshot...
2022-04-23 14:23:41 | [train_policy] epoch #766 | Saved
2022-04-23 14:23:41 | [train_policy] epoch #766 | Time 269.64 s
2022-04-23 14:23:41 | [train_policy] epoch #766 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118843
Evaluation/AverageDiscountedReturn          -40.0962
Evaluation/AverageReturn                    -40.0962
Evaluation/CompletionRate                     0
Evaluation/Iteration                        766
Evaluation/MaxReturn                        -29.182
Evaluation/MinReturn                        -65.149
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.84516
Extras/EpisodeRewardMean                    -39.9035
LinearFeatureBaseline/ExplainedVariance       0.871884
PolicyExecTime                                0.0956247
ProcessExecTime                               0.011174
TotalEnvSteps                            776204
policy/Entropy                               -1.86492
policy/KL                                     0.00659362
policy/KLBefore                               0
policy/LossAfter                             -0.0145989
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.154908
policy/dLoss                                  0.0145989
---------------------------------------  ----------------
2022-04-23 14:23:41 | [train_policy] epoch #767 | Obtaining samples for iteration 767...
2022-04-23 14:23:42 | [train_policy] epoch #767 | Logging diagnostics...
2022-04-23 14:23:42 | [train_policy] epoch #767 | Optimizing policy...
2022-04-23 14:23:42 | [train_policy] epoch #767 | Computing loss before
2022-04-23 14:23:42 | [train_policy] epoch #767 | Computing KL before
2022-04-23 14:23:42 | [train_policy] epoch #767 | Optimizing
2022-04-23 14:23:42 | [train_policy] epoch #767 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:42 | [train_policy] epoch #767 | computing loss before
2022-04-23 14:23:42 | [train_policy] epoch #767 | computing gradient
2022-04-23 14:23:42 | [train_policy] epoch #767 | gradient computed
2022-04-23 14:23:42 | [train_policy] epoch #767 | computing descent direction
2022-04-23 14:23:42 | [train_policy] epoch #767 | descent direction computed
2022-04-23 14:23:42 | [train_policy] epoch #767 | backtrack iters: 1
2022-04-23 14:23:42 | [train_policy] epoch #767 | optimization finished
2022-04-23 14:23:42 | [train_policy] epoch #767 | Computing KL after
2022-04-23 14:23:42 | [train_policy] epoch #767 | Computing loss after
2022-04-23 14:23:42 | [train_policy] epoch #767 | Fitting baseline...
2022-04-23 14:23:42 | [train_policy] epoch #767 | Saving snapshot...
2022-04-23 14:23:42 | [train_policy] epoch #767 | Saved
2022-04-23 14:23:42 | [train_policy] epoch #767 | Time 269.97 s
2022-04-23 14:23:42 | [train_policy] epoch #767 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119023
Evaluation/AverageDiscountedReturn          -42.1122
Evaluation/AverageReturn                    -42.1122
Evaluation/CompletionRate                     0
Evaluation/Iteration                        767
Evaluation/MaxReturn                        -29.9971
Evaluation/MinReturn                        -64.1174
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.52534
Extras/EpisodeRewardMean                    -42.0855
LinearFeatureBaseline/ExplainedVariance       0.873008
PolicyExecTime                                0.0963058
ProcessExecTime                               0.0113823
TotalEnvSteps                            777216
policy/Entropy                               -1.84122
policy/KL                                     0.00641377
policy/KLBefore                               0
policy/LossAfter                             -0.0131393
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.158623
policy/dLoss                                  0.0131393
---------------------------------------  ----------------
2022-04-23 14:23:42 | [train_policy] epoch #768 | Obtaining samples for iteration 768...
2022-04-23 14:23:42 | [train_policy] epoch #768 | Logging diagnostics...
2022-04-23 14:23:42 | [train_policy] epoch #768 | Optimizing policy...
2022-04-23 14:23:42 | [train_policy] epoch #768 | Computing loss before
2022-04-23 14:23:42 | [train_policy] epoch #768 | Computing KL before
2022-04-23 14:23:42 | [train_policy] epoch #768 | Optimizing
2022-04-23 14:23:42 | [train_policy] epoch #768 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:42 | [train_policy] epoch #768 | computing loss before
2022-04-23 14:23:42 | [train_policy] epoch #768 | computing gradient
2022-04-23 14:23:42 | [train_policy] epoch #768 | gradient computed
2022-04-23 14:23:42 | [train_policy] epoch #768 | computing descent direction
2022-04-23 14:23:42 | [train_policy] epoch #768 | descent direction computed
2022-04-23 14:23:42 | [train_policy] epoch #768 | backtrack iters: 0
2022-04-23 14:23:42 | [train_policy] epoch #768 | optimization finished
2022-04-23 14:23:42 | [train_policy] epoch #768 | Computing KL after
2022-04-23 14:23:42 | [train_policy] epoch #768 | Computing loss after
2022-04-23 14:23:42 | [train_policy] epoch #768 | Fitting baseline...
2022-04-23 14:23:42 | [train_policy] epoch #768 | Saving snapshot...
2022-04-23 14:23:42 | [train_policy] epoch #768 | Saved
2022-04-23 14:23:42 | [train_policy] epoch #768 | Time 270.29 s
2022-04-23 14:23:42 | [train_policy] epoch #768 | EpochTime 0.31 s
---------------------------------------  ---------------
EnvExecTime                                   0.119489
Evaluation/AverageDiscountedReturn          -41.4388
Evaluation/AverageReturn                    -41.4388
Evaluation/CompletionRate                     0
Evaluation/Iteration                        768
Evaluation/MaxReturn                        -29.4347
Evaluation/MinReturn                        -72.8797
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.40619
Extras/EpisodeRewardMean                    -41.35
LinearFeatureBaseline/ExplainedVariance       0.86203
PolicyExecTime                                0.0921621
ProcessExecTime                               0.011133
TotalEnvSteps                            778228
policy/Entropy                               -1.8423
policy/KL                                     0.00984161
policy/KLBefore                               0
policy/LossAfter                             -0.0222252
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.158452
policy/dLoss                                  0.0222252
---------------------------------------  ---------------
2022-04-23 14:23:42 | [train_policy] epoch #769 | Obtaining samples for iteration 769...
2022-04-23 14:23:42 | [train_policy] epoch #769 | Logging diagnostics...
2022-04-23 14:23:42 | [train_policy] epoch #769 | Optimizing policy...
2022-04-23 14:23:42 | [train_policy] epoch #769 | Computing loss before
2022-04-23 14:23:42 | [train_policy] epoch #769 | Computing KL before
2022-04-23 14:23:42 | [train_policy] epoch #769 | Optimizing
2022-04-23 14:23:42 | [train_policy] epoch #769 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:42 | [train_policy] epoch #769 | computing loss before
2022-04-23 14:23:42 | [train_policy] epoch #769 | computing gradient
2022-04-23 14:23:42 | [train_policy] epoch #769 | gradient computed
2022-04-23 14:23:42 | [train_policy] epoch #769 | computing descent direction
2022-04-23 14:23:42 | [train_policy] epoch #769 | descent direction computed
2022-04-23 14:23:42 | [train_policy] epoch #769 | backtrack iters: 1
2022-04-23 14:23:42 | [train_policy] epoch #769 | optimization finished
2022-04-23 14:23:42 | [train_policy] epoch #769 | Computing KL after
2022-04-23 14:23:42 | [train_policy] epoch #769 | Computing loss after
2022-04-23 14:23:42 | [train_policy] epoch #769 | Fitting baseline...
2022-04-23 14:23:42 | [train_policy] epoch #769 | Saving snapshot...
2022-04-23 14:23:42 | [train_policy] epoch #769 | Saved
2022-04-23 14:23:42 | [train_policy] epoch #769 | Time 270.62 s
2022-04-23 14:23:42 | [train_policy] epoch #769 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119043
Evaluation/AverageDiscountedReturn          -41.3649
Evaluation/AverageReturn                    -41.3649
Evaluation/CompletionRate                     0
Evaluation/Iteration                        769
Evaluation/MaxReturn                        -29.424
Evaluation/MinReturn                        -63.8085
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.03594
Extras/EpisodeRewardMean                    -41.2448
LinearFeatureBaseline/ExplainedVariance       0.865378
PolicyExecTime                                0.096158
ProcessExecTime                               0.0111468
TotalEnvSteps                            779240
policy/Entropy                               -1.84593
policy/KL                                     0.00665496
policy/KLBefore                               0
policy/LossAfter                             -0.0191342
policy/LossBefore                            -1.76694e-09
policy/Perplexity                             0.157878
policy/dLoss                                  0.0191342
---------------------------------------  ----------------
2022-04-23 14:23:42 | [train_policy] epoch #770 | Obtaining samples for iteration 770...
2022-04-23 14:23:43 | [train_policy] epoch #770 | Logging diagnostics...
2022-04-23 14:23:43 | [train_policy] epoch #770 | Optimizing policy...
2022-04-23 14:23:43 | [train_policy] epoch #770 | Computing loss before
2022-04-23 14:23:43 | [train_policy] epoch #770 | Computing KL before
2022-04-23 14:23:43 | [train_policy] epoch #770 | Optimizing
2022-04-23 14:23:43 | [train_policy] epoch #770 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:43 | [train_policy] epoch #770 | computing loss before
2022-04-23 14:23:43 | [train_policy] epoch #770 | computing gradient
2022-04-23 14:23:43 | [train_policy] epoch #770 | gradient computed
2022-04-23 14:23:43 | [train_policy] epoch #770 | computing descent direction
2022-04-23 14:23:43 | [train_policy] epoch #770 | descent direction computed
2022-04-23 14:23:43 | [train_policy] epoch #770 | backtrack iters: 1
2022-04-23 14:23:43 | [train_policy] epoch #770 | optimization finished
2022-04-23 14:23:43 | [train_policy] epoch #770 | Computing KL after
2022-04-23 14:23:43 | [train_policy] epoch #770 | Computing loss after
2022-04-23 14:23:43 | [train_policy] epoch #770 | Fitting baseline...
2022-04-23 14:23:43 | [train_policy] epoch #770 | Saving snapshot...
2022-04-23 14:23:43 | [train_policy] epoch #770 | Saved
2022-04-23 14:23:43 | [train_policy] epoch #770 | Time 270.94 s
2022-04-23 14:23:43 | [train_policy] epoch #770 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118991
Evaluation/AverageDiscountedReturn          -39.5595
Evaluation/AverageReturn                    -39.5595
Evaluation/CompletionRate                     0
Evaluation/Iteration                        770
Evaluation/MaxReturn                        -29.2046
Evaluation/MinReturn                        -71.7958
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.53079
Extras/EpisodeRewardMean                    -39.6478
LinearFeatureBaseline/ExplainedVariance       0.885761
PolicyExecTime                                0.0988517
ProcessExecTime                               0.011375
TotalEnvSteps                            780252
policy/Entropy                               -1.89105
policy/KL                                     0.00664906
policy/KLBefore                               0
policy/LossAfter                             -0.014702
policy/LossBefore                            -9.77705e-09
policy/Perplexity                             0.150914
policy/dLoss                                  0.014702
---------------------------------------  ----------------
2022-04-23 14:23:43 | [train_policy] epoch #771 | Obtaining samples for iteration 771...
2022-04-23 14:23:43 | [train_policy] epoch #771 | Logging diagnostics...
2022-04-23 14:23:43 | [train_policy] epoch #771 | Optimizing policy...
2022-04-23 14:23:43 | [train_policy] epoch #771 | Computing loss before
2022-04-23 14:23:43 | [train_policy] epoch #771 | Computing KL before
2022-04-23 14:23:43 | [train_policy] epoch #771 | Optimizing
2022-04-23 14:23:43 | [train_policy] epoch #771 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:43 | [train_policy] epoch #771 | computing loss before
2022-04-23 14:23:43 | [train_policy] epoch #771 | computing gradient
2022-04-23 14:23:43 | [train_policy] epoch #771 | gradient computed
2022-04-23 14:23:43 | [train_policy] epoch #771 | computing descent direction
2022-04-23 14:23:43 | [train_policy] epoch #771 | descent direction computed
2022-04-23 14:23:43 | [train_policy] epoch #771 | backtrack iters: 1
2022-04-23 14:23:43 | [train_policy] epoch #771 | optimization finished
2022-04-23 14:23:43 | [train_policy] epoch #771 | Computing KL after
2022-04-23 14:23:43 | [train_policy] epoch #771 | Computing loss after
2022-04-23 14:23:43 | [train_policy] epoch #771 | Fitting baseline...
2022-04-23 14:23:43 | [train_policy] epoch #771 | Saving snapshot...
2022-04-23 14:23:43 | [train_policy] epoch #771 | Saved
2022-04-23 14:23:43 | [train_policy] epoch #771 | Time 271.28 s
2022-04-23 14:23:43 | [train_policy] epoch #771 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119109
Evaluation/AverageDiscountedReturn          -40.3278
Evaluation/AverageReturn                    -40.3278
Evaluation/CompletionRate                     0
Evaluation/Iteration                        771
Evaluation/MaxReturn                        -29.1023
Evaluation/MinReturn                        -72.4709
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.70223
Extras/EpisodeRewardMean                    -39.9867
LinearFeatureBaseline/ExplainedVariance       0.862195
PolicyExecTime                                0.0999331
ProcessExecTime                               0.0111427
TotalEnvSteps                            781264
policy/Entropy                               -1.93035
policy/KL                                     0.00650543
policy/KLBefore                               0
policy/LossAfter                             -0.0191445
policy/LossBefore                            -9.89484e-09
policy/Perplexity                             0.145097
policy/dLoss                                  0.0191445
---------------------------------------  ----------------
2022-04-23 14:23:43 | [train_policy] epoch #772 | Obtaining samples for iteration 772...
2022-04-23 14:23:43 | [train_policy] epoch #772 | Logging diagnostics...
2022-04-23 14:23:43 | [train_policy] epoch #772 | Optimizing policy...
2022-04-23 14:23:43 | [train_policy] epoch #772 | Computing loss before
2022-04-23 14:23:43 | [train_policy] epoch #772 | Computing KL before
2022-04-23 14:23:43 | [train_policy] epoch #772 | Optimizing
2022-04-23 14:23:43 | [train_policy] epoch #772 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:43 | [train_policy] epoch #772 | computing loss before
2022-04-23 14:23:43 | [train_policy] epoch #772 | computing gradient
2022-04-23 14:23:43 | [train_policy] epoch #772 | gradient computed
2022-04-23 14:23:43 | [train_policy] epoch #772 | computing descent direction
2022-04-23 14:23:43 | [train_policy] epoch #772 | descent direction computed
2022-04-23 14:23:43 | [train_policy] epoch #772 | backtrack iters: 0
2022-04-23 14:23:43 | [train_policy] epoch #772 | optimization finished
2022-04-23 14:23:43 | [train_policy] epoch #772 | Computing KL after
2022-04-23 14:23:43 | [train_policy] epoch #772 | Computing loss after
2022-04-23 14:23:43 | [train_policy] epoch #772 | Fitting baseline...
2022-04-23 14:23:43 | [train_policy] epoch #772 | Saving snapshot...
2022-04-23 14:23:43 | [train_policy] epoch #772 | Saved
2022-04-23 14:23:43 | [train_policy] epoch #772 | Time 271.61 s
2022-04-23 14:23:43 | [train_policy] epoch #772 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119441
Evaluation/AverageDiscountedReturn          -61.2958
Evaluation/AverageReturn                    -61.2958
Evaluation/CompletionRate                     0
Evaluation/Iteration                        772
Evaluation/MaxReturn                        -29.0996
Evaluation/MinReturn                      -2046.7
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.245
Extras/EpisodeRewardMean                    -60.0478
LinearFeatureBaseline/ExplainedVariance       0.0107799
PolicyExecTime                                0.0994036
ProcessExecTime                               0.0111632
TotalEnvSteps                            782276
policy/Entropy                               -1.86568
policy/KL                                     0.00944365
policy/KLBefore                               0
policy/LossAfter                             -0.0183039
policy/LossBefore                             1.69626e-08
policy/Perplexity                             0.154791
policy/dLoss                                  0.0183039
---------------------------------------  ----------------
2022-04-23 14:23:43 | [train_policy] epoch #773 | Obtaining samples for iteration 773...
2022-04-23 14:23:44 | [train_policy] epoch #773 | Logging diagnostics...
2022-04-23 14:23:44 | [train_policy] epoch #773 | Optimizing policy...
2022-04-23 14:23:44 | [train_policy] epoch #773 | Computing loss before
2022-04-23 14:23:44 | [train_policy] epoch #773 | Computing KL before
2022-04-23 14:23:44 | [train_policy] epoch #773 | Optimizing
2022-04-23 14:23:44 | [train_policy] epoch #773 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:44 | [train_policy] epoch #773 | computing loss before
2022-04-23 14:23:44 | [train_policy] epoch #773 | computing gradient
2022-04-23 14:23:44 | [train_policy] epoch #773 | gradient computed
2022-04-23 14:23:44 | [train_policy] epoch #773 | computing descent direction
2022-04-23 14:23:44 | [train_policy] epoch #773 | descent direction computed
2022-04-23 14:23:44 | [train_policy] epoch #773 | backtrack iters: 1
2022-04-23 14:23:44 | [train_policy] epoch #773 | optimization finished
2022-04-23 14:23:44 | [train_policy] epoch #773 | Computing KL after
2022-04-23 14:23:44 | [train_policy] epoch #773 | Computing loss after
2022-04-23 14:23:44 | [train_policy] epoch #773 | Fitting baseline...
2022-04-23 14:23:44 | [train_policy] epoch #773 | Saving snapshot...
2022-04-23 14:23:44 | [train_policy] epoch #773 | Saved
2022-04-23 14:23:44 | [train_policy] epoch #773 | Time 271.94 s
2022-04-23 14:23:44 | [train_policy] epoch #773 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120414
Evaluation/AverageDiscountedReturn          -39.6761
Evaluation/AverageReturn                    -39.6761
Evaluation/CompletionRate                     0
Evaluation/Iteration                        773
Evaluation/MaxReturn                        -29.4054
Evaluation/MinReturn                        -63.8431
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.77432
Extras/EpisodeRewardMean                    -39.6456
LinearFeatureBaseline/ExplainedVariance     -48.126
PolicyExecTime                                0.0980213
ProcessExecTime                               0.0114787
TotalEnvSteps                            783288
policy/Entropy                               -1.87771
policy/KL                                     0.00645682
policy/KLBefore                               0
policy/LossAfter                             -0.0191072
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             0.15294
policy/dLoss                                  0.0191072
---------------------------------------  ----------------
2022-04-23 14:23:44 | [train_policy] epoch #774 | Obtaining samples for iteration 774...
2022-04-23 14:23:44 | [train_policy] epoch #774 | Logging diagnostics...
2022-04-23 14:23:44 | [train_policy] epoch #774 | Optimizing policy...
2022-04-23 14:23:44 | [train_policy] epoch #774 | Computing loss before
2022-04-23 14:23:44 | [train_policy] epoch #774 | Computing KL before
2022-04-23 14:23:44 | [train_policy] epoch #774 | Optimizing
2022-04-23 14:23:44 | [train_policy] epoch #774 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:44 | [train_policy] epoch #774 | computing loss before
2022-04-23 14:23:44 | [train_policy] epoch #774 | computing gradient
2022-04-23 14:23:44 | [train_policy] epoch #774 | gradient computed
2022-04-23 14:23:44 | [train_policy] epoch #774 | computing descent direction
2022-04-23 14:23:44 | [train_policy] epoch #774 | descent direction computed
2022-04-23 14:23:44 | [train_policy] epoch #774 | backtrack iters: 1
2022-04-23 14:23:44 | [train_policy] epoch #774 | optimization finished
2022-04-23 14:23:44 | [train_policy] epoch #774 | Computing KL after
2022-04-23 14:23:44 | [train_policy] epoch #774 | Computing loss after
2022-04-23 14:23:44 | [train_policy] epoch #774 | Fitting baseline...
2022-04-23 14:23:44 | [train_policy] epoch #774 | Saving snapshot...
2022-04-23 14:23:44 | [train_policy] epoch #774 | Saved
2022-04-23 14:23:44 | [train_policy] epoch #774 | Time 272.26 s
2022-04-23 14:23:44 | [train_policy] epoch #774 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119772
Evaluation/AverageDiscountedReturn          -41.3612
Evaluation/AverageReturn                    -41.3612
Evaluation/CompletionRate                     0
Evaluation/Iteration                        774
Evaluation/MaxReturn                        -29.2516
Evaluation/MinReturn                        -73.1921
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.67188
Extras/EpisodeRewardMean                    -41.2355
LinearFeatureBaseline/ExplainedVariance       0.868941
PolicyExecTime                                0.0950296
ProcessExecTime                               0.0111732
TotalEnvSteps                            784300
policy/Entropy                               -1.89519
policy/KL                                     0.0064356
policy/KLBefore                               0
policy/LossAfter                             -0.0125904
policy/LossBefore                             5.88979e-10
policy/Perplexity                             0.15029
policy/dLoss                                  0.0125904
---------------------------------------  ----------------
2022-04-23 14:23:44 | [train_policy] epoch #775 | Obtaining samples for iteration 775...
2022-04-23 14:23:44 | [train_policy] epoch #775 | Logging diagnostics...
2022-04-23 14:23:44 | [train_policy] epoch #775 | Optimizing policy...
2022-04-23 14:23:44 | [train_policy] epoch #775 | Computing loss before
2022-04-23 14:23:44 | [train_policy] epoch #775 | Computing KL before
2022-04-23 14:23:44 | [train_policy] epoch #775 | Optimizing
2022-04-23 14:23:44 | [train_policy] epoch #775 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:44 | [train_policy] epoch #775 | computing loss before
2022-04-23 14:23:44 | [train_policy] epoch #775 | computing gradient
2022-04-23 14:23:44 | [train_policy] epoch #775 | gradient computed
2022-04-23 14:23:44 | [train_policy] epoch #775 | computing descent direction
2022-04-23 14:23:44 | [train_policy] epoch #775 | descent direction computed
2022-04-23 14:23:44 | [train_policy] epoch #775 | backtrack iters: 1
2022-04-23 14:23:44 | [train_policy] epoch #775 | optimization finished
2022-04-23 14:23:44 | [train_policy] epoch #775 | Computing KL after
2022-04-23 14:23:44 | [train_policy] epoch #775 | Computing loss after
2022-04-23 14:23:44 | [train_policy] epoch #775 | Fitting baseline...
2022-04-23 14:23:44 | [train_policy] epoch #775 | Saving snapshot...
2022-04-23 14:23:44 | [train_policy] epoch #775 | Saved
2022-04-23 14:23:44 | [train_policy] epoch #775 | Time 272.59 s
2022-04-23 14:23:44 | [train_policy] epoch #775 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119787
Evaluation/AverageDiscountedReturn          -40.6447
Evaluation/AverageReturn                    -40.6447
Evaluation/CompletionRate                     0
Evaluation/Iteration                        775
Evaluation/MaxReturn                        -29.7864
Evaluation/MinReturn                        -65.1622
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.98812
Extras/EpisodeRewardMean                    -40.7338
LinearFeatureBaseline/ExplainedVariance       0.875665
PolicyExecTime                                0.0923841
ProcessExecTime                               0.011302
TotalEnvSteps                            785312
policy/Entropy                               -1.91148
policy/KL                                     0.00642669
policy/KLBefore                               0
policy/LossAfter                             -0.0164314
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.147862
policy/dLoss                                  0.0164314
---------------------------------------  ----------------
2022-04-23 14:23:44 | [train_policy] epoch #776 | Obtaining samples for iteration 776...
2022-04-23 14:23:45 | [train_policy] epoch #776 | Logging diagnostics...
2022-04-23 14:23:45 | [train_policy] epoch #776 | Optimizing policy...
2022-04-23 14:23:45 | [train_policy] epoch #776 | Computing loss before
2022-04-23 14:23:45 | [train_policy] epoch #776 | Computing KL before
2022-04-23 14:23:45 | [train_policy] epoch #776 | Optimizing
2022-04-23 14:23:45 | [train_policy] epoch #776 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:45 | [train_policy] epoch #776 | computing loss before
2022-04-23 14:23:45 | [train_policy] epoch #776 | computing gradient
2022-04-23 14:23:45 | [train_policy] epoch #776 | gradient computed
2022-04-23 14:23:45 | [train_policy] epoch #776 | computing descent direction
2022-04-23 14:23:45 | [train_policy] epoch #776 | descent direction computed
2022-04-23 14:23:45 | [train_policy] epoch #776 | backtrack iters: 0
2022-04-23 14:23:45 | [train_policy] epoch #776 | optimization finished
2022-04-23 14:23:45 | [train_policy] epoch #776 | Computing KL after
2022-04-23 14:23:45 | [train_policy] epoch #776 | Computing loss after
2022-04-23 14:23:45 | [train_policy] epoch #776 | Fitting baseline...
2022-04-23 14:23:45 | [train_policy] epoch #776 | Saving snapshot...
2022-04-23 14:23:45 | [train_policy] epoch #776 | Saved
2022-04-23 14:23:45 | [train_policy] epoch #776 | Time 272.91 s
2022-04-23 14:23:45 | [train_policy] epoch #776 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118653
Evaluation/AverageDiscountedReturn          -41.1485
Evaluation/AverageReturn                    -41.1485
Evaluation/CompletionRate                     0
Evaluation/Iteration                        776
Evaluation/MaxReturn                        -29.634
Evaluation/MinReturn                        -72.7448
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.72365
Extras/EpisodeRewardMean                    -41.1159
LinearFeatureBaseline/ExplainedVariance       0.865321
PolicyExecTime                                0.0907598
ProcessExecTime                               0.0111744
TotalEnvSteps                            786324
policy/Entropy                               -1.91554
policy/KL                                     0.00999501
policy/KLBefore                               0
policy/LossAfter                             -0.0228094
policy/LossBefore                            -6.47877e-09
policy/Perplexity                             0.147262
policy/dLoss                                  0.0228094
---------------------------------------  ----------------
2022-04-23 14:23:45 | [train_policy] epoch #777 | Obtaining samples for iteration 777...
2022-04-23 14:23:45 | [train_policy] epoch #777 | Logging diagnostics...
2022-04-23 14:23:45 | [train_policy] epoch #777 | Optimizing policy...
2022-04-23 14:23:45 | [train_policy] epoch #777 | Computing loss before
2022-04-23 14:23:45 | [train_policy] epoch #777 | Computing KL before
2022-04-23 14:23:45 | [train_policy] epoch #777 | Optimizing
2022-04-23 14:23:45 | [train_policy] epoch #777 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:45 | [train_policy] epoch #777 | computing loss before
2022-04-23 14:23:45 | [train_policy] epoch #777 | computing gradient
2022-04-23 14:23:45 | [train_policy] epoch #777 | gradient computed
2022-04-23 14:23:45 | [train_policy] epoch #777 | computing descent direction
2022-04-23 14:23:45 | [train_policy] epoch #777 | descent direction computed
2022-04-23 14:23:45 | [train_policy] epoch #777 | backtrack iters: 1
2022-04-23 14:23:45 | [train_policy] epoch #777 | optimization finished
2022-04-23 14:23:45 | [train_policy] epoch #777 | Computing KL after
2022-04-23 14:23:45 | [train_policy] epoch #777 | Computing loss after
2022-04-23 14:23:45 | [train_policy] epoch #777 | Fitting baseline...
2022-04-23 14:23:45 | [train_policy] epoch #777 | Saving snapshot...
2022-04-23 14:23:45 | [train_policy] epoch #777 | Saved
2022-04-23 14:23:45 | [train_policy] epoch #777 | Time 273.23 s
2022-04-23 14:23:45 | [train_policy] epoch #777 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119435
Evaluation/AverageDiscountedReturn          -40.6403
Evaluation/AverageReturn                    -40.6403
Evaluation/CompletionRate                     0
Evaluation/Iteration                        777
Evaluation/MaxReturn                        -29.0245
Evaluation/MinReturn                        -63.8745
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.01123
Extras/EpisodeRewardMean                    -40.3266
LinearFeatureBaseline/ExplainedVariance       0.899897
PolicyExecTime                                0.0952075
ProcessExecTime                               0.0113471
TotalEnvSteps                            787336
policy/Entropy                               -1.906
policy/KL                                     0.00672436
policy/KLBefore                               0
policy/LossAfter                             -0.0175953
policy/LossBefore                            -1.17796e-10
policy/Perplexity                             0.148675
policy/dLoss                                  0.0175953
---------------------------------------  ----------------
2022-04-23 14:23:45 | [train_policy] epoch #778 | Obtaining samples for iteration 778...
2022-04-23 14:23:45 | [train_policy] epoch #778 | Logging diagnostics...
2022-04-23 14:23:45 | [train_policy] epoch #778 | Optimizing policy...
2022-04-23 14:23:45 | [train_policy] epoch #778 | Computing loss before
2022-04-23 14:23:45 | [train_policy] epoch #778 | Computing KL before
2022-04-23 14:23:45 | [train_policy] epoch #778 | Optimizing
2022-04-23 14:23:45 | [train_policy] epoch #778 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:45 | [train_policy] epoch #778 | computing loss before
2022-04-23 14:23:45 | [train_policy] epoch #778 | computing gradient
2022-04-23 14:23:45 | [train_policy] epoch #778 | gradient computed
2022-04-23 14:23:45 | [train_policy] epoch #778 | computing descent direction
2022-04-23 14:23:45 | [train_policy] epoch #778 | descent direction computed
2022-04-23 14:23:45 | [train_policy] epoch #778 | backtrack iters: 1
2022-04-23 14:23:45 | [train_policy] epoch #778 | optimization finished
2022-04-23 14:23:45 | [train_policy] epoch #778 | Computing KL after
2022-04-23 14:23:45 | [train_policy] epoch #778 | Computing loss after
2022-04-23 14:23:45 | [train_policy] epoch #778 | Fitting baseline...
2022-04-23 14:23:45 | [train_policy] epoch #778 | Saving snapshot...
2022-04-23 14:23:45 | [train_policy] epoch #778 | Saved
2022-04-23 14:23:45 | [train_policy] epoch #778 | Time 273.55 s
2022-04-23 14:23:45 | [train_policy] epoch #778 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.11897
Evaluation/AverageDiscountedReturn          -40.8564
Evaluation/AverageReturn                    -40.8564
Evaluation/CompletionRate                     0
Evaluation/Iteration                        778
Evaluation/MaxReturn                        -29.1146
Evaluation/MinReturn                        -81.5377
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.87718
Extras/EpisodeRewardMean                    -40.7943
LinearFeatureBaseline/ExplainedVariance       0.776774
PolicyExecTime                                0.0933995
ProcessExecTime                               0.0112255
TotalEnvSteps                            788348
policy/Entropy                               -1.9045
policy/KL                                     0.00641919
policy/KLBefore                               0
policy/LossAfter                             -0.0204869
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.148897
policy/dLoss                                  0.0204869
---------------------------------------  ----------------
2022-04-23 14:23:45 | [train_policy] epoch #779 | Obtaining samples for iteration 779...
2022-04-23 14:23:46 | [train_policy] epoch #779 | Logging diagnostics...
2022-04-23 14:23:46 | [train_policy] epoch #779 | Optimizing policy...
2022-04-23 14:23:46 | [train_policy] epoch #779 | Computing loss before
2022-04-23 14:23:46 | [train_policy] epoch #779 | Computing KL before
2022-04-23 14:23:46 | [train_policy] epoch #779 | Optimizing
2022-04-23 14:23:46 | [train_policy] epoch #779 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:46 | [train_policy] epoch #779 | computing loss before
2022-04-23 14:23:46 | [train_policy] epoch #779 | computing gradient
2022-04-23 14:23:46 | [train_policy] epoch #779 | gradient computed
2022-04-23 14:23:46 | [train_policy] epoch #779 | computing descent direction
2022-04-23 14:23:46 | [train_policy] epoch #779 | descent direction computed
2022-04-23 14:23:46 | [train_policy] epoch #779 | backtrack iters: 1
2022-04-23 14:23:46 | [train_policy] epoch #779 | optimization finished
2022-04-23 14:23:46 | [train_policy] epoch #779 | Computing KL after
2022-04-23 14:23:46 | [train_policy] epoch #779 | Computing loss after
2022-04-23 14:23:46 | [train_policy] epoch #779 | Fitting baseline...
2022-04-23 14:23:46 | [train_policy] epoch #779 | Saving snapshot...
2022-04-23 14:23:46 | [train_policy] epoch #779 | Saved
2022-04-23 14:23:46 | [train_policy] epoch #779 | Time 273.87 s
2022-04-23 14:23:46 | [train_policy] epoch #779 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118788
Evaluation/AverageDiscountedReturn          -42.4554
Evaluation/AverageReturn                    -42.4554
Evaluation/CompletionRate                     0
Evaluation/Iteration                        779
Evaluation/MaxReturn                        -28.4889
Evaluation/MinReturn                        -73.0536
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.0259
Extras/EpisodeRewardMean                    -42.6766
LinearFeatureBaseline/ExplainedVariance       0.860495
PolicyExecTime                                0.0912995
ProcessExecTime                               0.0111458
TotalEnvSteps                            789360
policy/Entropy                               -1.89812
policy/KL                                     0.00662399
policy/KLBefore                               0
policy/LossAfter                             -0.0163628
policy/LossBefore                            -6.94995e-09
policy/Perplexity                             0.14985
policy/dLoss                                  0.0163628
---------------------------------------  ----------------
2022-04-23 14:23:46 | [train_policy] epoch #780 | Obtaining samples for iteration 780...
2022-04-23 14:23:46 | [train_policy] epoch #780 | Logging diagnostics...
2022-04-23 14:23:46 | [train_policy] epoch #780 | Optimizing policy...
2022-04-23 14:23:46 | [train_policy] epoch #780 | Computing loss before
2022-04-23 14:23:46 | [train_policy] epoch #780 | Computing KL before
2022-04-23 14:23:46 | [train_policy] epoch #780 | Optimizing
2022-04-23 14:23:46 | [train_policy] epoch #780 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:46 | [train_policy] epoch #780 | computing loss before
2022-04-23 14:23:46 | [train_policy] epoch #780 | computing gradient
2022-04-23 14:23:46 | [train_policy] epoch #780 | gradient computed
2022-04-23 14:23:46 | [train_policy] epoch #780 | computing descent direction
2022-04-23 14:23:46 | [train_policy] epoch #780 | descent direction computed
2022-04-23 14:23:46 | [train_policy] epoch #780 | backtrack iters: 0
2022-04-23 14:23:46 | [train_policy] epoch #780 | optimization finished
2022-04-23 14:23:46 | [train_policy] epoch #780 | Computing KL after
2022-04-23 14:23:46 | [train_policy] epoch #780 | Computing loss after
2022-04-23 14:23:46 | [train_policy] epoch #780 | Fitting baseline...
2022-04-23 14:23:46 | [train_policy] epoch #780 | Saving snapshot...
2022-04-23 14:23:46 | [train_policy] epoch #780 | Saved
2022-04-23 14:23:46 | [train_policy] epoch #780 | Time 274.19 s
2022-04-23 14:23:46 | [train_policy] epoch #780 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119271
Evaluation/AverageDiscountedReturn          -41.4371
Evaluation/AverageReturn                    -41.4371
Evaluation/CompletionRate                     0
Evaluation/Iteration                        780
Evaluation/MaxReturn                        -28.1714
Evaluation/MinReturn                        -72.3008
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3905
Extras/EpisodeRewardMean                    -41.4506
LinearFeatureBaseline/ExplainedVariance       0.848412
PolicyExecTime                                0.0918961
ProcessExecTime                               0.0111992
TotalEnvSteps                            790372
policy/Entropy                               -1.84931
policy/KL                                     0.00959691
policy/KLBefore                               0
policy/LossAfter                             -0.0228799
policy/LossBefore                            -2.59151e-09
policy/Perplexity                             0.157346
policy/dLoss                                  0.0228799
---------------------------------------  ----------------
2022-04-23 14:23:46 | [train_policy] epoch #781 | Obtaining samples for iteration 781...
2022-04-23 14:23:46 | [train_policy] epoch #781 | Logging diagnostics...
2022-04-23 14:23:46 | [train_policy] epoch #781 | Optimizing policy...
2022-04-23 14:23:46 | [train_policy] epoch #781 | Computing loss before
2022-04-23 14:23:46 | [train_policy] epoch #781 | Computing KL before
2022-04-23 14:23:46 | [train_policy] epoch #781 | Optimizing
2022-04-23 14:23:46 | [train_policy] epoch #781 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:46 | [train_policy] epoch #781 | computing loss before
2022-04-23 14:23:46 | [train_policy] epoch #781 | computing gradient
2022-04-23 14:23:46 | [train_policy] epoch #781 | gradient computed
2022-04-23 14:23:46 | [train_policy] epoch #781 | computing descent direction
2022-04-23 14:23:46 | [train_policy] epoch #781 | descent direction computed
2022-04-23 14:23:46 | [train_policy] epoch #781 | backtrack iters: 0
2022-04-23 14:23:46 | [train_policy] epoch #781 | optimization finished
2022-04-23 14:23:46 | [train_policy] epoch #781 | Computing KL after
2022-04-23 14:23:46 | [train_policy] epoch #781 | Computing loss after
2022-04-23 14:23:46 | [train_policy] epoch #781 | Fitting baseline...
2022-04-23 14:23:46 | [train_policy] epoch #781 | Saving snapshot...
2022-04-23 14:23:46 | [train_policy] epoch #781 | Saved
2022-04-23 14:23:46 | [train_policy] epoch #781 | Time 274.52 s
2022-04-23 14:23:46 | [train_policy] epoch #781 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120558
Evaluation/AverageDiscountedReturn          -41.2844
Evaluation/AverageReturn                    -41.2844
Evaluation/CompletionRate                     0
Evaluation/Iteration                        781
Evaluation/MaxReturn                        -30.658
Evaluation/MinReturn                        -72.8862
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.6341
Extras/EpisodeRewardMean                    -40.85
LinearFeatureBaseline/ExplainedVariance       0.862352
PolicyExecTime                                0.0971642
ProcessExecTime                               0.0116653
TotalEnvSteps                            791384
policy/Entropy                               -1.77347
policy/KL                                     0.00862418
policy/KLBefore                               0
policy/LossAfter                             -0.0165482
policy/LossBefore                            -2.12032e-09
policy/Perplexity                             0.169744
policy/dLoss                                  0.0165482
---------------------------------------  ----------------
2022-04-23 14:23:46 | [train_policy] epoch #782 | Obtaining samples for iteration 782...
2022-04-23 14:23:47 | [train_policy] epoch #782 | Logging diagnostics...
2022-04-23 14:23:47 | [train_policy] epoch #782 | Optimizing policy...
2022-04-23 14:23:47 | [train_policy] epoch #782 | Computing loss before
2022-04-23 14:23:47 | [train_policy] epoch #782 | Computing KL before
2022-04-23 14:23:47 | [train_policy] epoch #782 | Optimizing
2022-04-23 14:23:47 | [train_policy] epoch #782 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:47 | [train_policy] epoch #782 | computing loss before
2022-04-23 14:23:47 | [train_policy] epoch #782 | computing gradient
2022-04-23 14:23:47 | [train_policy] epoch #782 | gradient computed
2022-04-23 14:23:47 | [train_policy] epoch #782 | computing descent direction
2022-04-23 14:23:47 | [train_policy] epoch #782 | descent direction computed
2022-04-23 14:23:47 | [train_policy] epoch #782 | backtrack iters: 0
2022-04-23 14:23:47 | [train_policy] epoch #782 | optimization finished
2022-04-23 14:23:47 | [train_policy] epoch #782 | Computing KL after
2022-04-23 14:23:47 | [train_policy] epoch #782 | Computing loss after
2022-04-23 14:23:47 | [train_policy] epoch #782 | Fitting baseline...
2022-04-23 14:23:47 | [train_policy] epoch #782 | Saving snapshot...
2022-04-23 14:23:47 | [train_policy] epoch #782 | Saved
2022-04-23 14:23:47 | [train_policy] epoch #782 | Time 274.87 s
2022-04-23 14:23:47 | [train_policy] epoch #782 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.121544
Evaluation/AverageDiscountedReturn          -42.0265
Evaluation/AverageReturn                    -42.0265
Evaluation/CompletionRate                     0
Evaluation/Iteration                        782
Evaluation/MaxReturn                        -31.7198
Evaluation/MinReturn                        -72.7364
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.00861
Extras/EpisodeRewardMean                    -41.837
LinearFeatureBaseline/ExplainedVariance       0.853177
PolicyExecTime                                0.104464
ProcessExecTime                               0.0114384
TotalEnvSteps                            792396
policy/Entropy                               -1.75221
policy/KL                                     0.00989197
policy/KLBefore                               0
policy/LossAfter                             -0.0216169
policy/LossBefore                            -1.7905e-08
policy/Perplexity                             0.173391
policy/dLoss                                  0.0216169
---------------------------------------  ---------------
2022-04-23 14:23:47 | [train_policy] epoch #783 | Obtaining samples for iteration 783...
2022-04-23 14:23:47 | [train_policy] epoch #783 | Logging diagnostics...
2022-04-23 14:23:47 | [train_policy] epoch #783 | Optimizing policy...
2022-04-23 14:23:47 | [train_policy] epoch #783 | Computing loss before
2022-04-23 14:23:47 | [train_policy] epoch #783 | Computing KL before
2022-04-23 14:23:47 | [train_policy] epoch #783 | Optimizing
2022-04-23 14:23:47 | [train_policy] epoch #783 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:47 | [train_policy] epoch #783 | computing loss before
2022-04-23 14:23:47 | [train_policy] epoch #783 | computing gradient
2022-04-23 14:23:47 | [train_policy] epoch #783 | gradient computed
2022-04-23 14:23:47 | [train_policy] epoch #783 | computing descent direction
2022-04-23 14:23:47 | [train_policy] epoch #783 | descent direction computed
2022-04-23 14:23:47 | [train_policy] epoch #783 | backtrack iters: 0
2022-04-23 14:23:47 | [train_policy] epoch #783 | optimization finished
2022-04-23 14:23:47 | [train_policy] epoch #783 | Computing KL after
2022-04-23 14:23:47 | [train_policy] epoch #783 | Computing loss after
2022-04-23 14:23:47 | [train_policy] epoch #783 | Fitting baseline...
2022-04-23 14:23:47 | [train_policy] epoch #783 | Saving snapshot...
2022-04-23 14:23:47 | [train_policy] epoch #783 | Saved
2022-04-23 14:23:47 | [train_policy] epoch #783 | Time 275.22 s
2022-04-23 14:23:47 | [train_policy] epoch #783 | EpochTime 0.35 s
---------------------------------------  ----------------
EnvExecTime                                   0.120168
Evaluation/AverageDiscountedReturn          -40.4619
Evaluation/AverageReturn                    -40.4619
Evaluation/CompletionRate                     0
Evaluation/Iteration                        783
Evaluation/MaxReturn                        -28.66
Evaluation/MinReturn                        -63.7578
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.35917
Extras/EpisodeRewardMean                    -40.561
LinearFeatureBaseline/ExplainedVariance       0.883551
PolicyExecTime                                0.110212
ProcessExecTime                               0.0116787
TotalEnvSteps                            793408
policy/Entropy                               -1.71412
policy/KL                                     0.00961768
policy/KLBefore                               0
policy/LossAfter                             -0.0134273
policy/LossBefore                            -1.24863e-08
policy/Perplexity                             0.180123
policy/dLoss                                  0.0134273
---------------------------------------  ----------------
2022-04-23 14:23:47 | [train_policy] epoch #784 | Obtaining samples for iteration 784...
2022-04-23 14:23:47 | [train_policy] epoch #784 | Logging diagnostics...
2022-04-23 14:23:47 | [train_policy] epoch #784 | Optimizing policy...
2022-04-23 14:23:47 | [train_policy] epoch #784 | Computing loss before
2022-04-23 14:23:47 | [train_policy] epoch #784 | Computing KL before
2022-04-23 14:23:47 | [train_policy] epoch #784 | Optimizing
2022-04-23 14:23:47 | [train_policy] epoch #784 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:47 | [train_policy] epoch #784 | computing loss before
2022-04-23 14:23:47 | [train_policy] epoch #784 | computing gradient
2022-04-23 14:23:47 | [train_policy] epoch #784 | gradient computed
2022-04-23 14:23:47 | [train_policy] epoch #784 | computing descent direction
2022-04-23 14:23:47 | [train_policy] epoch #784 | descent direction computed
2022-04-23 14:23:47 | [train_policy] epoch #784 | backtrack iters: 0
2022-04-23 14:23:47 | [train_policy] epoch #784 | optimization finished
2022-04-23 14:23:47 | [train_policy] epoch #784 | Computing KL after
2022-04-23 14:23:47 | [train_policy] epoch #784 | Computing loss after
2022-04-23 14:23:47 | [train_policy] epoch #784 | Fitting baseline...
2022-04-23 14:23:47 | [train_policy] epoch #784 | Saving snapshot...
2022-04-23 14:23:47 | [train_policy] epoch #784 | Saved
2022-04-23 14:23:47 | [train_policy] epoch #784 | Time 275.58 s
2022-04-23 14:23:47 | [train_policy] epoch #784 | EpochTime 0.36 s
---------------------------------------  ----------------
EnvExecTime                                   0.119698
Evaluation/AverageDiscountedReturn          -40.72
Evaluation/AverageReturn                    -40.72
Evaluation/CompletionRate                     0
Evaluation/Iteration                        784
Evaluation/MaxReturn                        -29.4528
Evaluation/MinReturn                        -63.9498
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.68082
Extras/EpisodeRewardMean                    -40.5995
LinearFeatureBaseline/ExplainedVariance       0.885286
PolicyExecTime                                0.116708
ProcessExecTime                               0.0114172
TotalEnvSteps                            794420
policy/Entropy                               -1.64203
policy/KL                                     0.00947662
policy/KLBefore                               0
policy/LossAfter                             -0.0186526
policy/LossBefore                             2.59151e-09
policy/Perplexity                             0.193586
policy/dLoss                                  0.0186526
---------------------------------------  ----------------
2022-04-23 14:23:47 | [train_policy] epoch #785 | Obtaining samples for iteration 785...
2022-04-23 14:23:48 | [train_policy] epoch #785 | Logging diagnostics...
2022-04-23 14:23:48 | [train_policy] epoch #785 | Optimizing policy...
2022-04-23 14:23:48 | [train_policy] epoch #785 | Computing loss before
2022-04-23 14:23:48 | [train_policy] epoch #785 | Computing KL before
2022-04-23 14:23:48 | [train_policy] epoch #785 | Optimizing
2022-04-23 14:23:48 | [train_policy] epoch #785 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:48 | [train_policy] epoch #785 | computing loss before
2022-04-23 14:23:48 | [train_policy] epoch #785 | computing gradient
2022-04-23 14:23:48 | [train_policy] epoch #785 | gradient computed
2022-04-23 14:23:48 | [train_policy] epoch #785 | computing descent direction
2022-04-23 14:23:48 | [train_policy] epoch #785 | descent direction computed
2022-04-23 14:23:48 | [train_policy] epoch #785 | backtrack iters: 1
2022-04-23 14:23:48 | [train_policy] epoch #785 | optimization finished
2022-04-23 14:23:48 | [train_policy] epoch #785 | Computing KL after
2022-04-23 14:23:48 | [train_policy] epoch #785 | Computing loss after
2022-04-23 14:23:48 | [train_policy] epoch #785 | Fitting baseline...
2022-04-23 14:23:48 | [train_policy] epoch #785 | Saving snapshot...
2022-04-23 14:23:48 | [train_policy] epoch #785 | Saved
2022-04-23 14:23:48 | [train_policy] epoch #785 | Time 275.93 s
2022-04-23 14:23:48 | [train_policy] epoch #785 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119797
Evaluation/AverageDiscountedReturn          -41.6346
Evaluation/AverageReturn                    -41.6346
Evaluation/CompletionRate                     0
Evaluation/Iteration                        785
Evaluation/MaxReturn                        -29.2391
Evaluation/MinReturn                        -73.2436
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.68916
Extras/EpisodeRewardMean                    -41.6662
LinearFeatureBaseline/ExplainedVariance       0.85944
PolicyExecTime                                0.0999987
ProcessExecTime                               0.0116265
TotalEnvSteps                            795432
policy/Entropy                               -1.67043
policy/KL                                     0.00675725
policy/KLBefore                               0
policy/LossAfter                             -0.0154044
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.188166
policy/dLoss                                  0.0154044
---------------------------------------  ----------------
2022-04-23 14:23:48 | [train_policy] epoch #786 | Obtaining samples for iteration 786...
2022-04-23 14:23:48 | [train_policy] epoch #786 | Logging diagnostics...
2022-04-23 14:23:48 | [train_policy] epoch #786 | Optimizing policy...
2022-04-23 14:23:48 | [train_policy] epoch #786 | Computing loss before
2022-04-23 14:23:48 | [train_policy] epoch #786 | Computing KL before
2022-04-23 14:23:48 | [train_policy] epoch #786 | Optimizing
2022-04-23 14:23:48 | [train_policy] epoch #786 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:48 | [train_policy] epoch #786 | computing loss before
2022-04-23 14:23:48 | [train_policy] epoch #786 | computing gradient
2022-04-23 14:23:48 | [train_policy] epoch #786 | gradient computed
2022-04-23 14:23:48 | [train_policy] epoch #786 | computing descent direction
2022-04-23 14:23:48 | [train_policy] epoch #786 | descent direction computed
2022-04-23 14:23:48 | [train_policy] epoch #786 | backtrack iters: 0
2022-04-23 14:23:48 | [train_policy] epoch #786 | optimization finished
2022-04-23 14:23:48 | [train_policy] epoch #786 | Computing KL after
2022-04-23 14:23:48 | [train_policy] epoch #786 | Computing loss after
2022-04-23 14:23:48 | [train_policy] epoch #786 | Fitting baseline...
2022-04-23 14:23:48 | [train_policy] epoch #786 | Saving snapshot...
2022-04-23 14:23:48 | [train_policy] epoch #786 | Saved
2022-04-23 14:23:48 | [train_policy] epoch #786 | Time 276.28 s
2022-04-23 14:23:48 | [train_policy] epoch #786 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.11991
Evaluation/AverageDiscountedReturn          -41.8305
Evaluation/AverageReturn                    -41.8305
Evaluation/CompletionRate                     0
Evaluation/Iteration                        786
Evaluation/MaxReturn                        -29.3157
Evaluation/MinReturn                        -72.4114
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.24573
Extras/EpisodeRewardMean                    -41.5593
LinearFeatureBaseline/ExplainedVariance       0.867269
PolicyExecTime                                0.105815
ProcessExecTime                               0.0113044
TotalEnvSteps                            796444
policy/Entropy                               -1.64501
policy/KL                                     0.00942063
policy/KLBefore                               0
policy/LossAfter                             -0.0144514
policy/LossBefore                             4.73539e-08
policy/Perplexity                             0.193011
policy/dLoss                                  0.0144515
---------------------------------------  ----------------
2022-04-23 14:23:48 | [train_policy] epoch #787 | Obtaining samples for iteration 787...
2022-04-23 14:23:48 | [train_policy] epoch #787 | Logging diagnostics...
2022-04-23 14:23:48 | [train_policy] epoch #787 | Optimizing policy...
2022-04-23 14:23:48 | [train_policy] epoch #787 | Computing loss before
2022-04-23 14:23:48 | [train_policy] epoch #787 | Computing KL before
2022-04-23 14:23:48 | [train_policy] epoch #787 | Optimizing
2022-04-23 14:23:48 | [train_policy] epoch #787 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:48 | [train_policy] epoch #787 | computing loss before
2022-04-23 14:23:48 | [train_policy] epoch #787 | computing gradient
2022-04-23 14:23:48 | [train_policy] epoch #787 | gradient computed
2022-04-23 14:23:48 | [train_policy] epoch #787 | computing descent direction
2022-04-23 14:23:48 | [train_policy] epoch #787 | descent direction computed
2022-04-23 14:23:48 | [train_policy] epoch #787 | backtrack iters: 1
2022-04-23 14:23:48 | [train_policy] epoch #787 | optimization finished
2022-04-23 14:23:48 | [train_policy] epoch #787 | Computing KL after
2022-04-23 14:23:48 | [train_policy] epoch #787 | Computing loss after
2022-04-23 14:23:48 | [train_policy] epoch #787 | Fitting baseline...
2022-04-23 14:23:48 | [train_policy] epoch #787 | Saving snapshot...
2022-04-23 14:23:48 | [train_policy] epoch #787 | Saved
2022-04-23 14:23:48 | [train_policy] epoch #787 | Time 276.62 s
2022-04-23 14:23:48 | [train_policy] epoch #787 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.120617
Evaluation/AverageDiscountedReturn          -40.8037
Evaluation/AverageReturn                    -40.8037
Evaluation/CompletionRate                     0
Evaluation/Iteration                        787
Evaluation/MaxReturn                        -29.4228
Evaluation/MinReturn                        -63.6825
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.38173
Extras/EpisodeRewardMean                    -41.0696
LinearFeatureBaseline/ExplainedVariance       0.849303
PolicyExecTime                                0.100958
ProcessExecTime                               0.0112226
TotalEnvSteps                            797456
policy/Entropy                               -1.69168
policy/KL                                     0.00675474
policy/KLBefore                               0
policy/LossAfter                             -0.0163434
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.18421
policy/dLoss                                  0.0163434
---------------------------------------  ----------------
2022-04-23 14:23:48 | [train_policy] epoch #788 | Obtaining samples for iteration 788...
2022-04-23 14:23:49 | [train_policy] epoch #788 | Logging diagnostics...
2022-04-23 14:23:49 | [train_policy] epoch #788 | Optimizing policy...
2022-04-23 14:23:49 | [train_policy] epoch #788 | Computing loss before
2022-04-23 14:23:49 | [train_policy] epoch #788 | Computing KL before
2022-04-23 14:23:49 | [train_policy] epoch #788 | Optimizing
2022-04-23 14:23:49 | [train_policy] epoch #788 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:49 | [train_policy] epoch #788 | computing loss before
2022-04-23 14:23:49 | [train_policy] epoch #788 | computing gradient
2022-04-23 14:23:49 | [train_policy] epoch #788 | gradient computed
2022-04-23 14:23:49 | [train_policy] epoch #788 | computing descent direction
2022-04-23 14:23:49 | [train_policy] epoch #788 | descent direction computed
2022-04-23 14:23:49 | [train_policy] epoch #788 | backtrack iters: 0
2022-04-23 14:23:49 | [train_policy] epoch #788 | optimization finished
2022-04-23 14:23:49 | [train_policy] epoch #788 | Computing KL after
2022-04-23 14:23:49 | [train_policy] epoch #788 | Computing loss after
2022-04-23 14:23:49 | [train_policy] epoch #788 | Fitting baseline...
2022-04-23 14:23:49 | [train_policy] epoch #788 | Saving snapshot...
2022-04-23 14:23:49 | [train_policy] epoch #788 | Saved
2022-04-23 14:23:49 | [train_policy] epoch #788 | Time 276.96 s
2022-04-23 14:23:49 | [train_policy] epoch #788 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119326
Evaluation/AverageDiscountedReturn          -40.7614
Evaluation/AverageReturn                    -40.7614
Evaluation/CompletionRate                     0
Evaluation/Iteration                        788
Evaluation/MaxReturn                        -30.4354
Evaluation/MinReturn                        -71.5856
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.77422
Extras/EpisodeRewardMean                    -40.6107
LinearFeatureBaseline/ExplainedVariance       0.874868
PolicyExecTime                                0.107473
ProcessExecTime                               0.0113332
TotalEnvSteps                            798468
policy/Entropy                               -1.65327
policy/KL                                     0.00978229
policy/KLBefore                               0
policy/LossAfter                             -0.0205956
policy/LossBefore                            -9.54145e-09
policy/Perplexity                             0.191422
policy/dLoss                                  0.0205956
---------------------------------------  ----------------
2022-04-23 14:23:49 | [train_policy] epoch #789 | Obtaining samples for iteration 789...
2022-04-23 14:23:49 | [train_policy] epoch #789 | Logging diagnostics...
2022-04-23 14:23:49 | [train_policy] epoch #789 | Optimizing policy...
2022-04-23 14:23:49 | [train_policy] epoch #789 | Computing loss before
2022-04-23 14:23:49 | [train_policy] epoch #789 | Computing KL before
2022-04-23 14:23:49 | [train_policy] epoch #789 | Optimizing
2022-04-23 14:23:49 | [train_policy] epoch #789 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:49 | [train_policy] epoch #789 | computing loss before
2022-04-23 14:23:49 | [train_policy] epoch #789 | computing gradient
2022-04-23 14:23:49 | [train_policy] epoch #789 | gradient computed
2022-04-23 14:23:49 | [train_policy] epoch #789 | computing descent direction
2022-04-23 14:23:49 | [train_policy] epoch #789 | descent direction computed
2022-04-23 14:23:49 | [train_policy] epoch #789 | backtrack iters: 0
2022-04-23 14:23:49 | [train_policy] epoch #789 | optimization finished
2022-04-23 14:23:49 | [train_policy] epoch #789 | Computing KL after
2022-04-23 14:23:49 | [train_policy] epoch #789 | Computing loss after
2022-04-23 14:23:49 | [train_policy] epoch #789 | Fitting baseline...
2022-04-23 14:23:49 | [train_policy] epoch #789 | Saving snapshot...
2022-04-23 14:23:49 | [train_policy] epoch #789 | Saved
2022-04-23 14:23:49 | [train_policy] epoch #789 | Time 277.31 s
2022-04-23 14:23:49 | [train_policy] epoch #789 | EpochTime 0.34 s
---------------------------------------  ---------------
EnvExecTime                                   0.119493
Evaluation/AverageDiscountedReturn          -40.7566
Evaluation/AverageReturn                    -40.7566
Evaluation/CompletionRate                     0
Evaluation/Iteration                        789
Evaluation/MaxReturn                        -31.1523
Evaluation/MinReturn                        -71.3861
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.31637
Extras/EpisodeRewardMean                    -41.0888
LinearFeatureBaseline/ExplainedVariance       0.862456
PolicyExecTime                                0.102502
ProcessExecTime                               0.0115943
TotalEnvSteps                            799480
policy/Entropy                               -1.65398
policy/KL                                     0.0097793
policy/KLBefore                               0
policy/LossAfter                             -0.0116157
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.191287
policy/dLoss                                  0.0116157
---------------------------------------  ---------------
2022-04-23 14:23:49 | [train_policy] epoch #790 | Obtaining samples for iteration 790...
2022-04-23 14:23:49 | [train_policy] epoch #790 | Logging diagnostics...
2022-04-23 14:23:49 | [train_policy] epoch #790 | Optimizing policy...
2022-04-23 14:23:49 | [train_policy] epoch #790 | Computing loss before
2022-04-23 14:23:49 | [train_policy] epoch #790 | Computing KL before
2022-04-23 14:23:49 | [train_policy] epoch #790 | Optimizing
2022-04-23 14:23:49 | [train_policy] epoch #790 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:49 | [train_policy] epoch #790 | computing loss before
2022-04-23 14:23:49 | [train_policy] epoch #790 | computing gradient
2022-04-23 14:23:49 | [train_policy] epoch #790 | gradient computed
2022-04-23 14:23:49 | [train_policy] epoch #790 | computing descent direction
2022-04-23 14:23:49 | [train_policy] epoch #790 | descent direction computed
2022-04-23 14:23:49 | [train_policy] epoch #790 | backtrack iters: 1
2022-04-23 14:23:49 | [train_policy] epoch #790 | optimization finished
2022-04-23 14:23:49 | [train_policy] epoch #790 | Computing KL after
2022-04-23 14:23:49 | [train_policy] epoch #790 | Computing loss after
2022-04-23 14:23:49 | [train_policy] epoch #790 | Fitting baseline...
2022-04-23 14:23:49 | [train_policy] epoch #790 | Saving snapshot...
2022-04-23 14:23:49 | [train_policy] epoch #790 | Saved
2022-04-23 14:23:49 | [train_policy] epoch #790 | Time 277.64 s
2022-04-23 14:23:49 | [train_policy] epoch #790 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119664
Evaluation/AverageDiscountedReturn          -40.8024
Evaluation/AverageReturn                    -40.8024
Evaluation/CompletionRate                     0
Evaluation/Iteration                        790
Evaluation/MaxReturn                        -30.081
Evaluation/MinReturn                        -64.2002
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.68287
Extras/EpisodeRewardMean                    -40.7419
LinearFeatureBaseline/ExplainedVariance       0.879254
PolicyExecTime                                0.0956089
ProcessExecTime                               0.0112836
TotalEnvSteps                            800492
policy/Entropy                               -1.64664
policy/KL                                     0.00691788
policy/KLBefore                               0
policy/LossAfter                             -0.0179922
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.192697
policy/dLoss                                  0.0179922
---------------------------------------  ----------------
2022-04-23 14:23:49 | [train_policy] epoch #791 | Obtaining samples for iteration 791...
2022-04-23 14:23:50 | [train_policy] epoch #791 | Logging diagnostics...
2022-04-23 14:23:50 | [train_policy] epoch #791 | Optimizing policy...
2022-04-23 14:23:50 | [train_policy] epoch #791 | Computing loss before
2022-04-23 14:23:50 | [train_policy] epoch #791 | Computing KL before
2022-04-23 14:23:50 | [train_policy] epoch #791 | Optimizing
2022-04-23 14:23:50 | [train_policy] epoch #791 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:50 | [train_policy] epoch #791 | computing loss before
2022-04-23 14:23:50 | [train_policy] epoch #791 | computing gradient
2022-04-23 14:23:50 | [train_policy] epoch #791 | gradient computed
2022-04-23 14:23:50 | [train_policy] epoch #791 | computing descent direction
2022-04-23 14:23:50 | [train_policy] epoch #791 | descent direction computed
2022-04-23 14:23:50 | [train_policy] epoch #791 | backtrack iters: 1
2022-04-23 14:23:50 | [train_policy] epoch #791 | optimization finished
2022-04-23 14:23:50 | [train_policy] epoch #791 | Computing KL after
2022-04-23 14:23:50 | [train_policy] epoch #791 | Computing loss after
2022-04-23 14:23:50 | [train_policy] epoch #791 | Fitting baseline...
2022-04-23 14:23:50 | [train_policy] epoch #791 | Saving snapshot...
2022-04-23 14:23:50 | [train_policy] epoch #791 | Saved
2022-04-23 14:23:50 | [train_policy] epoch #791 | Time 277.97 s
2022-04-23 14:23:50 | [train_policy] epoch #791 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118617
Evaluation/AverageDiscountedReturn          -64.4665
Evaluation/AverageReturn                    -64.4665
Evaluation/CompletionRate                     0
Evaluation/Iteration                        791
Evaluation/MaxReturn                        -30.5283
Evaluation/MinReturn                      -2047.07
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.028
Extras/EpisodeRewardMean                    -62.6562
LinearFeatureBaseline/ExplainedVariance       0.00621966
PolicyExecTime                                0.0973015
ProcessExecTime                               0.0111339
TotalEnvSteps                            801504
policy/Entropy                               -1.67288
policy/KL                                     0.00751836
policy/KLBefore                               0
policy/LossAfter                             -0.0326544
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.187706
policy/dLoss                                  0.0326544
---------------------------------------  ----------------
2022-04-23 14:23:50 | [train_policy] epoch #792 | Obtaining samples for iteration 792...
2022-04-23 14:23:50 | [train_policy] epoch #792 | Logging diagnostics...
2022-04-23 14:23:50 | [train_policy] epoch #792 | Optimizing policy...
2022-04-23 14:23:50 | [train_policy] epoch #792 | Computing loss before
2022-04-23 14:23:50 | [train_policy] epoch #792 | Computing KL before
2022-04-23 14:23:50 | [train_policy] epoch #792 | Optimizing
2022-04-23 14:23:50 | [train_policy] epoch #792 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:50 | [train_policy] epoch #792 | computing loss before
2022-04-23 14:23:50 | [train_policy] epoch #792 | computing gradient
2022-04-23 14:23:50 | [train_policy] epoch #792 | gradient computed
2022-04-23 14:23:50 | [train_policy] epoch #792 | computing descent direction
2022-04-23 14:23:50 | [train_policy] epoch #792 | descent direction computed
2022-04-23 14:23:50 | [train_policy] epoch #792 | backtrack iters: 0
2022-04-23 14:23:50 | [train_policy] epoch #792 | optimization finished
2022-04-23 14:23:50 | [train_policy] epoch #792 | Computing KL after
2022-04-23 14:23:50 | [train_policy] epoch #792 | Computing loss after
2022-04-23 14:23:50 | [train_policy] epoch #792 | Fitting baseline...
2022-04-23 14:23:50 | [train_policy] epoch #792 | Saving snapshot...
2022-04-23 14:23:50 | [train_policy] epoch #792 | Saved
2022-04-23 14:23:50 | [train_policy] epoch #792 | Time 278.31 s
2022-04-23 14:23:50 | [train_policy] epoch #792 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119171
Evaluation/AverageDiscountedReturn          -40.9878
Evaluation/AverageReturn                    -40.9878
Evaluation/CompletionRate                     0
Evaluation/Iteration                        792
Evaluation/MaxReturn                        -30.2606
Evaluation/MinReturn                        -72.6753
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.87791
Extras/EpisodeRewardMean                    -61.3288
LinearFeatureBaseline/ExplainedVariance     -36.4345
PolicyExecTime                                0.0991571
ProcessExecTime                               0.0112362
TotalEnvSteps                            802516
policy/Entropy                               -1.6924
policy/KL                                     0.00979727
policy/KLBefore                               0
policy/LossAfter                             -0.0181853
policy/LossBefore                             1.27219e-08
policy/Perplexity                             0.184077
policy/dLoss                                  0.0181853
---------------------------------------  ----------------
2022-04-23 14:23:50 | [train_policy] epoch #793 | Obtaining samples for iteration 793...
2022-04-23 14:23:50 | [train_policy] epoch #793 | Logging diagnostics...
2022-04-23 14:23:50 | [train_policy] epoch #793 | Optimizing policy...
2022-04-23 14:23:50 | [train_policy] epoch #793 | Computing loss before
2022-04-23 14:23:50 | [train_policy] epoch #793 | Computing KL before
2022-04-23 14:23:50 | [train_policy] epoch #793 | Optimizing
2022-04-23 14:23:50 | [train_policy] epoch #793 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:50 | [train_policy] epoch #793 | computing loss before
2022-04-23 14:23:50 | [train_policy] epoch #793 | computing gradient
2022-04-23 14:23:50 | [train_policy] epoch #793 | gradient computed
2022-04-23 14:23:50 | [train_policy] epoch #793 | computing descent direction
2022-04-23 14:23:50 | [train_policy] epoch #793 | descent direction computed
2022-04-23 14:23:50 | [train_policy] epoch #793 | backtrack iters: 1
2022-04-23 14:23:50 | [train_policy] epoch #793 | optimization finished
2022-04-23 14:23:50 | [train_policy] epoch #793 | Computing KL after
2022-04-23 14:23:50 | [train_policy] epoch #793 | Computing loss after
2022-04-23 14:23:50 | [train_policy] epoch #793 | Fitting baseline...
2022-04-23 14:23:50 | [train_policy] epoch #793 | Saving snapshot...
2022-04-23 14:23:50 | [train_policy] epoch #793 | Saved
2022-04-23 14:23:50 | [train_policy] epoch #793 | Time 278.65 s
2022-04-23 14:23:50 | [train_policy] epoch #793 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119684
Evaluation/AverageDiscountedReturn          -40.9495
Evaluation/AverageReturn                    -40.9495
Evaluation/CompletionRate                     0
Evaluation/Iteration                        793
Evaluation/MaxReturn                        -31.0723
Evaluation/MinReturn                        -72.5654
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.89203
Extras/EpisodeRewardMean                    -40.9566
LinearFeatureBaseline/ExplainedVariance       0.855021
PolicyExecTime                                0.0960855
ProcessExecTime                               0.0111914
TotalEnvSteps                            803528
policy/Entropy                               -1.71244
policy/KL                                     0.00640884
policy/KLBefore                               0
policy/LossAfter                             -0.0504265
policy/LossBefore                            -2.59151e-09
policy/Perplexity                             0.180425
policy/dLoss                                  0.0504265
---------------------------------------  ----------------
2022-04-23 14:23:50 | [train_policy] epoch #794 | Obtaining samples for iteration 794...
2022-04-23 14:23:51 | [train_policy] epoch #794 | Logging diagnostics...
2022-04-23 14:23:51 | [train_policy] epoch #794 | Optimizing policy...
2022-04-23 14:23:51 | [train_policy] epoch #794 | Computing loss before
2022-04-23 14:23:51 | [train_policy] epoch #794 | Computing KL before
2022-04-23 14:23:51 | [train_policy] epoch #794 | Optimizing
2022-04-23 14:23:51 | [train_policy] epoch #794 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:51 | [train_policy] epoch #794 | computing loss before
2022-04-23 14:23:51 | [train_policy] epoch #794 | computing gradient
2022-04-23 14:23:51 | [train_policy] epoch #794 | gradient computed
2022-04-23 14:23:51 | [train_policy] epoch #794 | computing descent direction
2022-04-23 14:23:51 | [train_policy] epoch #794 | descent direction computed
2022-04-23 14:23:51 | [train_policy] epoch #794 | backtrack iters: 1
2022-04-23 14:23:51 | [train_policy] epoch #794 | optimization finished
2022-04-23 14:23:51 | [train_policy] epoch #794 | Computing KL after
2022-04-23 14:23:51 | [train_policy] epoch #794 | Computing loss after
2022-04-23 14:23:51 | [train_policy] epoch #794 | Fitting baseline...
2022-04-23 14:23:51 | [train_policy] epoch #794 | Saving snapshot...
2022-04-23 14:23:51 | [train_policy] epoch #794 | Saved
2022-04-23 14:23:51 | [train_policy] epoch #794 | Time 278.99 s
2022-04-23 14:23:51 | [train_policy] epoch #794 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119866
Evaluation/AverageDiscountedReturn          -42.652
Evaluation/AverageReturn                    -42.652
Evaluation/CompletionRate                     0
Evaluation/Iteration                        794
Evaluation/MaxReturn                        -31.4252
Evaluation/MinReturn                        -71.657
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.93419
Extras/EpisodeRewardMean                    -42.2294
LinearFeatureBaseline/ExplainedVariance       0.837019
PolicyExecTime                                0.102398
ProcessExecTime                               0.0112092
TotalEnvSteps                            804540
policy/Entropy                               -1.74936
policy/KL                                     0.00724838
policy/KLBefore                               0
policy/LossAfter                             -0.0165927
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.173885
policy/dLoss                                  0.0165927
---------------------------------------  ----------------
2022-04-23 14:23:51 | [train_policy] epoch #795 | Obtaining samples for iteration 795...
2022-04-23 14:23:51 | [train_policy] epoch #795 | Logging diagnostics...
2022-04-23 14:23:51 | [train_policy] epoch #795 | Optimizing policy...
2022-04-23 14:23:51 | [train_policy] epoch #795 | Computing loss before
2022-04-23 14:23:51 | [train_policy] epoch #795 | Computing KL before
2022-04-23 14:23:51 | [train_policy] epoch #795 | Optimizing
2022-04-23 14:23:51 | [train_policy] epoch #795 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:51 | [train_policy] epoch #795 | computing loss before
2022-04-23 14:23:51 | [train_policy] epoch #795 | computing gradient
2022-04-23 14:23:51 | [train_policy] epoch #795 | gradient computed
2022-04-23 14:23:51 | [train_policy] epoch #795 | computing descent direction
2022-04-23 14:23:51 | [train_policy] epoch #795 | descent direction computed
2022-04-23 14:23:51 | [train_policy] epoch #795 | backtrack iters: 1
2022-04-23 14:23:51 | [train_policy] epoch #795 | optimization finished
2022-04-23 14:23:51 | [train_policy] epoch #795 | Computing KL after
2022-04-23 14:23:51 | [train_policy] epoch #795 | Computing loss after
2022-04-23 14:23:51 | [train_policy] epoch #795 | Fitting baseline...
2022-04-23 14:23:51 | [train_policy] epoch #795 | Saving snapshot...
2022-04-23 14:23:51 | [train_policy] epoch #795 | Saved
2022-04-23 14:23:51 | [train_policy] epoch #795 | Time 279.33 s
2022-04-23 14:23:51 | [train_policy] epoch #795 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119432
Evaluation/AverageDiscountedReturn          -41.3729
Evaluation/AverageReturn                    -41.3729
Evaluation/CompletionRate                     0
Evaluation/Iteration                        795
Evaluation/MaxReturn                        -31.6783
Evaluation/MinReturn                        -64.7868
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.29205
Extras/EpisodeRewardMean                    -41.1158
LinearFeatureBaseline/ExplainedVariance       0.888644
PolicyExecTime                                0.0979998
ProcessExecTime                               0.0112336
TotalEnvSteps                            805552
policy/Entropy                               -1.77423
policy/KL                                     0.00651091
policy/KLBefore                               0
policy/LossAfter                             -0.0145157
policy/LossBefore                            -6.59656e-09
policy/Perplexity                             0.169614
policy/dLoss                                  0.0145157
---------------------------------------  ----------------
2022-04-23 14:23:51 | [train_policy] epoch #796 | Obtaining samples for iteration 796...
2022-04-23 14:23:51 | [train_policy] epoch #796 | Logging diagnostics...
2022-04-23 14:23:51 | [train_policy] epoch #796 | Optimizing policy...
2022-04-23 14:23:51 | [train_policy] epoch #796 | Computing loss before
2022-04-23 14:23:51 | [train_policy] epoch #796 | Computing KL before
2022-04-23 14:23:51 | [train_policy] epoch #796 | Optimizing
2022-04-23 14:23:51 | [train_policy] epoch #796 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:51 | [train_policy] epoch #796 | computing loss before
2022-04-23 14:23:51 | [train_policy] epoch #796 | computing gradient
2022-04-23 14:23:51 | [train_policy] epoch #796 | gradient computed
2022-04-23 14:23:51 | [train_policy] epoch #796 | computing descent direction
2022-04-23 14:23:51 | [train_policy] epoch #796 | descent direction computed
2022-04-23 14:23:51 | [train_policy] epoch #796 | backtrack iters: 0
2022-04-23 14:23:51 | [train_policy] epoch #796 | optimization finished
2022-04-23 14:23:51 | [train_policy] epoch #796 | Computing KL after
2022-04-23 14:23:51 | [train_policy] epoch #796 | Computing loss after
2022-04-23 14:23:51 | [train_policy] epoch #796 | Fitting baseline...
2022-04-23 14:23:51 | [train_policy] epoch #796 | Saving snapshot...
2022-04-23 14:23:51 | [train_policy] epoch #796 | Saved
2022-04-23 14:23:51 | [train_policy] epoch #796 | Time 279.66 s
2022-04-23 14:23:51 | [train_policy] epoch #796 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11938
Evaluation/AverageDiscountedReturn          -41.6972
Evaluation/AverageReturn                    -41.6972
Evaluation/CompletionRate                     0
Evaluation/Iteration                        796
Evaluation/MaxReturn                        -30.8448
Evaluation/MinReturn                        -57.7653
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.41042
Extras/EpisodeRewardMean                    -41.8207
LinearFeatureBaseline/ExplainedVariance       0.863829
PolicyExecTime                                0.0971353
ProcessExecTime                               0.0113621
TotalEnvSteps                            806564
policy/Entropy                               -1.77069
policy/KL                                     0.00984749
policy/KLBefore                               0
policy/LossAfter                             -0.0197191
policy/LossBefore                             1.08372e-08
policy/Perplexity                             0.170216
policy/dLoss                                  0.0197191
---------------------------------------  ----------------
2022-04-23 14:23:51 | [train_policy] epoch #797 | Obtaining samples for iteration 797...
2022-04-23 14:23:52 | [train_policy] epoch #797 | Logging diagnostics...
2022-04-23 14:23:52 | [train_policy] epoch #797 | Optimizing policy...
2022-04-23 14:23:52 | [train_policy] epoch #797 | Computing loss before
2022-04-23 14:23:52 | [train_policy] epoch #797 | Computing KL before
2022-04-23 14:23:52 | [train_policy] epoch #797 | Optimizing
2022-04-23 14:23:52 | [train_policy] epoch #797 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:52 | [train_policy] epoch #797 | computing loss before
2022-04-23 14:23:52 | [train_policy] epoch #797 | computing gradient
2022-04-23 14:23:52 | [train_policy] epoch #797 | gradient computed
2022-04-23 14:23:52 | [train_policy] epoch #797 | computing descent direction
2022-04-23 14:23:52 | [train_policy] epoch #797 | descent direction computed
2022-04-23 14:23:52 | [train_policy] epoch #797 | backtrack iters: 0
2022-04-23 14:23:52 | [train_policy] epoch #797 | optimization finished
2022-04-23 14:23:52 | [train_policy] epoch #797 | Computing KL after
2022-04-23 14:23:52 | [train_policy] epoch #797 | Computing loss after
2022-04-23 14:23:52 | [train_policy] epoch #797 | Fitting baseline...
2022-04-23 14:23:52 | [train_policy] epoch #797 | Saving snapshot...
2022-04-23 14:23:52 | [train_policy] epoch #797 | Saved
2022-04-23 14:23:52 | [train_policy] epoch #797 | Time 279.99 s
2022-04-23 14:23:52 | [train_policy] epoch #797 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120031
Evaluation/AverageDiscountedReturn          -40.3599
Evaluation/AverageReturn                    -40.3599
Evaluation/CompletionRate                     0
Evaluation/Iteration                        797
Evaluation/MaxReturn                        -29.7003
Evaluation/MinReturn                        -63.6963
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.96875
Extras/EpisodeRewardMean                    -40.3967
LinearFeatureBaseline/ExplainedVariance       0.873569
PolicyExecTime                                0.09798
ProcessExecTime                               0.0113113
TotalEnvSteps                            807576
policy/Entropy                               -1.75794
policy/KL                                     0.00981193
policy/KLBefore                               0
policy/LossAfter                             -0.0134573
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.1724
policy/dLoss                                  0.0134573
---------------------------------------  ----------------
2022-04-23 14:23:52 | [train_policy] epoch #798 | Obtaining samples for iteration 798...
2022-04-23 14:23:52 | [train_policy] epoch #798 | Logging diagnostics...
2022-04-23 14:23:52 | [train_policy] epoch #798 | Optimizing policy...
2022-04-23 14:23:52 | [train_policy] epoch #798 | Computing loss before
2022-04-23 14:23:52 | [train_policy] epoch #798 | Computing KL before
2022-04-23 14:23:52 | [train_policy] epoch #798 | Optimizing
2022-04-23 14:23:52 | [train_policy] epoch #798 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:52 | [train_policy] epoch #798 | computing loss before
2022-04-23 14:23:52 | [train_policy] epoch #798 | computing gradient
2022-04-23 14:23:52 | [train_policy] epoch #798 | gradient computed
2022-04-23 14:23:52 | [train_policy] epoch #798 | computing descent direction
2022-04-23 14:23:52 | [train_policy] epoch #798 | descent direction computed
2022-04-23 14:23:52 | [train_policy] epoch #798 | backtrack iters: 0
2022-04-23 14:23:52 | [train_policy] epoch #798 | optimization finished
2022-04-23 14:23:52 | [train_policy] epoch #798 | Computing KL after
2022-04-23 14:23:52 | [train_policy] epoch #798 | Computing loss after
2022-04-23 14:23:52 | [train_policy] epoch #798 | Fitting baseline...
2022-04-23 14:23:52 | [train_policy] epoch #798 | Saving snapshot...
2022-04-23 14:23:52 | [train_policy] epoch #798 | Saved
2022-04-23 14:23:52 | [train_policy] epoch #798 | Time 280.32 s
2022-04-23 14:23:52 | [train_policy] epoch #798 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119652
Evaluation/AverageDiscountedReturn          -40.9415
Evaluation/AverageReturn                    -40.9415
Evaluation/CompletionRate                     0
Evaluation/Iteration                        798
Evaluation/MaxReturn                        -30.0337
Evaluation/MinReturn                        -72.4362
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.34716
Extras/EpisodeRewardMean                    -41.0402
LinearFeatureBaseline/ExplainedVariance       0.845227
PolicyExecTime                                0.0972481
ProcessExecTime                               0.0111837
TotalEnvSteps                            808588
policy/Entropy                               -1.76645
policy/KL                                     0.00982762
policy/KLBefore                               0
policy/LossAfter                             -0.0186125
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.170938
policy/dLoss                                  0.0186125
---------------------------------------  ----------------
2022-04-23 14:23:52 | [train_policy] epoch #799 | Obtaining samples for iteration 799...
2022-04-23 14:23:52 | [train_policy] epoch #799 | Logging diagnostics...
2022-04-23 14:23:52 | [train_policy] epoch #799 | Optimizing policy...
2022-04-23 14:23:52 | [train_policy] epoch #799 | Computing loss before
2022-04-23 14:23:52 | [train_policy] epoch #799 | Computing KL before
2022-04-23 14:23:52 | [train_policy] epoch #799 | Optimizing
2022-04-23 14:23:52 | [train_policy] epoch #799 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:52 | [train_policy] epoch #799 | computing loss before
2022-04-23 14:23:52 | [train_policy] epoch #799 | computing gradient
2022-04-23 14:23:52 | [train_policy] epoch #799 | gradient computed
2022-04-23 14:23:52 | [train_policy] epoch #799 | computing descent direction
2022-04-23 14:23:52 | [train_policy] epoch #799 | descent direction computed
2022-04-23 14:23:52 | [train_policy] epoch #799 | backtrack iters: 1
2022-04-23 14:23:52 | [train_policy] epoch #799 | optimization finished
2022-04-23 14:23:52 | [train_policy] epoch #799 | Computing KL after
2022-04-23 14:23:52 | [train_policy] epoch #799 | Computing loss after
2022-04-23 14:23:52 | [train_policy] epoch #799 | Fitting baseline...
2022-04-23 14:23:52 | [train_policy] epoch #799 | Saving snapshot...
2022-04-23 14:23:52 | [train_policy] epoch #799 | Saved
2022-04-23 14:23:52 | [train_policy] epoch #799 | Time 280.65 s
2022-04-23 14:23:52 | [train_policy] epoch #799 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120099
Evaluation/AverageDiscountedReturn          -40.6619
Evaluation/AverageReturn                    -40.6619
Evaluation/CompletionRate                     0
Evaluation/Iteration                        799
Evaluation/MaxReturn                        -30.3226
Evaluation/MinReturn                        -73.3959
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.5499
Extras/EpisodeRewardMean                    -41.0471
LinearFeatureBaseline/ExplainedVariance       0.892623
PolicyExecTime                                0.0994751
ProcessExecTime                               0.0113246
TotalEnvSteps                            809600
policy/Entropy                               -1.79392
policy/KL                                     0.00640357
policy/KLBefore                               0
policy/LossAfter                             -0.0200479
policy/LossBefore                             4.71183e-10
policy/Perplexity                             0.166306
policy/dLoss                                  0.0200479
---------------------------------------  ----------------
2022-04-23 14:23:52 | [train_policy] epoch #800 | Obtaining samples for iteration 800...
2022-04-23 14:23:53 | [train_policy] epoch #800 | Logging diagnostics...
2022-04-23 14:23:53 | [train_policy] epoch #800 | Optimizing policy...
2022-04-23 14:23:53 | [train_policy] epoch #800 | Computing loss before
2022-04-23 14:23:53 | [train_policy] epoch #800 | Computing KL before
2022-04-23 14:23:53 | [train_policy] epoch #800 | Optimizing
2022-04-23 14:23:53 | [train_policy] epoch #800 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:53 | [train_policy] epoch #800 | computing loss before
2022-04-23 14:23:53 | [train_policy] epoch #800 | computing gradient
2022-04-23 14:23:53 | [train_policy] epoch #800 | gradient computed
2022-04-23 14:23:53 | [train_policy] epoch #800 | computing descent direction
2022-04-23 14:23:53 | [train_policy] epoch #800 | descent direction computed
2022-04-23 14:23:53 | [train_policy] epoch #800 | backtrack iters: 1
2022-04-23 14:23:53 | [train_policy] epoch #800 | optimization finished
2022-04-23 14:23:53 | [train_policy] epoch #800 | Computing KL after
2022-04-23 14:23:53 | [train_policy] epoch #800 | Computing loss after
2022-04-23 14:23:53 | [train_policy] epoch #800 | Fitting baseline...
2022-04-23 14:23:53 | [train_policy] epoch #800 | Saving snapshot...
2022-04-23 14:23:53 | [train_policy] epoch #800 | Saved
2022-04-23 14:23:53 | [train_policy] epoch #800 | Time 280.98 s
2022-04-23 14:23:53 | [train_policy] epoch #800 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119716
Evaluation/AverageDiscountedReturn          -41.7453
Evaluation/AverageReturn                    -41.7453
Evaluation/CompletionRate                     0
Evaluation/Iteration                        800
Evaluation/MaxReturn                        -31.0066
Evaluation/MinReturn                        -64.555
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.28939
Extras/EpisodeRewardMean                    -42.361
LinearFeatureBaseline/ExplainedVariance       0.889688
PolicyExecTime                                0.0989163
ProcessExecTime                               0.0114396
TotalEnvSteps                            810612
policy/Entropy                               -1.81557
policy/KL                                     0.00650097
policy/KLBefore                               0
policy/LossAfter                             -0.012459
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.162745
policy/dLoss                                  0.012459
---------------------------------------  ----------------
2022-04-23 14:23:53 | [train_policy] epoch #801 | Obtaining samples for iteration 801...
2022-04-23 14:23:53 | [train_policy] epoch #801 | Logging diagnostics...
2022-04-23 14:23:53 | [train_policy] epoch #801 | Optimizing policy...
2022-04-23 14:23:53 | [train_policy] epoch #801 | Computing loss before
2022-04-23 14:23:53 | [train_policy] epoch #801 | Computing KL before
2022-04-23 14:23:53 | [train_policy] epoch #801 | Optimizing
2022-04-23 14:23:53 | [train_policy] epoch #801 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:53 | [train_policy] epoch #801 | computing loss before
2022-04-23 14:23:53 | [train_policy] epoch #801 | computing gradient
2022-04-23 14:23:53 | [train_policy] epoch #801 | gradient computed
2022-04-23 14:23:53 | [train_policy] epoch #801 | computing descent direction
2022-04-23 14:23:53 | [train_policy] epoch #801 | descent direction computed
2022-04-23 14:23:53 | [train_policy] epoch #801 | backtrack iters: 1
2022-04-23 14:23:53 | [train_policy] epoch #801 | optimization finished
2022-04-23 14:23:53 | [train_policy] epoch #801 | Computing KL after
2022-04-23 14:23:53 | [train_policy] epoch #801 | Computing loss after
2022-04-23 14:23:53 | [train_policy] epoch #801 | Fitting baseline...
2022-04-23 14:23:53 | [train_policy] epoch #801 | Saving snapshot...
2022-04-23 14:23:53 | [train_policy] epoch #801 | Saved
2022-04-23 14:23:53 | [train_policy] epoch #801 | Time 281.30 s
2022-04-23 14:23:53 | [train_policy] epoch #801 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119389
Evaluation/AverageDiscountedReturn          -40.8077
Evaluation/AverageReturn                    -40.8077
Evaluation/CompletionRate                     0
Evaluation/Iteration                        801
Evaluation/MaxReturn                        -31.0468
Evaluation/MinReturn                        -72.8329
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.4022
Extras/EpisodeRewardMean                    -41.2724
LinearFeatureBaseline/ExplainedVariance       0.873292
PolicyExecTime                                0.0899425
ProcessExecTime                               0.0110805
TotalEnvSteps                            811624
policy/Entropy                               -1.82867
policy/KL                                     0.00653628
policy/KLBefore                               0
policy/LossAfter                             -0.00937668
policy/LossBefore                             4.82963e-09
policy/Perplexity                             0.160627
policy/dLoss                                  0.00937669
---------------------------------------  ----------------
2022-04-23 14:23:53 | [train_policy] epoch #802 | Obtaining samples for iteration 802...
2022-04-23 14:23:53 | [train_policy] epoch #802 | Logging diagnostics...
2022-04-23 14:23:53 | [train_policy] epoch #802 | Optimizing policy...
2022-04-23 14:23:53 | [train_policy] epoch #802 | Computing loss before
2022-04-23 14:23:53 | [train_policy] epoch #802 | Computing KL before
2022-04-23 14:23:53 | [train_policy] epoch #802 | Optimizing
2022-04-23 14:23:53 | [train_policy] epoch #802 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:53 | [train_policy] epoch #802 | computing loss before
2022-04-23 14:23:53 | [train_policy] epoch #802 | computing gradient
2022-04-23 14:23:53 | [train_policy] epoch #802 | gradient computed
2022-04-23 14:23:53 | [train_policy] epoch #802 | computing descent direction
2022-04-23 14:23:53 | [train_policy] epoch #802 | descent direction computed
2022-04-23 14:23:53 | [train_policy] epoch #802 | backtrack iters: 1
2022-04-23 14:23:53 | [train_policy] epoch #802 | optimization finished
2022-04-23 14:23:53 | [train_policy] epoch #802 | Computing KL after
2022-04-23 14:23:53 | [train_policy] epoch #802 | Computing loss after
2022-04-23 14:23:53 | [train_policy] epoch #802 | Fitting baseline...
2022-04-23 14:23:53 | [train_policy] epoch #802 | Saving snapshot...
2022-04-23 14:23:53 | [train_policy] epoch #802 | Saved
2022-04-23 14:23:53 | [train_policy] epoch #802 | Time 281.63 s
2022-04-23 14:23:53 | [train_policy] epoch #802 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118479
Evaluation/AverageDiscountedReturn          -41.3955
Evaluation/AverageReturn                    -41.3955
Evaluation/CompletionRate                     0
Evaluation/Iteration                        802
Evaluation/MaxReturn                        -29.7082
Evaluation/MinReturn                        -72.2382
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.43161
Extras/EpisodeRewardMean                    -41.1098
LinearFeatureBaseline/ExplainedVariance       0.877045
PolicyExecTime                                0.0956151
ProcessExecTime                               0.0115275
TotalEnvSteps                            812636
policy/Entropy                               -1.78342
policy/KL                                     0.00660208
policy/KLBefore                               0
policy/LossAfter                             -0.0142805
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.168063
policy/dLoss                                  0.0142805
---------------------------------------  ----------------
2022-04-23 14:23:53 | [train_policy] epoch #803 | Obtaining samples for iteration 803...
2022-04-23 14:23:54 | [train_policy] epoch #803 | Logging diagnostics...
2022-04-23 14:23:54 | [train_policy] epoch #803 | Optimizing policy...
2022-04-23 14:23:54 | [train_policy] epoch #803 | Computing loss before
2022-04-23 14:23:54 | [train_policy] epoch #803 | Computing KL before
2022-04-23 14:23:54 | [train_policy] epoch #803 | Optimizing
2022-04-23 14:23:54 | [train_policy] epoch #803 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:54 | [train_policy] epoch #803 | computing loss before
2022-04-23 14:23:54 | [train_policy] epoch #803 | computing gradient
2022-04-23 14:23:54 | [train_policy] epoch #803 | gradient computed
2022-04-23 14:23:54 | [train_policy] epoch #803 | computing descent direction
2022-04-23 14:23:54 | [train_policy] epoch #803 | descent direction computed
2022-04-23 14:23:54 | [train_policy] epoch #803 | backtrack iters: 1
2022-04-23 14:23:54 | [train_policy] epoch #803 | optimization finished
2022-04-23 14:23:54 | [train_policy] epoch #803 | Computing KL after
2022-04-23 14:23:54 | [train_policy] epoch #803 | Computing loss after
2022-04-23 14:23:54 | [train_policy] epoch #803 | Fitting baseline...
2022-04-23 14:23:54 | [train_policy] epoch #803 | Saving snapshot...
2022-04-23 14:23:54 | [train_policy] epoch #803 | Saved
2022-04-23 14:23:54 | [train_policy] epoch #803 | Time 281.95 s
2022-04-23 14:23:54 | [train_policy] epoch #803 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118971
Evaluation/AverageDiscountedReturn          -42.586
Evaluation/AverageReturn                    -42.586
Evaluation/CompletionRate                     0
Evaluation/Iteration                        803
Evaluation/MaxReturn                        -29.1206
Evaluation/MinReturn                        -72.7036
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.55382
Extras/EpisodeRewardMean                    -42.5609
LinearFeatureBaseline/ExplainedVariance       0.827249
PolicyExecTime                                0.0911763
ProcessExecTime                               0.0112054
TotalEnvSteps                            813648
policy/Entropy                               -1.80737
policy/KL                                     0.00672513
policy/KLBefore                               0
policy/LossAfter                             -0.0187824
policy/LossBefore                            -2.12032e-09
policy/Perplexity                             0.164085
policy/dLoss                                  0.0187824
---------------------------------------  ----------------
2022-04-23 14:23:54 | [train_policy] epoch #804 | Obtaining samples for iteration 804...
2022-04-23 14:23:54 | [train_policy] epoch #804 | Logging diagnostics...
2022-04-23 14:23:54 | [train_policy] epoch #804 | Optimizing policy...
2022-04-23 14:23:54 | [train_policy] epoch #804 | Computing loss before
2022-04-23 14:23:54 | [train_policy] epoch #804 | Computing KL before
2022-04-23 14:23:54 | [train_policy] epoch #804 | Optimizing
2022-04-23 14:23:54 | [train_policy] epoch #804 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:54 | [train_policy] epoch #804 | computing loss before
2022-04-23 14:23:54 | [train_policy] epoch #804 | computing gradient
2022-04-23 14:23:54 | [train_policy] epoch #804 | gradient computed
2022-04-23 14:23:54 | [train_policy] epoch #804 | computing descent direction
2022-04-23 14:23:54 | [train_policy] epoch #804 | descent direction computed
2022-04-23 14:23:54 | [train_policy] epoch #804 | backtrack iters: 1
2022-04-23 14:23:54 | [train_policy] epoch #804 | optimization finished
2022-04-23 14:23:54 | [train_policy] epoch #804 | Computing KL after
2022-04-23 14:23:54 | [train_policy] epoch #804 | Computing loss after
2022-04-23 14:23:54 | [train_policy] epoch #804 | Fitting baseline...
2022-04-23 14:23:54 | [train_policy] epoch #804 | Saving snapshot...
2022-04-23 14:23:54 | [train_policy] epoch #804 | Saved
2022-04-23 14:23:54 | [train_policy] epoch #804 | Time 282.28 s
2022-04-23 14:23:54 | [train_policy] epoch #804 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118963
Evaluation/AverageDiscountedReturn          -41.6395
Evaluation/AverageReturn                    -41.6395
Evaluation/CompletionRate                     0
Evaluation/Iteration                        804
Evaluation/MaxReturn                        -29.3911
Evaluation/MinReturn                        -72.337
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.97635
Extras/EpisodeRewardMean                    -41.6335
LinearFeatureBaseline/ExplainedVariance       0.859246
PolicyExecTime                                0.0927136
ProcessExecTime                               0.0111432
TotalEnvSteps                            814660
policy/Entropy                               -1.82972
policy/KL                                     0.0066697
policy/KLBefore                               0
policy/LossAfter                             -0.0175562
policy/LossBefore                            -8.95248e-09
policy/Perplexity                             0.160459
policy/dLoss                                  0.0175562
---------------------------------------  ----------------
2022-04-23 14:23:54 | [train_policy] epoch #805 | Obtaining samples for iteration 805...
2022-04-23 14:23:54 | [train_policy] epoch #805 | Logging diagnostics...
2022-04-23 14:23:54 | [train_policy] epoch #805 | Optimizing policy...
2022-04-23 14:23:54 | [train_policy] epoch #805 | Computing loss before
2022-04-23 14:23:54 | [train_policy] epoch #805 | Computing KL before
2022-04-23 14:23:54 | [train_policy] epoch #805 | Optimizing
2022-04-23 14:23:54 | [train_policy] epoch #805 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:54 | [train_policy] epoch #805 | computing loss before
2022-04-23 14:23:54 | [train_policy] epoch #805 | computing gradient
2022-04-23 14:23:54 | [train_policy] epoch #805 | gradient computed
2022-04-23 14:23:54 | [train_policy] epoch #805 | computing descent direction
2022-04-23 14:23:54 | [train_policy] epoch #805 | descent direction computed
2022-04-23 14:23:54 | [train_policy] epoch #805 | backtrack iters: 0
2022-04-23 14:23:54 | [train_policy] epoch #805 | optimization finished
2022-04-23 14:23:54 | [train_policy] epoch #805 | Computing KL after
2022-04-23 14:23:54 | [train_policy] epoch #805 | Computing loss after
2022-04-23 14:23:54 | [train_policy] epoch #805 | Fitting baseline...
2022-04-23 14:23:54 | [train_policy] epoch #805 | Saving snapshot...
2022-04-23 14:23:54 | [train_policy] epoch #805 | Saved
2022-04-23 14:23:54 | [train_policy] epoch #805 | Time 282.61 s
2022-04-23 14:23:54 | [train_policy] epoch #805 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120057
Evaluation/AverageDiscountedReturn          -41.5152
Evaluation/AverageReturn                    -41.5152
Evaluation/CompletionRate                     0
Evaluation/Iteration                        805
Evaluation/MaxReturn                        -29.7278
Evaluation/MinReturn                        -63.7871
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.94177
Extras/EpisodeRewardMean                    -41.4054
LinearFeatureBaseline/ExplainedVariance       0.863318
PolicyExecTime                                0.0977435
ProcessExecTime                               0.0112395
TotalEnvSteps                            815672
policy/Entropy                               -1.83618
policy/KL                                     0.00985065
policy/KLBefore                               0
policy/LossAfter                             -0.0181661
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.159425
policy/dLoss                                  0.0181661
---------------------------------------  ----------------
2022-04-23 14:23:54 | [train_policy] epoch #806 | Obtaining samples for iteration 806...
2022-04-23 14:23:55 | [train_policy] epoch #806 | Logging diagnostics...
2022-04-23 14:23:55 | [train_policy] epoch #806 | Optimizing policy...
2022-04-23 14:23:55 | [train_policy] epoch #806 | Computing loss before
2022-04-23 14:23:55 | [train_policy] epoch #806 | Computing KL before
2022-04-23 14:23:55 | [train_policy] epoch #806 | Optimizing
2022-04-23 14:23:55 | [train_policy] epoch #806 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:55 | [train_policy] epoch #806 | computing loss before
2022-04-23 14:23:55 | [train_policy] epoch #806 | computing gradient
2022-04-23 14:23:55 | [train_policy] epoch #806 | gradient computed
2022-04-23 14:23:55 | [train_policy] epoch #806 | computing descent direction
2022-04-23 14:23:55 | [train_policy] epoch #806 | descent direction computed
2022-04-23 14:23:55 | [train_policy] epoch #806 | backtrack iters: 1
2022-04-23 14:23:55 | [train_policy] epoch #806 | optimization finished
2022-04-23 14:23:55 | [train_policy] epoch #806 | Computing KL after
2022-04-23 14:23:55 | [train_policy] epoch #806 | Computing loss after
2022-04-23 14:23:55 | [train_policy] epoch #806 | Fitting baseline...
2022-04-23 14:23:55 | [train_policy] epoch #806 | Saving snapshot...
2022-04-23 14:23:55 | [train_policy] epoch #806 | Saved
2022-04-23 14:23:55 | [train_policy] epoch #806 | Time 282.95 s
2022-04-23 14:23:55 | [train_policy] epoch #806 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119308
Evaluation/AverageDiscountedReturn          -41.8056
Evaluation/AverageReturn                    -41.8056
Evaluation/CompletionRate                     0
Evaluation/Iteration                        806
Evaluation/MaxReturn                        -29.4818
Evaluation/MinReturn                        -72.4724
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.06809
Extras/EpisodeRewardMean                    -41.7163
LinearFeatureBaseline/ExplainedVariance       0.872612
PolicyExecTime                                0.10357
ProcessExecTime                               0.0113504
TotalEnvSteps                            816684
policy/Entropy                               -1.85956
policy/KL                                     0.00686582
policy/KLBefore                               0
policy/LossAfter                             -0.0175972
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.155741
policy/dLoss                                  0.0175972
---------------------------------------  ----------------
2022-04-23 14:23:55 | [train_policy] epoch #807 | Obtaining samples for iteration 807...
2022-04-23 14:23:55 | [train_policy] epoch #807 | Logging diagnostics...
2022-04-23 14:23:55 | [train_policy] epoch #807 | Optimizing policy...
2022-04-23 14:23:55 | [train_policy] epoch #807 | Computing loss before
2022-04-23 14:23:55 | [train_policy] epoch #807 | Computing KL before
2022-04-23 14:23:55 | [train_policy] epoch #807 | Optimizing
2022-04-23 14:23:55 | [train_policy] epoch #807 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:55 | [train_policy] epoch #807 | computing loss before
2022-04-23 14:23:55 | [train_policy] epoch #807 | computing gradient
2022-04-23 14:23:55 | [train_policy] epoch #807 | gradient computed
2022-04-23 14:23:55 | [train_policy] epoch #807 | computing descent direction
2022-04-23 14:23:55 | [train_policy] epoch #807 | descent direction computed
2022-04-23 14:23:55 | [train_policy] epoch #807 | backtrack iters: 1
2022-04-23 14:23:55 | [train_policy] epoch #807 | optimization finished
2022-04-23 14:23:55 | [train_policy] epoch #807 | Computing KL after
2022-04-23 14:23:55 | [train_policy] epoch #807 | Computing loss after
2022-04-23 14:23:55 | [train_policy] epoch #807 | Fitting baseline...
2022-04-23 14:23:55 | [train_policy] epoch #807 | Saving snapshot...
2022-04-23 14:23:55 | [train_policy] epoch #807 | Saved
2022-04-23 14:23:55 | [train_policy] epoch #807 | Time 283.27 s
2022-04-23 14:23:55 | [train_policy] epoch #807 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118782
Evaluation/AverageDiscountedReturn          -40.5539
Evaluation/AverageReturn                    -40.5539
Evaluation/CompletionRate                     0
Evaluation/Iteration                        807
Evaluation/MaxReturn                        -29.6873
Evaluation/MinReturn                        -61.3455
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.7506
Extras/EpisodeRewardMean                    -40.3229
LinearFeatureBaseline/ExplainedVariance       0.856142
PolicyExecTime                                0.0945745
ProcessExecTime                               0.0110953
TotalEnvSteps                            817696
policy/Entropy                               -1.87702
policy/KL                                     0.00656267
policy/KLBefore                               0
policy/LossAfter                             -0.0178627
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.153045
policy/dLoss                                  0.0178627
---------------------------------------  ----------------
2022-04-23 14:23:55 | [train_policy] epoch #808 | Obtaining samples for iteration 808...
2022-04-23 14:23:55 | [train_policy] epoch #808 | Logging diagnostics...
2022-04-23 14:23:55 | [train_policy] epoch #808 | Optimizing policy...
2022-04-23 14:23:55 | [train_policy] epoch #808 | Computing loss before
2022-04-23 14:23:55 | [train_policy] epoch #808 | Computing KL before
2022-04-23 14:23:55 | [train_policy] epoch #808 | Optimizing
2022-04-23 14:23:55 | [train_policy] epoch #808 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:55 | [train_policy] epoch #808 | computing loss before
2022-04-23 14:23:55 | [train_policy] epoch #808 | computing gradient
2022-04-23 14:23:55 | [train_policy] epoch #808 | gradient computed
2022-04-23 14:23:55 | [train_policy] epoch #808 | computing descent direction
2022-04-23 14:23:55 | [train_policy] epoch #808 | descent direction computed
2022-04-23 14:23:55 | [train_policy] epoch #808 | backtrack iters: 0
2022-04-23 14:23:55 | [train_policy] epoch #808 | optimization finished
2022-04-23 14:23:55 | [train_policy] epoch #808 | Computing KL after
2022-04-23 14:23:55 | [train_policy] epoch #808 | Computing loss after
2022-04-23 14:23:55 | [train_policy] epoch #808 | Fitting baseline...
2022-04-23 14:23:55 | [train_policy] epoch #808 | Saving snapshot...
2022-04-23 14:23:55 | [train_policy] epoch #808 | Saved
2022-04-23 14:23:55 | [train_policy] epoch #808 | Time 283.60 s
2022-04-23 14:23:55 | [train_policy] epoch #808 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119921
Evaluation/AverageDiscountedReturn          -40.5651
Evaluation/AverageReturn                    -40.5651
Evaluation/CompletionRate                     0
Evaluation/Iteration                        808
Evaluation/MaxReturn                        -29.9452
Evaluation/MinReturn                        -61.3282
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.16634
Extras/EpisodeRewardMean                    -40.7088
LinearFeatureBaseline/ExplainedVariance       0.889595
PolicyExecTime                                0.0942173
ProcessExecTime                               0.0111897
TotalEnvSteps                            818708
policy/Entropy                               -1.86054
policy/KL                                     0.00962364
policy/KLBefore                               0
policy/LossAfter                             -0.0173365
policy/LossBefore                             2.02609e-08
policy/Perplexity                             0.155589
policy/dLoss                                  0.0173365
---------------------------------------  ----------------
2022-04-23 14:23:55 | [train_policy] epoch #809 | Obtaining samples for iteration 809...
2022-04-23 14:23:56 | [train_policy] epoch #809 | Logging diagnostics...
2022-04-23 14:23:56 | [train_policy] epoch #809 | Optimizing policy...
2022-04-23 14:23:56 | [train_policy] epoch #809 | Computing loss before
2022-04-23 14:23:56 | [train_policy] epoch #809 | Computing KL before
2022-04-23 14:23:56 | [train_policy] epoch #809 | Optimizing
2022-04-23 14:23:56 | [train_policy] epoch #809 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:56 | [train_policy] epoch #809 | computing loss before
2022-04-23 14:23:56 | [train_policy] epoch #809 | computing gradient
2022-04-23 14:23:56 | [train_policy] epoch #809 | gradient computed
2022-04-23 14:23:56 | [train_policy] epoch #809 | computing descent direction
2022-04-23 14:23:56 | [train_policy] epoch #809 | descent direction computed
2022-04-23 14:23:56 | [train_policy] epoch #809 | backtrack iters: 1
2022-04-23 14:23:56 | [train_policy] epoch #809 | optimization finished
2022-04-23 14:23:56 | [train_policy] epoch #809 | Computing KL after
2022-04-23 14:23:56 | [train_policy] epoch #809 | Computing loss after
2022-04-23 14:23:56 | [train_policy] epoch #809 | Fitting baseline...
2022-04-23 14:23:56 | [train_policy] epoch #809 | Saving snapshot...
2022-04-23 14:23:56 | [train_policy] epoch #809 | Saved
2022-04-23 14:23:56 | [train_policy] epoch #809 | Time 283.92 s
2022-04-23 14:23:56 | [train_policy] epoch #809 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.11941
Evaluation/AverageDiscountedReturn          -42.0046
Evaluation/AverageReturn                    -42.0046
Evaluation/CompletionRate                     0
Evaluation/Iteration                        809
Evaluation/MaxReturn                        -29.6545
Evaluation/MinReturn                        -72.9376
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.29417
Extras/EpisodeRewardMean                    -41.6783
LinearFeatureBaseline/ExplainedVariance       0.880514
PolicyExecTime                                0.0924604
ProcessExecTime                               0.0113091
TotalEnvSteps                            819720
policy/Entropy                               -1.88563
policy/KL                                     0.00667722
policy/KLBefore                               0
policy/LossAfter                             -0.0148706
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.151733
policy/dLoss                                  0.0148706
---------------------------------------  ---------------
2022-04-23 14:23:56 | [train_policy] epoch #810 | Obtaining samples for iteration 810...
2022-04-23 14:23:56 | [train_policy] epoch #810 | Logging diagnostics...
2022-04-23 14:23:56 | [train_policy] epoch #810 | Optimizing policy...
2022-04-23 14:23:56 | [train_policy] epoch #810 | Computing loss before
2022-04-23 14:23:56 | [train_policy] epoch #810 | Computing KL before
2022-04-23 14:23:56 | [train_policy] epoch #810 | Optimizing
2022-04-23 14:23:56 | [train_policy] epoch #810 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:56 | [train_policy] epoch #810 | computing loss before
2022-04-23 14:23:56 | [train_policy] epoch #810 | computing gradient
2022-04-23 14:23:56 | [train_policy] epoch #810 | gradient computed
2022-04-23 14:23:56 | [train_policy] epoch #810 | computing descent direction
2022-04-23 14:23:56 | [train_policy] epoch #810 | descent direction computed
2022-04-23 14:23:56 | [train_policy] epoch #810 | backtrack iters: 1
2022-04-23 14:23:56 | [train_policy] epoch #810 | optimization finished
2022-04-23 14:23:56 | [train_policy] epoch #810 | Computing KL after
2022-04-23 14:23:56 | [train_policy] epoch #810 | Computing loss after
2022-04-23 14:23:56 | [train_policy] epoch #810 | Fitting baseline...
2022-04-23 14:23:56 | [train_policy] epoch #810 | Saving snapshot...
2022-04-23 14:23:56 | [train_policy] epoch #810 | Saved
2022-04-23 14:23:56 | [train_policy] epoch #810 | Time 284.24 s
2022-04-23 14:23:56 | [train_policy] epoch #810 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119743
Evaluation/AverageDiscountedReturn          -41.8589
Evaluation/AverageReturn                    -41.8589
Evaluation/CompletionRate                     0
Evaluation/Iteration                        810
Evaluation/MaxReturn                        -30.0847
Evaluation/MinReturn                        -72.7828
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.91588
Extras/EpisodeRewardMean                    -41.7842
LinearFeatureBaseline/ExplainedVariance       0.851241
PolicyExecTime                                0.0930111
ProcessExecTime                               0.0112786
TotalEnvSteps                            820732
policy/Entropy                               -1.90828
policy/KL                                     0.00668796
policy/KLBefore                               0
policy/LossAfter                             -0.0166385
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.148336
policy/dLoss                                  0.0166385
---------------------------------------  ----------------
2022-04-23 14:23:56 | [train_policy] epoch #811 | Obtaining samples for iteration 811...
2022-04-23 14:23:56 | [train_policy] epoch #811 | Logging diagnostics...
2022-04-23 14:23:56 | [train_policy] epoch #811 | Optimizing policy...
2022-04-23 14:23:56 | [train_policy] epoch #811 | Computing loss before
2022-04-23 14:23:56 | [train_policy] epoch #811 | Computing KL before
2022-04-23 14:23:56 | [train_policy] epoch #811 | Optimizing
2022-04-23 14:23:56 | [train_policy] epoch #811 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:56 | [train_policy] epoch #811 | computing loss before
2022-04-23 14:23:56 | [train_policy] epoch #811 | computing gradient
2022-04-23 14:23:56 | [train_policy] epoch #811 | gradient computed
2022-04-23 14:23:56 | [train_policy] epoch #811 | computing descent direction
2022-04-23 14:23:56 | [train_policy] epoch #811 | descent direction computed
2022-04-23 14:23:56 | [train_policy] epoch #811 | backtrack iters: 0
2022-04-23 14:23:56 | [train_policy] epoch #811 | optimization finished
2022-04-23 14:23:56 | [train_policy] epoch #811 | Computing KL after
2022-04-23 14:23:56 | [train_policy] epoch #811 | Computing loss after
2022-04-23 14:23:56 | [train_policy] epoch #811 | Fitting baseline...
2022-04-23 14:23:56 | [train_policy] epoch #811 | Saving snapshot...
2022-04-23 14:23:56 | [train_policy] epoch #811 | Saved
2022-04-23 14:23:56 | [train_policy] epoch #811 | Time 284.57 s
2022-04-23 14:23:56 | [train_policy] epoch #811 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120574
Evaluation/AverageDiscountedReturn          -41.0377
Evaluation/AverageReturn                    -41.0377
Evaluation/CompletionRate                     0
Evaluation/Iteration                        811
Evaluation/MaxReturn                        -29.96
Evaluation/MinReturn                        -72.0537
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.32111
Extras/EpisodeRewardMean                    -40.9103
LinearFeatureBaseline/ExplainedVariance       0.818812
PolicyExecTime                                0.0925462
ProcessExecTime                               0.0114572
TotalEnvSteps                            821744
policy/Entropy                               -1.83913
policy/KL                                     0.0095731
policy/KLBefore                               0
policy/LossAfter                             -0.0202615
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.158956
policy/dLoss                                  0.0202615
---------------------------------------  ----------------
2022-04-23 14:23:56 | [train_policy] epoch #812 | Obtaining samples for iteration 812...
2022-04-23 14:23:57 | [train_policy] epoch #812 | Logging diagnostics...
2022-04-23 14:23:57 | [train_policy] epoch #812 | Optimizing policy...
2022-04-23 14:23:57 | [train_policy] epoch #812 | Computing loss before
2022-04-23 14:23:57 | [train_policy] epoch #812 | Computing KL before
2022-04-23 14:23:57 | [train_policy] epoch #812 | Optimizing
2022-04-23 14:23:57 | [train_policy] epoch #812 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:57 | [train_policy] epoch #812 | computing loss before
2022-04-23 14:23:57 | [train_policy] epoch #812 | computing gradient
2022-04-23 14:23:57 | [train_policy] epoch #812 | gradient computed
2022-04-23 14:23:57 | [train_policy] epoch #812 | computing descent direction
2022-04-23 14:23:57 | [train_policy] epoch #812 | descent direction computed
2022-04-23 14:23:57 | [train_policy] epoch #812 | backtrack iters: 0
2022-04-23 14:23:57 | [train_policy] epoch #812 | optimization finished
2022-04-23 14:23:57 | [train_policy] epoch #812 | Computing KL after
2022-04-23 14:23:57 | [train_policy] epoch #812 | Computing loss after
2022-04-23 14:23:57 | [train_policy] epoch #812 | Fitting baseline...
2022-04-23 14:23:57 | [train_policy] epoch #812 | Saving snapshot...
2022-04-23 14:23:57 | [train_policy] epoch #812 | Saved
2022-04-23 14:23:57 | [train_policy] epoch #812 | Time 284.89 s
2022-04-23 14:23:57 | [train_policy] epoch #812 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119912
Evaluation/AverageDiscountedReturn          -40.6623
Evaluation/AverageReturn                    -40.6623
Evaluation/CompletionRate                     0
Evaluation/Iteration                        812
Evaluation/MaxReturn                        -30.3497
Evaluation/MinReturn                        -64.7509
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.4956
Extras/EpisodeRewardMean                    -40.5417
LinearFeatureBaseline/ExplainedVariance       0.896129
PolicyExecTime                                0.0956972
ProcessExecTime                               0.0114038
TotalEnvSteps                            822756
policy/Entropy                               -1.8261
policy/KL                                     0.00996024
policy/KLBefore                               0
policy/LossAfter                             -0.0235191
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.161041
policy/dLoss                                  0.0235191
---------------------------------------  ----------------
2022-04-23 14:23:57 | [train_policy] epoch #813 | Obtaining samples for iteration 813...
2022-04-23 14:23:57 | [train_policy] epoch #813 | Logging diagnostics...
2022-04-23 14:23:57 | [train_policy] epoch #813 | Optimizing policy...
2022-04-23 14:23:57 | [train_policy] epoch #813 | Computing loss before
2022-04-23 14:23:57 | [train_policy] epoch #813 | Computing KL before
2022-04-23 14:23:57 | [train_policy] epoch #813 | Optimizing
2022-04-23 14:23:57 | [train_policy] epoch #813 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:57 | [train_policy] epoch #813 | computing loss before
2022-04-23 14:23:57 | [train_policy] epoch #813 | computing gradient
2022-04-23 14:23:57 | [train_policy] epoch #813 | gradient computed
2022-04-23 14:23:57 | [train_policy] epoch #813 | computing descent direction
2022-04-23 14:23:57 | [train_policy] epoch #813 | descent direction computed
2022-04-23 14:23:57 | [train_policy] epoch #813 | backtrack iters: 0
2022-04-23 14:23:57 | [train_policy] epoch #813 | optimization finished
2022-04-23 14:23:57 | [train_policy] epoch #813 | Computing KL after
2022-04-23 14:23:57 | [train_policy] epoch #813 | Computing loss after
2022-04-23 14:23:57 | [train_policy] epoch #813 | Fitting baseline...
2022-04-23 14:23:57 | [train_policy] epoch #813 | Saving snapshot...
2022-04-23 14:23:57 | [train_policy] epoch #813 | Saved
2022-04-23 14:23:57 | [train_policy] epoch #813 | Time 285.22 s
2022-04-23 14:23:57 | [train_policy] epoch #813 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.121046
Evaluation/AverageDiscountedReturn          -41.49
Evaluation/AverageReturn                    -41.49
Evaluation/CompletionRate                     0
Evaluation/Iteration                        813
Evaluation/MaxReturn                        -29.4631
Evaluation/MinReturn                        -64.2616
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.12092
Extras/EpisodeRewardMean                    -41.5202
LinearFeatureBaseline/ExplainedVariance       0.863644
PolicyExecTime                                0.0957613
ProcessExecTime                               0.0113659
TotalEnvSteps                            823768
policy/Entropy                               -1.78828
policy/KL                                     0.00961055
policy/KLBefore                               0
policy/LossAfter                             -0.018028
policy/LossBefore                             2.12032e-09
policy/Perplexity                             0.167248
policy/dLoss                                  0.018028
---------------------------------------  ----------------
2022-04-23 14:23:57 | [train_policy] epoch #814 | Obtaining samples for iteration 814...
2022-04-23 14:23:57 | [train_policy] epoch #814 | Logging diagnostics...
2022-04-23 14:23:57 | [train_policy] epoch #814 | Optimizing policy...
2022-04-23 14:23:57 | [train_policy] epoch #814 | Computing loss before
2022-04-23 14:23:57 | [train_policy] epoch #814 | Computing KL before
2022-04-23 14:23:57 | [train_policy] epoch #814 | Optimizing
2022-04-23 14:23:57 | [train_policy] epoch #814 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:57 | [train_policy] epoch #814 | computing loss before
2022-04-23 14:23:57 | [train_policy] epoch #814 | computing gradient
2022-04-23 14:23:57 | [train_policy] epoch #814 | gradient computed
2022-04-23 14:23:57 | [train_policy] epoch #814 | computing descent direction
2022-04-23 14:23:57 | [train_policy] epoch #814 | descent direction computed
2022-04-23 14:23:57 | [train_policy] epoch #814 | backtrack iters: 1
2022-04-23 14:23:57 | [train_policy] epoch #814 | optimization finished
2022-04-23 14:23:57 | [train_policy] epoch #814 | Computing KL after
2022-04-23 14:23:57 | [train_policy] epoch #814 | Computing loss after
2022-04-23 14:23:57 | [train_policy] epoch #814 | Fitting baseline...
2022-04-23 14:23:57 | [train_policy] epoch #814 | Saving snapshot...
2022-04-23 14:23:57 | [train_policy] epoch #814 | Saved
2022-04-23 14:23:57 | [train_policy] epoch #814 | Time 285.55 s
2022-04-23 14:23:57 | [train_policy] epoch #814 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120683
Evaluation/AverageDiscountedReturn          -42.1912
Evaluation/AverageReturn                    -42.1912
Evaluation/CompletionRate                     0
Evaluation/Iteration                        814
Evaluation/MaxReturn                        -28.7501
Evaluation/MinReturn                        -72.0193
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.7018
Extras/EpisodeRewardMean                    -42.6599
LinearFeatureBaseline/ExplainedVariance       0.853844
PolicyExecTime                                0.0956738
ProcessExecTime                               0.0112433
TotalEnvSteps                            824780
policy/Entropy                               -1.82451
policy/KL                                     0.00658953
policy/KLBefore                               0
policy/LossAfter                             -0.0168829
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.161297
policy/dLoss                                  0.0168829
---------------------------------------  ----------------
2022-04-23 14:23:57 | [train_policy] epoch #815 | Obtaining samples for iteration 815...
2022-04-23 14:23:58 | [train_policy] epoch #815 | Logging diagnostics...
2022-04-23 14:23:58 | [train_policy] epoch #815 | Optimizing policy...
2022-04-23 14:23:58 | [train_policy] epoch #815 | Computing loss before
2022-04-23 14:23:58 | [train_policy] epoch #815 | Computing KL before
2022-04-23 14:23:58 | [train_policy] epoch #815 | Optimizing
2022-04-23 14:23:58 | [train_policy] epoch #815 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:58 | [train_policy] epoch #815 | computing loss before
2022-04-23 14:23:58 | [train_policy] epoch #815 | computing gradient
2022-04-23 14:23:58 | [train_policy] epoch #815 | gradient computed
2022-04-23 14:23:58 | [train_policy] epoch #815 | computing descent direction
2022-04-23 14:23:58 | [train_policy] epoch #815 | descent direction computed
2022-04-23 14:23:58 | [train_policy] epoch #815 | backtrack iters: 0
2022-04-23 14:23:58 | [train_policy] epoch #815 | optimization finished
2022-04-23 14:23:58 | [train_policy] epoch #815 | Computing KL after
2022-04-23 14:23:58 | [train_policy] epoch #815 | Computing loss after
2022-04-23 14:23:58 | [train_policy] epoch #815 | Fitting baseline...
2022-04-23 14:23:58 | [train_policy] epoch #815 | Saving snapshot...
2022-04-23 14:23:58 | [train_policy] epoch #815 | Saved
2022-04-23 14:23:58 | [train_policy] epoch #815 | Time 285.87 s
2022-04-23 14:23:58 | [train_policy] epoch #815 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119237
Evaluation/AverageDiscountedReturn          -42.1014
Evaluation/AverageReturn                    -42.1014
Evaluation/CompletionRate                     0
Evaluation/Iteration                        815
Evaluation/MaxReturn                        -29.7133
Evaluation/MinReturn                        -72.215
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.32956
Extras/EpisodeRewardMean                    -42.0595
LinearFeatureBaseline/ExplainedVariance       0.866386
PolicyExecTime                                0.0924735
ProcessExecTime                               0.0113769
TotalEnvSteps                            825792
policy/Entropy                               -1.80357
policy/KL                                     0.00982027
policy/KLBefore                               0
policy/LossAfter                             -0.0162134
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.16471
policy/dLoss                                  0.0162134
---------------------------------------  ----------------
2022-04-23 14:23:58 | [train_policy] epoch #816 | Obtaining samples for iteration 816...
2022-04-23 14:23:58 | [train_policy] epoch #816 | Logging diagnostics...
2022-04-23 14:23:58 | [train_policy] epoch #816 | Optimizing policy...
2022-04-23 14:23:58 | [train_policy] epoch #816 | Computing loss before
2022-04-23 14:23:58 | [train_policy] epoch #816 | Computing KL before
2022-04-23 14:23:58 | [train_policy] epoch #816 | Optimizing
2022-04-23 14:23:58 | [train_policy] epoch #816 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:58 | [train_policy] epoch #816 | computing loss before
2022-04-23 14:23:58 | [train_policy] epoch #816 | computing gradient
2022-04-23 14:23:58 | [train_policy] epoch #816 | gradient computed
2022-04-23 14:23:58 | [train_policy] epoch #816 | computing descent direction
2022-04-23 14:23:58 | [train_policy] epoch #816 | descent direction computed
2022-04-23 14:23:58 | [train_policy] epoch #816 | backtrack iters: 0
2022-04-23 14:23:58 | [train_policy] epoch #816 | optimization finished
2022-04-23 14:23:58 | [train_policy] epoch #816 | Computing KL after
2022-04-23 14:23:58 | [train_policy] epoch #816 | Computing loss after
2022-04-23 14:23:58 | [train_policy] epoch #816 | Fitting baseline...
2022-04-23 14:23:58 | [train_policy] epoch #816 | Saving snapshot...
2022-04-23 14:23:58 | [train_policy] epoch #816 | Saved
2022-04-23 14:23:58 | [train_policy] epoch #816 | Time 286.21 s
2022-04-23 14:23:58 | [train_policy] epoch #816 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116649
Evaluation/AverageDiscountedReturn          -41.5816
Evaluation/AverageReturn                    -41.5816
Evaluation/CompletionRate                     0
Evaluation/Iteration                        816
Evaluation/MaxReturn                        -29.2433
Evaluation/MinReturn                        -72.5635
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.094
Extras/EpisodeRewardMean                    -41.3537
LinearFeatureBaseline/ExplainedVariance       0.871645
PolicyExecTime                                0.101253
ProcessExecTime                               0.0116024
TotalEnvSteps                            826804
policy/Entropy                               -1.80741
policy/KL                                     0.00976709
policy/KLBefore                               0
policy/LossAfter                             -0.0107064
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.164078
policy/dLoss                                  0.0107064
---------------------------------------  ----------------
2022-04-23 14:23:58 | [train_policy] epoch #817 | Obtaining samples for iteration 817...
2022-04-23 14:23:58 | [train_policy] epoch #817 | Logging diagnostics...
2022-04-23 14:23:58 | [train_policy] epoch #817 | Optimizing policy...
2022-04-23 14:23:58 | [train_policy] epoch #817 | Computing loss before
2022-04-23 14:23:58 | [train_policy] epoch #817 | Computing KL before
2022-04-23 14:23:58 | [train_policy] epoch #817 | Optimizing
2022-04-23 14:23:58 | [train_policy] epoch #817 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:58 | [train_policy] epoch #817 | computing loss before
2022-04-23 14:23:58 | [train_policy] epoch #817 | computing gradient
2022-04-23 14:23:58 | [train_policy] epoch #817 | gradient computed
2022-04-23 14:23:58 | [train_policy] epoch #817 | computing descent direction
2022-04-23 14:23:58 | [train_policy] epoch #817 | descent direction computed
2022-04-23 14:23:58 | [train_policy] epoch #817 | backtrack iters: 1
2022-04-23 14:23:58 | [train_policy] epoch #817 | optimization finished
2022-04-23 14:23:58 | [train_policy] epoch #817 | Computing KL after
2022-04-23 14:23:58 | [train_policy] epoch #817 | Computing loss after
2022-04-23 14:23:58 | [train_policy] epoch #817 | Fitting baseline...
2022-04-23 14:23:58 | [train_policy] epoch #817 | Saving snapshot...
2022-04-23 14:23:58 | [train_policy] epoch #817 | Saved
2022-04-23 14:23:58 | [train_policy] epoch #817 | Time 286.54 s
2022-04-23 14:23:58 | [train_policy] epoch #817 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.121212
Evaluation/AverageDiscountedReturn          -40.2995
Evaluation/AverageReturn                    -40.2995
Evaluation/CompletionRate                     0
Evaluation/Iteration                        817
Evaluation/MaxReturn                        -29.456
Evaluation/MinReturn                        -72.4656
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.41063
Extras/EpisodeRewardMean                    -40.1021
LinearFeatureBaseline/ExplainedVariance       0.88076
PolicyExecTime                                0.0960402
ProcessExecTime                               0.0114307
TotalEnvSteps                            827816
policy/Entropy                               -1.8233
policy/KL                                     0.00641657
policy/KLBefore                               0
policy/LossAfter                             -0.0151515
policy/LossBefore                             1.71982e-08
policy/Perplexity                             0.161492
policy/dLoss                                  0.0151516
---------------------------------------  ----------------
2022-04-23 14:23:58 | [train_policy] epoch #818 | Obtaining samples for iteration 818...
2022-04-23 14:23:59 | [train_policy] epoch #818 | Logging diagnostics...
2022-04-23 14:23:59 | [train_policy] epoch #818 | Optimizing policy...
2022-04-23 14:23:59 | [train_policy] epoch #818 | Computing loss before
2022-04-23 14:23:59 | [train_policy] epoch #818 | Computing KL before
2022-04-23 14:23:59 | [train_policy] epoch #818 | Optimizing
2022-04-23 14:23:59 | [train_policy] epoch #818 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:59 | [train_policy] epoch #818 | computing loss before
2022-04-23 14:23:59 | [train_policy] epoch #818 | computing gradient
2022-04-23 14:23:59 | [train_policy] epoch #818 | gradient computed
2022-04-23 14:23:59 | [train_policy] epoch #818 | computing descent direction
2022-04-23 14:23:59 | [train_policy] epoch #818 | descent direction computed
2022-04-23 14:23:59 | [train_policy] epoch #818 | backtrack iters: 1
2022-04-23 14:23:59 | [train_policy] epoch #818 | optimization finished
2022-04-23 14:23:59 | [train_policy] epoch #818 | Computing KL after
2022-04-23 14:23:59 | [train_policy] epoch #818 | Computing loss after
2022-04-23 14:23:59 | [train_policy] epoch #818 | Fitting baseline...
2022-04-23 14:23:59 | [train_policy] epoch #818 | Saving snapshot...
2022-04-23 14:23:59 | [train_policy] epoch #818 | Saved
2022-04-23 14:23:59 | [train_policy] epoch #818 | Time 286.88 s
2022-04-23 14:23:59 | [train_policy] epoch #818 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119909
Evaluation/AverageDiscountedReturn          -41.1161
Evaluation/AverageReturn                    -41.1161
Evaluation/CompletionRate                     0
Evaluation/Iteration                        818
Evaluation/MaxReturn                        -29.505
Evaluation/MinReturn                        -72.3333
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.99363
Extras/EpisodeRewardMean                    -41.0191
LinearFeatureBaseline/ExplainedVariance       0.847448
PolicyExecTime                                0.0956306
ProcessExecTime                               0.01138
TotalEnvSteps                            828828
policy/Entropy                               -1.81377
policy/KL                                     0.0064051
policy/KLBefore                               0
policy/LossAfter                             -0.0119193
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.163038
policy/dLoss                                  0.0119193
---------------------------------------  ----------------
2022-04-23 14:23:59 | [train_policy] epoch #819 | Obtaining samples for iteration 819...
2022-04-23 14:23:59 | [train_policy] epoch #819 | Logging diagnostics...
2022-04-23 14:23:59 | [train_policy] epoch #819 | Optimizing policy...
2022-04-23 14:23:59 | [train_policy] epoch #819 | Computing loss before
2022-04-23 14:23:59 | [train_policy] epoch #819 | Computing KL before
2022-04-23 14:23:59 | [train_policy] epoch #819 | Optimizing
2022-04-23 14:23:59 | [train_policy] epoch #819 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:59 | [train_policy] epoch #819 | computing loss before
2022-04-23 14:23:59 | [train_policy] epoch #819 | computing gradient
2022-04-23 14:23:59 | [train_policy] epoch #819 | gradient computed
2022-04-23 14:23:59 | [train_policy] epoch #819 | computing descent direction
2022-04-23 14:23:59 | [train_policy] epoch #819 | descent direction computed
2022-04-23 14:23:59 | [train_policy] epoch #819 | backtrack iters: 1
2022-04-23 14:23:59 | [train_policy] epoch #819 | optimization finished
2022-04-23 14:23:59 | [train_policy] epoch #819 | Computing KL after
2022-04-23 14:23:59 | [train_policy] epoch #819 | Computing loss after
2022-04-23 14:23:59 | [train_policy] epoch #819 | Fitting baseline...
2022-04-23 14:23:59 | [train_policy] epoch #819 | Saving snapshot...
2022-04-23 14:23:59 | [train_policy] epoch #819 | Saved
2022-04-23 14:23:59 | [train_policy] epoch #819 | Time 287.22 s
2022-04-23 14:23:59 | [train_policy] epoch #819 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.1193
Evaluation/AverageDiscountedReturn          -63.7483
Evaluation/AverageReturn                    -63.7483
Evaluation/CompletionRate                     0
Evaluation/Iteration                        819
Evaluation/MaxReturn                        -30.0826
Evaluation/MinReturn                      -2047.32
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.105
Extras/EpisodeRewardMean                    -62.1056
LinearFeatureBaseline/ExplainedVariance       0.00556493
PolicyExecTime                                0.0973506
ProcessExecTime                               0.0112309
TotalEnvSteps                            829840
policy/Entropy                               -1.79726
policy/KL                                     0.00657024
policy/KLBefore                               0
policy/LossAfter                             -0.0210054
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.165752
policy/dLoss                                  0.0210054
---------------------------------------  ----------------
2022-04-23 14:23:59 | [train_policy] epoch #820 | Obtaining samples for iteration 820...
2022-04-23 14:23:59 | [train_policy] epoch #820 | Logging diagnostics...
2022-04-23 14:23:59 | [train_policy] epoch #820 | Optimizing policy...
2022-04-23 14:23:59 | [train_policy] epoch #820 | Computing loss before
2022-04-23 14:23:59 | [train_policy] epoch #820 | Computing KL before
2022-04-23 14:23:59 | [train_policy] epoch #820 | Optimizing
2022-04-23 14:23:59 | [train_policy] epoch #820 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:23:59 | [train_policy] epoch #820 | computing loss before
2022-04-23 14:23:59 | [train_policy] epoch #820 | computing gradient
2022-04-23 14:23:59 | [train_policy] epoch #820 | gradient computed
2022-04-23 14:23:59 | [train_policy] epoch #820 | computing descent direction
2022-04-23 14:23:59 | [train_policy] epoch #820 | descent direction computed
2022-04-23 14:23:59 | [train_policy] epoch #820 | backtrack iters: 0
2022-04-23 14:23:59 | [train_policy] epoch #820 | optimization finished
2022-04-23 14:23:59 | [train_policy] epoch #820 | Computing KL after
2022-04-23 14:23:59 | [train_policy] epoch #820 | Computing loss after
2022-04-23 14:23:59 | [train_policy] epoch #820 | Fitting baseline...
2022-04-23 14:23:59 | [train_policy] epoch #820 | Saving snapshot...
2022-04-23 14:23:59 | [train_policy] epoch #820 | Saved
2022-04-23 14:23:59 | [train_policy] epoch #820 | Time 287.55 s
2022-04-23 14:23:59 | [train_policy] epoch #820 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119723
Evaluation/AverageDiscountedReturn          -63.3764
Evaluation/AverageReturn                    -63.3764
Evaluation/CompletionRate                     0
Evaluation/Iteration                        820
Evaluation/MaxReturn                        -29.0315
Evaluation/MinReturn                      -2046.5
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.082
Extras/EpisodeRewardMean                    -61.4537
LinearFeatureBaseline/ExplainedVariance       0.170693
PolicyExecTime                                0.0973947
ProcessExecTime                               0.0112643
TotalEnvSteps                            830852
policy/Entropy                               -1.77387
policy/KL                                     0.00994164
policy/KLBefore                               0
policy/LossAfter                             -0.0386261
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.169675
policy/dLoss                                  0.038626
---------------------------------------  ----------------
2022-04-23 14:23:59 | [train_policy] epoch #821 | Obtaining samples for iteration 821...
2022-04-23 14:24:00 | [train_policy] epoch #821 | Logging diagnostics...
2022-04-23 14:24:00 | [train_policy] epoch #821 | Optimizing policy...
2022-04-23 14:24:00 | [train_policy] epoch #821 | Computing loss before
2022-04-23 14:24:00 | [train_policy] epoch #821 | Computing KL before
2022-04-23 14:24:00 | [train_policy] epoch #821 | Optimizing
2022-04-23 14:24:00 | [train_policy] epoch #821 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:00 | [train_policy] epoch #821 | computing loss before
2022-04-23 14:24:00 | [train_policy] epoch #821 | computing gradient
2022-04-23 14:24:00 | [train_policy] epoch #821 | gradient computed
2022-04-23 14:24:00 | [train_policy] epoch #821 | computing descent direction
2022-04-23 14:24:00 | [train_policy] epoch #821 | descent direction computed
2022-04-23 14:24:00 | [train_policy] epoch #821 | backtrack iters: 1
2022-04-23 14:24:00 | [train_policy] epoch #821 | optimization finished
2022-04-23 14:24:00 | [train_policy] epoch #821 | Computing KL after
2022-04-23 14:24:00 | [train_policy] epoch #821 | Computing loss after
2022-04-23 14:24:00 | [train_policy] epoch #821 | Fitting baseline...
2022-04-23 14:24:00 | [train_policy] epoch #821 | Saving snapshot...
2022-04-23 14:24:00 | [train_policy] epoch #821 | Saved
2022-04-23 14:24:00 | [train_policy] epoch #821 | Time 287.89 s
2022-04-23 14:24:00 | [train_policy] epoch #821 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.121734
Evaluation/AverageDiscountedReturn          -40.7514
Evaluation/AverageReturn                    -40.7514
Evaluation/CompletionRate                     0
Evaluation/Iteration                        821
Evaluation/MaxReturn                        -28.619
Evaluation/MinReturn                        -63.6898
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.85343
Extras/EpisodeRewardMean                    -41.1661
LinearFeatureBaseline/ExplainedVariance     -40.2665
PolicyExecTime                                0.0997972
ProcessExecTime                               0.0115259
TotalEnvSteps                            831864
policy/Entropy                               -1.79101
policy/KL                                     0.00646338
policy/KLBefore                               0
policy/LossAfter                             -0.0110795
policy/LossBefore                             4.94742e-09
policy/Perplexity                             0.166792
policy/dLoss                                  0.0110795
---------------------------------------  ----------------
2022-04-23 14:24:00 | [train_policy] epoch #822 | Obtaining samples for iteration 822...
2022-04-23 14:24:00 | [train_policy] epoch #822 | Logging diagnostics...
2022-04-23 14:24:00 | [train_policy] epoch #822 | Optimizing policy...
2022-04-23 14:24:00 | [train_policy] epoch #822 | Computing loss before
2022-04-23 14:24:00 | [train_policy] epoch #822 | Computing KL before
2022-04-23 14:24:00 | [train_policy] epoch #822 | Optimizing
2022-04-23 14:24:00 | [train_policy] epoch #822 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:00 | [train_policy] epoch #822 | computing loss before
2022-04-23 14:24:00 | [train_policy] epoch #822 | computing gradient
2022-04-23 14:24:00 | [train_policy] epoch #822 | gradient computed
2022-04-23 14:24:00 | [train_policy] epoch #822 | computing descent direction
2022-04-23 14:24:00 | [train_policy] epoch #822 | descent direction computed
2022-04-23 14:24:00 | [train_policy] epoch #822 | backtrack iters: 0
2022-04-23 14:24:00 | [train_policy] epoch #822 | optimization finished
2022-04-23 14:24:00 | [train_policy] epoch #822 | Computing KL after
2022-04-23 14:24:00 | [train_policy] epoch #822 | Computing loss after
2022-04-23 14:24:00 | [train_policy] epoch #822 | Fitting baseline...
2022-04-23 14:24:00 | [train_policy] epoch #822 | Saving snapshot...
2022-04-23 14:24:00 | [train_policy] epoch #822 | Saved
2022-04-23 14:24:00 | [train_policy] epoch #822 | Time 288.22 s
2022-04-23 14:24:00 | [train_policy] epoch #822 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.120696
Evaluation/AverageDiscountedReturn          -41.089
Evaluation/AverageReturn                    -41.089
Evaluation/CompletionRate                     0
Evaluation/Iteration                        822
Evaluation/MaxReturn                        -29.4664
Evaluation/MinReturn                        -71.2049
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.42557
Extras/EpisodeRewardMean                    -41.2659
LinearFeatureBaseline/ExplainedVariance       0.898914
PolicyExecTime                                0.0970333
ProcessExecTime                               0.0119603
TotalEnvSteps                            832876
policy/Entropy                               -1.76849
policy/KL                                     0.00976255
policy/KLBefore                               0
policy/LossAfter                             -0.0191333
policy/LossBefore                             8.2457e-10
policy/Perplexity                             0.170591
policy/dLoss                                  0.0191333
---------------------------------------  ---------------
2022-04-23 14:24:00 | [train_policy] epoch #823 | Obtaining samples for iteration 823...
2022-04-23 14:24:00 | [train_policy] epoch #823 | Logging diagnostics...
2022-04-23 14:24:00 | [train_policy] epoch #823 | Optimizing policy...
2022-04-23 14:24:00 | [train_policy] epoch #823 | Computing loss before
2022-04-23 14:24:00 | [train_policy] epoch #823 | Computing KL before
2022-04-23 14:24:00 | [train_policy] epoch #823 | Optimizing
2022-04-23 14:24:00 | [train_policy] epoch #823 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:00 | [train_policy] epoch #823 | computing loss before
2022-04-23 14:24:00 | [train_policy] epoch #823 | computing gradient
2022-04-23 14:24:00 | [train_policy] epoch #823 | gradient computed
2022-04-23 14:24:00 | [train_policy] epoch #823 | computing descent direction
2022-04-23 14:24:00 | [train_policy] epoch #823 | descent direction computed
2022-04-23 14:24:00 | [train_policy] epoch #823 | backtrack iters: 1
2022-04-23 14:24:00 | [train_policy] epoch #823 | optimization finished
2022-04-23 14:24:00 | [train_policy] epoch #823 | Computing KL after
2022-04-23 14:24:00 | [train_policy] epoch #823 | Computing loss after
2022-04-23 14:24:00 | [train_policy] epoch #823 | Fitting baseline...
2022-04-23 14:24:00 | [train_policy] epoch #823 | Saving snapshot...
2022-04-23 14:24:00 | [train_policy] epoch #823 | Saved
2022-04-23 14:24:00 | [train_policy] epoch #823 | Time 288.55 s
2022-04-23 14:24:00 | [train_policy] epoch #823 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.121667
Evaluation/AverageDiscountedReturn          -40.3919
Evaluation/AverageReturn                    -40.3919
Evaluation/CompletionRate                     0
Evaluation/Iteration                        823
Evaluation/MaxReturn                        -28.5385
Evaluation/MinReturn                        -56.9958
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.2288
Extras/EpisodeRewardMean                    -40.3858
LinearFeatureBaseline/ExplainedVariance       0.883475
PolicyExecTime                                0.096705
ProcessExecTime                               0.011373
TotalEnvSteps                            833888
policy/Entropy                               -1.81309
policy/KL                                     0.00685358
policy/KLBefore                               0
policy/LossAfter                             -0.019465
policy/LossBefore                            -2.12032e-09
policy/Perplexity                             0.16315
policy/dLoss                                  0.019465
---------------------------------------  ----------------
2022-04-23 14:24:00 | [train_policy] epoch #824 | Obtaining samples for iteration 824...
2022-04-23 14:24:01 | [train_policy] epoch #824 | Logging diagnostics...
2022-04-23 14:24:01 | [train_policy] epoch #824 | Optimizing policy...
2022-04-23 14:24:01 | [train_policy] epoch #824 | Computing loss before
2022-04-23 14:24:01 | [train_policy] epoch #824 | Computing KL before
2022-04-23 14:24:01 | [train_policy] epoch #824 | Optimizing
2022-04-23 14:24:01 | [train_policy] epoch #824 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:01 | [train_policy] epoch #824 | computing loss before
2022-04-23 14:24:01 | [train_policy] epoch #824 | computing gradient
2022-04-23 14:24:01 | [train_policy] epoch #824 | gradient computed
2022-04-23 14:24:01 | [train_policy] epoch #824 | computing descent direction
2022-04-23 14:24:01 | [train_policy] epoch #824 | descent direction computed
2022-04-23 14:24:01 | [train_policy] epoch #824 | backtrack iters: 0
2022-04-23 14:24:01 | [train_policy] epoch #824 | optimization finished
2022-04-23 14:24:01 | [train_policy] epoch #824 | Computing KL after
2022-04-23 14:24:01 | [train_policy] epoch #824 | Computing loss after
2022-04-23 14:24:01 | [train_policy] epoch #824 | Fitting baseline...
2022-04-23 14:24:01 | [train_policy] epoch #824 | Saving snapshot...
2022-04-23 14:24:01 | [train_policy] epoch #824 | Saved
2022-04-23 14:24:01 | [train_policy] epoch #824 | Time 288.88 s
2022-04-23 14:24:01 | [train_policy] epoch #824 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.121551
Evaluation/AverageDiscountedReturn          -40.3659
Evaluation/AverageReturn                    -40.3659
Evaluation/CompletionRate                     0
Evaluation/Iteration                        824
Evaluation/MaxReturn                        -28.684
Evaluation/MinReturn                        -71.7532
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.26563
Extras/EpisodeRewardMean                    -40.501
LinearFeatureBaseline/ExplainedVariance       0.873293
PolicyExecTime                                0.0958698
ProcessExecTime                               0.0112391
TotalEnvSteps                            834900
policy/Entropy                               -1.80001
policy/KL                                     0.00968195
policy/KLBefore                               0
policy/LossAfter                             -0.0243661
policy/LossBefore                            -4.47624e-09
policy/Perplexity                             0.165298
policy/dLoss                                  0.0243661
---------------------------------------  ----------------
2022-04-23 14:24:01 | [train_policy] epoch #825 | Obtaining samples for iteration 825...
2022-04-23 14:24:01 | [train_policy] epoch #825 | Logging diagnostics...
2022-04-23 14:24:01 | [train_policy] epoch #825 | Optimizing policy...
2022-04-23 14:24:01 | [train_policy] epoch #825 | Computing loss before
2022-04-23 14:24:01 | [train_policy] epoch #825 | Computing KL before
2022-04-23 14:24:01 | [train_policy] epoch #825 | Optimizing
2022-04-23 14:24:01 | [train_policy] epoch #825 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:01 | [train_policy] epoch #825 | computing loss before
2022-04-23 14:24:01 | [train_policy] epoch #825 | computing gradient
2022-04-23 14:24:01 | [train_policy] epoch #825 | gradient computed
2022-04-23 14:24:01 | [train_policy] epoch #825 | computing descent direction
2022-04-23 14:24:01 | [train_policy] epoch #825 | descent direction computed
2022-04-23 14:24:01 | [train_policy] epoch #825 | backtrack iters: 1
2022-04-23 14:24:01 | [train_policy] epoch #825 | optimization finished
2022-04-23 14:24:01 | [train_policy] epoch #825 | Computing KL after
2022-04-23 14:24:01 | [train_policy] epoch #825 | Computing loss after
2022-04-23 14:24:01 | [train_policy] epoch #825 | Fitting baseline...
2022-04-23 14:24:01 | [train_policy] epoch #825 | Saving snapshot...
2022-04-23 14:24:01 | [train_policy] epoch #825 | Saved
2022-04-23 14:24:01 | [train_policy] epoch #825 | Time 289.22 s
2022-04-23 14:24:01 | [train_policy] epoch #825 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.121787
Evaluation/AverageDiscountedReturn          -42.0692
Evaluation/AverageReturn                    -42.0692
Evaluation/CompletionRate                     0
Evaluation/Iteration                        825
Evaluation/MaxReturn                        -28.5619
Evaluation/MinReturn                        -72.5372
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.27545
Extras/EpisodeRewardMean                    -41.6017
LinearFeatureBaseline/ExplainedVariance       0.861104
PolicyExecTime                                0.0963728
ProcessExecTime                               0.0113509
TotalEnvSteps                            835912
policy/Entropy                               -1.84801
policy/KL                                     0.00667727
policy/KLBefore                               0
policy/LossAfter                             -0.0143658
policy/LossBefore                            -2.35591e-10
policy/Perplexity                             0.157551
policy/dLoss                                  0.0143658
---------------------------------------  ----------------
2022-04-23 14:24:01 | [train_policy] epoch #826 | Obtaining samples for iteration 826...
2022-04-23 14:24:01 | [train_policy] epoch #826 | Logging diagnostics...
2022-04-23 14:24:01 | [train_policy] epoch #826 | Optimizing policy...
2022-04-23 14:24:01 | [train_policy] epoch #826 | Computing loss before
2022-04-23 14:24:01 | [train_policy] epoch #826 | Computing KL before
2022-04-23 14:24:01 | [train_policy] epoch #826 | Optimizing
2022-04-23 14:24:01 | [train_policy] epoch #826 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:01 | [train_policy] epoch #826 | computing loss before
2022-04-23 14:24:01 | [train_policy] epoch #826 | computing gradient
2022-04-23 14:24:01 | [train_policy] epoch #826 | gradient computed
2022-04-23 14:24:01 | [train_policy] epoch #826 | computing descent direction
2022-04-23 14:24:01 | [train_policy] epoch #826 | descent direction computed
2022-04-23 14:24:01 | [train_policy] epoch #826 | backtrack iters: 1
2022-04-23 14:24:01 | [train_policy] epoch #826 | optimization finished
2022-04-23 14:24:01 | [train_policy] epoch #826 | Computing KL after
2022-04-23 14:24:01 | [train_policy] epoch #826 | Computing loss after
2022-04-23 14:24:01 | [train_policy] epoch #826 | Fitting baseline...
2022-04-23 14:24:01 | [train_policy] epoch #826 | Saving snapshot...
2022-04-23 14:24:01 | [train_policy] epoch #826 | Saved
2022-04-23 14:24:01 | [train_policy] epoch #826 | Time 289.56 s
2022-04-23 14:24:01 | [train_policy] epoch #826 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119993
Evaluation/AverageDiscountedReturn          -40.9978
Evaluation/AverageReturn                    -40.9978
Evaluation/CompletionRate                     0
Evaluation/Iteration                        826
Evaluation/MaxReturn                        -28.6329
Evaluation/MinReturn                        -72.3302
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.0955
Extras/EpisodeRewardMean                    -41.8925
LinearFeatureBaseline/ExplainedVariance       0.825007
PolicyExecTime                                0.0981526
ProcessExecTime                               0.0113678
TotalEnvSteps                            836924
policy/Entropy                               -1.8704
policy/KL                                     0.0064077
policy/KLBefore                               0
policy/LossAfter                             -0.0183109
policy/LossBefore                            -1.88473e-08
policy/Perplexity                             0.154062
policy/dLoss                                  0.0183108
---------------------------------------  ----------------
2022-04-23 14:24:01 | [train_policy] epoch #827 | Obtaining samples for iteration 827...
2022-04-23 14:24:02 | [train_policy] epoch #827 | Logging diagnostics...
2022-04-23 14:24:02 | [train_policy] epoch #827 | Optimizing policy...
2022-04-23 14:24:02 | [train_policy] epoch #827 | Computing loss before
2022-04-23 14:24:02 | [train_policy] epoch #827 | Computing KL before
2022-04-23 14:24:02 | [train_policy] epoch #827 | Optimizing
2022-04-23 14:24:02 | [train_policy] epoch #827 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:02 | [train_policy] epoch #827 | computing loss before
2022-04-23 14:24:02 | [train_policy] epoch #827 | computing gradient
2022-04-23 14:24:02 | [train_policy] epoch #827 | gradient computed
2022-04-23 14:24:02 | [train_policy] epoch #827 | computing descent direction
2022-04-23 14:24:02 | [train_policy] epoch #827 | descent direction computed
2022-04-23 14:24:02 | [train_policy] epoch #827 | backtrack iters: 0
2022-04-23 14:24:02 | [train_policy] epoch #827 | optimization finished
2022-04-23 14:24:02 | [train_policy] epoch #827 | Computing KL after
2022-04-23 14:24:02 | [train_policy] epoch #827 | Computing loss after
2022-04-23 14:24:02 | [train_policy] epoch #827 | Fitting baseline...
2022-04-23 14:24:02 | [train_policy] epoch #827 | Saving snapshot...
2022-04-23 14:24:02 | [train_policy] epoch #827 | Saved
2022-04-23 14:24:02 | [train_policy] epoch #827 | Time 289.89 s
2022-04-23 14:24:02 | [train_policy] epoch #827 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120425
Evaluation/AverageDiscountedReturn          -39.5813
Evaluation/AverageReturn                    -39.5813
Evaluation/CompletionRate                     0
Evaluation/Iteration                        827
Evaluation/MaxReturn                        -28.4437
Evaluation/MinReturn                        -72.6283
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.22917
Extras/EpisodeRewardMean                    -39.6562
LinearFeatureBaseline/ExplainedVariance       0.866483
PolicyExecTime                                0.0984769
ProcessExecTime                               0.0113161
TotalEnvSteps                            837936
policy/Entropy                               -1.8836
policy/KL                                     0.0097858
policy/KLBefore                               0
policy/LossAfter                             -0.0214363
policy/LossBefore                             5.18301e-09
policy/Perplexity                             0.152042
policy/dLoss                                  0.0214363
---------------------------------------  ----------------
2022-04-23 14:24:02 | [train_policy] epoch #828 | Obtaining samples for iteration 828...
2022-04-23 14:24:02 | [train_policy] epoch #828 | Logging diagnostics...
2022-04-23 14:24:02 | [train_policy] epoch #828 | Optimizing policy...
2022-04-23 14:24:02 | [train_policy] epoch #828 | Computing loss before
2022-04-23 14:24:02 | [train_policy] epoch #828 | Computing KL before
2022-04-23 14:24:02 | [train_policy] epoch #828 | Optimizing
2022-04-23 14:24:02 | [train_policy] epoch #828 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:02 | [train_policy] epoch #828 | computing loss before
2022-04-23 14:24:02 | [train_policy] epoch #828 | computing gradient
2022-04-23 14:24:02 | [train_policy] epoch #828 | gradient computed
2022-04-23 14:24:02 | [train_policy] epoch #828 | computing descent direction
2022-04-23 14:24:02 | [train_policy] epoch #828 | descent direction computed
2022-04-23 14:24:02 | [train_policy] epoch #828 | backtrack iters: 0
2022-04-23 14:24:02 | [train_policy] epoch #828 | optimization finished
2022-04-23 14:24:02 | [train_policy] epoch #828 | Computing KL after
2022-04-23 14:24:02 | [train_policy] epoch #828 | Computing loss after
2022-04-23 14:24:02 | [train_policy] epoch #828 | Fitting baseline...
2022-04-23 14:24:02 | [train_policy] epoch #828 | Saving snapshot...
2022-04-23 14:24:02 | [train_policy] epoch #828 | Saved
2022-04-23 14:24:02 | [train_policy] epoch #828 | Time 290.23 s
2022-04-23 14:24:02 | [train_policy] epoch #828 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.121016
Evaluation/AverageDiscountedReturn          -41.7143
Evaluation/AverageReturn                    -41.7143
Evaluation/CompletionRate                     0
Evaluation/Iteration                        828
Evaluation/MaxReturn                        -29.1604
Evaluation/MinReturn                        -72.5899
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.97267
Extras/EpisodeRewardMean                    -41.5984
LinearFeatureBaseline/ExplainedVariance       0.869635
PolicyExecTime                                0.101862
ProcessExecTime                               0.0116255
TotalEnvSteps                            838948
policy/Entropy                               -1.87806
policy/KL                                     0.00979799
policy/KLBefore                               0
policy/LossAfter                             -0.0153457
policy/LossBefore                             5.4775e-09
policy/Perplexity                             0.152887
policy/dLoss                                  0.0153457
---------------------------------------  ---------------
2022-04-23 14:24:02 | [train_policy] epoch #829 | Obtaining samples for iteration 829...
2022-04-23 14:24:02 | [train_policy] epoch #829 | Logging diagnostics...
2022-04-23 14:24:02 | [train_policy] epoch #829 | Optimizing policy...
2022-04-23 14:24:02 | [train_policy] epoch #829 | Computing loss before
2022-04-23 14:24:02 | [train_policy] epoch #829 | Computing KL before
2022-04-23 14:24:02 | [train_policy] epoch #829 | Optimizing
2022-04-23 14:24:02 | [train_policy] epoch #829 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:02 | [train_policy] epoch #829 | computing loss before
2022-04-23 14:24:02 | [train_policy] epoch #829 | computing gradient
2022-04-23 14:24:02 | [train_policy] epoch #829 | gradient computed
2022-04-23 14:24:02 | [train_policy] epoch #829 | computing descent direction
2022-04-23 14:24:02 | [train_policy] epoch #829 | descent direction computed
2022-04-23 14:24:02 | [train_policy] epoch #829 | backtrack iters: 1
2022-04-23 14:24:02 | [train_policy] epoch #829 | optimization finished
2022-04-23 14:24:02 | [train_policy] epoch #829 | Computing KL after
2022-04-23 14:24:02 | [train_policy] epoch #829 | Computing loss after
2022-04-23 14:24:02 | [train_policy] epoch #829 | Fitting baseline...
2022-04-23 14:24:02 | [train_policy] epoch #829 | Saving snapshot...
2022-04-23 14:24:02 | [train_policy] epoch #829 | Saved
2022-04-23 14:24:02 | [train_policy] epoch #829 | Time 290.56 s
2022-04-23 14:24:02 | [train_policy] epoch #829 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119132
Evaluation/AverageDiscountedReturn          -40.3338
Evaluation/AverageReturn                    -40.3338
Evaluation/CompletionRate                     0
Evaluation/Iteration                        829
Evaluation/MaxReturn                        -28.4135
Evaluation/MinReturn                        -64.5654
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.98865
Extras/EpisodeRewardMean                    -40.2411
LinearFeatureBaseline/ExplainedVariance       0.867273
PolicyExecTime                                0.0947392
ProcessExecTime                               0.0112503
TotalEnvSteps                            839960
policy/Entropy                               -1.90805
policy/KL                                     0.00645467
policy/KLBefore                               0
policy/LossAfter                             -0.0128396
policy/LossBefore                            -6.36097e-09
policy/Perplexity                             0.14837
policy/dLoss                                  0.0128396
---------------------------------------  ----------------
2022-04-23 14:24:02 | [train_policy] epoch #830 | Obtaining samples for iteration 830...
2022-04-23 14:24:03 | [train_policy] epoch #830 | Logging diagnostics...
2022-04-23 14:24:03 | [train_policy] epoch #830 | Optimizing policy...
2022-04-23 14:24:03 | [train_policy] epoch #830 | Computing loss before
2022-04-23 14:24:03 | [train_policy] epoch #830 | Computing KL before
2022-04-23 14:24:03 | [train_policy] epoch #830 | Optimizing
2022-04-23 14:24:03 | [train_policy] epoch #830 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:03 | [train_policy] epoch #830 | computing loss before
2022-04-23 14:24:03 | [train_policy] epoch #830 | computing gradient
2022-04-23 14:24:03 | [train_policy] epoch #830 | gradient computed
2022-04-23 14:24:03 | [train_policy] epoch #830 | computing descent direction
2022-04-23 14:24:03 | [train_policy] epoch #830 | descent direction computed
2022-04-23 14:24:03 | [train_policy] epoch #830 | backtrack iters: 1
2022-04-23 14:24:03 | [train_policy] epoch #830 | optimization finished
2022-04-23 14:24:03 | [train_policy] epoch #830 | Computing KL after
2022-04-23 14:24:03 | [train_policy] epoch #830 | Computing loss after
2022-04-23 14:24:03 | [train_policy] epoch #830 | Fitting baseline...
2022-04-23 14:24:03 | [train_policy] epoch #830 | Saving snapshot...
2022-04-23 14:24:03 | [train_policy] epoch #830 | Saved
2022-04-23 14:24:03 | [train_policy] epoch #830 | Time 290.89 s
2022-04-23 14:24:03 | [train_policy] epoch #830 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119702
Evaluation/AverageDiscountedReturn          -40.3304
Evaluation/AverageReturn                    -40.3304
Evaluation/CompletionRate                     0
Evaluation/Iteration                        830
Evaluation/MaxReturn                        -29.0884
Evaluation/MinReturn                        -72.024
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25311
Extras/EpisodeRewardMean                    -40.0469
LinearFeatureBaseline/ExplainedVariance       0.825775
PolicyExecTime                                0.0959907
ProcessExecTime                               0.0113637
TotalEnvSteps                            840972
policy/Entropy                               -1.92974
policy/KL                                     0.00656356
policy/KLBefore                               0
policy/LossAfter                             -0.0165784
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.145187
policy/dLoss                                  0.0165784
---------------------------------------  ----------------
2022-04-23 14:24:03 | [train_policy] epoch #831 | Obtaining samples for iteration 831...
2022-04-23 14:24:03 | [train_policy] epoch #831 | Logging diagnostics...
2022-04-23 14:24:03 | [train_policy] epoch #831 | Optimizing policy...
2022-04-23 14:24:03 | [train_policy] epoch #831 | Computing loss before
2022-04-23 14:24:03 | [train_policy] epoch #831 | Computing KL before
2022-04-23 14:24:03 | [train_policy] epoch #831 | Optimizing
2022-04-23 14:24:03 | [train_policy] epoch #831 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:03 | [train_policy] epoch #831 | computing loss before
2022-04-23 14:24:03 | [train_policy] epoch #831 | computing gradient
2022-04-23 14:24:03 | [train_policy] epoch #831 | gradient computed
2022-04-23 14:24:03 | [train_policy] epoch #831 | computing descent direction
2022-04-23 14:24:03 | [train_policy] epoch #831 | descent direction computed
2022-04-23 14:24:03 | [train_policy] epoch #831 | backtrack iters: 1
2022-04-23 14:24:03 | [train_policy] epoch #831 | optimization finished
2022-04-23 14:24:03 | [train_policy] epoch #831 | Computing KL after
2022-04-23 14:24:03 | [train_policy] epoch #831 | Computing loss after
2022-04-23 14:24:03 | [train_policy] epoch #831 | Fitting baseline...
2022-04-23 14:24:03 | [train_policy] epoch #831 | Saving snapshot...
2022-04-23 14:24:03 | [train_policy] epoch #831 | Saved
2022-04-23 14:24:03 | [train_policy] epoch #831 | Time 291.23 s
2022-04-23 14:24:03 | [train_policy] epoch #831 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.116442
Evaluation/AverageDiscountedReturn          -41.1145
Evaluation/AverageReturn                    -41.1145
Evaluation/CompletionRate                     0
Evaluation/Iteration                        831
Evaluation/MaxReturn                        -28.553
Evaluation/MinReturn                        -72.5112
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.57978
Extras/EpisodeRewardMean                    -41.378
LinearFeatureBaseline/ExplainedVariance       0.836719
PolicyExecTime                                0.0991952
ProcessExecTime                               0.0112803
TotalEnvSteps                            841984
policy/Entropy                               -1.94434
policy/KL                                     0.00663116
policy/KLBefore                               0
policy/LossAfter                             -0.0212544
policy/LossBefore                             1.40177e-08
policy/Perplexity                             0.143081
policy/dLoss                                  0.0212544
---------------------------------------  ----------------
2022-04-23 14:24:03 | [train_policy] epoch #832 | Obtaining samples for iteration 832...
2022-04-23 14:24:03 | [train_policy] epoch #832 | Logging diagnostics...
2022-04-23 14:24:03 | [train_policy] epoch #832 | Optimizing policy...
2022-04-23 14:24:03 | [train_policy] epoch #832 | Computing loss before
2022-04-23 14:24:03 | [train_policy] epoch #832 | Computing KL before
2022-04-23 14:24:03 | [train_policy] epoch #832 | Optimizing
2022-04-23 14:24:03 | [train_policy] epoch #832 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:03 | [train_policy] epoch #832 | computing loss before
2022-04-23 14:24:03 | [train_policy] epoch #832 | computing gradient
2022-04-23 14:24:03 | [train_policy] epoch #832 | gradient computed
2022-04-23 14:24:03 | [train_policy] epoch #832 | computing descent direction
2022-04-23 14:24:03 | [train_policy] epoch #832 | descent direction computed
2022-04-23 14:24:03 | [train_policy] epoch #832 | backtrack iters: 1
2022-04-23 14:24:03 | [train_policy] epoch #832 | optimization finished
2022-04-23 14:24:03 | [train_policy] epoch #832 | Computing KL after
2022-04-23 14:24:03 | [train_policy] epoch #832 | Computing loss after
2022-04-23 14:24:03 | [train_policy] epoch #832 | Fitting baseline...
2022-04-23 14:24:03 | [train_policy] epoch #832 | Saving snapshot...
2022-04-23 14:24:03 | [train_policy] epoch #832 | Saved
2022-04-23 14:24:03 | [train_policy] epoch #832 | Time 291.56 s
2022-04-23 14:24:03 | [train_policy] epoch #832 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.115387
Evaluation/AverageDiscountedReturn          -41.9995
Evaluation/AverageReturn                    -41.9995
Evaluation/CompletionRate                     0
Evaluation/Iteration                        832
Evaluation/MaxReturn                        -29.6793
Evaluation/MinReturn                        -71.6172
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.16796
Extras/EpisodeRewardMean                    -41.6808
LinearFeatureBaseline/ExplainedVariance       0.862002
PolicyExecTime                                0.0967257
ProcessExecTime                               0.0109913
TotalEnvSteps                            842996
policy/Entropy                               -1.97978
policy/KL                                     0.00669419
policy/KLBefore                               0
policy/LossAfter                             -0.0118583
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.138099
policy/dLoss                                  0.0118583
---------------------------------------  ----------------
2022-04-23 14:24:03 | [train_policy] epoch #833 | Obtaining samples for iteration 833...
2022-04-23 14:24:04 | [train_policy] epoch #833 | Logging diagnostics...
2022-04-23 14:24:04 | [train_policy] epoch #833 | Optimizing policy...
2022-04-23 14:24:04 | [train_policy] epoch #833 | Computing loss before
2022-04-23 14:24:04 | [train_policy] epoch #833 | Computing KL before
2022-04-23 14:24:04 | [train_policy] epoch #833 | Optimizing
2022-04-23 14:24:04 | [train_policy] epoch #833 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:04 | [train_policy] epoch #833 | computing loss before
2022-04-23 14:24:04 | [train_policy] epoch #833 | computing gradient
2022-04-23 14:24:04 | [train_policy] epoch #833 | gradient computed
2022-04-23 14:24:04 | [train_policy] epoch #833 | computing descent direction
2022-04-23 14:24:04 | [train_policy] epoch #833 | descent direction computed
2022-04-23 14:24:04 | [train_policy] epoch #833 | backtrack iters: 0
2022-04-23 14:24:04 | [train_policy] epoch #833 | optimization finished
2022-04-23 14:24:04 | [train_policy] epoch #833 | Computing KL after
2022-04-23 14:24:04 | [train_policy] epoch #833 | Computing loss after
2022-04-23 14:24:04 | [train_policy] epoch #833 | Fitting baseline...
2022-04-23 14:24:04 | [train_policy] epoch #833 | Saving snapshot...
2022-04-23 14:24:04 | [train_policy] epoch #833 | Saved
2022-04-23 14:24:04 | [train_policy] epoch #833 | Time 291.89 s
2022-04-23 14:24:04 | [train_policy] epoch #833 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.119813
Evaluation/AverageDiscountedReturn          -40.3325
Evaluation/AverageReturn                    -40.3325
Evaluation/CompletionRate                     0
Evaluation/Iteration                        833
Evaluation/MaxReturn                        -28.258
Evaluation/MinReturn                        -71.8396
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.10682
Extras/EpisodeRewardMean                    -40.6089
LinearFeatureBaseline/ExplainedVariance       0.890457
PolicyExecTime                                0.094681
ProcessExecTime                               0.0114291
TotalEnvSteps                            844008
policy/Entropy                               -1.95078
policy/KL                                     0.00914405
policy/KLBefore                               0
policy/LossAfter                             -0.0177837
policy/LossBefore                             8.2457e-09
policy/Perplexity                             0.142163
policy/dLoss                                  0.0177837
---------------------------------------  ---------------
2022-04-23 14:24:04 | [train_policy] epoch #834 | Obtaining samples for iteration 834...
2022-04-23 14:24:04 | [train_policy] epoch #834 | Logging diagnostics...
2022-04-23 14:24:04 | [train_policy] epoch #834 | Optimizing policy...
2022-04-23 14:24:04 | [train_policy] epoch #834 | Computing loss before
2022-04-23 14:24:04 | [train_policy] epoch #834 | Computing KL before
2022-04-23 14:24:04 | [train_policy] epoch #834 | Optimizing
2022-04-23 14:24:04 | [train_policy] epoch #834 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:04 | [train_policy] epoch #834 | computing loss before
2022-04-23 14:24:04 | [train_policy] epoch #834 | computing gradient
2022-04-23 14:24:04 | [train_policy] epoch #834 | gradient computed
2022-04-23 14:24:04 | [train_policy] epoch #834 | computing descent direction
2022-04-23 14:24:04 | [train_policy] epoch #834 | descent direction computed
2022-04-23 14:24:04 | [train_policy] epoch #834 | backtrack iters: 0
2022-04-23 14:24:04 | [train_policy] epoch #834 | optimization finished
2022-04-23 14:24:04 | [train_policy] epoch #834 | Computing KL after
2022-04-23 14:24:04 | [train_policy] epoch #834 | Computing loss after
2022-04-23 14:24:04 | [train_policy] epoch #834 | Fitting baseline...
2022-04-23 14:24:04 | [train_policy] epoch #834 | Saving snapshot...
2022-04-23 14:24:04 | [train_policy] epoch #834 | Saved
2022-04-23 14:24:04 | [train_policy] epoch #834 | Time 292.21 s
2022-04-23 14:24:04 | [train_policy] epoch #834 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118422
Evaluation/AverageDiscountedReturn          -40.3832
Evaluation/AverageReturn                    -40.3832
Evaluation/CompletionRate                     0
Evaluation/Iteration                        834
Evaluation/MaxReturn                        -29.2274
Evaluation/MinReturn                        -64.7421
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.63077
Extras/EpisodeRewardMean                    -40.1365
LinearFeatureBaseline/ExplainedVariance       0.877103
PolicyExecTime                                0.0913317
ProcessExecTime                               0.0111473
TotalEnvSteps                            845020
policy/Entropy                               -1.92801
policy/KL                                     0.00996889
policy/KLBefore                               0
policy/LossAfter                             -0.0195191
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.145438
policy/dLoss                                  0.0195191
---------------------------------------  ----------------
2022-04-23 14:24:04 | [train_policy] epoch #835 | Obtaining samples for iteration 835...
2022-04-23 14:24:04 | [train_policy] epoch #835 | Logging diagnostics...
2022-04-23 14:24:04 | [train_policy] epoch #835 | Optimizing policy...
2022-04-23 14:24:04 | [train_policy] epoch #835 | Computing loss before
2022-04-23 14:24:04 | [train_policy] epoch #835 | Computing KL before
2022-04-23 14:24:04 | [train_policy] epoch #835 | Optimizing
2022-04-23 14:24:04 | [train_policy] epoch #835 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:04 | [train_policy] epoch #835 | computing loss before
2022-04-23 14:24:04 | [train_policy] epoch #835 | computing gradient
2022-04-23 14:24:04 | [train_policy] epoch #835 | gradient computed
2022-04-23 14:24:04 | [train_policy] epoch #835 | computing descent direction
2022-04-23 14:24:04 | [train_policy] epoch #835 | descent direction computed
2022-04-23 14:24:04 | [train_policy] epoch #835 | backtrack iters: 0
2022-04-23 14:24:04 | [train_policy] epoch #835 | optimization finished
2022-04-23 14:24:04 | [train_policy] epoch #835 | Computing KL after
2022-04-23 14:24:04 | [train_policy] epoch #835 | Computing loss after
2022-04-23 14:24:04 | [train_policy] epoch #835 | Fitting baseline...
2022-04-23 14:24:04 | [train_policy] epoch #835 | Saving snapshot...
2022-04-23 14:24:04 | [train_policy] epoch #835 | Saved
2022-04-23 14:24:04 | [train_policy] epoch #835 | Time 292.53 s
2022-04-23 14:24:04 | [train_policy] epoch #835 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118582
Evaluation/AverageDiscountedReturn          -40.3127
Evaluation/AverageReturn                    -40.3127
Evaluation/CompletionRate                     0
Evaluation/Iteration                        835
Evaluation/MaxReturn                        -29.1825
Evaluation/MinReturn                        -63.7066
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.38975
Extras/EpisodeRewardMean                    -40.4849
LinearFeatureBaseline/ExplainedVariance       0.891194
PolicyExecTime                                0.0925272
ProcessExecTime                               0.0111876
TotalEnvSteps                            846032
policy/Entropy                               -1.87627
policy/KL                                     0.00942356
policy/KLBefore                               0
policy/LossAfter                             -0.0226485
policy/LossBefore                             4.71183e-09
policy/Perplexity                             0.153161
policy/dLoss                                  0.0226485
---------------------------------------  ----------------
2022-04-23 14:24:04 | [train_policy] epoch #836 | Obtaining samples for iteration 836...
2022-04-23 14:24:05 | [train_policy] epoch #836 | Logging diagnostics...
2022-04-23 14:24:05 | [train_policy] epoch #836 | Optimizing policy...
2022-04-23 14:24:05 | [train_policy] epoch #836 | Computing loss before
2022-04-23 14:24:05 | [train_policy] epoch #836 | Computing KL before
2022-04-23 14:24:05 | [train_policy] epoch #836 | Optimizing
2022-04-23 14:24:05 | [train_policy] epoch #836 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:05 | [train_policy] epoch #836 | computing loss before
2022-04-23 14:24:05 | [train_policy] epoch #836 | computing gradient
2022-04-23 14:24:05 | [train_policy] epoch #836 | gradient computed
2022-04-23 14:24:05 | [train_policy] epoch #836 | computing descent direction
2022-04-23 14:24:05 | [train_policy] epoch #836 | descent direction computed
2022-04-23 14:24:05 | [train_policy] epoch #836 | backtrack iters: 0
2022-04-23 14:24:05 | [train_policy] epoch #836 | optimization finished
2022-04-23 14:24:05 | [train_policy] epoch #836 | Computing KL after
2022-04-23 14:24:05 | [train_policy] epoch #836 | Computing loss after
2022-04-23 14:24:05 | [train_policy] epoch #836 | Fitting baseline...
2022-04-23 14:24:05 | [train_policy] epoch #836 | Saving snapshot...
2022-04-23 14:24:05 | [train_policy] epoch #836 | Saved
2022-04-23 14:24:05 | [train_policy] epoch #836 | Time 292.87 s
2022-04-23 14:24:05 | [train_policy] epoch #836 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120439
Evaluation/AverageDiscountedReturn          -41.5763
Evaluation/AverageReturn                    -41.5763
Evaluation/CompletionRate                     0
Evaluation/Iteration                        836
Evaluation/MaxReturn                        -30.4586
Evaluation/MinReturn                        -71.9136
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.56324
Extras/EpisodeRewardMean                    -41.2611
LinearFeatureBaseline/ExplainedVariance       0.883848
PolicyExecTime                                0.0977135
ProcessExecTime                               0.0117002
TotalEnvSteps                            847044
policy/Entropy                               -1.86049
policy/KL                                     0.00974561
policy/KLBefore                               0
policy/LossAfter                             -0.0455605
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.155596
policy/dLoss                                  0.0455605
---------------------------------------  ----------------
2022-04-23 14:24:05 | [train_policy] epoch #837 | Obtaining samples for iteration 837...
2022-04-23 14:24:05 | [train_policy] epoch #837 | Logging diagnostics...
2022-04-23 14:24:05 | [train_policy] epoch #837 | Optimizing policy...
2022-04-23 14:24:05 | [train_policy] epoch #837 | Computing loss before
2022-04-23 14:24:05 | [train_policy] epoch #837 | Computing KL before
2022-04-23 14:24:05 | [train_policy] epoch #837 | Optimizing
2022-04-23 14:24:05 | [train_policy] epoch #837 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:05 | [train_policy] epoch #837 | computing loss before
2022-04-23 14:24:05 | [train_policy] epoch #837 | computing gradient
2022-04-23 14:24:05 | [train_policy] epoch #837 | gradient computed
2022-04-23 14:24:05 | [train_policy] epoch #837 | computing descent direction
2022-04-23 14:24:05 | [train_policy] epoch #837 | descent direction computed
2022-04-23 14:24:05 | [train_policy] epoch #837 | backtrack iters: 0
2022-04-23 14:24:05 | [train_policy] epoch #837 | optimization finished
2022-04-23 14:24:05 | [train_policy] epoch #837 | Computing KL after
2022-04-23 14:24:05 | [train_policy] epoch #837 | Computing loss after
2022-04-23 14:24:05 | [train_policy] epoch #837 | Fitting baseline...
2022-04-23 14:24:05 | [train_policy] epoch #837 | Saving snapshot...
2022-04-23 14:24:05 | [train_policy] epoch #837 | Saved
2022-04-23 14:24:05 | [train_policy] epoch #837 | Time 293.19 s
2022-04-23 14:24:05 | [train_policy] epoch #837 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119047
Evaluation/AverageDiscountedReturn          -41.0299
Evaluation/AverageReturn                    -41.0299
Evaluation/CompletionRate                     0
Evaluation/Iteration                        837
Evaluation/MaxReturn                        -28.925
Evaluation/MinReturn                        -64.126
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.51365
Extras/EpisodeRewardMean                    -41.1224
LinearFeatureBaseline/ExplainedVariance       0.882953
PolicyExecTime                                0.0930042
ProcessExecTime                               0.0113261
TotalEnvSteps                            848056
policy/Entropy                               -1.84777
policy/KL                                     0.00983806
policy/KLBefore                               0
policy/LossAfter                             -0.0257874
policy/LossBefore                            -1.34287e-08
policy/Perplexity                             0.157589
policy/dLoss                                  0.0257874
---------------------------------------  ----------------
2022-04-23 14:24:05 | [train_policy] epoch #838 | Obtaining samples for iteration 838...
2022-04-23 14:24:05 | [train_policy] epoch #838 | Logging diagnostics...
2022-04-23 14:24:05 | [train_policy] epoch #838 | Optimizing policy...
2022-04-23 14:24:05 | [train_policy] epoch #838 | Computing loss before
2022-04-23 14:24:05 | [train_policy] epoch #838 | Computing KL before
2022-04-23 14:24:05 | [train_policy] epoch #838 | Optimizing
2022-04-23 14:24:05 | [train_policy] epoch #838 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:05 | [train_policy] epoch #838 | computing loss before
2022-04-23 14:24:05 | [train_policy] epoch #838 | computing gradient
2022-04-23 14:24:05 | [train_policy] epoch #838 | gradient computed
2022-04-23 14:24:05 | [train_policy] epoch #838 | computing descent direction
2022-04-23 14:24:05 | [train_policy] epoch #838 | descent direction computed
2022-04-23 14:24:05 | [train_policy] epoch #838 | backtrack iters: 0
2022-04-23 14:24:05 | [train_policy] epoch #838 | optimization finished
2022-04-23 14:24:05 | [train_policy] epoch #838 | Computing KL after
2022-04-23 14:24:05 | [train_policy] epoch #838 | Computing loss after
2022-04-23 14:24:05 | [train_policy] epoch #838 | Fitting baseline...
2022-04-23 14:24:05 | [train_policy] epoch #838 | Saving snapshot...
2022-04-23 14:24:05 | [train_policy] epoch #838 | Saved
2022-04-23 14:24:05 | [train_policy] epoch #838 | Time 293.51 s
2022-04-23 14:24:05 | [train_policy] epoch #838 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119447
Evaluation/AverageDiscountedReturn          -40.7922
Evaluation/AverageReturn                    -40.7922
Evaluation/CompletionRate                     0
Evaluation/Iteration                        838
Evaluation/MaxReturn                        -30.553
Evaluation/MinReturn                        -71.9615
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.63421
Extras/EpisodeRewardMean                    -40.5297
LinearFeatureBaseline/ExplainedVariance       0.878851
PolicyExecTime                                0.0923736
ProcessExecTime                               0.0111361
TotalEnvSteps                            849068
policy/Entropy                               -1.86361
policy/KL                                     0.00949918
policy/KLBefore                               0
policy/LossAfter                             -0.0225962
policy/LossBefore                             5.06522e-09
policy/Perplexity                             0.155112
policy/dLoss                                  0.0225962
---------------------------------------  ----------------
2022-04-23 14:24:05 | [train_policy] epoch #839 | Obtaining samples for iteration 839...
2022-04-23 14:24:06 | [train_policy] epoch #839 | Logging diagnostics...
2022-04-23 14:24:06 | [train_policy] epoch #839 | Optimizing policy...
2022-04-23 14:24:06 | [train_policy] epoch #839 | Computing loss before
2022-04-23 14:24:06 | [train_policy] epoch #839 | Computing KL before
2022-04-23 14:24:06 | [train_policy] epoch #839 | Optimizing
2022-04-23 14:24:06 | [train_policy] epoch #839 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:06 | [train_policy] epoch #839 | computing loss before
2022-04-23 14:24:06 | [train_policy] epoch #839 | computing gradient
2022-04-23 14:24:06 | [train_policy] epoch #839 | gradient computed
2022-04-23 14:24:06 | [train_policy] epoch #839 | computing descent direction
2022-04-23 14:24:06 | [train_policy] epoch #839 | descent direction computed
2022-04-23 14:24:06 | [train_policy] epoch #839 | backtrack iters: 1
2022-04-23 14:24:06 | [train_policy] epoch #839 | optimization finished
2022-04-23 14:24:06 | [train_policy] epoch #839 | Computing KL after
2022-04-23 14:24:06 | [train_policy] epoch #839 | Computing loss after
2022-04-23 14:24:06 | [train_policy] epoch #839 | Fitting baseline...
2022-04-23 14:24:06 | [train_policy] epoch #839 | Saving snapshot...
2022-04-23 14:24:06 | [train_policy] epoch #839 | Saved
2022-04-23 14:24:06 | [train_policy] epoch #839 | Time 293.84 s
2022-04-23 14:24:06 | [train_policy] epoch #839 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119934
Evaluation/AverageDiscountedReturn          -40.4776
Evaluation/AverageReturn                    -40.4776
Evaluation/CompletionRate                     0
Evaluation/Iteration                        839
Evaluation/MaxReturn                        -29.1464
Evaluation/MinReturn                        -63.9581
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.6948
Extras/EpisodeRewardMean                    -40.6156
LinearFeatureBaseline/ExplainedVariance       0.881539
PolicyExecTime                                0.0930867
ProcessExecTime                               0.0112574
TotalEnvSteps                            850080
policy/Entropy                               -1.84582
policy/KL                                     0.00663804
policy/KLBefore                               0
policy/LossAfter                             -0.0114787
policy/LossBefore                             2.12032e-09
policy/Perplexity                             0.157896
policy/dLoss                                  0.0114787
---------------------------------------  ----------------
2022-04-23 14:24:06 | [train_policy] epoch #840 | Obtaining samples for iteration 840...
2022-04-23 14:24:06 | [train_policy] epoch #840 | Logging diagnostics...
2022-04-23 14:24:06 | [train_policy] epoch #840 | Optimizing policy...
2022-04-23 14:24:06 | [train_policy] epoch #840 | Computing loss before
2022-04-23 14:24:06 | [train_policy] epoch #840 | Computing KL before
2022-04-23 14:24:06 | [train_policy] epoch #840 | Optimizing
2022-04-23 14:24:06 | [train_policy] epoch #840 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:06 | [train_policy] epoch #840 | computing loss before
2022-04-23 14:24:06 | [train_policy] epoch #840 | computing gradient
2022-04-23 14:24:06 | [train_policy] epoch #840 | gradient computed
2022-04-23 14:24:06 | [train_policy] epoch #840 | computing descent direction
2022-04-23 14:24:06 | [train_policy] epoch #840 | descent direction computed
2022-04-23 14:24:06 | [train_policy] epoch #840 | backtrack iters: 1
2022-04-23 14:24:06 | [train_policy] epoch #840 | optimization finished
2022-04-23 14:24:06 | [train_policy] epoch #840 | Computing KL after
2022-04-23 14:24:06 | [train_policy] epoch #840 | Computing loss after
2022-04-23 14:24:06 | [train_policy] epoch #840 | Fitting baseline...
2022-04-23 14:24:06 | [train_policy] epoch #840 | Saving snapshot...
2022-04-23 14:24:06 | [train_policy] epoch #840 | Saved
2022-04-23 14:24:06 | [train_policy] epoch #840 | Time 294.16 s
2022-04-23 14:24:06 | [train_policy] epoch #840 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119868
Evaluation/AverageDiscountedReturn          -41.6563
Evaluation/AverageReturn                    -41.6563
Evaluation/CompletionRate                     0
Evaluation/Iteration                        840
Evaluation/MaxReturn                        -28.729
Evaluation/MinReturn                        -71.8494
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.56993
Extras/EpisodeRewardMean                    -41.7432
LinearFeatureBaseline/ExplainedVariance       0.84667
PolicyExecTime                                0.0904365
ProcessExecTime                               0.0112085
TotalEnvSteps                            851092
policy/Entropy                               -1.84908
policy/KL                                     0.00646769
policy/KLBefore                               0
policy/LossAfter                             -0.0152395
policy/LossBefore                            -2.40303e-08
policy/Perplexity                             0.157382
policy/dLoss                                  0.0152395
---------------------------------------  ----------------
2022-04-23 14:24:06 | [train_policy] epoch #841 | Obtaining samples for iteration 841...
2022-04-23 14:24:06 | [train_policy] epoch #841 | Logging diagnostics...
2022-04-23 14:24:06 | [train_policy] epoch #841 | Optimizing policy...
2022-04-23 14:24:06 | [train_policy] epoch #841 | Computing loss before
2022-04-23 14:24:06 | [train_policy] epoch #841 | Computing KL before
2022-04-23 14:24:06 | [train_policy] epoch #841 | Optimizing
2022-04-23 14:24:06 | [train_policy] epoch #841 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:06 | [train_policy] epoch #841 | computing loss before
2022-04-23 14:24:06 | [train_policy] epoch #841 | computing gradient
2022-04-23 14:24:06 | [train_policy] epoch #841 | gradient computed
2022-04-23 14:24:06 | [train_policy] epoch #841 | computing descent direction
2022-04-23 14:24:06 | [train_policy] epoch #841 | descent direction computed
2022-04-23 14:24:06 | [train_policy] epoch #841 | backtrack iters: 1
2022-04-23 14:24:06 | [train_policy] epoch #841 | optimization finished
2022-04-23 14:24:06 | [train_policy] epoch #841 | Computing KL after
2022-04-23 14:24:06 | [train_policy] epoch #841 | Computing loss after
2022-04-23 14:24:06 | [train_policy] epoch #841 | Fitting baseline...
2022-04-23 14:24:06 | [train_policy] epoch #841 | Saving snapshot...
2022-04-23 14:24:06 | [train_policy] epoch #841 | Saved
2022-04-23 14:24:06 | [train_policy] epoch #841 | Time 294.48 s
2022-04-23 14:24:06 | [train_policy] epoch #841 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.118835
Evaluation/AverageDiscountedReturn          -41.5499
Evaluation/AverageReturn                    -41.5499
Evaluation/CompletionRate                     0
Evaluation/Iteration                        841
Evaluation/MaxReturn                        -29.0702
Evaluation/MinReturn                        -72.6143
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.14892
Extras/EpisodeRewardMean                    -41.3953
LinearFeatureBaseline/ExplainedVariance       0.868406
PolicyExecTime                                0.0891886
ProcessExecTime                               0.0110657
TotalEnvSteps                            852104
policy/Entropy                               -1.91919
policy/KL                                     0.00670443
policy/KLBefore                               0
policy/LossAfter                             -0.0161852
policy/LossBefore                             6.24317e-09
policy/Perplexity                             0.146725
policy/dLoss                                  0.0161852
---------------------------------------  ----------------
2022-04-23 14:24:06 | [train_policy] epoch #842 | Obtaining samples for iteration 842...
2022-04-23 14:24:07 | [train_policy] epoch #842 | Logging diagnostics...
2022-04-23 14:24:07 | [train_policy] epoch #842 | Optimizing policy...
2022-04-23 14:24:07 | [train_policy] epoch #842 | Computing loss before
2022-04-23 14:24:07 | [train_policy] epoch #842 | Computing KL before
2022-04-23 14:24:07 | [train_policy] epoch #842 | Optimizing
2022-04-23 14:24:07 | [train_policy] epoch #842 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:07 | [train_policy] epoch #842 | computing loss before
2022-04-23 14:24:07 | [train_policy] epoch #842 | computing gradient
2022-04-23 14:24:07 | [train_policy] epoch #842 | gradient computed
2022-04-23 14:24:07 | [train_policy] epoch #842 | computing descent direction
2022-04-23 14:24:07 | [train_policy] epoch #842 | descent direction computed
2022-04-23 14:24:07 | [train_policy] epoch #842 | backtrack iters: 1
2022-04-23 14:24:07 | [train_policy] epoch #842 | optimization finished
2022-04-23 14:24:07 | [train_policy] epoch #842 | Computing KL after
2022-04-23 14:24:07 | [train_policy] epoch #842 | Computing loss after
2022-04-23 14:24:07 | [train_policy] epoch #842 | Fitting baseline...
2022-04-23 14:24:07 | [train_policy] epoch #842 | Saving snapshot...
2022-04-23 14:24:07 | [train_policy] epoch #842 | Saved
2022-04-23 14:24:07 | [train_policy] epoch #842 | Time 294.80 s
2022-04-23 14:24:07 | [train_policy] epoch #842 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118367
Evaluation/AverageDiscountedReturn          -40.9367
Evaluation/AverageReturn                    -40.9367
Evaluation/CompletionRate                     0
Evaluation/Iteration                        842
Evaluation/MaxReturn                        -29.0096
Evaluation/MinReturn                        -63.7603
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.29516
Extras/EpisodeRewardMean                    -41.2794
LinearFeatureBaseline/ExplainedVariance       0.898779
PolicyExecTime                                0.0908098
ProcessExecTime                               0.0112381
TotalEnvSteps                            853116
policy/Entropy                               -1.95748
policy/KL                                     0.00682639
policy/KLBefore                               0
policy/LossAfter                             -0.0131488
policy/LossBefore                            -4.71183e-10
policy/Perplexity                             0.141213
policy/dLoss                                  0.0131488
---------------------------------------  ----------------
2022-04-23 14:24:07 | [train_policy] epoch #843 | Obtaining samples for iteration 843...
2022-04-23 14:24:07 | [train_policy] epoch #843 | Logging diagnostics...
2022-04-23 14:24:07 | [train_policy] epoch #843 | Optimizing policy...
2022-04-23 14:24:07 | [train_policy] epoch #843 | Computing loss before
2022-04-23 14:24:07 | [train_policy] epoch #843 | Computing KL before
2022-04-23 14:24:07 | [train_policy] epoch #843 | Optimizing
2022-04-23 14:24:07 | [train_policy] epoch #843 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:07 | [train_policy] epoch #843 | computing loss before
2022-04-23 14:24:07 | [train_policy] epoch #843 | computing gradient
2022-04-23 14:24:07 | [train_policy] epoch #843 | gradient computed
2022-04-23 14:24:07 | [train_policy] epoch #843 | computing descent direction
2022-04-23 14:24:07 | [train_policy] epoch #843 | descent direction computed
2022-04-23 14:24:07 | [train_policy] epoch #843 | backtrack iters: 1
2022-04-23 14:24:07 | [train_policy] epoch #843 | optimization finished
2022-04-23 14:24:07 | [train_policy] epoch #843 | Computing KL after
2022-04-23 14:24:07 | [train_policy] epoch #843 | Computing loss after
2022-04-23 14:24:07 | [train_policy] epoch #843 | Fitting baseline...
2022-04-23 14:24:07 | [train_policy] epoch #843 | Saving snapshot...
2022-04-23 14:24:07 | [train_policy] epoch #843 | Saved
2022-04-23 14:24:07 | [train_policy] epoch #843 | Time 295.12 s
2022-04-23 14:24:07 | [train_policy] epoch #843 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120411
Evaluation/AverageDiscountedReturn          -40.9227
Evaluation/AverageReturn                    -40.9227
Evaluation/CompletionRate                     0
Evaluation/Iteration                        843
Evaluation/MaxReturn                        -29.0672
Evaluation/MinReturn                        -64.6285
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.38398
Extras/EpisodeRewardMean                    -40.8174
LinearFeatureBaseline/ExplainedVariance       0.875201
PolicyExecTime                                0.0919659
ProcessExecTime                               0.0113564
TotalEnvSteps                            854128
policy/Entropy                               -1.93364
policy/KL                                     0.00675809
policy/KLBefore                               0
policy/LossAfter                             -0.0181871
policy/LossBefore                             7.53893e-09
policy/Perplexity                             0.144621
policy/dLoss                                  0.0181871
---------------------------------------  ----------------
2022-04-23 14:24:07 | [train_policy] epoch #844 | Obtaining samples for iteration 844...
2022-04-23 14:24:07 | [train_policy] epoch #844 | Logging diagnostics...
2022-04-23 14:24:07 | [train_policy] epoch #844 | Optimizing policy...
2022-04-23 14:24:07 | [train_policy] epoch #844 | Computing loss before
2022-04-23 14:24:07 | [train_policy] epoch #844 | Computing KL before
2022-04-23 14:24:07 | [train_policy] epoch #844 | Optimizing
2022-04-23 14:24:07 | [train_policy] epoch #844 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:07 | [train_policy] epoch #844 | computing loss before
2022-04-23 14:24:07 | [train_policy] epoch #844 | computing gradient
2022-04-23 14:24:07 | [train_policy] epoch #844 | gradient computed
2022-04-23 14:24:07 | [train_policy] epoch #844 | computing descent direction
2022-04-23 14:24:07 | [train_policy] epoch #844 | descent direction computed
2022-04-23 14:24:07 | [train_policy] epoch #844 | backtrack iters: 1
2022-04-23 14:24:07 | [train_policy] epoch #844 | optimization finished
2022-04-23 14:24:07 | [train_policy] epoch #844 | Computing KL after
2022-04-23 14:24:07 | [train_policy] epoch #844 | Computing loss after
2022-04-23 14:24:07 | [train_policy] epoch #844 | Fitting baseline...
2022-04-23 14:24:07 | [train_policy] epoch #844 | Saving snapshot...
2022-04-23 14:24:07 | [train_policy] epoch #844 | Saved
2022-04-23 14:24:07 | [train_policy] epoch #844 | Time 295.45 s
2022-04-23 14:24:07 | [train_policy] epoch #844 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119371
Evaluation/AverageDiscountedReturn          -39.8043
Evaluation/AverageReturn                    -39.8043
Evaluation/CompletionRate                     0
Evaluation/Iteration                        844
Evaluation/MaxReturn                        -29.7569
Evaluation/MinReturn                        -58.8803
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.50851
Extras/EpisodeRewardMean                    -39.7174
LinearFeatureBaseline/ExplainedVariance       0.907406
PolicyExecTime                                0.0944307
ProcessExecTime                               0.0112112
TotalEnvSteps                            855140
policy/Entropy                               -1.95344
policy/KL                                     0.00668325
policy/KLBefore                               0
policy/LossAfter                             -0.0157957
policy/LossBefore                             1.60202e-08
policy/Perplexity                             0.141786
policy/dLoss                                  0.0157957
---------------------------------------  ----------------
2022-04-23 14:24:07 | [train_policy] epoch #845 | Obtaining samples for iteration 845...
2022-04-23 14:24:07 | [train_policy] epoch #845 | Logging diagnostics...
2022-04-23 14:24:07 | [train_policy] epoch #845 | Optimizing policy...
2022-04-23 14:24:07 | [train_policy] epoch #845 | Computing loss before
2022-04-23 14:24:07 | [train_policy] epoch #845 | Computing KL before
2022-04-23 14:24:07 | [train_policy] epoch #845 | Optimizing
2022-04-23 14:24:07 | [train_policy] epoch #845 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:07 | [train_policy] epoch #845 | computing loss before
2022-04-23 14:24:08 | [train_policy] epoch #845 | computing gradient
2022-04-23 14:24:08 | [train_policy] epoch #845 | gradient computed
2022-04-23 14:24:08 | [train_policy] epoch #845 | computing descent direction
2022-04-23 14:24:08 | [train_policy] epoch #845 | descent direction computed
2022-04-23 14:24:08 | [train_policy] epoch #845 | backtrack iters: 0
2022-04-23 14:24:08 | [train_policy] epoch #845 | optimization finished
2022-04-23 14:24:08 | [train_policy] epoch #845 | Computing KL after
2022-04-23 14:24:08 | [train_policy] epoch #845 | Computing loss after
2022-04-23 14:24:08 | [train_policy] epoch #845 | Fitting baseline...
2022-04-23 14:24:08 | [train_policy] epoch #845 | Saving snapshot...
2022-04-23 14:24:08 | [train_policy] epoch #845 | Saved
2022-04-23 14:24:08 | [train_policy] epoch #845 | Time 295.78 s
2022-04-23 14:24:08 | [train_policy] epoch #845 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.11961
Evaluation/AverageDiscountedReturn          -41.09
Evaluation/AverageReturn                    -41.09
Evaluation/CompletionRate                     0
Evaluation/Iteration                        845
Evaluation/MaxReturn                        -30.6905
Evaluation/MinReturn                        -65.5651
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.78248
Extras/EpisodeRewardMean                    -40.8514
LinearFeatureBaseline/ExplainedVariance       0.887272
PolicyExecTime                                0.0948591
ProcessExecTime                               0.0111811
TotalEnvSteps                            856152
policy/Entropy                               -1.97188
policy/KL                                     0.00993055
policy/KLBefore                               0
policy/LossAfter                             -0.0214116
policy/LossBefore                            -8.2457e-10
policy/Perplexity                             0.139195
policy/dLoss                                  0.0214116
---------------------------------------  ---------------
2022-04-23 14:24:08 | [train_policy] epoch #846 | Obtaining samples for iteration 846...
2022-04-23 14:24:08 | [train_policy] epoch #846 | Logging diagnostics...
2022-04-23 14:24:08 | [train_policy] epoch #846 | Optimizing policy...
2022-04-23 14:24:08 | [train_policy] epoch #846 | Computing loss before
2022-04-23 14:24:08 | [train_policy] epoch #846 | Computing KL before
2022-04-23 14:24:08 | [train_policy] epoch #846 | Optimizing
2022-04-23 14:24:08 | [train_policy] epoch #846 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:08 | [train_policy] epoch #846 | computing loss before
2022-04-23 14:24:08 | [train_policy] epoch #846 | computing gradient
2022-04-23 14:24:08 | [train_policy] epoch #846 | gradient computed
2022-04-23 14:24:08 | [train_policy] epoch #846 | computing descent direction
2022-04-23 14:24:08 | [train_policy] epoch #846 | descent direction computed
2022-04-23 14:24:08 | [train_policy] epoch #846 | backtrack iters: 0
2022-04-23 14:24:08 | [train_policy] epoch #846 | optimization finished
2022-04-23 14:24:08 | [train_policy] epoch #846 | Computing KL after
2022-04-23 14:24:08 | [train_policy] epoch #846 | Computing loss after
2022-04-23 14:24:08 | [train_policy] epoch #846 | Fitting baseline...
2022-04-23 14:24:08 | [train_policy] epoch #846 | Saving snapshot...
2022-04-23 14:24:08 | [train_policy] epoch #846 | Saved
2022-04-23 14:24:08 | [train_policy] epoch #846 | Time 296.10 s
2022-04-23 14:24:08 | [train_policy] epoch #846 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119814
Evaluation/AverageDiscountedReturn          -41.6119
Evaluation/AverageReturn                    -41.6119
Evaluation/CompletionRate                     0
Evaluation/Iteration                        846
Evaluation/MaxReturn                        -32.1541
Evaluation/MinReturn                        -75.093
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.48924
Extras/EpisodeRewardMean                    -41.5413
LinearFeatureBaseline/ExplainedVariance       0.870833
PolicyExecTime                                0.0926228
ProcessExecTime                               0.0112813
TotalEnvSteps                            857164
policy/Entropy                               -1.91396
policy/KL                                     0.00940037
policy/KLBefore                               0
policy/LossAfter                             -0.0145292
policy/LossBefore                            -1.76694e-09
policy/Perplexity                             0.147496
policy/dLoss                                  0.0145292
---------------------------------------  ----------------
2022-04-23 14:24:08 | [train_policy] epoch #847 | Obtaining samples for iteration 847...
2022-04-23 14:24:08 | [train_policy] epoch #847 | Logging diagnostics...
2022-04-23 14:24:08 | [train_policy] epoch #847 | Optimizing policy...
2022-04-23 14:24:08 | [train_policy] epoch #847 | Computing loss before
2022-04-23 14:24:08 | [train_policy] epoch #847 | Computing KL before
2022-04-23 14:24:08 | [train_policy] epoch #847 | Optimizing
2022-04-23 14:24:08 | [train_policy] epoch #847 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:08 | [train_policy] epoch #847 | computing loss before
2022-04-23 14:24:08 | [train_policy] epoch #847 | computing gradient
2022-04-23 14:24:08 | [train_policy] epoch #847 | gradient computed
2022-04-23 14:24:08 | [train_policy] epoch #847 | computing descent direction
2022-04-23 14:24:08 | [train_policy] epoch #847 | descent direction computed
2022-04-23 14:24:08 | [train_policy] epoch #847 | backtrack iters: 0
2022-04-23 14:24:08 | [train_policy] epoch #847 | optimization finished
2022-04-23 14:24:08 | [train_policy] epoch #847 | Computing KL after
2022-04-23 14:24:08 | [train_policy] epoch #847 | Computing loss after
2022-04-23 14:24:08 | [train_policy] epoch #847 | Fitting baseline...
2022-04-23 14:24:08 | [train_policy] epoch #847 | Saving snapshot...
2022-04-23 14:24:08 | [train_policy] epoch #847 | Saved
2022-04-23 14:24:08 | [train_policy] epoch #847 | Time 296.43 s
2022-04-23 14:24:08 | [train_policy] epoch #847 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.116837
Evaluation/AverageDiscountedReturn          -41.4116
Evaluation/AverageReturn                    -41.4116
Evaluation/CompletionRate                     0
Evaluation/Iteration                        847
Evaluation/MaxReturn                        -30.8554
Evaluation/MinReturn                        -87.4291
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.45958
Extras/EpisodeRewardMean                    -41.2866
LinearFeatureBaseline/ExplainedVariance       0.824874
PolicyExecTime                                0.101001
ProcessExecTime                               0.0115185
TotalEnvSteps                            858176
policy/Entropy                               -1.90844
policy/KL                                     0.00971458
policy/KLBefore                               0
policy/LossAfter                             -0.0106825
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.148312
policy/dLoss                                  0.0106825
---------------------------------------  ---------------
2022-04-23 14:24:08 | [train_policy] epoch #848 | Obtaining samples for iteration 848...
2022-04-23 14:24:08 | [train_policy] epoch #848 | Logging diagnostics...
2022-04-23 14:24:08 | [train_policy] epoch #848 | Optimizing policy...
2022-04-23 14:24:08 | [train_policy] epoch #848 | Computing loss before
2022-04-23 14:24:08 | [train_policy] epoch #848 | Computing KL before
2022-04-23 14:24:08 | [train_policy] epoch #848 | Optimizing
2022-04-23 14:24:08 | [train_policy] epoch #848 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:08 | [train_policy] epoch #848 | computing loss before
2022-04-23 14:24:08 | [train_policy] epoch #848 | computing gradient
2022-04-23 14:24:08 | [train_policy] epoch #848 | gradient computed
2022-04-23 14:24:08 | [train_policy] epoch #848 | computing descent direction
2022-04-23 14:24:09 | [train_policy] epoch #848 | descent direction computed
2022-04-23 14:24:09 | [train_policy] epoch #848 | backtrack iters: 0
2022-04-23 14:24:09 | [train_policy] epoch #848 | optimization finished
2022-04-23 14:24:09 | [train_policy] epoch #848 | Computing KL after
2022-04-23 14:24:09 | [train_policy] epoch #848 | Computing loss after
2022-04-23 14:24:09 | [train_policy] epoch #848 | Fitting baseline...
2022-04-23 14:24:09 | [train_policy] epoch #848 | Saving snapshot...
2022-04-23 14:24:09 | [train_policy] epoch #848 | Saved
2022-04-23 14:24:09 | [train_policy] epoch #848 | Time 296.76 s
2022-04-23 14:24:09 | [train_policy] epoch #848 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.116687
Evaluation/AverageDiscountedReturn          -41.1877
Evaluation/AverageReturn                    -41.1877
Evaluation/CompletionRate                     0
Evaluation/Iteration                        848
Evaluation/MaxReturn                        -29.9931
Evaluation/MinReturn                        -72.3311
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.38452
Extras/EpisodeRewardMean                    -41.0384
LinearFeatureBaseline/ExplainedVariance       0.858433
PolicyExecTime                                0.101283
ProcessExecTime                               0.0121024
TotalEnvSteps                            859188
policy/Entropy                               -1.88172
policy/KL                                     0.00956355
policy/KLBefore                               0
policy/LossAfter                             -0.0135199
policy/LossBefore                            -4.91797e-09
policy/Perplexity                             0.152329
policy/dLoss                                  0.0135199
---------------------------------------  ----------------
2022-04-23 14:24:09 | [train_policy] epoch #849 | Obtaining samples for iteration 849...
2022-04-23 14:24:09 | [train_policy] epoch #849 | Logging diagnostics...
2022-04-23 14:24:09 | [train_policy] epoch #849 | Optimizing policy...
2022-04-23 14:24:09 | [train_policy] epoch #849 | Computing loss before
2022-04-23 14:24:09 | [train_policy] epoch #849 | Computing KL before
2022-04-23 14:24:09 | [train_policy] epoch #849 | Optimizing
2022-04-23 14:24:09 | [train_policy] epoch #849 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:09 | [train_policy] epoch #849 | computing loss before
2022-04-23 14:24:09 | [train_policy] epoch #849 | computing gradient
2022-04-23 14:24:09 | [train_policy] epoch #849 | gradient computed
2022-04-23 14:24:09 | [train_policy] epoch #849 | computing descent direction
2022-04-23 14:24:09 | [train_policy] epoch #849 | descent direction computed
2022-04-23 14:24:09 | [train_policy] epoch #849 | backtrack iters: 1
2022-04-23 14:24:09 | [train_policy] epoch #849 | optimization finished
2022-04-23 14:24:09 | [train_policy] epoch #849 | Computing KL after
2022-04-23 14:24:09 | [train_policy] epoch #849 | Computing loss after
2022-04-23 14:24:09 | [train_policy] epoch #849 | Fitting baseline...
2022-04-23 14:24:09 | [train_policy] epoch #849 | Saving snapshot...
2022-04-23 14:24:09 | [train_policy] epoch #849 | Saved
2022-04-23 14:24:09 | [train_policy] epoch #849 | Time 297.08 s
2022-04-23 14:24:09 | [train_policy] epoch #849 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119381
Evaluation/AverageDiscountedReturn          -40.4881
Evaluation/AverageReturn                    -40.4881
Evaluation/CompletionRate                     0
Evaluation/Iteration                        849
Evaluation/MaxReturn                        -29.4811
Evaluation/MinReturn                        -64.7418
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.06114
Extras/EpisodeRewardMean                    -40.7332
LinearFeatureBaseline/ExplainedVariance       0.88689
PolicyExecTime                                0.091598
ProcessExecTime                               0.0114522
TotalEnvSteps                            860200
policy/Entropy                               -1.91161
policy/KL                                     0.00644812
policy/KLBefore                               0
policy/LossAfter                             -0.0135987
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.147843
policy/dLoss                                  0.0135988
---------------------------------------  ----------------
2022-04-23 14:24:09 | [train_policy] epoch #850 | Obtaining samples for iteration 850...
2022-04-23 14:24:09 | [train_policy] epoch #850 | Logging diagnostics...
2022-04-23 14:24:09 | [train_policy] epoch #850 | Optimizing policy...
2022-04-23 14:24:09 | [train_policy] epoch #850 | Computing loss before
2022-04-23 14:24:09 | [train_policy] epoch #850 | Computing KL before
2022-04-23 14:24:09 | [train_policy] epoch #850 | Optimizing
2022-04-23 14:24:09 | [train_policy] epoch #850 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:09 | [train_policy] epoch #850 | computing loss before
2022-04-23 14:24:09 | [train_policy] epoch #850 | computing gradient
2022-04-23 14:24:09 | [train_policy] epoch #850 | gradient computed
2022-04-23 14:24:09 | [train_policy] epoch #850 | computing descent direction
2022-04-23 14:24:09 | [train_policy] epoch #850 | descent direction computed
2022-04-23 14:24:09 | [train_policy] epoch #850 | backtrack iters: 1
2022-04-23 14:24:09 | [train_policy] epoch #850 | optimization finished
2022-04-23 14:24:09 | [train_policy] epoch #850 | Computing KL after
2022-04-23 14:24:09 | [train_policy] epoch #850 | Computing loss after
2022-04-23 14:24:09 | [train_policy] epoch #850 | Fitting baseline...
2022-04-23 14:24:09 | [train_policy] epoch #850 | Saving snapshot...
2022-04-23 14:24:09 | [train_policy] epoch #850 | Saved
2022-04-23 14:24:09 | [train_policy] epoch #850 | Time 297.41 s
2022-04-23 14:24:09 | [train_policy] epoch #850 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118139
Evaluation/AverageDiscountedReturn          -40.8521
Evaluation/AverageReturn                    -40.8521
Evaluation/CompletionRate                     0
Evaluation/Iteration                        850
Evaluation/MaxReturn                        -29.8994
Evaluation/MinReturn                        -72.8048
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.63427
Extras/EpisodeRewardMean                    -40.7658
LinearFeatureBaseline/ExplainedVariance       0.848496
PolicyExecTime                                0.0937257
ProcessExecTime                               0.0111656
TotalEnvSteps                            861212
policy/Entropy                               -1.91781
policy/KL                                     0.00665716
policy/KLBefore                               0
policy/LossAfter                             -0.0122861
policy/LossBefore                             2.59151e-09
policy/Perplexity                             0.146929
policy/dLoss                                  0.0122861
---------------------------------------  ----------------
2022-04-23 14:24:09 | [train_policy] epoch #851 | Obtaining samples for iteration 851...
2022-04-23 14:24:09 | [train_policy] epoch #851 | Logging diagnostics...
2022-04-23 14:24:09 | [train_policy] epoch #851 | Optimizing policy...
2022-04-23 14:24:09 | [train_policy] epoch #851 | Computing loss before
2022-04-23 14:24:09 | [train_policy] epoch #851 | Computing KL before
2022-04-23 14:24:09 | [train_policy] epoch #851 | Optimizing
2022-04-23 14:24:09 | [train_policy] epoch #851 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:09 | [train_policy] epoch #851 | computing loss before
2022-04-23 14:24:09 | [train_policy] epoch #851 | computing gradient
2022-04-23 14:24:09 | [train_policy] epoch #851 | gradient computed
2022-04-23 14:24:09 | [train_policy] epoch #851 | computing descent direction
2022-04-23 14:24:09 | [train_policy] epoch #851 | descent direction computed
2022-04-23 14:24:09 | [train_policy] epoch #851 | backtrack iters: 0
2022-04-23 14:24:09 | [train_policy] epoch #851 | optimization finished
2022-04-23 14:24:09 | [train_policy] epoch #851 | Computing KL after
2022-04-23 14:24:09 | [train_policy] epoch #851 | Computing loss after
2022-04-23 14:24:09 | [train_policy] epoch #851 | Fitting baseline...
2022-04-23 14:24:10 | [train_policy] epoch #851 | Saving snapshot...
2022-04-23 14:24:10 | [train_policy] epoch #851 | Saved
2022-04-23 14:24:10 | [train_policy] epoch #851 | Time 297.72 s
2022-04-23 14:24:10 | [train_policy] epoch #851 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.119004
Evaluation/AverageDiscountedReturn          -40.787
Evaluation/AverageReturn                    -40.787
Evaluation/CompletionRate                     0
Evaluation/Iteration                        851
Evaluation/MaxReturn                        -32.5154
Evaluation/MinReturn                        -72.7307
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.70276
Extras/EpisodeRewardMean                    -41.1809
LinearFeatureBaseline/ExplainedVariance       0.872771
PolicyExecTime                                0.0915308
ProcessExecTime                               0.011049
TotalEnvSteps                            862224
policy/Entropy                               -1.9532
policy/KL                                     0.00927286
policy/KLBefore                               0
policy/LossAfter                             -0.0188093
policy/LossBefore                             1.26041e-08
policy/Perplexity                             0.141819
policy/dLoss                                  0.0188093
---------------------------------------  ----------------
2022-04-23 14:24:10 | [train_policy] epoch #852 | Obtaining samples for iteration 852...
2022-04-23 14:24:10 | [train_policy] epoch #852 | Logging diagnostics...
2022-04-23 14:24:10 | [train_policy] epoch #852 | Optimizing policy...
2022-04-23 14:24:10 | [train_policy] epoch #852 | Computing loss before
2022-04-23 14:24:10 | [train_policy] epoch #852 | Computing KL before
2022-04-23 14:24:10 | [train_policy] epoch #852 | Optimizing
2022-04-23 14:24:10 | [train_policy] epoch #852 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:10 | [train_policy] epoch #852 | computing loss before
2022-04-23 14:24:10 | [train_policy] epoch #852 | computing gradient
2022-04-23 14:24:10 | [train_policy] epoch #852 | gradient computed
2022-04-23 14:24:10 | [train_policy] epoch #852 | computing descent direction
2022-04-23 14:24:10 | [train_policy] epoch #852 | descent direction computed
2022-04-23 14:24:10 | [train_policy] epoch #852 | backtrack iters: 1
2022-04-23 14:24:10 | [train_policy] epoch #852 | optimization finished
2022-04-23 14:24:10 | [train_policy] epoch #852 | Computing KL after
2022-04-23 14:24:10 | [train_policy] epoch #852 | Computing loss after
2022-04-23 14:24:10 | [train_policy] epoch #852 | Fitting baseline...
2022-04-23 14:24:10 | [train_policy] epoch #852 | Saving snapshot...
2022-04-23 14:24:10 | [train_policy] epoch #852 | Saved
2022-04-23 14:24:10 | [train_policy] epoch #852 | Time 298.05 s
2022-04-23 14:24:10 | [train_policy] epoch #852 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120104
Evaluation/AverageDiscountedReturn          -40.8261
Evaluation/AverageReturn                    -40.8261
Evaluation/CompletionRate                     0
Evaluation/Iteration                        852
Evaluation/MaxReturn                        -29.5108
Evaluation/MinReturn                        -72.5419
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.92848
Extras/EpisodeRewardMean                    -40.3992
LinearFeatureBaseline/ExplainedVariance       0.874376
PolicyExecTime                                0.0920923
ProcessExecTime                               0.0112495
TotalEnvSteps                            863236
policy/Entropy                               -1.95161
policy/KL                                     0.00653423
policy/KLBefore                               0
policy/LossAfter                             -0.0153188
policy/LossBefore                             1.17796e-08
policy/Perplexity                             0.142045
policy/dLoss                                  0.0153188
---------------------------------------  ----------------
2022-04-23 14:24:10 | [train_policy] epoch #853 | Obtaining samples for iteration 853...
2022-04-23 14:24:10 | [train_policy] epoch #853 | Logging diagnostics...
2022-04-23 14:24:10 | [train_policy] epoch #853 | Optimizing policy...
2022-04-23 14:24:10 | [train_policy] epoch #853 | Computing loss before
2022-04-23 14:24:10 | [train_policy] epoch #853 | Computing KL before
2022-04-23 14:24:10 | [train_policy] epoch #853 | Optimizing
2022-04-23 14:24:10 | [train_policy] epoch #853 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:10 | [train_policy] epoch #853 | computing loss before
2022-04-23 14:24:10 | [train_policy] epoch #853 | computing gradient
2022-04-23 14:24:10 | [train_policy] epoch #853 | gradient computed
2022-04-23 14:24:10 | [train_policy] epoch #853 | computing descent direction
2022-04-23 14:24:10 | [train_policy] epoch #853 | descent direction computed
2022-04-23 14:24:10 | [train_policy] epoch #853 | backtrack iters: 1
2022-04-23 14:24:10 | [train_policy] epoch #853 | optimization finished
2022-04-23 14:24:10 | [train_policy] epoch #853 | Computing KL after
2022-04-23 14:24:10 | [train_policy] epoch #853 | Computing loss after
2022-04-23 14:24:10 | [train_policy] epoch #853 | Fitting baseline...
2022-04-23 14:24:10 | [train_policy] epoch #853 | Saving snapshot...
2022-04-23 14:24:10 | [train_policy] epoch #853 | Saved
2022-04-23 14:24:10 | [train_policy] epoch #853 | Time 298.37 s
2022-04-23 14:24:10 | [train_policy] epoch #853 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.119245
Evaluation/AverageDiscountedReturn          -42.0566
Evaluation/AverageReturn                    -42.0566
Evaluation/CompletionRate                     0
Evaluation/Iteration                        853
Evaluation/MaxReturn                        -30.902
Evaluation/MinReturn                        -63.9426
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.74625
Extras/EpisodeRewardMean                    -42.1954
LinearFeatureBaseline/ExplainedVariance       0.832283
PolicyExecTime                                0.0939739
ProcessExecTime                               0.0112791
TotalEnvSteps                            864248
policy/Entropy                               -1.96264
policy/KL                                     0.00661192
policy/KLBefore                               0
policy/LossAfter                             -0.0122481
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             0.140488
policy/dLoss                                  0.0122481
---------------------------------------  ---------------
2022-04-23 14:24:10 | [train_policy] epoch #854 | Obtaining samples for iteration 854...
2022-04-23 14:24:10 | [train_policy] epoch #854 | Logging diagnostics...
2022-04-23 14:24:10 | [train_policy] epoch #854 | Optimizing policy...
2022-04-23 14:24:10 | [train_policy] epoch #854 | Computing loss before
2022-04-23 14:24:10 | [train_policy] epoch #854 | Computing KL before
2022-04-23 14:24:10 | [train_policy] epoch #854 | Optimizing
2022-04-23 14:24:10 | [train_policy] epoch #854 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:10 | [train_policy] epoch #854 | computing loss before
2022-04-23 14:24:10 | [train_policy] epoch #854 | computing gradient
2022-04-23 14:24:10 | [train_policy] epoch #854 | gradient computed
2022-04-23 14:24:10 | [train_policy] epoch #854 | computing descent direction
2022-04-23 14:24:10 | [train_policy] epoch #854 | descent direction computed
2022-04-23 14:24:10 | [train_policy] epoch #854 | backtrack iters: 0
2022-04-23 14:24:10 | [train_policy] epoch #854 | optimization finished
2022-04-23 14:24:10 | [train_policy] epoch #854 | Computing KL after
2022-04-23 14:24:10 | [train_policy] epoch #854 | Computing loss after
2022-04-23 14:24:10 | [train_policy] epoch #854 | Fitting baseline...
2022-04-23 14:24:10 | [train_policy] epoch #854 | Saving snapshot...
2022-04-23 14:24:10 | [train_policy] epoch #854 | Saved
2022-04-23 14:24:10 | [train_policy] epoch #854 | Time 298.70 s
2022-04-23 14:24:10 | [train_policy] epoch #854 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119979
Evaluation/AverageDiscountedReturn          -40.8484
Evaluation/AverageReturn                    -40.8484
Evaluation/CompletionRate                     0
Evaluation/Iteration                        854
Evaluation/MaxReturn                        -31.9377
Evaluation/MinReturn                        -72.4544
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43184
Extras/EpisodeRewardMean                    -41.302
LinearFeatureBaseline/ExplainedVariance       0.865685
PolicyExecTime                                0.0932808
ProcessExecTime                               0.0114052
TotalEnvSteps                            865260
policy/Entropy                               -1.94233
policy/KL                                     0.00986793
policy/KLBefore                               0
policy/LossAfter                             -0.0175742
policy/LossBefore                            -5.88979e-09
policy/Perplexity                             0.14337
policy/dLoss                                  0.0175742
---------------------------------------  ----------------
2022-04-23 14:24:10 | [train_policy] epoch #855 | Obtaining samples for iteration 855...
2022-04-23 14:24:11 | [train_policy] epoch #855 | Logging diagnostics...
2022-04-23 14:24:11 | [train_policy] epoch #855 | Optimizing policy...
2022-04-23 14:24:11 | [train_policy] epoch #855 | Computing loss before
2022-04-23 14:24:11 | [train_policy] epoch #855 | Computing KL before
2022-04-23 14:24:11 | [train_policy] epoch #855 | Optimizing
2022-04-23 14:24:11 | [train_policy] epoch #855 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:11 | [train_policy] epoch #855 | computing loss before
2022-04-23 14:24:11 | [train_policy] epoch #855 | computing gradient
2022-04-23 14:24:11 | [train_policy] epoch #855 | gradient computed
2022-04-23 14:24:11 | [train_policy] epoch #855 | computing descent direction
2022-04-23 14:24:11 | [train_policy] epoch #855 | descent direction computed
2022-04-23 14:24:11 | [train_policy] epoch #855 | backtrack iters: 0
2022-04-23 14:24:11 | [train_policy] epoch #855 | optimization finished
2022-04-23 14:24:11 | [train_policy] epoch #855 | Computing KL after
2022-04-23 14:24:11 | [train_policy] epoch #855 | Computing loss after
2022-04-23 14:24:11 | [train_policy] epoch #855 | Fitting baseline...
2022-04-23 14:24:11 | [train_policy] epoch #855 | Saving snapshot...
2022-04-23 14:24:11 | [train_policy] epoch #855 | Saved
2022-04-23 14:24:11 | [train_policy] epoch #855 | Time 299.02 s
2022-04-23 14:24:11 | [train_policy] epoch #855 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.119695
Evaluation/AverageDiscountedReturn          -63.5947
Evaluation/AverageReturn                    -63.5947
Evaluation/CompletionRate                     0
Evaluation/Iteration                        855
Evaluation/MaxReturn                        -29.982
Evaluation/MinReturn                      -2052.29
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.644
Extras/EpisodeRewardMean                    -61.6003
LinearFeatureBaseline/ExplainedVariance       0.0136757
PolicyExecTime                                0.0934002
ProcessExecTime                               0.0112774
TotalEnvSteps                            866272
policy/Entropy                               -1.9425
policy/KL                                     0.00990132
policy/KLBefore                               0
policy/LossAfter                             -0.0217986
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.143346
policy/dLoss                                  0.0217986
---------------------------------------  ---------------
2022-04-23 14:24:11 | [train_policy] epoch #856 | Obtaining samples for iteration 856...
2022-04-23 14:24:11 | [train_policy] epoch #856 | Logging diagnostics...
2022-04-23 14:24:11 | [train_policy] epoch #856 | Optimizing policy...
2022-04-23 14:24:11 | [train_policy] epoch #856 | Computing loss before
2022-04-23 14:24:11 | [train_policy] epoch #856 | Computing KL before
2022-04-23 14:24:11 | [train_policy] epoch #856 | Optimizing
2022-04-23 14:24:11 | [train_policy] epoch #856 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:11 | [train_policy] epoch #856 | computing loss before
2022-04-23 14:24:11 | [train_policy] epoch #856 | computing gradient
2022-04-23 14:24:11 | [train_policy] epoch #856 | gradient computed
2022-04-23 14:24:11 | [train_policy] epoch #856 | computing descent direction
2022-04-23 14:24:11 | [train_policy] epoch #856 | descent direction computed
2022-04-23 14:24:11 | [train_policy] epoch #856 | backtrack iters: 0
2022-04-23 14:24:11 | [train_policy] epoch #856 | optimization finished
2022-04-23 14:24:11 | [train_policy] epoch #856 | Computing KL after
2022-04-23 14:24:11 | [train_policy] epoch #856 | Computing loss after
2022-04-23 14:24:11 | [train_policy] epoch #856 | Fitting baseline...
2022-04-23 14:24:11 | [train_policy] epoch #856 | Saving snapshot...
2022-04-23 14:24:11 | [train_policy] epoch #856 | Saved
2022-04-23 14:24:11 | [train_policy] epoch #856 | Time 299.34 s
2022-04-23 14:24:11 | [train_policy] epoch #856 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119565
Evaluation/AverageDiscountedReturn          -42.4844
Evaluation/AverageReturn                    -42.4844
Evaluation/CompletionRate                     0
Evaluation/Iteration                        856
Evaluation/MaxReturn                        -29.9112
Evaluation/MinReturn                        -72.4253
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.71439
Extras/EpisodeRewardMean                    -42.1826
LinearFeatureBaseline/ExplainedVariance     -57.3805
PolicyExecTime                                0.0936809
ProcessExecTime                               0.011384
TotalEnvSteps                            867284
policy/Entropy                               -1.90834
policy/KL                                     0.00950775
policy/KLBefore                               0
policy/LossAfter                             -0.0343708
policy/LossBefore                            -3.29828e-09
policy/Perplexity                             0.148327
policy/dLoss                                  0.0343708
---------------------------------------  ----------------
2022-04-23 14:24:11 | [train_policy] epoch #857 | Obtaining samples for iteration 857...
2022-04-23 14:24:11 | [train_policy] epoch #857 | Logging diagnostics...
2022-04-23 14:24:11 | [train_policy] epoch #857 | Optimizing policy...
2022-04-23 14:24:11 | [train_policy] epoch #857 | Computing loss before
2022-04-23 14:24:11 | [train_policy] epoch #857 | Computing KL before
2022-04-23 14:24:11 | [train_policy] epoch #857 | Optimizing
2022-04-23 14:24:11 | [train_policy] epoch #857 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:11 | [train_policy] epoch #857 | computing loss before
2022-04-23 14:24:11 | [train_policy] epoch #857 | computing gradient
2022-04-23 14:24:11 | [train_policy] epoch #857 | gradient computed
2022-04-23 14:24:11 | [train_policy] epoch #857 | computing descent direction
2022-04-23 14:24:11 | [train_policy] epoch #857 | descent direction computed
2022-04-23 14:24:11 | [train_policy] epoch #857 | backtrack iters: 1
2022-04-23 14:24:11 | [train_policy] epoch #857 | optimization finished
2022-04-23 14:24:11 | [train_policy] epoch #857 | Computing KL after
2022-04-23 14:24:11 | [train_policy] epoch #857 | Computing loss after
2022-04-23 14:24:11 | [train_policy] epoch #857 | Fitting baseline...
2022-04-23 14:24:11 | [train_policy] epoch #857 | Saving snapshot...
2022-04-23 14:24:11 | [train_policy] epoch #857 | Saved
2022-04-23 14:24:11 | [train_policy] epoch #857 | Time 299.66 s
2022-04-23 14:24:11 | [train_policy] epoch #857 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119886
Evaluation/AverageDiscountedReturn          -40.5
Evaluation/AverageReturn                    -40.5
Evaluation/CompletionRate                     0
Evaluation/Iteration                        857
Evaluation/MaxReturn                        -30.995
Evaluation/MinReturn                        -72.8524
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.98089
Extras/EpisodeRewardMean                    -40.7646
LinearFeatureBaseline/ExplainedVariance       0.875645
PolicyExecTime                                0.0916214
ProcessExecTime                               0.0114927
TotalEnvSteps                            868296
policy/Entropy                               -1.95635
policy/KL                                     0.00668766
policy/KLBefore                               0
policy/LossAfter                             -0.0150232
policy/LossBefore                             1.13084e-08
policy/Perplexity                             0.141373
policy/dLoss                                  0.0150233
---------------------------------------  ----------------
2022-04-23 14:24:11 | [train_policy] epoch #858 | Obtaining samples for iteration 858...
2022-04-23 14:24:12 | [train_policy] epoch #858 | Logging diagnostics...
2022-04-23 14:24:12 | [train_policy] epoch #858 | Optimizing policy...
2022-04-23 14:24:12 | [train_policy] epoch #858 | Computing loss before
2022-04-23 14:24:12 | [train_policy] epoch #858 | Computing KL before
2022-04-23 14:24:12 | [train_policy] epoch #858 | Optimizing
2022-04-23 14:24:12 | [train_policy] epoch #858 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:12 | [train_policy] epoch #858 | computing loss before
2022-04-23 14:24:12 | [train_policy] epoch #858 | computing gradient
2022-04-23 14:24:12 | [train_policy] epoch #858 | gradient computed
2022-04-23 14:24:12 | [train_policy] epoch #858 | computing descent direction
2022-04-23 14:24:12 | [train_policy] epoch #858 | descent direction computed
2022-04-23 14:24:12 | [train_policy] epoch #858 | backtrack iters: 1
2022-04-23 14:24:12 | [train_policy] epoch #858 | optimization finished
2022-04-23 14:24:12 | [train_policy] epoch #858 | Computing KL after
2022-04-23 14:24:12 | [train_policy] epoch #858 | Computing loss after
2022-04-23 14:24:12 | [train_policy] epoch #858 | Fitting baseline...
2022-04-23 14:24:12 | [train_policy] epoch #858 | Saving snapshot...
2022-04-23 14:24:12 | [train_policy] epoch #858 | Saved
2022-04-23 14:24:12 | [train_policy] epoch #858 | Time 299.98 s
2022-04-23 14:24:12 | [train_policy] epoch #858 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119434
Evaluation/AverageDiscountedReturn          -42.1529
Evaluation/AverageReturn                    -42.1529
Evaluation/CompletionRate                     0
Evaluation/Iteration                        858
Evaluation/MaxReturn                        -29.9816
Evaluation/MinReturn                        -73.5507
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.91565
Extras/EpisodeRewardMean                    -41.7452
LinearFeatureBaseline/ExplainedVariance       0.846818
PolicyExecTime                                0.0918183
ProcessExecTime                               0.011205
TotalEnvSteps                            869308
policy/Entropy                               -1.96016
policy/KL                                     0.00652927
policy/KLBefore                               0
policy/LossAfter                             -0.0158283
policy/LossBefore                             1.24863e-08
policy/Perplexity                             0.140836
policy/dLoss                                  0.0158283
---------------------------------------  ----------------
2022-04-23 14:24:12 | [train_policy] epoch #859 | Obtaining samples for iteration 859...
2022-04-23 14:24:12 | [train_policy] epoch #859 | Logging diagnostics...
2022-04-23 14:24:12 | [train_policy] epoch #859 | Optimizing policy...
2022-04-23 14:24:12 | [train_policy] epoch #859 | Computing loss before
2022-04-23 14:24:12 | [train_policy] epoch #859 | Computing KL before
2022-04-23 14:24:12 | [train_policy] epoch #859 | Optimizing
2022-04-23 14:24:12 | [train_policy] epoch #859 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:12 | [train_policy] epoch #859 | computing loss before
2022-04-23 14:24:12 | [train_policy] epoch #859 | computing gradient
2022-04-23 14:24:12 | [train_policy] epoch #859 | gradient computed
2022-04-23 14:24:12 | [train_policy] epoch #859 | computing descent direction
2022-04-23 14:24:12 | [train_policy] epoch #859 | descent direction computed
2022-04-23 14:24:12 | [train_policy] epoch #859 | backtrack iters: 1
2022-04-23 14:24:12 | [train_policy] epoch #859 | optimization finished
2022-04-23 14:24:12 | [train_policy] epoch #859 | Computing KL after
2022-04-23 14:24:12 | [train_policy] epoch #859 | Computing loss after
2022-04-23 14:24:12 | [train_policy] epoch #859 | Fitting baseline...
2022-04-23 14:24:12 | [train_policy] epoch #859 | Saving snapshot...
2022-04-23 14:24:12 | [train_policy] epoch #859 | Saved
2022-04-23 14:24:12 | [train_policy] epoch #859 | Time 300.30 s
2022-04-23 14:24:12 | [train_policy] epoch #859 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118512
Evaluation/AverageDiscountedReturn          -40.0983
Evaluation/AverageReturn                    -40.0983
Evaluation/CompletionRate                     0
Evaluation/Iteration                        859
Evaluation/MaxReturn                        -30.1868
Evaluation/MinReturn                        -72.3508
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.08561
Extras/EpisodeRewardMean                    -40.0421
LinearFeatureBaseline/ExplainedVariance       0.881259
PolicyExecTime                                0.0917938
ProcessExecTime                               0.0111628
TotalEnvSteps                            870320
policy/Entropy                               -1.9873
policy/KL                                     0.00658793
policy/KLBefore                               0
policy/LossAfter                             -0.014965
policy/LossBefore                            -1.36054e-08
policy/Perplexity                             0.137065
policy/dLoss                                  0.014965
---------------------------------------  ----------------
2022-04-23 14:24:12 | [train_policy] epoch #860 | Obtaining samples for iteration 860...
2022-04-23 14:24:12 | [train_policy] epoch #860 | Logging diagnostics...
2022-04-23 14:24:12 | [train_policy] epoch #860 | Optimizing policy...
2022-04-23 14:24:12 | [train_policy] epoch #860 | Computing loss before
2022-04-23 14:24:12 | [train_policy] epoch #860 | Computing KL before
2022-04-23 14:24:12 | [train_policy] epoch #860 | Optimizing
2022-04-23 14:24:12 | [train_policy] epoch #860 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:12 | [train_policy] epoch #860 | computing loss before
2022-04-23 14:24:12 | [train_policy] epoch #860 | computing gradient
2022-04-23 14:24:12 | [train_policy] epoch #860 | gradient computed
2022-04-23 14:24:12 | [train_policy] epoch #860 | computing descent direction
2022-04-23 14:24:12 | [train_policy] epoch #860 | descent direction computed
2022-04-23 14:24:12 | [train_policy] epoch #860 | backtrack iters: 1
2022-04-23 14:24:12 | [train_policy] epoch #860 | optimization finished
2022-04-23 14:24:12 | [train_policy] epoch #860 | Computing KL after
2022-04-23 14:24:12 | [train_policy] epoch #860 | Computing loss after
2022-04-23 14:24:12 | [train_policy] epoch #860 | Fitting baseline...
2022-04-23 14:24:12 | [train_policy] epoch #860 | Saving snapshot...
2022-04-23 14:24:12 | [train_policy] epoch #860 | Saved
2022-04-23 14:24:12 | [train_policy] epoch #860 | Time 300.63 s
2022-04-23 14:24:12 | [train_policy] epoch #860 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119366
Evaluation/AverageDiscountedReturn          -39.3528
Evaluation/AverageReturn                    -39.3528
Evaluation/CompletionRate                     0
Evaluation/Iteration                        860
Evaluation/MaxReturn                        -29.536
Evaluation/MinReturn                        -56.8087
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.87328
Extras/EpisodeRewardMean                    -39.7128
LinearFeatureBaseline/ExplainedVariance       0.899882
PolicyExecTime                                0.0938065
ProcessExecTime                               0.0112863
TotalEnvSteps                            871332
policy/Entropy                               -2.03151
policy/KL                                     0.00643454
policy/KLBefore                               0
policy/LossAfter                             -0.0144731
policy/LossBefore                             6.83215e-09
policy/Perplexity                             0.131138
policy/dLoss                                  0.0144732
---------------------------------------  ----------------
2022-04-23 14:24:12 | [train_policy] epoch #861 | Obtaining samples for iteration 861...
2022-04-23 14:24:13 | [train_policy] epoch #861 | Logging diagnostics...
2022-04-23 14:24:13 | [train_policy] epoch #861 | Optimizing policy...
2022-04-23 14:24:13 | [train_policy] epoch #861 | Computing loss before
2022-04-23 14:24:13 | [train_policy] epoch #861 | Computing KL before
2022-04-23 14:24:13 | [train_policy] epoch #861 | Optimizing
2022-04-23 14:24:13 | [train_policy] epoch #861 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:13 | [train_policy] epoch #861 | computing loss before
2022-04-23 14:24:13 | [train_policy] epoch #861 | computing gradient
2022-04-23 14:24:13 | [train_policy] epoch #861 | gradient computed
2022-04-23 14:24:13 | [train_policy] epoch #861 | computing descent direction
2022-04-23 14:24:13 | [train_policy] epoch #861 | descent direction computed
2022-04-23 14:24:13 | [train_policy] epoch #861 | backtrack iters: 1
2022-04-23 14:24:13 | [train_policy] epoch #861 | optimization finished
2022-04-23 14:24:13 | [train_policy] epoch #861 | Computing KL after
2022-04-23 14:24:13 | [train_policy] epoch #861 | Computing loss after
2022-04-23 14:24:13 | [train_policy] epoch #861 | Fitting baseline...
2022-04-23 14:24:13 | [train_policy] epoch #861 | Saving snapshot...
2022-04-23 14:24:13 | [train_policy] epoch #861 | Saved
2022-04-23 14:24:13 | [train_policy] epoch #861 | Time 300.96 s
2022-04-23 14:24:13 | [train_policy] epoch #861 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120713
Evaluation/AverageDiscountedReturn          -40.6099
Evaluation/AverageReturn                    -40.6099
Evaluation/CompletionRate                     0
Evaluation/Iteration                        861
Evaluation/MaxReturn                        -30.3393
Evaluation/MinReturn                        -65.3707
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.79613
Extras/EpisodeRewardMean                    -40.8233
LinearFeatureBaseline/ExplainedVariance       0.87412
PolicyExecTime                                0.0947835
ProcessExecTime                               0.0113988
TotalEnvSteps                            872344
policy/Entropy                               -2.05159
policy/KL                                     0.00665262
policy/KLBefore                               0
policy/LossAfter                             -0.0179548
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.128531
policy/dLoss                                  0.0179548
---------------------------------------  ----------------
2022-04-23 14:24:13 | [train_policy] epoch #862 | Obtaining samples for iteration 862...
2022-04-23 14:24:13 | [train_policy] epoch #862 | Logging diagnostics...
2022-04-23 14:24:13 | [train_policy] epoch #862 | Optimizing policy...
2022-04-23 14:24:13 | [train_policy] epoch #862 | Computing loss before
2022-04-23 14:24:13 | [train_policy] epoch #862 | Computing KL before
2022-04-23 14:24:13 | [train_policy] epoch #862 | Optimizing
2022-04-23 14:24:13 | [train_policy] epoch #862 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:13 | [train_policy] epoch #862 | computing loss before
2022-04-23 14:24:13 | [train_policy] epoch #862 | computing gradient
2022-04-23 14:24:13 | [train_policy] epoch #862 | gradient computed
2022-04-23 14:24:13 | [train_policy] epoch #862 | computing descent direction
2022-04-23 14:24:13 | [train_policy] epoch #862 | descent direction computed
2022-04-23 14:24:13 | [train_policy] epoch #862 | backtrack iters: 0
2022-04-23 14:24:13 | [train_policy] epoch #862 | optimization finished
2022-04-23 14:24:13 | [train_policy] epoch #862 | Computing KL after
2022-04-23 14:24:13 | [train_policy] epoch #862 | Computing loss after
2022-04-23 14:24:13 | [train_policy] epoch #862 | Fitting baseline...
2022-04-23 14:24:13 | [train_policy] epoch #862 | Saving snapshot...
2022-04-23 14:24:13 | [train_policy] epoch #862 | Saved
2022-04-23 14:24:13 | [train_policy] epoch #862 | Time 301.29 s
2022-04-23 14:24:13 | [train_policy] epoch #862 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11948
Evaluation/AverageDiscountedReturn          -40.4607
Evaluation/AverageReturn                    -40.4607
Evaluation/CompletionRate                     0
Evaluation/Iteration                        862
Evaluation/MaxReturn                        -30.1556
Evaluation/MinReturn                        -64.8215
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.52608
Extras/EpisodeRewardMean                    -40.9154
LinearFeatureBaseline/ExplainedVariance       0.85983
PolicyExecTime                                0.0960827
ProcessExecTime                               0.0114574
TotalEnvSteps                            873356
policy/Entropy                               -2.0436
policy/KL                                     0.00999278
policy/KLBefore                               0
policy/LossAfter                             -0.0762955
policy/LossBefore                             3.76946e-09
policy/Perplexity                             0.129562
policy/dLoss                                  0.0762955
---------------------------------------  ----------------
2022-04-23 14:24:13 | [train_policy] epoch #863 | Obtaining samples for iteration 863...
2022-04-23 14:24:13 | [train_policy] epoch #863 | Logging diagnostics...
2022-04-23 14:24:13 | [train_policy] epoch #863 | Optimizing policy...
2022-04-23 14:24:13 | [train_policy] epoch #863 | Computing loss before
2022-04-23 14:24:13 | [train_policy] epoch #863 | Computing KL before
2022-04-23 14:24:13 | [train_policy] epoch #863 | Optimizing
2022-04-23 14:24:13 | [train_policy] epoch #863 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:13 | [train_policy] epoch #863 | computing loss before
2022-04-23 14:24:13 | [train_policy] epoch #863 | computing gradient
2022-04-23 14:24:13 | [train_policy] epoch #863 | gradient computed
2022-04-23 14:24:13 | [train_policy] epoch #863 | computing descent direction
2022-04-23 14:24:13 | [train_policy] epoch #863 | descent direction computed
2022-04-23 14:24:13 | [train_policy] epoch #863 | backtrack iters: 1
2022-04-23 14:24:13 | [train_policy] epoch #863 | optimization finished
2022-04-23 14:24:13 | [train_policy] epoch #863 | Computing KL after
2022-04-23 14:24:13 | [train_policy] epoch #863 | Computing loss after
2022-04-23 14:24:13 | [train_policy] epoch #863 | Fitting baseline...
2022-04-23 14:24:13 | [train_policy] epoch #863 | Saving snapshot...
2022-04-23 14:24:13 | [train_policy] epoch #863 | Saved
2022-04-23 14:24:13 | [train_policy] epoch #863 | Time 301.63 s
2022-04-23 14:24:13 | [train_policy] epoch #863 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.11877
Evaluation/AverageDiscountedReturn          -40.7229
Evaluation/AverageReturn                    -40.7229
Evaluation/CompletionRate                     0
Evaluation/Iteration                        863
Evaluation/MaxReturn                        -30.4002
Evaluation/MinReturn                        -63.9929
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.09471
Extras/EpisodeRewardMean                    -40.7149
LinearFeatureBaseline/ExplainedVariance       0.870368
PolicyExecTime                                0.0977767
ProcessExecTime                               0.0112572
TotalEnvSteps                            874368
policy/Entropy                               -2.06077
policy/KL                                     0.00660958
policy/KLBefore                               0
policy/LossAfter                             -0.0161383
policy/LossBefore                             4.47624e-09
policy/Perplexity                             0.127356
policy/dLoss                                  0.0161383
---------------------------------------  ----------------
2022-04-23 14:24:13 | [train_policy] epoch #864 | Obtaining samples for iteration 864...
2022-04-23 14:24:14 | [train_policy] epoch #864 | Logging diagnostics...
2022-04-23 14:24:14 | [train_policy] epoch #864 | Optimizing policy...
2022-04-23 14:24:14 | [train_policy] epoch #864 | Computing loss before
2022-04-23 14:24:14 | [train_policy] epoch #864 | Computing KL before
2022-04-23 14:24:14 | [train_policy] epoch #864 | Optimizing
2022-04-23 14:24:14 | [train_policy] epoch #864 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:14 | [train_policy] epoch #864 | computing loss before
2022-04-23 14:24:14 | [train_policy] epoch #864 | computing gradient
2022-04-23 14:24:14 | [train_policy] epoch #864 | gradient computed
2022-04-23 14:24:14 | [train_policy] epoch #864 | computing descent direction
2022-04-23 14:24:14 | [train_policy] epoch #864 | descent direction computed
2022-04-23 14:24:14 | [train_policy] epoch #864 | backtrack iters: 1
2022-04-23 14:24:14 | [train_policy] epoch #864 | optimization finished
2022-04-23 14:24:14 | [train_policy] epoch #864 | Computing KL after
2022-04-23 14:24:14 | [train_policy] epoch #864 | Computing loss after
2022-04-23 14:24:14 | [train_policy] epoch #864 | Fitting baseline...
2022-04-23 14:24:14 | [train_policy] epoch #864 | Saving snapshot...
2022-04-23 14:24:14 | [train_policy] epoch #864 | Saved
2022-04-23 14:24:14 | [train_policy] epoch #864 | Time 301.96 s
2022-04-23 14:24:14 | [train_policy] epoch #864 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119113
Evaluation/AverageDiscountedReturn          -39.3165
Evaluation/AverageReturn                    -39.3165
Evaluation/CompletionRate                     0
Evaluation/Iteration                        864
Evaluation/MaxReturn                        -31.7635
Evaluation/MinReturn                        -66.0517
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.16256
Extras/EpisodeRewardMean                    -39.3423
LinearFeatureBaseline/ExplainedVariance       0.878099
PolicyExecTime                                0.0949166
ProcessExecTime                               0.0114155
TotalEnvSteps                            875380
policy/Entropy                               -2.05443
policy/KL                                     0.00646676
policy/KLBefore                               0
policy/LossAfter                             -0.019764
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.128166
policy/dLoss                                  0.019764
---------------------------------------  ----------------
2022-04-23 14:24:14 | [train_policy] epoch #865 | Obtaining samples for iteration 865...
2022-04-23 14:24:14 | [train_policy] epoch #865 | Logging diagnostics...
2022-04-23 14:24:14 | [train_policy] epoch #865 | Optimizing policy...
2022-04-23 14:24:14 | [train_policy] epoch #865 | Computing loss before
2022-04-23 14:24:14 | [train_policy] epoch #865 | Computing KL before
2022-04-23 14:24:14 | [train_policy] epoch #865 | Optimizing
2022-04-23 14:24:14 | [train_policy] epoch #865 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:14 | [train_policy] epoch #865 | computing loss before
2022-04-23 14:24:14 | [train_policy] epoch #865 | computing gradient
2022-04-23 14:24:14 | [train_policy] epoch #865 | gradient computed
2022-04-23 14:24:14 | [train_policy] epoch #865 | computing descent direction
2022-04-23 14:24:14 | [train_policy] epoch #865 | descent direction computed
2022-04-23 14:24:14 | [train_policy] epoch #865 | backtrack iters: 0
2022-04-23 14:24:14 | [train_policy] epoch #865 | optimization finished
2022-04-23 14:24:14 | [train_policy] epoch #865 | Computing KL after
2022-04-23 14:24:14 | [train_policy] epoch #865 | Computing loss after
2022-04-23 14:24:14 | [train_policy] epoch #865 | Fitting baseline...
2022-04-23 14:24:14 | [train_policy] epoch #865 | Saving snapshot...
2022-04-23 14:24:14 | [train_policy] epoch #865 | Saved
2022-04-23 14:24:14 | [train_policy] epoch #865 | Time 302.28 s
2022-04-23 14:24:14 | [train_policy] epoch #865 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.119765
Evaluation/AverageDiscountedReturn          -40.6614
Evaluation/AverageReturn                    -40.6614
Evaluation/CompletionRate                     0
Evaluation/Iteration                        865
Evaluation/MaxReturn                        -29.9656
Evaluation/MinReturn                        -63.8691
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.40633
Extras/EpisodeRewardMean                    -40.5539
LinearFeatureBaseline/ExplainedVariance       0.857504
PolicyExecTime                                0.0932176
ProcessExecTime                               0.0115235
TotalEnvSteps                            876392
policy/Entropy                               -2.06676
policy/KL                                     0.00992933
policy/KLBefore                               0
policy/LossAfter                             -0.0283085
policy/LossBefore                            -0
policy/Perplexity                             0.126595
policy/dLoss                                  0.0283085
---------------------------------------  ---------------
2022-04-23 14:24:14 | [train_policy] epoch #866 | Obtaining samples for iteration 866...
2022-04-23 14:24:14 | [train_policy] epoch #866 | Logging diagnostics...
2022-04-23 14:24:14 | [train_policy] epoch #866 | Optimizing policy...
2022-04-23 14:24:14 | [train_policy] epoch #866 | Computing loss before
2022-04-23 14:24:14 | [train_policy] epoch #866 | Computing KL before
2022-04-23 14:24:14 | [train_policy] epoch #866 | Optimizing
2022-04-23 14:24:14 | [train_policy] epoch #866 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:14 | [train_policy] epoch #866 | computing loss before
2022-04-23 14:24:14 | [train_policy] epoch #866 | computing gradient
2022-04-23 14:24:14 | [train_policy] epoch #866 | gradient computed
2022-04-23 14:24:14 | [train_policy] epoch #866 | computing descent direction
2022-04-23 14:24:14 | [train_policy] epoch #866 | descent direction computed
2022-04-23 14:24:14 | [train_policy] epoch #866 | backtrack iters: 0
2022-04-23 14:24:14 | [train_policy] epoch #866 | optimization finished
2022-04-23 14:24:14 | [train_policy] epoch #866 | Computing KL after
2022-04-23 14:24:14 | [train_policy] epoch #866 | Computing loss after
2022-04-23 14:24:14 | [train_policy] epoch #866 | Fitting baseline...
2022-04-23 14:24:14 | [train_policy] epoch #866 | Saving snapshot...
2022-04-23 14:24:14 | [train_policy] epoch #866 | Saved
2022-04-23 14:24:14 | [train_policy] epoch #866 | Time 302.60 s
2022-04-23 14:24:14 | [train_policy] epoch #866 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118957
Evaluation/AverageDiscountedReturn          -40.4391
Evaluation/AverageReturn                    -40.4391
Evaluation/CompletionRate                     0
Evaluation/Iteration                        866
Evaluation/MaxReturn                        -29.642
Evaluation/MinReturn                        -63.6919
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.98
Extras/EpisodeRewardMean                    -40.5563
LinearFeatureBaseline/ExplainedVariance       0.87388
PolicyExecTime                                0.0924976
ProcessExecTime                               0.0110157
TotalEnvSteps                            877404
policy/Entropy                               -2.03416
policy/KL                                     0.00957271
policy/KLBefore                               0
policy/LossAfter                             -0.0180422
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.13079
policy/dLoss                                  0.0180422
---------------------------------------  ----------------
2022-04-23 14:24:14 | [train_policy] epoch #867 | Obtaining samples for iteration 867...
2022-04-23 14:24:15 | [train_policy] epoch #867 | Logging diagnostics...
2022-04-23 14:24:15 | [train_policy] epoch #867 | Optimizing policy...
2022-04-23 14:24:15 | [train_policy] epoch #867 | Computing loss before
2022-04-23 14:24:15 | [train_policy] epoch #867 | Computing KL before
2022-04-23 14:24:15 | [train_policy] epoch #867 | Optimizing
2022-04-23 14:24:15 | [train_policy] epoch #867 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:15 | [train_policy] epoch #867 | computing loss before
2022-04-23 14:24:15 | [train_policy] epoch #867 | computing gradient
2022-04-23 14:24:15 | [train_policy] epoch #867 | gradient computed
2022-04-23 14:24:15 | [train_policy] epoch #867 | computing descent direction
2022-04-23 14:24:15 | [train_policy] epoch #867 | descent direction computed
2022-04-23 14:24:15 | [train_policy] epoch #867 | backtrack iters: 1
2022-04-23 14:24:15 | [train_policy] epoch #867 | optimization finished
2022-04-23 14:24:15 | [train_policy] epoch #867 | Computing KL after
2022-04-23 14:24:15 | [train_policy] epoch #867 | Computing loss after
2022-04-23 14:24:15 | [train_policy] epoch #867 | Fitting baseline...
2022-04-23 14:24:15 | [train_policy] epoch #867 | Saving snapshot...
2022-04-23 14:24:15 | [train_policy] epoch #867 | Saved
2022-04-23 14:24:15 | [train_policy] epoch #867 | Time 302.93 s
2022-04-23 14:24:15 | [train_policy] epoch #867 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118747
Evaluation/AverageDiscountedReturn          -39.4967
Evaluation/AverageReturn                    -39.4967
Evaluation/CompletionRate                     0
Evaluation/Iteration                        867
Evaluation/MaxReturn                        -29.3607
Evaluation/MinReturn                        -63.6636
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.98417
Extras/EpisodeRewardMean                    -39.4832
LinearFeatureBaseline/ExplainedVariance       0.882533
PolicyExecTime                                0.0932477
ProcessExecTime                               0.0111527
TotalEnvSteps                            878416
policy/Entropy                               -2.08635
policy/KL                                     0.00669026
policy/KLBefore                               0
policy/LossAfter                             -0.0138535
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             0.124139
policy/dLoss                                  0.0138535
---------------------------------------  ----------------
2022-04-23 14:24:15 | [train_policy] epoch #868 | Obtaining samples for iteration 868...
2022-04-23 14:24:15 | [train_policy] epoch #868 | Logging diagnostics...
2022-04-23 14:24:15 | [train_policy] epoch #868 | Optimizing policy...
2022-04-23 14:24:15 | [train_policy] epoch #868 | Computing loss before
2022-04-23 14:24:15 | [train_policy] epoch #868 | Computing KL before
2022-04-23 14:24:15 | [train_policy] epoch #868 | Optimizing
2022-04-23 14:24:15 | [train_policy] epoch #868 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:15 | [train_policy] epoch #868 | computing loss before
2022-04-23 14:24:15 | [train_policy] epoch #868 | computing gradient
2022-04-23 14:24:15 | [train_policy] epoch #868 | gradient computed
2022-04-23 14:24:15 | [train_policy] epoch #868 | computing descent direction
2022-04-23 14:24:15 | [train_policy] epoch #868 | descent direction computed
2022-04-23 14:24:15 | [train_policy] epoch #868 | backtrack iters: 0
2022-04-23 14:24:15 | [train_policy] epoch #868 | optimization finished
2022-04-23 14:24:15 | [train_policy] epoch #868 | Computing KL after
2022-04-23 14:24:15 | [train_policy] epoch #868 | Computing loss after
2022-04-23 14:24:15 | [train_policy] epoch #868 | Fitting baseline...
2022-04-23 14:24:15 | [train_policy] epoch #868 | Saving snapshot...
2022-04-23 14:24:15 | [train_policy] epoch #868 | Saved
2022-04-23 14:24:15 | [train_policy] epoch #868 | Time 303.26 s
2022-04-23 14:24:15 | [train_policy] epoch #868 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119412
Evaluation/AverageDiscountedReturn          -40.2386
Evaluation/AverageReturn                    -40.2386
Evaluation/CompletionRate                     0
Evaluation/Iteration                        868
Evaluation/MaxReturn                        -30.3134
Evaluation/MinReturn                        -61.2991
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.8489
Extras/EpisodeRewardMean                    -40.3379
LinearFeatureBaseline/ExplainedVariance       0.883899
PolicyExecTime                                0.0951846
ProcessExecTime                               0.0115986
TotalEnvSteps                            879428
policy/Entropy                               -2.1209
policy/KL                                     0.00999916
policy/KLBefore                               0
policy/LossAfter                             -0.0141659
policy/LossBefore                            -1.06016e-08
policy/Perplexity                             0.119924
policy/dLoss                                  0.0141659
---------------------------------------  ----------------
2022-04-23 14:24:15 | [train_policy] epoch #869 | Obtaining samples for iteration 869...
2022-04-23 14:24:15 | [train_policy] epoch #869 | Logging diagnostics...
2022-04-23 14:24:15 | [train_policy] epoch #869 | Optimizing policy...
2022-04-23 14:24:15 | [train_policy] epoch #869 | Computing loss before
2022-04-23 14:24:15 | [train_policy] epoch #869 | Computing KL before
2022-04-23 14:24:15 | [train_policy] epoch #869 | Optimizing
2022-04-23 14:24:15 | [train_policy] epoch #869 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:15 | [train_policy] epoch #869 | computing loss before
2022-04-23 14:24:15 | [train_policy] epoch #869 | computing gradient
2022-04-23 14:24:15 | [train_policy] epoch #869 | gradient computed
2022-04-23 14:24:15 | [train_policy] epoch #869 | computing descent direction
2022-04-23 14:24:15 | [train_policy] epoch #869 | descent direction computed
2022-04-23 14:24:15 | [train_policy] epoch #869 | backtrack iters: 1
2022-04-23 14:24:15 | [train_policy] epoch #869 | optimization finished
2022-04-23 14:24:15 | [train_policy] epoch #869 | Computing KL after
2022-04-23 14:24:15 | [train_policy] epoch #869 | Computing loss after
2022-04-23 14:24:15 | [train_policy] epoch #869 | Fitting baseline...
2022-04-23 14:24:15 | [train_policy] epoch #869 | Saving snapshot...
2022-04-23 14:24:15 | [train_policy] epoch #869 | Saved
2022-04-23 14:24:15 | [train_policy] epoch #869 | Time 303.59 s
2022-04-23 14:24:15 | [train_policy] epoch #869 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119274
Evaluation/AverageDiscountedReturn          -40.899
Evaluation/AverageReturn                    -40.899
Evaluation/CompletionRate                     0
Evaluation/Iteration                        869
Evaluation/MaxReturn                        -29.7763
Evaluation/MinReturn                        -64.3361
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.90021
Extras/EpisodeRewardMean                    -40.9333
LinearFeatureBaseline/ExplainedVariance       0.894509
PolicyExecTime                                0.0949523
ProcessExecTime                               0.011519
TotalEnvSteps                            880440
policy/Entropy                               -2.13175
policy/KL                                     0.00648821
policy/KLBefore                               0
policy/LossAfter                             -0.017043
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.118629
policy/dLoss                                  0.017043
---------------------------------------  ----------------
2022-04-23 14:24:15 | [train_policy] epoch #870 | Obtaining samples for iteration 870...
2022-04-23 14:24:16 | [train_policy] epoch #870 | Logging diagnostics...
2022-04-23 14:24:16 | [train_policy] epoch #870 | Optimizing policy...
2022-04-23 14:24:16 | [train_policy] epoch #870 | Computing loss before
2022-04-23 14:24:16 | [train_policy] epoch #870 | Computing KL before
2022-04-23 14:24:16 | [train_policy] epoch #870 | Optimizing
2022-04-23 14:24:16 | [train_policy] epoch #870 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:16 | [train_policy] epoch #870 | computing loss before
2022-04-23 14:24:16 | [train_policy] epoch #870 | computing gradient
2022-04-23 14:24:16 | [train_policy] epoch #870 | gradient computed
2022-04-23 14:24:16 | [train_policy] epoch #870 | computing descent direction
2022-04-23 14:24:16 | [train_policy] epoch #870 | descent direction computed
2022-04-23 14:24:16 | [train_policy] epoch #870 | backtrack iters: 1
2022-04-23 14:24:16 | [train_policy] epoch #870 | optimization finished
2022-04-23 14:24:16 | [train_policy] epoch #870 | Computing KL after
2022-04-23 14:24:16 | [train_policy] epoch #870 | Computing loss after
2022-04-23 14:24:16 | [train_policy] epoch #870 | Fitting baseline...
2022-04-23 14:24:16 | [train_policy] epoch #870 | Saving snapshot...
2022-04-23 14:24:16 | [train_policy] epoch #870 | Saved
2022-04-23 14:24:16 | [train_policy] epoch #870 | Time 303.92 s
2022-04-23 14:24:16 | [train_policy] epoch #870 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120745
Evaluation/AverageDiscountedReturn          -41.7693
Evaluation/AverageReturn                    -41.7693
Evaluation/CompletionRate                     0
Evaluation/Iteration                        870
Evaluation/MaxReturn                        -30.0101
Evaluation/MinReturn                        -71.7115
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.66357
Extras/EpisodeRewardMean                    -41.4495
LinearFeatureBaseline/ExplainedVariance       0.879592
PolicyExecTime                                0.0945444
ProcessExecTime                               0.0116122
TotalEnvSteps                            881452
policy/Entropy                               -2.18417
policy/KL                                     0.00663141
policy/KLBefore                               0
policy/LossAfter                             -0.0144951
policy/LossBefore                             2.16744e-08
policy/Perplexity                             0.112572
policy/dLoss                                  0.0144952
---------------------------------------  ----------------
2022-04-23 14:24:16 | [train_policy] epoch #871 | Obtaining samples for iteration 871...
2022-04-23 14:24:16 | [train_policy] epoch #871 | Logging diagnostics...
2022-04-23 14:24:16 | [train_policy] epoch #871 | Optimizing policy...
2022-04-23 14:24:16 | [train_policy] epoch #871 | Computing loss before
2022-04-23 14:24:16 | [train_policy] epoch #871 | Computing KL before
2022-04-23 14:24:16 | [train_policy] epoch #871 | Optimizing
2022-04-23 14:24:16 | [train_policy] epoch #871 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:16 | [train_policy] epoch #871 | computing loss before
2022-04-23 14:24:16 | [train_policy] epoch #871 | computing gradient
2022-04-23 14:24:16 | [train_policy] epoch #871 | gradient computed
2022-04-23 14:24:16 | [train_policy] epoch #871 | computing descent direction
2022-04-23 14:24:16 | [train_policy] epoch #871 | descent direction computed
2022-04-23 14:24:16 | [train_policy] epoch #871 | backtrack iters: 1
2022-04-23 14:24:16 | [train_policy] epoch #871 | optimization finished
2022-04-23 14:24:16 | [train_policy] epoch #871 | Computing KL after
2022-04-23 14:24:16 | [train_policy] epoch #871 | Computing loss after
2022-04-23 14:24:16 | [train_policy] epoch #871 | Fitting baseline...
2022-04-23 14:24:16 | [train_policy] epoch #871 | Saving snapshot...
2022-04-23 14:24:16 | [train_policy] epoch #871 | Saved
2022-04-23 14:24:16 | [train_policy] epoch #871 | Time 304.25 s
2022-04-23 14:24:16 | [train_policy] epoch #871 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118906
Evaluation/AverageDiscountedReturn          -41.6169
Evaluation/AverageReturn                    -41.6169
Evaluation/CompletionRate                     0
Evaluation/Iteration                        871
Evaluation/MaxReturn                        -30.1847
Evaluation/MinReturn                        -72.006
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.35061
Extras/EpisodeRewardMean                    -41.4403
LinearFeatureBaseline/ExplainedVariance       0.839494
PolicyExecTime                                0.0946
ProcessExecTime                               0.0115397
TotalEnvSteps                            882464
policy/Entropy                               -2.18773
policy/KL                                     0.00643518
policy/KLBefore                               0
policy/LossAfter                             -0.0328258
policy/LossBefore                            -4.47624e-09
policy/Perplexity                             0.112171
policy/dLoss                                  0.0328258
---------------------------------------  ----------------
2022-04-23 14:24:16 | [train_policy] epoch #872 | Obtaining samples for iteration 872...
2022-04-23 14:24:16 | [train_policy] epoch #872 | Logging diagnostics...
2022-04-23 14:24:16 | [train_policy] epoch #872 | Optimizing policy...
2022-04-23 14:24:16 | [train_policy] epoch #872 | Computing loss before
2022-04-23 14:24:16 | [train_policy] epoch #872 | Computing KL before
2022-04-23 14:24:16 | [train_policy] epoch #872 | Optimizing
2022-04-23 14:24:16 | [train_policy] epoch #872 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:16 | [train_policy] epoch #872 | computing loss before
2022-04-23 14:24:16 | [train_policy] epoch #872 | computing gradient
2022-04-23 14:24:16 | [train_policy] epoch #872 | gradient computed
2022-04-23 14:24:16 | [train_policy] epoch #872 | computing descent direction
2022-04-23 14:24:16 | [train_policy] epoch #872 | descent direction computed
2022-04-23 14:24:16 | [train_policy] epoch #872 | backtrack iters: 1
2022-04-23 14:24:16 | [train_policy] epoch #872 | optimization finished
2022-04-23 14:24:16 | [train_policy] epoch #872 | Computing KL after
2022-04-23 14:24:16 | [train_policy] epoch #872 | Computing loss after
2022-04-23 14:24:16 | [train_policy] epoch #872 | Fitting baseline...
2022-04-23 14:24:16 | [train_policy] epoch #872 | Saving snapshot...
2022-04-23 14:24:16 | [train_policy] epoch #872 | Saved
2022-04-23 14:24:16 | [train_policy] epoch #872 | Time 304.57 s
2022-04-23 14:24:16 | [train_policy] epoch #872 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118749
Evaluation/AverageDiscountedReturn          -41.0215
Evaluation/AverageReturn                    -41.0215
Evaluation/CompletionRate                     0
Evaluation/Iteration                        872
Evaluation/MaxReturn                        -30.9539
Evaluation/MinReturn                        -72.8668
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.0975
Extras/EpisodeRewardMean                    -40.7855
LinearFeatureBaseline/ExplainedVariance       0.867302
PolicyExecTime                                0.0921781
ProcessExecTime                               0.0111482
TotalEnvSteps                            883476
policy/Entropy                               -2.22751
policy/KL                                     0.00671131
policy/KLBefore                               0
policy/LossAfter                             -0.0149781
policy/LossBefore                            -1.00126e-08
policy/Perplexity                             0.107797
policy/dLoss                                  0.014978
---------------------------------------  ----------------
2022-04-23 14:24:16 | [train_policy] epoch #873 | Obtaining samples for iteration 873...
2022-04-23 14:24:17 | [train_policy] epoch #873 | Logging diagnostics...
2022-04-23 14:24:17 | [train_policy] epoch #873 | Optimizing policy...
2022-04-23 14:24:17 | [train_policy] epoch #873 | Computing loss before
2022-04-23 14:24:17 | [train_policy] epoch #873 | Computing KL before
2022-04-23 14:24:17 | [train_policy] epoch #873 | Optimizing
2022-04-23 14:24:17 | [train_policy] epoch #873 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:17 | [train_policy] epoch #873 | computing loss before
2022-04-23 14:24:17 | [train_policy] epoch #873 | computing gradient
2022-04-23 14:24:17 | [train_policy] epoch #873 | gradient computed
2022-04-23 14:24:17 | [train_policy] epoch #873 | computing descent direction
2022-04-23 14:24:17 | [train_policy] epoch #873 | descent direction computed
2022-04-23 14:24:17 | [train_policy] epoch #873 | backtrack iters: 1
2022-04-23 14:24:17 | [train_policy] epoch #873 | optimization finished
2022-04-23 14:24:17 | [train_policy] epoch #873 | Computing KL after
2022-04-23 14:24:17 | [train_policy] epoch #873 | Computing loss after
2022-04-23 14:24:17 | [train_policy] epoch #873 | Fitting baseline...
2022-04-23 14:24:17 | [train_policy] epoch #873 | Saving snapshot...
2022-04-23 14:24:17 | [train_policy] epoch #873 | Saved
2022-04-23 14:24:17 | [train_policy] epoch #873 | Time 304.89 s
2022-04-23 14:24:17 | [train_policy] epoch #873 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119445
Evaluation/AverageDiscountedReturn          -41.5625
Evaluation/AverageReturn                    -41.5625
Evaluation/CompletionRate                     0
Evaluation/Iteration                        873
Evaluation/MaxReturn                        -31.1866
Evaluation/MinReturn                        -71.7406
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.71141
Extras/EpisodeRewardMean                    -41.6311
LinearFeatureBaseline/ExplainedVariance       0.860361
PolicyExecTime                                0.0923047
ProcessExecTime                               0.0111539
TotalEnvSteps                            884488
policy/Entropy                               -2.23161
policy/KL                                     0.00640611
policy/KLBefore                               0
policy/LossAfter                             -0.0151544
policy/LossBefore                             1.17796e-08
policy/Perplexity                             0.107356
policy/dLoss                                  0.0151545
---------------------------------------  ----------------
2022-04-23 14:24:17 | [train_policy] epoch #874 | Obtaining samples for iteration 874...
2022-04-23 14:24:17 | [train_policy] epoch #874 | Logging diagnostics...
2022-04-23 14:24:17 | [train_policy] epoch #874 | Optimizing policy...
2022-04-23 14:24:17 | [train_policy] epoch #874 | Computing loss before
2022-04-23 14:24:17 | [train_policy] epoch #874 | Computing KL before
2022-04-23 14:24:17 | [train_policy] epoch #874 | Optimizing
2022-04-23 14:24:17 | [train_policy] epoch #874 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:17 | [train_policy] epoch #874 | computing loss before
2022-04-23 14:24:17 | [train_policy] epoch #874 | computing gradient
2022-04-23 14:24:17 | [train_policy] epoch #874 | gradient computed
2022-04-23 14:24:17 | [train_policy] epoch #874 | computing descent direction
2022-04-23 14:24:17 | [train_policy] epoch #874 | descent direction computed
2022-04-23 14:24:17 | [train_policy] epoch #874 | backtrack iters: 1
2022-04-23 14:24:17 | [train_policy] epoch #874 | optimization finished
2022-04-23 14:24:17 | [train_policy] epoch #874 | Computing KL after
2022-04-23 14:24:17 | [train_policy] epoch #874 | Computing loss after
2022-04-23 14:24:17 | [train_policy] epoch #874 | Fitting baseline...
2022-04-23 14:24:17 | [train_policy] epoch #874 | Saving snapshot...
2022-04-23 14:24:17 | [train_policy] epoch #874 | Saved
2022-04-23 14:24:17 | [train_policy] epoch #874 | Time 305.22 s
2022-04-23 14:24:17 | [train_policy] epoch #874 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119197
Evaluation/AverageDiscountedReturn          -40.6028
Evaluation/AverageReturn                    -40.6028
Evaluation/CompletionRate                     0
Evaluation/Iteration                        874
Evaluation/MaxReturn                        -30.6653
Evaluation/MinReturn                        -61.4677
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.73037
Extras/EpisodeRewardMean                    -40.6042
LinearFeatureBaseline/ExplainedVariance       0.870338
PolicyExecTime                                0.0944488
ProcessExecTime                               0.0110953
TotalEnvSteps                            885500
policy/Entropy                               -2.25906
policy/KL                                     0.00668041
policy/KLBefore                               0
policy/LossAfter                             -0.0160838
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.104448
policy/dLoss                                  0.0160838
---------------------------------------  ----------------
2022-04-23 14:24:17 | [train_policy] epoch #875 | Obtaining samples for iteration 875...
2022-04-23 14:24:17 | [train_policy] epoch #875 | Logging diagnostics...
2022-04-23 14:24:17 | [train_policy] epoch #875 | Optimizing policy...
2022-04-23 14:24:17 | [train_policy] epoch #875 | Computing loss before
2022-04-23 14:24:17 | [train_policy] epoch #875 | Computing KL before
2022-04-23 14:24:17 | [train_policy] epoch #875 | Optimizing
2022-04-23 14:24:17 | [train_policy] epoch #875 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:17 | [train_policy] epoch #875 | computing loss before
2022-04-23 14:24:17 | [train_policy] epoch #875 | computing gradient
2022-04-23 14:24:17 | [train_policy] epoch #875 | gradient computed
2022-04-23 14:24:17 | [train_policy] epoch #875 | computing descent direction
2022-04-23 14:24:17 | [train_policy] epoch #875 | descent direction computed
2022-04-23 14:24:17 | [train_policy] epoch #875 | backtrack iters: 1
2022-04-23 14:24:17 | [train_policy] epoch #875 | optimization finished
2022-04-23 14:24:17 | [train_policy] epoch #875 | Computing KL after
2022-04-23 14:24:17 | [train_policy] epoch #875 | Computing loss after
2022-04-23 14:24:17 | [train_policy] epoch #875 | Fitting baseline...
2022-04-23 14:24:17 | [train_policy] epoch #875 | Saving snapshot...
2022-04-23 14:24:17 | [train_policy] epoch #875 | Saved
2022-04-23 14:24:17 | [train_policy] epoch #875 | Time 305.55 s
2022-04-23 14:24:17 | [train_policy] epoch #875 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118498
Evaluation/AverageDiscountedReturn          -41.1195
Evaluation/AverageReturn                    -41.1195
Evaluation/CompletionRate                     0
Evaluation/Iteration                        875
Evaluation/MaxReturn                        -29.795
Evaluation/MinReturn                        -64.194
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.09788
Extras/EpisodeRewardMean                    -40.8249
LinearFeatureBaseline/ExplainedVariance       0.752961
PolicyExecTime                                0.0926785
ProcessExecTime                               0.0110466
TotalEnvSteps                            886512
policy/Entropy                               -2.26256
policy/KL                                     0.00641224
policy/KLBefore                               0
policy/LossAfter                             -0.0236898
policy/LossBefore                             3.53387e-10
policy/Perplexity                             0.104084
policy/dLoss                                  0.0236898
---------------------------------------  ----------------
2022-04-23 14:24:17 | [train_policy] epoch #876 | Obtaining samples for iteration 876...
2022-04-23 14:24:18 | [train_policy] epoch #876 | Logging diagnostics...
2022-04-23 14:24:18 | [train_policy] epoch #876 | Optimizing policy...
2022-04-23 14:24:18 | [train_policy] epoch #876 | Computing loss before
2022-04-23 14:24:18 | [train_policy] epoch #876 | Computing KL before
2022-04-23 14:24:18 | [train_policy] epoch #876 | Optimizing
2022-04-23 14:24:18 | [train_policy] epoch #876 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:18 | [train_policy] epoch #876 | computing loss before
2022-04-23 14:24:18 | [train_policy] epoch #876 | computing gradient
2022-04-23 14:24:18 | [train_policy] epoch #876 | gradient computed
2022-04-23 14:24:18 | [train_policy] epoch #876 | computing descent direction
2022-04-23 14:24:18 | [train_policy] epoch #876 | descent direction computed
2022-04-23 14:24:18 | [train_policy] epoch #876 | backtrack iters: 0
2022-04-23 14:24:18 | [train_policy] epoch #876 | optimization finished
2022-04-23 14:24:18 | [train_policy] epoch #876 | Computing KL after
2022-04-23 14:24:18 | [train_policy] epoch #876 | Computing loss after
2022-04-23 14:24:18 | [train_policy] epoch #876 | Fitting baseline...
2022-04-23 14:24:18 | [train_policy] epoch #876 | Saving snapshot...
2022-04-23 14:24:18 | [train_policy] epoch #876 | Saved
2022-04-23 14:24:18 | [train_policy] epoch #876 | Time 305.89 s
2022-04-23 14:24:18 | [train_policy] epoch #876 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119048
Evaluation/AverageDiscountedReturn          -41.4953
Evaluation/AverageReturn                    -41.4953
Evaluation/CompletionRate                     0
Evaluation/Iteration                        876
Evaluation/MaxReturn                        -30.789
Evaluation/MinReturn                        -71.7558
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.62765
Extras/EpisodeRewardMean                    -41.425
LinearFeatureBaseline/ExplainedVariance       0.856199
PolicyExecTime                                0.100112
ProcessExecTime                               0.0112123
TotalEnvSteps                            887524
policy/Entropy                               -2.2859
policy/KL                                     0.00962888
policy/KLBefore                               0
policy/LossAfter                             -0.012661
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.101682
policy/dLoss                                  0.012661
---------------------------------------  ----------------
2022-04-23 14:24:18 | [train_policy] epoch #877 | Obtaining samples for iteration 877...
2022-04-23 14:24:18 | [train_policy] epoch #877 | Logging diagnostics...
2022-04-23 14:24:18 | [train_policy] epoch #877 | Optimizing policy...
2022-04-23 14:24:18 | [train_policy] epoch #877 | Computing loss before
2022-04-23 14:24:18 | [train_policy] epoch #877 | Computing KL before
2022-04-23 14:24:18 | [train_policy] epoch #877 | Optimizing
2022-04-23 14:24:18 | [train_policy] epoch #877 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:18 | [train_policy] epoch #877 | computing loss before
2022-04-23 14:24:18 | [train_policy] epoch #877 | computing gradient
2022-04-23 14:24:18 | [train_policy] epoch #877 | gradient computed
2022-04-23 14:24:18 | [train_policy] epoch #877 | computing descent direction
2022-04-23 14:24:18 | [train_policy] epoch #877 | descent direction computed
2022-04-23 14:24:18 | [train_policy] epoch #877 | backtrack iters: 1
2022-04-23 14:24:18 | [train_policy] epoch #877 | optimization finished
2022-04-23 14:24:18 | [train_policy] epoch #877 | Computing KL after
2022-04-23 14:24:18 | [train_policy] epoch #877 | Computing loss after
2022-04-23 14:24:18 | [train_policy] epoch #877 | Fitting baseline...
2022-04-23 14:24:18 | [train_policy] epoch #877 | Saving snapshot...
2022-04-23 14:24:18 | [train_policy] epoch #877 | Saved
2022-04-23 14:24:18 | [train_policy] epoch #877 | Time 306.23 s
2022-04-23 14:24:18 | [train_policy] epoch #877 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.118854
Evaluation/AverageDiscountedReturn          -40.5846
Evaluation/AverageReturn                    -40.5846
Evaluation/CompletionRate                     0
Evaluation/Iteration                        877
Evaluation/MaxReturn                        -31.7748
Evaluation/MinReturn                        -72.5379
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.13347
Extras/EpisodeRewardMean                    -40.445
LinearFeatureBaseline/ExplainedVariance       0.864791
PolicyExecTime                                0.104362
ProcessExecTime                               0.0114543
TotalEnvSteps                            888536
policy/Entropy                               -2.2849
policy/KL                                     0.00645543
policy/KLBefore                               0
policy/LossAfter                             -0.0195465
policy/LossBefore                             6.36097e-09
policy/Perplexity                             0.101785
policy/dLoss                                  0.0195466
---------------------------------------  ----------------
2022-04-23 14:24:18 | [train_policy] epoch #878 | Obtaining samples for iteration 878...
2022-04-23 14:24:18 | [train_policy] epoch #878 | Logging diagnostics...
2022-04-23 14:24:18 | [train_policy] epoch #878 | Optimizing policy...
2022-04-23 14:24:18 | [train_policy] epoch #878 | Computing loss before
2022-04-23 14:24:18 | [train_policy] epoch #878 | Computing KL before
2022-04-23 14:24:18 | [train_policy] epoch #878 | Optimizing
2022-04-23 14:24:18 | [train_policy] epoch #878 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:18 | [train_policy] epoch #878 | computing loss before
2022-04-23 14:24:18 | [train_policy] epoch #878 | computing gradient
2022-04-23 14:24:18 | [train_policy] epoch #878 | gradient computed
2022-04-23 14:24:18 | [train_policy] epoch #878 | computing descent direction
2022-04-23 14:24:18 | [train_policy] epoch #878 | descent direction computed
2022-04-23 14:24:18 | [train_policy] epoch #878 | backtrack iters: 1
2022-04-23 14:24:18 | [train_policy] epoch #878 | optimization finished
2022-04-23 14:24:18 | [train_policy] epoch #878 | Computing KL after
2022-04-23 14:24:18 | [train_policy] epoch #878 | Computing loss after
2022-04-23 14:24:18 | [train_policy] epoch #878 | Fitting baseline...
2022-04-23 14:24:18 | [train_policy] epoch #878 | Saving snapshot...
2022-04-23 14:24:18 | [train_policy] epoch #878 | Saved
2022-04-23 14:24:18 | [train_policy] epoch #878 | Time 306.58 s
2022-04-23 14:24:18 | [train_policy] epoch #878 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.120365
Evaluation/AverageDiscountedReturn          -40.1349
Evaluation/AverageReturn                    -40.1349
Evaluation/CompletionRate                     0
Evaluation/Iteration                        878
Evaluation/MaxReturn                        -30.4835
Evaluation/MinReturn                        -57.9673
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.72746
Extras/EpisodeRewardMean                    -40.2856
LinearFeatureBaseline/ExplainedVariance       0.876684
PolicyExecTime                                0.102071
ProcessExecTime                               0.0113497
TotalEnvSteps                            889548
policy/Entropy                               -2.29802
policy/KL                                     0.00681659
policy/KLBefore                               0
policy/LossAfter                             -0.0155375
policy/LossBefore                             6.83215e-09
policy/Perplexity                             0.100458
policy/dLoss                                  0.0155375
---------------------------------------  ----------------
2022-04-23 14:24:18 | [train_policy] epoch #879 | Obtaining samples for iteration 879...
2022-04-23 14:24:19 | [train_policy] epoch #879 | Logging diagnostics...
2022-04-23 14:24:19 | [train_policy] epoch #879 | Optimizing policy...
2022-04-23 14:24:19 | [train_policy] epoch #879 | Computing loss before
2022-04-23 14:24:19 | [train_policy] epoch #879 | Computing KL before
2022-04-23 14:24:19 | [train_policy] epoch #879 | Optimizing
2022-04-23 14:24:19 | [train_policy] epoch #879 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:19 | [train_policy] epoch #879 | computing loss before
2022-04-23 14:24:19 | [train_policy] epoch #879 | computing gradient
2022-04-23 14:24:19 | [train_policy] epoch #879 | gradient computed
2022-04-23 14:24:19 | [train_policy] epoch #879 | computing descent direction
2022-04-23 14:24:19 | [train_policy] epoch #879 | descent direction computed
2022-04-23 14:24:19 | [train_policy] epoch #879 | backtrack iters: 0
2022-04-23 14:24:19 | [train_policy] epoch #879 | optimization finished
2022-04-23 14:24:19 | [train_policy] epoch #879 | Computing KL after
2022-04-23 14:24:19 | [train_policy] epoch #879 | Computing loss after
2022-04-23 14:24:19 | [train_policy] epoch #879 | Fitting baseline...
2022-04-23 14:24:19 | [train_policy] epoch #879 | Saving snapshot...
2022-04-23 14:24:19 | [train_policy] epoch #879 | Saved
2022-04-23 14:24:19 | [train_policy] epoch #879 | Time 306.91 s
2022-04-23 14:24:19 | [train_policy] epoch #879 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119931
Evaluation/AverageDiscountedReturn          -40.6502
Evaluation/AverageReturn                    -40.6502
Evaluation/CompletionRate                     0
Evaluation/Iteration                        879
Evaluation/MaxReturn                        -30.2811
Evaluation/MinReturn                        -71.9416
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25411
Extras/EpisodeRewardMean                    -40.5868
LinearFeatureBaseline/ExplainedVariance       0.866297
PolicyExecTime                                0.0953379
ProcessExecTime                               0.0112979
TotalEnvSteps                            890560
policy/Entropy                               -2.29347
policy/KL                                     0.0099006
policy/KLBefore                               0
policy/LossAfter                             -0.0234191
policy/LossBefore                             2.35591e-09
policy/Perplexity                             0.100915
policy/dLoss                                  0.0234191
---------------------------------------  ----------------
2022-04-23 14:24:19 | [train_policy] epoch #880 | Obtaining samples for iteration 880...
2022-04-23 14:24:19 | [train_policy] epoch #880 | Logging diagnostics...
2022-04-23 14:24:19 | [train_policy] epoch #880 | Optimizing policy...
2022-04-23 14:24:19 | [train_policy] epoch #880 | Computing loss before
2022-04-23 14:24:19 | [train_policy] epoch #880 | Computing KL before
2022-04-23 14:24:19 | [train_policy] epoch #880 | Optimizing
2022-04-23 14:24:19 | [train_policy] epoch #880 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:19 | [train_policy] epoch #880 | computing loss before
2022-04-23 14:24:19 | [train_policy] epoch #880 | computing gradient
2022-04-23 14:24:19 | [train_policy] epoch #880 | gradient computed
2022-04-23 14:24:19 | [train_policy] epoch #880 | computing descent direction
2022-04-23 14:24:19 | [train_policy] epoch #880 | descent direction computed
2022-04-23 14:24:19 | [train_policy] epoch #880 | backtrack iters: 1
2022-04-23 14:24:19 | [train_policy] epoch #880 | optimization finished
2022-04-23 14:24:19 | [train_policy] epoch #880 | Computing KL after
2022-04-23 14:24:19 | [train_policy] epoch #880 | Computing loss after
2022-04-23 14:24:19 | [train_policy] epoch #880 | Fitting baseline...
2022-04-23 14:24:19 | [train_policy] epoch #880 | Saving snapshot...
2022-04-23 14:24:19 | [train_policy] epoch #880 | Saved
2022-04-23 14:24:19 | [train_policy] epoch #880 | Time 307.24 s
2022-04-23 14:24:19 | [train_policy] epoch #880 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119222
Evaluation/AverageDiscountedReturn          -39.4135
Evaluation/AverageReturn                    -39.4135
Evaluation/CompletionRate                     0
Evaluation/Iteration                        880
Evaluation/MaxReturn                        -30.4178
Evaluation/MinReturn                        -72.5571
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.54344
Extras/EpisodeRewardMean                    -39.3477
LinearFeatureBaseline/ExplainedVariance       0.873468
PolicyExecTime                                0.0939083
ProcessExecTime                               0.0110695
TotalEnvSteps                            891572
policy/Entropy                               -2.31114
policy/KL                                     0.00649086
policy/KLBefore                               0
policy/LossAfter                             -0.02114
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.0991481
policy/dLoss                                  0.02114
---------------------------------------  ----------------
2022-04-23 14:24:19 | [train_policy] epoch #881 | Obtaining samples for iteration 881...
2022-04-23 14:24:19 | [train_policy] epoch #881 | Logging diagnostics...
2022-04-23 14:24:19 | [train_policy] epoch #881 | Optimizing policy...
2022-04-23 14:24:19 | [train_policy] epoch #881 | Computing loss before
2022-04-23 14:24:19 | [train_policy] epoch #881 | Computing KL before
2022-04-23 14:24:19 | [train_policy] epoch #881 | Optimizing
2022-04-23 14:24:19 | [train_policy] epoch #881 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:19 | [train_policy] epoch #881 | computing loss before
2022-04-23 14:24:19 | [train_policy] epoch #881 | computing gradient
2022-04-23 14:24:19 | [train_policy] epoch #881 | gradient computed
2022-04-23 14:24:19 | [train_policy] epoch #881 | computing descent direction
2022-04-23 14:24:19 | [train_policy] epoch #881 | descent direction computed
2022-04-23 14:24:19 | [train_policy] epoch #881 | backtrack iters: 1
2022-04-23 14:24:19 | [train_policy] epoch #881 | optimization finished
2022-04-23 14:24:19 | [train_policy] epoch #881 | Computing KL after
2022-04-23 14:24:19 | [train_policy] epoch #881 | Computing loss after
2022-04-23 14:24:19 | [train_policy] epoch #881 | Fitting baseline...
2022-04-23 14:24:19 | [train_policy] epoch #881 | Saving snapshot...
2022-04-23 14:24:19 | [train_policy] epoch #881 | Saved
2022-04-23 14:24:19 | [train_policy] epoch #881 | Time 307.57 s
2022-04-23 14:24:19 | [train_policy] epoch #881 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.119652
Evaluation/AverageDiscountedReturn          -40.0573
Evaluation/AverageReturn                    -40.0573
Evaluation/CompletionRate                     0
Evaluation/Iteration                        881
Evaluation/MaxReturn                        -31.1358
Evaluation/MinReturn                        -71.6207
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.57444
Extras/EpisodeRewardMean                    -40.222
LinearFeatureBaseline/ExplainedVariance       0.87787
PolicyExecTime                                0.0930331
ProcessExecTime                               0.011189
TotalEnvSteps                            892584
policy/Entropy                               -2.35011
policy/KL                                     0.00638734
policy/KLBefore                               0
policy/LossAfter                             -0.00904119
policy/LossBefore                            -1.5549e-08
policy/Perplexity                             0.0953591
policy/dLoss                                  0.00904117
---------------------------------------  ---------------
2022-04-23 14:24:19 | [train_policy] epoch #882 | Obtaining samples for iteration 882...
2022-04-23 14:24:20 | [train_policy] epoch #882 | Logging diagnostics...
2022-04-23 14:24:20 | [train_policy] epoch #882 | Optimizing policy...
2022-04-23 14:24:20 | [train_policy] epoch #882 | Computing loss before
2022-04-23 14:24:20 | [train_policy] epoch #882 | Computing KL before
2022-04-23 14:24:20 | [train_policy] epoch #882 | Optimizing
2022-04-23 14:24:20 | [train_policy] epoch #882 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:20 | [train_policy] epoch #882 | computing loss before
2022-04-23 14:24:20 | [train_policy] epoch #882 | computing gradient
2022-04-23 14:24:20 | [train_policy] epoch #882 | gradient computed
2022-04-23 14:24:20 | [train_policy] epoch #882 | computing descent direction
2022-04-23 14:24:20 | [train_policy] epoch #882 | descent direction computed
2022-04-23 14:24:20 | [train_policy] epoch #882 | backtrack iters: 1
2022-04-23 14:24:20 | [train_policy] epoch #882 | optimization finished
2022-04-23 14:24:20 | [train_policy] epoch #882 | Computing KL after
2022-04-23 14:24:20 | [train_policy] epoch #882 | Computing loss after
2022-04-23 14:24:20 | [train_policy] epoch #882 | Fitting baseline...
2022-04-23 14:24:20 | [train_policy] epoch #882 | Saving snapshot...
2022-04-23 14:24:20 | [train_policy] epoch #882 | Saved
2022-04-23 14:24:20 | [train_policy] epoch #882 | Time 307.90 s
2022-04-23 14:24:20 | [train_policy] epoch #882 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.119498
Evaluation/AverageDiscountedReturn          -39.5699
Evaluation/AverageReturn                    -39.5699
Evaluation/CompletionRate                     0
Evaluation/Iteration                        882
Evaluation/MaxReturn                        -31.3932
Evaluation/MinReturn                        -72.4087
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.20432
Extras/EpisodeRewardMean                    -39.5444
LinearFeatureBaseline/ExplainedVariance       0.891468
PolicyExecTime                                0.0920045
ProcessExecTime                               0.011167
TotalEnvSteps                            893596
policy/Entropy                               -2.36504
policy/KL                                     0.00654415
policy/KLBefore                               0
policy/LossAfter                             -0.0194199
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.0939458
policy/dLoss                                  0.0194199
---------------------------------------  ---------------
2022-04-23 14:24:20 | [train_policy] epoch #883 | Obtaining samples for iteration 883...
2022-04-23 14:24:20 | [train_policy] epoch #883 | Logging diagnostics...
2022-04-23 14:24:20 | [train_policy] epoch #883 | Optimizing policy...
2022-04-23 14:24:20 | [train_policy] epoch #883 | Computing loss before
2022-04-23 14:24:20 | [train_policy] epoch #883 | Computing KL before
2022-04-23 14:24:20 | [train_policy] epoch #883 | Optimizing
2022-04-23 14:24:20 | [train_policy] epoch #883 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:20 | [train_policy] epoch #883 | computing loss before
2022-04-23 14:24:20 | [train_policy] epoch #883 | computing gradient
2022-04-23 14:24:20 | [train_policy] epoch #883 | gradient computed
2022-04-23 14:24:20 | [train_policy] epoch #883 | computing descent direction
2022-04-23 14:24:20 | [train_policy] epoch #883 | descent direction computed
2022-04-23 14:24:20 | [train_policy] epoch #883 | backtrack iters: 0
2022-04-23 14:24:20 | [train_policy] epoch #883 | optimization finished
2022-04-23 14:24:20 | [train_policy] epoch #883 | Computing KL after
2022-04-23 14:24:20 | [train_policy] epoch #883 | Computing loss after
2022-04-23 14:24:20 | [train_policy] epoch #883 | Fitting baseline...
2022-04-23 14:24:20 | [train_policy] epoch #883 | Saving snapshot...
2022-04-23 14:24:20 | [train_policy] epoch #883 | Saved
2022-04-23 14:24:20 | [train_policy] epoch #883 | Time 308.22 s
2022-04-23 14:24:20 | [train_policy] epoch #883 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119866
Evaluation/AverageDiscountedReturn          -42.0007
Evaluation/AverageReturn                    -42.0007
Evaluation/CompletionRate                     0
Evaluation/Iteration                        883
Evaluation/MaxReturn                        -30.5881
Evaluation/MinReturn                        -71.9754
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.97829
Extras/EpisodeRewardMean                    -41.557
LinearFeatureBaseline/ExplainedVariance       0.807294
PolicyExecTime                                0.095665
ProcessExecTime                               0.0112386
TotalEnvSteps                            894608
policy/Entropy                               -2.38248
policy/KL                                     0.00999913
policy/KLBefore                               0
policy/LossAfter                             -0.0231136
policy/LossBefore                            -1.86117e-08
policy/Perplexity                             0.0923216
policy/dLoss                                  0.0231135
---------------------------------------  ----------------
2022-04-23 14:24:20 | [train_policy] epoch #884 | Obtaining samples for iteration 884...
2022-04-23 14:24:20 | [train_policy] epoch #884 | Logging diagnostics...
2022-04-23 14:24:20 | [train_policy] epoch #884 | Optimizing policy...
2022-04-23 14:24:20 | [train_policy] epoch #884 | Computing loss before
2022-04-23 14:24:20 | [train_policy] epoch #884 | Computing KL before
2022-04-23 14:24:20 | [train_policy] epoch #884 | Optimizing
2022-04-23 14:24:20 | [train_policy] epoch #884 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:20 | [train_policy] epoch #884 | computing loss before
2022-04-23 14:24:20 | [train_policy] epoch #884 | computing gradient
2022-04-23 14:24:20 | [train_policy] epoch #884 | gradient computed
2022-04-23 14:24:20 | [train_policy] epoch #884 | computing descent direction
2022-04-23 14:24:20 | [train_policy] epoch #884 | descent direction computed
2022-04-23 14:24:20 | [train_policy] epoch #884 | backtrack iters: 0
2022-04-23 14:24:20 | [train_policy] epoch #884 | optimization finished
2022-04-23 14:24:20 | [train_policy] epoch #884 | Computing KL after
2022-04-23 14:24:20 | [train_policy] epoch #884 | Computing loss after
2022-04-23 14:24:20 | [train_policy] epoch #884 | Fitting baseline...
2022-04-23 14:24:20 | [train_policy] epoch #884 | Saving snapshot...
2022-04-23 14:24:20 | [train_policy] epoch #884 | Saved
2022-04-23 14:24:20 | [train_policy] epoch #884 | Time 308.55 s
2022-04-23 14:24:20 | [train_policy] epoch #884 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120278
Evaluation/AverageDiscountedReturn          -39.9891
Evaluation/AverageReturn                    -39.9891
Evaluation/CompletionRate                     0
Evaluation/Iteration                        884
Evaluation/MaxReturn                        -31.2085
Evaluation/MinReturn                        -71.5251
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.24944
Extras/EpisodeRewardMean                    -39.9453
LinearFeatureBaseline/ExplainedVariance       0.882143
PolicyExecTime                                0.0939493
ProcessExecTime                               0.0110919
TotalEnvSteps                            895620
policy/Entropy                               -2.35833
policy/KL                                     0.00983505
policy/KLBefore                               0
policy/LossAfter                             -0.0150386
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.0945782
policy/dLoss                                  0.0150386
---------------------------------------  ----------------
2022-04-23 14:24:20 | [train_policy] epoch #885 | Obtaining samples for iteration 885...
2022-04-23 14:24:21 | [train_policy] epoch #885 | Logging diagnostics...
2022-04-23 14:24:21 | [train_policy] epoch #885 | Optimizing policy...
2022-04-23 14:24:21 | [train_policy] epoch #885 | Computing loss before
2022-04-23 14:24:21 | [train_policy] epoch #885 | Computing KL before
2022-04-23 14:24:21 | [train_policy] epoch #885 | Optimizing
2022-04-23 14:24:21 | [train_policy] epoch #885 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:21 | [train_policy] epoch #885 | computing loss before
2022-04-23 14:24:21 | [train_policy] epoch #885 | computing gradient
2022-04-23 14:24:21 | [train_policy] epoch #885 | gradient computed
2022-04-23 14:24:21 | [train_policy] epoch #885 | computing descent direction
2022-04-23 14:24:21 | [train_policy] epoch #885 | descent direction computed
2022-04-23 14:24:21 | [train_policy] epoch #885 | backtrack iters: 1
2022-04-23 14:24:21 | [train_policy] epoch #885 | optimization finished
2022-04-23 14:24:21 | [train_policy] epoch #885 | Computing KL after
2022-04-23 14:24:21 | [train_policy] epoch #885 | Computing loss after
2022-04-23 14:24:21 | [train_policy] epoch #885 | Fitting baseline...
2022-04-23 14:24:21 | [train_policy] epoch #885 | Saving snapshot...
2022-04-23 14:24:21 | [train_policy] epoch #885 | Saved
2022-04-23 14:24:21 | [train_policy] epoch #885 | Time 308.87 s
2022-04-23 14:24:21 | [train_policy] epoch #885 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.11926
Evaluation/AverageDiscountedReturn          -41.1275
Evaluation/AverageReturn                    -41.1275
Evaluation/CompletionRate                     0
Evaluation/Iteration                        885
Evaluation/MaxReturn                        -31.3713
Evaluation/MinReturn                        -72.1395
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.8614
Extras/EpisodeRewardMean                    -41.07
LinearFeatureBaseline/ExplainedVariance       0.885993
PolicyExecTime                                0.0947068
ProcessExecTime                               0.0113888
TotalEnvSteps                            896632
policy/Entropy                               -2.35438
policy/KL                                     0.0066242
policy/KLBefore                               0
policy/LossAfter                             -0.0192226
policy/LossBefore                            -5.18301e-09
policy/Perplexity                             0.0949521
policy/dLoss                                  0.0192226
---------------------------------------  ----------------
2022-04-23 14:24:21 | [train_policy] epoch #886 | Obtaining samples for iteration 886...
2022-04-23 14:24:21 | [train_policy] epoch #886 | Logging diagnostics...
2022-04-23 14:24:21 | [train_policy] epoch #886 | Optimizing policy...
2022-04-23 14:24:21 | [train_policy] epoch #886 | Computing loss before
2022-04-23 14:24:21 | [train_policy] epoch #886 | Computing KL before
2022-04-23 14:24:21 | [train_policy] epoch #886 | Optimizing
2022-04-23 14:24:21 | [train_policy] epoch #886 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:21 | [train_policy] epoch #886 | computing loss before
2022-04-23 14:24:21 | [train_policy] epoch #886 | computing gradient
2022-04-23 14:24:21 | [train_policy] epoch #886 | gradient computed
2022-04-23 14:24:21 | [train_policy] epoch #886 | computing descent direction
2022-04-23 14:24:21 | [train_policy] epoch #886 | descent direction computed
2022-04-23 14:24:21 | [train_policy] epoch #886 | backtrack iters: 0
2022-04-23 14:24:21 | [train_policy] epoch #886 | optimization finished
2022-04-23 14:24:21 | [train_policy] epoch #886 | Computing KL after
2022-04-23 14:24:21 | [train_policy] epoch #886 | Computing loss after
2022-04-23 14:24:21 | [train_policy] epoch #886 | Fitting baseline...
2022-04-23 14:24:21 | [train_policy] epoch #886 | Saving snapshot...
2022-04-23 14:24:21 | [train_policy] epoch #886 | Saved
2022-04-23 14:24:21 | [train_policy] epoch #886 | Time 309.20 s
2022-04-23 14:24:21 | [train_policy] epoch #886 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120281
Evaluation/AverageDiscountedReturn          -41.1434
Evaluation/AverageReturn                    -41.1434
Evaluation/CompletionRate                     0
Evaluation/Iteration                        886
Evaluation/MaxReturn                        -31.3671
Evaluation/MinReturn                        -73.035
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.92536
Extras/EpisodeRewardMean                    -41.155
LinearFeatureBaseline/ExplainedVariance       0.837496
PolicyExecTime                                0.0957
ProcessExecTime                               0.0116274
TotalEnvSteps                            897644
policy/Entropy                               -2.38565
policy/KL                                     0.0099479
policy/KLBefore                               0
policy/LossAfter                             -0.0211496
policy/LossBefore                            -7.06774e-09
policy/Perplexity                             0.0920293
policy/dLoss                                  0.0211496
---------------------------------------  ----------------
2022-04-23 14:24:21 | [train_policy] epoch #887 | Obtaining samples for iteration 887...
2022-04-23 14:24:21 | [train_policy] epoch #887 | Logging diagnostics...
2022-04-23 14:24:21 | [train_policy] epoch #887 | Optimizing policy...
2022-04-23 14:24:21 | [train_policy] epoch #887 | Computing loss before
2022-04-23 14:24:21 | [train_policy] epoch #887 | Computing KL before
2022-04-23 14:24:21 | [train_policy] epoch #887 | Optimizing
2022-04-23 14:24:21 | [train_policy] epoch #887 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:21 | [train_policy] epoch #887 | computing loss before
2022-04-23 14:24:21 | [train_policy] epoch #887 | computing gradient
2022-04-23 14:24:21 | [train_policy] epoch #887 | gradient computed
2022-04-23 14:24:21 | [train_policy] epoch #887 | computing descent direction
2022-04-23 14:24:21 | [train_policy] epoch #887 | descent direction computed
2022-04-23 14:24:21 | [train_policy] epoch #887 | backtrack iters: 1
2022-04-23 14:24:21 | [train_policy] epoch #887 | optimization finished
2022-04-23 14:24:21 | [train_policy] epoch #887 | Computing KL after
2022-04-23 14:24:21 | [train_policy] epoch #887 | Computing loss after
2022-04-23 14:24:21 | [train_policy] epoch #887 | Fitting baseline...
2022-04-23 14:24:21 | [train_policy] epoch #887 | Saving snapshot...
2022-04-23 14:24:21 | [train_policy] epoch #887 | Saved
2022-04-23 14:24:21 | [train_policy] epoch #887 | Time 309.52 s
2022-04-23 14:24:21 | [train_policy] epoch #887 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.11945
Evaluation/AverageDiscountedReturn          -40.36
Evaluation/AverageReturn                    -40.36
Evaluation/CompletionRate                     0
Evaluation/Iteration                        887
Evaluation/MaxReturn                        -30.9079
Evaluation/MinReturn                        -71.8493
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.42085
Extras/EpisodeRewardMean                    -40.1385
LinearFeatureBaseline/ExplainedVariance       0.866211
PolicyExecTime                                0.0927994
ProcessExecTime                               0.0112002
TotalEnvSteps                            898656
policy/Entropy                               -2.40435
policy/KL                                     0.0065509
policy/KLBefore                               0
policy/LossAfter                             -0.0125178
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.0903242
policy/dLoss                                  0.0125178
---------------------------------------  ----------------
2022-04-23 14:24:21 | [train_policy] epoch #888 | Obtaining samples for iteration 888...
2022-04-23 14:24:22 | [train_policy] epoch #888 | Logging diagnostics...
2022-04-23 14:24:22 | [train_policy] epoch #888 | Optimizing policy...
2022-04-23 14:24:22 | [train_policy] epoch #888 | Computing loss before
2022-04-23 14:24:22 | [train_policy] epoch #888 | Computing KL before
2022-04-23 14:24:22 | [train_policy] epoch #888 | Optimizing
2022-04-23 14:24:22 | [train_policy] epoch #888 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:22 | [train_policy] epoch #888 | computing loss before
2022-04-23 14:24:22 | [train_policy] epoch #888 | computing gradient
2022-04-23 14:24:22 | [train_policy] epoch #888 | gradient computed
2022-04-23 14:24:22 | [train_policy] epoch #888 | computing descent direction
2022-04-23 14:24:22 | [train_policy] epoch #888 | descent direction computed
2022-04-23 14:24:22 | [train_policy] epoch #888 | backtrack iters: 0
2022-04-23 14:24:22 | [train_policy] epoch #888 | optimization finished
2022-04-23 14:24:22 | [train_policy] epoch #888 | Computing KL after
2022-04-23 14:24:22 | [train_policy] epoch #888 | Computing loss after
2022-04-23 14:24:22 | [train_policy] epoch #888 | Fitting baseline...
2022-04-23 14:24:22 | [train_policy] epoch #888 | Saving snapshot...
2022-04-23 14:24:22 | [train_policy] epoch #888 | Saved
2022-04-23 14:24:22 | [train_policy] epoch #888 | Time 309.84 s
2022-04-23 14:24:22 | [train_policy] epoch #888 | EpochTime 0.31 s
---------------------------------------  ---------------
EnvExecTime                                   0.119439
Evaluation/AverageDiscountedReturn          -41.0335
Evaluation/AverageReturn                    -41.0335
Evaluation/CompletionRate                     0
Evaluation/Iteration                        888
Evaluation/MaxReturn                        -31.4687
Evaluation/MinReturn                        -63.7462
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.33662
Extras/EpisodeRewardMean                    -41.2648
LinearFeatureBaseline/ExplainedVariance       0.891381
PolicyExecTime                                0.0930972
ProcessExecTime                               0.0111594
TotalEnvSteps                            899668
policy/Entropy                               -2.39387
policy/KL                                     0.00988052
policy/KLBefore                               0
policy/LossAfter                             -0.0172697
policy/LossBefore                            -5.5364e-09
policy/Perplexity                             0.0912757
policy/dLoss                                  0.0172697
---------------------------------------  ---------------
2022-04-23 14:24:22 | [train_policy] epoch #889 | Obtaining samples for iteration 889...
2022-04-23 14:24:22 | [train_policy] epoch #889 | Logging diagnostics...
2022-04-23 14:24:22 | [train_policy] epoch #889 | Optimizing policy...
2022-04-23 14:24:22 | [train_policy] epoch #889 | Computing loss before
2022-04-23 14:24:22 | [train_policy] epoch #889 | Computing KL before
2022-04-23 14:24:22 | [train_policy] epoch #889 | Optimizing
2022-04-23 14:24:22 | [train_policy] epoch #889 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:22 | [train_policy] epoch #889 | computing loss before
2022-04-23 14:24:22 | [train_policy] epoch #889 | computing gradient
2022-04-23 14:24:22 | [train_policy] epoch #889 | gradient computed
2022-04-23 14:24:22 | [train_policy] epoch #889 | computing descent direction
2022-04-23 14:24:22 | [train_policy] epoch #889 | descent direction computed
2022-04-23 14:24:22 | [train_policy] epoch #889 | backtrack iters: 0
2022-04-23 14:24:22 | [train_policy] epoch #889 | optimization finished
2022-04-23 14:24:22 | [train_policy] epoch #889 | Computing KL after
2022-04-23 14:24:22 | [train_policy] epoch #889 | Computing loss after
2022-04-23 14:24:22 | [train_policy] epoch #889 | Fitting baseline...
2022-04-23 14:24:22 | [train_policy] epoch #889 | Saving snapshot...
2022-04-23 14:24:22 | [train_policy] epoch #889 | Saved
2022-04-23 14:24:22 | [train_policy] epoch #889 | Time 310.16 s
2022-04-23 14:24:22 | [train_policy] epoch #889 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.119284
Evaluation/AverageDiscountedReturn          -41.4046
Evaluation/AverageReturn                    -41.4046
Evaluation/CompletionRate                     0
Evaluation/Iteration                        889
Evaluation/MaxReturn                        -31.4798
Evaluation/MinReturn                        -72.1664
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.1198
Extras/EpisodeRewardMean                    -41.6423
LinearFeatureBaseline/ExplainedVariance       0.851502
PolicyExecTime                                0.0940599
ProcessExecTime                               0.0111372
TotalEnvSteps                            900680
policy/Entropy                               -2.37518
policy/KL                                     0.00983696
policy/KLBefore                               0
policy/LossAfter                             -0.0184876
policy/LossBefore                            -0
policy/Perplexity                             0.0929973
policy/dLoss                                  0.0184876
---------------------------------------  ---------------
2022-04-23 14:24:22 | [train_policy] epoch #890 | Obtaining samples for iteration 890...
2022-04-23 14:24:22 | [train_policy] epoch #890 | Logging diagnostics...
2022-04-23 14:24:22 | [train_policy] epoch #890 | Optimizing policy...
2022-04-23 14:24:22 | [train_policy] epoch #890 | Computing loss before
2022-04-23 14:24:22 | [train_policy] epoch #890 | Computing KL before
2022-04-23 14:24:22 | [train_policy] epoch #890 | Optimizing
2022-04-23 14:24:22 | [train_policy] epoch #890 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:22 | [train_policy] epoch #890 | computing loss before
2022-04-23 14:24:22 | [train_policy] epoch #890 | computing gradient
2022-04-23 14:24:22 | [train_policy] epoch #890 | gradient computed
2022-04-23 14:24:22 | [train_policy] epoch #890 | computing descent direction
2022-04-23 14:24:22 | [train_policy] epoch #890 | descent direction computed
2022-04-23 14:24:22 | [train_policy] epoch #890 | backtrack iters: 1
2022-04-23 14:24:22 | [train_policy] epoch #890 | optimization finished
2022-04-23 14:24:22 | [train_policy] epoch #890 | Computing KL after
2022-04-23 14:24:22 | [train_policy] epoch #890 | Computing loss after
2022-04-23 14:24:22 | [train_policy] epoch #890 | Fitting baseline...
2022-04-23 14:24:22 | [train_policy] epoch #890 | Saving snapshot...
2022-04-23 14:24:22 | [train_policy] epoch #890 | Saved
2022-04-23 14:24:22 | [train_policy] epoch #890 | Time 310.49 s
2022-04-23 14:24:22 | [train_policy] epoch #890 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.12136
Evaluation/AverageDiscountedReturn          -41.2906
Evaluation/AverageReturn                    -41.2906
Evaluation/CompletionRate                     0
Evaluation/Iteration                        890
Evaluation/MaxReturn                        -29.9951
Evaluation/MinReturn                        -71.5322
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.02937
Extras/EpisodeRewardMean                    -41.412
LinearFeatureBaseline/ExplainedVariance       0.860609
PolicyExecTime                                0.096009
ProcessExecTime                               0.0114186
TotalEnvSteps                            901692
policy/Entropy                               -2.38131
policy/KL                                     0.00641728
policy/KLBefore                               0
policy/LossAfter                             -0.0135032
policy/LossBefore                             1.31931e-08
policy/Perplexity                             0.0924291
policy/dLoss                                  0.0135032
---------------------------------------  ----------------
2022-04-23 14:24:22 | [train_policy] epoch #891 | Obtaining samples for iteration 891...
2022-04-23 14:24:23 | [train_policy] epoch #891 | Logging diagnostics...
2022-04-23 14:24:23 | [train_policy] epoch #891 | Optimizing policy...
2022-04-23 14:24:23 | [train_policy] epoch #891 | Computing loss before
2022-04-23 14:24:23 | [train_policy] epoch #891 | Computing KL before
2022-04-23 14:24:23 | [train_policy] epoch #891 | Optimizing
2022-04-23 14:24:23 | [train_policy] epoch #891 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:23 | [train_policy] epoch #891 | computing loss before
2022-04-23 14:24:23 | [train_policy] epoch #891 | computing gradient
2022-04-23 14:24:23 | [train_policy] epoch #891 | gradient computed
2022-04-23 14:24:23 | [train_policy] epoch #891 | computing descent direction
2022-04-23 14:24:23 | [train_policy] epoch #891 | descent direction computed
2022-04-23 14:24:23 | [train_policy] epoch #891 | backtrack iters: 1
2022-04-23 14:24:23 | [train_policy] epoch #891 | optimization finished
2022-04-23 14:24:23 | [train_policy] epoch #891 | Computing KL after
2022-04-23 14:24:23 | [train_policy] epoch #891 | Computing loss after
2022-04-23 14:24:23 | [train_policy] epoch #891 | Fitting baseline...
2022-04-23 14:24:23 | [train_policy] epoch #891 | Saving snapshot...
2022-04-23 14:24:23 | [train_policy] epoch #891 | Saved
2022-04-23 14:24:23 | [train_policy] epoch #891 | Time 310.83 s
2022-04-23 14:24:23 | [train_policy] epoch #891 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.121076
Evaluation/AverageDiscountedReturn          -40.4756
Evaluation/AverageReturn                    -40.4756
Evaluation/CompletionRate                     0
Evaluation/Iteration                        891
Evaluation/MaxReturn                        -30.3181
Evaluation/MinReturn                        -63.9408
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.94477
Extras/EpisodeRewardMean                    -40.6343
LinearFeatureBaseline/ExplainedVariance       0.885217
PolicyExecTime                                0.102678
ProcessExecTime                               0.0113432
TotalEnvSteps                            902704
policy/Entropy                               -2.40517
policy/KL                                     0.00643703
policy/KLBefore                               0
policy/LossAfter                             -0.0149373
policy/LossBefore                            -1.6138e-08
policy/Perplexity                             0.0902502
policy/dLoss                                  0.0149373
---------------------------------------  ---------------
2022-04-23 14:24:23 | [train_policy] epoch #892 | Obtaining samples for iteration 892...
2022-04-23 14:24:23 | [train_policy] epoch #892 | Logging diagnostics...
2022-04-23 14:24:23 | [train_policy] epoch #892 | Optimizing policy...
2022-04-23 14:24:23 | [train_policy] epoch #892 | Computing loss before
2022-04-23 14:24:23 | [train_policy] epoch #892 | Computing KL before
2022-04-23 14:24:23 | [train_policy] epoch #892 | Optimizing
2022-04-23 14:24:23 | [train_policy] epoch #892 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:23 | [train_policy] epoch #892 | computing loss before
2022-04-23 14:24:23 | [train_policy] epoch #892 | computing gradient
2022-04-23 14:24:23 | [train_policy] epoch #892 | gradient computed
2022-04-23 14:24:23 | [train_policy] epoch #892 | computing descent direction
2022-04-23 14:24:23 | [train_policy] epoch #892 | descent direction computed
2022-04-23 14:24:23 | [train_policy] epoch #892 | backtrack iters: 0
2022-04-23 14:24:23 | [train_policy] epoch #892 | optimization finished
2022-04-23 14:24:23 | [train_policy] epoch #892 | Computing KL after
2022-04-23 14:24:23 | [train_policy] epoch #892 | Computing loss after
2022-04-23 14:24:23 | [train_policy] epoch #892 | Fitting baseline...
2022-04-23 14:24:23 | [train_policy] epoch #892 | Saving snapshot...
2022-04-23 14:24:23 | [train_policy] epoch #892 | Saved
2022-04-23 14:24:23 | [train_policy] epoch #892 | Time 311.16 s
2022-04-23 14:24:23 | [train_policy] epoch #892 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120897
Evaluation/AverageDiscountedReturn          -39.7806
Evaluation/AverageReturn                    -39.7806
Evaluation/CompletionRate                     0
Evaluation/Iteration                        892
Evaluation/MaxReturn                        -30.3583
Evaluation/MinReturn                        -64.8363
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.39078
Extras/EpisodeRewardMean                    -40.0194
LinearFeatureBaseline/ExplainedVariance       0.870676
PolicyExecTime                                0.0991631
ProcessExecTime                               0.011225
TotalEnvSteps                            903716
policy/Entropy                               -2.35274
policy/KL                                     0.00948137
policy/KLBefore                               0
policy/LossAfter                             -0.0139693
policy/LossBefore                             9.07027e-09
policy/Perplexity                             0.0951083
policy/dLoss                                  0.0139693
---------------------------------------  ----------------
2022-04-23 14:24:23 | [train_policy] epoch #893 | Obtaining samples for iteration 893...
2022-04-23 14:24:23 | [train_policy] epoch #893 | Logging diagnostics...
2022-04-23 14:24:23 | [train_policy] epoch #893 | Optimizing policy...
2022-04-23 14:24:23 | [train_policy] epoch #893 | Computing loss before
2022-04-23 14:24:23 | [train_policy] epoch #893 | Computing KL before
2022-04-23 14:24:23 | [train_policy] epoch #893 | Optimizing
2022-04-23 14:24:23 | [train_policy] epoch #893 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:23 | [train_policy] epoch #893 | computing loss before
2022-04-23 14:24:23 | [train_policy] epoch #893 | computing gradient
2022-04-23 14:24:23 | [train_policy] epoch #893 | gradient computed
2022-04-23 14:24:23 | [train_policy] epoch #893 | computing descent direction
2022-04-23 14:24:23 | [train_policy] epoch #893 | descent direction computed
2022-04-23 14:24:23 | [train_policy] epoch #893 | backtrack iters: 1
2022-04-23 14:24:23 | [train_policy] epoch #893 | optimization finished
2022-04-23 14:24:23 | [train_policy] epoch #893 | Computing KL after
2022-04-23 14:24:23 | [train_policy] epoch #893 | Computing loss after
2022-04-23 14:24:23 | [train_policy] epoch #893 | Fitting baseline...
2022-04-23 14:24:23 | [train_policy] epoch #893 | Saving snapshot...
2022-04-23 14:24:23 | [train_policy] epoch #893 | Saved
2022-04-23 14:24:23 | [train_policy] epoch #893 | Time 311.48 s
2022-04-23 14:24:23 | [train_policy] epoch #893 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119387
Evaluation/AverageDiscountedReturn          -41.2922
Evaluation/AverageReturn                    -41.2922
Evaluation/CompletionRate                     0
Evaluation/Iteration                        893
Evaluation/MaxReturn                        -29.697
Evaluation/MinReturn                        -71.4925
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.09638
Extras/EpisodeRewardMean                    -41.1298
LinearFeatureBaseline/ExplainedVariance       0.860691
PolicyExecTime                                0.0973103
ProcessExecTime                               0.0113871
TotalEnvSteps                            904728
policy/Entropy                               -2.38988
policy/KL                                     0.00666971
policy/KLBefore                               0
policy/LossAfter                             -0.015713
policy/LossBefore                             6.47877e-09
policy/Perplexity                             0.0916409
policy/dLoss                                  0.015713
---------------------------------------  ----------------
2022-04-23 14:24:23 | [train_policy] epoch #894 | Obtaining samples for iteration 894...
2022-04-23 14:24:24 | [train_policy] epoch #894 | Logging diagnostics...
2022-04-23 14:24:24 | [train_policy] epoch #894 | Optimizing policy...
2022-04-23 14:24:24 | [train_policy] epoch #894 | Computing loss before
2022-04-23 14:24:24 | [train_policy] epoch #894 | Computing KL before
2022-04-23 14:24:24 | [train_policy] epoch #894 | Optimizing
2022-04-23 14:24:24 | [train_policy] epoch #894 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:24 | [train_policy] epoch #894 | computing loss before
2022-04-23 14:24:24 | [train_policy] epoch #894 | computing gradient
2022-04-23 14:24:24 | [train_policy] epoch #894 | gradient computed
2022-04-23 14:24:24 | [train_policy] epoch #894 | computing descent direction
2022-04-23 14:24:24 | [train_policy] epoch #894 | descent direction computed
2022-04-23 14:24:24 | [train_policy] epoch #894 | backtrack iters: 1
2022-04-23 14:24:24 | [train_policy] epoch #894 | optimization finished
2022-04-23 14:24:24 | [train_policy] epoch #894 | Computing KL after
2022-04-23 14:24:24 | [train_policy] epoch #894 | Computing loss after
2022-04-23 14:24:24 | [train_policy] epoch #894 | Fitting baseline...
2022-04-23 14:24:24 | [train_policy] epoch #894 | Saving snapshot...
2022-04-23 14:24:24 | [train_policy] epoch #894 | Saved
2022-04-23 14:24:24 | [train_policy] epoch #894 | Time 311.81 s
2022-04-23 14:24:24 | [train_policy] epoch #894 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119438
Evaluation/AverageDiscountedReturn          -40.8179
Evaluation/AverageReturn                    -40.8179
Evaluation/CompletionRate                     0
Evaluation/Iteration                        894
Evaluation/MaxReturn                        -31.093
Evaluation/MinReturn                        -71.6421
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.65653
Extras/EpisodeRewardMean                    -40.7403
LinearFeatureBaseline/ExplainedVariance       0.859792
PolicyExecTime                                0.0947671
ProcessExecTime                               0.0112407
TotalEnvSteps                            905740
policy/Entropy                               -2.38767
policy/KL                                     0.00658236
policy/KLBefore                               0
policy/LossAfter                             -0.01537
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.0918431
policy/dLoss                                  0.01537
---------------------------------------  ----------------
2022-04-23 14:24:24 | [train_policy] epoch #895 | Obtaining samples for iteration 895...
2022-04-23 14:24:24 | [train_policy] epoch #895 | Logging diagnostics...
2022-04-23 14:24:24 | [train_policy] epoch #895 | Optimizing policy...
2022-04-23 14:24:24 | [train_policy] epoch #895 | Computing loss before
2022-04-23 14:24:24 | [train_policy] epoch #895 | Computing KL before
2022-04-23 14:24:24 | [train_policy] epoch #895 | Optimizing
2022-04-23 14:24:24 | [train_policy] epoch #895 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:24 | [train_policy] epoch #895 | computing loss before
2022-04-23 14:24:24 | [train_policy] epoch #895 | computing gradient
2022-04-23 14:24:24 | [train_policy] epoch #895 | gradient computed
2022-04-23 14:24:24 | [train_policy] epoch #895 | computing descent direction
2022-04-23 14:24:24 | [train_policy] epoch #895 | descent direction computed
2022-04-23 14:24:24 | [train_policy] epoch #895 | backtrack iters: 1
2022-04-23 14:24:24 | [train_policy] epoch #895 | optimization finished
2022-04-23 14:24:24 | [train_policy] epoch #895 | Computing KL after
2022-04-23 14:24:24 | [train_policy] epoch #895 | Computing loss after
2022-04-23 14:24:24 | [train_policy] epoch #895 | Fitting baseline...
2022-04-23 14:24:24 | [train_policy] epoch #895 | Saving snapshot...
2022-04-23 14:24:24 | [train_policy] epoch #895 | Saved
2022-04-23 14:24:24 | [train_policy] epoch #895 | Time 312.13 s
2022-04-23 14:24:24 | [train_policy] epoch #895 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119736
Evaluation/AverageDiscountedReturn          -40.5489
Evaluation/AverageReturn                    -40.5489
Evaluation/CompletionRate                     0
Evaluation/Iteration                        895
Evaluation/MaxReturn                        -31.1302
Evaluation/MinReturn                        -71.5301
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.0624
Extras/EpisodeRewardMean                    -40.8622
LinearFeatureBaseline/ExplainedVariance       0.872341
PolicyExecTime                                0.0935154
ProcessExecTime                               0.0111876
TotalEnvSteps                            906752
policy/Entropy                               -2.39501
policy/KL                                     0.00659797
policy/KLBefore                               0
policy/LossAfter                             -0.0202088
policy/LossBefore                             8.48129e-09
policy/Perplexity                             0.0911716
policy/dLoss                                  0.0202088
---------------------------------------  ----------------
2022-04-23 14:24:24 | [train_policy] epoch #896 | Obtaining samples for iteration 896...
2022-04-23 14:24:24 | [train_policy] epoch #896 | Logging diagnostics...
2022-04-23 14:24:24 | [train_policy] epoch #896 | Optimizing policy...
2022-04-23 14:24:24 | [train_policy] epoch #896 | Computing loss before
2022-04-23 14:24:24 | [train_policy] epoch #896 | Computing KL before
2022-04-23 14:24:24 | [train_policy] epoch #896 | Optimizing
2022-04-23 14:24:24 | [train_policy] epoch #896 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:24 | [train_policy] epoch #896 | computing loss before
2022-04-23 14:24:24 | [train_policy] epoch #896 | computing gradient
2022-04-23 14:24:24 | [train_policy] epoch #896 | gradient computed
2022-04-23 14:24:24 | [train_policy] epoch #896 | computing descent direction
2022-04-23 14:24:24 | [train_policy] epoch #896 | descent direction computed
2022-04-23 14:24:24 | [train_policy] epoch #896 | backtrack iters: 0
2022-04-23 14:24:24 | [train_policy] epoch #896 | optimization finished
2022-04-23 14:24:24 | [train_policy] epoch #896 | Computing KL after
2022-04-23 14:24:24 | [train_policy] epoch #896 | Computing loss after
2022-04-23 14:24:24 | [train_policy] epoch #896 | Fitting baseline...
2022-04-23 14:24:24 | [train_policy] epoch #896 | Saving snapshot...
2022-04-23 14:24:24 | [train_policy] epoch #896 | Saved
2022-04-23 14:24:24 | [train_policy] epoch #896 | Time 312.46 s
2022-04-23 14:24:24 | [train_policy] epoch #896 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120103
Evaluation/AverageDiscountedReturn          -40.69
Evaluation/AverageReturn                    -40.69
Evaluation/CompletionRate                     0
Evaluation/Iteration                        896
Evaluation/MaxReturn                        -31.3672
Evaluation/MinReturn                        -71.3069
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.52057
Extras/EpisodeRewardMean                    -40.5472
LinearFeatureBaseline/ExplainedVariance       0.877127
PolicyExecTime                                0.0932021
ProcessExecTime                               0.0111978
TotalEnvSteps                            907764
policy/Entropy                               -2.38847
policy/KL                                     0.00994991
policy/KLBefore                               0
policy/LossAfter                             -0.0149843
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.0917704
policy/dLoss                                  0.0149843
---------------------------------------  ----------------
2022-04-23 14:24:24 | [train_policy] epoch #897 | Obtaining samples for iteration 897...
2022-04-23 14:24:24 | [train_policy] epoch #897 | Logging diagnostics...
2022-04-23 14:24:24 | [train_policy] epoch #897 | Optimizing policy...
2022-04-23 14:24:25 | [train_policy] epoch #897 | Computing loss before
2022-04-23 14:24:25 | [train_policy] epoch #897 | Computing KL before
2022-04-23 14:24:25 | [train_policy] epoch #897 | Optimizing
2022-04-23 14:24:25 | [train_policy] epoch #897 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:25 | [train_policy] epoch #897 | computing loss before
2022-04-23 14:24:25 | [train_policy] epoch #897 | computing gradient
2022-04-23 14:24:25 | [train_policy] epoch #897 | gradient computed
2022-04-23 14:24:25 | [train_policy] epoch #897 | computing descent direction
2022-04-23 14:24:25 | [train_policy] epoch #897 | descent direction computed
2022-04-23 14:24:25 | [train_policy] epoch #897 | backtrack iters: 1
2022-04-23 14:24:25 | [train_policy] epoch #897 | optimization finished
2022-04-23 14:24:25 | [train_policy] epoch #897 | Computing KL after
2022-04-23 14:24:25 | [train_policy] epoch #897 | Computing loss after
2022-04-23 14:24:25 | [train_policy] epoch #897 | Fitting baseline...
2022-04-23 14:24:25 | [train_policy] epoch #897 | Saving snapshot...
2022-04-23 14:24:25 | [train_policy] epoch #897 | Saved
2022-04-23 14:24:25 | [train_policy] epoch #897 | Time 312.79 s
2022-04-23 14:24:25 | [train_policy] epoch #897 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119246
Evaluation/AverageDiscountedReturn          -40.619
Evaluation/AverageReturn                    -40.619
Evaluation/CompletionRate                     0
Evaluation/Iteration                        897
Evaluation/MaxReturn                        -31.5603
Evaluation/MinReturn                        -61.4746
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.31948
Extras/EpisodeRewardMean                    -40.7826
LinearFeatureBaseline/ExplainedVariance       0.88189
PolicyExecTime                                0.092181
ProcessExecTime                               0.0111375
TotalEnvSteps                            908776
policy/Entropy                               -2.38812
policy/KL                                     0.00750187
policy/KLBefore                               0
policy/LossAfter                             -0.0132817
policy/LossBefore                             4.12285e-10
policy/Perplexity                             0.0918023
policy/dLoss                                  0.0132817
---------------------------------------  ----------------
2022-04-23 14:24:25 | [train_policy] epoch #898 | Obtaining samples for iteration 898...
2022-04-23 14:24:25 | [train_policy] epoch #898 | Logging diagnostics...
2022-04-23 14:24:25 | [train_policy] epoch #898 | Optimizing policy...
2022-04-23 14:24:25 | [train_policy] epoch #898 | Computing loss before
2022-04-23 14:24:25 | [train_policy] epoch #898 | Computing KL before
2022-04-23 14:24:25 | [train_policy] epoch #898 | Optimizing
2022-04-23 14:24:25 | [train_policy] epoch #898 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:25 | [train_policy] epoch #898 | computing loss before
2022-04-23 14:24:25 | [train_policy] epoch #898 | computing gradient
2022-04-23 14:24:25 | [train_policy] epoch #898 | gradient computed
2022-04-23 14:24:25 | [train_policy] epoch #898 | computing descent direction
2022-04-23 14:24:25 | [train_policy] epoch #898 | descent direction computed
2022-04-23 14:24:25 | [train_policy] epoch #898 | backtrack iters: 1
2022-04-23 14:24:25 | [train_policy] epoch #898 | optimization finished
2022-04-23 14:24:25 | [train_policy] epoch #898 | Computing KL after
2022-04-23 14:24:25 | [train_policy] epoch #898 | Computing loss after
2022-04-23 14:24:25 | [train_policy] epoch #898 | Fitting baseline...
2022-04-23 14:24:25 | [train_policy] epoch #898 | Saving snapshot...
2022-04-23 14:24:25 | [train_policy] epoch #898 | Saved
2022-04-23 14:24:25 | [train_policy] epoch #898 | Time 313.11 s
2022-04-23 14:24:25 | [train_policy] epoch #898 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.12022
Evaluation/AverageDiscountedReturn          -40.4779
Evaluation/AverageReturn                    -40.4779
Evaluation/CompletionRate                     0
Evaluation/Iteration                        898
Evaluation/MaxReturn                        -31.2004
Evaluation/MinReturn                        -64.7247
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.99457
Extras/EpisodeRewardMean                    -40.5567
LinearFeatureBaseline/ExplainedVariance       0.883248
PolicyExecTime                                0.0947382
ProcessExecTime                               0.0113809
TotalEnvSteps                            909788
policy/Entropy                               -2.38217
policy/KL                                     0.00653611
policy/KLBefore                               0
policy/LossAfter                             -0.0129651
policy/LossBefore                            -1.01304e-08
policy/Perplexity                             0.0923504
policy/dLoss                                  0.0129651
---------------------------------------  ----------------
2022-04-23 14:24:25 | [train_policy] epoch #899 | Obtaining samples for iteration 899...
2022-04-23 14:24:25 | [train_policy] epoch #899 | Logging diagnostics...
2022-04-23 14:24:25 | [train_policy] epoch #899 | Optimizing policy...
2022-04-23 14:24:25 | [train_policy] epoch #899 | Computing loss before
2022-04-23 14:24:25 | [train_policy] epoch #899 | Computing KL before
2022-04-23 14:24:25 | [train_policy] epoch #899 | Optimizing
2022-04-23 14:24:25 | [train_policy] epoch #899 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:25 | [train_policy] epoch #899 | computing loss before
2022-04-23 14:24:25 | [train_policy] epoch #899 | computing gradient
2022-04-23 14:24:25 | [train_policy] epoch #899 | gradient computed
2022-04-23 14:24:25 | [train_policy] epoch #899 | computing descent direction
2022-04-23 14:24:25 | [train_policy] epoch #899 | descent direction computed
2022-04-23 14:24:25 | [train_policy] epoch #899 | backtrack iters: 1
2022-04-23 14:24:25 | [train_policy] epoch #899 | optimization finished
2022-04-23 14:24:25 | [train_policy] epoch #899 | Computing KL after
2022-04-23 14:24:25 | [train_policy] epoch #899 | Computing loss after
2022-04-23 14:24:25 | [train_policy] epoch #899 | Fitting baseline...
2022-04-23 14:24:25 | [train_policy] epoch #899 | Saving snapshot...
2022-04-23 14:24:25 | [train_policy] epoch #899 | Saved
2022-04-23 14:24:25 | [train_policy] epoch #899 | Time 313.44 s
2022-04-23 14:24:25 | [train_policy] epoch #899 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119699
Evaluation/AverageDiscountedReturn          -41.6214
Evaluation/AverageReturn                    -41.6214
Evaluation/CompletionRate                     0
Evaluation/Iteration                        899
Evaluation/MaxReturn                        -30.4982
Evaluation/MinReturn                        -71.2841
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.2692
Extras/EpisodeRewardMean                    -41.7685
LinearFeatureBaseline/ExplainedVariance       0.861507
PolicyExecTime                                0.0963397
ProcessExecTime                               0.0111954
TotalEnvSteps                            910800
policy/Entropy                               -2.39968
policy/KL                                     0.00658809
policy/KLBefore                               0
policy/LossAfter                             -0.01749
policy/LossBefore                            -9.65925e-09
policy/Perplexity                             0.090747
policy/dLoss                                  0.01749
---------------------------------------  ----------------
2022-04-23 14:24:25 | [train_policy] epoch #900 | Obtaining samples for iteration 900...
2022-04-23 14:24:25 | [train_policy] epoch #900 | Logging diagnostics...
2022-04-23 14:24:25 | [train_policy] epoch #900 | Optimizing policy...
2022-04-23 14:24:25 | [train_policy] epoch #900 | Computing loss before
2022-04-23 14:24:25 | [train_policy] epoch #900 | Computing KL before
2022-04-23 14:24:25 | [train_policy] epoch #900 | Optimizing
2022-04-23 14:24:25 | [train_policy] epoch #900 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:25 | [train_policy] epoch #900 | computing loss before
2022-04-23 14:24:25 | [train_policy] epoch #900 | computing gradient
2022-04-23 14:24:25 | [train_policy] epoch #900 | gradient computed
2022-04-23 14:24:25 | [train_policy] epoch #900 | computing descent direction
2022-04-23 14:24:26 | [train_policy] epoch #900 | descent direction computed
2022-04-23 14:24:26 | [train_policy] epoch #900 | backtrack iters: 0
2022-04-23 14:24:26 | [train_policy] epoch #900 | optimization finished
2022-04-23 14:24:26 | [train_policy] epoch #900 | Computing KL after
2022-04-23 14:24:26 | [train_policy] epoch #900 | Computing loss after
2022-04-23 14:24:26 | [train_policy] epoch #900 | Fitting baseline...
2022-04-23 14:24:26 | [train_policy] epoch #900 | Saving snapshot...
2022-04-23 14:24:26 | [train_policy] epoch #900 | Saved
2022-04-23 14:24:26 | [train_policy] epoch #900 | Time 313.76 s
2022-04-23 14:24:26 | [train_policy] epoch #900 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.119905
Evaluation/AverageDiscountedReturn          -41.057
Evaluation/AverageReturn                    -41.057
Evaluation/CompletionRate                     0
Evaluation/Iteration                        900
Evaluation/MaxReturn                        -31.2272
Evaluation/MinReturn                        -71.4424
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.61033
Extras/EpisodeRewardMean                    -40.7182
LinearFeatureBaseline/ExplainedVariance       0.857779
PolicyExecTime                                0.0950692
ProcessExecTime                               0.0111935
TotalEnvSteps                            911812
policy/Entropy                               -2.41759
policy/KL                                     0.00999415
policy/KLBefore                               0
policy/LossAfter                             -0.0198599
policy/LossBefore                            -2.23812e-08
policy/Perplexity                             0.0891361
policy/dLoss                                  0.0198599
---------------------------------------  ----------------
2022-04-23 14:24:26 | [train_policy] epoch #901 | Obtaining samples for iteration 901...
2022-04-23 14:24:26 | [train_policy] epoch #901 | Logging diagnostics...
2022-04-23 14:24:26 | [train_policy] epoch #901 | Optimizing policy...
2022-04-23 14:24:26 | [train_policy] epoch #901 | Computing loss before
2022-04-23 14:24:26 | [train_policy] epoch #901 | Computing KL before
2022-04-23 14:24:26 | [train_policy] epoch #901 | Optimizing
2022-04-23 14:24:26 | [train_policy] epoch #901 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:26 | [train_policy] epoch #901 | computing loss before
2022-04-23 14:24:26 | [train_policy] epoch #901 | computing gradient
2022-04-23 14:24:26 | [train_policy] epoch #901 | gradient computed
2022-04-23 14:24:26 | [train_policy] epoch #901 | computing descent direction
2022-04-23 14:24:26 | [train_policy] epoch #901 | descent direction computed
2022-04-23 14:24:26 | [train_policy] epoch #901 | backtrack iters: 0
2022-04-23 14:24:26 | [train_policy] epoch #901 | optimization finished
2022-04-23 14:24:26 | [train_policy] epoch #901 | Computing KL after
2022-04-23 14:24:26 | [train_policy] epoch #901 | Computing loss after
2022-04-23 14:24:26 | [train_policy] epoch #901 | Fitting baseline...
2022-04-23 14:24:26 | [train_policy] epoch #901 | Saving snapshot...
2022-04-23 14:24:26 | [train_policy] epoch #901 | Saved
2022-04-23 14:24:26 | [train_policy] epoch #901 | Time 314.09 s
2022-04-23 14:24:26 | [train_policy] epoch #901 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119643
Evaluation/AverageDiscountedReturn          -39.6853
Evaluation/AverageReturn                    -39.6853
Evaluation/CompletionRate                     0
Evaluation/Iteration                        901
Evaluation/MaxReturn                        -31.2345
Evaluation/MinReturn                        -63.9886
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.49943
Extras/EpisodeRewardMean                    -39.5831
LinearFeatureBaseline/ExplainedVariance       0.888026
PolicyExecTime                                0.0969665
ProcessExecTime                               0.0114164
TotalEnvSteps                            912824
policy/Entropy                               -2.42363
policy/KL                                     0.00974238
policy/KLBefore                               0
policy/LossAfter                             -0.0191666
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.0885995
policy/dLoss                                  0.0191666
---------------------------------------  ----------------
2022-04-23 14:24:26 | [train_policy] epoch #902 | Obtaining samples for iteration 902...
2022-04-23 14:24:26 | [train_policy] epoch #902 | Logging diagnostics...
2022-04-23 14:24:26 | [train_policy] epoch #902 | Optimizing policy...
2022-04-23 14:24:26 | [train_policy] epoch #902 | Computing loss before
2022-04-23 14:24:26 | [train_policy] epoch #902 | Computing KL before
2022-04-23 14:24:26 | [train_policy] epoch #902 | Optimizing
2022-04-23 14:24:26 | [train_policy] epoch #902 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:26 | [train_policy] epoch #902 | computing loss before
2022-04-23 14:24:26 | [train_policy] epoch #902 | computing gradient
2022-04-23 14:24:26 | [train_policy] epoch #902 | gradient computed
2022-04-23 14:24:26 | [train_policy] epoch #902 | computing descent direction
2022-04-23 14:24:26 | [train_policy] epoch #902 | descent direction computed
2022-04-23 14:24:26 | [train_policy] epoch #902 | backtrack iters: 1
2022-04-23 14:24:26 | [train_policy] epoch #902 | optimization finished
2022-04-23 14:24:26 | [train_policy] epoch #902 | Computing KL after
2022-04-23 14:24:26 | [train_policy] epoch #902 | Computing loss after
2022-04-23 14:24:26 | [train_policy] epoch #902 | Fitting baseline...
2022-04-23 14:24:26 | [train_policy] epoch #902 | Saving snapshot...
2022-04-23 14:24:26 | [train_policy] epoch #902 | Saved
2022-04-23 14:24:26 | [train_policy] epoch #902 | Time 314.41 s
2022-04-23 14:24:26 | [train_policy] epoch #902 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119484
Evaluation/AverageDiscountedReturn          -39.3963
Evaluation/AverageReturn                    -39.3963
Evaluation/CompletionRate                     0
Evaluation/Iteration                        902
Evaluation/MaxReturn                        -30.6572
Evaluation/MinReturn                        -59.4728
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.6253
Extras/EpisodeRewardMean                    -39.6106
LinearFeatureBaseline/ExplainedVariance       0.868938
PolicyExecTime                                0.0929642
ProcessExecTime                               0.0111067
TotalEnvSteps                            913836
policy/Entropy                               -2.41893
policy/KL                                     0.00651915
policy/KLBefore                               0
policy/LossAfter                             -0.0145525
policy/LossBefore                             2.96845e-08
policy/Perplexity                             0.089017
policy/dLoss                                  0.0145525
---------------------------------------  ----------------
2022-04-23 14:24:26 | [train_policy] epoch #903 | Obtaining samples for iteration 903...
2022-04-23 14:24:26 | [train_policy] epoch #903 | Logging diagnostics...
2022-04-23 14:24:26 | [train_policy] epoch #903 | Optimizing policy...
2022-04-23 14:24:26 | [train_policy] epoch #903 | Computing loss before
2022-04-23 14:24:26 | [train_policy] epoch #903 | Computing KL before
2022-04-23 14:24:26 | [train_policy] epoch #903 | Optimizing
2022-04-23 14:24:26 | [train_policy] epoch #903 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:26 | [train_policy] epoch #903 | computing loss before
2022-04-23 14:24:26 | [train_policy] epoch #903 | computing gradient
2022-04-23 14:24:26 | [train_policy] epoch #903 | gradient computed
2022-04-23 14:24:26 | [train_policy] epoch #903 | computing descent direction
2022-04-23 14:24:26 | [train_policy] epoch #903 | descent direction computed
2022-04-23 14:24:27 | [train_policy] epoch #903 | backtrack iters: 1
2022-04-23 14:24:27 | [train_policy] epoch #903 | optimization finished
2022-04-23 14:24:27 | [train_policy] epoch #903 | Computing KL after
2022-04-23 14:24:27 | [train_policy] epoch #903 | Computing loss after
2022-04-23 14:24:27 | [train_policy] epoch #903 | Fitting baseline...
2022-04-23 14:24:27 | [train_policy] epoch #903 | Saving snapshot...
2022-04-23 14:24:27 | [train_policy] epoch #903 | Saved
2022-04-23 14:24:27 | [train_policy] epoch #903 | Time 314.73 s
2022-04-23 14:24:27 | [train_policy] epoch #903 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119701
Evaluation/AverageDiscountedReturn          -40.2549
Evaluation/AverageReturn                    -40.2549
Evaluation/CompletionRate                     0
Evaluation/Iteration                        903
Evaluation/MaxReturn                        -30.2481
Evaluation/MinReturn                        -63.6848
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.24768
Extras/EpisodeRewardMean                    -39.921
LinearFeatureBaseline/ExplainedVariance       0.881041
PolicyExecTime                                0.0931578
ProcessExecTime                               0.0112364
TotalEnvSteps                            914848
policy/Entropy                               -2.45629
policy/KL                                     0.00656823
policy/KLBefore                               0
policy/LossAfter                             -0.0102365
policy/LossBefore                            -8.83468e-09
policy/Perplexity                             0.0857525
policy/dLoss                                  0.0102365
---------------------------------------  ----------------
2022-04-23 14:24:27 | [train_policy] epoch #904 | Obtaining samples for iteration 904...
2022-04-23 14:24:27 | [train_policy] epoch #904 | Logging diagnostics...
2022-04-23 14:24:27 | [train_policy] epoch #904 | Optimizing policy...
2022-04-23 14:24:27 | [train_policy] epoch #904 | Computing loss before
2022-04-23 14:24:27 | [train_policy] epoch #904 | Computing KL before
2022-04-23 14:24:27 | [train_policy] epoch #904 | Optimizing
2022-04-23 14:24:27 | [train_policy] epoch #904 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:27 | [train_policy] epoch #904 | computing loss before
2022-04-23 14:24:27 | [train_policy] epoch #904 | computing gradient
2022-04-23 14:24:27 | [train_policy] epoch #904 | gradient computed
2022-04-23 14:24:27 | [train_policy] epoch #904 | computing descent direction
2022-04-23 14:24:27 | [train_policy] epoch #904 | descent direction computed
2022-04-23 14:24:27 | [train_policy] epoch #904 | backtrack iters: 1
2022-04-23 14:24:27 | [train_policy] epoch #904 | optimization finished
2022-04-23 14:24:27 | [train_policy] epoch #904 | Computing KL after
2022-04-23 14:24:27 | [train_policy] epoch #904 | Computing loss after
2022-04-23 14:24:27 | [train_policy] epoch #904 | Fitting baseline...
2022-04-23 14:24:27 | [train_policy] epoch #904 | Saving snapshot...
2022-04-23 14:24:27 | [train_policy] epoch #904 | Saved
2022-04-23 14:24:27 | [train_policy] epoch #904 | Time 315.06 s
2022-04-23 14:24:27 | [train_policy] epoch #904 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119583
Evaluation/AverageDiscountedReturn          -39.6799
Evaluation/AverageReturn                    -39.6799
Evaluation/CompletionRate                     0
Evaluation/Iteration                        904
Evaluation/MaxReturn                        -31.0904
Evaluation/MinReturn                        -63.7295
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.74729
Extras/EpisodeRewardMean                    -39.5101
LinearFeatureBaseline/ExplainedVariance       0.882698
PolicyExecTime                                0.092134
ProcessExecTime                               0.0111353
TotalEnvSteps                            915860
policy/Entropy                               -2.48249
policy/KL                                     0.00706038
policy/KLBefore                               0
policy/LossAfter                             -0.0142383
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.083535
policy/dLoss                                  0.0142383
---------------------------------------  ----------------
2022-04-23 14:24:27 | [train_policy] epoch #905 | Obtaining samples for iteration 905...
2022-04-23 14:24:27 | [train_policy] epoch #905 | Logging diagnostics...
2022-04-23 14:24:27 | [train_policy] epoch #905 | Optimizing policy...
2022-04-23 14:24:27 | [train_policy] epoch #905 | Computing loss before
2022-04-23 14:24:27 | [train_policy] epoch #905 | Computing KL before
2022-04-23 14:24:27 | [train_policy] epoch #905 | Optimizing
2022-04-23 14:24:27 | [train_policy] epoch #905 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:27 | [train_policy] epoch #905 | computing loss before
2022-04-23 14:24:27 | [train_policy] epoch #905 | computing gradient
2022-04-23 14:24:27 | [train_policy] epoch #905 | gradient computed
2022-04-23 14:24:27 | [train_policy] epoch #905 | computing descent direction
2022-04-23 14:24:27 | [train_policy] epoch #905 | descent direction computed
2022-04-23 14:24:27 | [train_policy] epoch #905 | backtrack iters: 1
2022-04-23 14:24:27 | [train_policy] epoch #905 | optimization finished
2022-04-23 14:24:27 | [train_policy] epoch #905 | Computing KL after
2022-04-23 14:24:27 | [train_policy] epoch #905 | Computing loss after
2022-04-23 14:24:27 | [train_policy] epoch #905 | Fitting baseline...
2022-04-23 14:24:27 | [train_policy] epoch #905 | Saving snapshot...
2022-04-23 14:24:27 | [train_policy] epoch #905 | Saved
2022-04-23 14:24:27 | [train_policy] epoch #905 | Time 315.38 s
2022-04-23 14:24:27 | [train_policy] epoch #905 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118761
Evaluation/AverageDiscountedReturn          -40.4752
Evaluation/AverageReturn                    -40.4752
Evaluation/CompletionRate                     0
Evaluation/Iteration                        905
Evaluation/MaxReturn                        -30.5019
Evaluation/MinReturn                        -63.8152
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.87689
Extras/EpisodeRewardMean                    -40.1371
LinearFeatureBaseline/ExplainedVariance       0.876243
PolicyExecTime                                0.093086
ProcessExecTime                               0.0110948
TotalEnvSteps                            916872
policy/Entropy                               -2.50133
policy/KL                                     0.00640663
policy/KLBefore                               0
policy/LossAfter                             -0.0128053
policy/LossBefore                             7.30334e-09
policy/Perplexity                             0.0819758
policy/dLoss                                  0.0128053
---------------------------------------  ----------------
2022-04-23 14:24:27 | [train_policy] epoch #906 | Obtaining samples for iteration 906...
2022-04-23 14:24:27 | [train_policy] epoch #906 | Logging diagnostics...
2022-04-23 14:24:27 | [train_policy] epoch #906 | Optimizing policy...
2022-04-23 14:24:27 | [train_policy] epoch #906 | Computing loss before
2022-04-23 14:24:27 | [train_policy] epoch #906 | Computing KL before
2022-04-23 14:24:27 | [train_policy] epoch #906 | Optimizing
2022-04-23 14:24:27 | [train_policy] epoch #906 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:27 | [train_policy] epoch #906 | computing loss before
2022-04-23 14:24:27 | [train_policy] epoch #906 | computing gradient
2022-04-23 14:24:27 | [train_policy] epoch #906 | gradient computed
2022-04-23 14:24:27 | [train_policy] epoch #906 | computing descent direction
2022-04-23 14:24:27 | [train_policy] epoch #906 | descent direction computed
2022-04-23 14:24:27 | [train_policy] epoch #906 | backtrack iters: 1
2022-04-23 14:24:27 | [train_policy] epoch #906 | optimization finished
2022-04-23 14:24:27 | [train_policy] epoch #906 | Computing KL after
2022-04-23 14:24:27 | [train_policy] epoch #906 | Computing loss after
2022-04-23 14:24:27 | [train_policy] epoch #906 | Fitting baseline...
2022-04-23 14:24:27 | [train_policy] epoch #906 | Saving snapshot...
2022-04-23 14:24:27 | [train_policy] epoch #906 | Saved
2022-04-23 14:24:27 | [train_policy] epoch #906 | Time 315.71 s
2022-04-23 14:24:27 | [train_policy] epoch #906 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.124653
Evaluation/AverageDiscountedReturn          -40.3113
Evaluation/AverageReturn                    -40.3113
Evaluation/CompletionRate                     0
Evaluation/Iteration                        906
Evaluation/MaxReturn                        -31.2399
Evaluation/MinReturn                        -64.1561
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.07078
Extras/EpisodeRewardMean                    -40.3858
LinearFeatureBaseline/ExplainedVariance       0.868519
PolicyExecTime                                0.0969162
ProcessExecTime                               0.0118034
TotalEnvSteps                            917884
policy/Entropy                               -2.50741
policy/KL                                     0.00647237
policy/KLBefore                               0
policy/LossAfter                             -0.0154771
policy/LossBefore                             1.76694e-09
policy/Perplexity                             0.0814789
policy/dLoss                                  0.0154771
---------------------------------------  ----------------
2022-04-23 14:24:28 | [train_policy] epoch #907 | Obtaining samples for iteration 907...
2022-04-23 14:24:28 | [train_policy] epoch #907 | Logging diagnostics...
2022-04-23 14:24:28 | [train_policy] epoch #907 | Optimizing policy...
2022-04-23 14:24:28 | [train_policy] epoch #907 | Computing loss before
2022-04-23 14:24:28 | [train_policy] epoch #907 | Computing KL before
2022-04-23 14:24:28 | [train_policy] epoch #907 | Optimizing
2022-04-23 14:24:28 | [train_policy] epoch #907 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:28 | [train_policy] epoch #907 | computing loss before
2022-04-23 14:24:28 | [train_policy] epoch #907 | computing gradient
2022-04-23 14:24:28 | [train_policy] epoch #907 | gradient computed
2022-04-23 14:24:28 | [train_policy] epoch #907 | computing descent direction
2022-04-23 14:24:28 | [train_policy] epoch #907 | descent direction computed
2022-04-23 14:24:28 | [train_policy] epoch #907 | backtrack iters: 0
2022-04-23 14:24:28 | [train_policy] epoch #907 | optimization finished
2022-04-23 14:24:28 | [train_policy] epoch #907 | Computing KL after
2022-04-23 14:24:28 | [train_policy] epoch #907 | Computing loss after
2022-04-23 14:24:28 | [train_policy] epoch #907 | Fitting baseline...
2022-04-23 14:24:28 | [train_policy] epoch #907 | Saving snapshot...
2022-04-23 14:24:28 | [train_policy] epoch #907 | Saved
2022-04-23 14:24:28 | [train_policy] epoch #907 | Time 316.04 s
2022-04-23 14:24:28 | [train_policy] epoch #907 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120445
Evaluation/AverageDiscountedReturn          -40.7832
Evaluation/AverageReturn                    -40.7832
Evaluation/CompletionRate                     0
Evaluation/Iteration                        907
Evaluation/MaxReturn                        -31.3434
Evaluation/MinReturn                        -63.7146
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.57108
Extras/EpisodeRewardMean                    -40.5605
LinearFeatureBaseline/ExplainedVariance       0.869962
PolicyExecTime                                0.0966222
ProcessExecTime                               0.0114958
TotalEnvSteps                            918896
policy/Entropy                               -2.47295
policy/KL                                     0.00966446
policy/KLBefore                               0
policy/LossAfter                             -0.0162301
policy/LossBefore                             2.63862e-08
policy/Perplexity                             0.0843361
policy/dLoss                                  0.0162301
---------------------------------------  ----------------
2022-04-23 14:24:28 | [train_policy] epoch #908 | Obtaining samples for iteration 908...
2022-04-23 14:24:28 | [train_policy] epoch #908 | Logging diagnostics...
2022-04-23 14:24:28 | [train_policy] epoch #908 | Optimizing policy...
2022-04-23 14:24:28 | [train_policy] epoch #908 | Computing loss before
2022-04-23 14:24:28 | [train_policy] epoch #908 | Computing KL before
2022-04-23 14:24:28 | [train_policy] epoch #908 | Optimizing
2022-04-23 14:24:28 | [train_policy] epoch #908 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:28 | [train_policy] epoch #908 | computing loss before
2022-04-23 14:24:28 | [train_policy] epoch #908 | computing gradient
2022-04-23 14:24:28 | [train_policy] epoch #908 | gradient computed
2022-04-23 14:24:28 | [train_policy] epoch #908 | computing descent direction
2022-04-23 14:24:28 | [train_policy] epoch #908 | descent direction computed
2022-04-23 14:24:28 | [train_policy] epoch #908 | backtrack iters: 0
2022-04-23 14:24:28 | [train_policy] epoch #908 | optimization finished
2022-04-23 14:24:28 | [train_policy] epoch #908 | Computing KL after
2022-04-23 14:24:28 | [train_policy] epoch #908 | Computing loss after
2022-04-23 14:24:28 | [train_policy] epoch #908 | Fitting baseline...
2022-04-23 14:24:28 | [train_policy] epoch #908 | Saving snapshot...
2022-04-23 14:24:28 | [train_policy] epoch #908 | Saved
2022-04-23 14:24:28 | [train_policy] epoch #908 | Time 316.37 s
2022-04-23 14:24:28 | [train_policy] epoch #908 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120742
Evaluation/AverageDiscountedReturn          -39.3214
Evaluation/AverageReturn                    -39.3214
Evaluation/CompletionRate                     0
Evaluation/Iteration                        908
Evaluation/MaxReturn                        -31.155
Evaluation/MinReturn                        -71.3771
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.63345
Extras/EpisodeRewardMean                    -39.451
LinearFeatureBaseline/ExplainedVariance       0.860104
PolicyExecTime                                0.0960412
ProcessExecTime                               0.011209
TotalEnvSteps                            919908
policy/Entropy                               -2.50082
policy/KL                                     0.00994115
policy/KLBefore                               0
policy/LossAfter                             -0.0200152
policy/LossBefore                             4.00506e-09
policy/Perplexity                             0.082018
policy/dLoss                                  0.0200152
---------------------------------------  ----------------
2022-04-23 14:24:28 | [train_policy] epoch #909 | Obtaining samples for iteration 909...
2022-04-23 14:24:28 | [train_policy] epoch #909 | Logging diagnostics...
2022-04-23 14:24:28 | [train_policy] epoch #909 | Optimizing policy...
2022-04-23 14:24:28 | [train_policy] epoch #909 | Computing loss before
2022-04-23 14:24:28 | [train_policy] epoch #909 | Computing KL before
2022-04-23 14:24:28 | [train_policy] epoch #909 | Optimizing
2022-04-23 14:24:28 | [train_policy] epoch #909 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:28 | [train_policy] epoch #909 | computing loss before
2022-04-23 14:24:28 | [train_policy] epoch #909 | computing gradient
2022-04-23 14:24:28 | [train_policy] epoch #909 | gradient computed
2022-04-23 14:24:28 | [train_policy] epoch #909 | computing descent direction
2022-04-23 14:24:28 | [train_policy] epoch #909 | descent direction computed
2022-04-23 14:24:28 | [train_policy] epoch #909 | backtrack iters: 0
2022-04-23 14:24:28 | [train_policy] epoch #909 | optimization finished
2022-04-23 14:24:28 | [train_policy] epoch #909 | Computing KL after
2022-04-23 14:24:28 | [train_policy] epoch #909 | Computing loss after
2022-04-23 14:24:28 | [train_policy] epoch #909 | Fitting baseline...
2022-04-23 14:24:28 | [train_policy] epoch #909 | Saving snapshot...
2022-04-23 14:24:28 | [train_policy] epoch #909 | Saved
2022-04-23 14:24:28 | [train_policy] epoch #909 | Time 316.69 s
2022-04-23 14:24:28 | [train_policy] epoch #909 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119849
Evaluation/AverageDiscountedReturn          -38.9095
Evaluation/AverageReturn                    -38.9095
Evaluation/CompletionRate                     0
Evaluation/Iteration                        909
Evaluation/MaxReturn                        -30.2663
Evaluation/MinReturn                        -64.1713
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.2066
Extras/EpisodeRewardMean                    -38.9384
LinearFeatureBaseline/ExplainedVariance       0.885888
PolicyExecTime                                0.0943422
ProcessExecTime                               0.0111766
TotalEnvSteps                            920920
policy/Entropy                               -2.47886
policy/KL                                     0.00964188
policy/KLBefore                               0
policy/LossAfter                             -0.016257
policy/LossBefore                            -1.01304e-08
policy/Perplexity                             0.0838384
policy/dLoss                                  0.016257
---------------------------------------  ----------------
2022-04-23 14:24:28 | [train_policy] epoch #910 | Obtaining samples for iteration 910...
2022-04-23 14:24:29 | [train_policy] epoch #910 | Logging diagnostics...
2022-04-23 14:24:29 | [train_policy] epoch #910 | Optimizing policy...
2022-04-23 14:24:29 | [train_policy] epoch #910 | Computing loss before
2022-04-23 14:24:29 | [train_policy] epoch #910 | Computing KL before
2022-04-23 14:24:29 | [train_policy] epoch #910 | Optimizing
2022-04-23 14:24:29 | [train_policy] epoch #910 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:29 | [train_policy] epoch #910 | computing loss before
2022-04-23 14:24:29 | [train_policy] epoch #910 | computing gradient
2022-04-23 14:24:29 | [train_policy] epoch #910 | gradient computed
2022-04-23 14:24:29 | [train_policy] epoch #910 | computing descent direction
2022-04-23 14:24:29 | [train_policy] epoch #910 | descent direction computed
2022-04-23 14:24:29 | [train_policy] epoch #910 | backtrack iters: 1
2022-04-23 14:24:29 | [train_policy] epoch #910 | optimization finished
2022-04-23 14:24:29 | [train_policy] epoch #910 | Computing KL after
2022-04-23 14:24:29 | [train_policy] epoch #910 | Computing loss after
2022-04-23 14:24:29 | [train_policy] epoch #910 | Fitting baseline...
2022-04-23 14:24:29 | [train_policy] epoch #910 | Saving snapshot...
2022-04-23 14:24:29 | [train_policy] epoch #910 | Saved
2022-04-23 14:24:29 | [train_policy] epoch #910 | Time 317.01 s
2022-04-23 14:24:29 | [train_policy] epoch #910 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119582
Evaluation/AverageDiscountedReturn          -41.5968
Evaluation/AverageReturn                    -41.5968
Evaluation/CompletionRate                     0
Evaluation/Iteration                        910
Evaluation/MaxReturn                        -30.3791
Evaluation/MinReturn                        -71.2899
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.98273
Extras/EpisodeRewardMean                    -41.5159
LinearFeatureBaseline/ExplainedVariance       0.83367
PolicyExecTime                                0.0956779
ProcessExecTime                               0.0111203
TotalEnvSteps                            921932
policy/Entropy                               -2.47119
policy/KL                                     0.00743082
policy/KLBefore                               0
policy/LossAfter                             -0.0148827
policy/LossBefore                             3.48675e-08
policy/Perplexity                             0.0844844
policy/dLoss                                  0.0148827
---------------------------------------  ----------------
2022-04-23 14:24:29 | [train_policy] epoch #911 | Obtaining samples for iteration 911...
2022-04-23 14:24:29 | [train_policy] epoch #911 | Logging diagnostics...
2022-04-23 14:24:29 | [train_policy] epoch #911 | Optimizing policy...
2022-04-23 14:24:29 | [train_policy] epoch #911 | Computing loss before
2022-04-23 14:24:29 | [train_policy] epoch #911 | Computing KL before
2022-04-23 14:24:29 | [train_policy] epoch #911 | Optimizing
2022-04-23 14:24:29 | [train_policy] epoch #911 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:29 | [train_policy] epoch #911 | computing loss before
2022-04-23 14:24:29 | [train_policy] epoch #911 | computing gradient
2022-04-23 14:24:29 | [train_policy] epoch #911 | gradient computed
2022-04-23 14:24:29 | [train_policy] epoch #911 | computing descent direction
2022-04-23 14:24:29 | [train_policy] epoch #911 | descent direction computed
2022-04-23 14:24:29 | [train_policy] epoch #911 | backtrack iters: 0
2022-04-23 14:24:29 | [train_policy] epoch #911 | optimization finished
2022-04-23 14:24:29 | [train_policy] epoch #911 | Computing KL after
2022-04-23 14:24:29 | [train_policy] epoch #911 | Computing loss after
2022-04-23 14:24:29 | [train_policy] epoch #911 | Fitting baseline...
2022-04-23 14:24:29 | [train_policy] epoch #911 | Saving snapshot...
2022-04-23 14:24:29 | [train_policy] epoch #911 | Saved
2022-04-23 14:24:29 | [train_policy] epoch #911 | Time 317.33 s
2022-04-23 14:24:29 | [train_policy] epoch #911 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119385
Evaluation/AverageDiscountedReturn          -39.8893
Evaluation/AverageReturn                    -39.8893
Evaluation/CompletionRate                     0
Evaluation/Iteration                        911
Evaluation/MaxReturn                        -30.7256
Evaluation/MinReturn                        -71.3138
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.34167
Extras/EpisodeRewardMean                    -40.1564
LinearFeatureBaseline/ExplainedVariance       0.823213
PolicyExecTime                                0.0938206
ProcessExecTime                               0.0111008
TotalEnvSteps                            922944
policy/Entropy                               -2.44326
policy/KL                                     0.00998049
policy/KLBefore                               0
policy/LossAfter                             -0.0178246
policy/LossBefore                            -9.18807e-09
policy/Perplexity                             0.0868772
policy/dLoss                                  0.0178245
---------------------------------------  ----------------
2022-04-23 14:24:29 | [train_policy] epoch #912 | Obtaining samples for iteration 912...
2022-04-23 14:24:29 | [train_policy] epoch #912 | Logging diagnostics...
2022-04-23 14:24:29 | [train_policy] epoch #912 | Optimizing policy...
2022-04-23 14:24:29 | [train_policy] epoch #912 | Computing loss before
2022-04-23 14:24:29 | [train_policy] epoch #912 | Computing KL before
2022-04-23 14:24:29 | [train_policy] epoch #912 | Optimizing
2022-04-23 14:24:29 | [train_policy] epoch #912 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:29 | [train_policy] epoch #912 | computing loss before
2022-04-23 14:24:29 | [train_policy] epoch #912 | computing gradient
2022-04-23 14:24:29 | [train_policy] epoch #912 | gradient computed
2022-04-23 14:24:29 | [train_policy] epoch #912 | computing descent direction
2022-04-23 14:24:29 | [train_policy] epoch #912 | descent direction computed
2022-04-23 14:24:29 | [train_policy] epoch #912 | backtrack iters: 0
2022-04-23 14:24:29 | [train_policy] epoch #912 | optimization finished
2022-04-23 14:24:29 | [train_policy] epoch #912 | Computing KL after
2022-04-23 14:24:29 | [train_policy] epoch #912 | Computing loss after
2022-04-23 14:24:29 | [train_policy] epoch #912 | Fitting baseline...
2022-04-23 14:24:29 | [train_policy] epoch #912 | Saving snapshot...
2022-04-23 14:24:29 | [train_policy] epoch #912 | Saved
2022-04-23 14:24:29 | [train_policy] epoch #912 | Time 317.66 s
2022-04-23 14:24:29 | [train_policy] epoch #912 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.120231
Evaluation/AverageDiscountedReturn          -40.0906
Evaluation/AverageReturn                    -40.0906
Evaluation/CompletionRate                     0
Evaluation/Iteration                        912
Evaluation/MaxReturn                        -30.6918
Evaluation/MinReturn                        -63.8132
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.31611
Extras/EpisodeRewardMean                    -39.8866
LinearFeatureBaseline/ExplainedVariance       0.902595
PolicyExecTime                                0.0938079
ProcessExecTime                               0.0112293
TotalEnvSteps                            923956
policy/Entropy                               -2.43427
policy/KL                                     0.009905
policy/KLBefore                               0
policy/LossAfter                             -0.0151086
policy/LossBefore                             1.7905e-08
policy/Perplexity                             0.0876614
policy/dLoss                                  0.0151086
---------------------------------------  ---------------
2022-04-23 14:24:29 | [train_policy] epoch #913 | Obtaining samples for iteration 913...
2022-04-23 14:24:30 | [train_policy] epoch #913 | Logging diagnostics...
2022-04-23 14:24:30 | [train_policy] epoch #913 | Optimizing policy...
2022-04-23 14:24:30 | [train_policy] epoch #913 | Computing loss before
2022-04-23 14:24:30 | [train_policy] epoch #913 | Computing KL before
2022-04-23 14:24:30 | [train_policy] epoch #913 | Optimizing
2022-04-23 14:24:30 | [train_policy] epoch #913 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:30 | [train_policy] epoch #913 | computing loss before
2022-04-23 14:24:30 | [train_policy] epoch #913 | computing gradient
2022-04-23 14:24:30 | [train_policy] epoch #913 | gradient computed
2022-04-23 14:24:30 | [train_policy] epoch #913 | computing descent direction
2022-04-23 14:24:30 | [train_policy] epoch #913 | descent direction computed
2022-04-23 14:24:30 | [train_policy] epoch #913 | backtrack iters: 0
2022-04-23 14:24:30 | [train_policy] epoch #913 | optimization finished
2022-04-23 14:24:30 | [train_policy] epoch #913 | Computing KL after
2022-04-23 14:24:30 | [train_policy] epoch #913 | Computing loss after
2022-04-23 14:24:30 | [train_policy] epoch #913 | Fitting baseline...
2022-04-23 14:24:30 | [train_policy] epoch #913 | Saving snapshot...
2022-04-23 14:24:30 | [train_policy] epoch #913 | Saved
2022-04-23 14:24:30 | [train_policy] epoch #913 | Time 317.98 s
2022-04-23 14:24:30 | [train_policy] epoch #913 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119208
Evaluation/AverageDiscountedReturn          -41.5143
Evaluation/AverageReturn                    -41.5143
Evaluation/CompletionRate                     0
Evaluation/Iteration                        913
Evaluation/MaxReturn                        -32.188
Evaluation/MinReturn                        -71.496
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.25921
Extras/EpisodeRewardMean                    -41.2797
LinearFeatureBaseline/ExplainedVariance       0.849797
PolicyExecTime                                0.0936034
ProcessExecTime                               0.0111172
TotalEnvSteps                            924968
policy/Entropy                               -2.45491
policy/KL                                     0.0098788
policy/KLBefore                               0
policy/LossAfter                             -0.0196103
policy/LossBefore                             1.18974e-08
policy/Perplexity                             0.0858709
policy/dLoss                                  0.0196103
---------------------------------------  ----------------
2022-04-23 14:24:30 | [train_policy] epoch #914 | Obtaining samples for iteration 914...
2022-04-23 14:24:30 | [train_policy] epoch #914 | Logging diagnostics...
2022-04-23 14:24:30 | [train_policy] epoch #914 | Optimizing policy...
2022-04-23 14:24:30 | [train_policy] epoch #914 | Computing loss before
2022-04-23 14:24:30 | [train_policy] epoch #914 | Computing KL before
2022-04-23 14:24:30 | [train_policy] epoch #914 | Optimizing
2022-04-23 14:24:30 | [train_policy] epoch #914 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:30 | [train_policy] epoch #914 | computing loss before
2022-04-23 14:24:30 | [train_policy] epoch #914 | computing gradient
2022-04-23 14:24:30 | [train_policy] epoch #914 | gradient computed
2022-04-23 14:24:30 | [train_policy] epoch #914 | computing descent direction
2022-04-23 14:24:30 | [train_policy] epoch #914 | descent direction computed
2022-04-23 14:24:30 | [train_policy] epoch #914 | backtrack iters: 0
2022-04-23 14:24:30 | [train_policy] epoch #914 | optimization finished
2022-04-23 14:24:30 | [train_policy] epoch #914 | Computing KL after
2022-04-23 14:24:30 | [train_policy] epoch #914 | Computing loss after
2022-04-23 14:24:30 | [train_policy] epoch #914 | Fitting baseline...
2022-04-23 14:24:30 | [train_policy] epoch #914 | Saving snapshot...
2022-04-23 14:24:30 | [train_policy] epoch #914 | Saved
2022-04-23 14:24:30 | [train_policy] epoch #914 | Time 318.30 s
2022-04-23 14:24:30 | [train_policy] epoch #914 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118696
Evaluation/AverageDiscountedReturn          -40.4598
Evaluation/AverageReturn                    -40.4598
Evaluation/CompletionRate                     0
Evaluation/Iteration                        914
Evaluation/MaxReturn                        -30.4738
Evaluation/MinReturn                        -64.2016
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.81813
Extras/EpisodeRewardMean                    -40.5764
LinearFeatureBaseline/ExplainedVariance       0.825878
PolicyExecTime                                0.0937769
ProcessExecTime                               0.0111217
TotalEnvSteps                            925980
policy/Entropy                               -2.44828
policy/KL                                     0.00996289
policy/KLBefore                               0
policy/LossAfter                             -0.0350863
policy/LossBefore                             1.31931e-08
policy/Perplexity                             0.0864421
policy/dLoss                                  0.0350863
---------------------------------------  ----------------
2022-04-23 14:24:30 | [train_policy] epoch #915 | Obtaining samples for iteration 915...
2022-04-23 14:24:30 | [train_policy] epoch #915 | Logging diagnostics...
2022-04-23 14:24:30 | [train_policy] epoch #915 | Optimizing policy...
2022-04-23 14:24:30 | [train_policy] epoch #915 | Computing loss before
2022-04-23 14:24:30 | [train_policy] epoch #915 | Computing KL before
2022-04-23 14:24:30 | [train_policy] epoch #915 | Optimizing
2022-04-23 14:24:30 | [train_policy] epoch #915 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:30 | [train_policy] epoch #915 | computing loss before
2022-04-23 14:24:30 | [train_policy] epoch #915 | computing gradient
2022-04-23 14:24:30 | [train_policy] epoch #915 | gradient computed
2022-04-23 14:24:30 | [train_policy] epoch #915 | computing descent direction
2022-04-23 14:24:30 | [train_policy] epoch #915 | descent direction computed
2022-04-23 14:24:30 | [train_policy] epoch #915 | backtrack iters: 1
2022-04-23 14:24:30 | [train_policy] epoch #915 | optimization finished
2022-04-23 14:24:30 | [train_policy] epoch #915 | Computing KL after
2022-04-23 14:24:30 | [train_policy] epoch #915 | Computing loss after
2022-04-23 14:24:30 | [train_policy] epoch #915 | Fitting baseline...
2022-04-23 14:24:30 | [train_policy] epoch #915 | Saving snapshot...
2022-04-23 14:24:30 | [train_policy] epoch #915 | Saved
2022-04-23 14:24:30 | [train_policy] epoch #915 | Time 318.63 s
2022-04-23 14:24:30 | [train_policy] epoch #915 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118734
Evaluation/AverageDiscountedReturn          -41.3257
Evaluation/AverageReturn                    -41.3257
Evaluation/CompletionRate                     0
Evaluation/Iteration                        915
Evaluation/MaxReturn                        -30.6526
Evaluation/MinReturn                        -65.0184
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.18758
Extras/EpisodeRewardMean                    -41.4167
LinearFeatureBaseline/ExplainedVariance       0.882378
PolicyExecTime                                0.093116
ProcessExecTime                               0.0111468
TotalEnvSteps                            926992
policy/Entropy                               -2.47907
policy/KL                                     0.00659498
policy/KLBefore                               0
policy/LossAfter                             -0.0109107
policy/LossBefore                             6.12538e-09
policy/Perplexity                             0.0838212
policy/dLoss                                  0.0109107
---------------------------------------  ----------------
2022-04-23 14:24:30 | [train_policy] epoch #916 | Obtaining samples for iteration 916...
2022-04-23 14:24:31 | [train_policy] epoch #916 | Logging diagnostics...
2022-04-23 14:24:31 | [train_policy] epoch #916 | Optimizing policy...
2022-04-23 14:24:31 | [train_policy] epoch #916 | Computing loss before
2022-04-23 14:24:31 | [train_policy] epoch #916 | Computing KL before
2022-04-23 14:24:31 | [train_policy] epoch #916 | Optimizing
2022-04-23 14:24:31 | [train_policy] epoch #916 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:31 | [train_policy] epoch #916 | computing loss before
2022-04-23 14:24:31 | [train_policy] epoch #916 | computing gradient
2022-04-23 14:24:31 | [train_policy] epoch #916 | gradient computed
2022-04-23 14:24:31 | [train_policy] epoch #916 | computing descent direction
2022-04-23 14:24:31 | [train_policy] epoch #916 | descent direction computed
2022-04-23 14:24:31 | [train_policy] epoch #916 | backtrack iters: 1
2022-04-23 14:24:31 | [train_policy] epoch #916 | optimization finished
2022-04-23 14:24:31 | [train_policy] epoch #916 | Computing KL after
2022-04-23 14:24:31 | [train_policy] epoch #916 | Computing loss after
2022-04-23 14:24:31 | [train_policy] epoch #916 | Fitting baseline...
2022-04-23 14:24:31 | [train_policy] epoch #916 | Saving snapshot...
2022-04-23 14:24:31 | [train_policy] epoch #916 | Saved
2022-04-23 14:24:31 | [train_policy] epoch #916 | Time 318.97 s
2022-04-23 14:24:31 | [train_policy] epoch #916 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.119063
Evaluation/AverageDiscountedReturn          -40.4324
Evaluation/AverageReturn                    -40.4324
Evaluation/CompletionRate                     0
Evaluation/Iteration                        916
Evaluation/MaxReturn                        -30.5993
Evaluation/MinReturn                        -64.2134
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.41582
Extras/EpisodeRewardMean                    -40.4545
LinearFeatureBaseline/ExplainedVariance       0.849607
PolicyExecTime                                0.0984046
ProcessExecTime                               0.0114458
TotalEnvSteps                            928004
policy/Entropy                               -2.48419
policy/KL                                     0.00648786
policy/KLBefore                               0
policy/LossAfter                             -0.0190632
policy/LossBefore                             2.59151e-09
policy/Perplexity                             0.0833931
policy/dLoss                                  0.0190632
---------------------------------------  ----------------
2022-04-23 14:24:31 | [train_policy] epoch #917 | Obtaining samples for iteration 917...
2022-04-23 14:24:31 | [train_policy] epoch #917 | Logging diagnostics...
2022-04-23 14:24:31 | [train_policy] epoch #917 | Optimizing policy...
2022-04-23 14:24:31 | [train_policy] epoch #917 | Computing loss before
2022-04-23 14:24:31 | [train_policy] epoch #917 | Computing KL before
2022-04-23 14:24:31 | [train_policy] epoch #917 | Optimizing
2022-04-23 14:24:31 | [train_policy] epoch #917 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:31 | [train_policy] epoch #917 | computing loss before
2022-04-23 14:24:31 | [train_policy] epoch #917 | computing gradient
2022-04-23 14:24:31 | [train_policy] epoch #917 | gradient computed
2022-04-23 14:24:31 | [train_policy] epoch #917 | computing descent direction
2022-04-23 14:24:31 | [train_policy] epoch #917 | descent direction computed
2022-04-23 14:24:31 | [train_policy] epoch #917 | backtrack iters: 1
2022-04-23 14:24:31 | [train_policy] epoch #917 | optimization finished
2022-04-23 14:24:31 | [train_policy] epoch #917 | Computing KL after
2022-04-23 14:24:31 | [train_policy] epoch #917 | Computing loss after
2022-04-23 14:24:31 | [train_policy] epoch #917 | Fitting baseline...
2022-04-23 14:24:31 | [train_policy] epoch #917 | Saving snapshot...
2022-04-23 14:24:31 | [train_policy] epoch #917 | Saved
2022-04-23 14:24:31 | [train_policy] epoch #917 | Time 319.31 s
2022-04-23 14:24:31 | [train_policy] epoch #917 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119734
Evaluation/AverageDiscountedReturn          -41.0335
Evaluation/AverageReturn                    -41.0335
Evaluation/CompletionRate                     0
Evaluation/Iteration                        917
Evaluation/MaxReturn                        -30.328
Evaluation/MinReturn                        -71.3528
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.83315
Extras/EpisodeRewardMean                    -40.5819
LinearFeatureBaseline/ExplainedVariance       0.858514
PolicyExecTime                                0.0980597
ProcessExecTime                               0.0113785
TotalEnvSteps                            929016
policy/Entropy                               -2.49338
policy/KL                                     0.00647398
policy/KLBefore                               0
policy/LossAfter                             -0.0206558
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.0826299
policy/dLoss                                  0.0206558
---------------------------------------  ----------------
2022-04-23 14:24:31 | [train_policy] epoch #918 | Obtaining samples for iteration 918...
2022-04-23 14:24:31 | [train_policy] epoch #918 | Logging diagnostics...
2022-04-23 14:24:31 | [train_policy] epoch #918 | Optimizing policy...
2022-04-23 14:24:31 | [train_policy] epoch #918 | Computing loss before
2022-04-23 14:24:31 | [train_policy] epoch #918 | Computing KL before
2022-04-23 14:24:31 | [train_policy] epoch #918 | Optimizing
2022-04-23 14:24:31 | [train_policy] epoch #918 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:31 | [train_policy] epoch #918 | computing loss before
2022-04-23 14:24:31 | [train_policy] epoch #918 | computing gradient
2022-04-23 14:24:31 | [train_policy] epoch #918 | gradient computed
2022-04-23 14:24:31 | [train_policy] epoch #918 | computing descent direction
2022-04-23 14:24:31 | [train_policy] epoch #918 | descent direction computed
2022-04-23 14:24:31 | [train_policy] epoch #918 | backtrack iters: 0
2022-04-23 14:24:31 | [train_policy] epoch #918 | optimization finished
2022-04-23 14:24:31 | [train_policy] epoch #918 | Computing KL after
2022-04-23 14:24:31 | [train_policy] epoch #918 | Computing loss after
2022-04-23 14:24:31 | [train_policy] epoch #918 | Fitting baseline...
2022-04-23 14:24:31 | [train_policy] epoch #918 | Saving snapshot...
2022-04-23 14:24:31 | [train_policy] epoch #918 | Saved
2022-04-23 14:24:31 | [train_policy] epoch #918 | Time 319.64 s
2022-04-23 14:24:31 | [train_policy] epoch #918 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118679
Evaluation/AverageDiscountedReturn          -39.9847
Evaluation/AverageReturn                    -39.9847
Evaluation/CompletionRate                     0
Evaluation/Iteration                        918
Evaluation/MaxReturn                        -30.9064
Evaluation/MinReturn                        -64.1913
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.91414
Extras/EpisodeRewardMean                    -40.3134
LinearFeatureBaseline/ExplainedVariance       0.817006
PolicyExecTime                                0.0967987
ProcessExecTime                               0.0111856
TotalEnvSteps                            930028
policy/Entropy                               -2.47899
policy/KL                                     0.00999351
policy/KLBefore                               0
policy/LossAfter                             -0.0350119
policy/LossBefore                            -2.35591e-10
policy/Perplexity                             0.0838279
policy/dLoss                                  0.0350119
---------------------------------------  ----------------
2022-04-23 14:24:31 | [train_policy] epoch #919 | Obtaining samples for iteration 919...
2022-04-23 14:24:32 | [train_policy] epoch #919 | Logging diagnostics...
2022-04-23 14:24:32 | [train_policy] epoch #919 | Optimizing policy...
2022-04-23 14:24:32 | [train_policy] epoch #919 | Computing loss before
2022-04-23 14:24:32 | [train_policy] epoch #919 | Computing KL before
2022-04-23 14:24:32 | [train_policy] epoch #919 | Optimizing
2022-04-23 14:24:32 | [train_policy] epoch #919 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:32 | [train_policy] epoch #919 | computing loss before
2022-04-23 14:24:32 | [train_policy] epoch #919 | computing gradient
2022-04-23 14:24:32 | [train_policy] epoch #919 | gradient computed
2022-04-23 14:24:32 | [train_policy] epoch #919 | computing descent direction
2022-04-23 14:24:32 | [train_policy] epoch #919 | descent direction computed
2022-04-23 14:24:32 | [train_policy] epoch #919 | backtrack iters: 1
2022-04-23 14:24:32 | [train_policy] epoch #919 | optimization finished
2022-04-23 14:24:32 | [train_policy] epoch #919 | Computing KL after
2022-04-23 14:24:32 | [train_policy] epoch #919 | Computing loss after
2022-04-23 14:24:32 | [train_policy] epoch #919 | Fitting baseline...
2022-04-23 14:24:32 | [train_policy] epoch #919 | Saving snapshot...
2022-04-23 14:24:32 | [train_policy] epoch #919 | Saved
2022-04-23 14:24:32 | [train_policy] epoch #919 | Time 319.97 s
2022-04-23 14:24:32 | [train_policy] epoch #919 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118753
Evaluation/AverageDiscountedReturn          -39.4891
Evaluation/AverageReturn                    -39.4891
Evaluation/CompletionRate                     0
Evaluation/Iteration                        919
Evaluation/MaxReturn                        -30.6142
Evaluation/MinReturn                        -71.2853
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.64125
Extras/EpisodeRewardMean                    -39.4312
LinearFeatureBaseline/ExplainedVariance       0.869132
PolicyExecTime                                0.0937824
ProcessExecTime                               0.0112727
TotalEnvSteps                            931040
policy/Entropy                               -2.46537
policy/KL                                     0.00639995
policy/KLBefore                               0
policy/LossAfter                             -0.0166595
policy/LossBefore                             2.35591e-10
policy/Perplexity                             0.0849774
policy/dLoss                                  0.0166595
---------------------------------------  ----------------
2022-04-23 14:24:32 | [train_policy] epoch #920 | Obtaining samples for iteration 920...
2022-04-23 14:24:32 | [train_policy] epoch #920 | Logging diagnostics...
2022-04-23 14:24:32 | [train_policy] epoch #920 | Optimizing policy...
2022-04-23 14:24:32 | [train_policy] epoch #920 | Computing loss before
2022-04-23 14:24:32 | [train_policy] epoch #920 | Computing KL before
2022-04-23 14:24:32 | [train_policy] epoch #920 | Optimizing
2022-04-23 14:24:32 | [train_policy] epoch #920 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:32 | [train_policy] epoch #920 | computing loss before
2022-04-23 14:24:32 | [train_policy] epoch #920 | computing gradient
2022-04-23 14:24:32 | [train_policy] epoch #920 | gradient computed
2022-04-23 14:24:32 | [train_policy] epoch #920 | computing descent direction
2022-04-23 14:24:32 | [train_policy] epoch #920 | descent direction computed
2022-04-23 14:24:32 | [train_policy] epoch #920 | backtrack iters: 1
2022-04-23 14:24:32 | [train_policy] epoch #920 | optimization finished
2022-04-23 14:24:32 | [train_policy] epoch #920 | Computing KL after
2022-04-23 14:24:32 | [train_policy] epoch #920 | Computing loss after
2022-04-23 14:24:32 | [train_policy] epoch #920 | Fitting baseline...
2022-04-23 14:24:32 | [train_policy] epoch #920 | Saving snapshot...
2022-04-23 14:24:32 | [train_policy] epoch #920 | Saved
2022-04-23 14:24:32 | [train_policy] epoch #920 | Time 320.29 s
2022-04-23 14:24:32 | [train_policy] epoch #920 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118406
Evaluation/AverageDiscountedReturn          -40.6357
Evaluation/AverageReturn                    -40.6357
Evaluation/CompletionRate                     0
Evaluation/Iteration                        920
Evaluation/MaxReturn                        -29.9263
Evaluation/MinReturn                        -64.448
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.88097
Extras/EpisodeRewardMean                    -40.3458
LinearFeatureBaseline/ExplainedVariance       0.81854
PolicyExecTime                                0.0950134
ProcessExecTime                               0.0111289
TotalEnvSteps                            932052
policy/Entropy                               -2.46066
policy/KL                                     0.00643851
policy/KLBefore                               0
policy/LossAfter                             -0.0239086
policy/LossBefore                            -1.17796e-09
policy/Perplexity                             0.0853785
policy/dLoss                                  0.0239086
---------------------------------------  ----------------
2022-04-23 14:24:32 | [train_policy] epoch #921 | Obtaining samples for iteration 921...
2022-04-23 14:24:32 | [train_policy] epoch #921 | Logging diagnostics...
2022-04-23 14:24:32 | [train_policy] epoch #921 | Optimizing policy...
2022-04-23 14:24:32 | [train_policy] epoch #921 | Computing loss before
2022-04-23 14:24:32 | [train_policy] epoch #921 | Computing KL before
2022-04-23 14:24:32 | [train_policy] epoch #921 | Optimizing
2022-04-23 14:24:32 | [train_policy] epoch #921 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:32 | [train_policy] epoch #921 | computing loss before
2022-04-23 14:24:32 | [train_policy] epoch #921 | computing gradient
2022-04-23 14:24:32 | [train_policy] epoch #921 | gradient computed
2022-04-23 14:24:32 | [train_policy] epoch #921 | computing descent direction
2022-04-23 14:24:32 | [train_policy] epoch #921 | descent direction computed
2022-04-23 14:24:32 | [train_policy] epoch #921 | backtrack iters: 1
2022-04-23 14:24:32 | [train_policy] epoch #921 | optimization finished
2022-04-23 14:24:32 | [train_policy] epoch #921 | Computing KL after
2022-04-23 14:24:32 | [train_policy] epoch #921 | Computing loss after
2022-04-23 14:24:32 | [train_policy] epoch #921 | Fitting baseline...
2022-04-23 14:24:32 | [train_policy] epoch #921 | Saving snapshot...
2022-04-23 14:24:32 | [train_policy] epoch #921 | Saved
2022-04-23 14:24:32 | [train_policy] epoch #921 | Time 320.62 s
2022-04-23 14:24:32 | [train_policy] epoch #921 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.117893
Evaluation/AverageDiscountedReturn          -40.3062
Evaluation/AverageReturn                    -40.3062
Evaluation/CompletionRate                     0
Evaluation/Iteration                        921
Evaluation/MaxReturn                        -29.6901
Evaluation/MinReturn                        -65.1951
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.20016
Extras/EpisodeRewardMean                    -40.2683
LinearFeatureBaseline/ExplainedVariance       0.86339
PolicyExecTime                                0.093462
ProcessExecTime                               0.0111022
TotalEnvSteps                            933064
policy/Entropy                               -2.47001
policy/KL                                     0.00652237
policy/KLBefore                               0
policy/LossAfter                             -0.0131186
policy/LossBefore                            -7.71562e-09
policy/Perplexity                             0.0845843
policy/dLoss                                  0.0131186
---------------------------------------  ----------------
2022-04-23 14:24:32 | [train_policy] epoch #922 | Obtaining samples for iteration 922...
2022-04-23 14:24:33 | [train_policy] epoch #922 | Logging diagnostics...
2022-04-23 14:24:33 | [train_policy] epoch #922 | Optimizing policy...
2022-04-23 14:24:33 | [train_policy] epoch #922 | Computing loss before
2022-04-23 14:24:33 | [train_policy] epoch #922 | Computing KL before
2022-04-23 14:24:33 | [train_policy] epoch #922 | Optimizing
2022-04-23 14:24:33 | [train_policy] epoch #922 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:33 | [train_policy] epoch #922 | computing loss before
2022-04-23 14:24:33 | [train_policy] epoch #922 | computing gradient
2022-04-23 14:24:33 | [train_policy] epoch #922 | gradient computed
2022-04-23 14:24:33 | [train_policy] epoch #922 | computing descent direction
2022-04-23 14:24:33 | [train_policy] epoch #922 | descent direction computed
2022-04-23 14:24:33 | [train_policy] epoch #922 | backtrack iters: 1
2022-04-23 14:24:33 | [train_policy] epoch #922 | optimization finished
2022-04-23 14:24:33 | [train_policy] epoch #922 | Computing KL after
2022-04-23 14:24:33 | [train_policy] epoch #922 | Computing loss after
2022-04-23 14:24:33 | [train_policy] epoch #922 | Fitting baseline...
2022-04-23 14:24:33 | [train_policy] epoch #922 | Saving snapshot...
2022-04-23 14:24:33 | [train_policy] epoch #922 | Saved
2022-04-23 14:24:33 | [train_policy] epoch #922 | Time 320.94 s
2022-04-23 14:24:33 | [train_policy] epoch #922 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.1187
Evaluation/AverageDiscountedReturn          -41.1009
Evaluation/AverageReturn                    -41.1009
Evaluation/CompletionRate                     0
Evaluation/Iteration                        922
Evaluation/MaxReturn                        -29.4865
Evaluation/MinReturn                        -71.5187
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.91776
Extras/EpisodeRewardMean                    -40.8349
LinearFeatureBaseline/ExplainedVariance       0.872309
PolicyExecTime                                0.0943227
ProcessExecTime                               0.0113375
TotalEnvSteps                            934076
policy/Entropy                               -2.46876
policy/KL                                     0.00653534
policy/KLBefore                               0
policy/LossAfter                             -0.0127271
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.0846898
policy/dLoss                                  0.0127271
---------------------------------------  ----------------
2022-04-23 14:24:33 | [train_policy] epoch #923 | Obtaining samples for iteration 923...
2022-04-23 14:24:33 | [train_policy] epoch #923 | Logging diagnostics...
2022-04-23 14:24:33 | [train_policy] epoch #923 | Optimizing policy...
2022-04-23 14:24:33 | [train_policy] epoch #923 | Computing loss before
2022-04-23 14:24:33 | [train_policy] epoch #923 | Computing KL before
2022-04-23 14:24:33 | [train_policy] epoch #923 | Optimizing
2022-04-23 14:24:33 | [train_policy] epoch #923 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:33 | [train_policy] epoch #923 | computing loss before
2022-04-23 14:24:33 | [train_policy] epoch #923 | computing gradient
2022-04-23 14:24:33 | [train_policy] epoch #923 | gradient computed
2022-04-23 14:24:33 | [train_policy] epoch #923 | computing descent direction
2022-04-23 14:24:33 | [train_policy] epoch #923 | descent direction computed
2022-04-23 14:24:33 | [train_policy] epoch #923 | backtrack iters: 1
2022-04-23 14:24:33 | [train_policy] epoch #923 | optimization finished
2022-04-23 14:24:33 | [train_policy] epoch #923 | Computing KL after
2022-04-23 14:24:33 | [train_policy] epoch #923 | Computing loss after
2022-04-23 14:24:33 | [train_policy] epoch #923 | Fitting baseline...
2022-04-23 14:24:33 | [train_policy] epoch #923 | Saving snapshot...
2022-04-23 14:24:33 | [train_policy] epoch #923 | Saved
2022-04-23 14:24:33 | [train_policy] epoch #923 | Time 321.27 s
2022-04-23 14:24:33 | [train_policy] epoch #923 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119502
Evaluation/AverageDiscountedReturn          -38.3517
Evaluation/AverageReturn                    -38.3517
Evaluation/CompletionRate                     0
Evaluation/Iteration                        923
Evaluation/MaxReturn                        -31.0992
Evaluation/MinReturn                        -64.9084
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.00913
Extras/EpisodeRewardMean                    -38.7778
LinearFeatureBaseline/ExplainedVariance       0.875119
PolicyExecTime                                0.096149
ProcessExecTime                               0.0112488
TotalEnvSteps                            935088
policy/Entropy                               -2.48462
policy/KL                                     0.00668541
policy/KLBefore                               0
policy/LossAfter                             -0.016571
policy/LossBefore                            -4.00506e-09
policy/Perplexity                             0.0833572
policy/dLoss                                  0.016571
---------------------------------------  ----------------
2022-04-23 14:24:33 | [train_policy] epoch #924 | Obtaining samples for iteration 924...
2022-04-23 14:24:33 | [train_policy] epoch #924 | Logging diagnostics...
2022-04-23 14:24:33 | [train_policy] epoch #924 | Optimizing policy...
2022-04-23 14:24:33 | [train_policy] epoch #924 | Computing loss before
2022-04-23 14:24:33 | [train_policy] epoch #924 | Computing KL before
2022-04-23 14:24:33 | [train_policy] epoch #924 | Optimizing
2022-04-23 14:24:33 | [train_policy] epoch #924 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:33 | [train_policy] epoch #924 | computing loss before
2022-04-23 14:24:33 | [train_policy] epoch #924 | computing gradient
2022-04-23 14:24:33 | [train_policy] epoch #924 | gradient computed
2022-04-23 14:24:33 | [train_policy] epoch #924 | computing descent direction
2022-04-23 14:24:33 | [train_policy] epoch #924 | descent direction computed
2022-04-23 14:24:33 | [train_policy] epoch #924 | backtrack iters: 1
2022-04-23 14:24:33 | [train_policy] epoch #924 | optimization finished
2022-04-23 14:24:33 | [train_policy] epoch #924 | Computing KL after
2022-04-23 14:24:33 | [train_policy] epoch #924 | Computing loss after
2022-04-23 14:24:33 | [train_policy] epoch #924 | Fitting baseline...
2022-04-23 14:24:33 | [train_policy] epoch #924 | Saving snapshot...
2022-04-23 14:24:33 | [train_policy] epoch #924 | Saved
2022-04-23 14:24:33 | [train_policy] epoch #924 | Time 321.61 s
2022-04-23 14:24:33 | [train_policy] epoch #924 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.117923
Evaluation/AverageDiscountedReturn          -41.753
Evaluation/AverageReturn                    -41.753
Evaluation/CompletionRate                     0
Evaluation/Iteration                        924
Evaluation/MaxReturn                        -30.2051
Evaluation/MinReturn                        -71.6474
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.71506
Extras/EpisodeRewardMean                    -41.4042
LinearFeatureBaseline/ExplainedVariance       0.849498
PolicyExecTime                                0.0956068
ProcessExecTime                               0.011122
TotalEnvSteps                            936100
policy/Entropy                               -2.51721
policy/KL                                     0.00661299
policy/KLBefore                               0
policy/LossAfter                             -0.0199611
policy/LossBefore                            -7.30334e-09
policy/Perplexity                             0.0806845
policy/dLoss                                  0.0199611
---------------------------------------  ----------------
2022-04-23 14:24:33 | [train_policy] epoch #925 | Obtaining samples for iteration 925...
2022-04-23 14:24:34 | [train_policy] epoch #925 | Logging diagnostics...
2022-04-23 14:24:34 | [train_policy] epoch #925 | Optimizing policy...
2022-04-23 14:24:34 | [train_policy] epoch #925 | Computing loss before
2022-04-23 14:24:34 | [train_policy] epoch #925 | Computing KL before
2022-04-23 14:24:34 | [train_policy] epoch #925 | Optimizing
2022-04-23 14:24:34 | [train_policy] epoch #925 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:34 | [train_policy] epoch #925 | computing loss before
2022-04-23 14:24:34 | [train_policy] epoch #925 | computing gradient
2022-04-23 14:24:34 | [train_policy] epoch #925 | gradient computed
2022-04-23 14:24:34 | [train_policy] epoch #925 | computing descent direction
2022-04-23 14:24:34 | [train_policy] epoch #925 | descent direction computed
2022-04-23 14:24:34 | [train_policy] epoch #925 | backtrack iters: 1
2022-04-23 14:24:34 | [train_policy] epoch #925 | optimization finished
2022-04-23 14:24:34 | [train_policy] epoch #925 | Computing KL after
2022-04-23 14:24:34 | [train_policy] epoch #925 | Computing loss after
2022-04-23 14:24:34 | [train_policy] epoch #925 | Fitting baseline...
2022-04-23 14:24:34 | [train_policy] epoch #925 | Saving snapshot...
2022-04-23 14:24:34 | [train_policy] epoch #925 | Saved
2022-04-23 14:24:34 | [train_policy] epoch #925 | Time 321.93 s
2022-04-23 14:24:34 | [train_policy] epoch #925 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118126
Evaluation/AverageDiscountedReturn          -42.2437
Evaluation/AverageReturn                    -42.2437
Evaluation/CompletionRate                     0
Evaluation/Iteration                        925
Evaluation/MaxReturn                        -29.9206
Evaluation/MinReturn                        -71.7603
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.2821
Extras/EpisodeRewardMean                    -41.9995
LinearFeatureBaseline/ExplainedVariance       0.86465
PolicyExecTime                                0.0923648
ProcessExecTime                               0.0111833
TotalEnvSteps                            937112
policy/Entropy                               -2.51613
policy/KL                                     0.00644286
policy/KLBefore                               0
policy/LossAfter                             -0.0157663
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.0807716
policy/dLoss                                  0.0157663
---------------------------------------  ----------------
2022-04-23 14:24:34 | [train_policy] epoch #926 | Obtaining samples for iteration 926...
2022-04-23 14:24:34 | [train_policy] epoch #926 | Logging diagnostics...
2022-04-23 14:24:34 | [train_policy] epoch #926 | Optimizing policy...
2022-04-23 14:24:34 | [train_policy] epoch #926 | Computing loss before
2022-04-23 14:24:34 | [train_policy] epoch #926 | Computing KL before
2022-04-23 14:24:34 | [train_policy] epoch #926 | Optimizing
2022-04-23 14:24:34 | [train_policy] epoch #926 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:34 | [train_policy] epoch #926 | computing loss before
2022-04-23 14:24:34 | [train_policy] epoch #926 | computing gradient
2022-04-23 14:24:34 | [train_policy] epoch #926 | gradient computed
2022-04-23 14:24:34 | [train_policy] epoch #926 | computing descent direction
2022-04-23 14:24:34 | [train_policy] epoch #926 | descent direction computed
2022-04-23 14:24:34 | [train_policy] epoch #926 | backtrack iters: 1
2022-04-23 14:24:34 | [train_policy] epoch #926 | optimization finished
2022-04-23 14:24:34 | [train_policy] epoch #926 | Computing KL after
2022-04-23 14:24:34 | [train_policy] epoch #926 | Computing loss after
2022-04-23 14:24:34 | [train_policy] epoch #926 | Fitting baseline...
2022-04-23 14:24:34 | [train_policy] epoch #926 | Saving snapshot...
2022-04-23 14:24:34 | [train_policy] epoch #926 | Saved
2022-04-23 14:24:34 | [train_policy] epoch #926 | Time 322.25 s
2022-04-23 14:24:34 | [train_policy] epoch #926 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118839
Evaluation/AverageDiscountedReturn          -40.4701
Evaluation/AverageReturn                    -40.4701
Evaluation/CompletionRate                     0
Evaluation/Iteration                        926
Evaluation/MaxReturn                        -30.2741
Evaluation/MinReturn                        -71.5688
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.35092
Extras/EpisodeRewardMean                    -40.3339
LinearFeatureBaseline/ExplainedVariance       0.880503
PolicyExecTime                                0.0923262
ProcessExecTime                               0.0112
TotalEnvSteps                            938124
policy/Entropy                               -2.52875
policy/KL                                     0.00638651
policy/KLBefore                               0
policy/LossAfter                             -0.016388
policy/LossBefore                             2.08498e-08
policy/Perplexity                             0.0797586
policy/dLoss                                  0.016388
---------------------------------------  ----------------
2022-04-23 14:24:34 | [train_policy] epoch #927 | Obtaining samples for iteration 927...
2022-04-23 14:24:34 | [train_policy] epoch #927 | Logging diagnostics...
2022-04-23 14:24:34 | [train_policy] epoch #927 | Optimizing policy...
2022-04-23 14:24:34 | [train_policy] epoch #927 | Computing loss before
2022-04-23 14:24:34 | [train_policy] epoch #927 | Computing KL before
2022-04-23 14:24:34 | [train_policy] epoch #927 | Optimizing
2022-04-23 14:24:34 | [train_policy] epoch #927 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:34 | [train_policy] epoch #927 | computing loss before
2022-04-23 14:24:34 | [train_policy] epoch #927 | computing gradient
2022-04-23 14:24:34 | [train_policy] epoch #927 | gradient computed
2022-04-23 14:24:34 | [train_policy] epoch #927 | computing descent direction
2022-04-23 14:24:34 | [train_policy] epoch #927 | descent direction computed
2022-04-23 14:24:34 | [train_policy] epoch #927 | backtrack iters: 0
2022-04-23 14:24:34 | [train_policy] epoch #927 | optimization finished
2022-04-23 14:24:34 | [train_policy] epoch #927 | Computing KL after
2022-04-23 14:24:34 | [train_policy] epoch #927 | Computing loss after
2022-04-23 14:24:34 | [train_policy] epoch #927 | Fitting baseline...
2022-04-23 14:24:34 | [train_policy] epoch #927 | Saving snapshot...
2022-04-23 14:24:34 | [train_policy] epoch #927 | Saved
2022-04-23 14:24:34 | [train_policy] epoch #927 | Time 322.57 s
2022-04-23 14:24:34 | [train_policy] epoch #927 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.117462
Evaluation/AverageDiscountedReturn          -41.2077
Evaluation/AverageReturn                    -41.2077
Evaluation/CompletionRate                     0
Evaluation/Iteration                        927
Evaluation/MaxReturn                        -31.6508
Evaluation/MinReturn                        -71.8535
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.04371
Extras/EpisodeRewardMean                    -41.3579
LinearFeatureBaseline/ExplainedVariance       0.826902
PolicyExecTime                                0.0919471
ProcessExecTime                               0.0110252
TotalEnvSteps                            939136
policy/Entropy                               -2.54669
policy/KL                                     0.00997721
policy/KLBefore                               0
policy/LossAfter                             -0.0218059
policy/LossBefore                             1.38999e-08
policy/Perplexity                             0.0783407
policy/dLoss                                  0.0218059
---------------------------------------  ----------------
2022-04-23 14:24:34 | [train_policy] epoch #928 | Obtaining samples for iteration 928...
2022-04-23 14:24:35 | [train_policy] epoch #928 | Logging diagnostics...
2022-04-23 14:24:35 | [train_policy] epoch #928 | Optimizing policy...
2022-04-23 14:24:35 | [train_policy] epoch #928 | Computing loss before
2022-04-23 14:24:35 | [train_policy] epoch #928 | Computing KL before
2022-04-23 14:24:35 | [train_policy] epoch #928 | Optimizing
2022-04-23 14:24:35 | [train_policy] epoch #928 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:35 | [train_policy] epoch #928 | computing loss before
2022-04-23 14:24:35 | [train_policy] epoch #928 | computing gradient
2022-04-23 14:24:35 | [train_policy] epoch #928 | gradient computed
2022-04-23 14:24:35 | [train_policy] epoch #928 | computing descent direction
2022-04-23 14:24:35 | [train_policy] epoch #928 | descent direction computed
2022-04-23 14:24:35 | [train_policy] epoch #928 | backtrack iters: 0
2022-04-23 14:24:35 | [train_policy] epoch #928 | optimization finished
2022-04-23 14:24:35 | [train_policy] epoch #928 | Computing KL after
2022-04-23 14:24:35 | [train_policy] epoch #928 | Computing loss after
2022-04-23 14:24:35 | [train_policy] epoch #928 | Fitting baseline...
2022-04-23 14:24:35 | [train_policy] epoch #928 | Saving snapshot...
2022-04-23 14:24:35 | [train_policy] epoch #928 | Saved
2022-04-23 14:24:35 | [train_policy] epoch #928 | Time 322.88 s
2022-04-23 14:24:35 | [train_policy] epoch #928 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.118807
Evaluation/AverageDiscountedReturn          -41.3995
Evaluation/AverageReturn                    -41.3995
Evaluation/CompletionRate                     0
Evaluation/Iteration                        928
Evaluation/MaxReturn                        -29.8591
Evaluation/MinReturn                        -75.073
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.48078
Extras/EpisodeRewardMean                    -41.2678
LinearFeatureBaseline/ExplainedVariance       0.830669
PolicyExecTime                                0.0924835
ProcessExecTime                               0.0110943
TotalEnvSteps                            940148
policy/Entropy                               -2.5356
policy/KL                                     0.00976653
policy/KLBefore                               0
policy/LossAfter                             -0.0198906
policy/LossBefore                             3.88726e-09
policy/Perplexity                             0.0792138
policy/dLoss                                  0.0198906
---------------------------------------  ----------------
2022-04-23 14:24:35 | [train_policy] epoch #929 | Obtaining samples for iteration 929...
2022-04-23 14:24:35 | [train_policy] epoch #929 | Logging diagnostics...
2022-04-23 14:24:35 | [train_policy] epoch #929 | Optimizing policy...
2022-04-23 14:24:35 | [train_policy] epoch #929 | Computing loss before
2022-04-23 14:24:35 | [train_policy] epoch #929 | Computing KL before
2022-04-23 14:24:35 | [train_policy] epoch #929 | Optimizing
2022-04-23 14:24:35 | [train_policy] epoch #929 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:35 | [train_policy] epoch #929 | computing loss before
2022-04-23 14:24:35 | [train_policy] epoch #929 | computing gradient
2022-04-23 14:24:35 | [train_policy] epoch #929 | gradient computed
2022-04-23 14:24:35 | [train_policy] epoch #929 | computing descent direction
2022-04-23 14:24:35 | [train_policy] epoch #929 | descent direction computed
2022-04-23 14:24:35 | [train_policy] epoch #929 | backtrack iters: 0
2022-04-23 14:24:35 | [train_policy] epoch #929 | optimization finished
2022-04-23 14:24:35 | [train_policy] epoch #929 | Computing KL after
2022-04-23 14:24:35 | [train_policy] epoch #929 | Computing loss after
2022-04-23 14:24:35 | [train_policy] epoch #929 | Fitting baseline...
2022-04-23 14:24:35 | [train_policy] epoch #929 | Saving snapshot...
2022-04-23 14:24:35 | [train_policy] epoch #929 | Saved
2022-04-23 14:24:35 | [train_policy] epoch #929 | Time 323.21 s
2022-04-23 14:24:35 | [train_policy] epoch #929 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118189
Evaluation/AverageDiscountedReturn          -40.5906
Evaluation/AverageReturn                    -40.5906
Evaluation/CompletionRate                     0
Evaluation/Iteration                        929
Evaluation/MaxReturn                        -30.4938
Evaluation/MinReturn                        -65.1391
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.82772
Extras/EpisodeRewardMean                    -40.6967
LinearFeatureBaseline/ExplainedVariance       0.869471
PolicyExecTime                                0.0965388
ProcessExecTime                               0.0114806
TotalEnvSteps                            941160
policy/Entropy                               -2.50963
policy/KL                                     0.00952924
policy/KLBefore                               0
policy/LossAfter                             -0.012044
policy/LossBefore                            -2.35591e-10
policy/Perplexity                             0.081298
policy/dLoss                                  0.012044
---------------------------------------  ----------------
2022-04-23 14:24:35 | [train_policy] epoch #930 | Obtaining samples for iteration 930...
2022-04-23 14:24:35 | [train_policy] epoch #930 | Logging diagnostics...
2022-04-23 14:24:35 | [train_policy] epoch #930 | Optimizing policy...
2022-04-23 14:24:35 | [train_policy] epoch #930 | Computing loss before
2022-04-23 14:24:35 | [train_policy] epoch #930 | Computing KL before
2022-04-23 14:24:35 | [train_policy] epoch #930 | Optimizing
2022-04-23 14:24:35 | [train_policy] epoch #930 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:35 | [train_policy] epoch #930 | computing loss before
2022-04-23 14:24:35 | [train_policy] epoch #930 | computing gradient
2022-04-23 14:24:35 | [train_policy] epoch #930 | gradient computed
2022-04-23 14:24:35 | [train_policy] epoch #930 | computing descent direction
2022-04-23 14:24:35 | [train_policy] epoch #930 | descent direction computed
2022-04-23 14:24:35 | [train_policy] epoch #930 | backtrack iters: 0
2022-04-23 14:24:35 | [train_policy] epoch #930 | optimization finished
2022-04-23 14:24:35 | [train_policy] epoch #930 | Computing KL after
2022-04-23 14:24:35 | [train_policy] epoch #930 | Computing loss after
2022-04-23 14:24:35 | [train_policy] epoch #930 | Fitting baseline...
2022-04-23 14:24:35 | [train_policy] epoch #930 | Saving snapshot...
2022-04-23 14:24:35 | [train_policy] epoch #930 | Saved
2022-04-23 14:24:35 | [train_policy] epoch #930 | Time 323.53 s
2022-04-23 14:24:35 | [train_policy] epoch #930 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118783
Evaluation/AverageDiscountedReturn          -42.6413
Evaluation/AverageReturn                    -42.6413
Evaluation/CompletionRate                     0
Evaluation/Iteration                        930
Evaluation/MaxReturn                        -30.3785
Evaluation/MinReturn                        -71.7708
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.28447
Extras/EpisodeRewardMean                    -42.2676
LinearFeatureBaseline/ExplainedVariance       0.849162
PolicyExecTime                                0.0943694
ProcessExecTime                               0.0111744
TotalEnvSteps                            942172
policy/Entropy                               -2.49865
policy/KL                                     0.00959719
policy/KLBefore                               0
policy/LossAfter                             -0.015155
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.0821955
policy/dLoss                                  0.015155
---------------------------------------  ----------------
2022-04-23 14:24:35 | [train_policy] epoch #931 | Obtaining samples for iteration 931...
2022-04-23 14:24:36 | [train_policy] epoch #931 | Logging diagnostics...
2022-04-23 14:24:36 | [train_policy] epoch #931 | Optimizing policy...
2022-04-23 14:24:36 | [train_policy] epoch #931 | Computing loss before
2022-04-23 14:24:36 | [train_policy] epoch #931 | Computing KL before
2022-04-23 14:24:36 | [train_policy] epoch #931 | Optimizing
2022-04-23 14:24:36 | [train_policy] epoch #931 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:36 | [train_policy] epoch #931 | computing loss before
2022-04-23 14:24:36 | [train_policy] epoch #931 | computing gradient
2022-04-23 14:24:36 | [train_policy] epoch #931 | gradient computed
2022-04-23 14:24:36 | [train_policy] epoch #931 | computing descent direction
2022-04-23 14:24:36 | [train_policy] epoch #931 | descent direction computed
2022-04-23 14:24:36 | [train_policy] epoch #931 | backtrack iters: 1
2022-04-23 14:24:36 | [train_policy] epoch #931 | optimization finished
2022-04-23 14:24:36 | [train_policy] epoch #931 | Computing KL after
2022-04-23 14:24:36 | [train_policy] epoch #931 | Computing loss after
2022-04-23 14:24:36 | [train_policy] epoch #931 | Fitting baseline...
2022-04-23 14:24:36 | [train_policy] epoch #931 | Saving snapshot...
2022-04-23 14:24:36 | [train_policy] epoch #931 | Saved
2022-04-23 14:24:36 | [train_policy] epoch #931 | Time 323.85 s
2022-04-23 14:24:36 | [train_policy] epoch #931 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118199
Evaluation/AverageDiscountedReturn          -39.2061
Evaluation/AverageReturn                    -39.2061
Evaluation/CompletionRate                     0
Evaluation/Iteration                        931
Evaluation/MaxReturn                        -29.5413
Evaluation/MinReturn                        -64.3244
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.70081
Extras/EpisodeRewardMean                    -39.3124
LinearFeatureBaseline/ExplainedVariance       0.870128
PolicyExecTime                                0.0950444
ProcessExecTime                               0.0111997
TotalEnvSteps                            943184
policy/Entropy                               -2.49921
policy/KL                                     0.00649768
policy/KLBefore                               0
policy/LossAfter                             -0.0181539
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.0821495
policy/dLoss                                  0.0181539
---------------------------------------  ----------------
2022-04-23 14:24:36 | [train_policy] epoch #932 | Obtaining samples for iteration 932...
2022-04-23 14:24:36 | [train_policy] epoch #932 | Logging diagnostics...
2022-04-23 14:24:36 | [train_policy] epoch #932 | Optimizing policy...
2022-04-23 14:24:36 | [train_policy] epoch #932 | Computing loss before
2022-04-23 14:24:36 | [train_policy] epoch #932 | Computing KL before
2022-04-23 14:24:36 | [train_policy] epoch #932 | Optimizing
2022-04-23 14:24:36 | [train_policy] epoch #932 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:36 | [train_policy] epoch #932 | computing loss before
2022-04-23 14:24:36 | [train_policy] epoch #932 | computing gradient
2022-04-23 14:24:36 | [train_policy] epoch #932 | gradient computed
2022-04-23 14:24:36 | [train_policy] epoch #932 | computing descent direction
2022-04-23 14:24:36 | [train_policy] epoch #932 | descent direction computed
2022-04-23 14:24:36 | [train_policy] epoch #932 | backtrack iters: 0
2022-04-23 14:24:36 | [train_policy] epoch #932 | optimization finished
2022-04-23 14:24:36 | [train_policy] epoch #932 | Computing KL after
2022-04-23 14:24:36 | [train_policy] epoch #932 | Computing loss after
2022-04-23 14:24:36 | [train_policy] epoch #932 | Fitting baseline...
2022-04-23 14:24:36 | [train_policy] epoch #932 | Saving snapshot...
2022-04-23 14:24:36 | [train_policy] epoch #932 | Saved
2022-04-23 14:24:36 | [train_policy] epoch #932 | Time 324.18 s
2022-04-23 14:24:36 | [train_policy] epoch #932 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.11985
Evaluation/AverageDiscountedReturn          -41.831
Evaluation/AverageReturn                    -41.831
Evaluation/CompletionRate                     0
Evaluation/Iteration                        932
Evaluation/MaxReturn                        -29.3216
Evaluation/MinReturn                        -71.6109
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.45187
Extras/EpisodeRewardMean                    -42.0778
LinearFeatureBaseline/ExplainedVariance       0.858367
PolicyExecTime                                0.0962033
ProcessExecTime                               0.0112352
TotalEnvSteps                            944196
policy/Entropy                               -2.50631
policy/KL                                     0.00984975
policy/KLBefore                               0
policy/LossAfter                             -0.0265343
policy/LossBefore                            -5.77199e-09
policy/Perplexity                             0.0815685
policy/dLoss                                  0.0265343
---------------------------------------  ----------------
2022-04-23 14:24:36 | [train_policy] epoch #933 | Obtaining samples for iteration 933...
2022-04-23 14:24:36 | [train_policy] epoch #933 | Logging diagnostics...
2022-04-23 14:24:36 | [train_policy] epoch #933 | Optimizing policy...
2022-04-23 14:24:36 | [train_policy] epoch #933 | Computing loss before
2022-04-23 14:24:36 | [train_policy] epoch #933 | Computing KL before
2022-04-23 14:24:36 | [train_policy] epoch #933 | Optimizing
2022-04-23 14:24:36 | [train_policy] epoch #933 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:36 | [train_policy] epoch #933 | computing loss before
2022-04-23 14:24:36 | [train_policy] epoch #933 | computing gradient
2022-04-23 14:24:36 | [train_policy] epoch #933 | gradient computed
2022-04-23 14:24:36 | [train_policy] epoch #933 | computing descent direction
2022-04-23 14:24:36 | [train_policy] epoch #933 | descent direction computed
2022-04-23 14:24:36 | [train_policy] epoch #933 | backtrack iters: 0
2022-04-23 14:24:36 | [train_policy] epoch #933 | optimization finished
2022-04-23 14:24:36 | [train_policy] epoch #933 | Computing KL after
2022-04-23 14:24:36 | [train_policy] epoch #933 | Computing loss after
2022-04-23 14:24:36 | [train_policy] epoch #933 | Fitting baseline...
2022-04-23 14:24:36 | [train_policy] epoch #933 | Saving snapshot...
2022-04-23 14:24:36 | [train_policy] epoch #933 | Saved
2022-04-23 14:24:36 | [train_policy] epoch #933 | Time 324.50 s
2022-04-23 14:24:36 | [train_policy] epoch #933 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.117809
Evaluation/AverageDiscountedReturn          -42.0817
Evaluation/AverageReturn                    -42.0817
Evaluation/CompletionRate                     0
Evaluation/Iteration                        933
Evaluation/MaxReturn                        -29.7753
Evaluation/MinReturn                        -71.6058
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3011
Extras/EpisodeRewardMean                    -41.7852
LinearFeatureBaseline/ExplainedVariance       0.866386
PolicyExecTime                                0.0914338
ProcessExecTime                               0.011035
TotalEnvSteps                            945208
policy/Entropy                               -2.47977
policy/KL                                     0.00971789
policy/KLBefore                               0
policy/LossAfter                             -0.012364
policy/LossBefore                             1.34287e-08
policy/Perplexity                             0.0837623
policy/dLoss                                  0.012364
---------------------------------------  ----------------
2022-04-23 14:24:36 | [train_policy] epoch #934 | Obtaining samples for iteration 934...
2022-04-23 14:24:37 | [train_policy] epoch #934 | Logging diagnostics...
2022-04-23 14:24:37 | [train_policy] epoch #934 | Optimizing policy...
2022-04-23 14:24:37 | [train_policy] epoch #934 | Computing loss before
2022-04-23 14:24:37 | [train_policy] epoch #934 | Computing KL before
2022-04-23 14:24:37 | [train_policy] epoch #934 | Optimizing
2022-04-23 14:24:37 | [train_policy] epoch #934 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:37 | [train_policy] epoch #934 | computing loss before
2022-04-23 14:24:37 | [train_policy] epoch #934 | computing gradient
2022-04-23 14:24:37 | [train_policy] epoch #934 | gradient computed
2022-04-23 14:24:37 | [train_policy] epoch #934 | computing descent direction
2022-04-23 14:24:37 | [train_policy] epoch #934 | descent direction computed
2022-04-23 14:24:37 | [train_policy] epoch #934 | backtrack iters: 0
2022-04-23 14:24:37 | [train_policy] epoch #934 | optimization finished
2022-04-23 14:24:37 | [train_policy] epoch #934 | Computing KL after
2022-04-23 14:24:37 | [train_policy] epoch #934 | Computing loss after
2022-04-23 14:24:37 | [train_policy] epoch #934 | Fitting baseline...
2022-04-23 14:24:37 | [train_policy] epoch #934 | Saving snapshot...
2022-04-23 14:24:37 | [train_policy] epoch #934 | Saved
2022-04-23 14:24:37 | [train_policy] epoch #934 | Time 324.82 s
2022-04-23 14:24:37 | [train_policy] epoch #934 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.118254
Evaluation/AverageDiscountedReturn          -41.6133
Evaluation/AverageReturn                    -41.6133
Evaluation/CompletionRate                     0
Evaluation/Iteration                        934
Evaluation/MaxReturn                        -28.7921
Evaluation/MinReturn                        -71.8522
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.01108
Extras/EpisodeRewardMean                    -41.9168
LinearFeatureBaseline/ExplainedVariance       0.849381
PolicyExecTime                                0.0926235
ProcessExecTime                               0.0112152
TotalEnvSteps                            946220
policy/Entropy                               -2.473
policy/KL                                     0.00981926
policy/KLBefore                               0
policy/LossAfter                             -0.0164634
policy/LossBefore                            -1.6138e-08
policy/Perplexity                             0.0843312
policy/dLoss                                  0.0164634
---------------------------------------  ---------------
2022-04-23 14:24:37 | [train_policy] epoch #935 | Obtaining samples for iteration 935...
2022-04-23 14:24:37 | [train_policy] epoch #935 | Logging diagnostics...
2022-04-23 14:24:37 | [train_policy] epoch #935 | Optimizing policy...
2022-04-23 14:24:37 | [train_policy] epoch #935 | Computing loss before
2022-04-23 14:24:37 | [train_policy] epoch #935 | Computing KL before
2022-04-23 14:24:37 | [train_policy] epoch #935 | Optimizing
2022-04-23 14:24:37 | [train_policy] epoch #935 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:37 | [train_policy] epoch #935 | computing loss before
2022-04-23 14:24:37 | [train_policy] epoch #935 | computing gradient
2022-04-23 14:24:37 | [train_policy] epoch #935 | gradient computed
2022-04-23 14:24:37 | [train_policy] epoch #935 | computing descent direction
2022-04-23 14:24:37 | [train_policy] epoch #935 | descent direction computed
2022-04-23 14:24:37 | [train_policy] epoch #935 | backtrack iters: 0
2022-04-23 14:24:37 | [train_policy] epoch #935 | optimization finished
2022-04-23 14:24:37 | [train_policy] epoch #935 | Computing KL after
2022-04-23 14:24:37 | [train_policy] epoch #935 | Computing loss after
2022-04-23 14:24:37 | [train_policy] epoch #935 | Fitting baseline...
2022-04-23 14:24:37 | [train_policy] epoch #935 | Saving snapshot...
2022-04-23 14:24:37 | [train_policy] epoch #935 | Saved
2022-04-23 14:24:37 | [train_policy] epoch #935 | Time 325.15 s
2022-04-23 14:24:37 | [train_policy] epoch #935 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120024
Evaluation/AverageDiscountedReturn          -40.8072
Evaluation/AverageReturn                    -40.8072
Evaluation/CompletionRate                     0
Evaluation/Iteration                        935
Evaluation/MaxReturn                        -29.3972
Evaluation/MinReturn                        -71.5976
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.06896
Extras/EpisodeRewardMean                    -41.1577
LinearFeatureBaseline/ExplainedVariance       0.866569
PolicyExecTime                                0.0961058
ProcessExecTime                               0.0114059
TotalEnvSteps                            947232
policy/Entropy                               -2.47923
policy/KL                                     0.00966683
policy/KLBefore                               0
policy/LossAfter                             -0.016647
policy/LossBefore                             6.59656e-09
policy/Perplexity                             0.083808
policy/dLoss                                  0.016647
---------------------------------------  ----------------
2022-04-23 14:24:37 | [train_policy] epoch #936 | Obtaining samples for iteration 936...
2022-04-23 14:24:37 | [train_policy] epoch #936 | Logging diagnostics...
2022-04-23 14:24:37 | [train_policy] epoch #936 | Optimizing policy...
2022-04-23 14:24:37 | [train_policy] epoch #936 | Computing loss before
2022-04-23 14:24:37 | [train_policy] epoch #936 | Computing KL before
2022-04-23 14:24:37 | [train_policy] epoch #936 | Optimizing
2022-04-23 14:24:37 | [train_policy] epoch #936 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:37 | [train_policy] epoch #936 | computing loss before
2022-04-23 14:24:37 | [train_policy] epoch #936 | computing gradient
2022-04-23 14:24:37 | [train_policy] epoch #936 | gradient computed
2022-04-23 14:24:37 | [train_policy] epoch #936 | computing descent direction
2022-04-23 14:24:37 | [train_policy] epoch #936 | descent direction computed
2022-04-23 14:24:37 | [train_policy] epoch #936 | backtrack iters: 0
2022-04-23 14:24:37 | [train_policy] epoch #936 | optimization finished
2022-04-23 14:24:37 | [train_policy] epoch #936 | Computing KL after
2022-04-23 14:24:37 | [train_policy] epoch #936 | Computing loss after
2022-04-23 14:24:37 | [train_policy] epoch #936 | Fitting baseline...
2022-04-23 14:24:37 | [train_policy] epoch #936 | Saving snapshot...
2022-04-23 14:24:37 | [train_policy] epoch #936 | Saved
2022-04-23 14:24:37 | [train_policy] epoch #936 | Time 325.47 s
2022-04-23 14:24:37 | [train_policy] epoch #936 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.120089
Evaluation/AverageDiscountedReturn          -39.4257
Evaluation/AverageReturn                    -39.4257
Evaluation/CompletionRate                     0
Evaluation/Iteration                        936
Evaluation/MaxReturn                        -29.8387
Evaluation/MinReturn                        -61.3955
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.38917
Extras/EpisodeRewardMean                    -39.2487
LinearFeatureBaseline/ExplainedVariance       0.893114
PolicyExecTime                                0.0968704
ProcessExecTime                               0.01142
TotalEnvSteps                            948244
policy/Entropy                               -2.47576
policy/KL                                     0.00971478
policy/KLBefore                               0
policy/LossAfter                             -0.0137647
policy/LossBefore                             2.886e-09
policy/Perplexity                             0.0840991
policy/dLoss                                  0.0137647
---------------------------------------  ---------------
2022-04-23 14:24:37 | [train_policy] epoch #937 | Obtaining samples for iteration 937...
2022-04-23 14:24:38 | [train_policy] epoch #937 | Logging diagnostics...
2022-04-23 14:24:38 | [train_policy] epoch #937 | Optimizing policy...
2022-04-23 14:24:38 | [train_policy] epoch #937 | Computing loss before
2022-04-23 14:24:38 | [train_policy] epoch #937 | Computing KL before
2022-04-23 14:24:38 | [train_policy] epoch #937 | Optimizing
2022-04-23 14:24:38 | [train_policy] epoch #937 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:38 | [train_policy] epoch #937 | computing loss before
2022-04-23 14:24:38 | [train_policy] epoch #937 | computing gradient
2022-04-23 14:24:38 | [train_policy] epoch #937 | gradient computed
2022-04-23 14:24:38 | [train_policy] epoch #937 | computing descent direction
2022-04-23 14:24:38 | [train_policy] epoch #937 | descent direction computed
2022-04-23 14:24:38 | [train_policy] epoch #937 | backtrack iters: 0
2022-04-23 14:24:38 | [train_policy] epoch #937 | optimization finished
2022-04-23 14:24:38 | [train_policy] epoch #937 | Computing KL after
2022-04-23 14:24:38 | [train_policy] epoch #937 | Computing loss after
2022-04-23 14:24:38 | [train_policy] epoch #937 | Fitting baseline...
2022-04-23 14:24:38 | [train_policy] epoch #937 | Saving snapshot...
2022-04-23 14:24:38 | [train_policy] epoch #937 | Saved
2022-04-23 14:24:38 | [train_policy] epoch #937 | Time 325.80 s
2022-04-23 14:24:38 | [train_policy] epoch #937 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119755
Evaluation/AverageDiscountedReturn          -39.0907
Evaluation/AverageReturn                    -39.0907
Evaluation/CompletionRate                     0
Evaluation/Iteration                        937
Evaluation/MaxReturn                        -29.0775
Evaluation/MinReturn                        -71.5252
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.87351
Extras/EpisodeRewardMean                    -39.3389
LinearFeatureBaseline/ExplainedVariance       0.881348
PolicyExecTime                                0.0932777
ProcessExecTime                               0.0111783
TotalEnvSteps                            949256
policy/Entropy                               -2.48304
policy/KL                                     0.00982019
policy/KLBefore                               0
policy/LossAfter                             -0.016626
policy/LossBefore                             9.42366e-09
policy/Perplexity                             0.0834892
policy/dLoss                                  0.016626
---------------------------------------  ----------------
2022-04-23 14:24:38 | [train_policy] epoch #938 | Obtaining samples for iteration 938...
2022-04-23 14:24:38 | [train_policy] epoch #938 | Logging diagnostics...
2022-04-23 14:24:38 | [train_policy] epoch #938 | Optimizing policy...
2022-04-23 14:24:38 | [train_policy] epoch #938 | Computing loss before
2022-04-23 14:24:38 | [train_policy] epoch #938 | Computing KL before
2022-04-23 14:24:38 | [train_policy] epoch #938 | Optimizing
2022-04-23 14:24:38 | [train_policy] epoch #938 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:38 | [train_policy] epoch #938 | computing loss before
2022-04-23 14:24:38 | [train_policy] epoch #938 | computing gradient
2022-04-23 14:24:38 | [train_policy] epoch #938 | gradient computed
2022-04-23 14:24:38 | [train_policy] epoch #938 | computing descent direction
2022-04-23 14:24:38 | [train_policy] epoch #938 | descent direction computed
2022-04-23 14:24:38 | [train_policy] epoch #938 | backtrack iters: 0
2022-04-23 14:24:38 | [train_policy] epoch #938 | optimization finished
2022-04-23 14:24:38 | [train_policy] epoch #938 | Computing KL after
2022-04-23 14:24:38 | [train_policy] epoch #938 | Computing loss after
2022-04-23 14:24:38 | [train_policy] epoch #938 | Fitting baseline...
2022-04-23 14:24:38 | [train_policy] epoch #938 | Saving snapshot...
2022-04-23 14:24:38 | [train_policy] epoch #938 | Saved
2022-04-23 14:24:38 | [train_policy] epoch #938 | Time 326.11 s
2022-04-23 14:24:38 | [train_policy] epoch #938 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.118688
Evaluation/AverageDiscountedReturn          -39.9132
Evaluation/AverageReturn                    -39.9132
Evaluation/CompletionRate                     0
Evaluation/Iteration                        938
Evaluation/MaxReturn                        -29.7149
Evaluation/MinReturn                        -61.7194
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.61339
Extras/EpisodeRewardMean                    -39.7068
LinearFeatureBaseline/ExplainedVariance       0.895808
PolicyExecTime                                0.0929508
ProcessExecTime                               0.0113549
TotalEnvSteps                            950268
policy/Entropy                               -2.45752
policy/KL                                     0.00964499
policy/KLBefore                               0
policy/LossAfter                             -0.016276
policy/LossBefore                            -4.24065e-09
policy/Perplexity                             0.0856472
policy/dLoss                                  0.016276
---------------------------------------  ----------------
2022-04-23 14:24:38 | [train_policy] epoch #939 | Obtaining samples for iteration 939...
2022-04-23 14:24:38 | [train_policy] epoch #939 | Logging diagnostics...
2022-04-23 14:24:38 | [train_policy] epoch #939 | Optimizing policy...
2022-04-23 14:24:38 | [train_policy] epoch #939 | Computing loss before
2022-04-23 14:24:38 | [train_policy] epoch #939 | Computing KL before
2022-04-23 14:24:38 | [train_policy] epoch #939 | Optimizing
2022-04-23 14:24:38 | [train_policy] epoch #939 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:38 | [train_policy] epoch #939 | computing loss before
2022-04-23 14:24:38 | [train_policy] epoch #939 | computing gradient
2022-04-23 14:24:38 | [train_policy] epoch #939 | gradient computed
2022-04-23 14:24:38 | [train_policy] epoch #939 | computing descent direction
2022-04-23 14:24:38 | [train_policy] epoch #939 | descent direction computed
2022-04-23 14:24:38 | [train_policy] epoch #939 | backtrack iters: 1
2022-04-23 14:24:38 | [train_policy] epoch #939 | optimization finished
2022-04-23 14:24:38 | [train_policy] epoch #939 | Computing KL after
2022-04-23 14:24:38 | [train_policy] epoch #939 | Computing loss after
2022-04-23 14:24:38 | [train_policy] epoch #939 | Fitting baseline...
2022-04-23 14:24:38 | [train_policy] epoch #939 | Saving snapshot...
2022-04-23 14:24:38 | [train_policy] epoch #939 | Saved
2022-04-23 14:24:38 | [train_policy] epoch #939 | Time 326.44 s
2022-04-23 14:24:38 | [train_policy] epoch #939 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118767
Evaluation/AverageDiscountedReturn          -61.2084
Evaluation/AverageReturn                    -61.2084
Evaluation/CompletionRate                     0
Evaluation/Iteration                        939
Evaluation/MaxReturn                        -29.9913
Evaluation/MinReturn                      -2045.68
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.17
Extras/EpisodeRewardMean                    -59.6225
LinearFeatureBaseline/ExplainedVariance       0.00586817
PolicyExecTime                                0.0938566
ProcessExecTime                               0.0111401
TotalEnvSteps                            951280
policy/Entropy                               -2.46453
policy/KL                                     0.00784769
policy/KLBefore                               0
policy/LossAfter                             -0.0119965
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             0.0850484
policy/dLoss                                  0.0119965
---------------------------------------  ----------------
2022-04-23 14:24:38 | [train_policy] epoch #940 | Obtaining samples for iteration 940...
2022-04-23 14:24:38 | [train_policy] epoch #940 | Logging diagnostics...
2022-04-23 14:24:38 | [train_policy] epoch #940 | Optimizing policy...
2022-04-23 14:24:38 | [train_policy] epoch #940 | Computing loss before
2022-04-23 14:24:38 | [train_policy] epoch #940 | Computing KL before
2022-04-23 14:24:38 | [train_policy] epoch #940 | Optimizing
2022-04-23 14:24:38 | [train_policy] epoch #940 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:38 | [train_policy] epoch #940 | computing loss before
2022-04-23 14:24:38 | [train_policy] epoch #940 | computing gradient
2022-04-23 14:24:38 | [train_policy] epoch #940 | gradient computed
2022-04-23 14:24:38 | [train_policy] epoch #940 | computing descent direction
2022-04-23 14:24:39 | [train_policy] epoch #940 | descent direction computed
2022-04-23 14:24:39 | [train_policy] epoch #940 | backtrack iters: 0
2022-04-23 14:24:39 | [train_policy] epoch #940 | optimization finished
2022-04-23 14:24:39 | [train_policy] epoch #940 | Computing KL after
2022-04-23 14:24:39 | [train_policy] epoch #940 | Computing loss after
2022-04-23 14:24:39 | [train_policy] epoch #940 | Fitting baseline...
2022-04-23 14:24:39 | [train_policy] epoch #940 | Saving snapshot...
2022-04-23 14:24:39 | [train_policy] epoch #940 | Saved
2022-04-23 14:24:39 | [train_policy] epoch #940 | Time 326.76 s
2022-04-23 14:24:39 | [train_policy] epoch #940 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119281
Evaluation/AverageDiscountedReturn          -39.8006
Evaluation/AverageReturn                    -39.8006
Evaluation/CompletionRate                     0
Evaluation/Iteration                        940
Evaluation/MaxReturn                        -29.3479
Evaluation/MinReturn                        -63.6993
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.42772
Extras/EpisodeRewardMean                    -39.7401
LinearFeatureBaseline/ExplainedVariance     -54.7315
PolicyExecTime                                0.0966806
ProcessExecTime                               0.0110397
TotalEnvSteps                            952292
policy/Entropy                               -2.43701
policy/KL                                     0.00983894
policy/KLBefore                               0
policy/LossAfter                             -0.0335676
policy/LossBefore                            -2.00253e-09
policy/Perplexity                             0.0874217
policy/dLoss                                  0.0335676
---------------------------------------  ----------------
2022-04-23 14:24:39 | [train_policy] epoch #941 | Obtaining samples for iteration 941...
2022-04-23 14:24:39 | [train_policy] epoch #941 | Logging diagnostics...
2022-04-23 14:24:39 | [train_policy] epoch #941 | Optimizing policy...
2022-04-23 14:24:39 | [train_policy] epoch #941 | Computing loss before
2022-04-23 14:24:39 | [train_policy] epoch #941 | Computing KL before
2022-04-23 14:24:39 | [train_policy] epoch #941 | Optimizing
2022-04-23 14:24:39 | [train_policy] epoch #941 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:39 | [train_policy] epoch #941 | computing loss before
2022-04-23 14:24:39 | [train_policy] epoch #941 | computing gradient
2022-04-23 14:24:39 | [train_policy] epoch #941 | gradient computed
2022-04-23 14:24:39 | [train_policy] epoch #941 | computing descent direction
2022-04-23 14:24:39 | [train_policy] epoch #941 | descent direction computed
2022-04-23 14:24:39 | [train_policy] epoch #941 | backtrack iters: 0
2022-04-23 14:24:39 | [train_policy] epoch #941 | optimization finished
2022-04-23 14:24:39 | [train_policy] epoch #941 | Computing KL after
2022-04-23 14:24:39 | [train_policy] epoch #941 | Computing loss after
2022-04-23 14:24:39 | [train_policy] epoch #941 | Fitting baseline...
2022-04-23 14:24:39 | [train_policy] epoch #941 | Saving snapshot...
2022-04-23 14:24:39 | [train_policy] epoch #941 | Saved
2022-04-23 14:24:39 | [train_policy] epoch #941 | Time 327.08 s
2022-04-23 14:24:39 | [train_policy] epoch #941 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118338
Evaluation/AverageDiscountedReturn          -41.8537
Evaluation/AverageReturn                    -41.8537
Evaluation/CompletionRate                     0
Evaluation/Iteration                        941
Evaluation/MaxReturn                        -29.2517
Evaluation/MinReturn                        -72.0626
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3372
Extras/EpisodeRewardMean                    -41.5921
LinearFeatureBaseline/ExplainedVariance       0.822133
PolicyExecTime                                0.0936103
ProcessExecTime                               0.0111248
TotalEnvSteps                            953304
policy/Entropy                               -2.44611
policy/KL                                     0.00994494
policy/KLBefore                               0
policy/LossAfter                             -0.0178066
policy/LossBefore                             2.53261e-08
policy/Perplexity                             0.0866297
policy/dLoss                                  0.0178066
---------------------------------------  ----------------
2022-04-23 14:24:39 | [train_policy] epoch #942 | Obtaining samples for iteration 942...
2022-04-23 14:24:39 | [train_policy] epoch #942 | Logging diagnostics...
2022-04-23 14:24:39 | [train_policy] epoch #942 | Optimizing policy...
2022-04-23 14:24:39 | [train_policy] epoch #942 | Computing loss before
2022-04-23 14:24:39 | [train_policy] epoch #942 | Computing KL before
2022-04-23 14:24:39 | [train_policy] epoch #942 | Optimizing
2022-04-23 14:24:39 | [train_policy] epoch #942 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:39 | [train_policy] epoch #942 | computing loss before
2022-04-23 14:24:39 | [train_policy] epoch #942 | computing gradient
2022-04-23 14:24:39 | [train_policy] epoch #942 | gradient computed
2022-04-23 14:24:39 | [train_policy] epoch #942 | computing descent direction
2022-04-23 14:24:39 | [train_policy] epoch #942 | descent direction computed
2022-04-23 14:24:39 | [train_policy] epoch #942 | backtrack iters: 0
2022-04-23 14:24:39 | [train_policy] epoch #942 | optimization finished
2022-04-23 14:24:39 | [train_policy] epoch #942 | Computing KL after
2022-04-23 14:24:39 | [train_policy] epoch #942 | Computing loss after
2022-04-23 14:24:39 | [train_policy] epoch #942 | Fitting baseline...
2022-04-23 14:24:39 | [train_policy] epoch #942 | Saving snapshot...
2022-04-23 14:24:39 | [train_policy] epoch #942 | Saved
2022-04-23 14:24:39 | [train_policy] epoch #942 | Time 327.40 s
2022-04-23 14:24:39 | [train_policy] epoch #942 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118406
Evaluation/AverageDiscountedReturn          -40.2936
Evaluation/AverageReturn                    -40.2936
Evaluation/CompletionRate                     0
Evaluation/Iteration                        942
Evaluation/MaxReturn                        -29.9581
Evaluation/MinReturn                        -72.0395
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.15597
Extras/EpisodeRewardMean                    -39.9305
LinearFeatureBaseline/ExplainedVariance       0.863389
PolicyExecTime                                0.0930426
ProcessExecTime                               0.0111358
TotalEnvSteps                            954316
policy/Entropy                               -2.38205
policy/KL                                     0.00942614
policy/KLBefore                               0
policy/LossAfter                             -0.034829
policy/LossBefore                            -9.65925e-09
policy/Perplexity                             0.0923614
policy/dLoss                                  0.034829
---------------------------------------  ----------------
2022-04-23 14:24:39 | [train_policy] epoch #943 | Obtaining samples for iteration 943...
2022-04-23 14:24:39 | [train_policy] epoch #943 | Logging diagnostics...
2022-04-23 14:24:39 | [train_policy] epoch #943 | Optimizing policy...
2022-04-23 14:24:39 | [train_policy] epoch #943 | Computing loss before
2022-04-23 14:24:39 | [train_policy] epoch #943 | Computing KL before
2022-04-23 14:24:39 | [train_policy] epoch #943 | Optimizing
2022-04-23 14:24:39 | [train_policy] epoch #943 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:39 | [train_policy] epoch #943 | computing loss before
2022-04-23 14:24:39 | [train_policy] epoch #943 | computing gradient
2022-04-23 14:24:39 | [train_policy] epoch #943 | gradient computed
2022-04-23 14:24:39 | [train_policy] epoch #943 | computing descent direction
2022-04-23 14:24:39 | [train_policy] epoch #943 | descent direction computed
2022-04-23 14:24:39 | [train_policy] epoch #943 | backtrack iters: 0
2022-04-23 14:24:39 | [train_policy] epoch #943 | optimization finished
2022-04-23 14:24:39 | [train_policy] epoch #943 | Computing KL after
2022-04-23 14:24:39 | [train_policy] epoch #943 | Computing loss after
2022-04-23 14:24:40 | [train_policy] epoch #943 | Fitting baseline...
2022-04-23 14:24:40 | [train_policy] epoch #943 | Saving snapshot...
2022-04-23 14:24:40 | [train_policy] epoch #943 | Saved
2022-04-23 14:24:40 | [train_policy] epoch #943 | Time 327.73 s
2022-04-23 14:24:40 | [train_policy] epoch #943 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.121722
Evaluation/AverageDiscountedReturn          -41.273
Evaluation/AverageReturn                    -41.273
Evaluation/CompletionRate                     0
Evaluation/Iteration                        943
Evaluation/MaxReturn                        -29.7814
Evaluation/MinReturn                        -71.8079
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.54189
Extras/EpisodeRewardMean                    -41.2162
LinearFeatureBaseline/ExplainedVariance       0.883698
PolicyExecTime                                0.0944333
ProcessExecTime                               0.0113702
TotalEnvSteps                            955328
policy/Entropy                               -2.31399
policy/KL                                     0.00949942
policy/KLBefore                               0
policy/LossAfter                             -0.0222331
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.0988659
policy/dLoss                                  0.0222331
---------------------------------------  ---------------
2022-04-23 14:24:40 | [train_policy] epoch #944 | Obtaining samples for iteration 944...
2022-04-23 14:24:40 | [train_policy] epoch #944 | Logging diagnostics...
2022-04-23 14:24:40 | [train_policy] epoch #944 | Optimizing policy...
2022-04-23 14:24:40 | [train_policy] epoch #944 | Computing loss before
2022-04-23 14:24:40 | [train_policy] epoch #944 | Computing KL before
2022-04-23 14:24:40 | [train_policy] epoch #944 | Optimizing
2022-04-23 14:24:40 | [train_policy] epoch #944 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:40 | [train_policy] epoch #944 | computing loss before
2022-04-23 14:24:40 | [train_policy] epoch #944 | computing gradient
2022-04-23 14:24:40 | [train_policy] epoch #944 | gradient computed
2022-04-23 14:24:40 | [train_policy] epoch #944 | computing descent direction
2022-04-23 14:24:40 | [train_policy] epoch #944 | descent direction computed
2022-04-23 14:24:40 | [train_policy] epoch #944 | backtrack iters: 1
2022-04-23 14:24:40 | [train_policy] epoch #944 | optimization finished
2022-04-23 14:24:40 | [train_policy] epoch #944 | Computing KL after
2022-04-23 14:24:40 | [train_policy] epoch #944 | Computing loss after
2022-04-23 14:24:40 | [train_policy] epoch #944 | Fitting baseline...
2022-04-23 14:24:40 | [train_policy] epoch #944 | Saving snapshot...
2022-04-23 14:24:40 | [train_policy] epoch #944 | Saved
2022-04-23 14:24:40 | [train_policy] epoch #944 | Time 328.05 s
2022-04-23 14:24:40 | [train_policy] epoch #944 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119678
Evaluation/AverageDiscountedReturn          -41.789
Evaluation/AverageReturn                    -41.789
Evaluation/CompletionRate                     0
Evaluation/Iteration                        944
Evaluation/MaxReturn                        -30.0181
Evaluation/MinReturn                        -72.8803
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.3383
Extras/EpisodeRewardMean                    -41.3502
LinearFeatureBaseline/ExplainedVariance       0.847127
PolicyExecTime                                0.0964172
ProcessExecTime                               0.0111952
TotalEnvSteps                            956340
policy/Entropy                               -2.31687
policy/KL                                     0.00643789
policy/KLBefore                               0
policy/LossAfter                             -0.0143301
policy/LossBefore                            -1.35465e-09
policy/Perplexity                             0.0985819
policy/dLoss                                  0.0143301
---------------------------------------  ----------------
2022-04-23 14:24:40 | [train_policy] epoch #945 | Obtaining samples for iteration 945...
2022-04-23 14:24:40 | [train_policy] epoch #945 | Logging diagnostics...
2022-04-23 14:24:40 | [train_policy] epoch #945 | Optimizing policy...
2022-04-23 14:24:40 | [train_policy] epoch #945 | Computing loss before
2022-04-23 14:24:40 | [train_policy] epoch #945 | Computing KL before
2022-04-23 14:24:40 | [train_policy] epoch #945 | Optimizing
2022-04-23 14:24:40 | [train_policy] epoch #945 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:40 | [train_policy] epoch #945 | computing loss before
2022-04-23 14:24:40 | [train_policy] epoch #945 | computing gradient
2022-04-23 14:24:40 | [train_policy] epoch #945 | gradient computed
2022-04-23 14:24:40 | [train_policy] epoch #945 | computing descent direction
2022-04-23 14:24:40 | [train_policy] epoch #945 | descent direction computed
2022-04-23 14:24:40 | [train_policy] epoch #945 | backtrack iters: 0
2022-04-23 14:24:40 | [train_policy] epoch #945 | optimization finished
2022-04-23 14:24:40 | [train_policy] epoch #945 | Computing KL after
2022-04-23 14:24:40 | [train_policy] epoch #945 | Computing loss after
2022-04-23 14:24:40 | [train_policy] epoch #945 | Fitting baseline...
2022-04-23 14:24:40 | [train_policy] epoch #945 | Saving snapshot...
2022-04-23 14:24:40 | [train_policy] epoch #945 | Saved
2022-04-23 14:24:40 | [train_policy] epoch #945 | Time 328.37 s
2022-04-23 14:24:40 | [train_policy] epoch #945 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118854
Evaluation/AverageDiscountedReturn          -43.1276
Evaluation/AverageReturn                    -43.1276
Evaluation/CompletionRate                     0
Evaluation/Iteration                        945
Evaluation/MaxReturn                        -30.5697
Evaluation/MinReturn                        -73.0602
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.86908
Extras/EpisodeRewardMean                    -43.1359
LinearFeatureBaseline/ExplainedVariance       0.87694
PolicyExecTime                                0.0913415
ProcessExecTime                               0.0110662
TotalEnvSteps                            957352
policy/Entropy                               -2.3306
policy/KL                                     0.00964117
policy/KLBefore                               0
policy/LossAfter                             -0.0229654
policy/LossBefore                             6.71436e-09
policy/Perplexity                             0.097237
policy/dLoss                                  0.0229654
---------------------------------------  ----------------
2022-04-23 14:24:40 | [train_policy] epoch #946 | Obtaining samples for iteration 946...
2022-04-23 14:24:40 | [train_policy] epoch #946 | Logging diagnostics...
2022-04-23 14:24:40 | [train_policy] epoch #946 | Optimizing policy...
2022-04-23 14:24:40 | [train_policy] epoch #946 | Computing loss before
2022-04-23 14:24:40 | [train_policy] epoch #946 | Computing KL before
2022-04-23 14:24:40 | [train_policy] epoch #946 | Optimizing
2022-04-23 14:24:40 | [train_policy] epoch #946 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:40 | [train_policy] epoch #946 | computing loss before
2022-04-23 14:24:40 | [train_policy] epoch #946 | computing gradient
2022-04-23 14:24:40 | [train_policy] epoch #946 | gradient computed
2022-04-23 14:24:40 | [train_policy] epoch #946 | computing descent direction
2022-04-23 14:24:40 | [train_policy] epoch #946 | descent direction computed
2022-04-23 14:24:40 | [train_policy] epoch #946 | backtrack iters: 1
2022-04-23 14:24:40 | [train_policy] epoch #946 | optimization finished
2022-04-23 14:24:40 | [train_policy] epoch #946 | Computing KL after
2022-04-23 14:24:40 | [train_policy] epoch #946 | Computing loss after
2022-04-23 14:24:40 | [train_policy] epoch #946 | Fitting baseline...
2022-04-23 14:24:40 | [train_policy] epoch #946 | Saving snapshot...
2022-04-23 14:24:40 | [train_policy] epoch #946 | Saved
2022-04-23 14:24:40 | [train_policy] epoch #946 | Time 328.69 s
2022-04-23 14:24:40 | [train_policy] epoch #946 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.118438
Evaluation/AverageDiscountedReturn          -62.5651
Evaluation/AverageReturn                    -62.5651
Evaluation/CompletionRate                     0
Evaluation/Iteration                        946
Evaluation/MaxReturn                        -29.9019
Evaluation/MinReturn                      -2045.76
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        208.123
Extras/EpisodeRewardMean                    -61.0488
LinearFeatureBaseline/ExplainedVariance       0.00990459
PolicyExecTime                                0.0911033
ProcessExecTime                               0.011019
TotalEnvSteps                            958364
policy/Entropy                               -2.31107
policy/KL                                     0.00644931
policy/KLBefore                               0
policy/LossAfter                             -0.0190654
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.099155
policy/dLoss                                  0.0190654
---------------------------------------  ---------------
2022-04-23 14:24:40 | [train_policy] epoch #947 | Obtaining samples for iteration 947...
2022-04-23 14:24:41 | [train_policy] epoch #947 | Logging diagnostics...
2022-04-23 14:24:41 | [train_policy] epoch #947 | Optimizing policy...
2022-04-23 14:24:41 | [train_policy] epoch #947 | Computing loss before
2022-04-23 14:24:41 | [train_policy] epoch #947 | Computing KL before
2022-04-23 14:24:41 | [train_policy] epoch #947 | Optimizing
2022-04-23 14:24:41 | [train_policy] epoch #947 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:41 | [train_policy] epoch #947 | computing loss before
2022-04-23 14:24:41 | [train_policy] epoch #947 | computing gradient
2022-04-23 14:24:41 | [train_policy] epoch #947 | gradient computed
2022-04-23 14:24:41 | [train_policy] epoch #947 | computing descent direction
2022-04-23 14:24:41 | [train_policy] epoch #947 | descent direction computed
2022-04-23 14:24:41 | [train_policy] epoch #947 | backtrack iters: 0
2022-04-23 14:24:41 | [train_policy] epoch #947 | optimization finished
2022-04-23 14:24:41 | [train_policy] epoch #947 | Computing KL after
2022-04-23 14:24:41 | [train_policy] epoch #947 | Computing loss after
2022-04-23 14:24:41 | [train_policy] epoch #947 | Fitting baseline...
2022-04-23 14:24:41 | [train_policy] epoch #947 | Saving snapshot...
2022-04-23 14:24:41 | [train_policy] epoch #947 | Saved
2022-04-23 14:24:41 | [train_policy] epoch #947 | Time 329.02 s
2022-04-23 14:24:41 | [train_policy] epoch #947 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.118939
Evaluation/AverageDiscountedReturn          -84.9948
Evaluation/AverageReturn                    -84.9948
Evaluation/CompletionRate                     0
Evaluation/Iteration                        947
Evaluation/MaxReturn                        -29.3581
Evaluation/MinReturn                      -2045.83
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        292.462
Extras/EpisodeRewardMean                    -81.1721
LinearFeatureBaseline/ExplainedVariance       0.235103
PolicyExecTime                                0.0934124
ProcessExecTime                               0.0111945
TotalEnvSteps                            959376
policy/Entropy                               -2.30597
policy/KL                                     0.00983822
policy/KLBefore                               0
policy/LossAfter                             -0.025942
policy/LossBefore                             5.6542e-09
policy/Perplexity                             0.0996624
policy/dLoss                                  0.025942
---------------------------------------  ---------------
2022-04-23 14:24:41 | [train_policy] epoch #948 | Obtaining samples for iteration 948...
2022-04-23 14:24:41 | [train_policy] epoch #948 | Logging diagnostics...
2022-04-23 14:24:41 | [train_policy] epoch #948 | Optimizing policy...
2022-04-23 14:24:41 | [train_policy] epoch #948 | Computing loss before
2022-04-23 14:24:41 | [train_policy] epoch #948 | Computing KL before
2022-04-23 14:24:41 | [train_policy] epoch #948 | Optimizing
2022-04-23 14:24:41 | [train_policy] epoch #948 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:41 | [train_policy] epoch #948 | computing loss before
2022-04-23 14:24:41 | [train_policy] epoch #948 | computing gradient
2022-04-23 14:24:41 | [train_policy] epoch #948 | gradient computed
2022-04-23 14:24:41 | [train_policy] epoch #948 | computing descent direction
2022-04-23 14:24:41 | [train_policy] epoch #948 | descent direction computed
2022-04-23 14:24:41 | [train_policy] epoch #948 | backtrack iters: 0
2022-04-23 14:24:41 | [train_policy] epoch #948 | optimization finished
2022-04-23 14:24:41 | [train_policy] epoch #948 | Computing KL after
2022-04-23 14:24:41 | [train_policy] epoch #948 | Computing loss after
2022-04-23 14:24:41 | [train_policy] epoch #948 | Fitting baseline...
2022-04-23 14:24:41 | [train_policy] epoch #948 | Saving snapshot...
2022-04-23 14:24:41 | [train_policy] epoch #948 | Saved
2022-04-23 14:24:41 | [train_policy] epoch #948 | Time 329.35 s
2022-04-23 14:24:41 | [train_policy] epoch #948 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.11997
Evaluation/AverageDiscountedReturn          -41.7187
Evaluation/AverageReturn                    -41.7187
Evaluation/CompletionRate                     0
Evaluation/Iteration                        948
Evaluation/MaxReturn                        -29.4241
Evaluation/MinReturn                        -78.2502
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.86359
Extras/EpisodeRewardMean                    -41.4755
LinearFeatureBaseline/ExplainedVariance    -154.117
PolicyExecTime                                0.0971806
ProcessExecTime                               0.0112822
TotalEnvSteps                            960388
policy/Entropy                               -2.29621
policy/KL                                     0.00991853
policy/KLBefore                               0
policy/LossAfter                             -0.0160844
policy/LossBefore                             8.01011e-09
policy/Perplexity                             0.100639
policy/dLoss                                  0.0160844
---------------------------------------  ----------------
2022-04-23 14:24:41 | [train_policy] epoch #949 | Obtaining samples for iteration 949...
2022-04-23 14:24:41 | [train_policy] epoch #949 | Logging diagnostics...
2022-04-23 14:24:41 | [train_policy] epoch #949 | Optimizing policy...
2022-04-23 14:24:41 | [train_policy] epoch #949 | Computing loss before
2022-04-23 14:24:41 | [train_policy] epoch #949 | Computing KL before
2022-04-23 14:24:41 | [train_policy] epoch #949 | Optimizing
2022-04-23 14:24:41 | [train_policy] epoch #949 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:41 | [train_policy] epoch #949 | computing loss before
2022-04-23 14:24:41 | [train_policy] epoch #949 | computing gradient
2022-04-23 14:24:41 | [train_policy] epoch #949 | gradient computed
2022-04-23 14:24:41 | [train_policy] epoch #949 | computing descent direction
2022-04-23 14:24:41 | [train_policy] epoch #949 | descent direction computed
2022-04-23 14:24:41 | [train_policy] epoch #949 | backtrack iters: 1
2022-04-23 14:24:41 | [train_policy] epoch #949 | optimization finished
2022-04-23 14:24:41 | [train_policy] epoch #949 | Computing KL after
2022-04-23 14:24:41 | [train_policy] epoch #949 | Computing loss after
2022-04-23 14:24:41 | [train_policy] epoch #949 | Fitting baseline...
2022-04-23 14:24:41 | [train_policy] epoch #949 | Saving snapshot...
2022-04-23 14:24:41 | [train_policy] epoch #949 | Saved
2022-04-23 14:24:41 | [train_policy] epoch #949 | Time 329.68 s
2022-04-23 14:24:41 | [train_policy] epoch #949 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.122604
Evaluation/AverageDiscountedReturn          -40.682
Evaluation/AverageReturn                    -40.682
Evaluation/CompletionRate                     0
Evaluation/Iteration                        949
Evaluation/MaxReturn                        -28.6931
Evaluation/MinReturn                        -73.0251
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.4699
Extras/EpisodeRewardMean                    -40.2883
LinearFeatureBaseline/ExplainedVariance       0.863839
PolicyExecTime                                0.0973604
ProcessExecTime                               0.0117455
TotalEnvSteps                            961400
policy/Entropy                               -2.30014
policy/KL                                     0.00640996
policy/KLBefore                               0
policy/LossAfter                             -0.013266
policy/LossBefore                             1.64914e-09
policy/Perplexity                             0.100245
policy/dLoss                                  0.013266
---------------------------------------  ----------------
2022-04-23 14:24:41 | [train_policy] epoch #950 | Obtaining samples for iteration 950...
2022-04-23 14:24:42 | [train_policy] epoch #950 | Logging diagnostics...
2022-04-23 14:24:42 | [train_policy] epoch #950 | Optimizing policy...
2022-04-23 14:24:42 | [train_policy] epoch #950 | Computing loss before
2022-04-23 14:24:42 | [train_policy] epoch #950 | Computing KL before
2022-04-23 14:24:42 | [train_policy] epoch #950 | Optimizing
2022-04-23 14:24:42 | [train_policy] epoch #950 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:42 | [train_policy] epoch #950 | computing loss before
2022-04-23 14:24:42 | [train_policy] epoch #950 | computing gradient
2022-04-23 14:24:42 | [train_policy] epoch #950 | gradient computed
2022-04-23 14:24:42 | [train_policy] epoch #950 | computing descent direction
2022-04-23 14:24:42 | [train_policy] epoch #950 | descent direction computed
2022-04-23 14:24:42 | [train_policy] epoch #950 | backtrack iters: 1
2022-04-23 14:24:42 | [train_policy] epoch #950 | optimization finished
2022-04-23 14:24:42 | [train_policy] epoch #950 | Computing KL after
2022-04-23 14:24:42 | [train_policy] epoch #950 | Computing loss after
2022-04-23 14:24:42 | [train_policy] epoch #950 | Fitting baseline...
2022-04-23 14:24:42 | [train_policy] epoch #950 | Saving snapshot...
2022-04-23 14:24:42 | [train_policy] epoch #950 | Saved
2022-04-23 14:24:42 | [train_policy] epoch #950 | Time 330.02 s
2022-04-23 14:24:42 | [train_policy] epoch #950 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120749
Evaluation/AverageDiscountedReturn          -53.3688
Evaluation/AverageReturn                    -53.3688
Evaluation/CompletionRate                     0
Evaluation/Iteration                        950
Evaluation/MaxReturn                        -28.5674
Evaluation/MinReturn                      -1180.35
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        118.958
Extras/EpisodeRewardMean                    -52.493
LinearFeatureBaseline/ExplainedVariance       0.0192322
PolicyExecTime                                0.0984759
ProcessExecTime                               0.01126
TotalEnvSteps                            962412
policy/Entropy                               -2.32113
policy/KL                                     0.0064982
policy/KLBefore                               0
policy/LossAfter                             -0.0159927
policy/LossBefore                            -9.42366e-10
policy/Perplexity                             0.0981623
policy/dLoss                                  0.0159927
---------------------------------------  ----------------
2022-04-23 14:24:42 | [train_policy] epoch #951 | Obtaining samples for iteration 951...
2022-04-23 14:24:42 | [train_policy] epoch #951 | Logging diagnostics...
2022-04-23 14:24:42 | [train_policy] epoch #951 | Optimizing policy...
2022-04-23 14:24:42 | [train_policy] epoch #951 | Computing loss before
2022-04-23 14:24:42 | [train_policy] epoch #951 | Computing KL before
2022-04-23 14:24:42 | [train_policy] epoch #951 | Optimizing
2022-04-23 14:24:42 | [train_policy] epoch #951 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:42 | [train_policy] epoch #951 | computing loss before
2022-04-23 14:24:42 | [train_policy] epoch #951 | computing gradient
2022-04-23 14:24:42 | [train_policy] epoch #951 | gradient computed
2022-04-23 14:24:42 | [train_policy] epoch #951 | computing descent direction
2022-04-23 14:24:42 | [train_policy] epoch #951 | descent direction computed
2022-04-23 14:24:42 | [train_policy] epoch #951 | backtrack iters: 1
2022-04-23 14:24:42 | [train_policy] epoch #951 | optimization finished
2022-04-23 14:24:42 | [train_policy] epoch #951 | Computing KL after
2022-04-23 14:24:42 | [train_policy] epoch #951 | Computing loss after
2022-04-23 14:24:42 | [train_policy] epoch #951 | Fitting baseline...
2022-04-23 14:24:42 | [train_policy] epoch #951 | Saving snapshot...
2022-04-23 14:24:42 | [train_policy] epoch #951 | Saved
2022-04-23 14:24:42 | [train_policy] epoch #951 | Time 330.35 s
2022-04-23 14:24:42 | [train_policy] epoch #951 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118464
Evaluation/AverageDiscountedReturn          -39.0201
Evaluation/AverageReturn                    -39.0201
Evaluation/CompletionRate                     0
Evaluation/Iteration                        951
Evaluation/MaxReturn                        -28.9487
Evaluation/MinReturn                        -66.5696
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.43836
Extras/EpisodeRewardMean                    -38.8899
LinearFeatureBaseline/ExplainedVariance      -5.03411
PolicyExecTime                                0.104177
ProcessExecTime                               0.0112844
TotalEnvSteps                            963424
policy/Entropy                               -2.31687
policy/KL                                     0.0064264
policy/KLBefore                               0
policy/LossAfter                             -0.018421
policy/LossBefore                            -3.76946e-09
policy/Perplexity                             0.098582
policy/dLoss                                  0.018421
---------------------------------------  ----------------
2022-04-23 14:24:42 | [train_policy] epoch #952 | Obtaining samples for iteration 952...
2022-04-23 14:24:42 | [train_policy] epoch #952 | Logging diagnostics...
2022-04-23 14:24:42 | [train_policy] epoch #952 | Optimizing policy...
2022-04-23 14:24:42 | [train_policy] epoch #952 | Computing loss before
2022-04-23 14:24:42 | [train_policy] epoch #952 | Computing KL before
2022-04-23 14:24:42 | [train_policy] epoch #952 | Optimizing
2022-04-23 14:24:42 | [train_policy] epoch #952 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:42 | [train_policy] epoch #952 | computing loss before
2022-04-23 14:24:42 | [train_policy] epoch #952 | computing gradient
2022-04-23 14:24:42 | [train_policy] epoch #952 | gradient computed
2022-04-23 14:24:42 | [train_policy] epoch #952 | computing descent direction
2022-04-23 14:24:42 | [train_policy] epoch #952 | descent direction computed
2022-04-23 14:24:42 | [train_policy] epoch #952 | backtrack iters: 0
2022-04-23 14:24:42 | [train_policy] epoch #952 | optimization finished
2022-04-23 14:24:42 | [train_policy] epoch #952 | Computing KL after
2022-04-23 14:24:42 | [train_policy] epoch #952 | Computing loss after
2022-04-23 14:24:42 | [train_policy] epoch #952 | Fitting baseline...
2022-04-23 14:24:42 | [train_policy] epoch #952 | Saving snapshot...
2022-04-23 14:24:42 | [train_policy] epoch #952 | Saved
2022-04-23 14:24:42 | [train_policy] epoch #952 | Time 330.67 s
2022-04-23 14:24:42 | [train_policy] epoch #952 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118194
Evaluation/AverageDiscountedReturn          -43.3776
Evaluation/AverageReturn                    -43.3776
Evaluation/CompletionRate                     0
Evaluation/Iteration                        952
Evaluation/MaxReturn                        -28.5458
Evaluation/MinReturn                        -88.5697
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         10.7107
Extras/EpisodeRewardMean                    -43.1493
LinearFeatureBaseline/ExplainedVariance       0.744297
PolicyExecTime                                0.0934227
ProcessExecTime                               0.0110681
TotalEnvSteps                            964436
policy/Entropy                               -2.35456
policy/KL                                     0.00974578
policy/KLBefore                               0
policy/LossAfter                             -0.0202783
policy/LossBefore                             1.88473e-09
policy/Perplexity                             0.0949351
policy/dLoss                                  0.0202783
---------------------------------------  ----------------
2022-04-23 14:24:42 | [train_policy] epoch #953 | Obtaining samples for iteration 953...
2022-04-23 14:24:43 | [train_policy] epoch #953 | Logging diagnostics...
2022-04-23 14:24:43 | [train_policy] epoch #953 | Optimizing policy...
2022-04-23 14:24:43 | [train_policy] epoch #953 | Computing loss before
2022-04-23 14:24:43 | [train_policy] epoch #953 | Computing KL before
2022-04-23 14:24:43 | [train_policy] epoch #953 | Optimizing
2022-04-23 14:24:43 | [train_policy] epoch #953 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:43 | [train_policy] epoch #953 | computing loss before
2022-04-23 14:24:43 | [train_policy] epoch #953 | computing gradient
2022-04-23 14:24:43 | [train_policy] epoch #953 | gradient computed
2022-04-23 14:24:43 | [train_policy] epoch #953 | computing descent direction
2022-04-23 14:24:43 | [train_policy] epoch #953 | descent direction computed
2022-04-23 14:24:43 | [train_policy] epoch #953 | backtrack iters: 1
2022-04-23 14:24:43 | [train_policy] epoch #953 | optimization finished
2022-04-23 14:24:43 | [train_policy] epoch #953 | Computing KL after
2022-04-23 14:24:43 | [train_policy] epoch #953 | Computing loss after
2022-04-23 14:24:43 | [train_policy] epoch #953 | Fitting baseline...
2022-04-23 14:24:43 | [train_policy] epoch #953 | Saving snapshot...
2022-04-23 14:24:43 | [train_policy] epoch #953 | Saved
2022-04-23 14:24:43 | [train_policy] epoch #953 | Time 331.00 s
2022-04-23 14:24:43 | [train_policy] epoch #953 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119111
Evaluation/AverageDiscountedReturn          -43.6439
Evaluation/AverageReturn                    -43.6439
Evaluation/CompletionRate                     0
Evaluation/Iteration                        953
Evaluation/MaxReturn                        -29.2498
Evaluation/MinReturn                       -140.29
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         15.3681
Extras/EpisodeRewardMean                    -43.3386
LinearFeatureBaseline/ExplainedVariance       0.571636
PolicyExecTime                                0.0930891
ProcessExecTime                               0.0112596
TotalEnvSteps                            965448
policy/Entropy                               -2.37013
policy/KL                                     0.00658218
policy/KLBefore                               0
policy/LossAfter                             -0.0188673
policy/LossBefore                             6.12538e-09
policy/Perplexity                             0.0934688
policy/dLoss                                  0.0188673
---------------------------------------  ----------------
2022-04-23 14:24:43 | [train_policy] epoch #954 | Obtaining samples for iteration 954...
2022-04-23 14:24:43 | [train_policy] epoch #954 | Logging diagnostics...
2022-04-23 14:24:43 | [train_policy] epoch #954 | Optimizing policy...
2022-04-23 14:24:43 | [train_policy] epoch #954 | Computing loss before
2022-04-23 14:24:43 | [train_policy] epoch #954 | Computing KL before
2022-04-23 14:24:43 | [train_policy] epoch #954 | Optimizing
2022-04-23 14:24:43 | [train_policy] epoch #954 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:43 | [train_policy] epoch #954 | computing loss before
2022-04-23 14:24:43 | [train_policy] epoch #954 | computing gradient
2022-04-23 14:24:43 | [train_policy] epoch #954 | gradient computed
2022-04-23 14:24:43 | [train_policy] epoch #954 | computing descent direction
2022-04-23 14:24:43 | [train_policy] epoch #954 | descent direction computed
2022-04-23 14:24:43 | [train_policy] epoch #954 | backtrack iters: 1
2022-04-23 14:24:43 | [train_policy] epoch #954 | optimization finished
2022-04-23 14:24:43 | [train_policy] epoch #954 | Computing KL after
2022-04-23 14:24:43 | [train_policy] epoch #954 | Computing loss after
2022-04-23 14:24:43 | [train_policy] epoch #954 | Fitting baseline...
2022-04-23 14:24:43 | [train_policy] epoch #954 | Saving snapshot...
2022-04-23 14:24:43 | [train_policy] epoch #954 | Saved
2022-04-23 14:24:43 | [train_policy] epoch #954 | Time 331.33 s
2022-04-23 14:24:43 | [train_policy] epoch #954 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.11902
Evaluation/AverageDiscountedReturn          -41.5056
Evaluation/AverageReturn                    -41.5056
Evaluation/CompletionRate                     0
Evaluation/Iteration                        954
Evaluation/MaxReturn                        -28.4667
Evaluation/MinReturn                        -72.7704
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.7364
Extras/EpisodeRewardMean                    -41.4114
LinearFeatureBaseline/ExplainedVariance       0.728013
PolicyExecTime                                0.0985024
ProcessExecTime                               0.01121
TotalEnvSteps                            966460
policy/Entropy                               -2.38699
policy/KL                                     0.00654902
policy/KLBefore                               0
policy/LossAfter                             -0.0160099
policy/LossBefore                            -5.6542e-09
policy/Perplexity                             0.0919062
policy/dLoss                                  0.0160099
---------------------------------------  ---------------
2022-04-23 14:24:43 | [train_policy] epoch #955 | Obtaining samples for iteration 955...
2022-04-23 14:24:43 | [train_policy] epoch #955 | Logging diagnostics...
2022-04-23 14:24:43 | [train_policy] epoch #955 | Optimizing policy...
2022-04-23 14:24:43 | [train_policy] epoch #955 | Computing loss before
2022-04-23 14:24:43 | [train_policy] epoch #955 | Computing KL before
2022-04-23 14:24:43 | [train_policy] epoch #955 | Optimizing
2022-04-23 14:24:43 | [train_policy] epoch #955 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:43 | [train_policy] epoch #955 | computing loss before
2022-04-23 14:24:43 | [train_policy] epoch #955 | computing gradient
2022-04-23 14:24:43 | [train_policy] epoch #955 | gradient computed
2022-04-23 14:24:43 | [train_policy] epoch #955 | computing descent direction
2022-04-23 14:24:43 | [train_policy] epoch #955 | descent direction computed
2022-04-23 14:24:43 | [train_policy] epoch #955 | backtrack iters: 0
2022-04-23 14:24:43 | [train_policy] epoch #955 | optimization finished
2022-04-23 14:24:43 | [train_policy] epoch #955 | Computing KL after
2022-04-23 14:24:43 | [train_policy] epoch #955 | Computing loss after
2022-04-23 14:24:43 | [train_policy] epoch #955 | Fitting baseline...
2022-04-23 14:24:43 | [train_policy] epoch #955 | Saving snapshot...
2022-04-23 14:24:43 | [train_policy] epoch #955 | Saved
2022-04-23 14:24:43 | [train_policy] epoch #955 | Time 331.65 s
2022-04-23 14:24:43 | [train_policy] epoch #955 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118896
Evaluation/AverageDiscountedReturn          -40.5546
Evaluation/AverageReturn                    -40.5546
Evaluation/CompletionRate                     0
Evaluation/Iteration                        955
Evaluation/MaxReturn                        -28.5115
Evaluation/MinReturn                        -66.2543
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.42514
Extras/EpisodeRewardMean                    -40.4881
LinearFeatureBaseline/ExplainedVariance       0.883472
PolicyExecTime                                0.0975211
ProcessExecTime                               0.0112476
TotalEnvSteps                            967472
policy/Entropy                               -2.37177
policy/KL                                     0.00963866
policy/KLBefore                               0
policy/LossAfter                             -0.0171017
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.0933158
policy/dLoss                                  0.0171017
---------------------------------------  ----------------
2022-04-23 14:24:43 | [train_policy] epoch #956 | Obtaining samples for iteration 956...
2022-04-23 14:24:44 | [train_policy] epoch #956 | Logging diagnostics...
2022-04-23 14:24:44 | [train_policy] epoch #956 | Optimizing policy...
2022-04-23 14:24:44 | [train_policy] epoch #956 | Computing loss before
2022-04-23 14:24:44 | [train_policy] epoch #956 | Computing KL before
2022-04-23 14:24:44 | [train_policy] epoch #956 | Optimizing
2022-04-23 14:24:44 | [train_policy] epoch #956 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:44 | [train_policy] epoch #956 | computing loss before
2022-04-23 14:24:44 | [train_policy] epoch #956 | computing gradient
2022-04-23 14:24:44 | [train_policy] epoch #956 | gradient computed
2022-04-23 14:24:44 | [train_policy] epoch #956 | computing descent direction
2022-04-23 14:24:44 | [train_policy] epoch #956 | descent direction computed
2022-04-23 14:24:44 | [train_policy] epoch #956 | backtrack iters: 0
2022-04-23 14:24:44 | [train_policy] epoch #956 | optimization finished
2022-04-23 14:24:44 | [train_policy] epoch #956 | Computing KL after
2022-04-23 14:24:44 | [train_policy] epoch #956 | Computing loss after
2022-04-23 14:24:44 | [train_policy] epoch #956 | Fitting baseline...
2022-04-23 14:24:44 | [train_policy] epoch #956 | Saving snapshot...
2022-04-23 14:24:44 | [train_policy] epoch #956 | Saved
2022-04-23 14:24:44 | [train_policy] epoch #956 | Time 331.98 s
2022-04-23 14:24:44 | [train_policy] epoch #956 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.12205
Evaluation/AverageDiscountedReturn          -41.619
Evaluation/AverageReturn                    -41.619
Evaluation/CompletionRate                     0
Evaluation/Iteration                        956
Evaluation/MaxReturn                        -31.1931
Evaluation/MinReturn                        -72.5187
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.26554
Extras/EpisodeRewardMean                    -41.1438
LinearFeatureBaseline/ExplainedVariance       0.857717
PolicyExecTime                                0.0968287
ProcessExecTime                               0.0115817
TotalEnvSteps                            968484
policy/Entropy                               -2.36702
policy/KL                                     0.00983861
policy/KLBefore                               0
policy/LossAfter                             -0.0172521
policy/LossBefore                             1.17796e-09
policy/Perplexity                             0.0937598
policy/dLoss                                  0.0172521
---------------------------------------  ----------------
2022-04-23 14:24:44 | [train_policy] epoch #957 | Obtaining samples for iteration 957...
2022-04-23 14:24:44 | [train_policy] epoch #957 | Logging diagnostics...
2022-04-23 14:24:44 | [train_policy] epoch #957 | Optimizing policy...
2022-04-23 14:24:44 | [train_policy] epoch #957 | Computing loss before
2022-04-23 14:24:44 | [train_policy] epoch #957 | Computing KL before
2022-04-23 14:24:44 | [train_policy] epoch #957 | Optimizing
2022-04-23 14:24:44 | [train_policy] epoch #957 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:44 | [train_policy] epoch #957 | computing loss before
2022-04-23 14:24:44 | [train_policy] epoch #957 | computing gradient
2022-04-23 14:24:44 | [train_policy] epoch #957 | gradient computed
2022-04-23 14:24:44 | [train_policy] epoch #957 | computing descent direction
2022-04-23 14:24:44 | [train_policy] epoch #957 | descent direction computed
2022-04-23 14:24:44 | [train_policy] epoch #957 | backtrack iters: 0
2022-04-23 14:24:44 | [train_policy] epoch #957 | optimization finished
2022-04-23 14:24:44 | [train_policy] epoch #957 | Computing KL after
2022-04-23 14:24:44 | [train_policy] epoch #957 | Computing loss after
2022-04-23 14:24:44 | [train_policy] epoch #957 | Fitting baseline...
2022-04-23 14:24:44 | [train_policy] epoch #957 | Saving snapshot...
2022-04-23 14:24:44 | [train_policy] epoch #957 | Saved
2022-04-23 14:24:44 | [train_policy] epoch #957 | Time 332.30 s
2022-04-23 14:24:44 | [train_policy] epoch #957 | EpochTime 0.31 s
---------------------------------------  ----------------
EnvExecTime                                   0.118833
Evaluation/AverageDiscountedReturn          -40.5174
Evaluation/AverageReturn                    -40.5174
Evaluation/CompletionRate                     0
Evaluation/Iteration                        957
Evaluation/MaxReturn                        -28.0037
Evaluation/MinReturn                        -72.6951
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.25313
Extras/EpisodeRewardMean                    -40.5675
LinearFeatureBaseline/ExplainedVariance       0.851736
PolicyExecTime                                0.0913174
ProcessExecTime                               0.0110993
TotalEnvSteps                            969496
policy/Entropy                               -2.36781
policy/KL                                     0.00984062
policy/KLBefore                               0
policy/LossAfter                             -0.0179235
policy/LossBefore                            -3.53387e-09
policy/Perplexity                             0.0936858
policy/dLoss                                  0.0179235
---------------------------------------  ----------------
2022-04-23 14:24:44 | [train_policy] epoch #958 | Obtaining samples for iteration 958...
2022-04-23 14:24:44 | [train_policy] epoch #958 | Logging diagnostics...
2022-04-23 14:24:44 | [train_policy] epoch #958 | Optimizing policy...
2022-04-23 14:24:44 | [train_policy] epoch #958 | Computing loss before
2022-04-23 14:24:44 | [train_policy] epoch #958 | Computing KL before
2022-04-23 14:24:44 | [train_policy] epoch #958 | Optimizing
2022-04-23 14:24:44 | [train_policy] epoch #958 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:44 | [train_policy] epoch #958 | computing loss before
2022-04-23 14:24:44 | [train_policy] epoch #958 | computing gradient
2022-04-23 14:24:44 | [train_policy] epoch #958 | gradient computed
2022-04-23 14:24:44 | [train_policy] epoch #958 | computing descent direction
2022-04-23 14:24:44 | [train_policy] epoch #958 | descent direction computed
2022-04-23 14:24:44 | [train_policy] epoch #958 | backtrack iters: 1
2022-04-23 14:24:44 | [train_policy] epoch #958 | optimization finished
2022-04-23 14:24:44 | [train_policy] epoch #958 | Computing KL after
2022-04-23 14:24:44 | [train_policy] epoch #958 | Computing loss after
2022-04-23 14:24:44 | [train_policy] epoch #958 | Fitting baseline...
2022-04-23 14:24:44 | [train_policy] epoch #958 | Saving snapshot...
2022-04-23 14:24:44 | [train_policy] epoch #958 | Saved
2022-04-23 14:24:44 | [train_policy] epoch #958 | Time 332.63 s
2022-04-23 14:24:44 | [train_policy] epoch #958 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120159
Evaluation/AverageDiscountedReturn          -44.5703
Evaluation/AverageReturn                    -44.5703
Evaluation/CompletionRate                     0
Evaluation/Iteration                        958
Evaluation/MaxReturn                        -31.9156
Evaluation/MinReturn                       -273.034
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         25.7128
Extras/EpisodeRewardMean                    -43.9136
LinearFeatureBaseline/ExplainedVariance       0.280044
PolicyExecTime                                0.0931077
ProcessExecTime                               0.011277
TotalEnvSteps                            970508
policy/Entropy                               -2.36998
policy/KL                                     0.00652345
policy/KLBefore                               0
policy/LossAfter                             -0.019394
policy/LossBefore                             4.24065e-09
policy/Perplexity                             0.0934824
policy/dLoss                                  0.019394
---------------------------------------  ----------------
2022-04-23 14:24:44 | [train_policy] epoch #959 | Obtaining samples for iteration 959...
2022-04-23 14:24:45 | [train_policy] epoch #959 | Logging diagnostics...
2022-04-23 14:24:45 | [train_policy] epoch #959 | Optimizing policy...
2022-04-23 14:24:45 | [train_policy] epoch #959 | Computing loss before
2022-04-23 14:24:45 | [train_policy] epoch #959 | Computing KL before
2022-04-23 14:24:45 | [train_policy] epoch #959 | Optimizing
2022-04-23 14:24:45 | [train_policy] epoch #959 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:45 | [train_policy] epoch #959 | computing loss before
2022-04-23 14:24:45 | [train_policy] epoch #959 | computing gradient
2022-04-23 14:24:45 | [train_policy] epoch #959 | gradient computed
2022-04-23 14:24:45 | [train_policy] epoch #959 | computing descent direction
2022-04-23 14:24:45 | [train_policy] epoch #959 | descent direction computed
2022-04-23 14:24:45 | [train_policy] epoch #959 | backtrack iters: 1
2022-04-23 14:24:45 | [train_policy] epoch #959 | optimization finished
2022-04-23 14:24:45 | [train_policy] epoch #959 | Computing KL after
2022-04-23 14:24:45 | [train_policy] epoch #959 | Computing loss after
2022-04-23 14:24:45 | [train_policy] epoch #959 | Fitting baseline...
2022-04-23 14:24:45 | [train_policy] epoch #959 | Saving snapshot...
2022-04-23 14:24:45 | [train_policy] epoch #959 | Saved
2022-04-23 14:24:45 | [train_policy] epoch #959 | Time 332.96 s
2022-04-23 14:24:45 | [train_policy] epoch #959 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.121121
Evaluation/AverageDiscountedReturn          -40.3601
Evaluation/AverageReturn                    -40.3601
Evaluation/CompletionRate                     0
Evaluation/Iteration                        959
Evaluation/MaxReturn                        -29.4864
Evaluation/MinReturn                        -65.2364
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.09399
Extras/EpisodeRewardMean                    -40.5478
LinearFeatureBaseline/ExplainedVariance       0.532226
PolicyExecTime                                0.095083
ProcessExecTime                               0.0113339
TotalEnvSteps                            971520
policy/Entropy                               -2.37752
policy/KL                                     0.00654052
policy/KLBefore                               0
policy/LossAfter                             -0.0122563
policy/LossBefore                             1.06016e-08
policy/Perplexity                             0.0927803
policy/dLoss                                  0.0122564
---------------------------------------  ----------------
2022-04-23 14:24:45 | [train_policy] epoch #960 | Obtaining samples for iteration 960...
2022-04-23 14:24:45 | [train_policy] epoch #960 | Logging diagnostics...
2022-04-23 14:24:45 | [train_policy] epoch #960 | Optimizing policy...
2022-04-23 14:24:45 | [train_policy] epoch #960 | Computing loss before
2022-04-23 14:24:45 | [train_policy] epoch #960 | Computing KL before
2022-04-23 14:24:45 | [train_policy] epoch #960 | Optimizing
2022-04-23 14:24:45 | [train_policy] epoch #960 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:45 | [train_policy] epoch #960 | computing loss before
2022-04-23 14:24:45 | [train_policy] epoch #960 | computing gradient
2022-04-23 14:24:45 | [train_policy] epoch #960 | gradient computed
2022-04-23 14:24:45 | [train_policy] epoch #960 | computing descent direction
2022-04-23 14:24:45 | [train_policy] epoch #960 | descent direction computed
2022-04-23 14:24:45 | [train_policy] epoch #960 | backtrack iters: 1
2022-04-23 14:24:45 | [train_policy] epoch #960 | optimization finished
2022-04-23 14:24:45 | [train_policy] epoch #960 | Computing KL after
2022-04-23 14:24:45 | [train_policy] epoch #960 | Computing loss after
2022-04-23 14:24:45 | [train_policy] epoch #960 | Fitting baseline...
2022-04-23 14:24:45 | [train_policy] epoch #960 | Saving snapshot...
2022-04-23 14:24:45 | [train_policy] epoch #960 | Saved
2022-04-23 14:24:45 | [train_policy] epoch #960 | Time 333.29 s
2022-04-23 14:24:45 | [train_policy] epoch #960 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119453
Evaluation/AverageDiscountedReturn          -40.4363
Evaluation/AverageReturn                    -40.4363
Evaluation/CompletionRate                     0
Evaluation/Iteration                        960
Evaluation/MaxReturn                        -28.282
Evaluation/MinReturn                        -66.7163
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.2027
Extras/EpisodeRewardMean                    -40.6475
LinearFeatureBaseline/ExplainedVariance       0.866664
PolicyExecTime                                0.0967307
ProcessExecTime                               0.011091
TotalEnvSteps                            972532
policy/Entropy                               -2.38269
policy/KL                                     0.00656705
policy/KLBefore                               0
policy/LossAfter                             -0.0122656
policy/LossBefore                            -1.27219e-08
policy/Perplexity                             0.0923022
policy/dLoss                                  0.0122656
---------------------------------------  ----------------
2022-04-23 14:24:45 | [train_policy] epoch #961 | Obtaining samples for iteration 961...
2022-04-23 14:24:45 | [train_policy] epoch #961 | Logging diagnostics...
2022-04-23 14:24:45 | [train_policy] epoch #961 | Optimizing policy...
2022-04-23 14:24:45 | [train_policy] epoch #961 | Computing loss before
2022-04-23 14:24:45 | [train_policy] epoch #961 | Computing KL before
2022-04-23 14:24:45 | [train_policy] epoch #961 | Optimizing
2022-04-23 14:24:45 | [train_policy] epoch #961 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:45 | [train_policy] epoch #961 | computing loss before
2022-04-23 14:24:45 | [train_policy] epoch #961 | computing gradient
2022-04-23 14:24:45 | [train_policy] epoch #961 | gradient computed
2022-04-23 14:24:45 | [train_policy] epoch #961 | computing descent direction
2022-04-23 14:24:45 | [train_policy] epoch #961 | descent direction computed
2022-04-23 14:24:45 | [train_policy] epoch #961 | backtrack iters: 1
2022-04-23 14:24:45 | [train_policy] epoch #961 | optimization finished
2022-04-23 14:24:45 | [train_policy] epoch #961 | Computing KL after
2022-04-23 14:24:45 | [train_policy] epoch #961 | Computing loss after
2022-04-23 14:24:45 | [train_policy] epoch #961 | Fitting baseline...
2022-04-23 14:24:45 | [train_policy] epoch #961 | Saving snapshot...
2022-04-23 14:24:45 | [train_policy] epoch #961 | Saved
2022-04-23 14:24:45 | [train_policy] epoch #961 | Time 333.61 s
2022-04-23 14:24:45 | [train_policy] epoch #961 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.11929
Evaluation/AverageDiscountedReturn          -41.608
Evaluation/AverageReturn                    -41.608
Evaluation/CompletionRate                     0
Evaluation/Iteration                        961
Evaluation/MaxReturn                        -28.1245
Evaluation/MinReturn                        -69.0809
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.48389
Extras/EpisodeRewardMean                    -41.1773
LinearFeatureBaseline/ExplainedVariance       0.83226
PolicyExecTime                                0.0962765
ProcessExecTime                               0.0111375
TotalEnvSteps                            973544
policy/Entropy                               -2.38169
policy/KL                                     0.00647912
policy/KLBefore                               0
policy/LossAfter                             -0.01953
policy/LossBefore                            -1.06016e-08
policy/Perplexity                             0.0923943
policy/dLoss                                  0.01953
---------------------------------------  ----------------
2022-04-23 14:24:45 | [train_policy] epoch #962 | Obtaining samples for iteration 962...
2022-04-23 14:24:46 | [train_policy] epoch #962 | Logging diagnostics...
2022-04-23 14:24:46 | [train_policy] epoch #962 | Optimizing policy...
2022-04-23 14:24:46 | [train_policy] epoch #962 | Computing loss before
2022-04-23 14:24:46 | [train_policy] epoch #962 | Computing KL before
2022-04-23 14:24:46 | [train_policy] epoch #962 | Optimizing
2022-04-23 14:24:46 | [train_policy] epoch #962 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:46 | [train_policy] epoch #962 | computing loss before
2022-04-23 14:24:46 | [train_policy] epoch #962 | computing gradient
2022-04-23 14:24:46 | [train_policy] epoch #962 | gradient computed
2022-04-23 14:24:46 | [train_policy] epoch #962 | computing descent direction
2022-04-23 14:24:46 | [train_policy] epoch #962 | descent direction computed
2022-04-23 14:24:46 | [train_policy] epoch #962 | backtrack iters: 1
2022-04-23 14:24:46 | [train_policy] epoch #962 | optimization finished
2022-04-23 14:24:46 | [train_policy] epoch #962 | Computing KL after
2022-04-23 14:24:46 | [train_policy] epoch #962 | Computing loss after
2022-04-23 14:24:46 | [train_policy] epoch #962 | Fitting baseline...
2022-04-23 14:24:46 | [train_policy] epoch #962 | Saving snapshot...
2022-04-23 14:24:46 | [train_policy] epoch #962 | Saved
2022-04-23 14:24:46 | [train_policy] epoch #962 | Time 333.95 s
2022-04-23 14:24:46 | [train_policy] epoch #962 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.121975
Evaluation/AverageDiscountedReturn          -41.1553
Evaluation/AverageReturn                    -41.1553
Evaluation/CompletionRate                     0
Evaluation/Iteration                        962
Evaluation/MaxReturn                        -28.0185
Evaluation/MinReturn                        -66.7682
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.04869
Extras/EpisodeRewardMean                    -41.0173
LinearFeatureBaseline/ExplainedVariance       0.854272
PolicyExecTime                                0.100721
ProcessExecTime                               0.0113966
TotalEnvSteps                            974556
policy/Entropy                               -2.39609
policy/KL                                     0.00654816
policy/KLBefore                               0
policy/LossAfter                             -0.0165372
policy/LossBefore                             7.06774e-09
policy/Perplexity                             0.0910738
policy/dLoss                                  0.0165372
---------------------------------------  ----------------
2022-04-23 14:24:46 | [train_policy] epoch #963 | Obtaining samples for iteration 963...
2022-04-23 14:24:46 | [train_policy] epoch #963 | Logging diagnostics...
2022-04-23 14:24:46 | [train_policy] epoch #963 | Optimizing policy...
2022-04-23 14:24:46 | [train_policy] epoch #963 | Computing loss before
2022-04-23 14:24:46 | [train_policy] epoch #963 | Computing KL before
2022-04-23 14:24:46 | [train_policy] epoch #963 | Optimizing
2022-04-23 14:24:46 | [train_policy] epoch #963 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:46 | [train_policy] epoch #963 | computing loss before
2022-04-23 14:24:46 | [train_policy] epoch #963 | computing gradient
2022-04-23 14:24:46 | [train_policy] epoch #963 | gradient computed
2022-04-23 14:24:46 | [train_policy] epoch #963 | computing descent direction
2022-04-23 14:24:46 | [train_policy] epoch #963 | descent direction computed
2022-04-23 14:24:46 | [train_policy] epoch #963 | backtrack iters: 1
2022-04-23 14:24:46 | [train_policy] epoch #963 | optimization finished
2022-04-23 14:24:46 | [train_policy] epoch #963 | Computing KL after
2022-04-23 14:24:46 | [train_policy] epoch #963 | Computing loss after
2022-04-23 14:24:46 | [train_policy] epoch #963 | Fitting baseline...
2022-04-23 14:24:46 | [train_policy] epoch #963 | Saving snapshot...
2022-04-23 14:24:46 | [train_policy] epoch #963 | Saved
2022-04-23 14:24:46 | [train_policy] epoch #963 | Time 334.29 s
2022-04-23 14:24:46 | [train_policy] epoch #963 | EpochTime 0.34 s
---------------------------------------  ----------------
EnvExecTime                                   0.120527
Evaluation/AverageDiscountedReturn          -41.4071
Evaluation/AverageReturn                    -41.4071
Evaluation/CompletionRate                     0
Evaluation/Iteration                        963
Evaluation/MaxReturn                        -29.3299
Evaluation/MinReturn                        -63.6462
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.55749
Extras/EpisodeRewardMean                    -41.4534
LinearFeatureBaseline/ExplainedVariance       0.871142
PolicyExecTime                                0.103932
ProcessExecTime                               0.0111449
TotalEnvSteps                            975568
policy/Entropy                               -2.41148
policy/KL                                     0.00661066
policy/KLBefore                               0
policy/LossAfter                             -0.0175743
policy/LossBefore                            -2.14388e-08
policy/Perplexity                             0.0896823
policy/dLoss                                  0.0175743
---------------------------------------  ----------------
2022-04-23 14:24:46 | [train_policy] epoch #964 | Obtaining samples for iteration 964...
2022-04-23 14:24:46 | [train_policy] epoch #964 | Logging diagnostics...
2022-04-23 14:24:46 | [train_policy] epoch #964 | Optimizing policy...
2022-04-23 14:24:46 | [train_policy] epoch #964 | Computing loss before
2022-04-23 14:24:46 | [train_policy] epoch #964 | Computing KL before
2022-04-23 14:24:46 | [train_policy] epoch #964 | Optimizing
2022-04-23 14:24:46 | [train_policy] epoch #964 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:46 | [train_policy] epoch #964 | computing loss before
2022-04-23 14:24:46 | [train_policy] epoch #964 | computing gradient
2022-04-23 14:24:46 | [train_policy] epoch #964 | gradient computed
2022-04-23 14:24:46 | [train_policy] epoch #964 | computing descent direction
2022-04-23 14:24:46 | [train_policy] epoch #964 | descent direction computed
2022-04-23 14:24:46 | [train_policy] epoch #964 | backtrack iters: 0
2022-04-23 14:24:46 | [train_policy] epoch #964 | optimization finished
2022-04-23 14:24:46 | [train_policy] epoch #964 | Computing KL after
2022-04-23 14:24:46 | [train_policy] epoch #964 | Computing loss after
2022-04-23 14:24:46 | [train_policy] epoch #964 | Fitting baseline...
2022-04-23 14:24:46 | [train_policy] epoch #964 | Saving snapshot...
2022-04-23 14:24:46 | [train_policy] epoch #964 | Saved
2022-04-23 14:24:46 | [train_policy] epoch #964 | Time 334.62 s
2022-04-23 14:24:46 | [train_policy] epoch #964 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119073
Evaluation/AverageDiscountedReturn          -41.8126
Evaluation/AverageReturn                    -41.8126
Evaluation/CompletionRate                     0
Evaluation/Iteration                        964
Evaluation/MaxReturn                        -28.1741
Evaluation/MinReturn                        -73.0511
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.21578
Extras/EpisodeRewardMean                    -41.9173
LinearFeatureBaseline/ExplainedVariance       0.862877
PolicyExecTime                                0.098099
ProcessExecTime                               0.0111008
TotalEnvSteps                            976580
policy/Entropy                               -2.43696
policy/KL                                     0.00984184
policy/KLBefore                               0
policy/LossAfter                             -0.0160313
policy/LossBefore                             1.97897e-08
policy/Perplexity                             0.0874259
policy/dLoss                                  0.0160314
---------------------------------------  ----------------
2022-04-23 14:24:46 | [train_policy] epoch #965 | Obtaining samples for iteration 965...
2022-04-23 14:24:47 | [train_policy] epoch #965 | Logging diagnostics...
2022-04-23 14:24:47 | [train_policy] epoch #965 | Optimizing policy...
2022-04-23 14:24:47 | [train_policy] epoch #965 | Computing loss before
2022-04-23 14:24:47 | [train_policy] epoch #965 | Computing KL before
2022-04-23 14:24:47 | [train_policy] epoch #965 | Optimizing
2022-04-23 14:24:47 | [train_policy] epoch #965 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:47 | [train_policy] epoch #965 | computing loss before
2022-04-23 14:24:47 | [train_policy] epoch #965 | computing gradient
2022-04-23 14:24:47 | [train_policy] epoch #965 | gradient computed
2022-04-23 14:24:47 | [train_policy] epoch #965 | computing descent direction
2022-04-23 14:24:47 | [train_policy] epoch #965 | descent direction computed
2022-04-23 14:24:47 | [train_policy] epoch #965 | backtrack iters: 0
2022-04-23 14:24:47 | [train_policy] epoch #965 | optimization finished
2022-04-23 14:24:47 | [train_policy] epoch #965 | Computing KL after
2022-04-23 14:24:47 | [train_policy] epoch #965 | Computing loss after
2022-04-23 14:24:47 | [train_policy] epoch #965 | Fitting baseline...
2022-04-23 14:24:47 | [train_policy] epoch #965 | Saving snapshot...
2022-04-23 14:24:47 | [train_policy] epoch #965 | Saved
2022-04-23 14:24:47 | [train_policy] epoch #965 | Time 334.95 s
2022-04-23 14:24:47 | [train_policy] epoch #965 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.118737
Evaluation/AverageDiscountedReturn          -41.2271
Evaluation/AverageReturn                    -41.2271
Evaluation/CompletionRate                     0
Evaluation/Iteration                        965
Evaluation/MaxReturn                        -29.5322
Evaluation/MinReturn                        -73.8319
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.35174
Extras/EpisodeRewardMean                    -41.3305
LinearFeatureBaseline/ExplainedVariance       0.820462
PolicyExecTime                                0.0966959
ProcessExecTime                               0.0112174
TotalEnvSteps                            977592
policy/Entropy                               -2.43525
policy/KL                                     0.00988858
policy/KLBefore                               0
policy/LossAfter                             -0.0225439
policy/LossBefore                             3.06269e-09
policy/Perplexity                             0.0875755
policy/dLoss                                  0.0225439
---------------------------------------  ----------------
2022-04-23 14:24:47 | [train_policy] epoch #966 | Obtaining samples for iteration 966...
2022-04-23 14:24:47 | [train_policy] epoch #966 | Logging diagnostics...
2022-04-23 14:24:47 | [train_policy] epoch #966 | Optimizing policy...
2022-04-23 14:24:47 | [train_policy] epoch #966 | Computing loss before
2022-04-23 14:24:47 | [train_policy] epoch #966 | Computing KL before
2022-04-23 14:24:47 | [train_policy] epoch #966 | Optimizing
2022-04-23 14:24:47 | [train_policy] epoch #966 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:47 | [train_policy] epoch #966 | computing loss before
2022-04-23 14:24:47 | [train_policy] epoch #966 | computing gradient
2022-04-23 14:24:47 | [train_policy] epoch #966 | gradient computed
2022-04-23 14:24:47 | [train_policy] epoch #966 | computing descent direction
2022-04-23 14:24:47 | [train_policy] epoch #966 | descent direction computed
2022-04-23 14:24:47 | [train_policy] epoch #966 | backtrack iters: 0
2022-04-23 14:24:47 | [train_policy] epoch #966 | optimization finished
2022-04-23 14:24:47 | [train_policy] epoch #966 | Computing KL after
2022-04-23 14:24:47 | [train_policy] epoch #966 | Computing loss after
2022-04-23 14:24:47 | [train_policy] epoch #966 | Fitting baseline...
2022-04-23 14:24:47 | [train_policy] epoch #966 | Saving snapshot...
2022-04-23 14:24:47 | [train_policy] epoch #966 | Saved
2022-04-23 14:24:47 | [train_policy] epoch #966 | Time 335.27 s
2022-04-23 14:24:47 | [train_policy] epoch #966 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120371
Evaluation/AverageDiscountedReturn          -42.0687
Evaluation/AverageReturn                    -42.0687
Evaluation/CompletionRate                     0
Evaluation/Iteration                        966
Evaluation/MaxReturn                        -30.1723
Evaluation/MinReturn                        -73.6443
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.81951
Extras/EpisodeRewardMean                    -41.6125
LinearFeatureBaseline/ExplainedVariance       0.798998
PolicyExecTime                                0.0990038
ProcessExecTime                               0.0111914
TotalEnvSteps                            978604
policy/Entropy                               -2.42338
policy/KL                                     0.00979839
policy/KLBefore                               0
policy/LossAfter                             -0.0216363
policy/LossBefore                             2.12621e-08
policy/Perplexity                             0.0886215
policy/dLoss                                  0.0216363
---------------------------------------  ----------------
2022-04-23 14:24:47 | [train_policy] epoch #967 | Obtaining samples for iteration 967...
2022-04-23 14:24:47 | [train_policy] epoch #967 | Logging diagnostics...
2022-04-23 14:24:47 | [train_policy] epoch #967 | Optimizing policy...
2022-04-23 14:24:47 | [train_policy] epoch #967 | Computing loss before
2022-04-23 14:24:47 | [train_policy] epoch #967 | Computing KL before
2022-04-23 14:24:47 | [train_policy] epoch #967 | Optimizing
2022-04-23 14:24:47 | [train_policy] epoch #967 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:47 | [train_policy] epoch #967 | computing loss before
2022-04-23 14:24:47 | [train_policy] epoch #967 | computing gradient
2022-04-23 14:24:47 | [train_policy] epoch #967 | gradient computed
2022-04-23 14:24:47 | [train_policy] epoch #967 | computing descent direction
2022-04-23 14:24:47 | [train_policy] epoch #967 | descent direction computed
2022-04-23 14:24:47 | [train_policy] epoch #967 | backtrack iters: 1
2022-04-23 14:24:47 | [train_policy] epoch #967 | optimization finished
2022-04-23 14:24:47 | [train_policy] epoch #967 | Computing KL after
2022-04-23 14:24:47 | [train_policy] epoch #967 | Computing loss after
2022-04-23 14:24:47 | [train_policy] epoch #967 | Fitting baseline...
2022-04-23 14:24:47 | [train_policy] epoch #967 | Saving snapshot...
2022-04-23 14:24:47 | [train_policy] epoch #967 | Saved
2022-04-23 14:24:47 | [train_policy] epoch #967 | Time 335.60 s
2022-04-23 14:24:47 | [train_policy] epoch #967 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120401
Evaluation/AverageDiscountedReturn          -41.7793
Evaluation/AverageReturn                    -41.7793
Evaluation/CompletionRate                     0
Evaluation/Iteration                        967
Evaluation/MaxReturn                        -29.2625
Evaluation/MinReturn                        -73.0855
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.2359
Extras/EpisodeRewardMean                    -41.7491
LinearFeatureBaseline/ExplainedVariance       0.839433
PolicyExecTime                                0.0964379
ProcessExecTime                               0.011235
TotalEnvSteps                            979616
policy/Entropy                               -2.43932
policy/KL                                     0.00663719
policy/KLBefore                               0
policy/LossAfter                             -0.0134096
policy/LossBefore                            -1.50779e-08
policy/Perplexity                             0.0872199
policy/dLoss                                  0.0134095
---------------------------------------  ----------------
2022-04-23 14:24:47 | [train_policy] epoch #968 | Obtaining samples for iteration 968...
2022-04-23 14:24:48 | [train_policy] epoch #968 | Logging diagnostics...
2022-04-23 14:24:48 | [train_policy] epoch #968 | Optimizing policy...
2022-04-23 14:24:48 | [train_policy] epoch #968 | Computing loss before
2022-04-23 14:24:48 | [train_policy] epoch #968 | Computing KL before
2022-04-23 14:24:48 | [train_policy] epoch #968 | Optimizing
2022-04-23 14:24:48 | [train_policy] epoch #968 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:48 | [train_policy] epoch #968 | computing loss before
2022-04-23 14:24:48 | [train_policy] epoch #968 | computing gradient
2022-04-23 14:24:48 | [train_policy] epoch #968 | gradient computed
2022-04-23 14:24:48 | [train_policy] epoch #968 | computing descent direction
2022-04-23 14:24:48 | [train_policy] epoch #968 | descent direction computed
2022-04-23 14:24:48 | [train_policy] epoch #968 | backtrack iters: 1
2022-04-23 14:24:48 | [train_policy] epoch #968 | optimization finished
2022-04-23 14:24:48 | [train_policy] epoch #968 | Computing KL after
2022-04-23 14:24:48 | [train_policy] epoch #968 | Computing loss after
2022-04-23 14:24:48 | [train_policy] epoch #968 | Fitting baseline...
2022-04-23 14:24:48 | [train_policy] epoch #968 | Saving snapshot...
2022-04-23 14:24:48 | [train_policy] epoch #968 | Saved
2022-04-23 14:24:48 | [train_policy] epoch #968 | Time 335.93 s
2022-04-23 14:24:48 | [train_policy] epoch #968 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119617
Evaluation/AverageDiscountedReturn          -41.2441
Evaluation/AverageReturn                    -41.2441
Evaluation/CompletionRate                     0
Evaluation/Iteration                        968
Evaluation/MaxReturn                        -29.8446
Evaluation/MinReturn                        -70.1554
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.41756
Extras/EpisodeRewardMean                    -41.5283
LinearFeatureBaseline/ExplainedVariance       0.8458
PolicyExecTime                                0.0949705
ProcessExecTime                               0.0112116
TotalEnvSteps                            980628
policy/Entropy                               -2.43322
policy/KL                                     0.00654007
policy/KLBefore                               0
policy/LossAfter                             -0.0111333
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.0877534
policy/dLoss                                  0.0111333
---------------------------------------  ----------------
2022-04-23 14:24:48 | [train_policy] epoch #969 | Obtaining samples for iteration 969...
2022-04-23 14:24:48 | [train_policy] epoch #969 | Logging diagnostics...
2022-04-23 14:24:48 | [train_policy] epoch #969 | Optimizing policy...
2022-04-23 14:24:48 | [train_policy] epoch #969 | Computing loss before
2022-04-23 14:24:48 | [train_policy] epoch #969 | Computing KL before
2022-04-23 14:24:48 | [train_policy] epoch #969 | Optimizing
2022-04-23 14:24:48 | [train_policy] epoch #969 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:48 | [train_policy] epoch #969 | computing loss before
2022-04-23 14:24:48 | [train_policy] epoch #969 | computing gradient
2022-04-23 14:24:48 | [train_policy] epoch #969 | gradient computed
2022-04-23 14:24:48 | [train_policy] epoch #969 | computing descent direction
2022-04-23 14:24:48 | [train_policy] epoch #969 | descent direction computed
2022-04-23 14:24:48 | [train_policy] epoch #969 | backtrack iters: 0
2022-04-23 14:24:48 | [train_policy] epoch #969 | optimization finished
2022-04-23 14:24:48 | [train_policy] epoch #969 | Computing KL after
2022-04-23 14:24:48 | [train_policy] epoch #969 | Computing loss after
2022-04-23 14:24:48 | [train_policy] epoch #969 | Fitting baseline...
2022-04-23 14:24:48 | [train_policy] epoch #969 | Saving snapshot...
2022-04-23 14:24:48 | [train_policy] epoch #969 | Saved
2022-04-23 14:24:48 | [train_policy] epoch #969 | Time 336.26 s
2022-04-23 14:24:48 | [train_policy] epoch #969 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.118894
Evaluation/AverageDiscountedReturn          -39.0209
Evaluation/AverageReturn                    -39.0209
Evaluation/CompletionRate                     0
Evaluation/Iteration                        969
Evaluation/MaxReturn                        -29.7105
Evaluation/MinReturn                        -63.6485
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          6.94719
Extras/EpisodeRewardMean                    -38.9044
LinearFeatureBaseline/ExplainedVariance       0.883962
PolicyExecTime                                0.0995922
ProcessExecTime                               0.0111697
TotalEnvSteps                            981640
policy/Entropy                               -2.41963
policy/KL                                     0.0099469
policy/KLBefore                               0
policy/LossAfter                             -0.0169302
policy/LossBefore                            -1.88473e-09
policy/Perplexity                             0.0889545
policy/dLoss                                  0.0169302
---------------------------------------  ----------------
2022-04-23 14:24:48 | [train_policy] epoch #970 | Obtaining samples for iteration 970...
2022-04-23 14:24:48 | [train_policy] epoch #970 | Logging diagnostics...
2022-04-23 14:24:48 | [train_policy] epoch #970 | Optimizing policy...
2022-04-23 14:24:48 | [train_policy] epoch #970 | Computing loss before
2022-04-23 14:24:48 | [train_policy] epoch #970 | Computing KL before
2022-04-23 14:24:48 | [train_policy] epoch #970 | Optimizing
2022-04-23 14:24:48 | [train_policy] epoch #970 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:48 | [train_policy] epoch #970 | computing loss before
2022-04-23 14:24:48 | [train_policy] epoch #970 | computing gradient
2022-04-23 14:24:48 | [train_policy] epoch #970 | gradient computed
2022-04-23 14:24:48 | [train_policy] epoch #970 | computing descent direction
2022-04-23 14:24:48 | [train_policy] epoch #970 | descent direction computed
2022-04-23 14:24:48 | [train_policy] epoch #970 | backtrack iters: 1
2022-04-23 14:24:48 | [train_policy] epoch #970 | optimization finished
2022-04-23 14:24:48 | [train_policy] epoch #970 | Computing KL after
2022-04-23 14:24:48 | [train_policy] epoch #970 | Computing loss after
2022-04-23 14:24:48 | [train_policy] epoch #970 | Fitting baseline...
2022-04-23 14:24:48 | [train_policy] epoch #970 | Saving snapshot...
2022-04-23 14:24:48 | [train_policy] epoch #970 | Saved
2022-04-23 14:24:48 | [train_policy] epoch #970 | Time 336.60 s
2022-04-23 14:24:48 | [train_policy] epoch #970 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119648
Evaluation/AverageDiscountedReturn          -41.944
Evaluation/AverageReturn                    -41.944
Evaluation/CompletionRate                     0
Evaluation/Iteration                        970
Evaluation/MaxReturn                        -29.2485
Evaluation/MinReturn                       -142.772
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         13.6966
Extras/EpisodeRewardMean                    -42.0272
LinearFeatureBaseline/ExplainedVariance       0.605232
PolicyExecTime                                0.0997531
ProcessExecTime                               0.0112822
TotalEnvSteps                            982652
policy/Entropy                               -2.4111
policy/KL                                     0.00645332
policy/KLBefore                               0
policy/LossAfter                             -0.023686
policy/LossBefore                             1.64914e-08
policy/Perplexity                             0.0897168
policy/dLoss                                  0.023686
---------------------------------------  ----------------
2022-04-23 14:24:48 | [train_policy] epoch #971 | Obtaining samples for iteration 971...
2022-04-23 14:24:49 | [train_policy] epoch #971 | Logging diagnostics...
2022-04-23 14:24:49 | [train_policy] epoch #971 | Optimizing policy...
2022-04-23 14:24:49 | [train_policy] epoch #971 | Computing loss before
2022-04-23 14:24:49 | [train_policy] epoch #971 | Computing KL before
2022-04-23 14:24:49 | [train_policy] epoch #971 | Optimizing
2022-04-23 14:24:49 | [train_policy] epoch #971 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:49 | [train_policy] epoch #971 | computing loss before
2022-04-23 14:24:49 | [train_policy] epoch #971 | computing gradient
2022-04-23 14:24:49 | [train_policy] epoch #971 | gradient computed
2022-04-23 14:24:49 | [train_policy] epoch #971 | computing descent direction
2022-04-23 14:24:49 | [train_policy] epoch #971 | descent direction computed
2022-04-23 14:24:49 | [train_policy] epoch #971 | backtrack iters: 0
2022-04-23 14:24:49 | [train_policy] epoch #971 | optimization finished
2022-04-23 14:24:49 | [train_policy] epoch #971 | Computing KL after
2022-04-23 14:24:49 | [train_policy] epoch #971 | Computing loss after
2022-04-23 14:24:49 | [train_policy] epoch #971 | Fitting baseline...
2022-04-23 14:24:49 | [train_policy] epoch #971 | Saving snapshot...
2022-04-23 14:24:49 | [train_policy] epoch #971 | Saved
2022-04-23 14:24:49 | [train_policy] epoch #971 | Time 336.93 s
2022-04-23 14:24:49 | [train_policy] epoch #971 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120054
Evaluation/AverageDiscountedReturn          -47.0844
Evaluation/AverageReturn                    -47.0844
Evaluation/CompletionRate                     0
Evaluation/Iteration                        971
Evaluation/MaxReturn                        -31.8483
Evaluation/MinReturn                       -580.987
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         56.58
Extras/EpisodeRewardMean                    -46.5628
LinearFeatureBaseline/ExplainedVariance       0.109705
PolicyExecTime                                0.0977778
ProcessExecTime                               0.0116718
TotalEnvSteps                            983664
policy/Entropy                               -2.38183
policy/KL                                     0.00991313
policy/KLBefore                               0
policy/LossAfter                             -0.0335388
policy/LossBefore                             2.54439e-08
policy/Perplexity                             0.0923817
policy/dLoss                                  0.0335388
---------------------------------------  ----------------
2022-04-23 14:24:49 | [train_policy] epoch #972 | Obtaining samples for iteration 972...
2022-04-23 14:24:49 | [train_policy] epoch #972 | Logging diagnostics...
2022-04-23 14:24:49 | [train_policy] epoch #972 | Optimizing policy...
2022-04-23 14:24:49 | [train_policy] epoch #972 | Computing loss before
2022-04-23 14:24:49 | [train_policy] epoch #972 | Computing KL before
2022-04-23 14:24:49 | [train_policy] epoch #972 | Optimizing
2022-04-23 14:24:49 | [train_policy] epoch #972 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:49 | [train_policy] epoch #972 | computing loss before
2022-04-23 14:24:49 | [train_policy] epoch #972 | computing gradient
2022-04-23 14:24:49 | [train_policy] epoch #972 | gradient computed
2022-04-23 14:24:49 | [train_policy] epoch #972 | computing descent direction
2022-04-23 14:24:49 | [train_policy] epoch #972 | descent direction computed
2022-04-23 14:24:49 | [train_policy] epoch #972 | backtrack iters: 0
2022-04-23 14:24:49 | [train_policy] epoch #972 | optimization finished
2022-04-23 14:24:49 | [train_policy] epoch #972 | Computing KL after
2022-04-23 14:24:49 | [train_policy] epoch #972 | Computing loss after
2022-04-23 14:24:49 | [train_policy] epoch #972 | Fitting baseline...
2022-04-23 14:24:49 | [train_policy] epoch #972 | Saving snapshot...
2022-04-23 14:24:49 | [train_policy] epoch #972 | Saved
2022-04-23 14:24:49 | [train_policy] epoch #972 | Time 337.25 s
2022-04-23 14:24:49 | [train_policy] epoch #972 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119635
Evaluation/AverageDiscountedReturn          -40.946
Evaluation/AverageReturn                    -40.946
Evaluation/CompletionRate                     0
Evaluation/Iteration                        972
Evaluation/MaxReturn                        -29.187
Evaluation/MinReturn                        -62.9632
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.05499
Extras/EpisodeRewardMean                    -41.1677
LinearFeatureBaseline/ExplainedVariance      -1.05372
PolicyExecTime                                0.092525
ProcessExecTime                               0.0112081
TotalEnvSteps                            984676
policy/Entropy                               -2.39356
policy/KL                                     0.00953449
policy/KLBefore                               0
policy/LossAfter                             -0.016619
policy/LossBefore                            -1.31931e-08
policy/Perplexity                             0.091304
policy/dLoss                                  0.016619
---------------------------------------  ----------------
2022-04-23 14:24:49 | [train_policy] epoch #973 | Obtaining samples for iteration 973...
2022-04-23 14:24:49 | [train_policy] epoch #973 | Logging diagnostics...
2022-04-23 14:24:49 | [train_policy] epoch #973 | Optimizing policy...
2022-04-23 14:24:49 | [train_policy] epoch #973 | Computing loss before
2022-04-23 14:24:49 | [train_policy] epoch #973 | Computing KL before
2022-04-23 14:24:49 | [train_policy] epoch #973 | Optimizing
2022-04-23 14:24:49 | [train_policy] epoch #973 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:49 | [train_policy] epoch #973 | computing loss before
2022-04-23 14:24:49 | [train_policy] epoch #973 | computing gradient
2022-04-23 14:24:49 | [train_policy] epoch #973 | gradient computed
2022-04-23 14:24:49 | [train_policy] epoch #973 | computing descent direction
2022-04-23 14:24:49 | [train_policy] epoch #973 | descent direction computed
2022-04-23 14:24:49 | [train_policy] epoch #973 | backtrack iters: 0
2022-04-23 14:24:49 | [train_policy] epoch #973 | optimization finished
2022-04-23 14:24:49 | [train_policy] epoch #973 | Computing KL after
2022-04-23 14:24:49 | [train_policy] epoch #973 | Computing loss after
2022-04-23 14:24:49 | [train_policy] epoch #973 | Fitting baseline...
2022-04-23 14:24:49 | [train_policy] epoch #973 | Saving snapshot...
2022-04-23 14:24:49 | [train_policy] epoch #973 | Saved
2022-04-23 14:24:49 | [train_policy] epoch #973 | Time 337.57 s
2022-04-23 14:24:49 | [train_policy] epoch #973 | EpochTime 0.32 s
---------------------------------------  ---------------
EnvExecTime                                   0.120096
Evaluation/AverageDiscountedReturn          -63.2471
Evaluation/AverageReturn                    -63.2471
Evaluation/CompletionRate                     0
Evaluation/Iteration                        973
Evaluation/MaxReturn                        -28.156
Evaluation/MinReturn                      -2063.04
Evaluation/NumTrajs                          92
Evaluation/StdReturn                        209.836
Extras/EpisodeRewardMean                    -61.4728
LinearFeatureBaseline/ExplainedVariance       0.0126321
PolicyExecTime                                0.0975173
ProcessExecTime                               0.0111802
TotalEnvSteps                            985688
policy/Entropy                               -2.40122
policy/KL                                     0.00986476
policy/KLBefore                               0
policy/LossAfter                             -0.0275452
policy/LossBefore                             2.8271e-09
policy/Perplexity                             0.0906076
policy/dLoss                                  0.0275452
---------------------------------------  ---------------
2022-04-23 14:24:49 | [train_policy] epoch #974 | Obtaining samples for iteration 974...
2022-04-23 14:24:50 | [train_policy] epoch #974 | Logging diagnostics...
2022-04-23 14:24:50 | [train_policy] epoch #974 | Optimizing policy...
2022-04-23 14:24:50 | [train_policy] epoch #974 | Computing loss before
2022-04-23 14:24:50 | [train_policy] epoch #974 | Computing KL before
2022-04-23 14:24:50 | [train_policy] epoch #974 | Optimizing
2022-04-23 14:24:50 | [train_policy] epoch #974 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:50 | [train_policy] epoch #974 | computing loss before
2022-04-23 14:24:50 | [train_policy] epoch #974 | computing gradient
2022-04-23 14:24:50 | [train_policy] epoch #974 | gradient computed
2022-04-23 14:24:50 | [train_policy] epoch #974 | computing descent direction
2022-04-23 14:24:50 | [train_policy] epoch #974 | descent direction computed
2022-04-23 14:24:50 | [train_policy] epoch #974 | backtrack iters: 1
2022-04-23 14:24:50 | [train_policy] epoch #974 | optimization finished
2022-04-23 14:24:50 | [train_policy] epoch #974 | Computing KL after
2022-04-23 14:24:50 | [train_policy] epoch #974 | Computing loss after
2022-04-23 14:24:50 | [train_policy] epoch #974 | Fitting baseline...
2022-04-23 14:24:50 | [train_policy] epoch #974 | Saving snapshot...
2022-04-23 14:24:50 | [train_policy] epoch #974 | Saved
2022-04-23 14:24:50 | [train_policy] epoch #974 | Time 337.90 s
2022-04-23 14:24:50 | [train_policy] epoch #974 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119726
Evaluation/AverageDiscountedReturn          -49.9714
Evaluation/AverageReturn                    -49.9714
Evaluation/CompletionRate                     0
Evaluation/Iteration                        974
Evaluation/MaxReturn                        -27.7848
Evaluation/MinReturn                       -825.926
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         81.8443
Extras/EpisodeRewardMean                    -49.5642
LinearFeatureBaseline/ExplainedVariance      -0.172213
PolicyExecTime                                0.0972445
ProcessExecTime                               0.0112183
TotalEnvSteps                            986700
policy/Entropy                               -2.4343
policy/KL                                     0.00646067
policy/KLBefore                               0
policy/LossAfter                             -0.0186389
policy/LossBefore                             6.83215e-09
policy/Perplexity                             0.0876589
policy/dLoss                                  0.0186389
---------------------------------------  ----------------
2022-04-23 14:24:50 | [train_policy] epoch #975 | Obtaining samples for iteration 975...
2022-04-23 14:24:50 | [train_policy] epoch #975 | Logging diagnostics...
2022-04-23 14:24:50 | [train_policy] epoch #975 | Optimizing policy...
2022-04-23 14:24:50 | [train_policy] epoch #975 | Computing loss before
2022-04-23 14:24:50 | [train_policy] epoch #975 | Computing KL before
2022-04-23 14:24:50 | [train_policy] epoch #975 | Optimizing
2022-04-23 14:24:50 | [train_policy] epoch #975 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:50 | [train_policy] epoch #975 | computing loss before
2022-04-23 14:24:50 | [train_policy] epoch #975 | computing gradient
2022-04-23 14:24:50 | [train_policy] epoch #975 | gradient computed
2022-04-23 14:24:50 | [train_policy] epoch #975 | computing descent direction
2022-04-23 14:24:50 | [train_policy] epoch #975 | descent direction computed
2022-04-23 14:24:50 | [train_policy] epoch #975 | backtrack iters: 1
2022-04-23 14:24:50 | [train_policy] epoch #975 | optimization finished
2022-04-23 14:24:50 | [train_policy] epoch #975 | Computing KL after
2022-04-23 14:24:50 | [train_policy] epoch #975 | Computing loss after
2022-04-23 14:24:50 | [train_policy] epoch #975 | Fitting baseline...
2022-04-23 14:24:50 | [train_policy] epoch #975 | Saving snapshot...
2022-04-23 14:24:50 | [train_policy] epoch #975 | Saved
2022-04-23 14:24:50 | [train_policy] epoch #975 | Time 338.23 s
2022-04-23 14:24:50 | [train_policy] epoch #975 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120427
Evaluation/AverageDiscountedReturn          -40.9905
Evaluation/AverageReturn                    -40.9905
Evaluation/CompletionRate                     0
Evaluation/Iteration                        975
Evaluation/MaxReturn                        -31.3965
Evaluation/MinReturn                        -74.1251
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.53299
Extras/EpisodeRewardMean                    -41.222
LinearFeatureBaseline/ExplainedVariance      -7.74119
PolicyExecTime                                0.0936017
ProcessExecTime                               0.0112011
TotalEnvSteps                            987712
policy/Entropy                               -2.43519
policy/KL                                     0.00649528
policy/KLBefore                               0
policy/LossAfter                             -0.0298086
policy/LossBefore                             6.12538e-09
policy/Perplexity                             0.0875815
policy/dLoss                                  0.0298086
---------------------------------------  ----------------
2022-04-23 14:24:50 | [train_policy] epoch #976 | Obtaining samples for iteration 976...
2022-04-23 14:24:50 | [train_policy] epoch #976 | Logging diagnostics...
2022-04-23 14:24:50 | [train_policy] epoch #976 | Optimizing policy...
2022-04-23 14:24:50 | [train_policy] epoch #976 | Computing loss before
2022-04-23 14:24:50 | [train_policy] epoch #976 | Computing KL before
2022-04-23 14:24:50 | [train_policy] epoch #976 | Optimizing
2022-04-23 14:24:50 | [train_policy] epoch #976 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:50 | [train_policy] epoch #976 | computing loss before
2022-04-23 14:24:50 | [train_policy] epoch #976 | computing gradient
2022-04-23 14:24:50 | [train_policy] epoch #976 | gradient computed
2022-04-23 14:24:50 | [train_policy] epoch #976 | computing descent direction
2022-04-23 14:24:50 | [train_policy] epoch #976 | descent direction computed
2022-04-23 14:24:50 | [train_policy] epoch #976 | backtrack iters: 0
2022-04-23 14:24:50 | [train_policy] epoch #976 | optimization finished
2022-04-23 14:24:50 | [train_policy] epoch #976 | Computing KL after
2022-04-23 14:24:50 | [train_policy] epoch #976 | Computing loss after
2022-04-23 14:24:50 | [train_policy] epoch #976 | Fitting baseline...
2022-04-23 14:24:50 | [train_policy] epoch #976 | Saving snapshot...
2022-04-23 14:24:50 | [train_policy] epoch #976 | Saved
2022-04-23 14:24:50 | [train_policy] epoch #976 | Time 338.56 s
2022-04-23 14:24:50 | [train_policy] epoch #976 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.121802
Evaluation/AverageDiscountedReturn          -41.2823
Evaluation/AverageReturn                    -41.2823
Evaluation/CompletionRate                     0
Evaluation/Iteration                        976
Evaluation/MaxReturn                        -28.4299
Evaluation/MinReturn                        -72.8501
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.90391
Extras/EpisodeRewardMean                    -40.9148
LinearFeatureBaseline/ExplainedVariance       0.805075
PolicyExecTime                                0.0984817
ProcessExecTime                               0.0112634
TotalEnvSteps                            988724
policy/Entropy                               -2.427
policy/KL                                     0.0097302
policy/KLBefore                               0
policy/LossAfter                             -0.0181477
policy/LossBefore                            -1.64914e-09
policy/Perplexity                             0.0883016
policy/dLoss                                  0.0181477
---------------------------------------  ----------------
2022-04-23 14:24:50 | [train_policy] epoch #977 | Obtaining samples for iteration 977...
2022-04-23 14:24:51 | [train_policy] epoch #977 | Logging diagnostics...
2022-04-23 14:24:51 | [train_policy] epoch #977 | Optimizing policy...
2022-04-23 14:24:51 | [train_policy] epoch #977 | Computing loss before
2022-04-23 14:24:51 | [train_policy] epoch #977 | Computing KL before
2022-04-23 14:24:51 | [train_policy] epoch #977 | Optimizing
2022-04-23 14:24:51 | [train_policy] epoch #977 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:51 | [train_policy] epoch #977 | computing loss before
2022-04-23 14:24:51 | [train_policy] epoch #977 | computing gradient
2022-04-23 14:24:51 | [train_policy] epoch #977 | gradient computed
2022-04-23 14:24:51 | [train_policy] epoch #977 | computing descent direction
2022-04-23 14:24:51 | [train_policy] epoch #977 | descent direction computed
2022-04-23 14:24:51 | [train_policy] epoch #977 | backtrack iters: 1
2022-04-23 14:24:51 | [train_policy] epoch #977 | optimization finished
2022-04-23 14:24:51 | [train_policy] epoch #977 | Computing KL after
2022-04-23 14:24:51 | [train_policy] epoch #977 | Computing loss after
2022-04-23 14:24:51 | [train_policy] epoch #977 | Fitting baseline...
2022-04-23 14:24:51 | [train_policy] epoch #977 | Saving snapshot...
2022-04-23 14:24:51 | [train_policy] epoch #977 | Saved
2022-04-23 14:24:51 | [train_policy] epoch #977 | Time 338.89 s
2022-04-23 14:24:51 | [train_policy] epoch #977 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120289
Evaluation/AverageDiscountedReturn          -40.5474
Evaluation/AverageReturn                    -40.5474
Evaluation/CompletionRate                     0
Evaluation/Iteration                        977
Evaluation/MaxReturn                        -28.7494
Evaluation/MinReturn                        -74.0358
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.16985
Extras/EpisodeRewardMean                    -40.2942
LinearFeatureBaseline/ExplainedVariance       0.845728
PolicyExecTime                                0.101105
ProcessExecTime                               0.0115638
TotalEnvSteps                            989736
policy/Entropy                               -2.44783
policy/KL                                     0.00670044
policy/KLBefore                               0
policy/LossAfter                             -0.0194132
policy/LossBefore                             1.13084e-08
policy/Perplexity                             0.0864807
policy/dLoss                                  0.0194132
---------------------------------------  ----------------
2022-04-23 14:24:51 | [train_policy] epoch #978 | Obtaining samples for iteration 978...
2022-04-23 14:24:51 | [train_policy] epoch #978 | Logging diagnostics...
2022-04-23 14:24:51 | [train_policy] epoch #978 | Optimizing policy...
2022-04-23 14:24:51 | [train_policy] epoch #978 | Computing loss before
2022-04-23 14:24:51 | [train_policy] epoch #978 | Computing KL before
2022-04-23 14:24:51 | [train_policy] epoch #978 | Optimizing
2022-04-23 14:24:51 | [train_policy] epoch #978 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:51 | [train_policy] epoch #978 | computing loss before
2022-04-23 14:24:51 | [train_policy] epoch #978 | computing gradient
2022-04-23 14:24:51 | [train_policy] epoch #978 | gradient computed
2022-04-23 14:24:51 | [train_policy] epoch #978 | computing descent direction
2022-04-23 14:24:51 | [train_policy] epoch #978 | descent direction computed
2022-04-23 14:24:51 | [train_policy] epoch #978 | backtrack iters: 0
2022-04-23 14:24:51 | [train_policy] epoch #978 | optimization finished
2022-04-23 14:24:51 | [train_policy] epoch #978 | Computing KL after
2022-04-23 14:24:51 | [train_policy] epoch #978 | Computing loss after
2022-04-23 14:24:51 | [train_policy] epoch #978 | Fitting baseline...
2022-04-23 14:24:51 | [train_policy] epoch #978 | Saving snapshot...
2022-04-23 14:24:51 | [train_policy] epoch #978 | Saved
2022-04-23 14:24:51 | [train_policy] epoch #978 | Time 339.22 s
2022-04-23 14:24:51 | [train_policy] epoch #978 | EpochTime 0.33 s
---------------------------------------  ---------------
EnvExecTime                                   0.120453
Evaluation/AverageDiscountedReturn          -39.0988
Evaluation/AverageReturn                    -39.0988
Evaluation/CompletionRate                     0
Evaluation/Iteration                        978
Evaluation/MaxReturn                        -28.5784
Evaluation/MinReturn                        -76.418
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.93738
Extras/EpisodeRewardMean                    -39.223
LinearFeatureBaseline/ExplainedVariance       0.835645
PolicyExecTime                                0.0996873
ProcessExecTime                               0.0111656
TotalEnvSteps                            990748
policy/Entropy                               -2.43066
policy/KL                                     0.00979294
policy/KLBefore                               0
policy/LossAfter                             -0.0216309
policy/LossBefore                            -2.8271e-09
policy/Perplexity                             0.0879783
policy/dLoss                                  0.0216309
---------------------------------------  ---------------
2022-04-23 14:24:51 | [train_policy] epoch #979 | Obtaining samples for iteration 979...
2022-04-23 14:24:51 | [train_policy] epoch #979 | Logging diagnostics...
2022-04-23 14:24:51 | [train_policy] epoch #979 | Optimizing policy...
2022-04-23 14:24:51 | [train_policy] epoch #979 | Computing loss before
2022-04-23 14:24:51 | [train_policy] epoch #979 | Computing KL before
2022-04-23 14:24:51 | [train_policy] epoch #979 | Optimizing
2022-04-23 14:24:51 | [train_policy] epoch #979 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:51 | [train_policy] epoch #979 | computing loss before
2022-04-23 14:24:51 | [train_policy] epoch #979 | computing gradient
2022-04-23 14:24:51 | [train_policy] epoch #979 | gradient computed
2022-04-23 14:24:51 | [train_policy] epoch #979 | computing descent direction
2022-04-23 14:24:51 | [train_policy] epoch #979 | descent direction computed
2022-04-23 14:24:51 | [train_policy] epoch #979 | backtrack iters: 1
2022-04-23 14:24:51 | [train_policy] epoch #979 | optimization finished
2022-04-23 14:24:51 | [train_policy] epoch #979 | Computing KL after
2022-04-23 14:24:51 | [train_policy] epoch #979 | Computing loss after
2022-04-23 14:24:51 | [train_policy] epoch #979 | Fitting baseline...
2022-04-23 14:24:51 | [train_policy] epoch #979 | Saving snapshot...
2022-04-23 14:24:51 | [train_policy] epoch #979 | Saved
2022-04-23 14:24:51 | [train_policy] epoch #979 | Time 339.56 s
2022-04-23 14:24:51 | [train_policy] epoch #979 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120608
Evaluation/AverageDiscountedReturn          -39.2263
Evaluation/AverageReturn                    -39.2263
Evaluation/CompletionRate                     0
Evaluation/Iteration                        979
Evaluation/MaxReturn                        -28.8193
Evaluation/MinReturn                        -63.6021
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.31044
Extras/EpisodeRewardMean                    -39.2607
LinearFeatureBaseline/ExplainedVariance       0.903741
PolicyExecTime                                0.099612
ProcessExecTime                               0.0113919
TotalEnvSteps                            991760
policy/Entropy                               -2.43169
policy/KL                                     0.00653396
policy/KLBefore                               0
policy/LossAfter                             -0.00920141
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             0.0878878
policy/dLoss                                  0.0092014
---------------------------------------  ----------------
2022-04-23 14:24:51 | [train_policy] epoch #980 | Obtaining samples for iteration 980...
2022-04-23 14:24:52 | [train_policy] epoch #980 | Logging diagnostics...
2022-04-23 14:24:52 | [train_policy] epoch #980 | Optimizing policy...
2022-04-23 14:24:52 | [train_policy] epoch #980 | Computing loss before
2022-04-23 14:24:52 | [train_policy] epoch #980 | Computing KL before
2022-04-23 14:24:52 | [train_policy] epoch #980 | Optimizing
2022-04-23 14:24:52 | [train_policy] epoch #980 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:52 | [train_policy] epoch #980 | computing loss before
2022-04-23 14:24:52 | [train_policy] epoch #980 | computing gradient
2022-04-23 14:24:52 | [train_policy] epoch #980 | gradient computed
2022-04-23 14:24:52 | [train_policy] epoch #980 | computing descent direction
2022-04-23 14:24:52 | [train_policy] epoch #980 | descent direction computed
2022-04-23 14:24:52 | [train_policy] epoch #980 | backtrack iters: 0
2022-04-23 14:24:52 | [train_policy] epoch #980 | optimization finished
2022-04-23 14:24:52 | [train_policy] epoch #980 | Computing KL after
2022-04-23 14:24:52 | [train_policy] epoch #980 | Computing loss after
2022-04-23 14:24:52 | [train_policy] epoch #980 | Fitting baseline...
2022-04-23 14:24:52 | [train_policy] epoch #980 | Saving snapshot...
2022-04-23 14:24:52 | [train_policy] epoch #980 | Saved
2022-04-23 14:24:52 | [train_policy] epoch #980 | Time 339.88 s
2022-04-23 14:24:52 | [train_policy] epoch #980 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119645
Evaluation/AverageDiscountedReturn          -40.7875
Evaluation/AverageReturn                    -40.7875
Evaluation/CompletionRate                     0
Evaluation/Iteration                        980
Evaluation/MaxReturn                        -30.0864
Evaluation/MinReturn                        -73.9209
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.1374
Extras/EpisodeRewardMean                    -40.4989
LinearFeatureBaseline/ExplainedVariance       0.864618
PolicyExecTime                                0.0978756
ProcessExecTime                               0.011215
TotalEnvSteps                            992772
policy/Entropy                               -2.42511
policy/KL                                     0.00995323
policy/KLBefore                               0
policy/LossAfter                             -0.0148274
policy/LossBefore                             2.59151e-09
policy/Perplexity                             0.0884682
policy/dLoss                                  0.0148274
---------------------------------------  ----------------
2022-04-23 14:24:52 | [train_policy] epoch #981 | Obtaining samples for iteration 981...
2022-04-23 14:24:52 | [train_policy] epoch #981 | Logging diagnostics...
2022-04-23 14:24:52 | [train_policy] epoch #981 | Optimizing policy...
2022-04-23 14:24:52 | [train_policy] epoch #981 | Computing loss before
2022-04-23 14:24:52 | [train_policy] epoch #981 | Computing KL before
2022-04-23 14:24:52 | [train_policy] epoch #981 | Optimizing
2022-04-23 14:24:52 | [train_policy] epoch #981 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:52 | [train_policy] epoch #981 | computing loss before
2022-04-23 14:24:52 | [train_policy] epoch #981 | computing gradient
2022-04-23 14:24:52 | [train_policy] epoch #981 | gradient computed
2022-04-23 14:24:52 | [train_policy] epoch #981 | computing descent direction
2022-04-23 14:24:52 | [train_policy] epoch #981 | descent direction computed
2022-04-23 14:24:52 | [train_policy] epoch #981 | backtrack iters: 0
2022-04-23 14:24:52 | [train_policy] epoch #981 | optimization finished
2022-04-23 14:24:52 | [train_policy] epoch #981 | Computing KL after
2022-04-23 14:24:52 | [train_policy] epoch #981 | Computing loss after
2022-04-23 14:24:52 | [train_policy] epoch #981 | Fitting baseline...
2022-04-23 14:24:52 | [train_policy] epoch #981 | Saving snapshot...
2022-04-23 14:24:52 | [train_policy] epoch #981 | Saved
2022-04-23 14:24:52 | [train_policy] epoch #981 | Time 340.21 s
2022-04-23 14:24:52 | [train_policy] epoch #981 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.120736
Evaluation/AverageDiscountedReturn          -40.7683
Evaluation/AverageReturn                    -40.7683
Evaluation/CompletionRate                     0
Evaluation/Iteration                        981
Evaluation/MaxReturn                        -30.232
Evaluation/MinReturn                        -74.27
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.6083
Extras/EpisodeRewardMean                    -40.7311
LinearFeatureBaseline/ExplainedVariance       0.815256
PolicyExecTime                                0.095746
ProcessExecTime                               0.0114238
TotalEnvSteps                            993784
policy/Entropy                               -2.44047
policy/KL                                     0.00996736
policy/KLBefore                               0
policy/LossAfter                             -0.0207108
policy/LossBefore                             9.42366e-10
policy/Perplexity                             0.0871199
policy/dLoss                                  0.0207108
---------------------------------------  ----------------
2022-04-23 14:24:52 | [train_policy] epoch #982 | Obtaining samples for iteration 982...
2022-04-23 14:24:52 | [train_policy] epoch #982 | Logging diagnostics...
2022-04-23 14:24:52 | [train_policy] epoch #982 | Optimizing policy...
2022-04-23 14:24:52 | [train_policy] epoch #982 | Computing loss before
2022-04-23 14:24:52 | [train_policy] epoch #982 | Computing KL before
2022-04-23 14:24:52 | [train_policy] epoch #982 | Optimizing
2022-04-23 14:24:52 | [train_policy] epoch #982 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:52 | [train_policy] epoch #982 | computing loss before
2022-04-23 14:24:52 | [train_policy] epoch #982 | computing gradient
2022-04-23 14:24:52 | [train_policy] epoch #982 | gradient computed
2022-04-23 14:24:52 | [train_policy] epoch #982 | computing descent direction
2022-04-23 14:24:52 | [train_policy] epoch #982 | descent direction computed
2022-04-23 14:24:52 | [train_policy] epoch #982 | backtrack iters: 1
2022-04-23 14:24:52 | [train_policy] epoch #982 | optimization finished
2022-04-23 14:24:52 | [train_policy] epoch #982 | Computing KL after
2022-04-23 14:24:52 | [train_policy] epoch #982 | Computing loss after
2022-04-23 14:24:52 | [train_policy] epoch #982 | Fitting baseline...
2022-04-23 14:24:52 | [train_policy] epoch #982 | Saving snapshot...
2022-04-23 14:24:52 | [train_policy] epoch #982 | Saved
2022-04-23 14:24:52 | [train_policy] epoch #982 | Time 340.54 s
2022-04-23 14:24:52 | [train_policy] epoch #982 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.120126
Evaluation/AverageDiscountedReturn          -41.4194
Evaluation/AverageReturn                    -41.4194
Evaluation/CompletionRate                     0
Evaluation/Iteration                        982
Evaluation/MaxReturn                        -29.3978
Evaluation/MinReturn                        -74.7681
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.84458
Extras/EpisodeRewardMean                    -41.0753
LinearFeatureBaseline/ExplainedVariance       0.857335
PolicyExecTime                                0.0969815
ProcessExecTime                               0.0112054
TotalEnvSteps                            994796
policy/Entropy                               -2.45786
policy/KL                                     0.00652794
policy/KLBefore                               0
policy/LossAfter                             -0.0132766
policy/LossBefore                            -4.94742e-09
policy/Perplexity                             0.085618
policy/dLoss                                  0.0132766
---------------------------------------  ----------------
2022-04-23 14:24:52 | [train_policy] epoch #983 | Obtaining samples for iteration 983...
2022-04-23 14:24:53 | [train_policy] epoch #983 | Logging diagnostics...
2022-04-23 14:24:53 | [train_policy] epoch #983 | Optimizing policy...
2022-04-23 14:24:53 | [train_policy] epoch #983 | Computing loss before
2022-04-23 14:24:53 | [train_policy] epoch #983 | Computing KL before
2022-04-23 14:24:53 | [train_policy] epoch #983 | Optimizing
2022-04-23 14:24:53 | [train_policy] epoch #983 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:53 | [train_policy] epoch #983 | computing loss before
2022-04-23 14:24:53 | [train_policy] epoch #983 | computing gradient
2022-04-23 14:24:53 | [train_policy] epoch #983 | gradient computed
2022-04-23 14:24:53 | [train_policy] epoch #983 | computing descent direction
2022-04-23 14:24:53 | [train_policy] epoch #983 | descent direction computed
2022-04-23 14:24:53 | [train_policy] epoch #983 | backtrack iters: 1
2022-04-23 14:24:53 | [train_policy] epoch #983 | optimization finished
2022-04-23 14:24:53 | [train_policy] epoch #983 | Computing KL after
2022-04-23 14:24:53 | [train_policy] epoch #983 | Computing loss after
2022-04-23 14:24:53 | [train_policy] epoch #983 | Fitting baseline...
2022-04-23 14:24:53 | [train_policy] epoch #983 | Saving snapshot...
2022-04-23 14:24:53 | [train_policy] epoch #983 | Saved
2022-04-23 14:24:53 | [train_policy] epoch #983 | Time 340.86 s
2022-04-23 14:24:53 | [train_policy] epoch #983 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.1197
Evaluation/AverageDiscountedReturn          -41.857
Evaluation/AverageReturn                    -41.857
Evaluation/CompletionRate                     0
Evaluation/Iteration                        983
Evaluation/MaxReturn                        -28.8227
Evaluation/MinReturn                       -108.023
Evaluation/NumTrajs                          92
Evaluation/StdReturn                         11.1899
Extras/EpisodeRewardMean                    -41.59
LinearFeatureBaseline/ExplainedVariance       0.735831
PolicyExecTime                                0.0967684
ProcessExecTime                               0.0112913
TotalEnvSteps                            995808
policy/Entropy                               -2.48285
policy/KL                                     0.00651582
policy/KLBefore                               0
policy/LossAfter                             -0.0168058
policy/LossBefore                             9.89484e-09
policy/Perplexity                             0.0835046
policy/dLoss                                  0.0168058
---------------------------------------  ----------------
2022-04-23 14:24:53 | [train_policy] epoch #984 | Obtaining samples for iteration 984...
2022-04-23 14:24:53 | [train_policy] epoch #984 | Logging diagnostics...
2022-04-23 14:24:53 | [train_policy] epoch #984 | Optimizing policy...
2022-04-23 14:24:53 | [train_policy] epoch #984 | Computing loss before
2022-04-23 14:24:53 | [train_policy] epoch #984 | Computing KL before
2022-04-23 14:24:53 | [train_policy] epoch #984 | Optimizing
2022-04-23 14:24:53 | [train_policy] epoch #984 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:53 | [train_policy] epoch #984 | computing loss before
2022-04-23 14:24:53 | [train_policy] epoch #984 | computing gradient
2022-04-23 14:24:53 | [train_policy] epoch #984 | gradient computed
2022-04-23 14:24:53 | [train_policy] epoch #984 | computing descent direction
2022-04-23 14:24:53 | [train_policy] epoch #984 | descent direction computed
2022-04-23 14:24:53 | [train_policy] epoch #984 | backtrack iters: 0
2022-04-23 14:24:53 | [train_policy] epoch #984 | optimization finished
2022-04-23 14:24:53 | [train_policy] epoch #984 | Computing KL after
2022-04-23 14:24:53 | [train_policy] epoch #984 | Computing loss after
2022-04-23 14:24:53 | [train_policy] epoch #984 | Fitting baseline...
2022-04-23 14:24:53 | [train_policy] epoch #984 | Saving snapshot...
2022-04-23 14:24:53 | [train_policy] epoch #984 | Saved
2022-04-23 14:24:53 | [train_policy] epoch #984 | Time 341.19 s
2022-04-23 14:24:53 | [train_policy] epoch #984 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119024
Evaluation/AverageDiscountedReturn          -40.0265
Evaluation/AverageReturn                    -40.0265
Evaluation/CompletionRate                     0
Evaluation/Iteration                        984
Evaluation/MaxReturn                        -28.7798
Evaluation/MinReturn                        -72.3893
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          8.66215
Extras/EpisodeRewardMean                    -40.1542
LinearFeatureBaseline/ExplainedVariance       0.843501
PolicyExecTime                                0.0971775
ProcessExecTime                               0.0113232
TotalEnvSteps                            996820
policy/Entropy                               -2.47468
policy/KL                                     0.00961443
policy/KLBefore                               0
policy/LossAfter                             -0.0196835
policy/LossBefore                            -1.46361e-08
policy/Perplexity                             0.0841896
policy/dLoss                                  0.0196835
---------------------------------------  ----------------
2022-04-23 14:24:53 | [train_policy] epoch #985 | Obtaining samples for iteration 985...
2022-04-23 14:24:53 | [train_policy] epoch #985 | Logging diagnostics...
2022-04-23 14:24:53 | [train_policy] epoch #985 | Optimizing policy...
2022-04-23 14:24:53 | [train_policy] epoch #985 | Computing loss before
2022-04-23 14:24:53 | [train_policy] epoch #985 | Computing KL before
2022-04-23 14:24:53 | [train_policy] epoch #985 | Optimizing
2022-04-23 14:24:53 | [train_policy] epoch #985 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:53 | [train_policy] epoch #985 | computing loss before
2022-04-23 14:24:53 | [train_policy] epoch #985 | computing gradient
2022-04-23 14:24:53 | [train_policy] epoch #985 | gradient computed
2022-04-23 14:24:53 | [train_policy] epoch #985 | computing descent direction
2022-04-23 14:24:53 | [train_policy] epoch #985 | descent direction computed
2022-04-23 14:24:53 | [train_policy] epoch #985 | backtrack iters: 1
2022-04-23 14:24:53 | [train_policy] epoch #985 | optimization finished
2022-04-23 14:24:53 | [train_policy] epoch #985 | Computing KL after
2022-04-23 14:24:53 | [train_policy] epoch #985 | Computing loss after
2022-04-23 14:24:53 | [train_policy] epoch #985 | Fitting baseline...
2022-04-23 14:24:53 | [train_policy] epoch #985 | Saving snapshot...
2022-04-23 14:24:53 | [train_policy] epoch #985 | Saved
2022-04-23 14:24:53 | [train_policy] epoch #985 | Time 341.52 s
2022-04-23 14:24:53 | [train_policy] epoch #985 | EpochTime 0.32 s
---------------------------------------  ----------------
EnvExecTime                                   0.119513
Evaluation/AverageDiscountedReturn          -41.127
Evaluation/AverageReturn                    -41.127
Evaluation/CompletionRate                     0
Evaluation/Iteration                        985
Evaluation/MaxReturn                        -31.3197
Evaluation/MinReturn                        -63.7162
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          7.73983
Extras/EpisodeRewardMean                    -40.8931
LinearFeatureBaseline/ExplainedVariance       0.870335
PolicyExecTime                                0.0952163
ProcessExecTime                               0.0112333
TotalEnvSteps                            997832
policy/Entropy                               -2.46237
policy/KL                                     0.00649812
policy/KLBefore                               0
policy/LossAfter                             -0.0216896
policy/LossBefore                            -4.71183e-09
policy/Perplexity                             0.0852323
policy/dLoss                                  0.0216896
---------------------------------------  ----------------
2022-04-23 14:24:53 | [train_policy] epoch #986 | Obtaining samples for iteration 986...
2022-04-23 14:24:54 | [train_policy] epoch #986 | Logging diagnostics...
2022-04-23 14:24:54 | [train_policy] epoch #986 | Optimizing policy...
2022-04-23 14:24:54 | [train_policy] epoch #986 | Computing loss before
2022-04-23 14:24:54 | [train_policy] epoch #986 | Computing KL before
2022-04-23 14:24:54 | [train_policy] epoch #986 | Optimizing
2022-04-23 14:24:54 | [train_policy] epoch #986 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:54 | [train_policy] epoch #986 | computing loss before
2022-04-23 14:24:54 | [train_policy] epoch #986 | computing gradient
2022-04-23 14:24:54 | [train_policy] epoch #986 | gradient computed
2022-04-23 14:24:54 | [train_policy] epoch #986 | computing descent direction
2022-04-23 14:24:54 | [train_policy] epoch #986 | descent direction computed
2022-04-23 14:24:54 | [train_policy] epoch #986 | backtrack iters: 0
2022-04-23 14:24:54 | [train_policy] epoch #986 | optimization finished
2022-04-23 14:24:54 | [train_policy] epoch #986 | Computing KL after
2022-04-23 14:24:54 | [train_policy] epoch #986 | Computing loss after
2022-04-23 14:24:54 | [train_policy] epoch #986 | Fitting baseline...
2022-04-23 14:24:54 | [train_policy] epoch #986 | Saving snapshot...
2022-04-23 14:24:54 | [train_policy] epoch #986 | Saved
2022-04-23 14:24:54 | [train_policy] epoch #986 | Time 341.85 s
2022-04-23 14:24:54 | [train_policy] epoch #986 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.121404
Evaluation/AverageDiscountedReturn          -40.3969
Evaluation/AverageReturn                    -40.3969
Evaluation/CompletionRate                     0
Evaluation/Iteration                        986
Evaluation/MaxReturn                        -29.3129
Evaluation/MinReturn                        -74.114
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.13106
Extras/EpisodeRewardMean                    -40.1943
LinearFeatureBaseline/ExplainedVariance       0.838339
PolicyExecTime                                0.0992205
ProcessExecTime                               0.0116923
TotalEnvSteps                            998844
policy/Entropy                               -2.44422
policy/KL                                     0.00986019
policy/KLBefore                               0
policy/LossAfter                             -0.0254541
policy/LossBefore                             1.41355e-09
policy/Perplexity                             0.0867935
policy/dLoss                                  0.0254541
---------------------------------------  ----------------
2022-04-23 14:24:54 | [train_policy] epoch #987 | Obtaining samples for iteration 987...
2022-04-23 14:24:54 | [train_policy] epoch #987 | Logging diagnostics...
2022-04-23 14:24:54 | [train_policy] epoch #987 | Optimizing policy...
2022-04-23 14:24:54 | [train_policy] epoch #987 | Computing loss before
2022-04-23 14:24:54 | [train_policy] epoch #987 | Computing KL before
2022-04-23 14:24:54 | [train_policy] epoch #987 | Optimizing
2022-04-23 14:24:54 | [train_policy] epoch #987 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:54 | [train_policy] epoch #987 | computing loss before
2022-04-23 14:24:54 | [train_policy] epoch #987 | computing gradient
2022-04-23 14:24:54 | [train_policy] epoch #987 | gradient computed
2022-04-23 14:24:54 | [train_policy] epoch #987 | computing descent direction
2022-04-23 14:24:54 | [train_policy] epoch #987 | descent direction computed
2022-04-23 14:24:54 | [train_policy] epoch #987 | backtrack iters: 1
2022-04-23 14:24:54 | [train_policy] epoch #987 | optimization finished
2022-04-23 14:24:54 | [train_policy] epoch #987 | Computing KL after
2022-04-23 14:24:54 | [train_policy] epoch #987 | Computing loss after
2022-04-23 14:24:54 | [train_policy] epoch #987 | Fitting baseline...
2022-04-23 14:24:54 | [train_policy] epoch #987 | Saving snapshot...
2022-04-23 14:24:54 | [train_policy] epoch #987 | Saved
2022-04-23 14:24:54 | [train_policy] epoch #987 | Time 342.18 s
2022-04-23 14:24:54 | [train_policy] epoch #987 | EpochTime 0.33 s
---------------------------------------  ----------------
EnvExecTime                                   0.119459
Evaluation/AverageDiscountedReturn          -41.3649
Evaluation/AverageReturn                    -41.3649
Evaluation/CompletionRate                     0
Evaluation/Iteration                        987
Evaluation/MaxReturn                        -28.8244
Evaluation/MinReturn                        -78.0042
Evaluation/NumTrajs                          92
Evaluation/StdReturn                          9.53657
Extras/EpisodeRewardMean                    -41.2646
LinearFeatureBaseline/ExplainedVariance       0.836953
PolicyExecTime                                0.0964682
ProcessExecTime                               0.011384
TotalEnvSteps                            999856
policy/Entropy                               -2.46304
policy/KL                                     0.00650375
policy/KLBefore                               0
policy/LossAfter                             -0.0210268
policy/LossBefore                            -7.30334e-09
policy/Perplexity                             0.0851759
policy/dLoss                                  0.0210268
---------------------------------------  ----------------
2022-04-23 14:24:54 | [train_policy] epoch #988 | Obtaining samples for iteration 988...
2022-04-23 14:24:54 | [train_policy] epoch #988 | Logging diagnostics...
2022-04-23 14:24:54 | [train_policy] epoch #988 | Optimizing policy...
2022-04-23 14:24:54 | [train_policy] epoch #988 | Computing loss before
2022-04-23 14:24:54 | [train_policy] epoch #988 | Computing KL before
2022-04-23 14:24:54 | [train_policy] epoch #988 | Optimizing
2022-04-23 14:24:54 | [train_policy] epoch #988 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:54 | [train_policy] epoch #988 | computing loss before
2022-04-23 14:24:54 | [train_policy] epoch #988 | computing gradient
2022-04-23 14:24:54 | [train_policy] epoch #988 | gradient computed
2022-04-23 14:24:54 | [train_policy] epoch #988 | computing descent direction
2022-04-23 14:24:54 | [train_policy] epoch #988 | descent direction computed
2022-04-23 14:24:54 | [train_policy] epoch #988 | backtrack iters: 1
2022-04-23 14:24:54 | [train_policy] epoch #988 | optimization finished
2022-04-23 14:24:54 | [train_policy] epoch #988 | Computing KL after
2022-04-23 14:24:54 | [train_policy] epoch #988 | Computing loss after
2022-04-23 14:24:54 | [train_policy] epoch #988 | Fitting baseline...
2022-04-23 14:24:54 | [train_policy] epoch #988 | Saving snapshot...
2022-04-23 14:24:54 | [train_policy] epoch #988 | Saved
2022-04-23 14:24:54 | [train_policy] epoch #988 | Time 342.51 s
2022-04-23 14:24:54 | [train_policy] epoch #988 | EpochTime 0.33 s
---------------------------------------  -------------
EnvExecTime                                0.119293
Evaluation/AverageDiscountedReturn       -40.0126
Evaluation/AverageReturn                 -40.0126
Evaluation/CompletionRate                  0
Evaluation/Iteration                     988
Evaluation/MaxReturn                     -29.0904
Evaluation/MinReturn                     -63.7833
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.55997
Extras/EpisodeRewardMean                 -39.9755
LinearFeatureBaseline/ExplainedVariance    0.892837
PolicyExecTime                             0.0981092
ProcessExecTime                            0.0111926
TotalEnvSteps                              1.00087e+06
policy/Entropy                            -2.51306
policy/KL                                  0.00668758
policy/KLBefore                            0
policy/LossAfter                          -0.0150308
policy/LossBefore                         -5.88979e-09
policy/Perplexity                          0.0810199
policy/dLoss                               0.0150308
---------------------------------------  -------------
2022-04-23 14:24:54 | [train_policy] epoch #989 | Obtaining samples for iteration 989...
2022-04-23 14:24:55 | [train_policy] epoch #989 | Logging diagnostics...
2022-04-23 14:24:55 | [train_policy] epoch #989 | Optimizing policy...
2022-04-23 14:24:55 | [train_policy] epoch #989 | Computing loss before
2022-04-23 14:24:55 | [train_policy] epoch #989 | Computing KL before
2022-04-23 14:24:55 | [train_policy] epoch #989 | Optimizing
2022-04-23 14:24:55 | [train_policy] epoch #989 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:55 | [train_policy] epoch #989 | computing loss before
2022-04-23 14:24:55 | [train_policy] epoch #989 | computing gradient
2022-04-23 14:24:55 | [train_policy] epoch #989 | gradient computed
2022-04-23 14:24:55 | [train_policy] epoch #989 | computing descent direction
2022-04-23 14:24:55 | [train_policy] epoch #989 | descent direction computed
2022-04-23 14:24:55 | [train_policy] epoch #989 | backtrack iters: 1
2022-04-23 14:24:55 | [train_policy] epoch #989 | optimization finished
2022-04-23 14:24:55 | [train_policy] epoch #989 | Computing KL after
2022-04-23 14:24:55 | [train_policy] epoch #989 | Computing loss after
2022-04-23 14:24:55 | [train_policy] epoch #989 | Fitting baseline...
2022-04-23 14:24:55 | [train_policy] epoch #989 | Saving snapshot...
2022-04-23 14:24:55 | [train_policy] epoch #989 | Saved
2022-04-23 14:24:55 | [train_policy] epoch #989 | Time 342.84 s
2022-04-23 14:24:55 | [train_policy] epoch #989 | EpochTime 0.32 s
---------------------------------------  -------------
EnvExecTime                                0.119176
Evaluation/AverageDiscountedReturn       -41.1516
Evaluation/AverageReturn                 -41.1516
Evaluation/CompletionRate                  0
Evaluation/Iteration                     989
Evaluation/MaxReturn                     -29.2304
Evaluation/MinReturn                     -69.1704
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.11865
Extras/EpisodeRewardMean                 -41.4363
LinearFeatureBaseline/ExplainedVariance    0.874765
PolicyExecTime                             0.0938392
ProcessExecTime                            0.0111148
TotalEnvSteps                              1.00188e+06
policy/Entropy                            -2.52073
policy/KL                                  0.00657618
policy/KLBefore                            0
policy/LossAfter                          -0.0135176
policy/LossBefore                          2.07321e-08
policy/Perplexity                          0.0804011
policy/dLoss                               0.0135176
---------------------------------------  -------------
2022-04-23 14:24:55 | [train_policy] epoch #990 | Obtaining samples for iteration 990...
2022-04-23 14:24:55 | [train_policy] epoch #990 | Logging diagnostics...
2022-04-23 14:24:55 | [train_policy] epoch #990 | Optimizing policy...
2022-04-23 14:24:55 | [train_policy] epoch #990 | Computing loss before
2022-04-23 14:24:55 | [train_policy] epoch #990 | Computing KL before
2022-04-23 14:24:55 | [train_policy] epoch #990 | Optimizing
2022-04-23 14:24:55 | [train_policy] epoch #990 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:55 | [train_policy] epoch #990 | computing loss before
2022-04-23 14:24:55 | [train_policy] epoch #990 | computing gradient
2022-04-23 14:24:55 | [train_policy] epoch #990 | gradient computed
2022-04-23 14:24:55 | [train_policy] epoch #990 | computing descent direction
2022-04-23 14:24:55 | [train_policy] epoch #990 | descent direction computed
2022-04-23 14:24:55 | [train_policy] epoch #990 | backtrack iters: 1
2022-04-23 14:24:55 | [train_policy] epoch #990 | optimization finished
2022-04-23 14:24:55 | [train_policy] epoch #990 | Computing KL after
2022-04-23 14:24:55 | [train_policy] epoch #990 | Computing loss after
2022-04-23 14:24:55 | [train_policy] epoch #990 | Fitting baseline...
2022-04-23 14:24:55 | [train_policy] epoch #990 | Saving snapshot...
2022-04-23 14:24:55 | [train_policy] epoch #990 | Saved
2022-04-23 14:24:55 | [train_policy] epoch #990 | Time 343.18 s
2022-04-23 14:24:55 | [train_policy] epoch #990 | EpochTime 0.33 s
---------------------------------------  -------------
EnvExecTime                                0.120113
Evaluation/AverageDiscountedReturn       -40.5929
Evaluation/AverageReturn                 -40.5929
Evaluation/CompletionRate                  0
Evaluation/Iteration                     990
Evaluation/MaxReturn                     -28.7233
Evaluation/MinReturn                     -75.1072
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.48109
Extras/EpisodeRewardMean                 -40.7961
LinearFeatureBaseline/ExplainedVariance    0.859497
PolicyExecTime                             0.102543
ProcessExecTime                            0.0111294
TotalEnvSteps                              1.00289e+06
policy/Entropy                            -2.54208
policy/KL                                  0.00652152
policy/KLBefore                            0
policy/LossAfter                          -0.0124841
policy/LossBefore                          2.47371e-09
policy/Perplexity                          0.0787023
policy/dLoss                               0.0124841
---------------------------------------  -------------
2022-04-23 14:24:55 | [train_policy] epoch #991 | Obtaining samples for iteration 991...
2022-04-23 14:24:55 | [train_policy] epoch #991 | Logging diagnostics...
2022-04-23 14:24:55 | [train_policy] epoch #991 | Optimizing policy...
2022-04-23 14:24:55 | [train_policy] epoch #991 | Computing loss before
2022-04-23 14:24:55 | [train_policy] epoch #991 | Computing KL before
2022-04-23 14:24:55 | [train_policy] epoch #991 | Optimizing
2022-04-23 14:24:55 | [train_policy] epoch #991 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:55 | [train_policy] epoch #991 | computing loss before
2022-04-23 14:24:55 | [train_policy] epoch #991 | computing gradient
2022-04-23 14:24:55 | [train_policy] epoch #991 | gradient computed
2022-04-23 14:24:55 | [train_policy] epoch #991 | computing descent direction
2022-04-23 14:24:55 | [train_policy] epoch #991 | descent direction computed
2022-04-23 14:24:55 | [train_policy] epoch #991 | backtrack iters: 0
2022-04-23 14:24:55 | [train_policy] epoch #991 | optimization finished
2022-04-23 14:24:55 | [train_policy] epoch #991 | Computing KL after
2022-04-23 14:24:55 | [train_policy] epoch #991 | Computing loss after
2022-04-23 14:24:55 | [train_policy] epoch #991 | Fitting baseline...
2022-04-23 14:24:55 | [train_policy] epoch #991 | Saving snapshot...
2022-04-23 14:24:55 | [train_policy] epoch #991 | Saved
2022-04-23 14:24:55 | [train_policy] epoch #991 | Time 343.50 s
2022-04-23 14:24:55 | [train_policy] epoch #991 | EpochTime 0.32 s
---------------------------------------  ------------
EnvExecTime                                0.118843
Evaluation/AverageDiscountedReturn       -40.6117
Evaluation/AverageReturn                 -40.6117
Evaluation/CompletionRate                  0
Evaluation/Iteration                     991
Evaluation/MaxReturn                     -28.7949
Evaluation/MinReturn                     -70.0685
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       8.61359
Extras/EpisodeRewardMean                 -40.678
LinearFeatureBaseline/ExplainedVariance    0.847003
PolicyExecTime                             0.0962932
ProcessExecTime                            0.011162
TotalEnvSteps                              1.0039e+06
policy/Entropy                            -2.54355
policy/KL                                  0.00990786
policy/KLBefore                            0
policy/LossAfter                          -0.0344346
policy/LossBefore                          2.8271e-09
policy/Perplexity                          0.0785869
policy/dLoss                               0.0344346
---------------------------------------  ------------
2022-04-23 14:24:55 | [train_policy] epoch #992 | Obtaining samples for iteration 992...
2022-04-23 14:24:56 | [train_policy] epoch #992 | Logging diagnostics...
2022-04-23 14:24:56 | [train_policy] epoch #992 | Optimizing policy...
2022-04-23 14:24:56 | [train_policy] epoch #992 | Computing loss before
2022-04-23 14:24:56 | [train_policy] epoch #992 | Computing KL before
2022-04-23 14:24:56 | [train_policy] epoch #992 | Optimizing
2022-04-23 14:24:56 | [train_policy] epoch #992 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:56 | [train_policy] epoch #992 | computing loss before
2022-04-23 14:24:56 | [train_policy] epoch #992 | computing gradient
2022-04-23 14:24:56 | [train_policy] epoch #992 | gradient computed
2022-04-23 14:24:56 | [train_policy] epoch #992 | computing descent direction
2022-04-23 14:24:56 | [train_policy] epoch #992 | descent direction computed
2022-04-23 14:24:56 | [train_policy] epoch #992 | backtrack iters: 1
2022-04-23 14:24:56 | [train_policy] epoch #992 | optimization finished
2022-04-23 14:24:56 | [train_policy] epoch #992 | Computing KL after
2022-04-23 14:24:56 | [train_policy] epoch #992 | Computing loss after
2022-04-23 14:24:56 | [train_policy] epoch #992 | Fitting baseline...
2022-04-23 14:24:56 | [train_policy] epoch #992 | Saving snapshot...
2022-04-23 14:24:56 | [train_policy] epoch #992 | Saved
2022-04-23 14:24:56 | [train_policy] epoch #992 | Time 343.82 s
2022-04-23 14:24:56 | [train_policy] epoch #992 | EpochTime 0.32 s
---------------------------------------  -------------
EnvExecTime                                0.118597
Evaluation/AverageDiscountedReturn       -40.8435
Evaluation/AverageReturn                 -40.8435
Evaluation/CompletionRate                  0
Evaluation/Iteration                     992
Evaluation/MaxReturn                     -29.3948
Evaluation/MinReturn                     -63.6844
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.15421
Extras/EpisodeRewardMean                 -40.6981
LinearFeatureBaseline/ExplainedVariance    0.888087
PolicyExecTime                             0.0947523
ProcessExecTime                            0.0110748
TotalEnvSteps                              1.00492e+06
policy/Entropy                            -2.58073
policy/KL                                  0.00655424
policy/KLBefore                            0
policy/LossAfter                          -0.0169048
policy/LossBefore                          3.18048e-09
policy/Perplexity                          0.0757189
policy/dLoss                               0.0169048
---------------------------------------  -------------
2022-04-23 14:24:56 | [train_policy] epoch #993 | Obtaining samples for iteration 993...
2022-04-23 14:24:56 | [train_policy] epoch #993 | Logging diagnostics...
2022-04-23 14:24:56 | [train_policy] epoch #993 | Optimizing policy...
2022-04-23 14:24:56 | [train_policy] epoch #993 | Computing loss before
2022-04-23 14:24:56 | [train_policy] epoch #993 | Computing KL before
2022-04-23 14:24:56 | [train_policy] epoch #993 | Optimizing
2022-04-23 14:24:56 | [train_policy] epoch #993 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:56 | [train_policy] epoch #993 | computing loss before
2022-04-23 14:24:56 | [train_policy] epoch #993 | computing gradient
2022-04-23 14:24:56 | [train_policy] epoch #993 | gradient computed
2022-04-23 14:24:56 | [train_policy] epoch #993 | computing descent direction
2022-04-23 14:24:56 | [train_policy] epoch #993 | descent direction computed
2022-04-23 14:24:56 | [train_policy] epoch #993 | backtrack iters: 0
2022-04-23 14:24:56 | [train_policy] epoch #993 | optimization finished
2022-04-23 14:24:56 | [train_policy] epoch #993 | Computing KL after
2022-04-23 14:24:56 | [train_policy] epoch #993 | Computing loss after
2022-04-23 14:24:56 | [train_policy] epoch #993 | Fitting baseline...
2022-04-23 14:24:56 | [train_policy] epoch #993 | Saving snapshot...
2022-04-23 14:24:56 | [train_policy] epoch #993 | Saved
2022-04-23 14:24:56 | [train_policy] epoch #993 | Time 344.15 s
2022-04-23 14:24:56 | [train_policy] epoch #993 | EpochTime 0.32 s
---------------------------------------  -------------
EnvExecTime                                0.118702
Evaluation/AverageDiscountedReturn       -39.3829
Evaluation/AverageReturn                 -39.3829
Evaluation/CompletionRate                  0
Evaluation/Iteration                     993
Evaluation/MaxReturn                     -28.9762
Evaluation/MinReturn                     -63.665
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.92559
Extras/EpisodeRewardMean                 -39.5825
LinearFeatureBaseline/ExplainedVariance    0.885277
PolicyExecTime                             0.0961738
ProcessExecTime                            0.0111072
TotalEnvSteps                              1.00593e+06
policy/Entropy                            -2.57462
policy/KL                                  0.00988863
policy/KLBefore                            0
policy/LossAfter                          -0.0138568
policy/LossBefore                         -5.6542e-09
policy/Perplexity                          0.0761829
policy/dLoss                               0.0138568
---------------------------------------  -------------
2022-04-23 14:24:56 | [train_policy] epoch #994 | Obtaining samples for iteration 994...
2022-04-23 14:24:56 | [train_policy] epoch #994 | Logging diagnostics...
2022-04-23 14:24:56 | [train_policy] epoch #994 | Optimizing policy...
2022-04-23 14:24:56 | [train_policy] epoch #994 | Computing loss before
2022-04-23 14:24:56 | [train_policy] epoch #994 | Computing KL before
2022-04-23 14:24:56 | [train_policy] epoch #994 | Optimizing
2022-04-23 14:24:56 | [train_policy] epoch #994 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:56 | [train_policy] epoch #994 | computing loss before
2022-04-23 14:24:56 | [train_policy] epoch #994 | computing gradient
2022-04-23 14:24:56 | [train_policy] epoch #994 | gradient computed
2022-04-23 14:24:56 | [train_policy] epoch #994 | computing descent direction
2022-04-23 14:24:56 | [train_policy] epoch #994 | descent direction computed
2022-04-23 14:24:56 | [train_policy] epoch #994 | backtrack iters: 1
2022-04-23 14:24:56 | [train_policy] epoch #994 | optimization finished
2022-04-23 14:24:56 | [train_policy] epoch #994 | Computing KL after
2022-04-23 14:24:56 | [train_policy] epoch #994 | Computing loss after
2022-04-23 14:24:56 | [train_policy] epoch #994 | Fitting baseline...
2022-04-23 14:24:56 | [train_policy] epoch #994 | Saving snapshot...
2022-04-23 14:24:56 | [train_policy] epoch #994 | Saved
2022-04-23 14:24:56 | [train_policy] epoch #994 | Time 344.48 s
2022-04-23 14:24:56 | [train_policy] epoch #994 | EpochTime 0.33 s
---------------------------------------  -------------
EnvExecTime                                0.120038
Evaluation/AverageDiscountedReturn       -40.5644
Evaluation/AverageReturn                 -40.5644
Evaluation/CompletionRate                  0
Evaluation/Iteration                     994
Evaluation/MaxReturn                     -30.1891
Evaluation/MinReturn                     -71.9339
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.24925
Extras/EpisodeRewardMean                 -40.7464
LinearFeatureBaseline/ExplainedVariance    0.848326
PolicyExecTime                             0.0976591
ProcessExecTime                            0.0112751
TotalEnvSteps                              1.00694e+06
policy/Entropy                            -2.60148
policy/KL                                  0.00661606
policy/KLBefore                            0
policy/LossAfter                          -0.0124568
policy/LossBefore                         -6.12538e-09
policy/Perplexity                          0.0741634
policy/dLoss                               0.0124568
---------------------------------------  -------------
2022-04-23 14:24:56 | [train_policy] epoch #995 | Obtaining samples for iteration 995...
2022-04-23 14:24:57 | [train_policy] epoch #995 | Logging diagnostics...
2022-04-23 14:24:57 | [train_policy] epoch #995 | Optimizing policy...
2022-04-23 14:24:57 | [train_policy] epoch #995 | Computing loss before
2022-04-23 14:24:57 | [train_policy] epoch #995 | Computing KL before
2022-04-23 14:24:57 | [train_policy] epoch #995 | Optimizing
2022-04-23 14:24:57 | [train_policy] epoch #995 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:57 | [train_policy] epoch #995 | computing loss before
2022-04-23 14:24:57 | [train_policy] epoch #995 | computing gradient
2022-04-23 14:24:57 | [train_policy] epoch #995 | gradient computed
2022-04-23 14:24:57 | [train_policy] epoch #995 | computing descent direction
2022-04-23 14:24:57 | [train_policy] epoch #995 | descent direction computed
2022-04-23 14:24:57 | [train_policy] epoch #995 | backtrack iters: 1
2022-04-23 14:24:57 | [train_policy] epoch #995 | optimization finished
2022-04-23 14:24:57 | [train_policy] epoch #995 | Computing KL after
2022-04-23 14:24:57 | [train_policy] epoch #995 | Computing loss after
2022-04-23 14:24:57 | [train_policy] epoch #995 | Fitting baseline...
2022-04-23 14:24:57 | [train_policy] epoch #995 | Saving snapshot...
2022-04-23 14:24:57 | [train_policy] epoch #995 | Saved
2022-04-23 14:24:57 | [train_policy] epoch #995 | Time 344.81 s
2022-04-23 14:24:57 | [train_policy] epoch #995 | EpochTime 0.32 s
---------------------------------------  -------------
EnvExecTime                                0.119508
Evaluation/AverageDiscountedReturn       -41.2709
Evaluation/AverageReturn                 -41.2709
Evaluation/CompletionRate                  0
Evaluation/Iteration                     995
Evaluation/MaxReturn                     -29.3453
Evaluation/MinReturn                     -75.1396
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.35241
Extras/EpisodeRewardMean                 -41.2965
LinearFeatureBaseline/ExplainedVariance    0.837752
PolicyExecTime                             0.0955286
ProcessExecTime                            0.0112247
TotalEnvSteps                              1.00795e+06
policy/Entropy                            -2.63418
policy/KL                                  0.00669312
policy/KLBefore                            0
policy/LossAfter                          -0.00690469
policy/LossBefore                          4.47624e-09
policy/Perplexity                          0.071778
policy/dLoss                               0.0069047
---------------------------------------  -------------
2022-04-23 14:24:57 | [train_policy] epoch #996 | Obtaining samples for iteration 996...
2022-04-23 14:24:57 | [train_policy] epoch #996 | Logging diagnostics...
2022-04-23 14:24:57 | [train_policy] epoch #996 | Optimizing policy...
2022-04-23 14:24:57 | [train_policy] epoch #996 | Computing loss before
2022-04-23 14:24:57 | [train_policy] epoch #996 | Computing KL before
2022-04-23 14:24:57 | [train_policy] epoch #996 | Optimizing
2022-04-23 14:24:57 | [train_policy] epoch #996 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:57 | [train_policy] epoch #996 | computing loss before
2022-04-23 14:24:57 | [train_policy] epoch #996 | computing gradient
2022-04-23 14:24:57 | [train_policy] epoch #996 | gradient computed
2022-04-23 14:24:57 | [train_policy] epoch #996 | computing descent direction
2022-04-23 14:24:57 | [train_policy] epoch #996 | descent direction computed
2022-04-23 14:24:57 | [train_policy] epoch #996 | backtrack iters: 0
2022-04-23 14:24:57 | [train_policy] epoch #996 | optimization finished
2022-04-23 14:24:57 | [train_policy] epoch #996 | Computing KL after
2022-04-23 14:24:57 | [train_policy] epoch #996 | Computing loss after
2022-04-23 14:24:57 | [train_policy] epoch #996 | Fitting baseline...
2022-04-23 14:24:57 | [train_policy] epoch #996 | Saving snapshot...
2022-04-23 14:24:57 | [train_policy] epoch #996 | Saved
2022-04-23 14:24:57 | [train_policy] epoch #996 | Time 345.13 s
2022-04-23 14:24:57 | [train_policy] epoch #996 | EpochTime 0.32 s
---------------------------------------  -------------
EnvExecTime                                0.119214
Evaluation/AverageDiscountedReturn       -39.7068
Evaluation/AverageReturn                 -39.7068
Evaluation/CompletionRate                  0
Evaluation/Iteration                     996
Evaluation/MaxReturn                     -29.0302
Evaluation/MinReturn                     -63.6445
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       7.69863
Extras/EpisodeRewardMean                 -40.0846
LinearFeatureBaseline/ExplainedVariance    0.868113
PolicyExecTime                             0.0949113
ProcessExecTime                            0.0110984
TotalEnvSteps                              1.00896e+06
policy/Entropy                            -2.63601
policy/KL                                  0.0099617
policy/KLBefore                            0
policy/LossAfter                          -0.030642
policy/LossBefore                          5.6542e-09
policy/Perplexity                          0.0716469
policy/dLoss                               0.030642
---------------------------------------  -------------
2022-04-23 14:24:57 | [train_policy] epoch #997 | Obtaining samples for iteration 997...
2022-04-23 14:24:57 | [train_policy] epoch #997 | Logging diagnostics...
2022-04-23 14:24:57 | [train_policy] epoch #997 | Optimizing policy...
2022-04-23 14:24:57 | [train_policy] epoch #997 | Computing loss before
2022-04-23 14:24:57 | [train_policy] epoch #997 | Computing KL before
2022-04-23 14:24:57 | [train_policy] epoch #997 | Optimizing
2022-04-23 14:24:57 | [train_policy] epoch #997 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:57 | [train_policy] epoch #997 | computing loss before
2022-04-23 14:24:57 | [train_policy] epoch #997 | computing gradient
2022-04-23 14:24:57 | [train_policy] epoch #997 | gradient computed
2022-04-23 14:24:57 | [train_policy] epoch #997 | computing descent direction
2022-04-23 14:24:57 | [train_policy] epoch #997 | descent direction computed
2022-04-23 14:24:57 | [train_policy] epoch #997 | backtrack iters: 0
2022-04-23 14:24:57 | [train_policy] epoch #997 | optimization finished
2022-04-23 14:24:57 | [train_policy] epoch #997 | Computing KL after
2022-04-23 14:24:57 | [train_policy] epoch #997 | Computing loss after
2022-04-23 14:24:57 | [train_policy] epoch #997 | Fitting baseline...
2022-04-23 14:24:57 | [train_policy] epoch #997 | Saving snapshot...
2022-04-23 14:24:57 | [train_policy] epoch #997 | Saved
2022-04-23 14:24:57 | [train_policy] epoch #997 | Time 345.45 s
2022-04-23 14:24:57 | [train_policy] epoch #997 | EpochTime 0.32 s
---------------------------------------  -------------
EnvExecTime                                0.119125
Evaluation/AverageDiscountedReturn       -40.0835
Evaluation/AverageReturn                 -40.0835
Evaluation/CompletionRate                  0
Evaluation/Iteration                     997
Evaluation/MaxReturn                     -29.0973
Evaluation/MinReturn                     -74.9073
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       8.81222
Extras/EpisodeRewardMean                 -40.4086
LinearFeatureBaseline/ExplainedVariance    0.835345
PolicyExecTime                             0.0952833
ProcessExecTime                            0.0111303
TotalEnvSteps                              1.00998e+06
policy/Entropy                            -2.62639
policy/KL                                  0.0097202
policy/KLBefore                            0
policy/LossAfter                          -0.0265581
policy/LossBefore                          1.41355e-09
policy/Perplexity                          0.0723394
policy/dLoss                               0.0265581
---------------------------------------  -------------
2022-04-23 14:24:57 | [train_policy] epoch #998 | Obtaining samples for iteration 998...
2022-04-23 14:24:57 | [train_policy] epoch #998 | Logging diagnostics...
2022-04-23 14:24:57 | [train_policy] epoch #998 | Optimizing policy...
2022-04-23 14:24:57 | [train_policy] epoch #998 | Computing loss before
2022-04-23 14:24:57 | [train_policy] epoch #998 | Computing KL before
2022-04-23 14:24:57 | [train_policy] epoch #998 | Optimizing
2022-04-23 14:24:58 | [train_policy] epoch #998 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:58 | [train_policy] epoch #998 | computing loss before
2022-04-23 14:24:58 | [train_policy] epoch #998 | computing gradient
2022-04-23 14:24:58 | [train_policy] epoch #998 | gradient computed
2022-04-23 14:24:58 | [train_policy] epoch #998 | computing descent direction
2022-04-23 14:24:58 | [train_policy] epoch #998 | descent direction computed
2022-04-23 14:24:58 | [train_policy] epoch #998 | backtrack iters: 1
2022-04-23 14:24:58 | [train_policy] epoch #998 | optimization finished
2022-04-23 14:24:58 | [train_policy] epoch #998 | Computing KL after
2022-04-23 14:24:58 | [train_policy] epoch #998 | Computing loss after
2022-04-23 14:24:58 | [train_policy] epoch #998 | Fitting baseline...
2022-04-23 14:24:58 | [train_policy] epoch #998 | Saving snapshot...
2022-04-23 14:24:58 | [train_policy] epoch #998 | Saved
2022-04-23 14:24:58 | [train_policy] epoch #998 | Time 345.78 s
2022-04-23 14:24:58 | [train_policy] epoch #998 | EpochTime 0.32 s
---------------------------------------  -------------
EnvExecTime                                0.119153
Evaluation/AverageDiscountedReturn       -42.2294
Evaluation/AverageReturn                 -42.2294
Evaluation/CompletionRate                  0
Evaluation/Iteration                     998
Evaluation/MaxReturn                     -29.4686
Evaluation/MinReturn                     -76.4234
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       9.48419
Extras/EpisodeRewardMean                 -41.7552
LinearFeatureBaseline/ExplainedVariance    0.832991
PolicyExecTime                             0.0940559
ProcessExecTime                            0.0113196
TotalEnvSteps                              1.01099e+06
policy/Entropy                            -2.62611
policy/KL                                  0.00649935
policy/KLBefore                            0
policy/LossAfter                          -0.0227515
policy/LossBefore                          1.88473e-09
policy/Perplexity                          0.0723597
policy/dLoss                               0.0227515
---------------------------------------  -------------
2022-04-23 14:24:58 | [train_policy] epoch #999 | Obtaining samples for iteration 999...
2022-04-23 14:24:58 | [train_policy] epoch #999 | Logging diagnostics...
2022-04-23 14:24:58 | [train_policy] epoch #999 | Optimizing policy...
2022-04-23 14:24:58 | [train_policy] epoch #999 | Computing loss before
2022-04-23 14:24:58 | [train_policy] epoch #999 | Computing KL before
2022-04-23 14:24:58 | [train_policy] epoch #999 | Optimizing
2022-04-23 14:24:58 | [train_policy] epoch #999 | Start CG optimization: #parameters: 1348, #inputs: 92, #subsample_inputs: 92
2022-04-23 14:24:58 | [train_policy] epoch #999 | computing loss before
2022-04-23 14:24:58 | [train_policy] epoch #999 | computing gradient
2022-04-23 14:24:58 | [train_policy] epoch #999 | gradient computed
2022-04-23 14:24:58 | [train_policy] epoch #999 | computing descent direction
2022-04-23 14:24:58 | [train_policy] epoch #999 | descent direction computed
2022-04-23 14:24:58 | [train_policy] epoch #999 | backtrack iters: 1
2022-04-23 14:24:58 | [train_policy] epoch #999 | optimization finished
2022-04-23 14:24:58 | [train_policy] epoch #999 | Computing KL after
2022-04-23 14:24:58 | [train_policy] epoch #999 | Computing loss after
2022-04-23 14:24:58 | [train_policy] epoch #999 | Fitting baseline...
2022-04-23 14:24:58 | [train_policy] epoch #999 | Saving snapshot...
2022-04-23 14:24:58 | [train_policy] epoch #999 | Saved
2022-04-23 14:24:58 | [train_policy] epoch #999 | Time 346.11 s
2022-04-23 14:24:58 | [train_policy] epoch #999 | EpochTime 0.32 s
---------------------------------------  -------------
EnvExecTime                                0.119135
Evaluation/AverageDiscountedReturn       -40.8957
Evaluation/AverageReturn                 -40.8957
Evaluation/CompletionRate                  0
Evaluation/Iteration                     999
Evaluation/MaxReturn                     -29.5046
Evaluation/MinReturn                     -75.1812
Evaluation/NumTrajs                       92
Evaluation/StdReturn                       8.75171
Extras/EpisodeRewardMean                 -41.4122
LinearFeatureBaseline/ExplainedVariance    0.826387
PolicyExecTime                             0.0957811
ProcessExecTime                            0.0110624
TotalEnvSteps                              1.012e+06
policy/Entropy                            -2.65341
policy/KL                                  0.00670469
policy/KLBefore                            0
policy/LossAfter                          -0.0188176
policy/LossBefore                          5.18301e-09
policy/Perplexity                          0.0704109
policy/dLoss                               0.0188176
---------------------------------------  -------------
